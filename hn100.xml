<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 28 May 2025 08:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Show HN: AutoThink – Boosts local LLM performance with adaptive reasoning (197 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44112326</link>
            <guid>44112326</guid>
            <pubDate>Wed, 28 May 2025 02:39:11 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44112326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I built AutoThink, a technique that makes local LLMs reason more efficiently by adaptively allocating computational resources based on query complexity.</p><p>The core idea: instead of giving every query the same "thinking time," classify queries as HIGH or LOW complexity and allocate thinking tokens accordingly. Complex reasoning gets 70-90% of tokens, simple queries get 20-40%.</p><p>I also implemented steering vectors derived from Pivotal Token Search (originally from Microsoft's Phi-4 paper) that guide the model's reasoning patterns during generation. These vectors encourage behaviors like numerical accuracy, self-correction, and thorough exploration.</p><p>Results on DeepSeek-R1-Distill-Qwen-1.5B:</p><p>- GPQA-Diamond: 31.06% vs 21.72% baseline (+43% relative improvement)</p><p>- MMLU-Pro: 26.38% vs 25.58% baseline</p><p>- Uses fewer tokens than baseline approaches</p><p>Works with any local reasoning model - DeepSeek, Qwen, custom fine-tuned models. No API dependencies.</p><p>The technique builds on two things I developed: an adaptive classification framework that can learn new complexity categories without retraining, and an open source implementation of Pivotal Token Search.</p><p>Technical paper: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327" rel="nofollow">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327</a></p><p>Code and examples: <a href="https://github.com/codelion/optillm/tree/main/optillm/autothink">https://github.com/codelion/optillm/tree/main/optillm/autoth...</a></p><p>PTS implementation: <a href="https://github.com/codelion/pts">https://github.com/codelion/pts</a></p><p>I'm curious about your thoughts on adaptive resource allocation for AI reasoning. Have you tried similar approaches with your local models?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Look ma, no bubbles designing a low-latency megakernel for Llama-1B (127 pts)]]></title>
            <link>https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles</link>
            <guid>44111673</guid>
            <pubDate>Wed, 28 May 2025 00:01:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles">https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles</a>, See on <a href="https://news.ycombinator.com/item?id=44111673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There are some applications that benefit from running LLMs really, really fast. This low-latency regime encompasses applications like chatbots and human-in-the-loop workflows, where users care a lot about seeing responses come back immediately.</p>
<p>Given the importance of these low-latency workloads, we wanted to explore just how fast we can run open-source models on modern GPUs. To really stress-test existing systems, we consider an aggressive low-latency scenario where we generate a single sequence with Llama-3.2-1B. This workload is strongly memory bound – our performance is dominated by how fast we can load model weights from GPU global memory.</p>
<p>It turns out that popular LLM inference engines – vLLM and SGLang – are only able to use at most 50% of available GPU bandwidth when running this workload on an H100. The root of the problem, which we'll describe more below, is that existing systems break down a model forward pass into around <strong>a hundred separate kernels</strong> that each implement a few operations (e.g. RMS norm, attention, an MLP layer + activation, rotary). Each kernel comes with a setup and teardown period and during this time no useful work gets done – for instance, the all-important task of loading model weights is stalled.</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/result.png" alt="Performance comparison graph"></p><p><em>Figure 1: Speed! Results generated with a 32-token prompt and 128 generated tokens, with no speculation</em></p></div>
<p>In this post, we show how we can bypass this problem by merging the entire Llama-1B forward pass into a single "megakernel" that eliminates kernel boundaries altogether. Doing this achieves brr – on an H100, we use 78% of memory bandwidth and outperform existing systems by over 1.5x. (To our knowledge, this is the lowest-latency forward pass for Llama-1B in bfloat16!) In the rest of this post, we'll walk through how and why one would do this. Specifically:</p>
<ul>
<li>First, we'll talk about how small kernels lead to AI systems that underutilize the GPU's full bandwidth.</li>
<li>Second, we'll describe three important points about how we built our megakernel: how we fused lots of kernels together, how we share hardware resources across them to minimize overhead, and how we synchronize them efficiently.</li>
</ul>
<p>If you're interested in learning more of the details or using these ideas yourself, we're <a href="https://github.com/HazyResearch/Megakernels">open-sourcing all of our code here</a>.</p>
<h2>Separate Kernels Kill the Vibe</h2>
<p>In general, the way one runs code on a GPU is by launching a "kernel" – a small program that does a well-defined operation (e.g. RMS norm, MLP). Today, all AI workloads run as long sequences of relatively small kernels. To get an initial sense, let's look at the operations in the Llama-1B transformer block, and some example kernel boundaries of how they might be divided up (Figure 2).</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/kernel_boundaries.png" alt="Kernel boundaries diagram"></p><p><em>Figure 2: An example set of kernel boundaries for the Llama-1B transformer block. Red boxes delineate the work done by individual kernels.</em></p></div>
<p>As we described earlier, decoding a single sequence with Llama-1B is a purely memory-bound workload: our performance depends on being able to <strong>always</strong> be loading weights from GPU global memory. So, why are existing approaches so far from using the full bandwidth of the GPU?</p>
<p>When we dug into it, we noticed a key problem was that the current kernel-based approach to running models introduces stalls that prevent us from constantly loading memory:</p>
<ul>
<li>First: GPU kernels are launched with a strict ordering, so that a thread block in one kernel can't start until all thread blocks in previous kernels have completely finished. Consequently, every time we start a kernel, we have to wait for all the straggler thread blocks from the prior one to finish. For example, if a kernel runs 512 thread blocks (like our Llama-1B down projection), but we only have 148 streaming multiprocessors (like on a B200), we end up with 80 empty SM's at the end.</li>
<li>Second, as we've <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">previously highlighted</a>, each kernel launch and teardown incurs costs. In principle, NVIDIA's CUDA graphs can help hide costs, but by our measurements they still leave a lot on the table. For a simple dummy kernel (which dumps a start time, sleeps, and dumps an end time) on an H100, we find that running on a CUDA stream incurs a launch cost of about 2.1 microseconds, and with CUDA graphs the launch cost only decreases to around 1.3 microseconds – time spent with the GPU doing no useful work! We'd like to have the GPU spend all of its time doing useful work.</li>
<li>Finally, even after we start the next kernel, we still have to wait to load weights and activations before any compute can start. These latencies leave the GPU sitting idle for thousands of cycles! Ideally, we'd start loading the next weights while the previous computations and stores are happening. NVIDIA has also built a mechanism for this called <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#programmatic-dependent-launch-and-synchronization">Programmatic Dependent Launch</a> (PDL), which allows the next kernel to start preparing while the previous kernel is running, but we found it still introduces unnecessary stalls because the PDL synchronization mechanism (cudaGridDependencySynchronize) is very coarse. For example, it means we have to wait for all queries, keys, and values to complete in order to start attention, as opposed to starting heads as soon as they are ready. We'll later show another specific case of where this is useful in Llama-1B.</li>
</ul>
<p>Taken together, these form the "memory pipeline bubbles" our title references – and they represent a key reason that we're <strong>not always loading from memory</strong>.  For short operations, these pauses add up, wasting a huge chunk of potential bandwidth. In part, this is because Llama-1B (actually 1.24B parameters) in batch size 1 is just so... small: if each operation is really fast, then the time spent in-between them really starts to matter.</p>
<p>To illustrate the magnitude of the problem: for single-sequence generation in 16-bit precision on a single H100, the <strong>memory limit</strong> is 3.35TB/s / 2.48GB = ~1350 forward passes per second. But with 7 kernel launches per layer, and 16 layers, even with an optimistic 5 us of stalling per kernel (counting stragglers, kernel launch, and memory latencies), generation would run at just ~770 forward passes per second. In practice, it's often worse. On low-latency workloads, GPUs spend only a fraction of their time actually doing any useful work!</p>
<p>So while CUDA does provide some existing features (e.g. graphs, streams, PDL) to partially solve these problems, we wanted to see if a different approach could solve all of these problems, where we just fuse the entire model forward pass into a single kernel.</p>
<h2>How to Megakernel</h2>
<p>Next, we'll show you how we fused a whole Llama forward pass into a single kernel, and our methods for resolving three key problems:</p>
<ol>
<li>Fusing dozens of operations is hard to do from scratch. We need a mechanism for executing these operations within the megakernel.</li>
<li>In order to overlap multiple operations on the same hardware, we need to prevent contention over limited resources, such as shared memory.</li>
<li>The GPU synchronizes after each kernel in the traditional kernel model. Without kernels, we have to synchronize the GPU all by ourselves!</li>
</ol>
<p>Let's start with the first issue:</p>
<h4>Issue 1/3: Fusing Lots of Operations</h4>
<p>Traditional kernel fusion generally merges just two or three operations together. In contrast, we need to fuse about a hundred. Consequently, we need to have a sensible abstraction for how we can actually program a megakernel.</p>
<p>Our approach is built on an on-GPU interpreter – essentially a more sophisticated version of our infrastructure underlying <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">ThunderMLA</a>. Our interpreter is designed such that each streaming multiprocessor (SM) within the GPU receives a sequence of <strong>instructions</strong> (each implemented using the same CUDA template) and executes them. We <strong>schedule</strong> each SM's instruction sequence ahead of time on the Python side, and notably we can reuse each schedule for hundreds of forward passes!</p>
<p>For our end-to-end Llama forwards pass megakernel, we define the following set of instructions:</p>
<ul>
<li>A fused RMS norm &amp; QKV &amp; RoPE instruction.</li>
<li>An attention computation instruction.</li>
<li>An attention reduction instruction (for ThunderGQA on long sequences).</li>
<li>An O-projection + residual instruction.</li>
<li>A fused RMS norm &amp; up-gate &amp; SiLU instruction.</li>
<li>A down-projection + residual instruction.</li>
<li>An RMS norm &amp; language modeling head instruction, for computing the final token logits.</li>
</ul>
<p>We implement each of these instructions using a common <a href="https://github.com/HazyResearch/Megakernels/blob/main/util/mk_init/sources/src/%7B%7BPROJECT_NAME_LOWER%7D%7D.cu">CUDA template</a> (with load, store, compute boilerplate functions), facilitating interoperability within our interpreter framework.</p>
<h4>Issue 2/3: <span>S</span><span>h</span><span>a</span><span>r</span><span>i</span><span>n</span><span>g</span> Shared Memory to Eliminate Memory Bubbles</h4>
<p>The instruction-and-interpreter structure lets us cleanly organize our megakernel. However, we haven't yet addressed the key issue: making sure that model weights are always being loaded in order to maximize memory bandwidth utilization.</p>
<p>The reason why a megakernel lets us solve this problem is that we can pipeline memory loads across instructions: our interpreter will start loading the model weights for an instruction as soon as it can, even if a previous instruction is still finishing up (e.g. storing out its results to global memory). It's this tight transitioning between instructions that minimizes the memory bubbles that would otherwise appear if we launched multiple kernels.</p>
<p>However, there's a catch: loading the weights from global memory for the next instruction doesn't do you much good if you have no place to put the data you loaded! More precisely, all of our weight matrices are loaded from GPU global memory into our SM's "shared memory" – NVIDIA's term for the fast memory on each SM. Shared memory is a scarce resource on each SM, and we can't start a load for a new instruction if a previous instruction is using all of it. This necessitates a way to keep track of which instruction is using which piece of shared memory and quickly transition shared memory to the next instruction when the current instruction is done with it.</p>
<p>We accomplish this by <strong>paging</strong> shared memory. We first divide the first 213kB of shared memory on an H100 into 13 16KiB pages, and use remaining shared memory for special purposes, like storing instruction parameters. To use one of these pages, instructions have to explicitly request and release them from the interpreter. The interpreter automatically passes released pages to the next instruction, allowing them to start issuing memory loads as early as shared memory becomes available.</p>
<h4>Issue 3/3: Synchronization</h4>
<p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/thanos.png" alt="Thanos illustration"></p>
<p>While megakernels let us minimize pipeline bubbles, they also introduce a new problem: synchronization. The performance limitation with the normal many-kernel execution model is that no thread blocks in a kernel can start until all thread blocks in previous kernels are finished. However, it's precisely this property that makes it easy to manage data dependencies. When a kernel launches, CUDA guarantees that all of the kernel's input tensors have already been produced and are safe to read from immediately.</p>
<p>With megakernels, we have no such guarantees: when an SM starts to execute a new instruction, its inputs might not be ready! To address this, we explicitly synchronize the instructions inside of our megakernel. We accomplish this with a simple counter system. Before the megakernel launches, we initialize an array of counters (i.e. integers) in GPU global memory with a starting value of zero. Whenever an instruction completes, it increments one of these counters. Similarly, whenever a new instruction starts, it must wait for some of these counters to reach a target value, indicating that all of its dependencies have finished.</p>
<p>One optimization this enables is in the big multi-layer perceptrons (MLPs) in Llama-1B.</p>
<ul>
<li>In a naive implementation using PDL, one must await completing the whole hidden state before beginning the down projection matrix multiply.</li>
<li>We instead produce and consume the intermediate state in four chunks, each with their own counter. This way, an instruction for the down projection only needs to wait for its input chunk to finish.</li>
</ul>
<h2>Putting It All Together</h2>
<p>To our knowledge, our H100 megakernel represents the first time anyone has run the forward pass for a 16-bit 1B+ parameter language model in under one millisecond on a GPU. Our B200 implementation pushes this even further to under 680 microseconds per forward pass!</p>
<p>As shown in Figure 1, our megakernel outperforms vLLM and SGLang baselines (which use CUDA graphs and torch compilation):</p>
<ul>
<li>On an H100, our megakernel runs almost 2.5x faster than vLLM and over 1.5x faster than SGLang.</li>
<li>On a B200, the gap with vLLM rises to over 3.5x, and we remain more than 1.5x faster than SGLang, too.</li>
</ul>
<p>We're still actually quite a ways off from the theoretical limit on a B200, which is around ~3,000 forward passes per second. Part of this gap is because this theoretical limit is based purely on memory bandwidth – but we still have to wait to load activations. And although these activations are small (and don't cost a lot of bandwidth), there are still latencies in loading them that we can't hide. A breakdown of the runtime of our current B200 forward pass (total runtime 600 microseconds):</p>
<ul>
<li>250 microseconds are spent storing activations, awaiting consistency, and loading them. This is about 20% higher than a simple model would suggest: since each instruction has a dependence on the last one, we need to pay two load latencies (check ready, and then load activations) and two store latencies (store activations, then mark ready) per instruction. Using ~500 nanoseconds latency per load / store, this would impose about 200 microseconds of overhead. (We suspect some of the remaining 50 microseconds comes from time spent processing atomics in global memory.)</li>
<li>200 microseconds are spent actually running RMS norm and matrix-vector computations. 95% of this portion is devoted to matrix-vector. On Blackwell, we find that using the tensor cores is marginally helpful for this; on Hopper, we find it better to simply run on the CUDA cores. This difference comes from the fact that both GPUs have relatively similar CUDA core performance, but Blackwell tensor cores are much faster.</li>
<li>30 microseconds are spent awaiting weights from global memory (pipelining works!) Of these, 40% are spent in the LM head, which is the best-pipelined part of the whole megakernel due to its homogeneity and huge size.</li>
<li>40 microseconds are spent on low-level synchronization overhead across warps. A key issue here is that CUDA's asynchronous barriers are relatively slow, even when they're already in the "pass" state, requiring about 60 nanoseconds each time.</li>
<li>80 microseconds are on setup and various other overheads (e.g. passing instruction barriers, marking pages as complete, etc.)</li>
</ul>
<p>We think there's probably more to do on each of these, but that'll have to wait for a future update!</p>
<h2>The Megakernel Cinematic Universe</h2>
<p>In this blog, we focus narrowly on designing a megakernel for low-latency, batch-size one LLM inference. However, we believe that the ability to more precisely control GPU execution with megakernels can more generally be applied to accelerate a much broader set of AI workloads. Stay tuned!</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/sonic.png" alt="Sonic illustration"></p><p><strong>The Main Message of this Blog Post</strong></p></div>
<p>If you'd like to learn more, please reach out to Ben or Jordan! Please include a tribute of at least five pictures of kittens in your email.</p>
<ul>
<li>Ben: <a href="mailto:bfs@stanford.edu">bfs@stanford.edu</a></li>
<li>Jordan: <a href="mailto:jbj@stanford.edu">jbj@stanford.edu</a></li>
</ul>
<p>And many, many thanks to Together AI for generously providing us with B200s and H100s to do this work, which would not have been possible without them!</p>
<p>See also: <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla"><strong>pretty big kernels</strong></a> | <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk"><strong>regular kernels</strong></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A UEFI app that sends LLDP-MED pkt at boot to negotiate PoE+ power before the OS (132 pts)]]></title>
            <link>https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution</link>
            <guid>44111609</guid>
            <pubDate>Tue, 27 May 2025 23:45:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution">https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution</a>, See on <a href="https://news.ycombinator.com/item?id=44111609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Back in 2015, I was working on a project to build PoE-powered embedded x86 computers and digital signage systems. These were full Windows 10 Professional machines running Intel Atom processors, designed to simplify deployment by drawing power directly over Ethernet. Our goal was to eliminate the need to run traditional AC power to these devices, which can be costly and impractical in many deployment scenarios. But unlike typical IoT or low-power devices, these were full-fledged x86 computers that required more power than what the standard PoE (802.3af) could deliver, which maxes out at 15.4W at the PSE (Power Sourcing Equipment), such as a PoE network switch or injector.</p>
<p>Our device required about 23W when fully operational, which pushed us into <strong>802.3at (PoE+)</strong> territory. In most client environments their PoE+ switches provided the power we needed with no problem. But some environments had network switches that would not give us the additional power.</p>
<h3><strong>PoE Standards Overview (IEEE 802.3)</strong></h3>
<table>
<thead>
<tr>
<th>Standard</th>
<th>Max Power at PSE</th>
<th>Max Power at PD</th>
<th>Voltage Range</th>
<th>Pairs Used</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>802.3af (PoE)</strong></td>
<td>15.4 W</td>
<td>12.95 W</td>
<td>44–57 V DC</td>
<td>2 pairs</td>
<td>2003</td>
</tr>
<tr>
<td><strong>802.3at (PoE+)</strong></td>
<td>30 W</td>
<td>25.5 W</td>
<td>50–57 V DC</td>
<td>2 pairs</td>
<td>2009</td>
</tr>
</tbody>
</table>
<p>The problem was that our embedded systems only supported physical‑layer classification which is limited to signaling power requirements through resistive detection and pulsed current signatures during initial PoE handshaking. Only relying on this method can be problematic if the switch is configured to require LLDP for Data Link Layer Classification for devices requiring more than 15.4W. Which is a problem because at minimum our computers required at least 18W in order to boot into the operating system. So our systems would initially start to boot, but then eventually shut off before it got into Windows. We were stuck in a frustrating Catch-22, we needed to send LLDP packets to get more power, but we couldn’t even boot the OS to send them.</p>
<h3><strong>So What Do You Do When the OS Can't Help?</strong></h3>
<p>We did some testing and measured power draw during various phases of the boot cycle. Fortunately, the system's power needs during initial startup (BIOS/UEFI initialization) were low enough to stay under the 15.4W limit. That gave us a brief window to request more power <em>before</em> booting Windows.</p>
<p>So the challenge became: negotiate higher PoE+ power <strong>before</strong> Windows starts. The answer was to handle LLDP negotiation at the BIOS level, or more accurately the UEFI (Unified Extensible Firmware Interface) firmware.  Through our research we discovered that UEFI supports the TCP/IP protocol and has access to the network stack, enabling communication over Ethernet without an OS.</p>
<p>Our first attempt was to work with the motherboard vendor and AMI (the BIOS provider) for a custom firmware build. We signed NDAs and had multiple discussions, but despite our efforts, they ultimately declined to create a custom BIOS for us. After hitting that roadblock and feeling the frustration of stalled progress, I refused to give up. I dug deeper and came across the concept of <strong>UEFI applications</strong>.</p>
<p>A UEFI application is a type of software designed to run in the pre-boot environment of a computer, managed entirely by the UEFI firmware. These applications are different from traditional programs that run once an operating system like Windows or Linux has loaded. Instead, UEFI applications operate with the services and resources provided by the firmware itself, bypassing the need for an OS.</p>
<p>They are typically stored on a dedicated partition called the EFI System Partition (ESP) and launched by the UEFI boot manager during the system's boot process. These apps can access low-level system functionality, including networking, file systems, and input/output devices. In our case, that meant we could build a standalone tool to t ransmit LLDP packets <em>before</em> the OS even initialized. This was the perfect solution, because it required no changes to the BIOS/UEFI firmware itself. I just needed to find someone with the embedded firmware expertise to bring it to life.</p>
<h3><strong>From Warsaw With Code</strong></h3>
<p>After some research, I found <a href="https://www.linkedin.com/in/krolpiotr/">Piotr Król</a>, a former BIOS software engineer at Intel who was doing freelance work out of Poland. He understood the problem immediately. We set up remote serial and IP-KVM access to our development hardware, and Piotr got to work.</p>
<p>There were some challenges along the way including lack of vendor support, incomplete firmware tooling, and remote hardware limitations. Our system didn't include <code>bcfg</code>, which meant we couldn't persistently change the boot order through standard UEFI tools. Piotr identified this early and suggested using <code>startup.nsh</code> as a workaround, a shell script that would automatically run our LLDP application when the EFI shell launched.</p>
<p>Four months later, Piotr delivered <strong>PoePwrNegotiator</strong>: a UEFI application written in C that transmits LLDP-MED (Link Layer Discovery Protocol – Media Endpoint Discovery) packets and requests the higher power levels we needed. No OS required. We deployed this UEFI application on all of our PoE devices in production and it worked flawlessly.</p>
<h3><strong>Sharing the Solution</strong></h3>
<p>This project began as an attempt to solve a very specific challenge we faced nearly a decade ago. I don’t know how many others have tackled this type of problem or taken this approach, but I wanted to share the work in case it helps someone else.</p>
<p>By open-sourcing <strong>PoePwrNegotiator</strong>, my goal is to preserve and document a unique solution to a problem that may still be relevant to those building PoE-powered x86 systems. If someone out there is working on a similar challenge, or even just wants to understand how UEFI applications can be used to control networking behavior at boot, I hope this gives them a useful head start.</p>
<p>PoePwrNegotiator is released under the <strong>MIT License</strong>, one of the most permissive open source licenses available. This means anyone can use, modify, or integrate this code into their own projects, commercial or personal, as long as the original license and copyright notice are included. The goal is to make this as accessible and useful as possible to anyone dealing with power negotiation challenges or looking to learn more about UEFI networking.</p>
<p><strong>GitHub Repo:</strong> <a href="https://github.com/orbitrod/PoePwrNegotiator">https://github.com/orbitrod/PoePwrNegotiator</a></p>
<h3><strong>Special Thanks</strong></h3>
<p><strong>Carlos</strong>, you were instrumental during the testing and the deployment of this application. You were my right hand throughout this project and far beyond it, and your dedication to me and to the work we were doing will never be forgotten. I cannot express enough how much your loyalty and commitment meant to me throughout that entire journey.</p>
<p><strong>Piotr</strong>, thank you for being brilliant, resourceful, and incredibly effective. Your deep expertise in firmware helped us solve a problem others wouldn’t touch. I’m grateful for your expertise and contribution to our project, your work solved the last piece of the puzzle.</p>
<hr>
<blockquote>
<p><em>This project reminded me that innovation often comes from working around limitations, not just within them. PoePwrNegotiator was a solution to a very specific challenge I faced in 2015, but the lessons and approach still feel relevant today. If it sparks ideas, helps someone overcome a similar obstacle, or contributes in any way to future PoE-powered system design, that’s all the reason I need to put it out there.</em></p>
<p><em>— Roderick</em></p>
</blockquote>
<hr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My LLM CLI tool can run tools now, from Python code or plugins (334 pts)]]></title>
            <link>https://simonwillison.net/2025/May/27/llm-tools/</link>
            <guid>44110584</guid>
            <pubDate>Tue, 27 May 2025 20:53:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/May/27/llm-tools/">https://simonwillison.net/2025/May/27/llm-tools/</a>, See on <a href="https://news.ycombinator.com/item?id=44110584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/May/27/llm-tools/">

<p>27th May 2025</p>



<p><strong><a href="https://llm.datasette.io/en/stable/changelog.html#v0-26">LLM 0.26</a></strong> is out with the biggest new feature since I started the project: <a href="https://llm.datasette.io/en/stable/tools.html"><strong>support for tools</strong></a>. You can now use the LLM <a href="https://llm.datasette.io/en/stable/usage.html">CLI tool</a>—and <a href="https://llm.datasette.io/en/stable/python-api.html">Python library</a>—to grant LLMs from OpenAI, Anthropic, Gemini and local models from Ollama with access to any tool that you can represent as a Python function.</p>
<p>LLM also now has <a href="https://llm.datasette.io/en/stable/plugins/directory.html#tools">tool plugins</a>, so you can install a plugin that adds new capabilities to whatever model you are currently using.</p>
<p>There’s a lot to cover here, but here are the highlights:</p>
<ul>
<li>
<strong>LLM can run tools now</strong>! You can <strong>install tools from plugins</strong> and load them by name with <code>--tool/-T name_of_tool</code>.</li>
<li>You can also <strong>pass in Python function code on the command-line</strong> with the <code>--functions</code> option.</li>
<li>The <strong>Python API supports tools too</strong>: <code>llm.get_model("gpt-4.1").chain("show me the locals", tools=[locals]).text()</code>
</li>
<li>Tools work in <strong>both async and sync contexts</strong>.</li>
</ul>
<p>Here’s what’s covered in this post:</p>
<ul>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#trying-it-out">Trying it out</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#more-interesting-tools-from-plugins">More interesting tools from plugins</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#ad-hoc-command-line-tools-with-functions">Ad-hoc command-line tools with --functions</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#tools-in-the-llm-python-api">Tools in the LLM Python API</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#why-did-this-take-me-so-long-">Why did this take me so long?</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#is-this-agents-then-">Is this agents then?</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#what-s-next-for-tools-in-llm-">What’s next for tools in LLM?</a></li>
</ul>


<h4 id="trying-it-out">Trying it out</h4>
<p>First, <a href="https://llm.datasette.io/en/stable/setup.html">install the latest LLM</a>. It may not be on Homebrew yet so I suggest using <code>pip</code> or <code>pipx</code> or <code>uv</code>:</p>

<p>If you have it already, <a href="https://llm.datasette.io/en/stable/setup.html#upgrading-to-the-latest-version">upgrade it</a>.</p>

<p>Tools work with other vendors, but let’s stick with OpenAI for the moment. Give LLM an OpenAI API key</p>
<div><pre>llm keys <span>set</span> openai
<span><span>#</span> Paste key here</span></pre></div>
<p>Now let’s run our first tool:</p>
<div><pre>llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td</pre></div>
<p>Here’s what I get:</p>
<p><img src="https://static.simonwillison.net/static/2025/llm-tools.gif" alt="Animated demo. I run that command, LLM shows Tool call: llm_version({}) in yellow, then 0.26a1 in green, then streams out the text The installed version is 0.26a1"></p>
<p><code>llm_version</code> is a very simple demo tool that ships with LLM. Running <code>--tool llm_version</code> exposes that tool to the model—you can specify that multiple times to enable multiple tools, and it has a shorter version of <code>-T</code> to save on typing.</p>
<p>The <code>--td</code> option stands for <code>--tools-debug</code>—it causes LLM to output information about tool calls and their responses so you can peek behind the scenes.</p>
<p>This is using the default LLM model, which is usually <code>gpt-4o-mini</code>. I switched it to <code>gpt-4.1-mini</code> (better but fractionally more expensive) by running:</p>
<div><pre>llm models default gpt-4.1-mini</pre></div>
<p>You can try other models using the <code>-m</code> option. Here’s how to run a similar demo of the <code>llm_time</code> built-in tool using <code>o4-mini</code>:</p>
<div><pre>llm --tool llm_time <span><span>"</span>What time is it?<span>"</span></span> --td -m o4-mini</pre></div>
<p>Outputs:</p>
<blockquote>
<p><code>Tool call: llm_time({})</code></p>
<div><pre>  {
    <span>"utc_time"</span>: <span><span>"</span>2025-05-27 19:15:55 UTC<span>"</span></span>,
    <span>"utc_time_iso"</span>: <span><span>"</span>2025-05-27T19:15:55.288632+00:00<span>"</span></span>,
    <span>"local_timezone"</span>: <span><span>"</span>PDT<span>"</span></span>,
    <span>"local_time"</span>: <span><span>"</span>2025-05-27 12:15:55<span>"</span></span>,
    <span>"timezone_offset"</span>: <span><span>"</span>UTC-7:00<span>"</span></span>,
    <span>"is_dst"</span>: <span>true</span>
  }</pre></div>
<p>The current time is 12:15 PM PDT (UTC−7:00) on May 27, 2025, which corresponds to 7:15 PM UTC.</p>
</blockquote>
<p>Models from (tool supporting) plugins work too. Anthropic’s Claude Sonnet 4:</p>
<div><pre>llm install llm-anthropic -U
llm keys <span>set</span> anthropic
<span><span>#</span> Paste Anthropic key here</span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m claude-4-sonnet</pre></div>
<p>Or Google’s Gemini 2.5 Flash:</p>
<div><pre>llm install llm-gemini -U
llm keys <span>set</span> gemini
<span><span>#</span> Paste Gemini key here</span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m gemini-2.5-flash-preview-05-20</pre></div>
<p>You can even run simple tools with Qwen3:4b, a <em>tiny</em> (2.6GB) model that I run using <a href="https://ollama.com/">Ollama</a>:</p>
<div><pre>ollama pull qwen3:4b
llm install <span><span>'</span>llm-ollama&gt;=0.11a0<span>'</span></span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m qwen3:4b</pre></div>
<p>Qwen 3 calls the tool, thinks about it a bit and then prints out a response:
<img src="https://static.simonwillison.net/static/2025/llm-tools-qwen.jpg" alt="Tool call: llm_version({}) 0.26a1<think> Okay, the user asked, &quot;What version?&quot; I need to respond with the version of the LLM. The tool provided is llm_version, which returns the installed version. I called that function and got the response 0.26a1. Now I should present this information clearly. Let me check if there's any additional context needed, but the user just asked for the version, so a straightforward answer should work. I'll state the version number and maybe mention that it's the installed version. Keep it simple and precise. </think> The installed version of the LLM is 0.26a1."></p>
<h4 id="more-interesting-tools-from-plugins">More interesting tools from plugins</h4>
<p>This demo has been pretty weak so far. Let’s do something a whole lot more interesting.</p>
<p>LLMs are notoriously bad at mathematics. This is deeply surprising to many people: supposedly the most sophisticated computer systems we’ve ever built can’t multiply two large numbers together?</p>
<p>We can fix that with tools.</p>
<p>The <a href="https://github.com/simonw/llm-tools-simpleeval">llm-tools-simpleeval</a> plugin exposes the <a href="https://github.com/danthedeckie/simpleeval">simpleeval</a> “Simple Safe Sandboxed Extensible Expression Evaluator for Python” library by Daniel Fairhead. This provides a robust-enough sandbox for executing simple Python expressions.</p>
<p>Here’s how to run a calculation:</p>
<div><pre>llm install llm-tools-simpleeval
llm -T simpleeval </pre></div>
<p>Trying that out:</p>
<div><pre>llm -T simple_eval <span><span>'</span>Calculate 1234 * 4346 / 32414 and square root it<span>'</span></span> --td</pre></div>
<p>I got back this—it tried <code>sqrt()</code> first, then when that didn’t work switched to <code>** 0.5</code> instead:</p>
<pre><code>Tool call: simple_eval({'expression': '1234 * 4346 / 32414'})
  165.45208860368976


Tool call: simple_eval({'expression': 'sqrt(1234 * 4346 / 32414)'})
  Error: Function 'sqrt' not defined, for expression 'sqrt(1234 * 4346 / 32414)'.


Tool call: simple_eval({'expression': '(1234 * 4346 / 32414) ** 0.5'})
  12.862818066181678

The result of (1234 * 4346 / 32414) is approximately
165.45, and the square root of this value is approximately 12.86.
</code></pre>
<p>I’ve released four tool plugins so far:</p>
<ul>
<li>
<strong><a href="https://github.com/simonw/llm-tools-simpleeval">llm-tools-simpleeval</a></strong>—as shown above, simple expression support for things like mathematics.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-quickjs">llm-tools-quickjs</a></strong>—provides access to a sandboxed QuickJS JavaScript interpreter, allowing LLMs to run JavaScript code. The environment persists between calls so the model can set variables and build functions and reuse them later on.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-sqlite">llm-tools-sqlite</a></strong>—read-only SQL query access to a local SQLite database.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-datasette">llm-tools-datasette</a></strong>—run SQL queries against a remote <a href="https://datasette.io/">Datasette</a> instance!</li>
</ul>
<p>Let’s try that Datasette one now:</p>
<div><pre>llm install llm-tools-datasette
llm -T <span><span>'</span>Datasette("https://datasette.io/content")<span>'</span></span> --td <span><span>"</span>What has the most stars?<span>"</span></span></pre></div>
<p>The syntax here is slightly different: the Datasette plugin is what I’m calling a “toolbox”—a plugin that has multiple tools inside it and can be configured with a constructor.</p>
<p>Specifying <code>--tool</code> as <code>Datasette("https://datasette.io/content")</code> provides the plugin with the URL to the Datasette instance it should use—in this case the <a href="https://datasette.io/content">content database</a> that powers the Datasette website.</p>
<p>Here’s the output, with the schema section truncated for brevity:</p>
<p><img src="https://static.simonwillison.net/static/2025/datasette-tool.jpg" alt="I run that command. It first does a Tool call to Datasette_query with SELECT name, stars, FROM repos ORDER BY stars DESC LIMIT 1. This returns an error message because there is no such column stars. It calls the Datasette_schema() function which returns a whole load of CREATE TABLE statements. Then it executes Datasette_query again this time with SELECT name, stargazers_count FROM repos ORDER BY stargazers_count DESC LIMIT 1. This returns name=datasette a count of 10020, so the model replies and says The repository with the most stars is &quot;datasette&quot; with 10,020 stars."></p>
<p>This question triggered three calls. The model started by guessing the query! It tried <code>SELECT name, stars FROM repos ORDER BY stars DESC LIMIT 1</code>, which failed because the <code>stars</code> column doesn’t exist.</p>
<p>The tool call returned an error, so the model had another go—this time calling the <code>Datasette_schema()</code> tool to get the schema of the database.</p>
<p>Based on that schema it assembled and then executed the correct query, and output its interpretation of the result:</p>
<blockquote>
<p>The repository with the most stars is “datasette” with 10,020 stars.</p>
</blockquote>
<p>Getting to this point was a real <a href="https://www.penny-arcade.com/comic/2010/09/17/mine-all-mine-part-one">Penny Arcade Minecraft moment</a> for me. The possibilities here are <em>limitless</em>. If you can write a Python function for it, you can trigger it from an LLM.</p>
<h4 id="ad-hoc-command-line-tools-with-functions">Ad-hoc command-line tools with <code>--functions</code>
</h4>
<p>I’m looking forward to people building more plugins, but there’s also much less structured and more ad-hoc way to use tools with the LLM CLI tool: the <code>--functions</code> option.</p>
<p>This was inspired by a similar feature <a href="https://sqlite-utils.datasette.io/en/stable/cli.html#defining-custom-sql-functions">I added to sqlite-utils</a> a while ago.</p>
<p>You can pass a block of literal Python code directly to the CLI tool using the <code>--functions</code> option, and any functions defined there will be made available to the model as tools.</p>
<p>Here’s an example that adds the ability to search my blog:</p>
<div><pre>llm --functions <span><span>'</span></span>
<span>import httpx</span>
<span>
<span>def search_blog(q):</span>
<span>    "Search Simon Willison blog"</span>
<span>    return httpx.get("https://simonwillison.net/search/", params={"q": q}).content</span>
<span><span>'</span></span> --td <span><span>'</span>Three features of sqlite-utils<span>'</span></span> -s <span><span>'</span>use Simon search<span>'</span></span></span></pre></div>
<p>This is <em>such a hack</em> of an implementation! I’m literally just hitting <a href="https://simonwillison.net/search/?q=pelicans">my search page</a> and dumping the HTML straight back into tho model.</p>
<p>It totally works though—it helps that the GPT-4.1 series all handle a million tokens now, so crufty HTML is no longer a problem for them.</p>
<p>(I had to add “use Simon search” as the system prompt because without it the model would try to answer the question itself, rather than using the search tool I provided. System prompts for tools are clearly a <em>big topic</em>, Anthropic’s own web search tool has <a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#search-instructions">6,471 tokens of instructions</a>!)</p>
<p>Here’s the output I got just now:</p>
<blockquote>
<p>Three features of sqlite-utils are:</p>
<ol>
<li>It is a combined CLI tool and Python library for manipulating SQLite databases.</li>
<li>It can automatically add columns to a database table if you attempt to insert data that doesn’t quite fit (using the alter=True option).</li>
<li>It supports plugins, allowing the extension of its functionality through third-party or custom plugins.</li>
</ol>
</blockquote>
<p>A better search tool would have more detailed instructions and would return relevant snippets of the results, not just the headline and first paragraph for each result. This is pretty great for just four lines of Python though!</p>
<h4 id="tools-in-the-llm-python-api">Tools in the LLM Python API</h4>
<p>LLM is both a CLI tool and a Python library at the same time (similar to my other project <a href="https://sqlite-utils.datasette.io/">sqlite-utils</a>). The LLM Python library <a href="https://llm.datasette.io/en/stable/python-api.html#tools">grew tool support</a> in LLM 0.26 as well.</p>
<p>Here’s a simple example solving one of the previously hardest problems in LLMs: counting the number of Rs in “strawberry”:</p>
<pre><span>import</span> <span>llm</span>

<span>def</span> <span>count_char_in_text</span>(<span>char</span>: <span>str</span>, <span>text</span>: <span>str</span>) <span>-&gt;</span> <span>int</span>:
    <span>"How many times does char appear in text?"</span>
    <span>return</span> <span>text</span>.<span>count</span>(<span>char</span>)

<span>model</span> <span>=</span> <span>llm</span>.<span>get_model</span>(<span>"gpt-4.1-mini"</span>)
<span>chain_response</span> <span>=</span> <span>model</span>.<span>chain</span>(
    <span>"Rs in strawberry?"</span>,
    <span>tools</span><span>=</span>[<span>count_char_in_text</span>],
    <span>after_call</span><span>=</span><span>print</span>
)
<span>for</span> <span>chunk</span> <span>in</span> <span>chain_response</span>:
    <span>print</span>(<span>chunk</span>, <span>end</span><span>=</span><span>""</span>, <span>flush</span><span>=</span><span>True</span>)</pre>
<p>The <code>after_call=print</code> argument is a way to peek at the tool calls, the Python equivalent of the <code>--td</code> option from earlier.</p>
<p>The <code>model.chain()</code> method is new: it’s similar to <code>model.prompt()</code> but knows how to spot returned tool call requests, execute them and then prompt the model again with the results. A <code>model.chain()</code> could potentially execute dozens of responses on the way to giving you a final answer.</p>
<p>You can iterate over the <code>chain_response</code> to output those tokens as they are returned by the model, even across multiple responses.</p>
<p>I got back this:</p>
<blockquote>
<p><code>Tool(name='count_char_in_text', description='How many times does char appear in text?', input_schema={'properties': {'char': {'type': 'string'}, 'text': {'type': 'string'}}, 'required': ['char', 'text'], 'type': 'object'}, implementation=&lt;function count_char_in_text at 0x109dd4f40&gt;, plugin=None) ToolCall(name='count_char_in_text', arguments={'char': 'r', 'text': 'strawberry'}, tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu') ToolResult(name='count_char_in_text', output='3', tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu', instance=None, exception=None)</code><br></p>
<p>There are 3 letter “r”s in the word “strawberry”.</p>
</blockquote>
<p>LLM’s Python library also supports <code>asyncio</code>, and tools can be <code>async def</code> functions <a href="https://llm.datasette.io/en/latest/python-api.html#tool-functions-can-be-sync-or-async">as described here</a>. If a model requests multiple async tools at once the library will run them concurrently with <code>asyncio.gather()</code>.</p>
<p>The Toolbox form of tools is supported too: you can pass <code>tools=[Datasette("https://datasette.io/content")]</code> to that <code>chain()</code> method to achieve the same effect as the <code>--tool 'Datasette(...)</code> option from earlier.</p>
<h4 id="why-did-this-take-me-so-long-">Why did this take me so long?</h4>
<p>I’ve been tracking <a href="https://simonwillison.net/tags/llm-tool-use/">llm-tool-use</a> for a while. I first saw the trick described in <a href="https://arxiv.org/abs/2210.03629">the ReAcT paper</a>, first published in October 2022 (a month before the initial release of ChatGPT). I built <a href="https://til.simonwillison.net/llms/python-react-pattern">a simple implementation of that</a> in a few dozen lines of Python. It was clearly a very neat pattern!</p>
<p>Over the past few years it has become <em>very</em> apparent that tool use is the single most effective way to extend the abilities of language models. It’s such a simple trick: you tell the model that there are tools it can use, and have it output special syntax (JSON or XML or <code>tool_name(arguments)</code>, it doesn’t matter which) requesting a tool action, then stop.</p>
<p>Your code parses that output, runs the requested tools and then starts a new prompt to the model with the results.</p>
<p>This works with almost <strong>every model</strong> now. Most of them are specifically trained for tool usage, and there are leaderboards like the <a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard</a> dedicated to tracking which models do the best job of it.</p>
<p>All of the big model vendors—OpenAI, Anthropic, Google, Mistral, Meta—have a version of this baked into their API, either called tool usage or function calling. It’s all the same underlying pattern.</p>
<p>The models you can run locally are getting good at this too. Ollama <a href="https://ollama.com/blog/tool-support">added tool support</a> last year, and it’s baked into the <a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md">llama.cpp</a> server as well.</p>
<p>It’s been clear for a while that LLM absolutely needed to grow support for tools. I released <a href="https://simonwillison.net/2025/Feb/28/llm-schemas/">LLM schema support</a> back in February as a stepping stone towards this. I’m glad to finally have it over the line.</p>
<p>As always with LLM, the challenge was designing an abstraction layer that could work across as many different models as possible. A year ago I didn’t feel that model tool support was mature enough to figure this out. Today there’s a very definite consensus among vendors about how this should work, which finally gave me the confidence to implement it.</p>
<p>I also presented a workshop at PyCon US two weeks ago about <a href="https://simonwillison.net/2025/May/15/building-on-llms/">Building software on top of Large Language Models</a>, which was exactly the incentive I needed to finally get this working in an alpha! Here’s the <a href="https://building-with-llms-pycon-2025.readthedocs.io/en/latest/tools.html">tools section</a> from that tutorial.</p>
<h4 id="is-this-agents-then-">Is this agents then?</h4>
<p><em>Sigh</em>.</p>
<p>I still <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#-agents-still-haven-t-really-happened-yet">don’t like</a> using the term “agents”. I worry that developers will think <a href="https://simonwillison.net/2025/May/22/tools-in-a-loop/">tools in a loop</a>, regular people will think virtual AI assistants <a href="https://en.m.wikipedia.org/wiki/Her_(2013_film)">voiced by Scarlett Johansson</a> and academics will <a href="https://simonwillison.net/2025/Mar/19/worms-and-dogs-and-countries/">grumble about thermostats</a>. But in the LLM world we appear to be converging on “tools in a loop”, and that’s absolutely what this.</p>
<p>So yes, if you want to build “agents” then LLM 0.26 is a great way to do that.</p>
<h4 id="what-s-next-for-tools-in-llm-">What’s next for tools in LLM?</h4>
<p>I already have a <a href="https://github.com/simonw/llm/milestone/13">LLM tools v2 milestone</a> with 13 issues in it, mainly around improvements to how tool execution logs are displayed but with quite a few minor issues I decided shouldn’t block this release. There’s a bunch more stuff in the <a href="https://github.com/simonw/llm/issues?q=is%3Aissue%20state%3Aopen%20label%3Atools">tools label</a>.</p>
<p>I’m most excited about the potential for plugins.</p>
<p>Writing tool plugins is <em>really fun</em>. I have an <a href="https://github.com/simonw/llm-plugin-tools">llm-plugin-tools</a> cookiecutter template that I’ve been using for my own, and I plan to put together a tutorial around that soon.</p>
<p>There’s more work to be done adding tool support to more model plugins. I added <a href="https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#supporting-tools">details of this</a> to the advanced plugins documentation. This commit <a href="https://github.com/simonw/llm-gemini/commit/a7f1096cfbb733018eb41c29028a8cc6160be298">adding tool support for Gemini</a> is a useful illustratino of what’s involved.</p>

<p>And yes, <strong>Model Context Protocol</strong> support is clearly on the agenda as well. MCP is emerging as the standard way for models to access tools at a frankly bewildering speed. Two weeks ago it wasn’t directly supported by the APIs of any of the major vendors. In just the past eight days <a href="https://simonwillison.net/2025/May/27/mistral-agents-api/">it’s been added</a> by OpenAI, Anthropic <em>and</em> Mistral! It’s feeling like a lot less of a moving target today.</p>
<p>I want LLM to be able to act as an MCP client, so that any of the MCP servers people are writing can be easily accessed as additional sources of tools for LLM.</p>
<p>If you’re interested in talking more about what comes next for LLM, <a href="https://datasette.io/discord-llm">come and chat to us in our Discord</a>.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why the Original Macintosh Had a Screen Resolution of 512×324 (153 pts)]]></title>
            <link>https://512pixels.net/2025/05/original-macintosh-resolution/</link>
            <guid>44110219</guid>
            <pubDate>Tue, 27 May 2025 20:02:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://512pixels.net/2025/05/original-macintosh-resolution/">https://512pixels.net/2025/05/original-macintosh-resolution/</a>, See on <a href="https://news.ycombinator.com/item?id=44110219">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-32349">
	<!-- .entry-header -->

	<div>
		<p>Many classic Macs came with — or supported — displays running at 512×384 pixels, but many compact Macs, ranging from <a href="https://support.apple.com/en-us/112190">the original 1984 machine</a> up through 1991’s <a href="https://support.apple.com/en-us/112201">Macintosh Classic II</a> had built-in CRTs running at 512×342 pixels. That covers all black-and-white compact Macs with a 9-inch screen. The later Color Classic and Color Classic II used a 10-inch CRT at a full 512×384.</p>
<p>This came up when <a href="https://512pixels.net/2025/05/oh-hey-it-me/">I joined John Gruber on The Talk Show</a>. At one point in the show, I rattled off the original Mac’s resolution as being 512×384.</p>
<p>Except… it wasn’t. The original Mac screen ran at 512×342. I remembered the right number and corrected myself a moment later, but given the name of this website, it was pretty embarrassing. This set me off on a journey to understand why Apple made this decision. Why were the displays on early Macs 42 pixels shorter in height than later ones?</p>
<p>After doing <em>a lot</em> of reading, there are several factors to consider when trying to answer this question.</p>
<p><img decoding="async" src="https://512pixels.net/wp-content/uploads/2025/05/apple1984mac-full.png" alt="Original Macintosh"></p>
<h2>Memory</h2>
<p>The original Mac had a mere 128 <em>kilobytes</em> of memory. The photo of the original Macintosh in this blog post is 604 KB in size, some 4.7x larger than the entire memory footprint of the 1984 machine. Of course, Apple would improve this with later models, but many design decisions made to accommodate the original Mac would forward for years.</p>
<p>Over at Folklore.org, <a href="https://www.folklore.org/Five_Different_Macs.html">Andy Hertzfeld wrote a great post</a> walking through several early versions of what would become the Macintosh, including ones with even <em>less</em> memory:</p>
<blockquote><p>
  In the beginning of 1982, the original 68000 design was more than a year old, and the software was nowhere near finished, so Burrell [Smith] was afraid some of the trade-offs of the original design were no longer current. He used the expansive canvas of a custom chip, where additional logic was almost free, to update the architecture.</p>
<p>  The most important decision was admitting that the software would never fit into 64K of memory and going with a full 16-bit memory bus, requiring 16 RAM chips instead of 8. The extra memory bandwidth allowed him to double the display resolution, going to dimensions of 512 by 342 instead of 384 by 256. He also added bells and whistles like a fancy, DMA-fed sound generator with four independent voices. This was the fourth version of the Macintosh design.
</p></blockquote>
<p>Shipping a computer in the 1980s with a resolution of 384×256 wouldn’t have been too wild. 1982’s Commodore 64 ran at a maximum resolution 320×200. The Apple IIe that shipped in 1983 offered several display modes:</p>
<ul>
<li>40 or 80 columns text, white-on-black, with 24 lines</li>
<li>Low-Resolution: 40×48 (16 colors)</li>
<li>High-Resolution: 280×192 (6 colors)</li>
<li>Double-Low-Resolution: 80×48 (16 colors)</li>
<li>Double-High-Resolution: 560×192 (16 colors)</li>
</ul>
<p>Computers like the C64 and Apple II had to pull off a lot of tricks to pull off getting graphics on the screen. The Macintosh was going to be powered by a full GUI, and 384×256 would have been just too chunky, so thinking about 128 kilobytes of RAM as an <em>upgrade</em> is a fun twist on the normal take of “Wow, the original Mac was so held back!” Really, it’s amazing that it could do what it did.</p>
<p>That feeling takes on new life when you realize the Mac used a portion of its memory to drive the display. At 512×342, the memory needed to draw the screen was a total of 21.8 KB. Had Apple opted for a 4:3 display running at 512×384, the system would have needed 24 KB for the display. Every byte was precious back in the day. Again, <a href="https://folklore.org/Monkey_Lives.html">we turn to Andy Hertzfeld</a>:</p>
<blockquote><p>
  The original Macintosh only had 128K bytes of RAM (that’s one eighth of a megabyte), so dealing with memory management was usually the hardest part of writing both the system and applications. We allocated around 16K bytes for system use, and another 22K bytes for the 512 by 342 black and white screen, so applications were left with only 90K bytes or so. The bigger ones like MacWrite or MacPaint seemed to be bursting at the seams.
</p></blockquote>
<p>It seems that two things are true:</p>
<ul>
<li>The original Macintosh shipped with more memory than earlier designs</li>
<li>Even at 128K, things were extremely tight</li>
</ul>
<p>Daniel Knight pointed this out when writing about the original Mac:</p>
<blockquote><p>
  As&nbsp;<a href="http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Scrooge_McDuck.txt&amp;topic=Origins&amp;sortOrder=Sort%20by%20Date">Andy Hertzfeld writes</a>, the Mac was only going to have a 256×256 pixel display (a step up from the 280×192 graphics of the Apple II). It wasn’t until&nbsp;<a href="http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Good_Earth.txt&amp;topic=Origins&amp;sortOrder=Sort%20by%20Date">January 1981</a>&nbsp;that the Mac team decided to give the Motorola 68000 a try. A good thing, too, as the first Mac shipped with a 512×342 pixel display, and that would have consumed over 30% of the 64 KB of memory originally envisioned for the low-cost information appliance.
</p></blockquote>
<h2>CPU Timing</h2>
<p>At the heart of the Macintosh was a Motorola 68000 CPU running at 8 MHz. Just like the 128 kilobytes of RAM, this came with some inherit limitations when paired with the display hardware.</p>
<p>To minimize CRT flicker, Apple worked to achieve a vertical refresh rate of 60 Hz. This meant the CPU spent one-third of its time drawing the display. Just as with the memory, a taller screen would have taken more resources away from running the Mac’s operating system and programs.</p>
<p>If you are familiar with standard TV formats, you probably have already picked up on the fact that this refresh rate/screen size combination meant the Mac was incompatible with NTSC composite video, which the Apple II supported. (It’s also different than PAL systems.) This let Apple balance performance and picture quality in a way the team saw fit, given the hardware that they had, <a href="https://www.folklore.org/Joining_Apple_Computer.html">as Bill Atkinson wrote:</a></p>
<blockquote><p>
  The Apple II displayed white text on a black background. I argued that to do graphics properly we had to switch to a white background like paper. It works fine to invert text when printing, but it would not work for a photo to be printed in negative. The Lisa hardware team complained the screen would flicker too much, and they would need faster refresh with more expensive RAM to prevent smearing when scrolling. Steve listened to all the pros and cons then sided with a white background for the sake of graphics.
</p></blockquote>
<p>Here’s the thing: the original Mac <em>did not</em> run at 8 MHz, but rather 7.83 MHz. This slight tuning meant Apple could time the CPU’s cycles and the CRT’s need for updating more easily.</p>
<h2>Square Pixels</h2>
<p>Running the 9-inch CRT at 512×342 gave the original Mac a pixels density of 72 PPI, but more importantly, the screen size allowed the Mac to have square pixels.</p>
<p>Apple’s first GUI-powered machine, <a href="https://www.macstories.net/mac/the-lisa/">the Lisa</a>, famously had rectangular pixels, <a href="https://folklore.org/Square_Dots.html">as Andy Hertzfeld covered here</a>:</p>
<blockquote><p>
  The Lisa team decided to optimize their display for horizontal resolution, in order to be able to display 80 columns of text in an attractive font. The vertical resolution wasn’t as important, because vertical scrolling works much better for text than horizontal scrolling. The designers decided to endow Lisa with twice as much horizontal resolution as vertical, using a 720 by 360 pixel display, with pixels that were twice as high as they were wide. This was great for text oriented applications like the word processor, but it made things somewhat awkward for the more graphical applications.</p>
<p>  When Burrell redesigned the Macintosh in December 1980 to use the same microprocessor as Lisa, the Motorola 68000, it set off shock waves within Apple. Not only was Burrell’s new design much simpler than Lisa, with less than half the chip count, but it also ran almost twice as fast, using an 8 megahertz clock instead of a 5 megahertz clock. Among other advantages was the fact that the Mac’s 384 by 256 pixel display had the identical horizontal and vertical resolution, a feature that we called “square dots”. Square dots made it easier to write graphical applications, since you didn’t have to worry about the resolution disparity.
</p></blockquote>
<p>Hertzfeld goes on to share that the Mac team tried to get the Lisa team to switch to square pixels, bumping the machine’s resolution to a mind-blowing-for-the-time 768×512 pixels, but it wasn’t in the cards, as the Lisa was well on its way to shipping by the time this meeting took place.</p>
<p><img decoding="async" src="https://512pixels.net/wp-content/uploads/2025/05/apple-lisa.jpg" alt="Apple Lisa"></p>
<p>Of course, the Lisa would end up being a doomed product, and in 1985, Apple rebadged a later revision of the machine as the “Macintosh XL.” It shipped with <a href="https://en.wikipedia.org/wiki/MacWorks_XL">a software shim called “MacWorks XL”</a> that allowed Mac software to run on the Lisa, but the rectangular pixels made the software appear stretched. To solve this, Apple sold a product named the  Macintosh XL Screen Kit, which changed the resolution to 608×432 pixels. This is how the product is described in <a href="https://512pixels.net/wp-content/uploads/2025/05/Lisa-DIY-Guide.pdf">a document outlining DIY upgrades</a> from <a href="https://en.wikipedia.org/wiki/Sun_Remarketing">Sun Remarketing</a>, a company that was focused on keeping Lisa hardware up and running.</p>
<blockquote><p>
  No recently restored Lisa/Mac XL is complete without a Macintosh XL Screen Kit. Unlike the standard 9-inch Macintosh which has square pixels, the stock Lisa/XL has rectangular pixels. With rectangular pixels, circles look like footballs, squares look like spaghetti boxes. The purpose of the Macintosh XL Screen Kit is to square up the pixels. Proportions become exactly the same as on other Macs (1 to 1 ), but the overall display area (608 pixels x 432 pixels) is made roughly the same as a 12-inch Macintosh 11 WYSIWYG monitor (640×480). Standard 9-inch Macs only display 512×342 pixels.</p>
<p>  The complete screen modification kit includes new 3A boot ROMs, a new video ROM and a new yoke coil. (Newer software requires System Update 5.0 and MacWorks Plus as well.) Conscientious installation of the complete screen kit requires one to two hours.
</p></blockquote>
<h2>Mimicking the Real World</h2>
<p>In addition to their square pixels making on-screen graphics look better, the Macintosh team also wanted the computer to be useful for those who needed to print their work. The 72 DPI screen was more than sharp enough for work in applications like MacWrite, MacPaint, and the page layout tools that would follow them. Users could see their work full-sized or at a reduced scale to get a sense of the overall page they were working on. Larger displays would come, but for 1984, 512×342 was plenty.</p>
<h2>Everything in Balance</h2>
<p>In short, there’s no easy answer to explain why early compact Macs ran at a screen resolution of 512×342. Rather, Apple was doing what it does best: designing a product with the right trade-offs for performance, ease of use, and cost. In a few short years, the Mac would grow to support larger displays, but for 1984, the balance was set correctly.</p>
<p>In the very first edition of <em>Macworld</em> magazine, <a href="https://archive.org/details/MacWorld_8404_April_1984_premier">Matthew Douglas wrote</a>:</p>
<blockquote><p>
  Appearances can be deceiving. Most computers display text on one of 24 or 25 “invisible” horizontal lines on the screen. This display is called text mode. To display graphics, the software switches to graphics mode, and the display becomes a field of dots. Each dot, or pixel, is either off (invisible) or on (visible). Of course, a computer may have more than one text mode or two or more graphics modes, or it may be a mixed mode of graphics and text.</p>
<p>  The Mac display has only one mode: graphics. The entire screen is made up of dots: 512 dots horizontally and 342 dots vertically, a total of175,104 dots that combine to display everything you’ll ever see on a Mac screen. (Now you know the secret behind the incredible range of type fonts, attributes, and type sizes.)
</p></blockquote>
<p>In the same edition, David Bunnell interviewed Bill Gates, and he was asked about what made the Mac special. Gates — who had previously said the Mac was a computer he would want his mom to try — replied:</p>
<blockquote><p>
  The Mac was designed as a graphics machine. Apple didn’t put in a ROM character generator or a bunch of video modes. They put in only one video mode, and that’s the pure bit-mapped, 512-by 342-pixel screen. The monitor was designed into the machine so that they could get extremely crisp pictures and have one integrated system. They knew what the aspect ratio was and how the dots would appear. And they also made sure that the mouse would be used and that the 64K ROM would support very rich graphics interaction.</p>
<p>  You can configure a PC with one of the better graphics boards and add a Microsoft mouse and the necessary software, but that’s not the thrust of the machine. The PC is used primarily in its text mode, and to date it’s used mostly without a mouse; you couldn’t get performance or graphics like the Mac’s out of the PC at a comparable price. Although they’re both “turing” machines (that is, they have finite memory), the thrust of the Mac is quite different.</p>
<p>  Of all the personal computers available today, the Mac is unique. It’s the first time somebody said, “We don’t need a lot of the things that other personal computers have, so let’s optimize a few areas and make sure the software is designed around them.”
</p></blockquote>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running GPT-2 in WebGL: Rediscovering the Lost Art of GPU Shader Programming (125 pts)]]></title>
            <link>https://nathan.rs/posts/gpu-shader-programming/</link>
            <guid>44109257</guid>
            <pubDate>Tue, 27 May 2025 18:02:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nathan.rs/posts/gpu-shader-programming/">https://nathan.rs/posts/gpu-shader-programming/</a>, See on <a href="https://news.ycombinator.com/item?id=44109257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="content"><article><p><i>May, 24 2025 •
16 min read •
2461 words</i></p><p>Preface: A few weeks back, I implemented GPT-2 using WebGL and shaders (<a href="https://github.com/nathan-barry/gpt2-webgl/tree/main">Github Repo</a>) which made the front page of Hacker News (<a href="https://news.ycombinator.com/item?id=43870998">discussion</a>). By popular demand, here is a short write-up over the main ideas behind GPU shader programming (for general-purpose computing).</p><div><h2>Table Of Contents</h2><hr><div><nav id="TableOfContents"><ol><li><a href="#the-origins-of-general-purpose-gpu-programming">The Origins of General Purpose GPU Programming</a></li><li><a href="#graphics-api-vs-compute-api">Graphics API vs Compute API</a></li><li><a href="#implementing-gpt-2-with-shaders">Implementing GPT-2 with shaders</a><ol><li><a href="#textures--framebuffers-the-data-bus">Textures &amp; Framebuffers: The Data Bus</a></li><li><a href="#fragment-shaders-as-compute-kernels">Fragment Shaders as Compute Kernels</a></li><li><a href="#chaining-passes">Chaining Passes</a></li><li><a href="#limitations">Limitations</a></li></ol></li></ol></nav></div></div><h2 id="the-origins-of-general-purpose-gpu-programming">The Origins of General Purpose GPU Programming</h2><hr><p>In the early 2000s, NVIDIA introduced programmable shaders with the GeForce 3 (2001) and GeForce FX (2003). Instead of being limited to predetermined transformations and effects of earlier GPUs, developers were now given unprecedented control over the rendering pipeline, enabling much more sophisticated visual effects. These programmable shaders laid the foundation for modern GPU computing.</p><p>Researchers soon discovered that certain computations (like linear algebra involving matrices and vectors) could be accelerated by casting them as graphics operations on the GPU.
However, using shader languages like OpenGL’s GLSL for no-graphics tasks was cumbersome. By the mid-2000s, the need for a more straightforward, non-graphics interface to GPUs had become clear, and NVIDIA saw a new opportunity.</p><p>Inspired by the demand for <strong>general-purpose GPU (GPGPU)</strong> programming, in November 2006, NVIDIA released <strong>CUDA</strong>, the <strong>Compute Unified Device Architecture</strong>. CUDA is a parallel computing platform and programming model that gives developers direct access to the GPU’s computational power without the intermediary of a graphics API. With CUDA, one could write C/C++ code to execute on the CPU using straightforward extensions for parallel kernels and managing GPU memory explicitly. This meant that developers could now ignore graphics-specific concepts and dramatically lowered the barrier for general-purpose GPU computing. Following CUDA came OpenCL which expanded general purpose computing beyond the NVIDIA ecosystem.</p><h2 id="graphics-api-vs-compute-api">Graphics API vs Compute API</h2><hr><p>Traditional graphics APIs like OpenGL are centered around a fixed-function pipeline tailored for rendering images. The pipeline consists of stages like vertex processing, rasterization, fragment processing, etc. Each stage can be programmable with shaders, but the overall flow is fixed.
Using OpenGL for computation required a lot of boilerplate. One had to pack data into texture formats, use off-screen framebuffers to capture the results, and often perform multiple render passes to accomplish multi-stage algorithms.</p><p>In contrast, OpenCL and CUDA expose a direct compute model which lets you treat the GPU as one giant SIMD processor:</p><ul><li><strong>Kernels, not shaders</strong>: You write a function and then launch thousands of copies to run in parallel (no notion of vertices or fragments).</li><li><strong>Raw buffers</strong>: Allocate arrays of floats or integers, read/write them directly, and move them back and forth between host and device with explicit copy calls.</li><li><strong>User-driven pipeline</strong>: You define exactly what happens and when instead of using a predefined fixed sequence of rendering stages.</li></ul><p>The result is a far more natural fit for linear algebra, simulations, physics, ML, and any algorithm where you just want to compute independent calculations in bulk.</p><p>In OpenGL, the output of your computation would ultimately be pixels in a framebuffer or values in a texture; in OpenCL, the output can be data in any form (float arrays, computed lists of numbers, etc.) which you then transfer back to the CPU or use in further computations. This makes OpenCL more suitable for general algorithms where you just want the numerical results.</p><h2 id="implementing-gpt-2-with-shaders">Implementing GPT-2 with shaders</h2><hr><p>Below covers textures, framebuffers, vertex and fragment shaders, and other graphics specific concepts I hijacked to get GPT-2 running on a GPU using shaders.</p><h3 id="textures--framebuffers-the-data-bus">Textures &amp; Framebuffers: The Data Bus</h3><p>In traditional graphics rendering, a <strong>texture</strong> is simply a 2D (or 3D) array of pixel data stored in GPU memory. When you map a texture onto a triangle, the fragment shader “samples” it to look up color values. In our compute‐as‐graphics paradigm, we hijack this same mechanism to store and manipulate numerical data instead of colors:</p><ul><li><p><strong>Textures as tensors</strong>:
Each texture is an array of floating‐point values (we use single‐channel R32F formats), where each pixel encodes one element of a matrix or vector. Just like you’d think of an H×W texture as holding H×W RGB pixels for an image, here it holds H×W scalar values for a weight matrix or activation map.</p></li><li><p><strong>Sampling without filtering</strong>:
We use functions like <code>texelFetch</code> to read texture data by exact integer coordinates, bypassing any interpolation. This gives us deterministic, “random access” reads into our weight and activation arrays, akin to indexing into a CPU array by row and column.</p></li></ul><p>A <strong>Framebuffer Object (FBO)</strong> is a lightweight container that lets us redirect rendering output from the screen into one of our textures:</p><ol><li><p><strong>Attach a texture as the render target</strong>:
By binding a texture to an FBO, any draw call you make, normally destined for your monitor, writes into that texture instead. The fragment shader’s <code>out</code> variable becomes a write port into GPU memory.</p></li><li><p><strong>Offscreen and ping-pong rendering</strong>:
Because we can attach different textures in succession, we “ping-pong” between them: one pass writes into <strong>Texture A</strong>, the next pass reads from <strong>Texture A</strong> while writing into <strong>Texture B</strong>, and so on. This avoids ever copying data back to the CPU until the very end.</p></li><li><p><strong>High‐throughput data bus</strong>:
All of this happens entirely on the GPU’s VRAM bus. Binding textures and framebuffers is just pointer swapping on the GPU. Once set up, your fragment shader passes stream through millions of cores in parallel, reading, computing, and writing without ever touching system memory.</p></li></ol><p>Together, textures and FBOs form the <strong>data bus</strong> of our shader‐based compute engine: textures hold the raw bits of your neural network (weights, intermediate activations, and outputs), and framebuffers let you chain shader passes seamlessly, keeping everything on the high-speed GPU pipeline until you explicitly pull the final logits back to the CPU.</p><h3 id="fragment-shaders-as-compute-kernels">Fragment Shaders as Compute Kernels</h3><p>Fragment shaders are where the magic happens. Instead of using fragment shaders to shade pixels for display, we hijack them as compute kernels; each fragment invocation becomes one “thread” that calculates a single output value. The GPU will launch thousands of these in parallel, giving us massive throughput for neural-network operations.</p><p>Below is an example fragment shader for matrix multiplication:</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// Matrix Multiply (C = A × B)</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>precision</span> <span>highp</span> <span>float</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_A<span>;</span>    <span>// Texture holding matrix A (M x K)</span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_B<span>;</span>    <span>// Texture holding matrix B (K x N)</span>
</span></span><span><span><span>uniform</span> <span>int</span>        u_K<span>;</span>   <span>// Shared inner dimension</span>
</span></span><span><span><span>out</span> <span>vec4</span>           outColor<span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  <span>// Determine the output coordinate (i, j) from the fragment’s pixel position</span>
</span></span><span><span>  <span>ivec2</span> coord <span>=</span> <span>ivec2</span><span>(</span>gl_FragCoord<span>.</span>xy<span>);</span>
</span></span><span><span>  <span>float</span> sum <span>=</span> <span>0.0</span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>// Perform the dot–product along the K dimension</span>
</span></span><span><span>  <span>for</span> <span>(</span><span>int</span> k <span>=</span> <span>0</span><span>;</span> k <span>&lt;</span> u_K<span>;</span> <span>++</span>k<span>)</span> <span>{</span>
</span></span><span><span>    <span>float</span> a <span>=</span> texelFetch<span>(</span>u_A<span>,</span> <span>ivec2</span><span>(</span>k<span>,</span> coord<span>.</span>y<span>),</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>    <span>float</span> b <span>=</span> texelFetch<span>(</span>u_B<span>,</span> <span>ivec2</span><span>(</span>coord<span>.</span>x<span>,</span> k<span>),</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>    sum <span>+=</span> a <span>*</span> b<span>;</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span>
</span></span><span><span>  <span>// Write the result into the single‐channel R component of the output texture</span>
</span></span><span><span>  outColor <span>=</span> <span>vec4</span><span>(</span>sum<span>,</span> <span>0.0</span><span>,</span> <span>0.0</span><span>,</span> <span>1.0</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Here we have:</p><ul><li><strong>Per-pixel work item</strong>: Each fragment corresponds to one matrix element (i, j). The GPU runs this loop for every (i, j) in parallel across its shader cores.</li><li><strong>Exact indexing</strong>: texelFetch reads a single float by its integer coordinate.</li><li><strong>Write-back</strong>: Assigning to outColor.r writes that computed value directly into the bound FBO’s texture at (i, j).</li></ul><p>Here is another fragment shader but for the GELU activation function:</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// GELU Activation</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>precision</span> <span>highp</span> <span>float</span><span>;</span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_X<span>;</span>
</span></span><span><span><span>out</span> <span>vec4</span> o<span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  <span>ivec2</span> c <span>=</span> <span>ivec2</span><span>(</span>gl_FragCoord<span>.</span>xy<span>);</span>
</span></span><span><span>  <span>float</span> x <span>=</span> texelFetch<span>(</span>u_X<span>,</span> c<span>,</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>  <span>float</span> t <span>=</span> <span>0.5</span> <span>*</span> <span>(</span><span>1.0</span> <span>+</span> tanh<span>(</span><span>0.79788456</span> <span>*</span> <span>(</span>x <span>+</span> <span>0.044715</span> <span>*</span> x<span>*</span>x<span>*</span>x<span>)));</span>
</span></span><span><span>  o <span>=</span> <span>vec4</span><span>(</span>x <span>*</span> t<span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h4 id="shared-vertex-shader">Shared Vertex Shader</h4><p>Every operation gets its own fragment shader since that’s where the math for the operation happens. The vertex shader, on the other hand, is simple and the same for each. All it does is draw two triangles which covers the entire view port.</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// Shared Vertex Shader</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>in</span> <span>vec2</span> a_position<span>;</span>
</span></span><span><span><span>out</span> <span>vec2</span> v_uv<span>;</span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  v_uv <span>=</span> a_position <span>*</span> <span>0.5</span> <span>+</span> <span>0.5</span><span>;</span>  <span>// map [-1,+1] to [0,1]</span>
</span></span><span><span>  gl_Position <span>=</span> <span>vec4</span><span>(</span>a_position<span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><ul><li><strong>Full-screen quad</strong>: Two triangles cover the viewport. Every pixel in the fragment stage maps to one tensor element.</li><li><strong>Reusable</strong>: Because the vertex work is identical for all operations, we compile it once and reuse it across every matrix multiply, activation, and bias-add pass.</li></ul><p>With this structure in mind, every “shader pass” is really just:</p><ol><li><strong>Vertex shader</strong>: map two triangles to the viewport</li><li><strong>Fragment shader</strong>: perform one tiny piece of your transformer math per pixel</li></ol><h3 id="chaining-passes">Chaining Passes</h3><p>Under the hood, every neural‐network operation, whether it’s a matrix multiply, an activation function, or a bias addition, boils down to four simple GPU steps:</p><ol><li>Bind inputs as textures (weights, activations, or intermediate results).</li><li>Attach a fresh output texture to an offscreen framebuffer (FBO).</li><li>Draw a full‐screen quad using the shared vertex shader.</li><li>Execute the fragment shader, which performs the actual computation per pixel.</li></ol><p>All of the WebGL boilerplate lives in our <code>_runPass</code> helper, so each pass in the GPT-2 forward loop feels like a single, declarative call:</p><div><pre tabindex="0"><code data-lang="typescript"><span><span><span>private</span> <span>_runPass</span><span>(</span>
</span></span><span><span>  <span>name</span>: <span>string</span><span>,</span>
</span></span><span><span>  <span>inputs</span><span>:</span> <span>{</span> <span>[</span><span>u</span>: <span>string</span><span>]</span><span>:</span> <span>WebGLTexture</span> <span>},</span>
</span></span><span><span>  <span>ints</span><span>:</span> <span>{</span> <span>[</span><span>u</span>: <span>string</span><span>]</span><span>:</span> <span>number</span> <span>},</span>
</span></span><span><span>  <span>outTex</span>: <span>WebGLTexture</span><span>,</span>
</span></span><span><span>  <span>W</span>: <span>number</span><span>,</span>
</span></span><span><span>  <span>H</span>: <span>number</span>
</span></span><span><span><span>)</span> <span>{</span>
</span></span><span><span>  <span>// Grab the WebGL2 context and compiled shader program for this pass
</span></span></span><span><span><span></span>  <span>const</span> <span>gl</span> <span>=</span> <span>this</span><span>.</span><span>gl</span><span>;</span>
</span></span><span><span>  <span>const</span> <span>prog</span> <span>=</span> <span>this</span><span>.</span><span>programs</span><span>[</span><span>name</span><span>];</span> <span>// This has our vertex and frag shaders
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>useProgram</span><span>(</span><span>prog</span><span>);</span>
</span></span><span><span>
</span></span><span><span>  <span>// BOILERPLATE: Bind all input textures as uniforms
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// BOILERPLATE: Bind an FBO and attach our empty texture to it.
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// BOILERPLATE: Set up the full-screen quad geometry
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Draw into our texture
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>viewport</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>W</span><span>,</span> <span>H</span><span>);</span>            <span>// Ensure viewport matches texture size
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>drawArrays</span><span>(</span><span>gl</span><span>.</span><span>TRIANGLES</span><span>,</span> <span>0</span><span>,</span> <span>6</span><span>);</span>  <span>// Runs our shaders
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Clean up: Unbind FBO
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>bindFramebuffer</span><span>(</span><span>gl</span><span>.</span><span>FRAMEBUFFER</span><span>,</span> <span>null</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h4 id="forward-pass-layer-by-layer">Forward Pass: Layer by Layer</h4><p>Because each pass leaves its results in VRAM, we never pay the cost of round-trips back to the CPU until the very end. Below is a high-level description of the entire forward pass:</p><ol><li><strong>Upload Embeddings</strong>: Compute the token+position embeddings on the CPU and send them to the GPU as one texture.</li><li><strong>Transformer Layers (12 in total)</strong>:<ul><li><em>Normalize &amp; Project</em>: Apply layer normalization, then run the attention and feed-forward sublayers entirely on the GPU.</li><li><em>Attention</em>: Compute queries, keys, values; calculate attention weights; combine values.</li><li><em>Feed-Forward</em>: Two matrix multiplies with a GELU activation in between.</li><li><em>Residuals</em>: Add the layer’s input back in at each substep.</li></ul></li><li><strong>Final Normalization &amp; Output</strong>: Do one last layer normalization, multiply by the output weight matrix, then read the resulting logits back to the CPU.</li></ol><p>Once logits are back on the CPU, we apply softmax and sample (top-k or top-p) to pick the next token. Then the process starts over again with the new token being appended to the context.</p><p>By chaining these operation passes together, we keep the entire GPT-2 pipeline on the GPU until the final logits. This is how programmable shaders let us treat the graphics pipeline as a general-purpose parallel engine.</p><h3 id="limitations">Limitations</h3><p>While hijacking WebGL allows us to run machine learning models on the GPU, it carries several key limitations:</p><ul><li><strong>No shared/local memory</strong>: Fragment shaders can only read/write global textures. There’s no on-chip scratchpad for blocking or data reuse, so you’re limited to element-wise passes.</li><li><strong>Texture size limits</strong>: GPUs enforce a maximum 2D texture dimension (e.g. 16 K×16 K). Anything larger must be manually split into tiles, adding bookkeeping and extra draw calls.</li><li><strong>No synchronization or atomics</strong>: You can’t barrier or coordinate between fragments in a pass, making reductions, scatter/gather, and other data-dependent patterns difficult or impossible.</li><li><strong>Draw-call and precision overhead</strong>: Every neural-net operation requires binding an FBO, swapping textures, and issuing a draw call (dozens per layer) which incurs CPU overhead. Plus, you’re bound to 16- or 32-bit floats (via <code>EXT_color_buffer_float</code>), with no double precision or integer textures.</li></ul><p>Taken together, these constraints make shader-based compute an interesting educational project but a only a historical novelty for real world use. Compute APIs like CUDA or OpenCL give easier and better tools to achieve the same thing.</p><p>Thanks for reading! You can view the code and run the demo locally at the repo <a href="https://github.com/nathan-barry/gpt2-webgl/tree/main">here</a>. Contact me on <a href="https://x.com/nathanbarrydev">X</a> if you have any suggestions.</p><br></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US pauses new student visa interviews as it mulls expanding social media vetting (120 pts)]]></title>
            <link>https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501</link>
            <guid>44109253</guid>
            <pubDate>Tue, 27 May 2025 18:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501">https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501</a>, See on <a href="https://news.ycombinator.com/item?id=44109253">Hacker News</a></p>
Couldn't get https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I salvaged $6k of luxury items discarded by Duke students (241 pts)]]></title>
            <link>https://indyweek.com/culture/duke-students-dumpster-diving/</link>
            <guid>44108207</guid>
            <pubDate>Tue, 27 May 2025 15:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://indyweek.com/culture/duke-students-dumpster-diving/">https://indyweek.com/culture/duke-students-dumpster-diving/</a>, See on <a href="https://news.ycombinator.com/item?id=44108207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					

<article id="post-278832">
	<div>

		
		<p><strong>I</strong> live in an apartment building in downtown Durham that houses more Duke University undergrads than any other category of person—a friend once characterized it as an “adult dorm”—so it wasn’t all that surprising when, last week, I found a cute little table in the trash room on my floor. At the end of the school year, a lot gets thrown away.</p><p>The table was in great condition, amid stacks of linens and unopened boxes of date-nut energy bites. Made from clear acrylic, its edges were tinged a neon lemon-lime color that changed with the light—sometimes appearing to be part of the acrylic itself, other times a reflection dancing along its curves.&nbsp;</p><p>I took it home. When I looked it up online, I discovered it costs $900. (Shipping cost: $199.)</p><p>That was retrieved from the trash room at the end of my hall, where you put things down the chute. The real gold mine is the ground-floor room that the chute empties into, accessible by one of the elevators.</p><p>This is where, around graduation each year, you can find dozens of vacuums, Keurigs, stainless steel trash cans in every size and shape imaginable, mattresses, mirrors, and enough luxury goods to make a reseller weep with joy. The first time I went down there, last week, I noticed something neon in a tote bag and pulled out $395 Balenciaga slides. Nearby were $980 Valentino sneakers—worn, but definitely wearable. More than $1,000 of Lululemon workout clothing tumbled from a bag onto a couch.</p><p>You don’t really have to do any digging—most of the stuff I’ve gotten was sitting on top of discarded furniture. But you do have to rush. After I took the Lululemon haul upstairs, I returned to find city waste workers loading things into a garbage truck, off to a landfill. The volume of discarded clothing seems consistent with generational trends: textile waste in the United States went up by more than 50 percent between 2000 and 2018.&nbsp;</p><div><figure><img decoding="async" width="768" height="1024" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-768x1024.jpeg" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-768x1024.jpeg 768w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-225x300.jpeg 225w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1152x1536.jpeg 1152w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1536x2048.jpeg 1536w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-900x1200.jpeg?crop=1 900w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-600x800.jpeg?crop=1 600w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-450x600.jpeg?crop=1 450w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-300x400.jpeg?crop=1 300w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-150x200.jpeg?crop=1 150w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1200x1600.jpeg 1200w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-2000x2667.jpeg 2000w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-780x1040.jpeg 780w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-400x533.jpeg 400w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-706x941.jpeg 706w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-scaled.jpeg 1920w" sizes="(max-width: 768px) 100vw, 768px"><figcaption>The trash room. Photo by Lena Geller. </figcaption></figure></div><p>Not every treasure is a flashy brand-name item. I also recovered pink satin pajamas that I remember seeing on someone attending a pajama party on my floor, and a ruffled olive-green <em>Top Gun</em> romper from a Halloween event. (Sadly, there was nothing from the risqué Dr. Seuss party. A few months ago, a fire alarm went off, and it became apparent just how much of the building is occupied by Duke students, as nearly everyone except me, my roommate, and a family with two young kids was drunk and dressed in <em>Cat in the Hat</em> costumes.)</p><p>It feels wrong for this much stuff to have been thrown out in the first place, but it also feels mildly wrong to take it. So it was nice to get intermittent reassurance from my building’s maintenance man, Eric. </p><p>The first time, as I was scurrying back to my room, carrying an upholstered kitchen chair that my cats now spend all their time in, I passed Eric in the hallway. He asked me how I was.</p><p>“Just doing some scavenging,” I said. I must have looked guilty, because he said, “That’s OK.”</p><p>A few days later, I was again downstairs in the big trash room when Eric walked in. I moved to leave, feeling awkward about being caught again. “You’re welcome here anytime,” he assured.&nbsp;</p><p>The sheer volume of valuable, usable things being discarded boggles the brain, particularly when it comes to items like clothing with the tags still on and unopened, unexpired food items.&nbsp;</p><p>In trying to make sense of things, I made spreadsheets.</p><p>The first tracks the prices and brands of the items that I kept, donated, or sold. The total value came to around $6,000, not including several items I couldn’t find prices for.</p><div><figure><img decoding="async" width="1024" height="982" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59%E2%80%AFAM-1024x982.png" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-1024x982.png 1024w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-300x288.png 300w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-768x737.png 768w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-1200x1151.png 1200w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-780x748.png 780w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-400x384.png 400w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-706x677.png 706w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM.png 1370w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>A screenshot of one section of the author’s itemized spreadsheet</figcaption></figure></div><p>The second spreadsheet compares Duke’s donation collection data with that at other universities, in an effort to understand whether this college-town phenomenon is universal. I gathered publicly available data from university websites and press releases, supplemented by direct inquiries.</p><p>Duke told me their “Devils Care Donations” initiative collected 32,000 pounds this year through partnerships with TROSA and Goodwill. Ali Harrison, senior associate dean for residence life, says that the university places donation bins in every residence hall on campus, plus off-campus Duke housing like Blue Light and Swift Apartments. Harrison also notes that “Duke students who live off campus in non-Duke housing can schedule a TROSA pickup for large or bulky items and large donations.”</p><p>I emailed six universities, asking about their donation programs and collection data. Most didn’t respond or declined. One directed me to a public web page. Rice University, whose “Give a Hoot! Donate Your Loot!” campaign recently won a statewide award in Texas, sent a detailed response. They reported that they collected around 11,000 pounds of “durable goods” from students this year. (Rice has around 9,000 total students, with roughly half undergrads and half graduate students.)</p><p>Rice’s approach is to implement collections every semester, not just during spring move-out. “By maintaining a consistent presence throughout the academic year,” a spokesperson wrote, “the campaign has become a familiar part of the student experience,” helping students plan ahead to donate rather than discard.</p><p>Looking at the data, Duke’s per-undergraduate donation rate (about 4.9 pounds) is comparable to that at other wealthy private universities like Princeton (7.6 pounds) and Georgetown (6.1 pounds). Duke actually outperforms some schools with similar student demographics like the University of Chicago (0.8 pounds) and Northwestern (0.9 pounds). Most large public universities hover around one pound per student.</p><figure><img decoding="async" width="1024" height="683" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1024x683.jpg" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1024x683.jpg 1024w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-300x200.jpg 300w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-768x512.jpg 768w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1536x1024.jpg 1536w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-2048x1365.jpg 2048w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1200x800.jpg 1200w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-2000x1333.jpg 2000w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-780x520.jpg 780w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-400x267.jpg 400w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-706x471.jpg 706w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>INDY </em>staff writer Lena Geller poses for a picture while wearing Valentino Garavani tennis shoes on Tuesday, May 20, 2025, in Durham. Photo by Angelica Edwards. </figcaption></figure><p><strong>T</strong>he emotional reality of my salvaging week was harder to organize into neat columns. For one, I started feeling like everything I own is shitty. When you’re pulling something out of the trash, it doesn’t feel like it’s going to be a luxury item, so at first, I didn’t think much of a comforter I salvaged and offered it to my boyfriend, who’s always looking for blankets for his dog to lie on. After looking up the cost ($222) and thread count (600), I went back on that offer and replaced my existing comforter with the salvaged one. (The next day, my boyfriend found his own down comforter in the trash.)</p><p>Most items I salvaged were like new, but some needed attention. It felt good to wash, clean, and mend things—removing stains from a blouse, fixing belt loops on black slacks. But then futility would set in. I tried to get the stains out of a pair of muddy Nike high-tops with floral embroidery, using a Solo cup I salvaged as a mixing receptacle to stir together baking soda and hydrogen peroxide into a thick paste, but even after slathering it onto the shoes, the stains persist.&nbsp;</p><p>I also spent some time scrubbing a toaster oven, only to go back to the trash room a few days later and find one that’s cleaner and fancier. Retail value: $400.</p><p>In what would become my final scavenging trip of the year, I tried carrying too many things at once—a handheld vacuum, an air filter, some velvet hangers—and dropped the toaster oven, which splashed water all over me from its steam reservoir.&nbsp;</p><p>Sometimes it’s a spill that does it. I stood there, damp, surrounded by other people’s discards, feeling ridiculous. My apartment was already filled with rescued items. I went home, found that the air filter didn’t fit my unit, and cried.</p><p>The next night, my cat jumped down from the salvaged chair he loves, used his litter box, and then kicked litter everywhere—as per usual. Managing litter has been an ongoing struggle. Various vacuums have proved too weak or too bulky to reach the corners behind the box, so I usually just sweep with a handheld broom and dustpan.</p><p>But as I bent over with my dustpan that night, I remembered the handheld vacuum I’d salvaged just before dropping the toaster oven. I’d found it with its charging cord sitting right next to it, still coiled neatly with a twist tie.</p><p>I grabbed it from my pile of findings and turned it on. It was the most powerful little vacuum I’ve ever seen, its pointed nose perfect for crevices.</p><p><em>Reach Staff Writer Lena Geller at&nbsp;<a href="mailto:lgeller@indyweek.com"><em>lgeller@indyweek.com.</em></a>&nbsp;Comment on this story at&nbsp;<a href="mailto:backtalk@indyweek.com"><em>backtalk@indyweek.com</em></a>.</em><br></p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
</article><!-- #post-${ID} -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Square Theory (543 pts)]]></title>
            <link>https://aaronson.org/blog/square-theory</link>
            <guid>44107942</guid>
            <pubDate>Tue, 27 May 2025 15:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aaronson.org/blog/square-theory">https://aaronson.org/blog/square-theory</a>, See on <a href="https://news.ycombinator.com/item?id=44107942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The story starts in <a href="https://discord.com/invite/GPyU97XBGX">Crosscord</a>, the crossword Discord server. Over 5,000 users strong, the server has emerged as a central hub for the online crossword community, a buzzing, sometimes overwhelming, sometimes delightful town square where total noobs, veteran constructors, and champion solvers alike come together to talk about words that cross each other.</p>

<h2 id="square-roots">Square roots</h2>

<p>We direct our attention toward the #etuiposting channel, Crosscord’s designated space for shitposting (so named because ETUI, a sewing case, is a prototypically shitty piece of crosswordese). There, one afternoon in January 2022, crossword constructor and <a href="https://crosswordnexus.com/">Crossword Nexus</a> warden Alex Boisvert posted what seemed at the time to be an innocuous, mildly interesting observation:</p>

<p><img src="https://aaronson.org/assets/images/square-boisvert.png" alt="Alex Boisvert: JET BLACK and JETBLUE have very different meanings, even though they look superficially similar.  Same thing with CATNAP and DOGNAP.  Any other examples of this?"></p>

<p>Suffice to say, the Crosscord hivemind had other examples of this. <a href="http://blog.bewilderinglypuzzles.com/">Will Nediger</a> replied a few minutes later with the clever MULTITOOL and MULTIPLIERS (words with completely unrelated meanings, despite the fact that PLIERS are a TOOL). Several messages later, Alex chimed back in with the elegant PUB QUIZ and BAR EXAM, a pairing that had been used in some form in crosswords by constructors <a href="http://arctanxwords.blogspot.com/2018/04/puzzle-53-i-thought-this-was-speed.html">Christopher Adams</a> (2018) and <a href="https://www.nytimes.com/crosswords/game/daily/2021/01/29">Robyn Weintraub</a> (2021).</p>

<p>Something about this concept—two sets of synonyms (PUB and BAR, QUIZ and EXAM), which when paired together, form phrases that themselves are not synonyms (PUB QUIZ and BAR EXAM)—captured the minds of Crosscord. Suddenly, the floodgates were open.</p>

<p><img src="https://aaronson.org/assets/images/square-discord-posts.png" alt="Will Nediger: UBEREATS / SUPERFOOD; Assorted-Interests: THROW SHADE / PITCH BLACK; Tyler Hinman, Aged Prodigy: With this topic resurrected, it seems nobody posted what I think is the canonical one: BOOTY CALL and BUTT DIAL; jenna lafleur: ROMAN MARS / CLASSICAL RUINS; gppasco: GRAND CANYON / K-HOLE; robinyu: DAD BOD and FATHER FIGURE; kareila: PERMANENT PRESS / FOREVER STAMP; heywhatsupitsbob: FRIENDLY FAVOR / PLATONIC SOLID"></p>

<p>Intermittently over the next <em>year</em>, #etuiposting would be flooded with these pairs of pairs. They became too much even for the shitposting channel, and were ultimately confined to a thread called #double-doubles (a name <a href="https://heywhatsupitsbob.com/">Bob Weisz</a> and I both proposed simultaneously). Today, more than three years after Alex’s original prompt, the thread still remains active, a wordplay oasis of over 3,000 posts.</p>

<p>There’s something going on here. Something more than a shitpost or an ephemeral trend. Double doubles have the proverbial juice, and the juice lies in their structure. Each pair of pairs can be modeled as a square, where the corners are words and the sides are relations between those words:</p>

<p><img src="https://aaronson.org/assets/images/square-booty-call.jpeg" alt="Square showing BOOTY - phrase - CALL connected via synonyms to BUTT - phrase - DIAL"></p>

<p>It’s this square structure that makes each double double feel tight, feel satisfying, feel like a real “find”. This is the essence of what I’ve started calling <strong>square theory</strong>, and it applies to much more than just posts in a Discord server.</p>

<p>Just like it’s satisfying when an essay or a news story comes full circle, or mindblowing when you find an unexpected cycle in your network of friends, it’s inherently compelling when things wrap around and complete the square. Let’s break it down.</p>

<h2 id="the-theory-of-everything">The theory of everything</h2>

<p>Crosscord wasn’t the first to catch onto this kind of formation: Ricki Heicklen has maintained a <a href="https://rickiheicklen.com/unparalleled-misalignments.html">huge list</a> of double doubles (which she calls “Unparalleled Misalignments”—itself a sort of double double) since 2018, and the success of her recent <a href="https://x.com/tradegal_/status/1920189768261828748">Twitter thread</a> about them is another testament to their widespread appeal. Pairs of the same form pop up on a regular basis in the form of crossword clues and Twitter jokes:</p>

<p><img src="https://aaronson.org/assets/images/square-top-gun.png" alt="Crossword clue [Top gun?] for TSHIRTCANNON, with a square showing TOP - phrase - GUN connected via synonyms to TSHIRT - phrase CANNON">
    <img src="https://aaronson.org/assets/images/square-dad-bod.png" alt="Tweet by Steven W Skinner that says 'Why is it called a dad-bod and not a father-figure', with a square showing DAD - phrase - BOD connected via synonyms to FATHER - phrase - FIGURE">
</p>

<p>However, there’s nothing about the square structure that dictates the edges must represent phrases and synonyms. Each edge of the square can be any relation that connects its vertices (but generally, the stronger the relations, the stronger the square). The vertices don’t even necessarily have to be words—they can be any entity or concept.</p>

<p>It evokes the mathematical field of <a href="https://en.wikipedia.org/wiki/Category_theory">category theory</a>, which very abstractly studies mathematical objects and the relations between them. It’s the topology of the square that makes it satisfying, regardless of what the edges and vertices represent.</p>

<p>Members of the #double-doubles thread have already noticed this, consciously or not, with many of the posts interpreting the original prompt more liberally and swapping out the “synonym” relation for something else:</p>

<p><img src="https://aaronson.org/assets/images/square-left-on-read.png" alt="Crosscord post from Joah: LEFT ON READ vs. RIGHT ON RED. Same number of letters too. Maybe I'll make a mini out of it; Square showing LEFT on READ connected via antonym and homophone to RIGHT on RED">
    <img src="https://aaronson.org/assets/images/square-arizona-wildcat.png" alt="Crosscord post from Quiara, Newsletter Economist: ARIZONA WILDCAT / ARIGATO; Square showing ARIZONA phrase WILDCAT connected via abbr. and translation to ARI word GATO">
</p>

<p>Sometimes it feels like the #double-doubles thread has devolved into just #question-mark-clues (crossword clues that are trying to trick you, requiring you to reinterpret them beyond their words’ most likely meaning, or “surface sense”). But that’s no coincidence—abstractly, every question mark clue takes the form of a square.</p>

<p>When brainstorming for question mark clues, crossword constructors experience this on a regular basis. You start with the answer at hand, playing word association with it in search of a combination of words that usually means one thing (the surface sense) but can be interpreted differently (the intended interpretation) to point to the answer, thus completing the square:</p>

<p><img src="https://aaronson.org/assets/images/square-question-mark-clue.png" alt="Square showing word(s) connected to word(s) by surface sense, which are connected by homonyms to word(s) connected to word(s) by intended interpretation, which leads to the answer"></p>

<p>Take <a href="https://www.nytimes.com/2001/04/08/magazine/endpaper-how-to-solve-the-new-york-times-crossword-puzzle.html">Will Shortz’s all-time favorite clue</a> for instance, from a 1995 Martin Ashwood-Smith puzzle: [It turns into a different story] (which deviously didn’t even include the question mark). On the surface, “turns into a different story” typically means something like “develops into another situation.” But the intended interpretation takes the clue’s words to mean “rotates into another floor,” leading to the correct answer of SPIRAL STAIRCASE:</p>

<p><img src="https://aaronson.org/assets/images/square-spiral-staircase.png" alt="Square showing turns (develops) connected to story (situation) by &quot;develops into another situation&quot;, which are connected by homonyms to turns (rotates) connected to story (floor) by &quot;rotates into another floor&quot;, which leads to the answer SPIRAL STAIRCASE"></p>

<p>You might be familiar with this same sort of brainstorming if you’ve ever tried to come up with a clever title for a research paper, or an apt name for a company. There are plenty of names that might make you go “I guess that could work,” but it’s the square-completing ones that make you go “that’s the one,” or “that’s so good!”</p>

<p>One of my favorite examples of this is <a href="https://www.underconsideration.com/brandnew/">Brand New</a>, the blog that catalogues the latest in corporate rebrands. Leave it to a branding blog to have a name this immaculate:</p>

<p><img src="https://aaronson.org/assets/images/square-brand-new.png" alt="Square showing BRAND phrase NEW connected via homonym and synonym to what the blog chronicles, updated brands"></p>

<p>Even a seemingly straightforward brand name like <a href="https://www.grubhub.com/">Grubhub</a> can exemplify the power of square theory. Presumably, Grubhub’s branding team started with a concept (a centralized app for food deliveries) and came up with a name that completes the square. But remove any edge of the square (besides the edge that dictates the app’s purpose), and you’re left with a name that only <em>kinda</em> works:</p>

<p><img src="https://aaronson.org/assets/images/square-grubhub.png" alt="Complete square showing GRUB rhyme HUB connected via synonyms to what the app provides, a central place for food">
    <img src="https://aaronson.org/assets/images/square-grubnexus.png" alt="Incomplete square showing GRUB and NEXUS connected via synonyms to what the app provides, a central place for food">
</p>
<p><img src="https://aaronson.org/assets/images/square-grubcub.png" alt="Incomplete square showing GRUB rhyme CUB connected via only one synonym to what the app provides, a central place for food">
    <img src="https://aaronson.org/assets/images/square-tubhub.png" alt="Incomplete square showing TUB rhyme HUB connected via only one synonym to what the app provides, a central place for food">
</p>

<p>Aside from crossword clues and brand names, squares appear in the wild all the time in the form of jokes. There’s a vast universe of pun-based jokes (often in the form of dad jokes, or Twitter jokes, or <a href="https://www.timeout.com/newyork/clubs/punderdome">Punderdome</a> puns) that can be modeled as a square, where one side of the square is the contrived setup (“what do you call an X with a Y?”) that connects in at least two ways to the punchline (“an algebra problem!”) on the opposite side.</p>

<p>The strength of the joke rests in the strength of the setup, the punchline, and the connections between them—and if every side of the square is strong, you might have created something funny:</p>

<p><img src="https://aaronson.org/assets/images/square-joke-abstract.png" alt="Square showing a setup (contrived) of at least two words, which are connected by synonyms or homonyms, usually, to at least two other words, the punchline (a real word or phrase, or a play on one)">
    <img src="https://aaronson.org/assets/images/square-impasta.png" alt="Square showing FAKE and NOODLE connected by the setup 'What do you call a fake noodle', which connects via synonyms to IMPOSTOR and PASTA, forming the portmanteau punchline 'An impasta!'">
</p>

<h2 id="getting-into-shape">Getting into shape</h2>

<p>You might be thinking: what’s so special about a square? What about triangle theory, or pentagon theory? (Or rectangle theory? Or rhombus theory? Okay, side lengths and angles <a href="https://en.wikipedia.org/wiki/Topology">don’t matter here</a>.)</p>

<p>Well, it’s true that there’s something compelling about any loop-closing property, regardless of side count—a story that comes full circle is still satisfying no matter how many points it hits in between, and it’s still neat to discover a triangle of people who coincidentally know each other.</p>

<p>But here’s what I think makes squares special: a square is the simplest polygon that has non-adjacent sides. In a triangle, each side is adjacent to the other two sides. But in a square, opposite sides have no points in common, which makes any connection between them feel surprising, like a coincidence. In pentagons and beyond, this still holds, but the extra sides add complexity that make them feel slightly less elegant. Nevertheless, other shapes can be interesting too, but I see them as the exception, not the rule.</p>

<p>Remember Alex Boisvert’s original JET BLACK / JETBLUE example? Seems like it could be modeled as a triangle, right? Well, it turns out the “jet” in JET BLACK refers to the gemstone <a href="https://en.wikipedia.org/wiki/Jet_(gemstone)">jet</a>, which is <a href="https://www.etymonline.com/word/jet">etymologically unrelated</a> to JETBLUE’s airplane jet, so it’s actually more properly modeled as a square:</p>

<p><img src="https://aaronson.org/assets/images/square-jet-triangle.png" alt="Triangle showing JET phrase BLACK colors BLUE airline JET">
    <img src="https://aaronson.org/assets/images/square-jet-square.png" alt="Square showing JET homonym JET phrase BLACK colors BLUE airline JET">
</p>

<h2 id="times-square"><em>Times</em> square</h2>

<p>Now that I’ve established that square theory applies to more than just crosswords, it’s time to talk about crosswords again.</p>

<p>It’s typical for American-style crosswords (à la <em>New York Times</em>) to have a theme, which will generally consist of the 4–6 longest Across entries in the grid, often including a “revealer” that ties the theme together. Nowadays, it’s common gospel among crossword constructors that themes should have some sort of wordplay-based connection—that is, a theme like “famous basketball players” or “brands of cereal” is unlikely to elicit a real “aha” moment from solvers, and thus unlikely to be accepted at major crossword outlets.</p>

<p>So what makes for a <em>good</em> crossword theme? Consistency is definitely key, and a notion of “tightness” is important too (the set of possible theme entries shouldn’t be too much bigger than the theme set that appears in the puzzle). But time and time again, I’ve noticed that what makes a theme really pop is—you guessed it—when it completes the square.</p>

<p>Take, for example, the <a href="https://www.xwordinfo.com/Crossword?date=2/17/2025">Monday, February 17, 2025 <em>New York Times</em> crossword</a> by Kate Hawkins and Erica Hsiung Wojcik, which features a great execution of a typical Monday theme type. In this puzzle, the seemingly unrelated theme entries SCRAPBOOK, POPEYES, UNDER PRESSURE, and GIDDY UP are united by the fact that they each end in an affirmative (OK, YES, SURE, YUP).</p>

<p>In a vacuum, this fact wouldn’t be that interesting, but Kate and Erica give the theme a <em>raison d’etre</em> with the revealer YEAH RIGHT, clued as [“Uh-huh, I bet” … or a literal description of what 17-, 24-, 36- and 50-Across all have]—that is, each themer has a synonym for “YEAH” on its “RIGHT” side. The key here is that YEAH RIGHT itself is an idiomatic phrase (meaning “Uh-huh, I bet”), and not just an arbitrary description of the theme mechanic, so it completes the square:</p>

<p><img src="https://aaronson.org/assets/images/square-yeah.png" alt="Square showing what the entries have, an affirmative ending, connected via synonyms to the phrase YEAH RIGHT"></p>

<p>But it doesn’t stop there. Consider the <a href="https://www.xwordinfo.com/Crossword?date=2/18/2019">Monday, February 18, 2019 <em>New York Times</em> crossword</a> by Leslie Young and Andrea Carla Michaels. The theme entries here are NIGHT NIGHT, WHITE WEDDING, and MUSHROOM BALL (you know, like a vegetarian meatball), and the revealer, clued as [Graduation garb … or what the compound answers to 17-, 28- and 44-Across represent?], is CAP AND GOWN. That is, the first part of each themer can precede CAP (e.g. MUSHROOM CAP), and the second part can precede GOWN (e.g. BALL GOWN). This maps pretty squarely onto not one, but three squares, one for each theme entry:</p>

<p><img src="https://aaronson.org/assets/images/square-night-night.png" alt="Three squares, for NIGHT NIGHT, WHITE WEDDING, and MUSHROOM BALL, each showing the phrase connected by two phrases to CAP and GOWN"></p>

<p>And just for fun, we can conjoin the three squares by their CAP AND GOWN edges to form a unified graph that represents the entire theme’s topology:</p>

<p><img src="https://aaronson.org/assets/images/square-cap-and-gown.png" alt="Unified CAP AND GOWN square graph"></p>

<p>The final crossword we’ll look at, and maybe my favorite crossword of all time, is Alina Abidi’s <a href="https://www.xwordinfo.com/Crossword?date=8/18/2021">Wednesday, August 18, 2021 <em>New York Times</em> crossword</a>, with a theme that feels almost impossibly tight.</p>

<p>The puzzle has essentially two theme entries, PIN THE TAIL ON THE DONKEY and WHITE ELEPHANT, with the apt revealer PARTY ANIMAL [Frequent reveler, or a hint to 16-/26- and 36-Across]. That alone is clever, since both themers are party games with animals in their names. But then Alina hits you with the <em>second</em> revealer of THOMAS NAST [Cartoonist suggested by this puzzle’s theme], pointing to the fact that not only are the DONKEY and ELEPHANT animals in party games, but they are also the animals that symbolize the Democratic and Republican <em>parties</em>, as popularized by <a href="https://en.wikipedia.org/wiki/Thomas_Nast">Thomas Nast</a>’s political cartoons.</p>

<p>This is the kind of theme that really sticks with you. Or at least it stuck with me, and I tried for years to understand why it felt so amazing. And then I realized square theory offered an explanation. Squares, as we know, feel tight, satisfying, and clever. But Alina’s theme takes that one step further, creating for each theme entry a square with an <em>extra diagonal</em> through it, reflecting the connection between each animal and a political PARTY:</p>

<p><img src="https://aaronson.org/assets/images/square-democrat.png" alt="Square showing PIN THE TAIL ON THE DONKEY containing DONKEY connected to PARTY ANIMAL by setting and example, with an additional Democrats diagonal connecting PARTY and DONKEY">
    <img src="https://aaronson.org/assets/images/square-republican.png" alt="Square showing WHITE ELEPHANT containing ELEPHANT connected to PARTY ANIMAL by setting and example, with an additional Republican diagonal connecting PARTY and ELEPHANT">
</p>

<p>And again, we can combine these two super-squares into one unified theme graph:</p>

<p><img src="https://aaronson.org/assets/images/square-party-animals.png" alt="Unified PARTY ANIMALS square graph"></p>

<p>Granted, there’s more to a crossword than the structure of its theme, and it can be reductive to distill it into a graph like this. Still, for many puzzles, square theory can serve as an illuminating proxy for the intricacy and tightness of a theme. But that’s not all it can do.</p>

<h2 id="letter-box">Letter box</h2>

<p>Let’s talk about Scrabble, one of the <a href="https://www.nytimes.com/2022/01/25/books/review/seven-games-oliver-roeder.html">seven most important games</a> out there. If you’ve ever played Scrabble (or similar games like Bananagrams), you’d know that every word you play has to intersect another word that’s already on the board.</p>

<p><img src="https://aaronson.org/assets/images/square-scrabble-normal.png" alt="Scrabble play that is boring and the word only intersects one other word"></p>

<p>But occasionally, you’ll think up a play that validly intersects not one, but two words on the board, forming a rectangle of words. Plays like this have a certain panache. They’re satisfying, they make you think, “ooh, nice.” And of course, they can be modeled with square theory:</p>

<p><img src="https://aaronson.org/assets/images/square-scrabble-cool.png" alt="Scrabble play where the word MICE intersects two already-on-the-board words CHASM and SINCE">
    <img src="https://aaronson.org/assets/images/square-scrabble.png" alt="Square showing the Scrabble board words CHASM linking A and M, AVID linking A and I, SINCE linking I and C, and MICE linking M and C">
</p>

<p>You might be thinking that the edge relation here (a word that contains both letters) feels a little flimsy, since not every letter in the word is used. But what if every letter in the word <em>was</em> used? What if we could have a dense network of interlocking squares, where every letter was part of exactly two words? Well, we can, and it’s called an American-style crossword.</p>

<p>In American-style crosswords, every letter is mandatorily “checked” (part of an Across and a Down word), which means <em>every</em> letter is a vertex of a square:</p>

<p><img src="https://aaronson.org/assets/images/square-crossword-grid.png" alt="3x3 crossword grid, and a grid of interconnected squares whose vertices are the letters in the crossword and whose edges are the words that connect those letters"></p>

<p>If you’ve ever tried to construct a crossword, you’ll find that the framing of a crossword grid under square theory <em>feels</em> right. When you’re nearing the end of the grid-filling process, finding valid crossings of words to fill that final corner of a grid, there’s a satisfying “clicking” feeling—a sense of magic—when it all fits together, analogous to the wrapping-around feeling of completing the square.</p>

<p>Taking a step back, that means the clues, the themes, and the very grids of crosswords all share the same abstract fundamental structure, the square:</p>

<p><img src="https://aaronson.org/assets/images/square-crosswords-everything.png" alt="Squares from earlier in the post representing clues, themes, and grids of crosswords"></p>

<p>If you accept the premise that squares are satisfying, square theory offers a unified theory for why crosswords are satisfying too. And if squares are fundamentally compelling, the crossword, in its recursively square structure, starts to look like an equally fundamental art form. Like if you started an English-speaking civilization from scratch, someone, somewhere would inevitably reinvent the crossword. And then someone would start a crossword Discord server, and maybe they’d call it Crosscord.</p>

<p><img src="https://aaronson.org/assets/images/square-crosscord.png" alt="Square showing what the server is, a Discord server for crossword puzzles, connected by keywords to CROSSWORD / DISCORD which are portmanteaued into the server's name, CROSSCORD"></p>

<h2 id="its-hip-to-be-square">It’s hip to be square</h2>

<p>If you’ve read this far, I promise you’ll start to notice squares popping up all over the place in your daily life. I can attest, because I’ve been honing the concept for this post for about two years now, and I often find myself thinking “that’s a square!” whenever I come across a tight joke or title or crossword theme.</p>

<p>If you’re a creative person, square theory is a useful framework to keep in mind. If you’re coming up with a title for a paper or a brand name, try to see if you can think of one that completes the square. If you’re writing puns for a popsicle stick or a Laffy Taffy wrapper, you can use squares to model your setups and punchlines. If you’re constructing a crossword, consider whether your theme or your question mark clues can form squares.</p>

<p>And if you’re writing a story or a news article or a blog post, there’s fundamental value in making it come full circle, or perhaps full square.</p>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pyrefly vs. Ty: Comparing Python's Two New Rust-Based Type Checkers (314 pts)]]></title>
            <link>https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/</link>
            <guid>44107655</guid>
            <pubDate>Tue, 27 May 2025 15:01:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/">https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/</a>, See on <a href="https://news.ycombinator.com/item?id=44107655">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
      <blockquote>
<p>Note: I like using em-dashes while writing! Don’t worry, this is not written by AI. <sup><a href="https://medium.com/@brentcsutoras/the-em-dash-dilemma-how-a-punctuation-mark-became-ais-stubborn-signature-684fbcc9f559">(context)</a></sup></p></blockquote>
<p>Earlier this month, two new Rust-based Python type checkers hit the spotlight: <a href="https://github.com/facebook/pyrefly">pyrefly</a> and <a href="https://github.com/astral-sh/ty">ty</a>. Although neither is <em>officially</em> released, they are a welcome change to the Python type checking world, historically dominated by <a href="https://mypy-lang.org/">mypy</a> and <a href="https://pypi.org/project/pylance/">pylance</a>.</p>
<p>While both are open-source and publicly downloadable for quite some time, there have not been any official announcements by Meta nor Astral on their brand new next-generation Python type checkers — <strong>until last week</strong>.</p>
<p>At <a href="https://us.pycon.org/2025/">PyCon 2025</a>, nestled away in a quiet Room 319 at the <a href="https://us.pycon.org/2025/events/typing-summit/">Typing Summit</a>, we had our first official sneak peek into both of these tools — the team behind them, their goals, visions, and ambitions — and their unique approaches to tackling Python’s typing problems.</p>
<figure>
  <img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-introduction.png" alt="ty introduction presentation at PyCon 2025">
  <figcaption>ty team presenting at the typing summit</figcaption>
</figure>
<blockquote>
<p>This blog is a collection of rough notes scribbled during the event, personal conversations with the team, and not-too-thorough experiments that I’ve run myself. As such, some details might be a little blurry.</p></blockquote>
<blockquote>
<p><strong>Also, both of these tools are still in early alpha!</strong></p>
<p>Please do not use this as a definitive judgment as to which one is better and/or worse. This blog is just for fun to see what state the two tools are at now!</p></blockquote>
<blockquote>
<p>The following tests and experiments were performed on the latest versions of pyrefly, ty, mypy, and pyright as of writing this blog:</p>
<ul>
<li><code>pyrefly 0.17.0</code></li>
<li><code>ty 0.0.1-alpha.7 (afb20f6fe 2025-05-26)</code></li>
<li><code>mypy 1.15.0 (compiled: yes)</code></li>
<li><code>pyright 1.1.401</code></li>
</ul></blockquote>
<h2 id="pyrefly">Pyrefly</h2>
<p>Pyrefly is Meta’s new Rust-based Python type checker, replacing <a href="https://pyre-check.org/">Pyre</a> — Meta’s previous Python type checker written in OCaml. The hopes are that Pyrefly should be faster, more portable, and more capable compared to Pyre.</p>
<p>One key thing the Pyrefly team made very clear this year is that they want to be <em><strong>truly open source</strong></em>. Pyre was also <em>technically</em> open source, but it was more of a “we built this for our needs, but here’s the source code if you want it”. In contrast, one of the foundational goals of Pyrefly is to be more engaged with the needs of the open-source community.</p>
<figure>
  <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ZTSZ1OCUaeQ?si=Rc3-M7a7Yh7SSq-X&amp;start=1405" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>pyrefly introduction presentation</figcaption>
</figure>
<h2 id="ty">ty</h2>
<p>ty is also a Rust-based Python type checker currently under development by <a href="https://astral.sh/">Astral</a>, the team behind <a href="https://docs.astral.sh/uv/">uv</a> and <a href="https://github.com/astral-sh/ruff">ruff</a>. The project was formerly known as Red-Knot, but now has its official name: ty. Compared to Meta, Astral is a lot more quiet on its announcement: just a soft launch on GitHub, a quick 30-minute presentation, and a couple of blog articles as podcasts here and there.</p>
<figure>
  <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/7uixlNTOY4s?si=qMCrwoIekSkoH3xF&amp;start=3558" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
  <figcaption>ty introduction presentation</figcaption>
</figure>
<h2 id="similarities">Similarities</h2>
<p>Both pyrefly and ty are written in Rust, both are incremental (albeit implemented slightly differently: see details below), and both are powered under the hood by <a href="https://github.com/astral-sh/ruff">Ruff</a> for AST parsing. Also, both have first-class support for command-line type checking and LSP/IDE integration.</p>
<p>However, other than the fact that they are both fast Python type checkers, that’s where the similarities end. In my opinion, there are four categories in which these two tools differ: <strong>in Speed, Goals, Incrementalization, and Capabilities.</strong> That’s what we’ll explore today.</p>
<h2 id="speed">Speed</h2>
<p>Speed seemed like one of the main focuses of Pyrefly, being mentioned multiple times during the intro presentation. According to the team, it’s 35x faster than Pyre and 14x faster than Mypy/Pyright, with support of up to 1.8 million lines of code per second. Fast enough to “type check on every keystroke”.</p>
<p>In comparison, speed was also one of the main design goals for ty, but it felt like less of a focus during the introduction. The only claim was “1-2 orders of magnitude faster than current generation type checkers”. Naturally, I wanted to test performance out for myself.</p>
<h2 id="benchmarking---pytorch">Benchmarking - PyTorch</h2>
<p>For the first test, I cloned and checked out the latest release of PyTorch (<code>v2.7.0</code>) and compared type check times between pyrefly, ty, mypy, and pyright on a MacBook M4. Two tests were run, one on the entire <code>pytorch</code> repository and another on just the <code>torch</code> subdirectory:</p>
<blockquote>
<p>PyTorch on the latest mypy is not supported. Using <code>mypy 1.14.0</code> instead.</p></blockquote>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-pytorch-benchmarks.svg" alt="pytorch benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'pyrefly check'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'ty check'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'mypy --cache-dir=/dev/null .'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 5 --ignore-failure 'pyright'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      4.039 s ±  0.234 s    [User: 19.135 s, System: 3.850 s]
  Range (min … max):    3.888 s …  4.455 s    5 runs

pyrefly
  Time (mean ± σ):     13.029 s ±  0.136 s    [User: 60.489 s, System: 6.297 s]
  Range (min … max):   12.916 s … 13.184 s    5 run

mypy
  dnf

pyright
  Time (mean ± σ):     262.742 s ±  4.948 s    [User: 472.717 s, System: 18.898 s]
  Range (min … max):   259.173 s … 270.617 s    5 runs
</code></pre></details>

<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-pytorch-torch-benchmarks.svg" alt="pytorch torch benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyrefly check torch'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'ty check torch'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'mypy --cache-dir=/dev/null torch'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyright torch'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      1.123 s ±  0.022 s    [User: 6.460 s, System: 0.604 s]
  Range (min … max):    1.082 s …  1.167 s    10 runs

pyrefly
  Time (mean ± σ):      2.347 s ±  0.261 s    [User: 15.876 s, System: 0.919 s]
  Range (min … max):    2.089 s …  2.988 s    10 runs
  
mypy
  Time (mean ± σ):     24.731 s ±  0.238 s    [User: 24.144 s, System: 0.519 s]
  Range (min … max):   24.299 s … 25.016 s    10 runs
  
pyright
  Time (mean ± σ):     48.096 s ±  1.705 s    [User: 68.526 s, System: 4.072 s]
  Range (min … max):   46.037 s … 50.488 s    10 runs
</code></pre></details>

<p>Out of the gate, we see that for both <code>pytorch</code> and just <code>torch</code>, ty is about 2-3x faster compared to pyrefly, and both are over 10x-20x faster than mypy and pyright.</p>
<blockquote>
<p>One interesting note is that pyrefly detected more source files than ty: about 8600 for pyrefly and 6500 for ty on <code>pytorch</code> (I’m not sure where the discrepancy comes from).</p></blockquote>
<blockquote>
<p><strong>It’s also important to remember that both pyrefly and ty are still in early alpha, and are not feature complete. This may skew the results!</strong></p></blockquote>
<h2 id="benchmarking---django">Benchmarking - Django</h2>
<p>Next, I ran the same benchmark on Django version 5.2.1.</p>
<blockquote>
<p>Note: mypy errored out during this test.</p></blockquote>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-django-benchmarks.svg" alt="django benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyrefly check'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'ty check'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'mypy --cache-dir=/dev/null .'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 10 --ignore-failure 'pyright'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):     578.2 ms ±  27.8 ms    [User: 2980.4 ms, System: 546.9 ms]
  Range (min … max):   557.1 ms … 634.0 ms    10 runs

pyrefly
  Time (mean ± σ):     910.7 ms ±  26.2 ms    [User: 3033.0 ms, System: 565.0 ms]
  Range (min … max):   879.6 ms … 963.1 ms    10 runs
  
mypy
  dnf
  
pyright
  Time (mean ± σ):     16.324 s ±  0.476 s    [User: 24.477 s, System: 1.682 s]
  Range (min … max):   15.845 s … 17.182 s    10 runs
</code></pre></details>

<p>We see the same results across the board with ty being the fastest (2,900 files at 0.6s), pyrefly as a close second (3,200 files at 0.9s), and pyright being the slowest (16s).</p>
<h2 id="benchmarking---mypy">Benchmarking - Mypy</h2>
<p>Finally, I ran the benchmark on the <code>mypy</code> repo itself (more specifically the <code>mypyc</code> subdirectory). Similar results here.</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/fig-mypy-mypyc-benchmarks.svg" alt="mypy mypyc benchmarks"></p><details>
  <summary>Commands Run</summary>
  <ul>
<li><strong>pyrefly:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'pyrefly check mypyc'</code></li>
<li><strong>ty:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'ty check mypyc'</code></li>
<li><strong>mypy:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'mypy --cache-dir=/dev/null mypyc'</code></li>
<li><strong>pyright:</strong> <code>hyperfine --warmup 3 --runs 20 --ignore-failure 'pyright mypyc'</code></li>
</ul>
</details>


<details>
  <summary>Raw Data</summary>
  <pre tabindex="0"><code>ty
  Time (mean ± σ):      74.2 ms ±   1.5 ms    [User: 403.4 ms, System: 41.6 ms]
  Range (min … max):    71.9 ms …  78.1 ms    20 runs

pyrefly
  Time (mean ± σ):     136.0 ms ±   1.5 ms    [User: 728.3 ms, System: 54.5 ms]
  Range (min … max):   133.4 ms … 139.6 ms    20 runs
  
mypy
  Time (mean ± σ):      3.544 s ±  0.099 s    [User: 3.442 s, System: 0.093 s]
  Range (min … max):    3.420 s …  3.774 s    20 runs
  
pyright
  Time (mean ± σ):      2.852 s ±  0.103 s    [User: 4.315 s, System: 0.227 s]
  Range (min … max):    2.704 s …  3.105 s    20 runs
</code></pre></details>

<h2 id="goals">Goals</h2>
<p>The primary goals between pyrefly and ty are where I feel the main difference lies. Pyrefly tries to be as aggressive as possible when typing — inferring as much as possible so that even code with absolutely no explicit types can have some amount of typing guarantees.</p>
<p>ty, on the other hand, follows a different mantra: <strong>the gradual guarantee</strong>. The principal idea is that in a well-typed program, removing a type annotation should not cause a type error. In other words: you shouldn’t need to add new types to working code to resolve type errors.</p>
<figure>
  <img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/gradual-guarantee.png" alt="the gradual guarantee slide from ty presentation">
  <figcaption>the gradual guarantee slide from ty presentation</figcaption>
</figure>
<p>This is shown in this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>MyClass</span>:
</span></span><span><span>    attr <span>=</span> <span>None</span>
</span></span><span><span>
</span></span><span><span>foo <span>=</span> MyClass()
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | revealed type: None</span>
</span></span><span><span><span># ✅ ty.     | Revealed type: `Unknown | None`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "None"</span>
</span></span><span><span><span># ➖ pyright | Type of "foo.attr" is "None"</span>
</span></span><span><span>reveal_type(foo<span>.</span>attr)
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | ERROR: Literal[1] is not assignable to attribute attr with type None</span>
</span></span><span><span><span># ✅ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Incompatible types in assignment (expression has type "int", variable has type "None")</span>
</span></span><span><span><span># ➖ pyright | ERROR: Cannot assign to attribute "attr" for class "MyClass"</span>
</span></span><span><span>foo<span>.</span>attr <span>=</span> <span>1</span>
</span></span></code></pre></div><p>In this example, pyrefly, mypy, and pyright eagerly type <code>foo.attr</code> as <code>None</code> and throw an exception when assigned as <code>1</code> — whereas ty understands that <code>foo.attr = 1</code> should not actually cause a syntax error, and instead types <code>foo.attr</code> as <code>Unknown | None</code> to allow the assignment. (<code>Unknown</code> is a new type added by ty to denote between an <em>explicit</em> <code>Any</code> versus an <em>“unknown”</em> <code>Any</code>.)</p>
<p>As a consequence, this also means that pyrefly can catch some errors that other type checkers cannot. Take this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>my_list <span>=</span> [<span>1</span>, <span>"b"</span>, <span>None</span>]
</span></span><span><span>val <span>=</span> my_list<span>.</span>pop(<span>1</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int | str | None</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "builtins.object"</span>
</span></span><span><span><span># ➖ pyright | Type of "val" is "Unknown"</span>
</span></span><span><span>reveal_type(val)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: `*` is not supported between `None` and `Literal[2]`</span>
</span></span><span><span><span># ➖ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Unsupported operand types for * ("object" and "int")</span>
</span></span><span><span><span># ➖ pyright | &lt; No Error &gt;</span>
</span></span><span><span>new_val <span>=</span> val <span>*</span> <span>2</span>
</span></span></code></pre></div><blockquote>
<p>mypy <em>technically</em> did throw an error, but for the wrong reasons. For example, setting <code>my_list = [1, "b"]</code> would fix the program, but mypy still reports a mismatch between <code>object</code> and <code>int</code>.</p></blockquote>
<p>Pyrefly implicitly types <code>val</code> as <code>int | str | None</code>, even though neither <code>val</code> nor <code>my_list</code> was explicitly typed. This correctly catches the <code>val * 2</code> error below.</p>
<p>This is just one of many examples, as more will be shown later in the <strong>Capabilities</strong> section.</p>
<h2 id="incrementalism">Incrementalism</h2>
<p>Both pyrefly and ty claim to be incremental — meaning that changing one file would only cause a re-parse on the affected area, and not the entire program. Pyrefly uses a custom incremental engine behind the scenes for its type checker. In constrast, ty uses <a href="https://github.com/salsa-rs/salsa">Salsa</a>, the same incremental framework that powers <a href="https://rust-analyzer.github.io/">Rust Analyzer</a>.</p>
<p>Interestingly, what that means is that ty has fine-grained incrementalization: changing a single function would only cause a re-parse on that function itself (and nothing else), and its dependent functions, and so on. Pyrefly, on the other hand, uses module-level incrementation: changing a single function would cause a re-parse on the entire file/module, and its dependent files/modules, etc.</p>
<p>The reason why pyrefly chose module-level over fine-grained (at least from what I’ve gathered) is that module-level incrementalization is already fast enough in Rust, and fine-grained incrementalization results in a much more complex and harder to maintain codebase with minimal performance improvements.</p>
<h2 id="capabilities">Capabilities</h2>
<p>Both the pyrefly and ty teams make it VERY CLEAR that they are still unfinished and in early alpha, with known issues, bugs, and incomplete features. Despite that, I think it’s cool to go over what each supports <em>as of now</em> as it showcases what each team has focused on and determined to be important so far for their next-generation Python type checkers.</p>
<h2 id="implicit-type-inference">Implicit Type Inference</h2>
<p>Implicit type inference is one of the showcase features of pyrefly. For example, here is a simple case of inferring return types:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>foo</span>(imp: Any):
</span></span><span><span>    <span>return</span> str(imp)
</span></span><span><span>
</span></span><span><span>a <span>=</span> foo(<span>123</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: str</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ➖ mypy.   | Revealed type is "Any"</span>
</span></span><span><span><span># ✅ pyright | Type of "a" is "str"</span>
</span></span><span><span>reveal_type(a)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: `+` is not supported between `str` and `Literal[1]`</span>
</span></span><span><span><span># ➖ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | &lt; No Error &gt;</span>
</span></span><span><span><span># ✅ pyright | ERROR: Operator "+" not supported for types "str" and "Literal[1]"</span>
</span></span><span><span>a <span>+</span> <span>1</span>
</span></span></code></pre></div><p>Here’s another example with inferring types of more complex collection objects (in this case, a <code>dict</code>):</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> typing <span>import</span> reveal_type
</span></span><span><span>
</span></span><span><span>my_dict <span>=</span> {
</span></span><span><span>    key: value <span>*</span> <span>2</span>
</span></span><span><span>    <span>for</span> key, value <span>in</span> {<span>"apple"</span>: <span>2</span>, <span>"banana"</span>: <span>3</span>, <span>"cherry"</span>: <span>1</span>}<span>.</span>items()
</span></span><span><span>    <span>if</span> value <span>&gt;</span> <span>1</span>
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: dict[str, int]</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `@Todo`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.dict[builtins.str, builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "my_dict" is "dict[str, int]"</span>
</span></span><span><span>reveal_type(my_dict)
</span></span></code></pre></div><p><strong>But,</strong> here is where the “gradual guarantee” of ty comes in. Take this example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span>my_list <span>=</span> [<span>1</span>, <span>2</span>, <span>3</span>]
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: list[int]</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `list[Unknown]`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.list[builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "my_list" is "list[int]"</span>
</span></span><span><span>reveal_type(my_list)
</span></span><span><span>
</span></span><span><span><span># ➖ pyrefly | ERROR: Argument `Literal['foo']` is not assignable to parameter with type `int` in function `list.append`</span>
</span></span><span><span><span># ✅ ty.     | &lt; No Error &gt;</span>
</span></span><span><span><span># ➖ mypy.   | ERROR: Argument 1 to "append" of "list" has incompatible type "str"; expected "int" </span>
</span></span><span><span><span># ➖ pyright | ERROR: Argument of type "Literal['foo']" cannot be assigned to parameter "object" of type "int" in function "append"</span>
</span></span><span><span>my_list<span>.</span>append(<span>"foo"</span>)
</span></span></code></pre></div><p>pyrefly, mypy, and pyright all assume that <code>my_list.append("foo")</code> is a typing error, even though it is <em>technically</em> allowed (Python collections can have multiple types of objects!) If this is the intended behavior, ty is the only checker that implicitly allows this without requiring additional explicit typing on <code>my_list</code>.</p>
<h2 id="generics">Generics</h2>
<p>Another thing the pyrefly team mentioned during their talk was that while redesigning pyrefly from the ground up, they focused on the “hard problems first”. This means that a lot of the architecture around pyrefly was built around things like generics, overloads, and wildcard imports.</p>
<p>For example, here are some examples where pyrefly and ty both have correct generic resolution:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># === Simple Case ===</span>
</span></span><span><span><span>class</span> <span>Box</span>[T]:
</span></span><span><span>    <span>def</span> <span>__init__</span>(self, val: T) <span>-&gt;</span> <span>None</span>:
</span></span><span><span>        self<span>.</span>val <span>=</span> val
</span></span><span><span>
</span></span><span><span>b: Box[int] <span>=</span> Box(<span>42</span>)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span><span># ✅ ty.     | Revealed type: `Unknown | int`</span>
</span></span><span><span><span># ✅ mypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span><span># ✅ pyright | Type of "b.val" is "int"</span>
</span></span><span><span>reveal_type(b<span>.</span>val)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | ERROR: Argument `Literal[100]` is not assignable to parameter `val` with type `str` in function `Box.__init__`</span>
</span></span><span><span><span># ✅ ty.     | ERROR: Object of type `Box[int]` is not assignable to `Box[str]`</span>
</span></span><span><span><span># ✅ mypy.   | ERROR: Argument 1 to "Box" has incompatible type "int"; expected "str"</span>
</span></span><span><span><span># ✅ pyright | ERROR: Type "Box[int]" is not assignable to declared type "Box[str]"</span>
</span></span><span><span>b2: Box[str] <span>=</span> Box(<span>100</span>)
</span></span><span><span>
</span></span><span><span><span># === Bounded Types with Attribute ===</span>
</span></span><span><span><span>class</span> <span>A</span>:
</span></span><span><span>    x: int <span>|</span> str
</span></span><span><span>
</span></span><span><span><span>def</span> <span>f</span>[T: A](x: T) <span>-&gt;</span> T:
</span></span><span><span>    <span># ✅ pyrefly | revealed type: int | str</span>
</span></span><span><span>    <span># ✅ ty.     | Revealed type: `int | str`</span>
</span></span><span><span>    <span># ✅ mypy.   | Revealed type is "Union[builtins.int, builtins.str]"</span>
</span></span><span><span>    <span># ✅ pyright | Type of "x.x" is "int | str"</span>
</span></span><span><span>    reveal_type(x<span>.</span>x)
</span></span><span><span>    <span>return</span> x
</span></span></code></pre></div><p>Whereas here are some examples where pyrefly has better generic resolution compared to ty:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> typing <span>import</span> Callable, TypeVar, assert_type, reveal_type
</span></span><span><span>    
</span></span><span><span><span># === Generic Class Without Explicit Type Param ===</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>C</span>[T]:
</span></span><span><span>    x: T
</span></span><span><span>
</span></span><span><span>c: C[int] <span>=</span> C()
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: C[int]</span>
</span></span><span><span><span># ➖ ty.     | `C[Unknown]`</span>
</span></span><span><span><span># ✅ pypy.   | Revealed type is "__main__.C[builtins.int]"</span>
</span></span><span><span><span># ✅ pyright | Type of "c" is "C[int]"</span>
</span></span><span><span>reveal_type(c)
</span></span><span><span>
</span></span><span><span><span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span><span># ➖ ty.     | Revealed type: `Unknown`</span>
</span></span><span><span><span># ✅ pypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span><span># ✅ pyright | Type of "c.x" is "int"</span>
</span></span><span><span>reveal_type(c<span>.</span>x)
</span></span><span><span>
</span></span><span><span><span># === Bounded Types with Callable Attribute ===</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>func</span>[T: Callable[[int], int]](a: T, b: int) <span>-&gt;</span> T:
</span></span><span><span>    <span># ✅ pyrefly | revealed type: int</span>
</span></span><span><span>    <span># ➖ ty.     | ERROR: &lt;Error: Object of type `T` is not callable&gt;</span>
</span></span><span><span>    <span># ✅ pypy.   | Revealed type is "builtins.int"</span>
</span></span><span><span>    <span># ✅ pyright | Type of "a(b)" is "int"</span>
</span></span><span><span>    reveal_type(a(b))
</span></span><span><span>    <span>return</span> a
</span></span></code></pre></div><p>Interestingly enough, both pyrefly and ty seem to struggle with resolving covariance and contravariance relationships. Example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> __future__ <span>import</span> annotations
</span></span><span><span>
</span></span><span><span><span>class</span> <span>A</span>[X]:
</span></span><span><span>    <span>def</span> <span>f</span>(self) <span>-&gt;</span> B[X]:
</span></span><span><span>        <span>...</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>B</span>[Y]:
</span></span><span><span>    <span>def</span> <span>h</span>(self) <span>-&gt;</span> B[Y]:
</span></span><span><span>        <span>...</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>cast_a</span>(a: A[bool]) <span>-&gt;</span> A[int]:
</span></span><span><span>    <span># ➖ pyrefly | ERROR: Return type does not match returned value: expected `A[int]`, found `A[bool]`</span>
</span></span><span><span>    <span># ➖ ty.     | ERROR: Returned type `A[bool]` is not assignable to declared return type `A[int]`</span>
</span></span><span><span>    <span># ✅ mypy.   | &lt; No Error &gt;</span>
</span></span><span><span>    <span># ✅ pyright | &lt; No Error &gt;</span>
</span></span><span><span>    <span>return</span> a  <span># Allowed</span>
</span></span></code></pre></div><h2 id="informative-error-messages">Informative Error Messages</h2>
<p>One explicit feature of ty is to have clear and concise error messages.</p>
<p>For example, here is a simple example of a function call with mismatched types:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-error-message.png" alt="ty-error-message.png"></p>
<p>Compared to pyrefly, mypy, and pyright:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/pyrefly-error-message.png" alt="pyrefly-error-message.png"></p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/mypy-error-message.png" alt="mypy-error-message.png"></p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/pyright-error-message.png" alt="pyright-error-message.png"></p>
<p>Here is another example with mismatched return types:</p>
<p><img src="https://blog.edward-li.com/tech/comparing-pyrefly-vs-ty/ty-error-message-2.png" alt="ty-error-message-2.png"></p>
<p>In my opinion, much cleaner! It’s exciting to see new and improved error messages coming to Python.</p>
<h2 id="intersection-and-negation-types">Intersection and Negation Types</h2>
<p>Finally, one really cool feature the Astral team showed off was support for intersection and negation types — which they claim is the only Python type checker to implement. To illustrate this, take a look at this example:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>WithX</span>:
</span></span><span><span>  x: int
</span></span><span><span>
</span></span><span><span><span>@final</span>
</span></span><span><span><span>class</span> <span>Other</span>:
</span></span><span><span>  <span>pass</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>foo</span>(obj: WithX <span>|</span> Other):
</span></span><span><span>  <span>if</span> hasattr(obj, <span>"x"</span>):
</span></span><span><span>    <span># ➖ pyrefly | revealed type: Other | WithX</span>
</span></span><span><span>    <span># ✅ ty.     | Revealed type: `WithX`</span>
</span></span><span><span>    <span># ➖ mypy.   | Revealed type is "Union[__main__.WithX, __main__.Other]"</span>
</span></span><span><span>    <span># ➖ pyright | Type of "obj" is "WithX | Other"</span>
</span></span><span><span>    reveal_type(obj)
</span></span></code></pre></div><blockquote>
<p><code>@final</code> is a new feature in Python 3.12 that prevents a class from being subclassed. This is important for the type checker to know that <code>Other</code> cannot be subclassed with <code>x</code> in the future.</p></blockquote>
<p>Given the constraints that <code>obj</code> is either <code>WithX</code> or final type <code>Other</code>, and <code>obj</code> <em>has</em> to have attribute <code>x</code>, the only resolvable type for <code>obj</code> at <code>reveal_type(obj)</code> is <code>WithX</code>. Breaking down what happens behind the scenes:</p>
<pre tabindex="0"><code>(WithX | Other) &amp; &lt;Protocol with members 'x'&gt;
=&gt; (WithX &amp; &lt;Protocol with members 'x'&gt; | (Other &amp; &lt;Protocol with members 'x'&gt;)
=&gt; WithX | Never
=&gt; WithX
</code></pre><p>Take a look at another example here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>class</span> <span>MyClass</span>:
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>MySubclass</span>(MyClass):
</span></span><span><span>    <span>...</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>bar</span>(obj: MyClass):
</span></span><span><span>    <span>if</span> <span>not</span> isinstance(obj, MySubclass):
</span></span><span><span>        <span># ➖ pyrefly | revealed type: MyClass</span>
</span></span><span><span>        <span># ✅ ty.     | Revealed type: `MyClass &amp; ~MySubclass`</span>
</span></span><span><span>        <span># ➖ mypy.   | Revealed type is "__main__.MyClass"</span>
</span></span><span><span>        <span># ➖ pyright | Type of "obj" is "MyClass"</span>
</span></span><span><span>        reveal_type(obj)
</span></span></code></pre></div><p>ty is the only type checker to resolve <code>obj</code> at <code>reveal_type(obj)</code> to <code>MyClass &amp; ~MySubclass</code>. This means that ty introduces new paradigms to Python types:</p>
<p><strong>intersections and negations!</strong> Neat!</p>
<p>However, this is still in early alpha! For example, this case here:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span>def</span> <span>bar</span>(obj: HasFoo):
</span></span><span><span>    <span>if</span> <span>not</span> hasattr(obj, <span>"bar"</span>):
</span></span><span><span>        reveal_type(obj)
</span></span><span><span>        reveal_type(obj<span>.</span>foo)
</span></span></code></pre></div><p><code>reveal_type(obj)</code> has the correct type of <code>HasFoo &amp; ~&lt;Protocol with members 'bar'&gt;</code>, but <code>reveal_type(obj.foo)</code> resolves to <code>@Todo</code> even though <code>obj.foo</code> should be resolvable to the function <code>foo</code> given the constraints.</p>
<p>As one final fun party trick, here is ty using intersection and negation types to “solve” <a href="https://en.wikipedia.org/wiki/Diophantine_equation">diophantine equations</a>:</p>
<div><pre tabindex="0"><code data-lang="python"><span><span><span># Simply provide a list of all natural numbers here ...</span>
</span></span><span><span>type Nat <span>=</span> Literal[<span>1</span>, <span>2</span>, <span>3</span>, <span>4</span>, <span>5</span>, <span>6</span>, <span>7</span>, <span>8</span>, <span>9</span>, <span>10</span>, <span>11</span>, <span>12</span>, <span>13</span>]
</span></span><span><span>
</span></span><span><span><span>def</span> <span>pythagorean_triples</span>(a: Nat, b: Nat, c: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>2</span> <span>+</span> b<span>**</span><span>2</span> <span>==</span> c<span>**</span><span>2</span>)
</span></span><span><span>    <span># reveals 'bool': solutions exist (3² + 4² == 5²)</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>fermats_last_theorem</span>(a: Nat, b: Nat, c: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>3</span> <span>+</span> b<span>**</span><span>3</span> <span>==</span> c<span>**</span><span>3</span>)
</span></span><span><span>    <span># reveals 'Literal[False]': no solutions!</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>catalan_conjecture</span>(a: Nat, b: Nat):
</span></span><span><span>    reveal_type(a<span>**</span><span>2</span> <span>-</span> b<span>**</span><span>3</span> <span>==</span> <span>1</span>)
</span></span><span><span>    <span># reveals 'bool': solutions exist (3² - 2³ == 1)</span>
</span></span></code></pre></div><h2 id="final-thoughts">Final Thoughts</h2>
<p>Overall, it’s exciting to have two new faster type checkers in the Python ecosystem! As of right now, pyrefly and ty seem to follow two different systematic goals. Ty takes a gradual approach to typing - given a program that (theoretically) runs flawlessly, running a type checker should not raise any new typing errors - and if it does, it probably indicates an actual flaw somewhere in the code. Pyrefly takes a different approach, one that is similar to many state-of-the-art Python type checkers today - infer as many types as possible, at the cost of possibly introducing typing errors where it shouldn’t.</p>
<p>As mentioned multiple times, both pyrefly and ty are in early alpha. I strongly suspect the features and capabilities of both tools will converge as time goes on, but nevertheless, it is still cool to see where the two type checkers are at now and how they might come into play in different scenarios sometime in the future.</p>
<p><strong>Go try these out for yourself now!</strong></p>
<p>You can try out pyrefly over at <strong><a href="https://pyrefly.org/sandbox">pyrefly.org/sandbox</a></strong>, and ty over at <strong><a href="https://play.ty.dev/">play.ty.dev</a></strong>. Both also have their respective <code>pip install</code> commands and plugins for your editor (VSCode, Cursor, etc).</p>
<p>In the meantime, I heard rumors that Google is planning on open-sourcing their own Go-based Python type checker, so it’ll be very cool to check that out once it comes out 👀 …</p>
<h2 id="appendix">Appendix</h2>
<p>I just wanted to call out that ty’s tests are written in… <strong>MARKDOWN</strong>! How cool is that?</p>
<blockquote>
<p><strong><a href="https://github.com/astral-sh/ruff/tree/main/crates/ty_python_semantic/resources/mdtest">https://github.com/astral-sh/ruff/tree/main/crates/ty_python_semantic/resources/mdtest</a></strong></p></blockquote>
<hr>
<p><em>Thanks for reading!</em></p>
<p><em>If you notice any mistakes, comments, or feedback, please let me know!</em></p>
<p><em>Contact: <a href="mailto:blog@edward-li.com">blog@edward-li.com</a></em></p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Malai – securely share local TCP services (database/SSH) with others (104 pts)]]></title>
            <link>https://malai.sh/hello-tcp/</link>
            <guid>44107393</guid>
            <pubDate>Tue, 27 May 2025 14:34:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://malai.sh/hello-tcp/">https://malai.sh/hello-tcp/</a>, See on <a href="https://news.ycombinator.com/item?id=44107393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="38"><div data-id="42"><p>Introducing Malai TCP &amp; A Bonus!</p><comment data-id="44"></comment></div><div data-id="45"><p><a href="https://github.com/kulfi-project/kulfi/releases/"><code>malai-0.2.5</code></a> is out now!
It brings a new feature to share your local TCP server with the world!</p><p>
Now you can share any TCP-based service running locally — including your
SSH service, Postgres database, Redis, or even a custom TCP protocol — using the
same seamless workflow that you used with <code>malai http</code>.</p></div><p>Install <code>malai</code> today using:</p><div data-id="47"><comment data-id="48"></comment><pre data-id="57"><code data-id="58">curl -fsSL https://malai.sh/install.sh | sh
</code></pre><comment data-id="59"></comment></div><p>And run:</p><div data-id="67"><comment data-id="68"></comment><pre data-id="77"><code data-id="78">$ malai tcp 5432 --public
Malai: Sharing port 5432
Run malai tcp-bridge &lt;id52&gt; &lt;some-port&gt;
to connect to it from any machine.
</code></pre><comment data-id="79"></comment></div><p>This will share your local TCP server running on port 5432 with the world. You
can connect to it from any machine using the command:</p><div data-id="87"><comment data-id="88"></comment><pre data-id="97"><code data-id="98">$ malai tcp-bridge &lt;id52&gt; 9091
Listening on 127.0.0.1:9091
</code></pre><comment data-id="99"></comment></div><p>Now you can connect to <code>localhost:9091</code> and it'll go through <code>malai</code> and
connect to the exposed service.</p><div data-id="107"><p>Share your SSH server</p><comment data-id="109"></comment><div data-id="110"><p>You can even use <code>malai tcp</code> to expose your local SSH server for remote access — without opening port 22 publicly.</p><p>
First, make sure the OpenSSH server is running:</p></div></div><p>Then, run the following on the machine where the SSH server is running:</p><div data-id="131"><comment data-id="132"></comment><pre data-id="141"><code data-id="142">$ malai tcp 22 --public
Malai: Sharing port 5432
Run malai tcp-bridge &lt;id52&gt; &lt;some-port&gt;
to connect to it from any machine.
</code></pre><comment data-id="143"></comment></div><p>On another machine, use the bridge command:</p><div data-id="151"><comment data-id="152"></comment><pre data-id="161"><code data-id="162">$ malai tcp-bridge &lt;id52&gt; 9090
</code></pre><comment data-id="163"></comment></div><p>Replace <code>&lt;id52&gt;</code> with the ID printed by the <code>malai tcp</code> command. Once the
bridge is running, SSH into your machine like this:</p><div data-id="171"><comment data-id="172"></comment><pre data-id="181"><code data-id="182">ssh -p 9090 user@localhost
</code></pre><comment data-id="183"></comment></div><p>You're connecting to <code>localhost:9090</code>, which is where the <code>tcp-bridge</code> is
listening. It forwards your SSH traffic to the original machine via the Kulfi
network. Make sure to use the correct <code>user</code> that exists on the remote machine.</p><div data-id="191"><p>Use cases</p><comment data-id="193"></comment><div data-id="194"><ul>
<li>Secure your SSH server behind the Kulfi network.</li>
<li>Share a local Postgres or Redis instance with your team.</li>
<li>Demo a multiplayer game server or custom TCP service.</li>
<li>Students can share networked apps or environments with instructors for
real-time help or grading.</li>
</ul></div></div><p>To learn more about <code>malai tcp</code>, check out the <a href="https://malai.sh/tcp/">documentation</a>.</p><div data-id="196"><p>Wait, we have more!</p><comment data-id="198"></comment><p>We've also added a new <code>malai folder</code> command to share a folder with everyone.
This is similar to <code>malai http</code> but it serves your local files and folders.
This is more like a call for testing than launching a new feature. Try it out
and give us feedback!</p></div><div data-id="200"><comment data-id="201"></comment><pre data-id="211"><code data-id="212">$ malai folder ~/projects/fastn/assets/ --public
Serving "/Users/siddhant/projects/fastn/assets" on http://127.0.0.1:59136
Malai: Sharing http://127.0.0.1:59136 at
https://pubqaksutn9im0ncln2bki3i8diekh3sr4vp94o2cg1agjrb8dhg.kulfi.site
To avoid the public proxy, run your own with: malai http-bridge

Or use: malai browse kulfi://pubqaksutn9im0ncln2bki3i8diekh3sr4vp94o2cg1agjrb8dhg
</code></pre><comment data-id="213"></comment></div><p>This spins up a basic HTTP server behind the scenes to serve the provided folder:</p><div data-id="221"><div data-id="222"><p><img data-id="223" src="https://malai.sh/-/malai.sh/assets/malai-folder-browser-view.png"></p><comment data-id="224"></comment><p>Browsing a folder served by <code>malai</code></p></div><comment data-id="226"></comment></div><div data-id="230"><p>We're just getting started, and your support means a lot.</p><p>
If you like what we're building, consider <a href="https://github.com/kulfi-project/kulfi">starring the
repo</a> on GitHub. It helps others
discover the project and keeps us motivated to build more!</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Agents API (146 pts)]]></title>
            <link>https://mistral.ai/news/agents-api</link>
            <guid>44107187</guid>
            <pubDate>Tue, 27 May 2025 14:09:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/agents-api">https://mistral.ai/news/agents-api</a>, See on <a href="https://news.ycombinator.com/item?id=44107187">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr"><img src="https://cms.mistral.ai/assets/f2a4b295-ff64-4c16-a42a-14f858c65766.png?width=1080&amp;height=457" alt="Cover"></p>
<p dir="ltr">Today we announce our new Agents API, a major step forward in making AI more capable, useful, and an active problem-solver.</p>
<p dir="ltr">Traditional language models excel at generating text but are limited in their ability to perform actions or maintain context. Our new Agents API addresses these limitations by combining Mistral's powerful language models with:</p>
<ul>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Built-in connectors for code execution, web search, image generation, and MCP tools&nbsp;</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Persistent memory across conversations&nbsp;</p>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation">Agentic orchestration capabilities</p>
</li>
</ul>
<p dir="ltr">The Agents API complements our <a href="https://docs.mistral.ai/capabilities/completion/" target="_blank" rel="noopener">Chat Completion API</a> by offering a dedicated framework that simplifies implementing agentic use cases. It serves as the backbone of enterprise-grade agentic platforms.</p>
<p dir="ltr">By providing a reliable framework for AI agents to handle complex tasks, maintain context, and coordinate multiple actions, the Agents API enables enterprises to use AI in more practical and impactful ways.</p>
<h2 dir="ltr">Mistral agents in action.</h2>
<p dir="ltr">Explore the diverse applications of Mistral’s Agents API across various sectors:</p>
<ul>
<li id="demo-github" dir="ltr">
<h3>Coding assistant with Github.</h3>
<p dir="ltr">An agentic workflow built with Mistral's agents API where an agent interacts with Github and oversees a developer agent, powered by DevStral to write code. The agent is granted full authority over Github, showcasing automated software development task management.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/1Tt9Fq1pUPQ?si=j4fIT7TqM1RGsyRG" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/github_agent" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-linear" dir="ltr">
<h3>Linear tickets assistant.</h3>
<p dir="ltr">An intelligent task coordination assistant powered by our Agents API, using multi-server MCP architecture to transform call transcripts to PRDs to actionable Linear issues and track project deliverables.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/4UPP-JEjcKo?si=gMuPof7qCpuHuc2z" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/prd_linear_ticket" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-finance" dir="ltr">
<h3>Financial analyst.</h3>
<p dir="ltr">A financial advisory agent constructed with our Agents API, orchestrating multiple MCP servers to source financial metrics, compile insights, and archive results securely.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/ocxRKz73UJw?si=2xJffa3oIFBViA56" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/financial_analyst" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-travel" dir="ltr">
<h3>Travel assistant.</h3>
<p dir="ltr">A powerful AI travel assistant that helps users plan their trips, book accommodations, and manage travel needs.</p>
<iframe title="YouTube video player" src="https://www.youtube.com/embed/DSYlhtG2UNM?si=ZZH4OSd1u3QzhpwF" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe><a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/travel_assistant" target="_blank" rel="noopener">Read our cookbook</a></li>
<li id="demo-nutrition" dir="ltr">
<h3>Nutrition assistant.</h3>
<p dir="ltr">An AI-powered food diet companion designed to help users establish goals, log meals, receive personalized food suggestions, track their daily achievements, and discover dining options that align with their nutritional targets.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/uEG2z2esl14?si=Ca_PY02gfVeWChgJ" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<a href="https://github.com/mistralai/cookbook/tree/main/mistral/agents/food_diet_companion" target="_blank" rel="noopener">Read our cookbook</a></li>
</ul>
<h2 dir="ltr">Create an agent with built-in connectors and MCP tools.</h2>
<p dir="ltr">Each agent can be equipped with powerful built-in connectors, which are tools that are deployed and ready for Agents to call on demand, and MCP tools:&nbsp;</p>
<ul>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/code_interpreter/" target="_blank" rel="noopener">Code execution</a></h3>
<p dir="ltr" role="presentation">The Agents API can use the code execution connector, empowering developers to create agents that execute Python code in a secure sandboxed environment. This enables agents to tackle a wide range of tasks, including mathematical calculations and analysis, data visualization and plotting, and scientific computing.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/image_generation/" target="_blank" rel="noopener">Image generation</a></h3>
<p dir="ltr" role="presentation">The image generation connector tool, powered by Black Forest Lab FLUX1.1 [pro] Ultra, enables agents to create images for diverse applications. This feature can be leveraged for various use cases such as generating visual aids for educational content, creating custom graphics for marketing materials, or even producing artistic images.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/document_library/" target="_blank" rel="noopener">Document library</a></h3>
<p dir="ltr" role="presentation">Document Library is a built-in connector tool that enables agents to access documents from Mistral Cloud. It powers the integrated RAG functionality, strengthening agents’ knowledge by leveraging the content of user-uploaded documents.</p>
</li>
<li dir="ltr" aria-level="1">
<h3><a href="https://docs.mistral.ai/agents/connectors/websearch/" target="_blank" rel="noopener">Web search</a></h3>
<p dir="ltr" role="presentation">The Agents API offers web search as a connector, enabling developers to combine Mistral models with diverse, up-to-date information from web search, reputable news, and other sources. This integration facilitates the delivery of up-to-date, informed, evidence-supported responses.</p>
<p dir="ltr">Agents with web search capabilities show a significant improvement in performance. In the SimpleQA benchmark, Mistral Large and Mistral Medium with web search achieve scores of 75% and 82.32%, respectively, compared to 23% and 22.08% without web search (see figure below).</p>
<h4>SimpleQA Accuracy (Higher is better)</h4>

</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/mcp/" target="_blank" rel="noopener">MCP tools</a></h3>
<p dir="ltr" role="presentation">The Agents API SDK can also leverage tools built on the Model Context Protocol (MCP)—an open, standardized protocol that enables seamless integration between agents and external systems. MCP tools provide a flexible and extensible interface for agents to access real-world context, including APIs, databases, user data, documents, and other dynamic resources. Check out the <a href="#demo-github" rel="noopener">Github</a>, <a href="#demo-finance" rel="noopener">Financial Analyst</a>, and <a href="#demo-linear" rel="noopener">Linear</a> MCP demos to learn how to use MCP tools with Mistral Agents in action.</p>
<img src="https://cms.mistral.ai/assets/5a0eb67b-819c-4a3f-9cc0-7dba190d58d2.svg?width=null&amp;height=null" alt="Mcp Mistral"></li>
</ul>
<h2 dir="ltr">Memory and context with stateful conversations.</h2>
<p dir="ltr">The Agents API provides robust conversation management through a flexible and stateful conversation system. Each conversation retains its context, allowing for seamless and coherent interactions over time.</p>
<ul>
<li>
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#conversations" target="_blank" rel="noopener">Conversation management</a></h3>
<p dir="ltr">There are two ways to start a conversation:</p>
<ol>
<li dir="ltr">With an Agent: Create a conversation with a specific agent_id to leverage its specialized capabilities.</li>
<li dir="ltr">Direct Access: Start a conversation by directly specifying the model and completion parameters, providing quick access to built-in connectors.</li>
</ol>
<p dir="ltr">Each conversation maintains a structured history through conversation entries, ensuring that the context is preserved across interactions.</p>
</li>
<li>
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#continue-a-conversation-working" target="_blank" rel="noopener">Stateful interactions and conversation branching</a></h3>
<p dir="ltr">Developers are no longer required to monitor conversion history; they have the ability to view past conversations. They can always continue any conversation or initiate new conversation paths from any point.&nbsp;</p>
</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/agents_basics/#streaming-output-working" target="_blank" rel="noopener">Streaming output</a></h3>
<p dir="ltr">The API also supports streaming outputs, both when starting a conversation and continuing a previous one. This feature allows for real-time updates and interactions.&nbsp;</p>
</li>
</ul>
<h2 dir="ltr">Agent orchestration.</h2>
<p dir="ltr">The true power of our Agents API lies in its ability to orchestrate multiple agents to solve complex problems. Through dynamic orchestration, agents can be added or removed from a conversation as needed—each one contributing its unique capabilities to tackle different parts of a problem.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/55ca02be-4dfa-4f0e-ba6a-adc7c54dce4c.svg?width=null&amp;height=null" alt="Agents"></p>
<ul>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/agents/handoffs/#create-an-agentic-workflow" target="_blank" rel="noopener">Creating an agentic workflow</a></h3>
<p dir="ltr">To build a workflow with handoffs, start by creating all necessary agents. You can create as many agents as needed, each with specific tools and models, to form a tailored workflow.</p>
</li>
<li dir="ltr">
<h3><a href="https://docs.mistral.ai/https://docs.mistral.ai/agents/handoffs/#create-an-agentic-workflow" target="_blank" rel="noopener">Agent handoffs</a></h3>
<p dir="ltr">Once agents are created, define which agents can hand off tasks to others. For example, a finance agent might delegate tasks to a web search agent or a calculator agent based on the conversation's needs.</p>
<p dir="ltr">Handoffs enable a seamless chain of actions. A single request can trigger tasks across multiple agents, each handling specific parts of the request. This collaborative approach allows for efficient and effective problem-solving, unlocking powerful possibilities for real-world applications.</p>
</li>
</ul>
<h2 dir="ltr">Get started.</h2>
<p dir="ltr">To get started, check out our <a href="https://docs.mistral.ai/agents/agents_introduction" target="_blank" rel="noopener">docs</a>, create your first agent, and start building!&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Cline Doesn't Index Your Codebase (and Why That's a Good Thing) (156 pts)]]></title>
            <link>https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing</link>
            <guid>44106944</guid>
            <pubDate>Tue, 27 May 2025 13:44:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing">https://cline.bot/blog/why-cline-doesnt-index-your-codebase-and-why-thats-a-good-thing</a>, See on <a href="https://news.ycombinator.com/item?id=44106944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div href="/install?utm_source=website&amp;utm_medium=header"><span><svg fill="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M23.15 2.587L18.21.21a1.494 1.494 0 0 0-1.705.29l-9.46 8.63-4.12-3.128a.999.999 0 0 0-1.276.057L.327 7.261A1 1 0 0 0 .326 8.74L3.899 12 .326 15.26a1 1 0 0 0 .001 1.479L1.65 17.94a.999.999 0 0 0 1.276.057l4.12-3.128 9.46 8.63a1.492 1.492 0 0 0 1.704.29l4.942-2.377A1.5 1.5 0 0 0 24 20.06V3.939a1.5 1.5 0 0 0-.85-1.352zm-5.146 14.861L10.826 12l7.178-5.448v10.896z"></path></svg></span><p><span>Install Cline<!-- --> • <!-- -->1.6M<!-- --> <!-- -->installs</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckLake is an integrated data lake and catalog format (235 pts)]]></title>
            <link>https://ducklake.select/</link>
            <guid>44106934</guid>
            <pubDate>Tue, 27 May 2025 13:43:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ducklake.select/">https://ducklake.select/</a>, See on <a href="https://news.ycombinator.com/item?id=44106934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<!-- <div class="searchoverlay">
	<div>
		<form autocomplete="off">
			<div class="autocomplete">
				<div class="empty_input"></div>
				<input id="q" type="text" name="q" placeholder="Search docs & blog">
			</div>
		</form>
		<div id="search_results"></div>
		<div class="shortcuts">
			Search Shortcut <span>cmd</span> + <span>k</span> | <span>ctrl</span> + <span>k</span>
		</div>
	</div>
</div> -->

		

			

<div>
      
    
			<p>DuckLake delivers advanced data&nbsp;lake features without traditional lakehouse complexity by using Parquet files and your SQL database. It's an open, standalone format from the DuckDB team.</p>
			
		</div>



<div>
		<div>
      <h2>
        
        Deployment scenarios
        
      </h2>
    
			<p>DuckLake uses a database system to manage your metadata for the catalog. All you need to run your own data warehouse is a database system and storage for Parquet files.</p>
		</div>
		<div>
					<div>
						<ul>
							
							<li data-tab="arch-tab2" data-iconclass="postgre" data-multi="true">PostgreSQL</li>
							<li data-tab="arch-tab3" data-iconclass="sqlite" data-multi="true">SQLite</li>
							<li data-tab="arch-tab4" data-iconclass="mysql" data-multi="true">MySQL</li>
							<li data-tab="arch-tab1" data-iconclass="duckdb" data-multi="false">DuckDB</li>
						</ul>
						<p>← Choose catalog database</p>
					</div>
					
				
					<div>
						<div>
      <h4>
        
        Client
        
      </h4>
    
							
							
							
							<div>
								<div>
      <h4>
        
        Clients
        
      </h4>
    
									<p>Users can run multiple DuckLake clients and connect concurrently to PostgreSQL, MySQL or SQLite.</p>
								</div>
								<div>
      <h4>
        
        Client
        
      </h4>
    
									<p>DuckDB also works with DuckLake as the catalog database. In this case, you are limited to a single client.</p>
								</div>
							</div>
							
						</div>
						<div>
							<div>
      <h4>
        
        Catalog database
        
      </h4>
    
								
								<div>
									<p><img src="https://ducklake.select/images/deployment_diagram/database.svg" alt="Database Icon"></p>
								</div>
								
								<div>
      <h4>
        
        Catalog database
        
      </h4>
    
										<p>DuckLake can use any SQL system as its catalog database, provided that it supports ACID transactions and primary key constraints.</p>
									</div>
								
								
							</div>
							<div>
      <h4>
        
        Storage
        
      </h4>
    
								
								<div>
									<p><img src="https://ducklake.select/images/deployment_diagram/parquet_folder.svg" alt="Parquet Folder"></p><p>Parquet</p>
								</div>
								
								<div>
      <h4>
        
        Storage
        
      </h4>
    
										<p>DuckLake can store your data on any object storage such as AWS S3.</p>
									</div>
								
								
							</div>
						</div>
					</div>
					
					
				</div>
	</div>


<!--
<section>
	<div class="wrap">
      <h2>
        
        Use cases
        
      </h2>
    
		<div class="cards vertical images">
			<div class="card">
				<div class="image"></div>
				<div class="content">
      <h3>
        
        Multiplayer DuckDB
        
      </h3>
    
					<p>DuckLake unlocks concurrency for multiple DuckDB clients.</p>
					<a href="#" class="textbutton arrow-right">Read more</a>
				</div>
			</div>
			<div class="card">
				<div class="image"></div>
				<div class="content">
      <h3>
        
        Self-hosted data warehouse
        
      </h3>
    
					<p>DuckLake allows you to host your own local data warehouse.</p>
					<a href="#" class="textbutton arrow-right">Read more</a>
				</div>
			</div>
		</div>
	</div>
</section>
-->


<div>
      <h2>
        
        DuckLake’s key features
        
      </h2>
    
		<div>
			<div>
				<p><img src="https://ducklake.select/images/icons/waves.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Data lake operations
        
      </h3>
    
					<p>DuckLake supports snapshots, time travel queries, schema evolution and partitioning.</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/documents.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Lightweight snapshots
        
      </h3>
    
					<p>You can have as many snapshots as you want without frequent compacting steps!</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/pipette.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        ACID transactions
        
      </h3>
    
					<p>DuckLake allows concurrent access with ACID transactional guarantees over multi-table operations.</p>
				</div>
			</div>
			<div>
				<p><img src="https://ducklake.select/images/icons/clock.svg" alt="wave icon">
				</p>
				<div>
      <h3>
        
        Performance-oriented
        
      </h3>
    
					<p>DuckLake uses statistics for filter pushdown, enabling fast queries even on large datasets.</p>
				</div>
			</div>
		</div>
	</div>

<div>
		<div>
      <h2>
        
        In Conversation: DuckDB Founders on DuckLake
        
      </h2>
    
			<p>Listen to Hannes Mühleisen and Mark Raasveldt walk through the history of data lakes and introduce DuckLake, a new lakehouse format.</p>
		</div>
		<div data-video-id="zeonmOO9jm4">
				<p><img src="https://ducklake.select/images/thumb_introducting-ducklake.png" alt="Thumbnail: Introducing DuckLake"></p>
			</div>
	</div>

<div id="quickinstall">
		<div>
      <h2>
        
        Create your first DuckLake with DuckDB
        
      </h2>
    
			<p>DuckDB provides first-class support for DuckLake through its highly portable extension, running wherever DuckDB does.</p>
			<!--<a href="/docs/installation/" class="button transparent">More installation options</a>-->
		</div>
		<div>
				<div>
					<ul>
						
						<li data-client="duckdb">DuckDB</li>
						<li data-client="sqlite">SQLite</li>
						<li data-client="postgresql">PostgreSQL</li>
						<li data-client="mysql">MySQL</li>
					</ul>
				</div>
				
				<div>
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:metadata.ducklake'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
				</div>
				
	</div></div>

<div id="quick-installation">

<div data-install="duckdb">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:metadata.ducklake'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="postgresql">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> postgres</span><span>;</span>

<span>-- Make sure that the database `ducklake_catalog` exists in PostgreSQL.</span>
<span>ATTACH</span> <span>'ducklake:postgres:dbname=ducklake_catalog host=your_postgres_host'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="sqlite">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> sqlite</span><span>;</span>

<span>ATTACH</span> <span>'ducklake:sqlite:metadata.sqlite'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

<div data-install="mysql">
<figure><pre><code data-lang="sql"><span>INSTALL</span><span> ducklake</span><span>;</span>
<span>INSTALL</span><span> mysql</span><span>;</span>

<span>-- Make sure that the database `ducklake_catalog` exists in MySQL</span>
<span>ATTACH</span> <span>'ducklake:mysql:db=ducklake_catalog host=your_mysql_host'</span> <span>AS</span> <span>my_ducklake</span><span>;</span>
<span>USE</span> <span>my_ducklake</span><span>;</span></code></pre></figure>
</div>

</div>


<section>
	<div>
      <h2>
        
        Frequently asked questions
        
      </h2>
    
			<p>Answers to common questions to help you understand and make the most of DuckLake.</p>
		</div>
	
	<div>
		<div>
      <h3>
        
        
				Why should I use DuckLake?
			
        
      </h3>
    
			<div>
				<p>DuckLake provides a lightweight one-stop solution for if you need a data lake and catalog.

				</p><p>You can use DuckLake for a “multiplayer DuckDB” setup with multiple DuckDB instances reading and writing the same dataset –
				a concurrency model <a href="https://duckdb.org/docs/stable/connect/concurrency">not supported by vanilla DuckDB</a>.</p>

				<p>If you only use DuckDB for both your DuckLake entry point and your catalog database, you can still benefit from using DuckLake:
				you can run time travel queries,
				exploit data partitioning,
				and can store your data in multiple files instead of using a single (potentially very large) database file.</p>
			</div>
		</div>

		<div>
      <h3>
        
        
				What is DuckLake?
			
        
      </h3>
    
			<div><p>
				First of all, a catchy name for a DuckDB-originated technology for data lakes and lakehouses.
				More seriously, the term “DuckLake” can refer to three things:

				</p><ol>
					<li>the <i>specification</i> of the DuckLake lakehouse format,</li>
					<li>the <a href="https://duckdb.org/docs/stable/core_extensions/ducklake"><code>ducklake</code> <i>DuckDB extension</i></a>, which supports reading/writing datasets in the DuckLake specification,</li>
					<li>a DuckLake, a <i>dataset</i> stored using the DuckLake lakehouse format.</li>
				</ol>
			</div>
		</div>

		<div>
      <h3>
        
        
				What is the license of DuckLake?
			
        
      </h3>
    
			<p>
				The DuckLake specification and the DuckLake DuckDB extension are released under the MIT license.
			</p>
		</div>

	</div>
	
	
	
</section>


		




	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Art of Fugue – Contrapunctus I (2021) (120 pts)]]></title>
            <link>https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/</link>
            <guid>44106764</guid>
            <pubDate>Tue, 27 May 2025 13:25:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/">https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/</a>, See on <a href="https://news.ycombinator.com/item?id=44106764">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-22677">
		
	
	<div>
		
<p>JS Bach’s last set of works, collectively titled <em><a href="https://en.wikipedia.org/wiki/The_Art_of_Fugue">The Art of Fugue</a></em>, was published shortly after his death. It was not a big hit. Dense counterpoint was deeply unfashionable at that time, as Western European aristocratic tastes shifted toward singable melodies over block chords. The first published edition of <em>The Art of Fugue</em> only sold about thirty copies, and it wasn’t performed in its entirety until 1922.</p>
<p>Eventually the classical music audience did come to admire Bach’s final fugue collection, but it took <a href="https://en.wikipedia.org/wiki/Johann_Sebastian_Bach#19th_century">almost 100 years after it was written</a>. The fugues still aren’t the easiest listening experience. They were meant to be didactic, to be played and studied rather than to be listened to–though of course you are free to listen to and enjoy them. I’m finding that my own enjoyment is much enhanced by opening up the structure through visualization, so that’s what I’ve done with <a href="https://www.amazon.com/Bach-J-S-Fugue-Angela-Hewitt/dp/B00MX51FHW">Angela Hewitt’s recording of Contrapunctus I</a> using Ableton Live.</p>
<p><iframe title="Bach - The Art of Fugue Contrapunctus I - Ableton Live visualization" width="640" height="480" src="https://www.youtube.com/embed/-yRqKp2rqPk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>The main thing to listen (and watch) for here is <a href="https://en.wikipedia.org/wiki/The_Art_of_Fugue#Structure">the subject</a>, the little melody that each voice plays as it enters. After the subject, the voices wander off to play other intertwining parts, occasionally returning to the subject as they go. In the subsequent <em>Art of Fugue</em> pieces, Bach does all kinds of twisting and warping of the subject, writing it <a href="https://en.wikipedia.org/wiki/Inversion_(music)#Melodies">upside down</a>, <a href="https://en.wikipedia.org/wiki/Retrograde_(music)">backwards</a>, <a href="https://en.wikipedia.org/wiki/Diminution#Diminution_in_composition">twice as fast</a>, <a href="https://en.wikipedia.org/wiki/Augmentation_(music)">half as fast</a>, <a href="https://en.wikipedia.org/wiki/Stretto">overlaid on top of itself</a>, and so on. In Contrapunctus I, however, he doesn’t do any of these formal games. It sounds more like he’s just riffing around the subject. It’s almost casual, at least by his standards.</p>
<p><span id="more-22677"></span>Here’s Glenn Gould playing Contrapunctus I on organ.</p>
<p><iframe title="Glenn Gould plays Bach &quot;The Art Of Fugue BWV 1080&quot; Organ/Piano" width="640" height="360" src="https://www.youtube.com/embed/GnXHnEz94os?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>And here he is playing it live on piano toward the end of his life, with a lovely slow tempo.</p>
<p><iframe title="Glenn Gould-J.S. Bach-The Art of Fugue (HD)" width="640" height="480" src="https://www.youtube.com/embed/4uX-5HOx2Wc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>Bach published <em>The Art of Fugue</em> in “open score,” meaning that each voice of the counterpoint is on its own line, rather than being grouped together in the usual two-staff notation that we’re used to. Here’s an excerpt of <a href="https://www.youtube.com/watch?v=zQXPoJjfz0I">Contrapunctus VII</a> in open score in Bach’s own handwriting, with some informational color-coding added by Guido Magnano:</p>
<p><a href="https://commons.wikimedia.org/wiki/File:ContrapunctusVII.jpg"><img data-recalc-dims="1" loading="lazy" decoding="async" data-attachment-id="22882" data-permalink="https://www.ethanhein.com/wp/2021/the-art-of-fugue-contrapunctus-i/contrapunctusvii/" data-orig-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=1719%2C1720&amp;ssl=1" data-orig-size="1719,1720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Bach – Art of Fugue – Contrapunctus VII – color-coded open score" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?fit=640%2C640&amp;ssl=1" src="https://i0.wp.com/ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII-1024x1024.jpeg?resize=640%2C640&amp;ssl=1" alt="" width="640" height="640" srcset="https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?resize=1536%2C1536&amp;ssl=1 1536w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?w=1719&amp;ssl=1 1719w, https://i0.wp.com/www.ethanhein.com/wp/wp-content/uploads/2021/04/ContrapunctusVII.jpeg?w=1280&amp;ssl=1 1280w" sizes="auto, (max-width: 640px) 100vw, 640px"></a></p>
<p>Open score was already considered an old-fashioned way to write keyboard music in Bach’s time, and people stopped using it entirely soon afterward. Since the 19th century Bach revival, musicians have taken the open score format as an invitation to play <em>The Art of Fugue</em> on four separate instruments. For example, there have been lots of string quartet recordings. Here’s a good one:</p>
<p><iframe loading="lazy" title="J.S. Bach: The Art Of Fugue, BWV 1080 - Version For String Quartet - Contrapunctus 1" width="640" height="480" src="https://www.youtube.com/embed/3A8iR7cGHHQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>Contrapunctus I also sounds cool on four viols (cousins of the viola and cello, but with frets like a guitar):</p>
<p><iframe loading="lazy" title="Bach-The Art of Fugue- Contrapunctus 1" width="640" height="480" src="https://www.youtube.com/embed/gU8Vu5YEo48?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>It sounds amazing on four saxophones:</p>
<p><iframe loading="lazy" title="J.S. Bach Contrapunctus 1 - Rascher Saxophone Quartet" width="640" height="360" src="https://www.youtube.com/embed/wEJUOUaGlBY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>And it has a nice organ-like quality on four recorders:</p>
<p><iframe loading="lazy" title="Woodpeckers Recorder Quartet - JS Bach - die Kunst der Fuge BWV 1080 - Contrapunctus I &amp; IX" width="640" height="360" src="https://www.youtube.com/embed/aLEL9WcbGLU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p><a href="https://en.wikipedia.org/wiki/Joseph_Kerman">Joseph Kerman</a> writes about Contrapunctus I in his book <em><a href="https://www.ucpress.edu/book/9780520287631/the-art-of-fugue">Art of Fugue</a></em>:</p>
<blockquote>
<p>[I]n order to set off the technical virtuosity that was the work’s raison d’être, Bach had the extraordinary idea of making its first number a fugue without contrapuntal devices. Contrapunctus 1 has neither strettos, diminutions, and so on, nor even <a href="https://en.wikipedia.org/wiki/Subject_(music)#Countersubject">countersubjects</a> or recurring <a href="https://en.wikipedia.org/wiki/Fugue#Episode">episodes</a>. These devices will be introduced only in the succeeding contrapuncti, one by one. In Contrapunctus 1 <a href="https://www.teoria.com/en/reference/i/invertible-counterpoint.php">invertible counterpoint</a> itself is in very short supply. This elemental fugue never modulates beyond the obligatory dominant and subdominant keys.</p>
<p>In any case, this most basic of fugues is necessarily also one of Bach’s freest and must also be one of his smoothest… The contrapuntal lines, consisting mostly of quarter- and eighth-note patterns, move stepwise or by the smallest leaps, and the expectations of eighteenth-century harmony often go unfulfilled. Strong cadences are shunned. While such generalities only begin to explain the almost mesmeric fluency of Bach’s late style, they may help sensitize us to contrasts where it is abrogated, such as at those episodes featuring larger leaps [bars 29–30, 36–40, 49–53], and at the one really, decisively strong cadence [bar 74].</p>
<p>Eventually the surface does begin to ruffle, when in a new exposition the bass steps in on the heels of its predecessor and enters after three bars rather than four [bar 32]. This entry—it can be heard as a second stab at stretto, after a previous, premature effort in bars 29–30, what is sometimes called a false stretto—moves rather hastily from the dominant around to the subdominant, twisting and turning the subject oddly. Then the tenor entry, as though checked by the low As in the bass, hesitates, accumulating dissonances—sevenths, ninths, and pungent augmented intervals [bars 41, 42, 43].</p>
</blockquote>
<p>These intervals are a lot less “pungent” in <a href="https://www.ethanhein.com/wp/2019/why-cant-you-tune-your-guitar/">12-tone equal temperament</a> than they would have been in <a href="https://www.ethanhein.com/wp/2020/what-does-the-well-tempered-clavier-sound-like-in-actual-well-temperament/">the uneven temperament of Bach’s era</a>.</p>
<blockquote>
<p>The soprano in this group of entries emerges as a sort of ethereal climax, led into by another false stretto. The bass drops out, allowing for heightened activity in the remaining voices, like a beating of wings [bars 48–54].</p>
</blockquote>
<blockquote>
<p>Past the exposition, then, the piece can be seen to grow increasingly complex, though the feeling seems to me not exactly of complexity but of complexities tested out and drifted past, ideas considered and shelved, in a constantly changing improvisational field of a unique kind. Endlessly fertile and quite unstoppable, Bach proceeds spontaneously, almost distractedly, until the piece pulls itself together with one grand gesture, the long dominant pedal in the bass from bar 63 to bar 73.</p>
<p>Literally, of course, the pitch A drops out at bar 66, but in the ear it lasts all the way, so the passage has the effect of a cadenza, an increasingly rhapsodic epilogue during which pitch rises and tension mounts until it is too much to bear—or so we must infer; the buildup is so smooth we had no inkling of impending crisis. This programmatically seamless music literally breaks off, stammers, and finally sinks—truly sinks—to rest.</p>
</blockquote>
<p>Bach doesn’t sound much like jazz, but Kerman identifies qualities in Contrapunctus I that are the things I like about jazz: the not-so-rigid development of themes and interplay of voices, the “complexities tested out and drifted past.” The later fugues are full of complexities that are tested all the way out and then some. There are even a couple of palindrome-like <a href="https://en.wikipedia.org/wiki/Mirror_fugue" target="_blank" rel="noopener">mirror fugues</a>. These are fascinating in that <em><a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach" target="_blank" rel="noopener">Gödel, Escher, Bach</a></em> way, but also exhausting. Sometimes you want to just listen to music without doing a whole Rubik’s cube worth of combinatorial math.</p>
<p>My attention span for this music improves when I hear it quantized over a beat. Here’s Angela Hewitt’s recording over the beat from “<a href="https://www.youtube.com/watch?v=QsZlY0Vz4-o">Empire State of Mind</a>” by Jay-Z and Alicia Keys, inspired by an arrangement by <a href="https://www.notesbyheather.com/">Heather Fortune</a>.</p>

<p>In spite of the jokey title, this remix is not meant to be ironic. (Well, not totally ironic.) The beat helps me stay focused and present, rather than having my mind drift into a, you know, fugue state. That’s what beats are for. This music is supposed to be didactic, right? I learn best when I’m learning to a groove. But I also just like the aesthetic effect, and the suggestion that anything has groove potential.</p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How a hawk learned to use traffic signals to hunt more successfully (382 pts)]]></title>
            <link>https://www.frontiersin.org/news/2025/05/23/street-smarts-hawk-use-traffic-signals-hunting</link>
            <guid>44105965</guid>
            <pubDate>Tue, 27 May 2025 11:46:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.frontiersin.org/news/2025/05/23/street-smarts-hawk-use-traffic-signals-hunting">https://www.frontiersin.org/news/2025/05/23/street-smarts-hawk-use-traffic-signals-hunting</a>, See on <a href="https://news.ycombinator.com/item?id=44105965">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="tts-content"><!--[--><!----><!--]--><figure><picture><!--[--><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=380&amp;fm=webp&amp;q=80" media="(max-width: 767px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=642&amp;fm=webp&amp;q=80" media="(min-width: 768px and max-width: 1023px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=824&amp;fm=webp&amp;q=80" media="(min-width: 1024px and max-width: 1279px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=644&amp;fm=webp&amp;q=80" media="(min-width: 1280px and max-width: 1439px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=672&amp;fm=webp&amp;q=80" media="(min-width: 1440px and max-width: 1919px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=912&amp;fm=webp&amp;q=80" media="(min-width: 1920px)"><!--]--><img src="https://images.ctfassets.net/mrbo2ykgx5lt/2BngtKIXLF0Ua93HkvlZo0/100e0cb34aa7a8d3085eb572d82ea38b/Screenshot_2025-05-08_092315.png?&amp;w=912&amp;fm=webp&amp;q=80" alt="" loading="eager"></picture><figcaption>Adult Cooper’s hawk dispatching a house sparrow. Image: Vladimir Dinets. </figcaption></figure><!--[--><p><strong>Dr Vladimir Dinets, a research assistant professor at the University of Tennessee, is a zoologist who studies animal behavior, ecology, and conservation. As of 2025, he also teaches mathematics at Rudgers University. He is the author of a recently published </strong><a href="https://www.frontiersin.org/journals/ethology/articles/10.3389/fetho.2025.1539103/abstract"><strong><em>Frontiers in Ethology</em></strong><strong> article</strong></a><strong> that documents the impressive adaptation of an avian newcomer to the city. A Cooper’s hawk, a medium-sized raptor native to North America, appears to have learned how to adapt its hunting strategy and strike at a flock of birds precisely when cars at an intersection lined up after traffic lights switched to red, having been alerted by a sound signal that the red phase would last longer than usual. In the following guest editorial, he describes his observations. </strong></p><p>by <a href="https://loop.frontiersin.org/people/2127032/overview">Dr Vladimir Dinets</a></p><p>Many years ago, I got to spend some time in Ngorongoro Crater, a unique place in Africa where immense herds of animals are being watched by equally immense crowds of 4x4-riding tourists, and traffic jams of all kinds are frequent. On my last evening there, a local guide told me at a campfire that some buffalo in the crater had figured out the meaning of car turn signals and used that understanding to get out of the way of turning Jeeps and Land Rovers.</p><p>I never had a chance to return to the crater and still don’t know if that story was true, but it got me interested in animals’ perception of – and interactions with – human-made vehicles. Of course, the most common interaction is the animal becoming a roadkill, but it’s not the whole story. Many animals have learned to use cars for their own benefit, and birds seem to be particularly good at it. Crows drop walnuts, clams, even small vertebrates onto busy roads to have them killed and/or crushed by cars. Carrion-eating birds routinely monitor or patrol busy roads to immediately snatch roadkill. For example, many American highways are partitioned by families of ravens who watch them from dawn till dusk, waiting for meals from under wheels. Songbirds glean dead insects from cars and even nest in moving cars, trains and boats. Small birds use moving cars as mobile shelters from pursuing hawks, while hawks in one Ukrainian city have long been known to use moving cars and streetcars as cover to sneak up on their prey.</p><h2>Hunt at the crosswalk</h2><p>So I’ve been keeping an eye for unusual bird-car play, and that’s why I noticed something interesting going on at a street intersection near my home. The intersection wasn’t particularly busy, and even during morning rush hour, when I was driving my daughter to school, there were usually only a few cars waiting for the green light. But sometimes a pedestrian pressed a button, and that caused the red light to last a lot longer, so the car queue became longer, too, stretching all the way to a small streetside tree with a particularly dense crown. When that happened, the streetlight produced a sound signal, letting blind people know that it was safe to cross.</p><figure><picture><!--[--><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=380&amp;fm=webp&amp;q=80" media="(max-width: 767px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=642&amp;fm=webp&amp;q=80" media="(min-width: 768px and max-width: 1023px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=824&amp;fm=webp&amp;q=80" media="(min-width: 1024px and max-width: 1279px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=644&amp;fm=webp&amp;q=80" media="(min-width: 1280px and max-width: 1439px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=672&amp;fm=webp&amp;q=80" media="(min-width: 1440px and max-width: 1919px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=912&amp;fm=webp&amp;q=80" media="(min-width: 1920px)"><!--]--><img src="https://images.ctfassets.net/mrbo2ykgx5lt/7EjYAFrSOJtfrqpHsTozaR/89d47415f405e29584ff549eaef532c1/1539103_Figure_1.JPEG?&amp;w=912&amp;fm=webp&amp;q=80" alt="" loading="lazy"></picture><figcaption>The study area. The route used by the hawk to attack a flock of birds feeding in front of house #2 is shown with white arrows. The hawk appeared in the tree in front of house #11 as soon as sound signals at the streetlight at the intersection (marked with white asterisks) indicated that red light will be longer than usual, and attacked when the queue of cars reached house #8, making it possible for the hawk to move to the tree in front of house #1 without being visible to potential prey. Credit: Dinets, 2025. </figcaption></figure><p>One winter morning I was in my car waiting for the light to change and suddenly saw a Cooper’s hawk: it emerged from that small tree, flew very low above the sidewalk along the line of cars, made a sharp turn, crossed the street between the cars, and dove onto something near one of the houses.</p><p>A few days later I saw the same thing happen again and decided to investigate. It turned out that the house targeted by the hawk’s attacks was inhabited by a nice big family that liked to eat dinner in the front yard. Next morning their breadcrumbs and other leftovers attracted a small flock of birds – sparrows, doves, and sometimes starlings. That’s what the hawk was after.</p><p>But what was really interesting, and took me much longer to figure out, was that the hawk always attacked when the car queue was long enough to provide cover all the way to the small tree, and that only happened after someone had pressed the pedestrian crossing button. As soon as the sound signal was activated, the raptor would fly from somewhere into the small tree, wait for the cars to line up, and then strike.</p><hr><p><a href="https://www.frontiersin.org/journals/ethology/articles/10.3389/fetho.2025.1539103/full">Download and read original article</a></p><hr><h2>Survival of the smartest?</h2><p>That meant that the hawk understood the connection between the sound and the eventual car queue length. The bird also had to have a good mental map of the place, because when the car queue reached its tree, the raptor could no longer see the place where its prey was and had to get there by memory.</p><p>It was an immature bird. Cooper’s hawks rarely nest in cities in our area but are common winter visitors. So the bird I was watching was almost certainly a migrant, having moved to the city just a few weeks earlier. And it had already figured out how to use traffic signals and patterns. To me it seemed very impressive.</p><figure><picture><!--[--><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=380&amp;fm=webp&amp;q=80" media="(max-width: 767px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=642&amp;fm=webp&amp;q=80" media="(min-width: 768px and max-width: 1023px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=824&amp;fm=webp&amp;q=80" media="(min-width: 1024px and max-width: 1279px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=644&amp;fm=webp&amp;q=80" media="(min-width: 1280px and max-width: 1439px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=672&amp;fm=webp&amp;q=80" media="(min-width: 1440px and max-width: 1919px)"><source srcset="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=912&amp;fm=webp&amp;q=80" media="(min-width: 1920px)"><!--]--><img src="https://images.ctfassets.net/mrbo2ykgx5lt/1jzyGvKDMUSCAXO50XsrCt/e48186aea92b2120b4ff6fbb7952dfbe/Fig1.jpg?&amp;w=912&amp;fm=webp&amp;q=80" alt="" loading="lazy"></picture><figcaption>Immature Cooper’s hawk in an ambush. Image: Vladimir Dinets.</figcaption></figure><p>Next winter I saw a hawk in adult plumage hunt in exactly the same way, and I’m pretty sure it was the same bird. The following summer, the sound signal at the streetlight stopped working, and the residents of the house moved out, so there were no more bird flocks. I haven’t seen any Cooper’s hawks around here ever since.</p><p>Cooper’s hawk is on a rather short list of bird of prey species that have successfully adapted to life in cities. A city is a difficult and very dangerous habitat for any bird, but particularly for a large raptor specializing in live prey: you have to avoid windows, cars, utility wires, and countless other dangers while catching something to eat every day. I think my observations show that Cooper’s hawks manage to survive and thrive there, at least in part, by being very smart.</p><p><strong>REPUBLISHING GUIDELINES</strong>: Open access and sharing research is part of <a href="https://www.frontiersin.org/about/about-frontiers">Frontiers’ mission</a>. Unless otherwise noted, you can republish articles posted in the Frontiers news site — as long as you include a link back to the original research. Selling the articles is not allowed. </p><!--]--><!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just make it scale: An Aurora DSQL story (119 pts)]]></title>
            <link>https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html</link>
            <guid>44105878</guid>
            <pubDate>Tue, 27 May 2025 11:31:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html">https://www.allthingsdistributed.com/2025/05/just-make-it-scale-an-aurora-dsql-story.html</a>, See on <a href="https://news.ycombinator.com/item?id=44105878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><header></header><hr><section><p><time itemprop="datePublished" datetime="2025-05-27">May 27, 2025</time> • 3404 words</p><span itemprop="articleBody"><p><img src="https://www.allthingsdistributed.com/images/aurora-dsql-header.png" alt="Aurora DSQL Team" loading="lazy"></p><p>At re:Invent we announced Aurora DSQL, and since then I’ve had many conversations with builders about what this means for database engineering. What’s particularly interesting isn’t just the technology itself, but the journey that got us here. I’ve been wanting to dive deeper into this story, to share not just the what, but the how and why behind DSQL’s development. Then, a few weeks ago, at our internal developer conference — DevCon — I watched a talk from two of our senior principal engineers (PEs) on building DSQL (a project that started 100% in JVM and finished 100% Rust). After the presentation, I asked <a href="https://www.linkedin.com/in/nicholas-matsakis-615614/">Niko Matsakis</a> and <a href="https://www.linkedin.com/in/marc-bowes-952b5518/">Marc Bowes</a> if they’d be willing to work with me to turn their insights into a deeper exploration of DSQL’s development. They not only agreed, but offered to help explain some of the more technically complex parts of the story.</p><p>In the blog that follows, Niko and Marc provide deep technical insights on Rust and how we’ve used it to build DSQL. It’s an interesting story on the pursuit of engineering efficiency and why it’s so important to question past decisions – even if they’ve worked very well in the past.</p><div><p><strong>Note from the author</strong></p><div><p>Before we get into it, a quick but important note. This was (and continues to be) an ambitious project that requires a tremendous amount of expertise in everything from storage to control plane engineering. Throughout this write-up we've incorporated the learnings and wisdom of many of the Principal and Sr. Principal Engineers that brought DSQL to life. I hope you enjoy reading this as much as I have.</p><p>Special thanks to: Marc Brooker, Marc Bowes, Niko Matsakis, James Morle, Mike Hershey, Zak van der Merwe, Gourav Roy, Matthys Strydom.</p></div></div><h2 id="a-brief-timeline-of-purpose-built-databases-at-aws">A brief timeline of purpose-built databases at AWS <a href="#a-brief-timeline-of-purpose-built-databases-at-aws"></a></h2><p>Since the early days of AWS, the needs of our customers have grown more varied — and in many cases, more urgent. What started with a push to make traditional relational databases easier to manage with the launch of Amazon RDS in 2009 quickly expanded into a portfolio of purpose-built options: DynamoDB for internet-scale NoSQL workloads, Redshift for fast analytical queries over massive datasets, Aurora for those looking to escape the cost and complexity of legacy commercial engines without sacrificing performance. These weren’t just incremental steps—they were answers to real constraints our customers were hitting in production. And time after time, what unlocked the right solution wasn’t a flash of genius, but listening closely and building iteratively, often with the customer in the loop.</p><p>Of course, speed and scale aren’t the only forces at play. In-memory caching with ElastiCache emerged from developers needing to squeeze more from their relational databases. Neptune came later, as graph-based workloads and relationship-heavy applications pushed the limits of traditional database approaches. What’s remarkable looking back isn’t just how the portfolio grew, but how it grew in tandem with new computing patterns—serverless, edge, real-time analytics. Behind each launch was a team willing to experiment, challenge prior assumptions, and work in close collaboration with product teams across Amazon. That’s the part that’s harder to see from the outside: innovation almost never happens overnight. It almost always comes from taking incremental steps forward. Building on successes and learning from (but not fearing) failures.</p><p>While each database service we’ve launched has solved critical problems for our customers, we kept encountering a persistent challenge: how do you build a relational database that requires no infrastructure management and which scales automatically with load? One that combines the familiarity and power of SQL with genuine serverless scalability, seamless multi-region deployment, and zero operational overhead? Our previous attempts had each moved us closer to this goal. Aurora brought cloud-optimized storage and simplified operations, Aurora Serverless automated vertical scaling, but we knew we needed to go further. This wasn’t just about adding features or improving performance - it was about fundamentally rethinking what a cloud database could be.</p><p>Which brings us to Aurora DSQL.</p><h2 id="aurora-dsql">Aurora DSQL <a href="#aurora-dsql"></a></h2><p>The goal with Aurora DSQL’s design is to break up the database into bite-sized chunks with clear interfaces and explicit contracts. Each component follows the Unix mantra—do one thing, and do it well—but working together they are able to offer all the features users expect from a database (transactions, durability, queries, isolation, consistency, recovery, concurrency, performance, logging, and so on).</p><p>At a high-level, this is DSQL’s architecture.</p><img src="https://www.allthingsdistributed.com/images/aurora-dsql-architecture.png" alt="Aurora DSQL Architecture Diagram" width="80%"><p>We had already worked out how to handle reads in 2021—what we didn’t have was a good way to scale writes horizontally. The conventional solution for scaling out writes to a database is <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commit (2PC)</a>. Each journal would be responsible for a subset of the rows, just like storage. This all works great so long as transactions are only modifying nearby rows. But it gets really complicated when your transaction has to update rows across multiple journals. You end up in a complex dance of checks and locks, followed by an atomic commit. Sure, the happy path works fine in theory, but reality is messier. You have to account for timeouts, maintain liveness, handle rollbacks, and figure out what happens when your coordinator fails — the operational complexity compounds quickly. For DSQL, we felt we needed a new approach – a way to maintain availability and latency even under duress.</p><h2 id="scaling-the-journal-layer">Scaling the Journal layer <a href="#scaling-the-journal-layer"></a></h2><p>Instead of pre-assigning rows to specific journals, we made the architectural decision to write the entire commit into a single journal, no matter how many rows it modifies. This solved both the atomic and durable requirements of <a href="https://en.wikipedia.org/wiki/ACID">ACID</a>. The good news? This made scaling the write path straightforward. The challenge? It made the read path significantly more complex. If you want to know the latest value for a particular row, you now have to check all the journals, because any one of them might have a modification. Storage therefore needed to maintain connections to every journal because updates could come from anywhere. As we added more journals to increase transactions per second, we would inevitably hit network bandwidth limitations.</p><p>The solution was the Crossbar, which separates the scaling of the read path and write path. It offers a subscription API to storage, allowing storage nodes to subscribe to keys in a specific range. When transactions come through, the Crossbar routes the updates to the subscribed nodes. Conceptually, it’s quite simple, but challenging to implement efficiently. Each journal is ordered by transaction time, and the Crossbar has to follow each journal to create the total order.</p><p><img src="https://www.allthingsdistributed.com/images/aurora-dsql-crossbar.png" alt="Aurora DSQL Crossbar Diagram" loading="lazy"></p><p>Adding to the complexity, each layer has to provide a high degree of fan out (we want to be efficient with our hardware), but in the real world, subscribers can fall behind for any number of reasons, so you end up with a bunch of buffering requirements. These problems made us worried about garbage collection, especially GC pauses.</p><p>The reality of distributed systems hit us hard here - when you need to read from every journal to provide total ordering, the probability of any host encountering tail latency events approaches 1 surprisingly quickly – something <a href="https://brooker.co.za/blog/2021/04/19/latency.html">Marc Brooker has spent some time writing about</a>.</p><p>To validate our concerns, we ran simulation testing of the system – specifically modeling how our crossbar architecture would perform when scaling up the number of hosts, while accounting for occasional 1-second stalls. The results were sobering: with 40 hosts, instead of achieving the expected million TPS in the crossbar simulation, we were only hitting about 6,000 TPS. Even worse, our tail latency had exploded from an acceptable 1 second to a catastrophic 10 seconds. This wasn’t just an edge case - it was fundamental to our architecture. Every transaction had to read from multiple hosts, which meant that as we scaled up, the likelihood of encountering at least one GC pause during a transaction approached 100%. In other words, at scale, nearly every transaction would be affected by the worst-case latency of any single host in the system.</p><h2 id="short-term-pain-long-term-gain">Short term pain, long term gain <a href="#short-term-pain-long-term-gain"></a></h2><p>We found ourselves at a crossroads. The concerns about garbage collection, throughput, and stalls weren’t theoretical – they were very real problems we needed to solve. We had options: we could dive deep into JVM optimization and try to minimize garbage creation (a path many of our engineers knew well), we could consider C or C++ (and lose out on memory safety), or we could explore Rust. We chose Rust. The language offered us predictable performance without garbage collection overhead, memory safety without sacrificing control, and zero-cost abstractions that let us write high-level code that compiled down to efficient machine instructions.</p><p>The decision to switch programming languages isn’t something to take lightly. It’s often a <a href="https://www.youtube.com/watch?v=rxsdOQa_QkM">one-way door</a> — once you’ve got a significant codebase, it’s extremely difficult to change course. These decisions can make or break a project. Not only does it impact your immediate team, but it influences how teams collaborate, share best practices, and move between projects.</p><p>Rather than tackle the complex Crossbar implementation, we chose to start with the Adjudicator – a relatively simple component that sits in front of the journal and ensures only one transaction wins when there are conflicts. This was our team’s first foray into Rust, and we picked the Adjudicator for a few reasons: it was less complex than the Crossbar, we already had a Rust client for the journal, and we had an existing JVM (Kotlin) implementation to compare against. This is the kind of pragmatic choice that has served us well for over two decades – start small, learn fast, and adjust course based on data.</p><p>We assigned two engineers to the project. They had never written C, C++, or Rust before. And yes, there were plenty of battles with the compiler. The Rust community has a saying, “<a href="https://nostarch.com/blog/software-engineer-jon-gjengset-gets-nitty-gritty-rust">with Rust you have the hangover first</a>.” We certainly felt that pain. We got used to the compiler telling us “no” a lot.</p><figure><img src="https://www.allthingsdistributed.com/images/aurora-dsql-compiler-no.jpeg" alt="Compiler says “No” image" loading="lazy"><figcaption>(Image by Lee Baillie)</figcaption></figure><p>But after a few weeks, it compiled and the results surprised us. The code was 10x faster than our carefully tuned Kotlin implementation – despite no attempt to make it faster. To put this in perspective, we had spent years incrementally improving the Kotlin version from 2,000 to 3,000 transactions per second (TPS). The Rust version, written by Java developers who were new to the language, clocked 30,000 TPS.</p><p>This was one of those moments that fundamentally shifts your thinking. Suddenly, the couple of weeks spent learning Rust no longer looked like a big deal, when compared with how long it’d have taken us to get the same results on the JVM. We stopped asking, “Should we be using Rust?” and started asking “Where else could Rust help us solve our problems?”</p><p>Our conclusion was to rewrite our data plane entirely in Rust. We decided to keep the control plane in Kotlin. This seemed like the best of both worlds: high-level logic in a high-level, garbage collected language, do the latency sensitive parts in Rust. This logic didn’t turn out to be quite right, but we’ll get to that later in the story.</p><h2 id="its-easier-to-fix-one-hard-problem-then-never-write-a-memory-safety-bug">It’s easier to fix one hard problem then never write a memory safety bug <a href="#its-easier-to-fix-one-hard-problem-then-never-write-a-memory-safety-bug"></a></h2><p>Making the decision to use Rust for the data plane was just the beginning. We had decided, after quite a bit of internal discussion, to build on PostgreSQL (which we’ll just call Postgres from here on). The modularity and extensibility of Postgres allowed us to use it for query processing (i.e., the parser and planner), while replacing replication, concurrency control, durability, storage, the way transaction sessions are managed.</p><p>But now we had to figure out how to go about making changes to a project that started in 1986, with over a million lines of C code, thousands of contributors, and continuous active development. The easy path would have been to hard fork it, but that would have meant missing out on new features and performance improvements. We’d seen this movie before - forks that start with the best intentions but slowly drift into maintenance nightmares.</p><p>Extension points seemed like the obvious answer. Postgres was designed from the beginning to be an extensible database system. These extension points are part of Postgres’ public API, allowing you to modify behavior without changing core code. Our extension code could run in the same process as Postgres but live in separate files and packages, making it much easier to maintain as Postgres evolved. Rather than creating a hard fork that would drift further from upstream with each change, we could build on top of Postgres while still benefiting from its ongoing development and improvements.</p><p>The question was, do we write these extensions in C or Rust? Initially, the team felt C was a better choice. We already had to read and understand C to work with Postgres, and it would offer a lower impedance mismatch. As the work progressed though, we realized a critical flaw in this thinking. The Postgres C code is reliable: it’s been thoroughly battled tested over the years. But our extensions were freshly written, and every new line of C code was a chance to add some kind of memory safety bug, like a use-after-free or buffer overrun. The “a-ha!” moment came during a code review when we found several memory safety issues in a seemingly simple data structure implementation. With Rust, we could have just grabbed a proven, memory-safe implementation from Crates.io.</p><p>Interestingly, the <a href="https://security.googleblog.com/2024/09/eliminating-memory-safety-vulnerabilities-Android.html">Android team published research last September</a> that confirmed our thinking. Their data showed that the vast majority of new bugs come from new code. This reinforced our belief that to prevent memory safety issues, we needed to stop introducing memory-unsafe code altogether.</p><figure><img src="https://www.allthingsdistributed.com/images/aurora-dsql-google-mem-safe-vulns.png" alt="New Memory Unsafe Code and Memory safety Vulns" loading="lazy"><figcaption>(Research from the Android team shows that most new bugs come from new code. So if you pick a memory safe language – you prevent memory safety bugs.)</figcaption></figure><p>We decided to pivot and write the extensions in Rust. Given that the Rust code is interacting closely with Postgres APIs, it may seem like using Rust wouldn’t offer much of a memory safety advantage, but that turned out not to be true. The team was able to create abstractions that enforce safe patterns of memory access. For example, in C code it’s common to have two fields that need to be used together safely, like a <code>char*</code> and a <code>len</code> field. You end up relying on conventions or comments to explain the relationship between these fields and warn programmers not to access the string beyond len. In Rust, this is wrapped up behind a single String type that encapsulates the safety. We found many examples in the Postgres codebase where header files had to explain how to use a struct safely. With our Rust abstractions, we could encode those rules into the type system, making it impossible to break the invariants. Writing these abstractions had to be done very carefully, but the rest of the code could use them to avoid errors.</p><p>It’s a reminder that decisions about scalability, security, and resilience should be prioritized – even when they’re difficult. The investment in learning a new language is minuscule compared to the long-term cost of addressing memory safety vulnerabilities.</p><h2 id="about-the-control-plane">About the control plane <a href="#about-the-control-plane"></a></h2><p>Writing the control plane in Kotlin seemed like the obvious choice when we started. After all, services like Amazon’s Aurora and RDS had proven that JVM languages were a solid choice for control planes. The benefits we saw with Rust in the data plane – throughput, latency, memory safety – weren’t as critical here. We also needed internal libraries that weren’t yet available in Rust, and we had engineers that were already productive in Kotlin. It was a practical decision based on what we knew at the time. It also turned out to be the wrong one.</p><p>At first, things went well. We had both the data and control planes working as expected in isolation. However, once we started integrating them together, we started hitting problems. DSQL’s control plane does a lot more than CRUD operations, it’s the brain behind our hands-free operations and scaling, detecting when clusters get hot and orchestrating topology changes. To make all this work, the control plane has to share some amount of logic with the data plane. Best practice would be to create a shared library to avoid “<a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">repeating ourselves</a>”. But we couldn’t do that, because we were using different languages, which meant that sometimes the Kotlin and Rust versions of the code were slightly different. We also couldn’t share testing platforms, which meant the team had to rely on documentation and whiteboard sessions to stay aligned. And every misunderstanding, even a small one, led to a costly debug-fix-deploy cycles. We had a hard decision to make. Do we spend the time rewriting our <a href="https://brooker.co.za/blog/2022/04/11/simulation.html">simulation tools</a> to work with both Rust and Kotlin? Or do we rewrite the control plane in Rust?</p><p>The decision wasn’t as difficult this time around. A lot had changed in a year. Rust’s 2021 edition had addressed many of the pain points and paper cuts we’d encountered early on. Our internal library support had expanded considerably – in some cases, such as the AWS Authentication Runtime client, the Rust implementations were outperforming their Java counterparts. We’d also moved many integration concerns to API Gateway and Lambda, simplifying our architecture.</p><p>But perhaps most surprising was the team’s response. Rather than resistance to Rust, we saw enthusiasm. Our Kotlin developers weren’t asking “do we have to?” They were asking “when can we start?” They’d watched their colleagues working with Rust and wanted to be part of it.</p><p>A lot of this enthusiasm came from how we approached learning and development. Marc Brooker had written what we now call “The DSQL Book” – an internal guide that walks developers through everything from philosophy to design decisions, including the hard choices we had to defer. The team dedicated time each week to learning sessions on distributed computing, paper reviews, and deep architectural discussions. We brought in Rust experts like Niko who, true to our working backwards approach, helped us think through thorny problems before we wrote a single line of code. These investments didn’t just build technical knowledge – they gave the team confidence that they could tackle complex problems in a new language.</p><p>When we took everything into account, the choice was clear. It was Rust. We needed the control and data planes working together in simulation, and we couldn’t afford to maintain critical business logic in two different languages. We had observed significant throughput performance in the crossbar, and once we had the entire system written in Rust tail latencies were remarkably consistent. Our p99 latencies tracked very close to our p50 medians, meaning even our slowest operations maintained predictable, production-grade performance.</p><h2 id="its-so-much-more-than-just-writing-code">It’s so much more than just writing code <a href="#its-so-much-more-than-just-writing-code"></a></h2><p>Rust turned out to be a great fit for DSQL. It gave us the control we needed to avoid tail latency in the core parts of the system, the flexibility to integrate with a C codebase like Postgres, and the high-level productivity we needed to stand up our control plane. We even wound up using Rust (via WebAssembly) to power our internal ops web page.</p><p>We assumed Rust would be lower productivity than a language like Java, but that turned out to be an illusion. There was definitely a learning curve, but once the team was ramped up, they moved just as fast as they ever had.</p><p>This doesn’t mean that Rust is right for every project. Modern Java implementations like JDK21 offer great performance that is more than enough for many services. The key is to make these decisions the same way you make other architectural choices: based on your specific requirements, your team’s capabilities, and your operational environment. If you’re building a service where tail latency is critical, Rust might be the right choice. But if you’re the only team using Rust in an organization standardized on Java, you need to carefully weigh that isolation cost. What matters is empowering your teams to make these choices thoughtfully, and supporting them as they learn, take risks, and occasionally need to revisit past decisions. That’s how you build for the long term.</p><p>Now, go build!</p><h2 id="recommended-reading">Recommended reading <a href="#recommended-reading"></a></h2><p>If you’d like to learn more about DSQL and the thinking behind it, Marc Brooker has written an in-depth set of posts called DSQL Vignettes:</p><ul><li><a href="https://brooker.co.za/blog/2024/12/03/aurora-dsql.html">Aurora DSQL, and A Personal Story</a></li><li><a href="https://brooker.co.za/blog/2024/12/04/inside-dsql.html">Reads and Compute</a></li><li><a href="https://brooker.co.za/blog/2024/12/05/inside-dsql-writes.html">Transactions and Durability</a></li><li><a href="https://brooker.co.za/blog/2024/12/06/inside-dsql-cap.html">Wait! Isn’t That Impossible?</a></li></ul></span></section><hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BGP handling bug causes widespread internet routing instability (285 pts)]]></title>
            <link>https://blog.benjojo.co.uk/post/bgp-attr-40-junos-arista-session-reset-incident</link>
            <guid>44105796</guid>
            <pubDate>Tue, 27 May 2025 11:15:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.benjojo.co.uk/post/bgp-attr-40-junos-arista-session-reset-incident">https://blog.benjojo.co.uk/post/bgp-attr-40-junos-arista-session-reset-incident</a>, See on <a href="https://news.ycombinator.com/item?id=44105796">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h3>May 27 2025</h3>


<p><img src="https://blog.benjojo.co.uk/asset/g89a1FM2rr" alt=""></p>

<p>At 7AM (UTC) on Wednesday May 20th 2025 a BGP message was propagated that triggered surprising (to many) behaviours with two major BGP implementations that are often used for carrying internet traffic.</p>

<p>This caused a large number of “internet facing” BGP sessions to automatically shut down, causing at the very least some routing instability, and at worst brief loss of connectivity for some networks.</p>

<h2>What was the message?</h2>

<p><img src="https://blog.benjojo.co.uk/asset/EnGiUkEy31" alt=""></p>

<p>Using the sessions that people feed to bgp.tools, we can see here a version of the update that caused this behavior, it is a relatively unremarkable BGP Update for a /16, except it had a BGP Prefix-SID Attribute that was not only unwelcome (it is unexpected to see this on internet table BGP updates), but it was also corrupt with all of its internal data being 0x00.</p>

<p>Most implementations (IOS-XR/Nokia SR-OS) correctly filtered this out without causing any problems assuming their systems have been setup for <a href="https://datatracker.ietf.org/doc/html/rfc7606">RFC7606</a> (“BGP error tolerance”), however an interesting interaction with JunOS and Arista EOS caused JunOS to carry the corrupt message, and Arista EOS devices to reset sessions when receiving the message from (likely) a JunOS device.</p>

<p>Since a lot of internet transit carriers use Juniper hardware running JunOS, this meant that those running Arista EOS and connected to an upstream transit carrier router running JunOS would have had their access to the internet severed for a period (likely up to 10 mins).</p>

<h2>Who emitted the message?</h2>

<p><img src="https://blog.benjojo.co.uk/asset/7ajp55dCNF" alt=""></p>

<p>After filtering through the whole bgp.tools archive for that period, it would appear that a number of AS origins were involved with this incident. Suggesting that rather than the attribute having been added by the network that originated the prefix, it was added by a carrier in the middle on its way to the wider internet.</p>

<p>The 4 candidates that appear in all of the offending messages are:</p>

<ul>
<li><a href="https://bgp.tools/as/9304">AS9304</a> ( Hutchison Global Communications Limited )</li>
<li><a href="https://bgp.tools/as/135338">AS135338</a> ( Starcloud Information Limited )</li>
<li><a href="https://bgp.tools/as/151326">AS151326</a> ( DCConnect Communication Pte. Ltd. )</li>
<li><a href="https://bgp.tools/as/138077">AS138077</a> ( PT Abhinawa Sumberdaya Asia )</li>
</ul>

<p>However, bgp.tools has captured routes for the impacted prefixes <em>without the faulty BGP attribute</em> from “[…] 151326 138077 […]“, meaning the culprit that added the attribute was likely Starcloud (AS135338) or Hutchison (AS9304).</p>

<p>Some prefixes seen in updates carrying the attribute (despite very likely not being the ones that added the offending attribute) are</p>

<ul>
<li>156.230.0.0/16</li>
<li>138.113.116.0/24</li>
<li>163.171.102.0/24</li>
<li>163.171.103.0/24</li>
<li>163.171.104.0/24</li>
</ul>

<p>This incident was further amplified by Hutchison/AS9304 being on a large number of internet exchanges, meaning that the offending messages were sent to IX route servers that typically are running <a href="https://bird.network.cz/">bird</a>. Since Bird does not support BGP SID, the message was distributed to many multi-terabit internet exchanges without being filtered, spreading the chaos to more than just internet transit sessions.</p>

<h2>What is BGP Prefix-SID?</h2>

<p>BGP Prefix-SID Attribute should generally only be seen in internal BGP sessions, as the point of them (as defined in <a href="https://datatracker.ietf.org/doc/rfc8669/">RFC8669</a>) is to help define the route the traffic will take within a single network to get to the destination.</p>

<p>The reason that one of these attributes leaked out into the global routing table in the first place could have been caused by an external BGP session being configured as an internal one.</p>

<h2>Who was impacted?</h2>

<p>While it is hard to definitively claim who was impacted, after looking at networks with very large churn (compared to their size) immediately after the initial problematic BGP message was emitted, I count around 100 separate networks that hit issues, some high confidence examples include:</p>

<ul>
<li>SpaceX Starlink <a href="https://bgp.tools/as/14593">AS14593</a></li>
<li>Zscaler <a href="https://bgp.tools/as/62044">AS62044</a> / <a href="https://bgp.tools/as/53813">AS53813</a></li>
<li>Bytedance <a href="https://bgp.tools/as/396986">AS396986</a></li>
<li>Disney Worldwide Services <a href="https://bgp.tools/as/23344">AS23344</a></li>
<li>Nagasaki Cable Media Inc <a href="https://bgp.tools/as/10000">AS10000</a></li>
<li>Global Secure Layer <a href="https://bgp.tools/as/7578">AS7578</a></li>
<li>UpCloud <a href="https://bgp.tools/as/202053">AS202053</a></li>
<li>Netskope <a href="https://bgp.tools/as/55256">AS55256</a></li>
<li>Teleguam Holdings <a href="https://bgp.tools/as/9246">AS9246</a></li>
</ul>

<p>In “normal” times the bgp.tools’s route collector ingests around 20,000 to 30,000 messages per second, during this incident the average 10 second message rate was well over 150,000 /s. Indicating significant disruption to many internet paths.</p>

<h2>Vendors need to get their act together</h2>

<p>While the root cause (or even perpetrator) is not entirely clear, the fact that it propagated over the internet at scale is a demonstration of the situation/risk that I described in my previous post <a href="https://blog.benjojo.co.uk/post/bgp-path-attributes-grave-error-handling">“Grave flaws in BGP Error handling” - August 2023</a>.</p>

<p>In this case while other vendors detected the faulty attribute and suppressed the announcement, Juniper allowed it to propagate to peers, until it ultimately hit Arista devices that did not (or contained faulty) have BGP error tolerance code.</p>

<p>Junipers <a href="https://www.juniper.net/documentation/us/en/software/junos/bgp/topics/topic-map/bgp-error-messages.html">own documentation for JunOS’s BGP error tolerance</a> points out that it does not look at all parts of the message, despite it potentially being able to understand that it is faulty.</p>

<p>This is a curious decision, in which JunOS will save itself from a remote induced session reset, but then forward such messages to other peers (or in business words, likely towards your customers).</p>

<h2>Conclusions</h2>

<p>I have no happy ending for this. While the outage was short, the impact could have been worse. These kinds of incidents/bugs keep me up at night. As more and more services move to be IP based the scope of internet outages is no longer “consumers cannot get to their email”, but it starts to become “TV broadcasts fail” and “emergency service calls no longer work”. These begin to increase the chance of real world human casualties triggered (or at least exacerbated) by bugs such as this.</p>

<p>Filtering through the updates and piecing together this incident was a lot of fun, if you run a network yourself with a full routing table, and you are not part of the already <a href="https://bgp.tools/features">2570 running sessions that give bgp.tools data</a>, you can help the debugging of these future incidents by <a href="https://bgp.tools/kb/setup-sessions">setting up such data feeds</a>!</p>

<hr>

<p>If you want to stay up to date with the blog you can use the <a href="https://blog.benjojo.co.uk/rss.xml">RSS feed</a> or you can follow me on Fediverse <a href="https://benjojo.co.uk/u/benjojo">@benjojo@benjojo.co.uk</a></p>

<p>Until next time!</p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LumoSQL (228 pts)]]></title>
            <link>https://lumosql.org/src/lumosql/doc/trunk/README.md</link>
            <guid>44105619</guid>
            <pubDate>Tue, 27 May 2025 10:39:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lumosql.org/src/lumosql/doc/trunk/README.md">https://lumosql.org/src/lumosql/doc/trunk/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=44105619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<!-- Copyright 2020 The LumoSQL Authors, see LICENSES/MIT -->

<!-- SPDX-License-Identifier: MIT -->

<!-- SPDX-FileCopyrightText: 2020 The LumoSQL Authors -->

<!-- SPDX-ArtifactOfProjectName: LumoSQL -->

<!-- SPDX-FileType: Documentation -->

<!-- SPDX-FileComment: Original by Dan Shearer, December 2019 -->

<!-- toc -->


<p><a href="https://lumosql.org/src/lumosql/doc/trunk/lumosql.org">LumoSQL</a> is a modification (<a href="https://lumosql.org/src/not-forking/">not a fork</a>) of the
<a href="https://sqlite.org/">SQLite</a> embedded data storage library, which is among the <a href="https://sqlite.org/mostdeployed.html">most-deployed software</a>. We are currently in <a href="https://lumosql.org/src/lumosql/doc/trunk/doc/LumoSQL-PhaseII-Announce.md">Phase II of the project</a>.</p>

<p>If you are reading this on GitHub you are looking at a read-only mirror.
The master is always available at <a href="https://lumosql.org/src/lumosql">lumosql.org</a>.
LumoSQL adds security, privacy, performance and measurement features to SQLite.</p>

<h2>Benchmarking</h2>
<p>SQLite can test and compare results consistently across many kinds of system and configurations using the <a href="https://lumosql.org/src/not-forking">Not-forking tool</a>. Example:</p>

<p><img src="https://lumosql.org/src/lumosql/doc/trunk/new-doc/images/LumoSQLBenchmarkExample.png" alt="Example statistical result from LumoSQL benchmarking"></p>

<h2>Pluggable backends</h2>
<p>LumoSQL can swap back end key-value store engines in and out of SQLite. LMDB is
the most famous (but not the only) example of an alternative key-value store,
and LumoSQL can combine dozes of versions of LMDB and SQLite source code like
this:</p>

<p><img src="https://lumosql.org/src/lumosql/doc/trunk/new-doc/images/LumoSQL-Notforking-diagram.png" alt="Diagram of Not-forking used to create two example binaries"></p>

<p>In LumoSQL 0.4 there are three LumoSQL backends:</p>

<ul>
<li>the default SQLite Btree storage system</li>
<li><a href="https://github.com/LMDB/lmdb">LMDB</a></li>
<li><a href="https://en.wikipedia.org/wiki/Berkeley_DB">the ancient Berkley Database</a></li>
</ul>

<p>We are looking at some interesting new development in key-value storage to add and benchmark.</p>

<h2>Encryption and corruption detection, optionally per-row</h2>
<p>LumoSQL adds modern encryption to SQLite, including <a href="https://en.wikipedia.org/wiki/Attribute-based_encryption">Attribute-Based Encryption
(ABE)</a>. This can be
done on a per-row basis, and also includes per-row checksums so that any error
can be noticed quickly and located down to the individual row. Per-row
checksums also make some search and comparison operations much faster.</p>

<h2>Organised and Supported</h2>
<p>LumoSQL is distributed under <a href="https://lumosql.org/src/lumosql/doc/trunk/LICENCES/README.md">very liberal MIT licence terms</a>.</p>

<p>LumoSQL is supported by the <a href="https://nlnet.nl/">NLNet Foundation</a>.</p>

<p>LumoSQL runs on x86, ARM-32 and RISC-V architectures, and many Linux and BSD OSs.</p>

<h2>Table of Contents</h2>
<ul>
<li><a href="#design-not-forking-and-participating">Design, Not-Forking and Participating</a></li>
<li><a href="#lumosql-and-sqlites-billions-of-users">LumoSQL, and SQLite's Billions of Users</a></li>
<li><a href="#limitations-of-lumosql">Limitations of LumoSQL</a></li>
<li><a href="#build-environment-and-dependencies">Build Environment and Dependencies</a></li>
<li><a href="#using-the-build-and-benchmark-system">Using the Build and Benchmark System</a></li>
<li><a href="#a-brief-history-of-lumosql">A Brief History of LumoSQL</a></li>
</ul>



<h2>Design, Not-Forking and Participating</h2>
<p>If you are reading this on Github, then you are looking at a mirror. LumoSQL is
is maintained using <a href="https://lumosql.org/src/lumosql/">the Fossil repository</a>. If you 
want to participate in LumoSQL there is a forum, and if you have code contributions
you can ask for access to the respository.</p>

<p>LumoSQL has multiple upstreams, but does not fork any of them despite needing modifications.
The novel <a href="https://lumosql.org/src/not-forking">Not-forking</a> tool semi-automatically 
tracks upstream changes and is a requirement for building LumoSQL. Between not-forking 
and the <a href="https://lumosql.org/src/lumosql/doc/trunk/doc/lumo-build-benchmark.md">LumoSQL Build and Benchmark System</a>,
LumoSQL is as much about combining and configuring upstreams as it is about creating
original database software. By maintaining Not-forking outside LumoSQL, we hope
other projects will find it useful.</p>

<p>The LumoSQL and SQLite projects are cooperating, so any merge friction is
expected to become less over time, and key to that is the approach of not
forking.</p>



<h2>LumoSQL, and SQLite's Billions of Users</h2>
<p>LumoSQL exists to demonstrate changes to SQLite that might be useful, but which
SQLite probably cannot consider for many years because of SQLite's unique
position of being used by a majority of the world's population. </p>

<p>SQLite is used by thousands of software projects, just three being
Google's Android, Mozilla's Firefox and Apple's iOS which between them have
billions of users. That is a main reason why SQLite is so careful and conservative
with all changes.</p>

<p>On the other hand, many of these same users need SQLite to have new features
which do not fit with the SQLite project's cautious approach, and LumoSQL is a
demonstration of some of these improvements. </p>

<p>The LumoSQL documentation project reviews dozens of relevant codebases.  SQLite
has become ubiquitous over two decades, which means there is a great deal of
preparation needed when considering architectural changes.</p>



<h2>Limitations of LumoSQL</h2>
<p>As of LumoSQL 0.4, there are many obvious limitations, including:</p>

<ul>
<li>The tests used in benchmarking mostly come from an ancient version of SQLite's
speedtest.tcl modified many times, to which DATASIZE
and DEBUG have been added. Experts in SQLite and LMDB database testing 
should review the files in not-fork.d/sqlite3/benchmark/*test. There are 
<a href="https://sqlite.org/src/dir?ci=tip&amp;name=tool">9 tools named *speed*</a> 
in the SQLite source, and any/all of them should be added here.</li>
<li>Neither LMDB nor BDB backends ship with latest SQLite builds. Now all the LumoSQL infrastructure
exists, that is a smaller, more maintainable and repeatable task. But it is not done yet.
There are some generic problems to be solved in the process, such as the optimal way to
address keysize disparities between a KVP store provider and SQLite's internal large keysize.</li>
<li>If we import more of the speed tests from SQLite identified above, then we will 
have a problem with several LMDB and at least two BDB instances, where the SQLite
tests will fail. In most cases this is about the LMDB port needing to be more 
complete but in some it is about relevance, where some SQLite tests will not apply. In
addition some backends will always need
to have additional tests (for example, BDB has more extensive user management than 
SQLite).</li>
</ul>



<h2>Build Environment and Dependencies</h2>
<p>Most developers already have the required minimum of git and core unix-style
development tools. SQLite has very few dependencies (mostly Tcl), and
LumoSQL adds one Perl-based processing tool.</p>

<p>LumoSQL is mirrored to Github and application developers can use git
with Github in the usual way. LumoSQL developers working on the LumoSQL
library internals choose to use <a href="https://fossil-scm.org/">Fossil source code
manager</a> instead of git, and if you're planning
to develop LumoSQL internals then you need Fossil.</p>

<p>There are many <a href="https://www.fossil-scm.org/home/doc/trunk/www/fossil-v-git.wiki">reasons why people choose
Fossil</a>.
For LumoSQL one of them is that SQLite and Fossil are symbiotic
projects, each written in the other.</p>

<h4>Debian or Ubuntu-derived Operating Systems</h4>
<p>Uncomment existing <code>deb-src</code> line in /etc/apt/sources.list, for example
for Ubuntu 20.04.2 a valid line is:
<b>
</b></p><pre><b><code>deb-src http://gb.archive.ubuntu.com/ubuntu focal main restricted
</code></b></pre>

<p>Then run
<b>
</b></p><pre><b><code>sudo apt update                              # this fetches the deb-src updates
sudo apt full-upgrade                        # this gets the latest OS updates
sudo apt install git build-essential tclx
sudo apt build-dep sqlite3
</code></b></pre>

<p>The <em>exact</em> commands above have been tested on a pristine install of Ubuntu
20.04.2 LTS, as installed from ISO or one of the operating systems shipped with
Windows Services for Linux.</p>

<h4>Fedora-derived Operating Systems</h4>
<p>On any reasonably recent Fedora-derived Linux distribution, including Red Hat:</p>

<pre><b><code>sudo dnf install --assumeyes \
  git make gcc ncurses-devel readline-devel glibc-devel autoconf tcl-devel tclx-devel
</code></b></pre>

<h4>Common to all Linux Operating Systems</h4>
<p>Once you have done the setup specific to your operating system in the previous
steps, the following should work on reaonably recent Debian and Fedora-related
operating systems, and Gentoo. </p>

<p>Other required tools can be installed from your operating system's standard packages.
Here are the tool dependencies:</p>

<ul>
<li>Mandatory: <a href="https://lumosql.org/src/not-forking/">the not-forking tool</a>, 
which is a Perl script that needs to be downloaded and installed manually. The instructions for not-forking are on its website.</li>
<li>Recommended: <a href="https://fossil-scm.org/">Fossil</a>. As described above, you don't necessarily need Fossil. But Fossil is very easy to install: if you can't get version 2.13 or later from your distrbution then it is easy to build from source. 
(<em>Note!</em> Ubuntu 20.04, Debian Buster and Gentoo do not include a sufficiently modern Fossil, while NetBSD
and Ubuntu 20.10 do.) Since you now have a development environment anyway you can 
<a href="https://fossil-scm.org/home/doc/trunk/www/build.wiki">build Fossil trunk according to the official instructions</a> or this simpler version (tested on Ubuntu 20.04 LTS):

<ul>
<li>wget -O- https://fossil-scm.org/home/tarball/trunk/Fossil-trunk.tar.gz |  tar -zxf -</li>
<li>sudo apt install libssl-dev</li>
<li>cd Fossil-trunk ; ./configure ; make</li>
<li>sudo make install</li>
</ul></li>
<li>For completeness (although every modern Linux/Unix includes these), to build and benchmark any of the Oracle Berkeley DB targets, you need either "curl" or "wget", and also "file", "gzip" and GNU "tar". Just about any version of these will be sufficient, even on Windows.</li>
<li>If you are running inside a fresh <a href="https://docker.io/">Docker</a> or similar container system, Fossil may be confused about the user id. One solution is to add a user (eg "adduser lumosql" and answer the questions) and then "export USER=lumosql".</li>
</ul>

<p>The not-forking tool will advise you with a message
if you need a tool or a version that is not installed. </p>

<p>On <a href="https://www.debian.org/releases/buster/">Debian 10 "Buster" Stable Release</a>, the not-forking makefile
("perl Makefile.PL") will warn that git needs to be version 2.22 or higher.
Buster has version 2.20, however this is not a critical error. If you don't
like error messages scrolling past during a build, then install a more recent
git <a href="https://backports.debian.org/Instructions/">from Buster backports</a>.</p>



<h2>Quickstart: Using the Build and Benchmark System</h2>
<p>This is a very brief quickstart, for full detail see the
<a href="https://lumosql.org/src/lumosql/doc/trunk/doc/lumo-build-benchmark.md">Build and Benchmark System documentation</a>. </p>

<p>Now you have the dependencies installed, clone the LumoSQL repository using
<code>fossil clone https://lumosql.org/src/lumosql</code> , which will create a new subdirectory called <code>lumosql</code> and
a file called <code>lumosql.fossil</code> in the current directory.</p>

<p>Try:
<b>
</b></p><pre><b><code>cd lumosql
make what
</code></b></pre>

<p>To see what the default sources and options are. The <code>what</code> target does not make any changes although it may generate a file <code>Makefile.options</code> to help <code>make</code> parse the command line.</p>

<p>Benchmarking a single binary should take no longer than 4 minutes to complete depending
on hardware. The results are stored in an SQLite database stored in the LumoSQL 
top-level directory by default, that is, the directory you just created using <code>fossil clone</code>.</p>

<p>Start by building and benchmarking the official SQLite release version 3.35.5, which is the current
release at the time of writing this README.</p>

<p><b>
<code>make benchmark USE_LMDB=no USE_BDB=no SQLITE_VERSIONS='3.35.5'</code>
</b></p>

<p>All source files fetched are cached in ~/.cache/LumoSQL in a way that maximises reuse regardless of 
their origin (Fossil, git, wget etc) and which minimises errors. The LumoSQL build system is driving the
<code>not-fork</code> tool, which maintains the cache. Not-fork will download just the differences of a remote 
version if most of the code is already in cache.</p>

<p>The output from this make command will be lots of build messages followed by something like this:</p>

<pre><b><code>*** Running benchmark 3.35.5
    TITLE = sqlite 3.35.5
    SQLITE_ID = 1b256d97b553a9611efca188a3d995a2fff71275
    SQLITE_NAME = 3.35.5 2021-04-19 18:32:05 1b256d97b553a9611efca188a3d995a2fff712759044ba480f9a0c9e98faalt1
    DATASIZE = 1
    DEBUG = off
    LMDB_DEBUG = off
    LMDB_FIXED_ROWID = off
    LMDB_TRANSACTION = optimistic
    ROWSUM = off
    ROWSUM_ALGORITHM = sha3_256
    SQLITE3_JOURNAL = default
    RUN_ID = 70EA47101F68CDD6D3C0ED255962A2AA50F1540EE4FEBB46A03FAD888B49676C
          OK     0.003   1 Creating database and tables
          OK     0.019   2 1000 INSERTs
          OK     0.007   3 100 UPDATEs without an index, upgrading a read-only transaction
          OK     0.052   4 25000 INSERTs in a transaction
          OK     0.113   5 100 SELECTs without an index
          OK     0.243   6 100 SELECTs on a string comparison
          OK     0.012   7 Creating an index
          OK     0.046   8 5000 SELECTs with an index
          OK     0.036   9 1000 UPDATEs without an index
          OK     0.113  10 25000 UPDATEs with an index
          OK     0.093  11 25000 text UPDATEs with an index
          OK     0.032  12 INSERTs from a SELECT
          OK     0.020  13 DELETE without an index
          OK     0.028  14 DELETE with an index
          OK     0.027  15 A big INSERT after a big DELETE
          OK     0.010  16 A big DELETE followed by many small INSERTs
          OK     0.005  17 DROP TABLE
                 0.859 (total time)
</code></b></pre>

<p>A database with the default name of <code>benchmarks.sqlite</code> has been created with
two tables containing the results. This is one single test run, and the test
run data is kept in the table <code>test_data</code>. The table <code>run_data</code> contains data
relative to a set of runs (version numbers, time test started, etc). This is cumulative,
so another invocation of <code>make benchmark</code> will append to <code>benchmarks.sqlite</code>.</p>

<p>Every run is assigned a SHA3 hash, which helps in making results persistent over time and 
across the internet.</p>

<p>The tool <code>benchmark-filter.tcl</code> does some basic processing of these results:</p>

<pre><b><code>tool/benchmark-filter.tcl
RUN_ID                                                            TARGET  DATE        TIME         DURATION
70EA47101F68CDD6D3C0ED255962A2AA50F1540EE4FEBB46A03FAD888B49676C  3.35.5  2021-05-20  16:13:18        0.859
</code></b></pre>

<p>The option DATASIZE=<strong>parameter</strong> is a multiplication factor on the size of the chunks that is used for 
benchmarking. This is useful because it can affect the time it takes to run the tests by a very different
multiplication factor:</p>

<pre><b><code>make benchmark USE_LMDB=no USE_BDB=no DATASIZE=2 SQLITE_VERSIONS='3.35.5 3.33.0'
</code></b></pre>

<p>followed by:</p>

<pre><b><code>tool/benchmark-filter.tcl 
RUN_ID                                                            TARGET              DATE        TIME         DURATION
70EA47101F68CDD6D3C0ED255962A2AA50F1540EE4FEBB46A03FAD888B49676C  3.35.5              2021-05-20  16:13:18        0.859
65DD0759B133FF5DFBBD04C494F4631E013C64E475FC5AC06EC70F4E0333372F  3.35.5++datasize-2  2021-05-20  16:18:30        2.511
931B1489FC4477A41914A5E0AFDEF3927C306339FBB863B5FB4CF801C8F2F3D0  3.33.0++datasize-2  2021-05-20  16:18:51        2.572
</code></b></pre>

<p>Simplistically, these results suggest that SQLite version 3.35.5 is faster than
3.33.0 on larger data sizes, but that 3.35.5 is much faster with smaller data
sizes. After adding more versions and running the benchmarking tool again, we would
soon discover that SQLite 3.25.0 seems faster than 3.33.0, and other interesting things. 
Simplistic interpretations can be misleading :-)</p>

<p>This is a Quickstart, so for full detail you will need the 
<a href="https://lumosql.org/src/lumosql/doc/trunk/doc/lumo-build-benchmark.md">Build/Benchmark documentation</a>. However as a teaser, and since LMDB
was the original inspiration for LumoSQL (see the 
<a href="https://lumosql.org/src/lumosql/doc/trunk/(#a-brief-history-of-lumosql">History section below</a> for more on that) here are some more things that
can be done with the LMDB target:</p>

<pre><b><code>$ make what LMDB_VERSIONS=all
tclsh tool/build.tcl what not-fork.d MAKE_COMMAND='make' LMDB_VERSIONS='all'
BENCHMARK_RUNS=1
COPY_DATABASES=
COPY_SQL=
MAKE_COMMAND=make
NOTFORK_COMMAND=not-fork
NOTFORK_ONLINE=0
NOTFORK_UPDATE=0
SQLITE_VERSIONS=3.35.5
USE_SQLITE=yes
USE_BDB=yes
SQLITE_FOR_BDB=
BDB_VERSIONS=
BDB_STANDALONE=18.1.32=3.18.2
USE_LMDB=yes
SQLITE_FOR_LMDB=3.35.5
LMDB_VERSIONS=all
LMDB_STANDALONE=
OPTION_DATASIZE=1
OPTION_DEBUG=off
OPTION_LMDB_DEBUG=off
OPTION_LMDB_FIXED_ROWID=off
OPTION_LMDB_TRANSACTION=optimistic
OPTION_ROWSUM=off
OPTION_ROWSUM_ALGORITHM=sha3_256
OPTION_SQLITE3_JOURNAL=default
BUILDS=
    3.35.5
    3.18.2
    +bdb-18.1.32
    3.35.5+lmdb-0.9.11
    3.35.5+lmdb-0.9.12
    3.35.5+lmdb-0.9.13
    3.35.5+lmdb-0.9.14
    3.35.5+lmdb-0.9.15
    3.35.5+lmdb-0.9.16
    3.35.5+lmdb-0.9.17
    3.35.5+lmdb-0.9.18
    3.35.5+lmdb-0.9.19
    3.35.5+lmdb-0.9.20
    3.35.5+lmdb-0.9.21
    3.35.5+lmdb-0.9.22
    3.35.5+lmdb-0.9.23
    3.35.5+lmdb-0.9.24
    3.35.5+lmdb-0.9.25
    3.35.5+lmdb-0.9.26
    3.35.5+lmdb-0.9.27
    3.35.5+lmdb-0.9.28
    3.35.5+lmdb-0.9.29
TARGETS=
    3.35.5
    3.18.2
    +bdb-18.1.32
    3.35.5+lmdb-0.9.11
    3.35.5+lmdb-0.9.12
    3.35.5+lmdb-0.9.13
    3.35.5+lmdb-0.9.14
    3.35.5+lmdb-0.9.15
    3.35.5+lmdb-0.9.16
    3.35.5+lmdb-0.9.17
    3.35.5+lmdb-0.9.18
    3.35.5+lmdb-0.9.19
    3.35.5+lmdb-0.9.20
    3.35.5+lmdb-0.9.21
    3.35.5+lmdb-0.9.22
    3.35.5+lmdb-0.9.23
    3.35.5+lmdb-0.9.24
    3.35.5+lmdb-0.9.25
    3.35.5+lmdb-0.9.26
    3.35.5+lmdb-0.9.27
    3.35.5+lmdb-0.9.28
    3.35.5+lmdb-0.9.29
</code></b></pre>

<p>After executing this build with <code>make benchmark</code> rather than <code>make what</code>, here are summary results using a 
a new parameter to <code>benchmark-filter.tcl</code>:</p>

<pre><b><code>$ tool/benchmark-filter.tcl -fields TARGET,DURATION
TARGET                 DURATION
3.35.5                    0.852
3.35.5+lmdb-0.9.11        1.201
3.35.5+lmdb-0.9.12        1.211
3.35.5+lmdb-0.9.13        1.212
3.35.5+lmdb-0.9.14        1.219
3.35.5+lmdb-0.9.15        1.193
3.35.5+lmdb-0.9.16        1.191
3.35.5+lmdb-0.9.17        1.213
3.35.5+lmdb-0.9.18        1.217
3.35.5+lmdb-0.9.19        1.209
3.35.5+lmdb-0.9.20        1.223
3.35.5+lmdb-0.9.21        1.229
3.35.5+lmdb-0.9.22        1.230
3.35.5+lmdb-0.9.23        1.215
3.35.5+lmdb-0.9.24        1.218
3.35.5+lmdb-0.9.25        1.219
3.35.5+lmdb-0.9.26        1.220
3.35.5+lmdb-0.9.27        1.220
3.35.5+lmdb-0.9.28        1.209
3.35.5+lmdb-0.9.29        1.209
</code></b></pre>

<p>Again, simplistic interpretations are insufficient, but the data here suggests that LMDB has decreased
in performance over time, to improve again with the most recent versions, and no version of LMDB is faster than native SQLite 3.35.5 . However, further
benchmark runs indicate that is not the final story, as LMDB run on slower hard disks improve in relative 
speed rapidly. And using the <code>DATASIZE</code> option also changes the picture.</p>

<p>The results for the Berkely DB backend are also most interesting.</p>



<h2>A Brief History of LumoSQL</h2>
<p>There have been several implementations of new storage backends to SQLite, all of them hard forks
and nearly all dead forks. A backend needs certain characteristics:</p>

<ul>
<li>btree-based key-value store</li>
<li>transactions, or fully ACID</li>
<li>full concurrency support, or fully MVCC</li>
</ul>

<p>There are not many candidate key-value stores. One of the most widely-used is
Howard Chu's LMDB. There was a lot of attention in 2013 when Howard released
his <a href="https://github.com/LMDB/sqlightning">proof of concept SQLite port</a>. LMDB
operates on a very different and more modern principle to all other widely-used
key/value stores, potentially bringing benefits to some users of SQLite. In
2013, the ported SQLite gave significant performance benefits.</p>

<p>The original 2013 code modified the SQLite <code>btree.c</code> from version SQLite
version 3.7.17 to use LMDB 0.9.9 . It took considerable work for LumoSQL to
excavate the ancient code and reproduce the results.</p>

<p>By January 2020 the LumoSQL project concluded:</p>

<ul>
<li>Howard's 2013 performance work is reproducible</li>
<li>SQLite's key-value store improved in performance since 2013, getting close to
parity with LMDB by some measures</li>
<li>SQLite can be readily modified to have multiple storage backends and still
pass 'make test'</li>
<li>SQLite doesn't expect there to be multiple backends, and this has many effects
including for example in error handling. An abstraction layer was needed.</li>
</ul>

<p>Since then, many new possibilities have emerged for LumoSQL, and new collaborations.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Myth of Developer Obsolescence (327 pts)]]></title>
            <link>https://alonso.network/the-recurring-cycle-of-developer-replacement-hype/</link>
            <guid>44105592</guid>
            <pubDate>Tue, 27 May 2025 10:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alonso.network/the-recurring-cycle-of-developer-replacement-hype/">https://alonso.network/the-recurring-cycle-of-developer-replacement-hype/</a>, See on <a href="https://news.ycombinator.com/item?id=44105592">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2 id="from-nocode-to-ai-assisted">From NoCode to AI-Assisted</h2><p>Every few years, a shiny new technology emerges that promises to make software developers obsolete. The headlines follow a predictable pattern: "The End of Coding," "Anyone Can Build Apps Now," or my personal favorite, "Why Your Five-Year-Old Will Be Programming Before Learning to Read."</p><p>The executives get excited. The consultants circle like sharks. PowerPoint decks multiply. Budgets shift.</p><p>And then reality sets in.</p><p>What actually happens isn't replacement, it's transformation. Technologies that promised to eliminate the need for technical expertise end up creating entirely new specializations, often at higher salary points than before. The NoCode movement didn't eliminate developers; it created NoCode specialists and backend integrators. The cloud didn't eliminate system administrators; it transformed them into DevOps engineers at double the salary.</p><p>Now we're witnessing the same pattern with AI-assisted development. The promise that "AI will write all your code" is evolving into the reality that we need engineers who can effectively orchestrate AI systems, which is essentially the same engineers, but now with new skills and higher salary expectations.</p><p>But there's something deeper happening with this particular transformation. Unlike previous technological shifts that primarily changed how we implement solutions, AI-assisted development is highlighting a fundamental truth about software engineering that has always existed but is now impossible to ignore:</p><p><strong>The most valuable skill in software isn't writing code, it's architecting systems.</strong></p><p>And as we'll see, that's the one skill AI isn't close to replacing.</p><h2 id="the-endless-carousel-of-replacement-promises">The Endless Carousel of Replacement Promises</h2><p>How many times have we ridden this merry-go-round? Let's count the rotations:</p><h3 id="the-nocodelowcode-revolution">The NoCode/LowCode Revolution</h3><p>Remember when drag-and-drop interfaces were going to let business users build their own applications? The promise was clear: "Why hire expensive developers when anyone can build an app?"</p><p>What actually happened: These tools created a new class of problems. Someone still needed to design the data models underpinning those shiny interfaces, integrate with existing systems and databases, handle edge cases the visual tools couldn't address, and maintain and upgrade as requirements evolved.</p><p>The result wasn't fewer developers—it was the birth of "NoCode specialists" who understood both the business domain and the technical limitations of these platforms. And guess what? They commanded higher salaries than the developers they supposedly replaced.</p><h3 id="the-cloud-revolution">The Cloud Revolution</h3><p>"Move to the cloud and you won't need system administrators anymore!"</p><p>As if infrastructure would somehow manage itself once it was someone else's server. The cloud didn't eliminate the need for systems expertise. Instead, it transformed what that expertise looked like and dramatically expanded its scope.</p><p>The sysadmins weren't eliminated; they were reborn as DevOps engineers with fancy new job titles and substantially higher compensation packages. The work didn't disappear; it evolved into infrastructure-as-code, automated deployment pipelines, and distributed systems management.</p><p>As I noted in my <a href="https://www.linkedin.com/posts/danilo-alonso_softwarearchitecture-microservicesdebate-activity-7327489408041984001-xqL-?ref=alonso.network" rel="noreferrer">LinkedIn post about microservices</a>: "I've watched teams spend months decomposing perfectly functional systems into microservices only to discover they've traded one set of problems for a more expensive set." The cloud enabled this complexity and someone still needed to manage it. That someone was still a systems expert, just operating at a higher level of abstraction.</p><h3 id="the-offshore-development-wave">The Offshore Development Wave</h3><p>"Why pay local developers when you can get the same work done for a fraction of the cost overseas?"</p><p>The promise of dramatic cost savings quickly collided with the reality of communication challenges, quality issues, and the discovery that effective software development requires deep contextual knowledge and continuous collaboration.</p><p>What emerged instead was a more nuanced approach: distributed teams with clear ownership boundaries, stronger architecture practices, and—surprise—higher total costs than initially projected.</p><h3 id="the-ai-coding-assistant-revolution">The AI Coding Assistant Revolution</h3><p>And now we have AI promising to write our code for us. "Just describe what you want, and the AI will generate it!"</p><p>The early reality is already emerging. AI generates plausible-looking code that often contains subtle inconsistencies and errors. Senior engineers spend significant time verifying and correcting AI output. The "vibe coding" phenomenon means experienced developers extract far more value than novices. Systems built entirely with AI assistance often lack coherent architecture.</p><blockquote>"In the world of the chisel, you just gave carpenters a CNC machine. Guess who will make the better furniture?"</blockquote><p>The pattern is becoming clear once again: the technology doesn't replace the skill, it elevates it to a higher level of abstraction.</p><h2 id="why-this-time-is-different">Why This Time Is Different</h2><p>Here's what the "AI will replace developers" crowd fundamentally misunderstands: <strong>code is not an asset—it's a liability.</strong> Every line must be maintained, debugged, secured, and eventually replaced. The real asset is the business capability that code enables.</p><p>If AI makes writing code faster and cheaper, it's really making it easier to create liability. When you can generate liability at unprecedented speed, the ability to manage and minimize that liability strategically becomes exponentially more valuable.</p><p>This is particularly true because AI excels at local optimization but fails at global design. It can optimize individual functions but can't determine whether a service should exist in the first place, or how it should interact with the broader system. When implementation speed increases dramatically, architectural mistakes get baked in before you realize they're mistakes.</p><p>For agency work building disposable marketing sites, this doesn't matter. For systems that need to evolve over years, it's catastrophic.</p><p>The pattern of technological transformation remains consistent—sysadmins became DevOps engineers, backend developers became cloud architects—but AI accelerates everything. The skill that survives and thrives isn't writing code.</p><p>It's architecting systems. And that's the one thing AI can't do.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Revisiting the algorithm that changed horse race betting (2023) (120 pts)]]></title>
            <link>https://actamachina.com/posts/annotated-benter-paper</link>
            <guid>44105470</guid>
            <pubDate>Tue, 27 May 2025 10:03:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://actamachina.com/posts/annotated-benter-paper">https://actamachina.com/posts/annotated-benter-paper</a>, See on <a href="https://news.ycombinator.com/item?id=44105470">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <blockquote>
  <p>The remarkable story of Bill Benter and how he amassed a staggering $1B fortune betting on horses in Hong Kong has been extensively documented in the article, <a href="https://www.bloomberg.com/news/features/2018-05-03/the-gambler-who-cracked-the-horse-racing-code">The Gambler Who Cracked the Horse-Racing Code</a><sup id="fnref:chellel18" role="doc-noteref"><a href="#fn:chellel18" rel="footnote">1</a></sup>. In 1994, Benter published an academic paper titled <a href="https://www.gwern.net/docs/statistics/decision/1994-benter.pdf">Computer Based Horse Race Handicapping and Wagering Systems: A Report</a><sup id="fnref:benter08" role="doc-noteref"><a href="#fn:benter08" rel="footnote">2</a></sup>. In it, he documents the implementation of a successful horse race betting model, which by virtue of it being published, likely meant that the model was outdated and superceded by more sophisticated models. Despite this, the paper remains an insightful read, offering a rare glimpse into the lucrative application of mathematics to an unconventional field, made even more interesting by the substantial hardware and software limitations of the time.</p>

  <p>In this post, we present an annotated version of the paper with added code blocks and blockquoted comments. The main text is all from the paper itself. Instead of building the fundamental and combined models, we’ll be highlighting interesting aspects of the paper using the public estimate, derived from the <a href="https://racing.hkjc.com/racing/information/English/racing/LocalResults.aspx">Hong Kong Jockey Club’s historical win odds</a>. We’ll look at how the model calibration tables were generated in the original paper, assess how the public estimate has improved through the years and try our hand at fitting the adjustments factors from scratch using PyTorch. While the original paper uses data samples between 1986–1993, we’ll also be using data from the subsequent three decades (1996–2003, 2006–2013 and 2016–2023) for comparison.</p>
</blockquote>

<div><pre><code><span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>

<span>DATA_FILE</span> <span>=</span> <span>"</span><span>../data/HKJC/df_hkjc.csv</span><span>"</span>

<span>DATE_RANGES</span> <span>=</span> <span>[</span>
    <span>(</span><span>"</span><span>1986-08-01</span><span>"</span><span>,</span> <span>"</span><span>1993-08-01</span><span>"</span><span>),</span>
    <span>(</span><span>"</span><span>1996-08-01</span><span>"</span><span>,</span> <span>"</span><span>2003-08-01</span><span>"</span><span>),</span>
    <span>(</span><span>"</span><span>2006-08-01</span><span>"</span><span>,</span> <span>"</span><span>2013-08-01</span><span>"</span><span>),</span>
    <span>(</span><span>"</span><span>2016-08-01</span><span>"</span><span>,</span> <span>"</span><span>2023-08-01</span><span>"</span><span>),</span>
<span>]</span>

<span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>DATA_FILE</span><span>,</span> <span>index_col</span><span>=</span><span>[</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>
</code></pre></div>

<figure>
  <figcaption>Table A: Historical Win Odds from Hong Kong Jockey Club</figcaption>

  <table>
    <thead>
      <tr>
        <th>date</th>
        <th>venue</th>
        <th>number</th>
        <th>horse_name</th>
        <th>place</th>
        <th>win_odds</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>1979-09-22</strong></td>
        <td><strong>Happy Valley</strong></td>
        <td><strong>1</strong></td>
        <td><strong>Victorious II</strong></td>
        <td>1</td>
        <td>3.4</td>
      </tr>
      <tr>
        <td><strong>1979-09-22</strong></td>
        <td><strong>Happy Valley</strong></td>
        <td><strong>1</strong></td>
        <td><strong>Money First</strong></td>
        <td>2</td>
        <td>16.0</td>
      </tr>
      <tr>
        <td><strong>1979-09-22</strong></td>
        <td><strong>Happy Valley</strong></td>
        <td><strong>1</strong></td>
        <td><strong>Star Trouper</strong></td>
        <td>3</td>
        <td>3.2</td>
      </tr>
      <tr>
        <td><strong>1979-09-22</strong></td>
        <td><strong>Happy Valley</strong></td>
        <td><strong>1</strong></td>
        <td><strong>Red Rocker</strong></td>
        <td>4</td>
        <td>27.0</td>
      </tr>
      <tr>
        <td><strong>1979-09-22</strong></td>
        <td><strong>Happy Valley</strong></td>
        <td><strong>1</strong></td>
        <td><strong>New Gem</strong></td>
        <td>5</td>
        <td>27.0</td>
      </tr>
      <tr>
        <td><strong>…</strong></td>
        <td><strong>…</strong></td>
        <td><strong>…</strong></td>
        <td><strong>…</strong></td>
        <td>…</td>
        <td>…</td>
      </tr>
      <tr>
        <td><strong>2023-07-16</strong></td>
        <td><strong>Sha Tin</strong></td>
        <td><strong>9</strong></td>
        <td><strong>Master Of Fortune</strong></td>
        <td>5</td>
        <td>5.1</td>
      </tr>
      <tr>
        <td><strong>2023-07-16</strong></td>
        <td><strong>Sha Tin</strong></td>
        <td><strong>9</strong></td>
        <td><strong>Pegasus General</strong></td>
        <td>6</td>
        <td>122.0</td>
      </tr>
      <tr>
        <td><strong>2023-07-16</strong></td>
        <td><strong>Sha Tin</strong></td>
        <td><strong>9</strong></td>
        <td><strong>Charity Bingo</strong></td>
        <td>7</td>
        <td>13.0</td>
      </tr>
      <tr>
        <td><strong>2023-07-16</strong></td>
        <td><strong>Sha Tin</strong></td>
        <td><strong>9</strong></td>
        <td><strong>Chiu Chow Spirit</strong></td>
        <td>8</td>
        <td>10.0</td>
      </tr>
      <tr>
        <td><strong>2023-07-16</strong></td>
        <td><strong>Sha Tin</strong></td>
        <td><strong>9</strong></td>
        <td><strong>So We Joy</strong></td>
        <td>9</td>
        <td>33.0</td>
      </tr>
    </tbody>
  </table>

  <p>337003 rows × 2 columns</p>
</figure>

<h2 id="abstract">Abstract</h2>

<p>This paper examines the elements necessary for a practical and successful computerized horse race handicapping and wagering system. Data requirements, handicapping model development, wagering strategy, and feasibility are addressed. A logit-based technique and a corresponding heuristic measure of improvement are described for combining a fundamental handicapping model with the public’s implied probability estimates. The author reports significant positive results in five years of actual implementation of such a system. This result can be interpreted as evidence of inefficiency in pari-mutuel racetrack wagering. This paper aims to emphasize those aspects of computer handicapping which the author has found most important in practical application of such a system.</p>

<h2 id="introduction">Introduction</h2>

<p>The question of whether a fully mechanical system can ever “beat the races” has been widely discussed in both the academic and popular literature. Certain authors have convincingly demonstrated that profitable wagering systems do exist for the races. The most well documented of these have generally been of the <em>technical</em> variety, that is, they are concerned mainly with the public odds, and do not attempt to predict horse performance from fundamental factors. Technical systems for place and show betting, (Ziemba and Hausch, 1987) and exotic pool betting, (Ziemba and Hausch, 1986) as well as the “odds movement” system developed by Asch and Quandt (1986), fall into this category. A benefit of these systems is that they require relatively little preparatory effort, and can be effectively employed by the occasional racegoer. Their downside is that betting opportunities tend to occur infrequently and the maximum expected profit achievable is usually relatively modest. It is debatable whether any racetracks exist where these systems could be profitable enough to sustain a full-time professional effort.</p>

<p>To be truly viable, a system must provide a large number of high advantage betting opportunities in order that a sufficient amount of expected profit can be generated. An approach which does promise to provide a large number of betting opportunities is to <em>fundamentally</em> handicap each race, that is, to empirically assess each horse’s chance of winning, and utilize that assessment to find profitable wagering opportunities. A natural way to attempt to do this is to develop a computer model to estimate each horse’s probability of winning and calculate the appropriate amount to wager.</p>

<p>A complete survey of this subject is beyond the scope of this paper. The general requirements for a computer based fundamental handicapping model have been well presented by Bolton and Chapman (1986) and Brecher (1980). These two references are “required reading” for anyone interested in developing such a system. Much of what is said here has already been explained in those two works, as is much of the theoretical background which has been omitted here. What the author would hope to add, is a discussion of a few points which have not been addressed in the literature, some practical recommendations, and a report that a <em>fundamental</em> approach can in fact work in practice.</p>

<h2 id="features-of-the-computer-handicapping-approach">Features of the Computer Handicapping Approach</h2>

<p>Several features of the computer approach give it advantages over traditional handicapping. First, because of its empirical nature, one need not possess specific handicapping expertise to undertake this enterprise, as everything one needs to know can be learned from the data. Second is the testability of a computer system. By carefully partitioning data, one can develop a model and test it on <em>unseen</em> races. With this procedure one avoids the danger of overfitting past data. Using this “holdout” technique, one can obtain a reasonable estimate of the system’s real-time performance before wagering any actual money. A third positive attribute of a computerized handicapping system is its consistency. Handicapping races manually is an extremely taxing undertaking. A computer will effortlessly handicap races with the same level of care day after day, regardless of the mental state of the operator. This is a non-trivial advantage considering that a professional level betting operation may want to bet several races a day for extended periods.</p>

<p>The downside of the computer approach is the level of preparatory effort necessary to develop a winning system. Large amounts of past data must be collected, verified and computerized. In the past, this has meant considerable manual entry of printed data. This situation may be changing as optical scanners can speed data entry, and as more online horseracing database services become available. Additionally, several man-years of programming and data analysis will probably be necessary to develop a sufficiently profitable system. Given these considerations, it is clear that the computer approach is not suitable for the casual racegoer.</p>

<h2 id="handicapping-model-development">Handicapping Model Development</h2>

<p>The most difficult and time-consuming step in creating a computer based betting system is the development of the fundamental handicapping model. That is, the model whose final output is an estimate of each horse’s probability of winning. The type of model used by the author is the multinomial logit model proposed by Bolton and Chapman (1986). This model is well suited to horse racing and has the convenient property that its output is a set of probability estimates which sum to 1 within each race.</p>

<blockquote>
  <p>At this point, Benter is still using the multinomial logit model, which assumes that errors follow the Laplace distribution. In Multinomial Probit Models for Competitive Horse Racing<sup id="fnref:gu03" role="doc-noteref"><a href="#fn:gu03" rel="footnote">3</a></sup>, he later replaces this with the Probit model, which assumes Normally distributed errors. His method of training the model using Maximum Likelihood Estimation (MLE) is surprisingly similar to what we now know as the list-wise approach to <a href="https://en.wikipedia.org/wiki/Learning_to_rank">Learning-to-Rank</a>.</p>
</blockquote>

<p>The overall goal is to estimate each horse’s current performance potential. “Current performance potential” being a single overall summery index of a horse’s expected performance in a particular race. To construct a model to estimate current performance potential one must investigate the available data to find those variables or <em>factors</em> which have predictive significance. The profitability of the resulting betting system will be largely determined by the predictive power of the factors chosen. The odds set by the public betting yield a sophisticated estimate of the horses’ win probabilities. In order for a fundamental statistical model to be able to compete effectively, it must rival the public in sophistication and comprehensiveness. Various types of factors can be classified into groups:</p>

<ul>
  <li>Current condition:
    <ul>
      <li>performance in recent races</li>
      <li>time since last race</li>
      <li>recent workout data</li>
      <li>age of horse</li>
    </ul>
  </li>
  <li>Past performance:
    <ul>
      <li>finishing position in past races</li>
      <li>lengths behind winner in past races</li>
      <li>normalized times of past races</li>
    </ul>
  </li>
  <li>Adjustments to past performance:
    <ul>
      <li>strength of competition in past races</li>
      <li>weight carried in past races</li>
      <li>jockey’s contribution to past performances</li>
      <li>compensation for bad luck in past races</li>
      <li>compensation for advantageous or disadvantageous post position in past races</li>
    </ul>
  </li>
  <li>Present race situational factors:
    <ul>
      <li>weight to be carried</li>
      <li>today’s jockey’s ability</li>
      <li>advantages or disadvantages of the assigned post position</li>
    </ul>
  </li>
  <li>Preferences which could influence the horse’s performance in today’s race:
    <ul>
      <li>distance preference</li>
      <li>surface preference (turf vs dirt)</li>
      <li>condition of surface preference (wet vs dry)</li>
      <li>specific track preference</li>
    </ul>
  </li>
</ul>

<p>More detailed discussions of fundamental handicapping can be found in the extensive popular literature on the subject (for the author’s suggested references see the list in the appendix). The data needed to calculate these factors must be entered a d checked for accuracy. This can involve considerable effort. Often, multiple sources must be used to assemble complete past performance records for each of the horses. This is especially the case when the horses have run past races at many different tracks. The easiest type of racing jurisdiction to collect data and develop a model for is one with a <em>closed</em> population of horses, that is, one where horses from a single population race only against each other at a limited number of tracks. When horses have raced at venues not covered in the database, it is difficult to evaluate the elapsed times of races and to estimate the strength of their opponents. Also unknown will be the post position biases, and the relative abilities of the jockeys in those races.</p>

<p>In the author’s experience the minimum amount of data needed for adequate model development and testing samples is in the range of 500 to 1000 races. More is helpful, but out-of-sample predictive accuracy does not seem to improve dramatically with development samples greater than 1000 races. Remember that <em>data for one race</em> means full past data on all of the runners in that race. This suggests another advantage of a <em>closed</em> racing population; by collecting the results of all races run in that jurisdiction one automatically accumulates the full set of past performance data for each horse in the population. It is important to define factors which extract as much information as possible out of the data in each of the relevant areas. As an example, consider three different specifications of a “distance preference” factor.</p>

<p>The first is from Bolton and Chapman (1986):</p>

<ul>
  <li><strong>NEWDIST:</strong> this variable equals one if a horse has run three of its four previous races at a distance less than a mile, zero otherwise. (Note: Bolton and Chapman’s model was only used to predict races of 1-1.25 miles.)</li>
</ul>

<p>The second is from Brecher (1980):</p>

<ul>
  <li><strong>DOK:</strong> this variable equals one if the horse finished in the upper 50th percentile or within 6.25 lengths of the winner in a prior race within 1/16 of a mile of today’s distance, or zero otherwise</li>
</ul>

<p>The last is from the author’s current model:</p>

<ul>
  <li><strong>DP6A:</strong> for each of a horse’s past races, a predicted finishing position is calculated via multiple regression based on all factors except those relating to distance. This predicted finishing position in each race is then subtracted from the horse’s actual finishing position. The resulting quantity can be considered to be the unexplained residual which may be due to some unknown distance preference that the horse may possess plus a certain amount of random error. To estimate the horse’s preference or aversion to today’s distance, the residual in each of its past races is used to estimate a linear relationship between performance and similarity to today’s distance. Given the statistical uncertainty of estimating this relationship from the usually small sample of past races, the final magnitude of the estimate is standardized by dividing it by its standard error. The result is that horses with a clearly defined distance preference demonstrated over a large number of races will be awarded a relatively larger magnitude value than in cases where the evidence is less clear.</li>
</ul>

<blockquote>
  <p>Benter later refines this idea in the paper <a href="http://www.mathsportinternational.com/anziam/Mathsport%203.pdf">Modelling Distance Preference in Thoroughbred Racehorses</a><sup id="fnref:benter96" role="doc-noteref"><a href="#fn:benter96" rel="footnote">4</a></sup> by adding synthetic data points when performing a constrained parabolic least squares fit for the distance preference. These “tack points” allow a solution in cases of less than three observations as well as having a regularisation effect.</p>
</blockquote>

<p>The last factor is the result of a large number of progressive refinements. The subroutines involved in calculating it run to several thousand lines of code. The author’s guiding principle in factor improvement has been a combination of educated guessing and trial and error. Fortunately, the historical data makes the final decision as to which particular definition is superior. The best is the one that produces the greatest increase in predictive accuracy when included in the model. The general thrust of model development is to continually experiment with refinements of the various factors. Although time-consuming, the gains are worthwhile. In the author’s experience, a model involving only simplistic specifications of factors does not provide sufficiently accurate estimates of winning probabilities. Care must be taken in this process of model development not to overfit past data. Some overfitting will always occur, and for this reason it is important to use data partitioning to maintain sets of unseen races for out-of-sample testing.</p>

<p>The complexity of predicting horse performance makes the specification of an elegant handicapping model quite difficult. Ideally, each independent variable would capture a unique aspect of the influences effecting horse performance. In the author’s experience, the trial and error method of adding independent variables to increase the model’s goodness-of-fit, results in the model tending to become a hodgepodge of highly correlated variables whose individual significances are difficult to determine and often counter-intuitive. Although aesthetically unpleasing, this tendency is of little consequence for the purpose which the model will be used, namely, prediction of future race outcomes. What it does suggest, is that careful and conservative statistical tests and methods should be used on as large a data sample as possible.</p>

<p>For example, “number of past races” is one of the more significant factors in the author’s handicapping model, and contributes greatly to the overall accuracy of the predictions. The author knows of no “common sense” reason why this factor should be important. The only reason it can be confidently included in the model is because the large data sample allows its significance to be established beyond a reasonable doubt.</p>

<p>Additionally, there will always be a significant amount of “inside information” in horse racing that cannot be readily included in a statistical model. Trainer’s and jockey’s intentions, secret workouts, whether the horse ate its breakfast, and the like, will be available to certain parties who will no doubt take advantage of it. Their betting will be reflected in the odds. This presents an obstacle to the model developer with access to published information only. For a statistical model to compete in this environment, it must make full use of the advantages of computer modelling, namely, the ability to make complex calculations on large data sets.</p>

<h2 id="creating-unbiased-probability-estimates">Creating Unbiased Probability Estimates</h2>

<p>It can be presumed that valid fundamental information exists which can not be systematically or practically incorporated into a statistical model. Therefore, any statistical model, however well developed, will always be incomplete. An extremely important step in model development, and one that the author believes has been generally overlooked in the literature, is the estimation of the relation of the model’s probability estimates to the public’s estimates, and the adjustment of the model’s estimates to incorporate whatever information can be gleaned from the public’s estimates. The public’s implied probability estimates generally correspond well with the actual frequencies of winning. This can be shown with a table of estimated probability versus actual frequency of winning (Table 1)</p>

<figure>
  <figcaption>Table 1: Public Estimate vs. Actual Frequency</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>1343</td>
        <td>0.007</td>
        <td>0.007</td>
        <td>0.0</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>4356</td>
        <td>0.017</td>
        <td>0.020</td>
        <td>1.3</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>6193</td>
        <td>0.037</td>
        <td>0.042</td>
        <td>2.1</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>8720</td>
        <td>0.073</td>
        <td>0.069</td>
        <td>-1.5</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>5395</td>
        <td>0.123</td>
        <td>0.125</td>
        <td>0.6</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>3016</td>
        <td>0.172</td>
        <td>0.173</td>
        <td>0.1</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1811</td>
        <td>0.222</td>
        <td>0.219</td>
        <td>-0.3</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>1015</td>
        <td>0.273</td>
        <td>0.253</td>
        <td>-1.4</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>716</td>
        <td>0.339</td>
        <td>0.339</td>
        <td>0.0</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>312</td>
        <td>0.467</td>
        <td>0.484</td>
        <td>0.6</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 32877</p>
</figure>

<figure>
  <figcaption>Table 2: Fundamental Model vs. Actual Frequency</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>1173</td>
        <td>0.006</td>
        <td>0.005</td>
        <td>-0.6</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>3641</td>
        <td>0.018</td>
        <td>0.015</td>
        <td>-1.2</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>6503</td>
        <td>0.037</td>
        <td>0.037</td>
        <td>-0.3</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>9642</td>
        <td>0.073</td>
        <td>0.074</td>
        <td>0.1</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>5405</td>
        <td>0.123</td>
        <td>0.12</td>
        <td>-0.7</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>2979</td>
        <td>0.173</td>
        <td>0.183</td>
        <td>1.6</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1599</td>
        <td>0.223</td>
        <td>0.232</td>
        <td>0.9</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>870</td>
        <td>0.272</td>
        <td>0.285</td>
        <td>0.9</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>741</td>
        <td>0.341</td>
        <td>0.32</td>
        <td>-1.2</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>324</td>
        <td>0.475</td>
        <td>0.432</td>
        <td>-1.6</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 32877</p>
</figure>

<p><strong>range</strong> = the range of estimated probabilities</p>

<p><strong>n</strong> = the number of horses falling within a range</p>

<p><strong>exp.</strong> = the mean expected probability</p>

<p><strong>act.</strong> = the actual win frequency observed</p>

<p><strong>Z</strong> = the discrepancy ( + or - ) in units of standard errors</p>

<p>In each range of estimated probabilities, the actual frequencies correspond closely. This is not the case at all tracks (Ali, 1977) and if not, suitable corrections should be made when using the public’s probability estimates for the purposes which will be discussed later. (Unless otherwise noted, data samples consist of all races run by the Royal Hong Kong Jockey Club from September 1986 through June 1993.)</p>

<blockquote>
  <p>Here, Benter discusses the merits of model calibration, that estimated probabilities should occur with the same relative frequency in observed outcomes. Below, we regenerate Table 1 (Public Estimate vs. Actual Frequency) from the paper using our data for each of the four decades. Note that our table for the 1986–1993 period contains 179 additional races and it is unclear where the discrepancy lies.</p>
</blockquote>

<div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>


<span>def</span> <span>table</span><span>(</span><span>data_file</span><span>,</span> <span>column</span><span>,</span> <span>start</span><span>,</span> <span>end</span><span>):</span>
    <span>"""</span><span>Generate a table comparing estimated probabilities to actual win frequencies.

    Parameters:
    - data_file (str): The path to the CSV file containing the data.
    - column (str): The name of the column containing the estimated probabilities.
    - start (str): The start date for filtering the dataset.
    - end (str): The end date for filtering the dataset.

    Displays:
    - DataFrame: A table with columns: range, n, exp., act., and Z.
    - Summary: A printout of start/end date, number of races, and number of starters.
    </span><span>"""</span>

    <span># Load the dataset from a CSV file with specified multi-index columns
</span>    <span>df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>data_file</span><span>,</span> <span>index_col</span><span>=</span><span>[</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>

    <span># Compute win probabilities from win odds
</span>    <span>df</span><span>[</span><span>"</span><span>p_overround</span><span>"</span><span>]</span> <span>=</span> <span>1</span> <span>/</span> <span>df</span><span>.</span><span>win_odds</span>
    <span>df</span><span>[</span><span>"</span><span>p</span><span>"</span><span>]</span> <span>=</span> <span>df</span><span>.</span><span>p_overround</span> <span>/</span> <span>df</span><span>.</span><span>groupby</span><span>([</span><span>"</span><span>date</span><span>"</span><span>,</span> <span>"</span><span>venue</span><span>"</span><span>,</span> <span>"</span><span>number</span><span>"</span><span>]).</span><span>p_overround</span><span>.</span><span>sum</span><span>()</span>

    <span># Filter the DataFrame based on the date range
</span>    <span>df</span> <span>=</span> <span>df</span><span>[(</span><span>df</span><span>.</span><span>index</span><span>.</span><span>get_level_values</span><span>(</span><span>"</span><span>date</span><span>"</span><span>)</span> <span>&gt;</span> <span>start</span><span>)</span> <span>&amp;</span>
            <span>(</span><span>df</span><span>.</span><span>index</span><span>.</span><span>get_level_values</span><span>(</span><span>"</span><span>date</span><span>"</span><span>)</span> <span>&lt;</span> <span>end</span><span>)]</span>

    <span># Compute the number of unique races and the total number of starters
</span>    <span>races</span> <span>=</span> <span>df</span><span>.</span><span>groupby</span><span>([</span><span>"</span><span>date</span><span>"</span><span>,</span> <span>"</span><span>venue</span><span>"</span><span>,</span> <span>"</span><span>number</span><span>"</span><span>]).</span><span>ngroups</span>
    <span>starters</span> <span>=</span> <span>len</span><span>(</span><span>df</span><span>)</span>

    <span># Create a binary column indicating whether the horse was a winner
</span>    <span>df</span><span>[</span><span>"</span><span>winner</span><span>"</span><span>]</span> <span>=</span> <span>df</span><span>.</span><span>place</span> <span>==</span> <span>1</span>

    <span># Define probability bins and labels for them
</span>    <span>bins</span> <span>=</span> <span>[</span><span>0.00</span><span>,</span> <span>0.01</span><span>,</span> <span>0.025</span><span>,</span> <span>0.050</span><span>,</span> <span>0.100</span><span>,</span> <span>0.150</span><span>,</span> <span>0.200</span><span>,</span> <span>0.250</span><span>,</span> <span>0.300</span><span>,</span> <span>0.400</span><span>,</span> <span>1.000</span><span>]</span>
    <span>labels</span> <span>=</span> <span>[</span><span>f</span><span>"</span><span>{</span><span>a</span><span>:</span><span>.</span><span>3</span><span>f</span><span>}</span><span>-</span><span>{</span><span>b</span><span>:</span><span>.</span><span>3</span><span>f</span><span>}</span><span>"</span> <span>for</span> <span>a</span><span>,</span> <span>b</span> <span>in</span> <span>zip</span><span>(</span><span>bins</span><span>,</span> <span>bins</span><span>[</span><span>1</span><span>:])]</span>

    <span># Group by the probability range and compute required aggregate values
</span>    <span>df</span> <span>=</span> <span>df</span><span>.</span><span>groupby</span><span>(</span><span>pd</span><span>.</span><span>cut</span><span>(</span><span>df</span><span>[</span><span>column</span><span>].</span><span>rename</span><span>(</span><span>"</span><span>range</span><span>"</span><span>),</span> <span>bins</span><span>=</span><span>bins</span><span>,</span> <span>labels</span><span>=</span><span>labels</span><span>)).</span><span>agg</span><span>(</span>
        <span>**</span><span>{</span>
            <span>"</span><span>n</span><span>"</span><span>:</span> <span>(</span><span>column</span><span>,</span> <span>"</span><span>size</span><span>"</span><span>),</span>
            <span>"</span><span>exp.</span><span>"</span><span>:</span> <span>(</span><span>column</span><span>,</span> <span>"</span><span>mean</span><span>"</span><span>),</span>
            <span>"</span><span>act.</span><span>"</span><span>:</span> <span>(</span><span>"</span><span>winner</span><span>"</span><span>,</span> <span>"</span><span>mean</span><span>"</span><span>),</span>
            <span>"</span><span>p_std</span><span>"</span><span>:</span> <span>(</span><span>column</span><span>,</span> <span>"</span><span>std</span><span>"</span><span>),</span>
        <span>}</span>
    <span>)</span>

    <span># Compute the Z score to show the discrepancy in units of standard errors
</span>    <span>df</span><span>[</span><span>"</span><span>Z</span><span>"</span><span>]</span> <span>=</span> <span>(</span><span>df</span><span>[</span><span>"</span><span>act.</span><span>"</span><span>]</span> <span>-</span> <span>df</span><span>[</span><span>"</span><span>exp.</span><span>"</span><span>])</span> <span>/</span> <span>(</span><span>df</span><span>.</span><span>p_std</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>df</span><span>.</span><span>n</span><span>))</span> <span>/</span> <span>25</span>

    <span># Display the computed table and summary information
</span>    <span>display</span><span>(</span><span>df</span><span>.</span><span>drop</span><span>(</span><span>"</span><span>p_std</span><span>"</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>))</span>
    <span>print</span><span>(</span><span>f</span><span>"</span><span>{</span><span>start</span><span>=</span><span>}</span><span> </span><span>{</span><span>end</span><span>=</span><span>}</span><span> </span><span>{</span><span>races</span><span>=</span><span>}</span><span> </span><span>{</span><span>starters</span><span>=</span><span>}</span><span>"</span><span>)</span>


<span>for</span> <span>start</span><span>,</span> <span>end</span> <span>in</span> <span>DATE_RANGES</span><span>:</span>
    <span>table</span><span>(</span><span>DATA_FILE</span><span>,</span> <span>"</span><span>p</span><span>"</span><span>,</span> <span>start</span><span>,</span> <span>end</span><span>)</span>
</code></pre></div>

<figure>
  <figcaption>Table B: Public Estimate vs. Actual Frequency (1986-1993)</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>1703</td>
        <td>0.008470</td>
        <td>0.005872</td>
        <td>-8.250281</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>4754</td>
        <td>0.017405</td>
        <td>0.019352</td>
        <td>1.244934</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>6192</td>
        <td>0.036615</td>
        <td>0.041021</td>
        <td>1.980053</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>9422</td>
        <td>0.073415</td>
        <td>0.069518</td>
        <td>-1.047902</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>5519</td>
        <td>0.122698</td>
        <td>0.126110</td>
        <td>0.711950</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>3123</td>
        <td>0.172767</td>
        <td>0.174832</td>
        <td>0.311883</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1822</td>
        <td>0.222284</td>
        <td>0.221734</td>
        <td>-0.067020</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>1001</td>
        <td>0.272763</td>
        <td>0.254745</td>
        <td>-1.701837</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>790</td>
        <td>0.336680</td>
        <td>0.337975</td>
        <td>0.052713</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>419</td>
        <td>0.478898</td>
        <td>0.496420</td>
        <td>0.196101</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3377, # starters = 34745</p>
</figure>

<figure>
  <figcaption>Table C: Public Estimate vs. Actual Frequency (1996-2003)</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>5580</td>
        <td>0.008393</td>
        <td>0.005197</td>
        <td>-19.884794</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>9848</td>
        <td>0.017252</td>
        <td>0.017262</td>
        <td>0.009684</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>11572</td>
        <td>0.036291</td>
        <td>0.036467</td>
        <td>0.108673</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>15455</td>
        <td>0.072603</td>
        <td>0.072986</td>
        <td>0.132493</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>7978</td>
        <td>0.122074</td>
        <td>0.127852</td>
        <td>1.467941</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>3726</td>
        <td>0.172453</td>
        <td>0.163983</td>
        <td>-1.418650</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1883</td>
        <td>0.222604</td>
        <td>0.222517</td>
        <td>-0.010583</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>1001</td>
        <td>0.272983</td>
        <td>0.299700</td>
        <td>2.473131</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>836</td>
        <td>0.337409</td>
        <td>0.330144</td>
        <td>-0.308628</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>393</td>
        <td>0.468867</td>
        <td>0.424936</td>
        <td>-0.508843</td>
      </tr>
    </tbody>
  </table>

  <p># races = 4534, # starters = 58272</p>
</figure>

<figure>
  <figcaption>Table D: Public Estimate vs. Actual Frequency (2006-2013)</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>6367</td>
        <td>0.008398</td>
        <td>0.004869</td>
        <td>-23.329629</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>11433</td>
        <td>0.017268</td>
        <td>0.015219</td>
        <td>-2.002755</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>13139</td>
        <td>0.036298</td>
        <td>0.036609</td>
        <td>0.204394</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>17404</td>
        <td>0.072359</td>
        <td>0.072052</td>
        <td>-0.113371</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>8199</td>
        <td>0.121985</td>
        <td>0.120746</td>
        <td>-0.314975</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>4095</td>
        <td>0.172668</td>
        <td>0.179243</td>
        <td>1.144066</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>2180</td>
        <td>0.223207</td>
        <td>0.229817</td>
        <td>0.866324</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>1296</td>
        <td>0.272618</td>
        <td>0.273920</td>
        <td>0.134999</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>1207</td>
        <td>0.339081</td>
        <td>0.345485</td>
        <td>0.325636</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>672</td>
        <td>0.472380</td>
        <td>0.497024</td>
        <td>0.391450</td>
      </tr>
    </tbody>
  </table>

  <p># races = 5261, # starters = 65992</p>
</figure>

<figure>
  <figcaption>Table E: Public Estimate vs. Actual Frequency (2016-2023)</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>8135</td>
        <td>0.006063</td>
        <td>0.004179</td>
        <td>-3.080125</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>11967</td>
        <td>0.017164</td>
        <td>0.015961</td>
        <td>-1.216143</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>12685</td>
        <td>0.036488</td>
        <td>0.033819</td>
        <td>-1.709530</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>17920</td>
        <td>0.072736</td>
        <td>0.069420</td>
        <td>-1.248783</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>8340</td>
        <td>0.122367</td>
        <td>0.126859</td>
        <td>1.141811</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>4536</td>
        <td>0.172814</td>
        <td>0.177028</td>
        <td>0.771925</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>2584</td>
        <td>0.224027</td>
        <td>0.229102</td>
        <td>0.724819</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>1520</td>
        <td>0.272469</td>
        <td>0.276974</td>
        <td>0.518236</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>1519</td>
        <td>0.340612</td>
        <td>0.356814</td>
        <td>0.915666</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>865</td>
        <td>0.486887</td>
        <td>0.528324</td>
        <td>0.621833</td>
      </tr>
    </tbody>
  </table>

  <p># races = 5757, # starters = 70071</p>
</figure>

<p>A multinomial logit model using fundamental factors will also naturally produce an internally consistent set of probability estimates (Table 2). Here again there is generally good correspondence between estimated and actual frequencies. Table 2 however conceals a major, (and from a wagering point of view, disastrous) type of bias inherent in the fundamental model’s probabilities. Consider the following two tables which represent roughly equal halves of the sample in Table 2. Table 3 shows the fundamental model’s estimate versus actual frequency for those horses where the public’s probability estimate was greater the fundamental model’s. Table 4 is the same except that it is for those horses whose public estimate was less than the fundamental model’s.</p>

<figure>
  <figcaption>Table 3: Fundamental Model vs. Actual Frequency When Public Estimate Is Greater Than Model Estimate</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>920</td>
        <td>0.006</td>
        <td>0.005</td>
        <td>-0.3</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>2130</td>
        <td>0.017</td>
        <td>0.018</td>
        <td>0.3</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>3454</td>
        <td>0.037</td>
        <td>0.044</td>
        <td>2.1</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>4626</td>
        <td>0.073</td>
        <td>0.091</td>
        <td>4.7</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>2413</td>
        <td>0.122</td>
        <td>0.147</td>
        <td>3.7</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>1187</td>
        <td>0.172</td>
        <td>0.227</td>
        <td>5.0</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>540</td>
        <td>0.223</td>
        <td>0.302</td>
        <td>4.4</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>252</td>
        <td>0.270</td>
        <td>0.333</td>
        <td>2.3</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>165</td>
        <td>0.342</td>
        <td>0.448</td>
        <td>2.9</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>54</td>
        <td>0.453</td>
        <td>0.519</td>
        <td>1.0</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 15741</p>
</figure>

<figure>
  <figcaption>Table 4: Fundamental Model vs. Actual Frequency When Public Estimate Is Less Than Model Estimate</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>253</td>
        <td>0.007</td>
        <td>0.004</td>
        <td>-0.6</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>1511</td>
        <td>0.018</td>
        <td>0.011</td>
        <td>-2.2</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>3049</td>
        <td>0.037</td>
        <td>0.029</td>
        <td>-2.6</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>5016</td>
        <td>0.074</td>
        <td>0.058</td>
        <td>-4.3</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>2992</td>
        <td>0.123</td>
        <td>0.098</td>
        <td>-4.2</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>1792</td>
        <td>0.173</td>
        <td>0.154</td>
        <td>-2.1</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1059</td>
        <td>0.223</td>
        <td>0.196</td>
        <td>-2.1</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>618</td>
        <td>0.273</td>
        <td>0.265</td>
        <td>-0.4</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>576</td>
        <td>0.341</td>
        <td>0.283</td>
        <td>-2.9</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>270</td>
        <td>0.480</td>
        <td>0.415</td>
        <td>-2.1</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 17136</p>
</figure>

<p>There is an extreme and consistent bias in both tables. In virtually every range the actual frequency is significantly different than the fundamental model’s estimate, and always in the direction of being closer to the public’s estimate. The fundamental model’s estimate of the probability cannot be considered to be an unbiased estimate independent of the public’s estimate. Table 4 is particularly important because it is comprised of those horses that the model would have one bet on, that is, horses whose model-estimated probability is greater than their public probability. It is necessary to correct for this bias in order to accurately estimate the advantage of any particular bet.’</p>

<p>In a sense, what is needed is a way to combine the judgements of two experts, (i.e. the fundamental model and the public). One practical technique for accomplishing this is as follows: (Asch and Quandt, 1986; pp. 123–125). See also White, Dattero and Flores, (1992).</p>

<p>Estimate a second logit model using the two probability estimates as independent variables. For a race with entrants \((1.2, ..., N)\) the win probability of horse \(i\) is given by:</p><p>

\[c_i = \frac{\exp(\alpha f_i + \beta \pi_i)}{\sum_{j=1}^{N} \exp (\alpha f_j + \beta \pi_j)}\]

</p><p>\(f_i\) = log of “out-of-sample” fundamental model probability estimate</p>

<p>\(\pi_i\) = log of public’s implied probability estimate</p>

<p>\(c_i\) = combined probability estimate</p>

<p>(Natural log of probability is used rather than probability as this transformation provides a better fit)</p>

<div><pre><code><span>import</span> <span>lightning.pytorch</span> <span>as</span> <span>pl</span>

<span>from</span> <span>torchmetrics.functional.classification</span> <span>import</span> <span>multiclass_accuracy</span>


<span>class</span> <span>CombinedModel</span><span>(</span><span>pl</span><span>.</span><span>LightningModule</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>num_features</span><span>:</span> <span>int</span><span>,</span> <span>places</span><span>:</span> <span>int</span> <span>=</span> <span>4</span><span>,</span> <span>lr</span><span>:</span> <span>float</span> <span>=</span> <span>1e-3</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>example_input_array</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>(</span><span>32</span><span>,</span> <span>num_features</span><span>),</span> <span>torch</span><span>.</span><span>rand</span><span>(</span><span>32</span><span>,</span> <span>1</span><span>)</span>
        <span>self</span><span>.</span><span>save_hyperparameters</span><span>()</span>

        <span>self</span><span>.</span><span>fundamental</span> <span>=</span> <span>nn</span><span>.</span><span>Linear</span><span>(</span><span>num_features</span><span>,</span> <span>1</span><span>,</span> <span>bias</span><span>=</span><span>False</span><span>)</span>
        <span>self</span><span>.</span><span>alpha</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>1</span><span>))</span>
        <span>self</span><span>.</span><span>beta</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>zeros</span><span>(</span><span>1</span><span>))</span>
        <span>self</span><span>.</span><span>adjustments</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>places</span> <span>-</span> <span>1</span><span>))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>p</span><span>):</span>
        <span>fundamental_log_probs</span> <span>=</span> <span>F</span><span>.</span><span>log_softmax</span><span>(</span><span>self</span><span>.</span><span>fundamental</span><span>(</span><span>x</span><span>).</span><span>squeeze</span><span>(),</span> <span>dim</span><span>=-</span><span>1</span><span>)</span>
        <span>public_log_probs</span> <span>=</span> <span>torch</span><span>.</span><span>log</span><span>(</span><span>p</span><span>)</span>
        <span>logits</span> <span>=</span> <span>self</span><span>.</span><span>alpha</span> <span>*</span> <span>fundamental_log_probs</span> <span>+</span> <span>self</span><span>.</span><span>beta</span> <span>*</span> <span>public_log_probs</span>
        <span>return</span> <span>softmax</span><span>(</span><span>logits</span><span>,</span> <span>self</span><span>.</span><span>adjustments</span><span>,</span> <span>self</span><span>.</span><span>hparams</span><span>.</span><span>places</span><span>)</span>

    <span>def</span> <span>training_step</span><span>(</span><span>self</span><span>,</span> <span>batch</span><span>,</span> <span>batch_idx</span><span>):</span>
        <span>loss</span><span>,</span> <span>_</span> <span>=</span> <span>self</span><span>.</span><span>_shared_evaluation</span><span>(</span><span>batch</span><span>,</span> <span>batch_idx</span><span>)</span>
        <span>self</span><span>.</span><span>log</span><span>(</span><span>"</span><span>train_loss</span><span>"</span><span>,</span> <span>loss</span><span>,</span> <span>on_step</span><span>=</span><span>True</span><span>,</span> <span>on_epoch</span><span>=</span><span>True</span><span>,</span> <span>prog_bar</span><span>=</span><span>True</span><span>)</span>
        <span>return</span> <span>loss</span>

    <span>def</span> <span>validation_step</span><span>(</span><span>self</span><span>,</span> <span>batch</span><span>,</span> <span>batch_idx</span><span>):</span>
        <span>loss</span><span>,</span> <span>acc</span> <span>=</span> <span>self</span><span>.</span><span>_shared_evaluation</span><span>(</span><span>batch</span><span>,</span> <span>batch_idx</span><span>)</span>
        <span>metrics</span> <span>=</span> <span>{</span><span>"</span><span>val_acc</span><span>"</span><span>:</span> <span>acc</span><span>,</span> <span>"</span><span>val_loss</span><span>"</span><span>:</span> <span>loss</span><span>}</span>
        <span>self</span><span>.</span><span>log_dict</span><span>(</span><span>metrics</span><span>,</span> <span>on_epoch</span><span>=</span><span>True</span><span>,</span> <span>prog_bar</span><span>=</span><span>True</span><span>)</span>
        <span>return</span> <span>metrics</span>

    <span>def</span> <span>_shared_evaluation</span><span>(</span><span>self</span><span>,</span> <span>batch</span><span>,</span> <span>batch_idx</span><span>):</span>
        <span>x</span><span>,</span> <span>p</span><span>,</span> <span>y</span> <span>=</span> <span>batch</span>
        <span>y_hat</span> <span>=</span> <span>self</span><span>(</span><span>x</span><span>,</span> <span>p</span><span>)</span>
        <span>loss</span> <span>=</span> <span>harville_loss</span><span>(</span><span>y_hat</span><span>,</span> <span>y</span><span>,</span> <span>self</span><span>.</span><span>hparams</span><span>.</span><span>places</span><span>)</span>
        <span>acc</span> <span>=</span> <span>multiclass_accuracy</span><span>(</span><span>y_hat</span><span>[:,:,</span><span>0</span><span>].</span><span>argmax</span><span>(</span><span>dim</span><span>=-</span><span>1</span><span>),</span> <span>y</span><span>.</span><span>argmin</span><span>(</span><span>dim</span><span>=-</span><span>1</span><span>),</span> <span>14</span><span>)</span>
        <span>return</span> <span>loss</span><span>,</span> <span>acc</span>

    <span>def</span> <span>predict_step</span><span>(</span><span>self</span><span>,</span> <span>batch</span><span>,</span> <span>batch_idx</span><span>):</span>
        <span>x</span><span>,</span> <span>p</span><span>,</span> <span>y</span> <span>=</span> <span>batch</span>
        <span>return</span> <span>self</span><span>(</span><span>x</span><span>,</span> <span>p</span><span>),</span> <span>p</span><span>,</span> <span>y</span>

    <span>def</span> <span>configure_optimizers</span><span>(</span><span>self</span><span>):</span>
        <span>optimizer</span> <span>=</span> <span>optim</span><span>.</span><span>Adam</span><span>(</span><span>self</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>self</span><span>.</span><span>hparams</span><span>.</span><span>lr</span><span>)</span>
        <span>scheduler</span> <span>=</span> <span>optim</span><span>.</span><span>lr_scheduler</span><span>.</span><span>ReduceLROnPlateau</span><span>(</span>
            <span>optimizer</span><span>,</span> <span>patience</span><span>=</span><span>3</span><span>,</span> <span>verbose</span><span>=</span><span>True</span>
        <span>)</span>
        <span>return</span> <span>{</span>
            <span>"</span><span>optimizer</span><span>"</span><span>:</span> <span>optimizer</span><span>,</span>
            <span>"</span><span>lr_scheduler</span><span>"</span><span>:</span> <span>{</span><span>"</span><span>scheduler</span><span>"</span><span>:</span> <span>scheduler</span><span>,</span> <span>"</span><span>monitor</span><span>"</span><span>:</span> <span>"</span><span>val_loss</span><span>"</span><span>},</span>
        <span>}</span>
</code></pre></div>

<p>Given a set of past races \((1, 2, ..., R)\), for which both public probability estimates and fundamental model estimates are available, the parameters \(\alpha\) and \(\beta\) can be estimated by maximizing the log likelihood function of the given set of races with respect to \(\alpha\) and \(\beta\):</p><p>

\[\exp(L) = \prod_{j-1}^{R} c_{ji*}\]

</p><p>where \(c_{ji*}\) denotes the probability as given by equation (1) for the horse \(i*\) observed to win race \(j\) (Bolton and Chapman, 1986 p. 1044). Equation (1) should be evaluated using fundamental probability estimates from a model developed on a separate sample of races. Use of “out-of-sample” estimates prevents overestimation of the fundamental model’s significance due to “custom-fitting” of the model development sample. The estimated values of \(\alpha\) and \(\beta\) can be interpreted roughly as the relative correctness of the model’s and the public’s estimates. The greater the value of a,the better the model. The probabilities that result from this model also show good correspondence between predicted and actual frequencies of winning (Table 5).</p>

<figure>
  <figcaption>Table 5: Combined Model vs. Actual Frequencies</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>1520</td>
        <td>0.007</td>
        <td>0.005</td>
        <td>-1.0</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>4309</td>
        <td>0.017</td>
        <td>0.018</td>
        <td>0.1</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>6362</td>
        <td>0.037</td>
        <td>0.038</td>
        <td>0.6</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>8732</td>
        <td>0.073</td>
        <td>0.071</td>
        <td>-0.5</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>5119</td>
        <td>0.123</td>
        <td>0.119</td>
        <td>-0.8</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>2974</td>
        <td>0.173</td>
        <td>0.18</td>
        <td>1.0</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1657</td>
        <td>0.223</td>
        <td>0.223</td>
        <td>0.0</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>993</td>
        <td>0.272</td>
        <td>0.281</td>
        <td>0.6</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>853</td>
        <td>0.34</td>
        <td>0.328</td>
        <td>0.7</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>358</td>
        <td>0.479</td>
        <td>0.492</td>
        <td>0.5</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 32877</p>
</figure>

<p>By comparison with Tables 1 and 2, Table 5 shows that there is more <em>spread</em> in the combined model’s probabilities than in either the public’s or the fundamental model’s alone, that is, there are more horses in both the very high and very low probability ranges. This indicates that the combined model is more informative. More important is that the new probability estimates are without the bias shown in Tables 3 and 4, and thus are suitable for the accurate estimation of betting advantage. This is borne out by Tables 6 and 7, which are analogous to Tables 3 and 4 above except that they use the combined model probabilities instead of the raw fundamental model probabilities.</p>

<figure>
  <figcaption>Table 6: Combined Model vs. Actual Frequency When Public Estimate Is Greater Than Model Estimate</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>778</td>
        <td>0.006</td>
        <td>0.005</td>
        <td>-0.4</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>1811</td>
        <td>0.017</td>
        <td>0.015</td>
        <td>-0.6</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>2874</td>
        <td>0.037</td>
        <td>0.035</td>
        <td>-0.7</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>4221</td>
        <td>0.073</td>
        <td>0.073</td>
        <td>0.0</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>2620</td>
        <td>0.123</td>
        <td>0.116</td>
        <td>-1.0</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>1548</td>
        <td>0.173</td>
        <td>0.185</td>
        <td>1.2</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>844</td>
        <td>0.223</td>
        <td>0.231</td>
        <td>0.6</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>493</td>
        <td>0.272</td>
        <td>0.292</td>
        <td>1.0</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>393</td>
        <td>0.337</td>
        <td>0.349</td>
        <td>0.5</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>159</td>
        <td>0.471</td>
        <td>0.509</td>
        <td>1.0</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 15741</p>
</figure>

<figure>
  <figcaption>Table 7: Combined Model vs. Actual Frequency When Public Estimate Is Less Than Model Estimate</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>742</td>
        <td>0.007</td>
        <td>0.004</td>
        <td>-0.9</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>2498</td>
        <td>0.018</td>
        <td>0.019</td>
        <td>0.6</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>3488</td>
        <td>0.037</td>
        <td>0.041</td>
        <td>1.4</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>4511</td>
        <td>0.072</td>
        <td>0.069</td>
        <td>-0.7</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>2499</td>
        <td>0.123</td>
        <td>0.122</td>
        <td>-0.1</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>1426</td>
        <td>0.173</td>
        <td>0.174</td>
        <td>0.1</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>813</td>
        <td>0.223</td>
        <td>0.215</td>
        <td>-0.5</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>500</td>
        <td>0.272</td>
        <td>0.270</td>
        <td>-0.1</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>460</td>
        <td>0.342</td>
        <td>0.311</td>
        <td>-1.4</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>199</td>
        <td>0.485</td>
        <td>0.477</td>
        <td>-0.2</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 17136</p>
</figure>

<p>Observe that the above tables show no significant bias one way or the other.</p>

<h2 id="assessing-the-value-of-a-handicapping-model">Assessing the Value of a Handicapping Model</h2>

<p>The log likelihood function of equation (2) can be used to produce a measure of fit analogous to the R<sup>2</sup> of multiple linear regression (Equation 3). This pseudo-R<sup>2</sup> \((R^2)\) can be used to compare models and to assess the value of a particular model as a betting tool. Each set of probability estimates, either the public’s or those of a model, achieve a certain \(R^2\), defined as (Bolton and Chapman, 1986)</p><p>

\[R^2 = 1 - \frac{L(\textrm{model})}{L(1/N_j)}\]

</p><p>The \(R^2\) value is a measure of the “explanatory power” of the model. An \(R^2\) of 1 indicates perfect predictive ability while an \(R^2\) of 0 means that the model is no better than random guessing. An important benchmark is the \(R^2\) value achieved by the public probability estimate. A heuristic measure of the potential profitability of a handicapping model, borne out in practice, is the amount by which its inclusion in the combined model of equation (1) along with the public probability estimate causes the \(R^2\) to increase over the value achieved by the public estimate alone:</p><p>

\[\Delta R^2 = R^2_{\textrm{C}} - R^2_{\textrm{P}}\]

</p><p>where the subscript \(\textrm{P}\) denotes the public’s probability estimate and \(\textrm{C}\) stands for the combined (fundamental and public) model of equation (1) above. In a sense \(\Delta R^2\) may be taken as a measure of the amount of information added by the fundamental model. In the case of the models which produced Tables 1, 2 and 5 above these values are:</p><p>

\[\begin{aligned}
R^2_{\textrm{P}} &amp;= 0.1218 \ \textrm{(public)}\\

R^2_{\textrm{F}} &amp;= 0.1245 \ \textrm{(fundamental model)}\\

R^2_{\textrm{C}} &amp;= 0.1396 \ \textrm{(combined model)}\\

\Delta R^2_{\textrm{C} \cdot \textrm{P}} &amp;= 0.1396 - 0.1218 = 0.0178\\
\end{aligned}\]

</p><p>Though this value may appear small, it actually indicates that significant profits could be made with that model. The \(\Delta R^2\) value is a useful measure of the potential profitability of a particular model. It can be used to measure and compare models without the the time consuming step of a full wagering simulation. In the author’s experience, greater \(\Delta R^2\) values have been invariably associated with greater wagering simulation profitability. To illustrate the point that the important criteria is the gain in \(R^2\) in the combined model over the public’s \(R^2\), and not simply the \(R^2\) of the handicapping model alone, consider the following two models.</p>

<p>The first is a logit-derived fundamental handicapping model using 9 significant fundamental factors. It achieves an out-of-sample \(R^2\) of 0.1016. The second is a probability estimate derived from tallying the picks of approximately 48 newspaper tipsters. (Figlewski, 1979) The tipsters each make a selection for 1st, 2nd, and 3rd in each race. The procedure was to count the number of times each horse was picked, awarding 6 points for 1st, 3 points for 2nd and 1 point for 3rd. The point total for each horse is then divided by the total points awarded in the race (i.e. 48 * 10). This fraction of points is then taken to be the “tipster” probability estimate. Using the log of this estimate as the sole independent variable in a logit model produces an \(R^2\) of 0.1014. On the basis of their stand-alone \(R^2\)’s the above two models would appear to be equivalently informative predictors of race outcome. Their vast difference appears when we perform the ‘second stage’ of combining these estimates with the public’s. The following results were derived from logit runs on 2,313 races (September 1988 to June 1993).</p><p>

\[\begin{aligned}
R^2_{\textrm{P}} &amp;= 0.1237 \ \textrm{(public estimate)}\\

R^2_{\textrm{F}} &amp;= 0.1016 \ \textrm{(fundamental model)}\\

R^2_{\textrm{T}} &amp;= 0.1014 \ \textrm{(tipster model)}\\

R^2_{(\textrm{F} \&amp; \textrm{P}) - \textrm{P}} &amp;= 0.1327 \ \textrm{(fundamental and public)}\\

R^2_{(\textrm{T} \&amp; \textrm{P}) - \textrm{P}} &amp;= 0.1239 \ \textrm{(tipster and public)}\\

\Delta R^2_{(\textrm{F} \&amp; \textrm{P}) - \textrm{P}} &amp;= 0.1327 - 0.1237 = 0.0090\\

\Delta R^2_{(\textrm{T} \&amp; \textrm{P}) - \textrm{P}} &amp;= 0.1239 - 0.1237 = 0.0002\\
\end{aligned}\]

</p><p>As indicated by the \(\Delta R^2\) values, the tipster model adds very little to the public’s estimate. The insignificant contribution of the tipster estimate to the overall explanatory power of the combined model effectively means that when there is a difference between the public estimate and the tipster estimate, then the public’s estimate is superior. The fundamental model on the other hand, does contribute significantly when combined with the public’s. For a player considering betting with the “tipster” model, carrying out this “second stage” would have saved that player from losing money; the output of the second stage model would always be virtually identical to the public estimate, thus never indicating an advantage bet.</p>

<blockquote>
  <p>While we may use negative log likelihood loss when training a fundamental model, Benter’s pseudo-R<sup>2</sup> metric is easy to interpret. We can see that with each subsequent decade, the public estimate improves.</p>
</blockquote>

<div><pre><code><span>import</span> <span>torch</span>
<span>import</span> <span>torch.nn.functional</span> <span>as</span> <span>F</span>

<span>from</span> <span>torch</span> <span>import</span> <span>nn</span><span>,</span> <span>optim</span><span>,</span> <span>Tensor</span>
<span>from</span> <span>torch.nn.utils.rnn</span> <span>import</span> <span>pad_sequence</span>
<span>from</span> <span>torch.utils</span> <span>import</span> <span>data</span>


<span>def</span> <span>dataset</span><span>(</span><span>data_file</span><span>:</span> <span>str</span><span>,</span> <span>start_date</span><span>:</span> <span>str</span><span>,</span> <span>end_date</span><span>:</span> <span>str</span><span>):</span>
    <span>"""</span><span>
    Load a horse race dataset, filter by date range, and convert to pytorch tensors.

    Parameters:
    - data_file (str): The path to the CSV file containing the data.
    - start (str): The start date for filtering the dataset.
    - end (str): The end date for filtering the dataset.

    Returns:
    - TensorDataset: A pytorch TensorDataset containing padded sequences for
      probabilities (p), and places (y).
    </span><span>"""</span>

    <span># Load the dataset from a CSV file with specified multi-index columns
</span>    <span>df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>data_file</span><span>,</span> <span>index_col</span><span>=</span><span>[</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>])</span>
    
    <span># Compute win probabilities from win odds
</span>    <span>df</span><span>[</span><span>"</span><span>p_overround</span><span>"</span><span>]</span> <span>=</span> <span>1</span> <span>/</span> <span>df</span><span>.</span><span>win_odds</span>
    <span>df</span><span>[</span><span>"</span><span>p</span><span>"</span><span>]</span> <span>=</span> <span>df</span><span>.</span><span>p_overround</span> <span>/</span> <span>df</span><span>.</span><span>groupby</span><span>([</span><span>"</span><span>date</span><span>"</span><span>,</span> <span>"</span><span>venue</span><span>"</span><span>,</span> <span>"</span><span>number</span><span>"</span><span>]).</span><span>p_overround</span><span>.</span><span>sum</span><span>()</span>

    <span># Filter the DataFrame based on the date range
</span>    <span>df</span> <span>=</span> <span>df</span><span>[(</span><span>df</span><span>.</span><span>index</span><span>.</span><span>get_level_values</span><span>(</span><span>"</span><span>date</span><span>"</span><span>)</span> <span>&gt;=</span> <span>start</span><span>)</span> <span>&amp;</span>
            <span>(</span><span>df</span><span>.</span><span>index</span><span>.</span><span>get_level_values</span><span>(</span><span>"</span><span>date</span><span>"</span><span>)</span> <span>&lt;</span> <span>end</span><span>)]</span>

    <span># Separate probabilities (p), and places (y)
</span>    <span>p</span> <span>=</span> <span>df</span><span>.</span><span>p</span>
    <span>y</span> <span>=</span> <span>df</span><span>.</span><span>place</span>

    <span># Group by date, venue, and race number, and convert each group to a torch tensor
</span>    <span>p_race</span> <span>=</span> <span>p</span><span>.</span><span>groupby</span><span>([</span><span>"</span><span>date</span><span>"</span><span>,</span> <span>"</span><span>venue</span><span>"</span><span>,</span> <span>"</span><span>number</span><span>"</span><span>],</span> <span>observed</span><span>=</span><span>True</span><span>).</span><span>apply</span><span>(</span>
        <span>lambda</span> <span>df</span><span>:</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>df</span><span>.</span><span>values</span><span>)</span>
    <span>)</span>
    <span>y_race</span> <span>=</span> <span>y</span><span>.</span><span>groupby</span><span>([</span><span>"</span><span>date</span><span>"</span><span>,</span> <span>"</span><span>venue</span><span>"</span><span>,</span> <span>"</span><span>number</span><span>"</span><span>],</span> <span>observed</span><span>=</span><span>True</span><span>).</span><span>apply</span><span>(</span>
        <span>lambda</span> <span>df</span><span>:</span> <span>torch</span><span>.</span><span>tensor</span><span>(</span><span>df</span><span>.</span><span>values</span><span>)</span>
    <span>)</span>

    <span># Pad the sequences for each group to have the same length
</span>    <span>p_pad</span> <span>=</span> <span>pad_sequence</span><span>(</span><span>tuple</span><span>(</span><span>p_race</span><span>),</span> <span>batch_first</span><span>=</span><span>True</span><span>,</span> <span>padding_value</span><span>=</span><span>1e-9</span><span>).</span><span>float</span><span>()</span>
    <span>y_pad</span> <span>=</span> <span>pad_sequence</span><span>(</span><span>tuple</span><span>(</span><span>y_race</span><span>),</span> <span>batch_first</span><span>=</span><span>True</span><span>,</span> <span>padding_value</span><span>=</span><span>999</span><span>).</span><span>long</span><span>()</span>

    <span># Return as a pytorch TensorDataset
</span>    <span>return</span> <span>data</span><span>.</span><span>TensorDataset</span><span>(</span><span>p_pad</span><span>,</span> <span>y_pad</span><span>)</span>
</code></pre></div>

<div><pre><code><span>def</span> <span>pseudo_r2</span><span>(</span><span>probs</span><span>:</span> <span>Tensor</span><span>,</span> <span>targets</span><span>:</span> <span>Tensor</span><span>)</span> <span>-&gt;</span> <span>Tensor</span><span>:</span>
    <span>"""</span><span>
    Calculate the pseudo R-squared metric for a horse race betting model.

    Parameters:
    - probs (Tensor): A tensor containing the predicted probabilities for each horse in each race.
    - targets (Tensor): A tensor containing the true finishing positions for each horse in each race.

    Returns:
    - Tensor: A tensor containing the pseudo R-squared value.

    Note: This function assumes that targets with values &gt;= 100 are not participating in the race.
    </span><span>"""</span>
    
    <span># Count the number of horses participating in each race
</span>    <span>horses_per_race</span> <span>=</span> <span>(</span><span>targets</span> <span>&lt;</span> <span>100</span><span>).</span><span>sum</span><span>(</span><span>axis</span><span>=</span><span>1</span><span>)</span>
    
    <span># Generate random noise to break dead heats
</span>    <span>noise</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>(</span><span>targets</span><span>.</span><span>shape</span><span>).</span><span>to</span><span>(</span><span>targets</span><span>.</span><span>device</span><span>)</span>
    
    <span># Sort targets and get the index of the winning horse
</span>    <span>win_index</span> <span>=</span> <span>torch</span><span>.</span><span>argsort</span><span>(</span><span>targets</span> <span>+</span> <span>noise</span><span>)[:,</span> <span>0</span><span>]</span>
    
    <span># Extract the predicted probabilities of the winners
</span>    <span>win_probs</span> <span>=</span> <span>torch</span><span>.</span><span>gather</span><span>(</span><span>probs</span><span>,</span> <span>1</span><span>,</span> <span>win_index</span><span>.</span><span>unsqueeze</span><span>(</span><span>-</span><span>1</span><span>))</span>
    
    <span># Calculate log-likelihood for the model
</span>    <span>L_model</span> <span>=</span> <span>torch</span><span>.</span><span>log</span><span>(</span><span>win_probs</span><span>).</span><span>sum</span><span>()</span>
    
    <span># Calculate log-likelihood under a random model
</span>    <span>L_random</span> <span>=</span> <span>torch</span><span>.</span><span>log</span><span>(</span><span>1</span> <span>/</span> <span>horses_per_race</span><span>).</span><span>sum</span><span>()</span>
    
    <span># Calculate the pseudo R-squared value
</span>    <span>r2</span> <span>=</span> <span>1</span> <span>-</span> <span>L_model</span> <span>/</span> <span>L_random</span>
    
    <span>return</span> <span>r2</span>
</code></pre></div>

<div><pre><code><span>for</span> <span>start</span><span>,</span> <span>end</span> <span>in</span> <span>DATE_RANGES</span><span>:</span>
    <span>p_pad</span><span>,</span> <span>y_pad</span> <span>=</span> <span>dataset</span><span>(</span><span>DATA_FILE</span><span>,</span> <span>start</span><span>,</span> <span>end</span><span>)[:]</span>
    <span>r2</span> <span>=</span> <span>pseudo_r2</span><span>(</span><span>p_pad</span><span>,</span> <span>y_pad</span><span>).</span><span>item</span><span>()</span>
    <span>print</span><span>(</span><span>f</span><span>"</span><span>Public Estimate (</span><span>{</span><span>start</span><span>[</span><span>:</span><span>4</span><span>]</span><span>}</span><span>-</span><span>{</span><span>end</span><span>[</span><span>:</span><span>4</span><span>]</span><span>}</span><span> Seasons): r2=</span><span>{</span><span>r2</span><span>:</span><span>.</span><span>4</span><span>f</span><span>}</span><span>"</span><span>)</span>
</code></pre></div>

<div><pre><code>Public Estimate (1986-1993 Seasons)
r2=0.1325

Public Estimate (1996-2003 Seasons)
r2=0.1437

Public Estimate (2006-2013 Seasons)
r2=0.1668

Public Estimate (2016-2023 Seasons)
r2=0.1863
</code></pre></div>

<h2 id="wagering-strategy">Wagering Strategy</h2>

<p>After computing the combined and therefore unbiased probability estimates as described above, one can make accurate estimations of the advantage of any particular bet. A way of expressing advantage is as the expected return per dollar bet:</p><p>

\[\textrm{expected return} = er = c * div\]

\[\textrm{advantage} = er - 1\]

</p><p>where \(c\) is the estimated probability of winning the bet and \(div\) is the expected dividend. For win betting the situation is straightforward. The \(c\)’s are the probability estimates produced by equation (1) above, and the \(div\)’s are the win dividends (as a payoff for a $1 bet) displayed on the tote board. The situation for an example race is illustrated in Table 8.</p>

<figure>
  <figcaption>Table 8</figcaption>

  <table>
    <thead>
      <tr>
        <th>#</th>
        <th>c</th>
        <th>p</th>
        <th>er</th>
        <th>div</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>1</td>
        <td>0.021</td>
        <td>0.025</td>
        <td>0.68</td>
        <td>33.0</td>
      </tr>
      <tr>
        <td>2</td>
        <td>0.125</td>
        <td>0.088</td>
        <td>1.17</td>
        <td>9.3</td>
      </tr>
      <tr>
        <td>3</td>
        <td>0.239</td>
        <td>0.289</td>
        <td>0.69</td>
        <td>2.8</td>
      </tr>
      <tr>
        <td>4</td>
        <td>0.141</td>
        <td>0.134</td>
        <td>0.87</td>
        <td>6.1</td>
      </tr>
      <tr>
        <td>5</td>
        <td>0.066</td>
        <td>0.042</td>
        <td>1.29</td>
        <td>19.0</td>
      </tr>
      <tr>
        <td>6</td>
        <td>0.012</td>
        <td>0.013</td>
        <td>0.75</td>
        <td>61.0</td>
      </tr>
      <tr>
        <td>7</td>
        <td>0.107</td>
        <td>0.136</td>
        <td>0.64</td>
        <td>6.0</td>
      </tr>
      <tr>
        <td>8</td>
        <td>0.144</td>
        <td>0.089</td>
        <td>1.33</td>
        <td>9.2</td>
      </tr>
      <tr>
        <td>9</td>
        <td>0.019</td>
        <td>0.014</td>
        <td>1.18</td>
        <td>60.0</td>
      </tr>
      <tr>
        <td>10</td>
        <td>0.067</td>
        <td>0.066</td>
        <td>0.68</td>
        <td>12.0</td>
      </tr>
      <tr>
        <td>11</td>
        <td>0.012</td>
        <td>0.012</td>
        <td>0.83</td>
        <td>68.0u</td>
      </tr>
      <tr>
        <td>12</td>
        <td>0.028</td>
        <td>0.047</td>
        <td>0.50</td>
        <td>17.0</td>
      </tr>
      <tr>
        <td>13</td>
        <td>0.011</td>
        <td>0.027</td>
        <td>0.32</td>
        <td>30.0</td>
      </tr>
      <tr>
        <td>14</td>
        <td>0.009</td>
        <td>0.019</td>
        <td>0.41</td>
        <td>43.0</td>
      </tr>
    </tbody>
  </table>

</figure>

<p><strong>c</strong> = combined (second stage) probability estimate</p>

<p><strong>p</strong> = public’s probability estimate (1-take) / div</p>

<p><strong>er</strong> = expected return on a $1 win bet</p>

<p><strong>div</strong> = win dividend for a $1 bet</p>

<p>The “u” after the win dividend for horse #11 stands for <em>unratable</em> and indicates that this is a horse for which the fundamental model could not produce a probability estimate. Often this is because the horse is running in its first race. A good procedure for handling such horses is to assign them the same probability as that implied by the public win odds, and renormalize the probabilities on the other horses so that the total probability for the race sums to 1, This is equivalent to saying that we have no information which would allow us to dispute the public’s estimate so we will take theirs.</p>

<p>From Table 8 we can see that the advantage win bets are those with an er greater than 1. There is a positive expected return from betting on each of these horses. Given that there are several different types of wager available, it is necessary to have a strategy for determining which bets to make and in what amounts.</p>

<h2 id="kelly-betting-and-pool-size-limitations">Kelly Betting and Pool Size Limitations</h2>

<p>Given the high cost in time and effort of developing a winning handicapping system, a wagering strategy which produces maximum expected profits is desirable. The stochastic nature of horse race wagering however, guarantees that losing streaks of various durations will occur. Therefore a strategy which balances the tradeoff between risk and returns is necessary. A solution to this problem is provided by the Kelly betting strategy (Kelly, 1956). The Kelly strategy specifies the fraction of total wealth to wager so as to maximize the exponential rate of growth of wealth, in situations where the advantage and payoff odds are known. As a fixed fraction strategy, it also never risks ruin. (This last point is not strictly true, as the minimum bet limit prevents strict adherence to the strategy.) For a more complete discussion of the properties of the Kelly strategy see MacLean, Ziemba and Blazenko (1992), see also Epstein (1977) and Brecher (1980).</p>

<p>The Kelly strategy defines the optimal bet (or set of bets) as those which maximize the expected log of wealth. In pari-mutuel wagering, where multiple bets are available in each race, and each bet effects the final payoff odds,the exact solution requires maximizing a concave logarithmic function of several variables. For a single bet, assuming no effect on the payoff odds, the formula simplifies to</p><p>

\[K = \frac{\textrm{advantage}}{\textrm{dividend} - 1}\]

</p><p>where \(K\) is the fraction of total wealth to wager, When one is simultaneously making wagers in multiple pools, further complications to the exact multiple bet Kelly solution arise due to “exotic” bets in which one must specify the order of finish in two or more races. The expected returns from these bets must be taken into account when calculating bets for the single race pools in those races.</p>

<p>In the author’s experience, betting the full amount recommended by the Kelly formula is unwise for a number of reasons. Firstly, accurate estimation of the advantage of the bets is critical; if one overestimates the advantage by more than a factor of two, Kelly betting will cause a negative rate of capital growth. (As a practical matter, many factors may cause one’s real-time advantage to be less than past simulations would suggest, and very few can cause it to be greater. Overestimating the advantage by a factor of two is easily done in practice.) Secondly, if it is known that regular withdrawals from the betting bankroll will be made for paying expenses or taking profits, then one’s effective wealth is less than their actual current wealth. Thirdly, full Kelly betting is a “rough ride”, downswings during which more than 50% of total wealth is lost are a common occurrence. For these and other reasons, <em>fractional</em> Kelly betting strategy is advisable, that is, a strategy wherein one bets some fraction of the recommended Kelly bet (e.g. 1/2 or 1/3), For further discussion of fractional Kelly betting, and a quantitative analysis of the risk/reward tradeoffs involved, see MacLean, Ziemba and Blazenko (1992).</p>

<p>Another even more important constraint on betting is the effect that one’s bet has on the advantage. In pari-mutuel betting markets each bet decreases the dividend. Even if the bettor possesses infinite wealth, there is a maximum bet producing the greatest expected profit, any amount beyond which lowers the expected profit. The maximum bet can be calculated by writing the equation for expected profit as a function of bet size, and solving for the bet size which maximizes expected profit. This maximum can be surprisingly low as the following example illustrates.</p>

<figure>

  <table>
    <thead>
      <tr>
        <th>c</th>
        <th>div</th>
        <th>er</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>06</td>
        <td>20</td>
        <td>1.20</td>
      </tr>
    </tbody>
  </table>

</figure><p>

\[\begin{aligned}
\textrm{total pool size} &amp;= \$100,000\\

\textrm{maximum } er \textrm{ bet} &amp;= \$416\\

\textrm{expected profit} &amp;= \$39.60\\
\end{aligned}\]

</p><p>A further consideration concerns the shape of the “expected profit versus bet size” curve when the bet size is approaching the maximum. In this example, the maximum expected profit is with a bet of $416. If one made a bet of only 2/3 the maximum, i.e. $277, the expected profit would be 35.5 dollars, or 90% of the maximum. There is very little additional gain for risking a much larger sum of money. Solving the fully formulated Kelly model (i.e. taking into account the bets’ effects on the dividends) will optimally balance this tradeoff. See Kallberg and Ziemba (1994) for a discussion of the optimization properties of such formulations.</p>

<p>As a practical matter; given the relatively small sizes of most pari-mutuel pools, a successful betting operation will soon find that all of its bets are <em>pool-size-limited</em>. As a rule of thumb, as the bettor’s wealth approaches the total pool size, the dominant factor limiting bet size becomes the effect of the bet on the dividend, not the bettor’s wealth.</p>

<blockquote>
  <p>In Chapter 7 of <a href="https://www.amazon.com/Precision-Statistical-Mathematical-Methods-Racing/dp/1432768522">Precision: Statistical and Mathematical Methods in Horse Racing</a><sup id="fnref:wong11" role="doc-noteref"><a href="#fn:wong11" rel="footnote">5</a></sup>, CX Wong discusses the strategic implication of the Kelly Criterion, particularly the common misconception that only the horse with the highest advantage should be bet on, with the Kelly Criterion only used to size the bet. In fact, we should bet on all overlays in a race with varying amounts, with the intuition being that we should trade-off a little return for an increased chance of winning.</p>
</blockquote>

<h3 id="exotic-bets">Exotic Bets</h3>

<p>In addition to win bets, racetracks offer numerous so-called <em>exotic</em> bets. These offer some of the highest advantage wagering opportunities. This results from the multiplicative effect on overall advantage of combining more than one advantage horse. For example, suppose that in a particular race there are two horses for which the model’s estimate of the win probability is greater than the public’s, though not enough so as to make them positive expectation win bets.</p>

<figure>

  <table>
    <thead>
      <tr>
        <th>c</th>
        <th>div</th>
        <th>p</th>
        <th>er</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.115</td>
        <td>8.3</td>
        <td>0.100</td>
        <td>0.955</td>
      </tr>
      <tr>
        <td>0.060</td>
        <td>16.6</td>
        <td>0.050</td>
        <td>0.996</td>
      </tr>
    </tbody>
  </table>

</figure>

<p>By the Harville formula (Harville 1973), the estimated probability of a 1,2 or 2,1 finish is</p><p>

\[\begin{aligned}
C_{12,21} =&amp; (0.115 * 0.060) / (1 - 0.115) + \\
           &amp; (0.060 * 0.115) / (1 - 0.060)   \\
          =&amp; 0.0151
\end{aligned}\]

</p><p>The public’s implied probability estimate is</p><p>

\[\begin{aligned}
P_{12,21} =&amp; (0.100 * 0.050) / (1 - 0.100) + \\
           &amp; (0.050 * 0.100) / (1 - 0.050)   \\
          =&amp; 0.0108
\end{aligned}\]

</p><p>Therefore (assuming a 17% track take) the public’s rational quinella dividend should be</p><p>

\[\textrm{qdiv} \cong (1 - 0.17) / 0.0108 = 76.85\]

</p><p>Assuming that the estimated probability is correct the expected return of a bet on this combination is</p><p>

\[er = 0.0151 * 76.85 = 1.16\]

</p><p>In the above example two horses which had expected returns of less than 1 as individual win bets, in combination produce a 16% advantage quinella bet. The same principle applies, only more so, for bets in which one must specify the finishing positions of more than two horses. In <em>ultra-exotic</em> bets such as the pick-six, even a handicapping model with only modest predictive ability can produce advantage bets. The situation may be roughly summarized by stating that for a bettor in possession of accurate probability estimates which differ from the public estimates; “the more <em>exotic</em> (i.e. specific) the bet, the higher the advantage”. Place and show bets are not considered exotic in this sense as they are less specific than normal bets. The probability differences are “watered down” in the place and show pools. Some professional players make only exotic wagers to capitalize on this effect.</p>

<h3 id="first-second-and-third">First, Second, and Third</h3>

<p>In exotic bets that involve specifying the finishing order of two or more horses in one race, a method is needed to estimate these probabilities. A popular approach is the Harville formula. (Handle, 1973):</p>

<p>For three horses \((i, j, k)\) with win probabilities \((\) the Harville formula specifies the probability that they will finish in order as</p><p>

\[\pi_{ijk} = \frac{\pi_i \pi_j \pi_k}{(1 - \pi_i) (1 - \pi_i - \pi_j)}\]

</p><p>This formula is significantly biased, and should not be used for betting purposes, as it will lead to serious errors in probability estimations if not corrected for in some way.’ (Henery 1981, Stem 1990, Lo and Bacon-Shone 1992). Its principle deficiency is the fact that it does not recognize the increasing randomness of the contests for second and third place. The bias in the Harville formula is demonstrated in Tables 9 and 10 which show the formula’s estimated probabilities for horses to finish second and third given that the identity of the horses finishing first (and second) are known. The data set used is the same as that which produced Table 1 above.</p>

<figure>
  <figcaption>Table 9: Harville Model Conditional Probability of 2nd</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>962</td>
        <td>0.007</td>
        <td>0.010</td>
        <td>0.9</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>3449</td>
        <td>0.018</td>
        <td>0.030</td>
        <td>5.3</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>5253</td>
        <td>0.037</td>
        <td>0.045</td>
        <td>2.8</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>7682</td>
        <td>0.073</td>
        <td>0.080</td>
        <td>2.3</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>4957</td>
        <td>0.123</td>
        <td>0.132</td>
        <td>1.9</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>3023</td>
        <td>0.173</td>
        <td>0.161</td>
        <td>-1.8</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1834</td>
        <td>0.223</td>
        <td>0.195</td>
        <td>-3.0</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>1113</td>
        <td>0.272</td>
        <td>0.243</td>
        <td>-2.3</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>1011</td>
        <td>0.338</td>
        <td>0.317</td>
        <td>-1.4</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>395</td>
        <td>0.476</td>
        <td>0.372</td>
        <td>-4.3</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 29679</p>
</figure>

<figure>
  <figcaption>Table 10: Harville Model Conditional Probability of 3rd</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>660</td>
        <td>0.007</td>
        <td>0.009</td>
        <td>0.5</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>2680</td>
        <td>0.018</td>
        <td>0.033</td>
        <td>4.3</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>4347</td>
        <td>0.037</td>
        <td>0.062</td>
        <td>6.8</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>6646</td>
        <td>0.073</td>
        <td>0.087</td>
        <td>4.0</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>4325</td>
        <td>0.123</td>
        <td>0.136</td>
        <td>2.5</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>2923</td>
        <td>0.173</td>
        <td>0.178</td>
        <td>0.7</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1831</td>
        <td>0.223</td>
        <td>0.192</td>
        <td>-3.4</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>1249</td>
        <td>0.273</td>
        <td>0.213</td>
        <td>-4.9</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>1219</td>
        <td>0.341</td>
        <td>0.273</td>
        <td>-5.3</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>601</td>
        <td>0.492</td>
        <td>0.333</td>
        <td>-8.3</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 26481</p>
</figure>

<p>The large values of the Z-statistics show the significance of the bias in the Harville formula. The tendency is for low probability horses to finish second and third more often than predicted, and for high probability horses to finish second and third less often. The effect is more pronounced for 3rd place than for 2nd. An effective, and computationally economical way to correct for this is as follows: Given the win probability array, (\(\pi_1, \pi_2, ..., \pi_N)\), create a second array \(\sigma\) such that,</p><p>

\[\sigma_i = \frac{\exp(\gamma \log(\pi_i))}{\sum_{j=1}^{N} \exp(\gamma \log(\pi_j))}\]

</p><p>and a third array \(\tau\) such that,</p><p>

\[\tau_i = \frac{\exp(\delta \log(\pi_i))}{\sum_{j=1}^{N} \exp(\delta \log(\pi_j))}\]

</p><p>The probability of the three horses (\(i, j, k)\) finishing in order is then</p><p>

\[\pi_{ijk} = \frac{\pi_i \sigma_j \tau_k}{(1 - \sigma_i) (1 - \tau_i - \tau_j)}\]

</p><p>The parameters \(\gamma\) and \(\delta\) can be estimated via maximum likelihood estimation on a sample of past races. For the above data set the maximum likelihood values of the parameters are \(\gamma = 0.81\) and \(\delta = 0.65\). Reproducing Tables 9 and 10 above using equations (7–9) with these parameter values substantially corrects for the Harville formula bias as can be seen in Tables 11 and 12.</p>

<figure>
  <figcaption>Table 11: Logistic Model Conditional Probability of 2nd \((\gamma = 0.81)\)</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>251</td>
        <td>0.008</td>
        <td>0.012</td>
        <td>0.6</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>2282</td>
        <td>0.018</td>
        <td>0.024</td>
        <td>1.9</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>5195</td>
        <td>0.037</td>
        <td>0.033</td>
        <td>-1.6</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>8819</td>
        <td>0.074</td>
        <td>0.073</td>
        <td>-0.4</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>6054</td>
        <td>0.123</td>
        <td>0.125</td>
        <td>0.5</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>3388</td>
        <td>0.173</td>
        <td>0.176</td>
        <td>0.5</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>1927</td>
        <td>0.222</td>
        <td>0.216</td>
        <td>-0.8</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>973</td>
        <td>0.272</td>
        <td>0.275</td>
        <td>0.2</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>616</td>
        <td>0.336</td>
        <td>0.349</td>
        <td>0.7</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>174</td>
        <td>0.456</td>
        <td>0.397</td>
        <td>-1.6</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 29679</p>
</figure>

<figure>
  <figcaption>Table 12: Logistic Model Conditional Probability of 3rd \((\delta = 0.65)\)</figcaption>

  <table>
    <thead>
      <tr>
        <th>range</th>
        <th>n</th>
        <th>exp.</th>
        <th>act.</th>
        <th>Z</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>0.000–0.010</td>
        <td>4</td>
        <td>0.009</td>
        <td>0.000</td>
        <td>-0.2</td>
      </tr>
      <tr>
        <td>0.010–0.025</td>
        <td>712</td>
        <td>0.020</td>
        <td>0.010</td>
        <td>-2.7</td>
      </tr>
      <tr>
        <td>0.025–0.050</td>
        <td>3525</td>
        <td>0.039</td>
        <td>0.035</td>
        <td>-1.3</td>
      </tr>
      <tr>
        <td>0.050–0.100</td>
        <td>8272</td>
        <td>0.075</td>
        <td>0.073</td>
        <td>-0.7</td>
      </tr>
      <tr>
        <td>0.100–0.150</td>
        <td>6379</td>
        <td>0.123</td>
        <td>0.130</td>
        <td>1.7</td>
      </tr>
      <tr>
        <td>0.150–0.200</td>
        <td>3860</td>
        <td>0.172</td>
        <td>0.175</td>
        <td>0.5</td>
      </tr>
      <tr>
        <td>0.200–0.250</td>
        <td>2075</td>
        <td>0.222</td>
        <td>0.228</td>
        <td>0.7</td>
      </tr>
      <tr>
        <td>0.250–0.300</td>
        <td>921</td>
        <td>0.271</td>
        <td>0.268</td>
        <td>-0.2</td>
      </tr>
      <tr>
        <td>0.300–0.400</td>
        <td>582</td>
        <td>0.337</td>
        <td>0.299</td>
        <td>-2.0</td>
      </tr>
      <tr>
        <td>&gt;0.400</td>
        <td>151</td>
        <td>0.480</td>
        <td>0.450</td>
        <td>-0.7</td>
      </tr>
    </tbody>
  </table>

  <p># races = 3198, # horses = 26481</p>
</figure>

<p>The better fit provided by this model can be readily seen from the much smaller discrepancies between expected and actual frequencies. The parameter values used here should not be considered to be universal constants, as other authors have derived significantly different values for the parameters y and 6 using data from different racetracks (Lo, Bacon-Shone and Busche, 1994).</p>

<blockquote>
  <p>To obtain the probabilities of subsequent places (second, third, etc.), we multiply the logits with a set of adjustment factors, which are themselves free parameters. For the loss function, we apply the Harville formula to compute the predicted probability for the actual finishing order.</p>
</blockquote>

<div><pre><code><span>def</span> <span>softmax</span><span>(</span><span>logits</span><span>:</span> <span>Tensor</span><span>,</span> <span>adjustments</span><span>:</span> <span>Tensor</span><span>,</span> <span>places</span><span>:</span> <span>int</span> <span>=</span> <span>4</span><span>)</span> <span>-&gt;</span> <span>Tensor</span><span>:</span>
    <span>"""</span><span>
    Compute the softmax probabilities with adjustments, following Benter</span><span>'</span><span>s approach.

    Parameters:
    - logits (Tensor): A tensor containing the base utilities for each horse in each race.
    - adjustments (Tensor): A tensor containing the adjustments for each horse in each place.
    - places (int): The number of places to be considered.

    Returns:
    - Tensor: A tensor containing adjusted probabilities for each horse in each race for each place.

    Note:
    The function performs clamping to avoid NaNs when taking logarithms later.
    </span><span>"""</span>

    <span># Compute the initial softmax probabilities based on logits
</span>    <span>p1</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>logits</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>).</span><span>unsqueeze</span><span>(</span><span>2</span><span>)</span>

    <span># Create a tensor by repeating p1 to match the number of places considered (excluding the first place)
</span>    <span>ps</span> <span>=</span> <span>p1</span><span>.</span><span>repeat</span><span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>places</span> <span>-</span> <span>1</span><span>)</span>

    <span># Apply the Benter adjustments and recompute the softmax probabilities
</span>    <span>ps</span> <span>=</span> <span>F</span><span>.</span><span>softmax</span><span>(</span><span>adjustments</span> <span>*</span> <span>torch</span><span>.</span><span>log</span><span>(</span><span>torch</span><span>.</span><span>clamp</span><span>(</span><span>ps</span><span>,</span> <span>min</span><span>=</span><span>1e-16</span><span>)),</span> <span>dim</span><span>=</span><span>1</span><span>)</span>

    <span># Concatenate the initial probabilities with the adjusted probabilities for other places
</span>    <span>probs</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>p1</span><span>,</span> <span>ps</span><span>],</span> <span>dim</span><span>=</span><span>2</span><span>)</span>

    <span># Clamp the probabilities to avoid NaNs when applying the logarithm later
</span>    <span>probs</span> <span>=</span> <span>torch</span><span>.</span><span>clamp</span><span>(</span><span>probs</span><span>,</span> <span>min</span><span>=</span><span>1e-16</span><span>)</span>

    <span>return</span> <span>probs</span>  <span># shape: races x horses x places
</span></code></pre></div>

<div><pre><code><span>def</span> <span>harville_loss</span><span>(</span><span>probs</span><span>:</span> <span>Tensor</span><span>,</span> <span>targets</span><span>:</span> <span>Tensor</span><span>,</span> <span>places</span><span>:</span> <span>int</span> <span>=</span> <span>4</span><span>)</span> <span>-&gt;</span> <span>Tensor</span><span>:</span>
    <span>"""</span><span>
    Compute the Harville loss for a horse race betting model.

    Parameters:
    - probs (Tensor): A tensor containing the predicted probabilities for each horse in each race.
    - targets (Tensor): A tensor containing the true finishing positions for each horse in each race.
    - places (int): The number of places to be considered for the loss computation. Default is 4.

    Returns:
    - Tensor: A tensor containing the Harville loss, normalised by the number of places.

    Note: The function uses noise to handle dead heat places and performs clamping to avoid NaNs.
    </span><span>"""</span>

    <span># Generate random noise to shuffle positions in case of dead heat
</span>    <span>noise</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>(</span><span>targets</span><span>.</span><span>shape</span><span>).</span><span>to</span><span>(</span><span>targets</span><span>.</span><span>device</span><span>)</span>

    <span># Sort targets based on noise-added values to handle dead heats
</span>    <span>targets</span> <span>=</span> <span>torch</span><span>.</span><span>argsort</span><span>(</span><span>targets</span> <span>+</span> <span>noise</span><span>)</span>

    <span># Compute the logarithm of the predicted probabilities
</span>    <span>log_probs</span> <span>=</span> <span>torch</span><span>.</span><span>log</span><span>(</span><span>probs</span><span>)</span>

    <span># Compute the initial negative log likelihood loss
</span>    <span>loss</span> <span>=</span> <span>F</span><span>.</span><span>nll_loss</span><span>(</span><span>log_probs</span><span>[:,</span> <span>:,</span> <span>:</span><span>places</span><span>],</span> <span>targets</span><span>[:,</span> <span>:</span><span>places</span><span>],</span> <span>reduction</span><span>=</span><span>"</span><span>none</span><span>"</span><span>)</span>
    <span>loss</span> <span>=</span> <span>loss</span><span>.</span><span>sum</span><span>(</span><span>dim</span><span>=</span><span>1</span><span>)</span>

    <span># Adjust the loss for subsequent places
</span>    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>places</span><span>):</span>
        
        <span># Compute the probability for finishing in "i-th" place
</span>        <span>probs_place_i</span> <span>=</span> <span>-</span><span>F</span><span>.</span><span>nll_loss</span><span>(</span>
            <span>probs</span><span>[:,</span> <span>:,</span> <span>i</span> <span>:</span> <span>i</span> <span>+</span> <span>1</span><span>].</span><span>repeat</span><span>(</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>i</span><span>),</span> <span>targets</span><span>[:,</span> <span>:</span><span>i</span><span>],</span> <span>reduction</span><span>=</span><span>"</span><span>none</span><span>"</span>
        <span>)</span>

        <span># Compute the denominator term for "i-th" place
</span>        <span>denominator_place_i</span> <span>=</span> <span>-</span><span>torch</span><span>.</span><span>log</span><span>(</span>
            <span>torch</span><span>.</span><span>clamp</span><span>(</span><span>1</span> <span>-</span> <span>probs_place_i</span><span>.</span><span>sum</span><span>(</span><span>dim</span><span>=</span><span>1</span><span>),</span> <span>min</span><span>=</span><span>1e-16</span><span>)</span>
        <span>)</span>

        <span># Adjust the loss
</span>        <span>loss</span> <span>-=</span> <span>denominator_place_i</span>

    <span># Return the mean loss, normalised by the number of places
</span>    <span>return</span> <span>loss</span><span>.</span><span>mean</span><span>()</span> <span>/</span> <span>places</span>
</code></pre></div>

<div><pre><code><span>class</span> <span>PublicModel</span><span>(</span><span>pl</span><span>.</span><span>LightningModule</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>places</span><span>:</span> <span>int</span> <span>=</span> <span>4</span><span>,</span> <span>lr</span><span>:</span> <span>float</span> <span>=</span> <span>1e-3</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>()</span>
        <span>self</span><span>.</span><span>example_input_array</span> <span>=</span> <span>torch</span><span>.</span><span>rand</span><span>(</span><span>32</span><span>,</span> <span>1</span><span>)</span>
        <span>self</span><span>.</span><span>save_hyperparameters</span><span>()</span>
        <span>self</span><span>.</span><span>adjustments</span> <span>=</span> <span>nn</span><span>.</span><span>Parameter</span><span>(</span><span>torch</span><span>.</span><span>ones</span><span>(</span><span>places</span> <span>-</span> <span>1</span><span>))</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>p</span><span>):</span>
        <span>public_log_probs</span> <span>=</span> <span>torch</span><span>.</span><span>log</span><span>(</span><span>p</span><span>)</span>
        <span>return</span> <span>softmax</span><span>(</span><span>public_log_probs</span><span>,</span> <span>self</span><span>.</span><span>adjustments</span><span>,</span> <span>self</span><span>.</span><span>hparams</span><span>.</span><span>places</span><span>)</span>

    <span>def</span> <span>training_step</span><span>(</span><span>self</span><span>,</span> <span>batch</span><span>,</span> <span>batch_idx</span><span>):</span>
        <span>loss</span><span>,</span> <span>_</span> <span>=</span> <span>self</span><span>.</span><span>_shared_evaluation</span><span>(</span><span>batch</span><span>,</span> <span>batch_idx</span><span>)</span>
        <span>self</span><span>.</span><span>log</span><span>(</span><span>"</span><span>train_loss</span><span>"</span><span>,</span> <span>loss</span><span>,</span> <span>on_step</span><span>=</span><span>True</span><span>,</span> <span>on_epoch</span><span>=</span><span>True</span><span>,</span> <span>prog_bar</span><span>=</span><span>True</span><span>)</span>
        <span>return</span> <span>loss</span>

    <span>def</span> <span>validation_step</span><span>(</span><span>self</span><span>,</span> <span>batch</span><span>,</span> <span>batch_idx</span><span>):</span>
        <span>loss</span><span>,</span> <span>acc</span> <span>=</span> <span>self</span><span>.</span><span>_shared_evaluation</span><span>(</span><span>batch</span><span>,</span> <span>batch_idx</span><span>)</span>
        <span>metrics</span> <span>=</span> <span>{</span><span>"</span><span>val_acc</span><span>"</span><span>:</span> <span>acc</span><span>,</span> <span>"</span><span>val_loss</span><span>"</span><span>:</span> <span>loss</span><span>}</span>
        <span>self</span><span>.</span><span>log_dict</span><span>(</span><span>metrics</span><span>,</span> <span>on_epoch</span><span>=</span><span>True</span><span>,</span> <span>prog_bar</span><span>=</span><span>True</span><span>)</span>
        <span>return</span> <span>metrics</span>

    <span>def</span> <span>_shared_evaluation</span><span>(</span><span>self</span><span>,</span> <span>batch</span><span>,</span> <span>batch_idx</span><span>):</span>
        <span>p</span><span>,</span> <span>y</span> <span>=</span> <span>batch</span>
        <span>y_hat</span> <span>=</span> <span>self</span><span>(</span><span>p</span><span>)</span>
        <span>loss</span> <span>=</span> <span>harville_loss</span><span>(</span><span>y_hat</span><span>,</span> <span>y</span><span>,</span> <span>self</span><span>.</span><span>hparams</span><span>.</span><span>places</span><span>)</span>
        <span>acc</span> <span>=</span> <span>multiclass_accuracy</span><span>(</span><span>y_hat</span><span>[:,:,</span><span>0</span><span>].</span><span>argmax</span><span>(</span><span>dim</span><span>=-</span><span>1</span><span>),</span> <span>y</span><span>.</span><span>argmin</span><span>(</span><span>dim</span><span>=-</span><span>1</span><span>),</span> <span>14</span><span>)</span>
        <span>return</span> <span>loss</span><span>,</span> <span>acc</span>

    <span>def</span> <span>configure_optimizers</span><span>(</span><span>self</span><span>):</span>
        <span>optimizer</span> <span>=</span> <span>optim</span><span>.</span><span>Adam</span><span>(</span><span>self</span><span>.</span><span>parameters</span><span>(),</span> <span>lr</span><span>=</span><span>self</span><span>.</span><span>hparams</span><span>.</span><span>lr</span><span>)</span>
        <span>scheduler</span> <span>=</span> <span>optim</span><span>.</span><span>lr_scheduler</span><span>.</span><span>ReduceLROnPlateau</span><span>(</span>
            <span>optimizer</span><span>,</span> <span>patience</span><span>=</span><span>3</span><span>,</span> <span>verbose</span><span>=</span><span>True</span>
        <span>)</span>
        <span>return</span> <span>{</span>
            <span>"</span><span>optimizer</span><span>"</span><span>:</span> <span>optimizer</span><span>,</span>
            <span>"</span><span>lr_scheduler</span><span>"</span><span>:</span> <span>{</span><span>"</span><span>scheduler</span><span>"</span><span>:</span> <span>scheduler</span><span>,</span> <span>"</span><span>monitor</span><span>"</span><span>:</span> <span>"</span><span>val_loss</span><span>"</span><span>},</span>
        <span>}</span>
</code></pre></div>

<div><pre><code><span>from</span> <span>lightning.pytorch.callbacks</span> <span>import</span> <span>LearningRateFinder</span>
<span>from</span> <span>lightning.pytorch.callbacks.early_stopping</span> <span>import</span> <span>EarlyStopping</span>

<span>adjustments</span> <span>=</span> <span>[]</span>
<span>for</span> <span>start</span><span>,</span> <span>end</span> <span>in</span> <span>DATE_RANGES</span><span>:</span>
    <span>val</span> <span>=</span> <span>(</span><span>pd</span><span>.</span><span>Timestamp</span><span>(</span><span>end</span><span>)</span> <span>+</span> <span>pd</span><span>.</span><span>DateOffset</span><span>(</span><span>years</span><span>=</span><span>1</span><span>)).</span><span>strftime</span><span>(</span><span>'</span><span>%Y-%m-%d</span><span>'</span><span>)</span>
    
    <span>datamodule</span> <span>=</span> <span>pl</span><span>.</span><span>LightningDataModule</span><span>.</span><span>from_datasets</span><span>(</span>
        <span>train_dataset</span><span>=</span><span>dataset</span><span>(</span><span>DATA_FILE</span><span>,</span> <span>start</span><span>,</span> <span>end</span><span>),</span>
        <span>val_dataset</span><span>=</span><span>dataset</span><span>(</span><span>DATA_FILE</span><span>,</span> <span>end</span><span>,</span> <span>val</span><span>),</span>
        <span>batch_size</span><span>=</span><span>32</span><span>,</span>
        <span>num_workers</span><span>=</span><span>4</span><span>,</span>
    <span>)</span>

    <span>model</span> <span>=</span> <span>PublicModel</span><span>()</span>

    <span>early_stopping</span> <span>=</span> <span>EarlyStopping</span><span>(</span><span>monitor</span><span>=</span><span>"</span><span>val_loss</span><span>"</span><span>,</span> <span>patience</span><span>=</span><span>6</span><span>)</span>
    <span>learning_rate_finder</span> <span>=</span> <span>LearningRateFinder</span><span>()</span>
    <span>trainer</span> <span>=</span> <span>pl</span><span>.</span><span>Trainer</span><span>(</span>
        <span>callbacks</span><span>=</span><span>[</span><span>early_stopping</span><span>,</span> <span>learning_rate_finder</span><span>],</span>
        <span>max_epochs</span><span>=</span><span>100</span><span>,</span>
        <span>accelerator</span><span>=</span><span>"</span><span>gpu</span><span>"</span><span>,</span>
        <span>devices</span><span>=</span><span>1</span><span>,</span>
    <span>)</span>
    <span>trainer</span><span>.</span><span>fit</span><span>(</span><span>model</span><span>=</span><span>model</span><span>,</span> <span>datamodule</span><span>=</span><span>datamodule</span><span>)</span>
    <span>trainer</span><span>.</span><span>validate</span><span>(</span><span>model</span><span>=</span><span>model</span><span>,</span> <span>datamodule</span><span>=</span><span>datamodule</span><span>)</span>
    
    <span>γ</span><span>,</span> <span>δ</span><span>,</span> <span>ε</span> <span>=</span> <span>model</span><span>.</span><span>adjustments</span><span>.</span><span>tolist</span><span>()</span>
    <span>adjustments</span><span>.</span><span>append</span><span>((</span><span>start</span><span>,</span> <span>end</span><span>,</span> <span>γ</span><span>,</span> <span>δ</span><span>,</span> <span>ε</span><span>))</span>
</code></pre></div>

<div><pre><code><span>for</span> <span>start</span><span>,</span> <span>end</span><span>,</span> <span>γ</span><span>,</span> <span>δ</span><span>,</span> <span>ε</span> <span>in</span> <span>adjustments</span><span>:</span>
    <span>print</span><span>(</span><span>f</span><span>"</span><span>Public Estimate Adjustments (</span><span>{</span><span>start</span><span>[</span><span>:</span><span>4</span><span>]</span><span>}</span><span>-</span><span>{</span><span>end</span><span>[</span><span>:</span><span>4</span><span>]</span><span>}</span><span> Seasons)</span><span>\n</span><span>γ=</span><span>{</span><span>γ</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>, δ=</span><span>{</span><span>δ</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>, ε=</span><span>{</span><span>ε</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>\n</span><span>"</span><span>)</span>
</code></pre></div>

<div><pre><code>Public Estimate Adjustments (1986-1993 Seasons)
γ=0.83, δ=0.68, ε=0.58

Public Estimate Adjustments (1996-2003 Seasons)
γ=0.81, δ=0.75, ε=0.67

Public Estimate Adjustments (2006-2013 Seasons)
γ=0.88, δ=0.73, ε=0.67

Public Estimate Adjustments (2016-2023 Seasons)
γ=0.86, δ=0.74, ε=0.64
</code></pre></div>

<h2 id="feasibility">Feasibility</h2>

<p>A computer based handicapping and betting system could in principle be. developed and implemented at most of the world’s racetracks. Today’s portable computers have sufficient capacity not only for real-time calculation of the bets, but for model development as well. However, several important factors should be considered in selecting a target venue, as potential profitability varies <ins>considerably</ins> among racetracks. The following are a few practical recommendations based on the author’s experience.</p>

<h3 id="data-availability">Data availability</h3>

<p>A reliable source of historical data must be available for developing the model and test samples. The track must have been in existence long enough, running races under conditions similar to today, in order to develop reliable predictions. Data availability in computer form is of great help, as data entry and checking are extremely time-consuming. The same data used in model development must also be available for real-time computer entry sufficient time before the start of each race. Additionally, final betting odds must be available over the development sample for the “combined model” estimation of equation (1) as well as for wagering simulations.</p>

<h3 id="ease-of-operation">Ease of operation</h3>

<p>Having an accurate estimate of the final odds is imperative for betting purposes. Profitability will suffer greatly if the final odds are much different than the ones used to calculate the probabilities and bet sizes. The ideal venue is one which allows off-track telephone betting, and disseminates the odds electronically. This enables the handicapper to bet from the convenience of an office, and eliminates the need to take a portable computer to the track and type in the odds from the tote board at the last minute. Even given ideal circumstances, a professional effort will require several participants. Data entry and verification, general systems programming, and ongoing model development all require full-time efforts, as well as the day-today tasks of running a small business. Startup capital requirements are large, (mainly for research and development) unless the participants forgo salaries during the development phase.</p>

<h3 id="beatability-of-the-opposition">Beatability of the opposition</h3>

<p>Pari-mutuel wagering is a competition amongst participants in a highly negative sum game. Whether a sufficiently effective model can be developed depends on the predictability of the racing, and the level of skill of fellow bettors. If the races are largely dishonest, and the public odds are dominated by inside information then it is unlikely that a fundamental model will perform well. Even if the racing is honest, if the general public skill level is high, or if some well financed minority is skillful, then the relative advantage obtainable will be less. Particularly unfavorable is the presence of other computer handicappers. Even independently developed computer models will probably have a high correlation with each other and thus will be lowering the dividends on the same horses, reducing the profitability for all. Unfortunately, it is difficult to know how great an edge can be achieved at a particular track until one develops a model for that track and tests it, which requires considerable effort. Should that prove successful, there is still no guarantee that the future will be as profitable as past simulations might indicate. The public may become more skillful, or the dishonesty of the races may increase, or another computer handicapper may start playing at the same time.</p>

<h3 id="pool-size-limitations">Pool size limitations</h3>

<p>Perhaps the most serious and inescapable limitation on profitability is a result of the finite amount of money in the betting pools. The high track take means that only the most extreme public probability mis-estimations will result in profitable betting opportunities, and the maximum bet size imposed by the bets’ effects on the dividends limits the amount that can be wagered. Simulations by the author have indicated that a realistic estimate of the maximum expected profit achievable, as a percentage of total per-race turnover, is in the range of 0.25–0.5 per cent. This is for the case of a player with an effectively infinite bankroll. It may be true that at tracks with small pool sizes, that this percentage is higher due to the lack of sophistication of the public, but in any case, it is unlikely that this value could exceed 1.5 per cent. A more realistic goal for a start-up operation with a bankroll equal to approximately one half of the per-race turnover might be to win between 0.1 and 0.2 per cent of the total track turnover. The unfortunate implication of this is that at small volume tracks one could probably not make enough money for the operation to be viable.</p>

<p>Racetracks with small betting volumes also tend to have highly volatile betting odds. In order to have time to calculate and place one’s wagers it is necessary to use the public odds available a few minutes before post time. The inaccuracy involved in using these volatile pre-post-time odds will decrease the effectiveness of the model.</p>

<h2 id="results">Results</h2>

<p>The author has conducted a betting operation in Hong Kong following the principles outlined above for the past five years. Approximately five man-years of effort were necessary to organize the database and develop a handicapping model which showed a significant advantage. An additional five man-years were necessary to develop the operation to a high level of profitability. Under near-ideal circumstances, ongoing operations still require the full time effort of several persons.</p>

<p>A sample of approximately 2,000 races (with complete past performance records for each entrant) was initially used for model development and testing. Improvements to the model were made on a continuing basis, as were regular re-estimations of the model which incorporated the additional data accumulated. A conservative fractional Kelly betting strategy was employed throughout, with wagers being placed on all positive expectation bets available in both normal and exotic pools (except place and show bets). Extremely large pool sizes, (&gt; USD $10,000,000 per race turnover) made for low volatility odds, therefore bets could be placed with accurate estimations of the final public odds. Bets were made on all available races except for races containing only <em>unratable</em> horses (-5%), resulting in approximately 470 races bet per year. The average track take was -19% during this period.</p>

<p>Four of the five seasons resulted in net profits, the loss incurred during the losing season being approximately 20% of starting capital. A strong upward trend in rate of return has been observed as improvements were made to the handicapping model. Returns in the various betting pools have correlated well with theory, with the rate-of-return in exotic pools being generally higher than that in simple pools. While a precise calculation has not been made, the statistical significance of this result is evident. Following is a graph of the natural logarithm of [(wealth) / (initial wealth)] versus races bet.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The question; “Can a system beat the races?” can surely be answered in the affirmative. The author’s experience has shown that at least at some times, at some tracks, a statistically derived fundamental handicapping model can achieve a significant positive expectation. It will always remain an empirical question whether the racing at a particular track at a particular time can be beaten with such a system. It is the author’s conviction that we are now experiencing the <em>golden age</em> for such systems. Advances in computer technology have only recently made portable and affordable the processing power necessary to implement such a model. In the future, computer handicappers may become more numerous, or one of the racing publications may start publishing competent computer ratings of the horses, either of which will likely cause the market to become efficient to such predictions. The profits have gone, and will go, to those who are “in action” first with sophisticated models.</p>

<h2 id="references">References</h2>



  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LiveStore: State management based on reactive SQLite and built-in sync engine (137 pts)]]></title>
            <link>https://livestore.dev</link>
            <guid>44105412</guid>
            <pubDate>Tue, 27 May 2025 09:50:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://livestore.dev">https://livestore.dev</a>, See on <a href="https://news.ycombinator.com/item?id=44105412">Hacker News</a></p>
<div id="readability-page-1" class="page">  <header>  </header>   <div>  <div> <h2>How it works</h2> <p> LiveStore is a fully-featured, client-centric data layer (replacing libraries like Redux, MobX, etc.) with a reactive embedded SQLite database powered by real-time sync (via event-sourcing). </p> </div> <astro-island uid="Z122SxV" prefix="r53" component-url="/_astro/HowItWorks.Hd-YyVeC.js" component-export="HowItWorks" renderer-url="/_astro/client.DxZNQU9M.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;HowItWorksDiagram&quot;,&quot;value&quot;:true}" await-children=""><div id="headlessui-tabs-panel-«r53Rq»" role="tabpanel" tabindex="0" data-headlessui-state="selected" data-selected=""><p>Client A</p><div><svg width="16" height="16" fill="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M8.5 3.75v1.897c0 .46.375.838.844.838h5.312a.84.84 0 0 0 .844-.838v-1.83a2.5 2.5 0 0 0-.57-.067zm8.813 18H6.688a3.93 3.93 0 0 1-3.938-3.927V6.176A3.93 3.93 0 0 1 6.688 2.25h8.242a3.95 3.95 0 0 1 2.783 1.15l2.383 2.374A3.92 3.92 0 0 1 21.25 8.55v9.272a3.93 3.93 0 0 1-3.937 3.927m-1.813-1.5v-4.015a.84.84 0 0 0-.844-.838H9.344a.84.84 0 0 0-.844.838v4.015z"></path></svg><p><span> = Persisted in device storage</span></p></div></div><!--astro:end--></astro-island>  </div> <div>  <div> <h2>Let's look at a real example</h2> <p> The following is a simple TodoMVC app built with LiveStore showing how to model your events, state and reactively query the database. </p> <p> <span>See more examples on </span> <a href="https://github.com/livestorejs/examples"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg> <span>GitHub</span></a> </p> </div> <astro-island uid="2675Hv" component-url="/_astro/Example.CdMvAiP0.js" component-export="Example" renderer-url="/_astro/client.DxZNQU9M.js" props="{}" ssr="" client="only" opts="{&quot;name&quot;:&quot;InteractiveExample&quot;,&quot;value&quot;:&quot;react&quot;}"></astro-island>   </div> <div>  <div> <h2>Demos speak louder than words</h2> <p> LiveStore is designed for demanding &amp; high-performance apps. Let's see it in action. </p> </div> <astro-island uid="LuMUb" prefix="r55" component-url="/_astro/DemoTabs.D27XaGe-.js" component-export="DemoTabs" renderer-url="/_astro/client.DxZNQU9M.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;DemoTabs&quot;,&quot;value&quot;:true}" await-children=""><div id="headlessui-tabs-panel-«r55Rq»" role="tabpanel" tabindex="0" data-headlessui-state="selected" data-selected=""><p>Fun fact: LiveStore was originally developed as a part of Overtone and later factored out.</p></div><!--astro:end--></astro-island>  </div> <div>  <div> <h2>Designed and optimized for demanding applications</h2> <p> LiveStore is based on years of research and was developed as the data foundation for uncompromising apps like Overtone. </p> </div> <astro-island uid="ZLoC8p" prefix="r56" component-url="/_astro/FeatureTabs.CJHMTF8J.js" component-export="FeatureTabs" renderer-url="/_astro/client.DxZNQU9M.js" props="{}" ssr="" client="load" opts="{&quot;name&quot;:&quot;FeatureTabs&quot;,&quot;value&quot;:true}" await-children=""><div id="headlessui-tabs-panel-«r56Rq»" role="tabpanel" tabindex="0" data-headlessui-state="selected" data-selected=""><div><p><img loading="lazy" src="https://livestore.dev/images/sqlite.svg" alt="Reactive &amp; persisted SQLite"></p><h3>Reactive &amp; persisted SQLite</h3><p>LiveStore is based on SQLite enabling instant reactive queries while efficiently persisting data in the background.</p><a href="https://livestore.dev/"><span>Learn more</span></a></div><div><p><img loading="lazy" src="https://livestore.dev/images/git-style-sync.svg" alt="Real-time sync engine"></p><h3>Real-time sync engine</h3><p>LiveStore includes a built-in sync engine based on event sourcing (similar to Git) allowing for complex syncing scenarios.</p><a href="https://livestore.dev/"><span>Learn more</span></a></div><div><p><img loading="lazy" src="https://livestore.dev/images/devtools.svg" alt="Premium DX &amp; devtools"></p><h3>Premium DX &amp; devtools</h3><p>For best-in-class developer experience, LiveStore offers first-class devtools similar to Chrome DevTools but for your data.</p><a href="https://livestore.dev/"><span>Learn more</span></a></div><div><h3>High performance</h3><p>LiveStore was designed for high-performance applications enabling developers to build complex apps running at 120 FPS.</p></div><div><h3>Powerful type-safe schema</h3><p>LiveStore offers a powerful type-safe schema API allowing for ergonomic data modeling and evolution without database migrations.</p></div><div><h3>Local-first</h3><p>LiveStore allows you to build local-first/offline-first apps by taking care of the hardest part: data management.</p></div></div><!--astro:end--></astro-island>  </div>  <div>  <div> <h2>What LiveStore does vs. what not</h2> <p>LiveStore was designed to be a principled and flexible data layer. It's design decisions might make it unsuitable for some use cases. Learn more about <a href="https://docs.livestore.dev/evaluation/when-livestore" target="_blank" rel="noopener noreferrer">when to use LiveStore</a>.</p> </div> <div> <div> <h3>What LiveStore does</h3> <ul> <li>  <span>Provide a powerful data foundation for your app.</span> </li><li>  <span>Reactive query layer with full SQLite support.</span> </li><li>  <span>Adapters for most platforms (web, mobile, server/edge, desktop).</span> </li><li>  <span>Flexible data modeling and schema management.</span> </li><li>  <span>Support true offline-first workflows.</span> </li><li>  <span>Custom merge conflict resolution.</span> </li><li>  <span>Sync with a <a href="https://docs.livestore.dev/reference/syncing/sync-provider/" target="_blank" rel="noopener noreferrer">supported provider</a> or roll your own.</span> </li><li>  <span>Helps avoid data vendor lock-in.</span> </li> </ul> </div> <div> <h3>What LiveStore doesn't do</h3> <ul> <li>  <span>Not a batteries-included framework (no auth, file upload, etc).</span> </li><li>  <span>Not a good fit for some <a href="https://docs.livestore.dev/evaluation/when-livestore" target="_blank" rel="noopener noreferrer">use cases</a>.</span> </li><li>  <span>Doesn't sync with your existing database.</span> </li><li>  <span>Doesn't provide a hosted service.</span> </li><li>  <span>Doesn't scale for unbounded amounts of data.</span> </li><li>  <span>Doesn't support peer-to-peer/decentralized syncing.</span> </li><li>  <span>Sell your data.</span> </li> </ul> </div> </div> </div>  <div>  <p> <h2>What others are saying</h2> </p> <astro-island uid="ZWYBfC" prefix="r57" component-url="/_astro/TweetGrid.DN_6z7Ux.js" component-export="TweetGrid" renderer-url="/_astro/client.DxZNQU9M.js" props="{&quot;tweets&quot;:[1,[[0,{&quot;avatar&quot;:[0,{&quot;src&quot;:[0,&quot;/images/avatars/david-khourshid.jpg&quot;],&quot;name&quot;:[0,&quot;David Khourshid&quot;],&quot;role&quot;:[0,&quot;Creator of XState&quot;]}],&quot;tweet&quot;:[0,&quot;Events are the most accurate representation of state. Everything else is a lossy abstraction.\n\nLiveStore gets it right ⚡️\n&quot;],&quot;tweetUrl&quot;:[0,&quot;https://x.com/DavidKPiano/status/1923740185520378365&quot;]}],[0,{&quot;avatar&quot;:[0,{&quot;src&quot;:[0,&quot;/images/avatars/sunil-pai.png&quot;],&quot;name&quot;:[0,&quot;Sunil Pai&quot;],&quot;role&quot;:[0,&quot;Engineer @Cloudflare&quot;]}],&quot;tweet&quot;:[0,&quot;I'm so very excited for @schickling's livestore to drop, really deeply considered and principled way to build great user interfaces.&quot;],&quot;tweetUrl&quot;:[0,&quot;https://x.com/threepointone/status/1922935305557942401&quot;]}],[0,{&quot;avatar&quot;:[0,{&quot;src&quot;:[0,&quot;https://pbs.twimg.com/profile_images/1905007412450320384/UB7A8k0T_400x400.jpg&quot;],&quot;name&quot;:[0,&quot;Beto Moedano&quot;],&quot;role&quot;:[0,&quot;Developer Advocate @Expo&quot;]}],&quot;tweet&quot;:[0,&quot;@livestoredev + @expo + @CloudflareDev = Local-First app with real-time sync, offline persistence, and smooth performance. 🚀&quot;]}],[0,{&quot;avatar&quot;:[0,{&quot;src&quot;:[0,&quot;https://pbs.twimg.com/profile_images/1808122460820430848/sUSrPA5N_400x400.jpg&quot;],&quot;name&quot;:[0,&quot;Jacob Clausen&quot;],&quot;role&quot;:[0,&quot;App Developer&quot;]}],&quot;tweet&quot;:[0,&quot;There's so much to be excited about with @livestoredev. But what really gets me is the extra mile they've gone with the dev tools. Top-tier stuff that adds serious value. Plus, it's an @expo dev plugin, making it seamless and well integrated. A dream combo for offline-first!&quot;]}],[0,{&quot;avatar&quot;:[0,{&quot;src&quot;:[0,&quot;/images/avatars/peter-pistorius.jpg&quot;],&quot;name&quot;:[0,&quot;Peter Pistorius&quot;],&quot;role&quot;:[0,&quot;Co-creator RedwoodSDK&quot;]}],&quot;tweet&quot;:[0,&quot;What are you syncing about?\n\nJust got a preview of @livestoredev v2 by @schickling: it's next-level.&quot;],&quot;tweetUrl&quot;:[0,&quot;https://x.com/appfactory/status/1922660472525857235&quot;]}],[0,{&quot;avatar&quot;:[0,{&quot;src&quot;:[0,&quot;/images/avatars/johannes-schickling.jpg&quot;],&quot;name&quot;:[0,&quot;Johannes Schickling&quot;],&quot;role&quot;:[0,&quot;Creator of LiveStore&quot;]}],&quot;tweet&quot;:[0,&quot;I think LiveStore is pretty cool but I'm biased. However, you should give it a try.&quot;]}],[0,{&quot;avatar&quot;:[0,{&quot;src&quot;:[0,&quot;https://pbs.twimg.com/profile_images/1913014527039541248/DcaBXxHW_400x400.jpg&quot;],&quot;name&quot;:[0,&quot;Andrew Patton&quot;],&quot;role&quot;:[0,&quot;Founder Outlyne&quot;]}],&quot;tweet&quot;:[0,&quot;I’ve spent years wrestling with app state &amp; sync logic that never quite felt right.\n\n@livestoredev has been incredible for us at Outlyne. The declarative model is easy to reason about, sync just works, and the devtools are super useful.\n\nFinally found the one⚡️\n    &quot;],&quot;tweetUrl&quot;:[0,&quot;https://x.com/andpatton/status/1925996952820252886&quot;]}]]]}" ssr="" client="load" opts="{&quot;name&quot;:&quot;TweetGrid&quot;,&quot;value&quot;:true}" await-children=""><!--astro:end--></astro-island>  </div> <div>  <div> <h2>Additional resources</h2> <p> Check out the following resources to learn more about LiveStore. </p> </div> <div> <div>  <h3>Conference talk</h3> <p>You can learn more about LiveStore in some of our past conference talks.</p> <a href="https://docs.livestore.dev/misc/community"> <span>Read more</span>  </a> </div><div>  <h3>Riffle essay</h3> <p>In the Riffle essay (+ PhD thesis by <a href="https://www.geoffreylitt.com/" target="_blank" rel="noopener noreferrer">Geoffrey Litt</a>), we explored the idea of reactive SQLite as a modern state management system.</p> <a href="https://riffle.systems/"> <span>Read more</span>  </a> </div> </div>  </div> <div>  <h2>The story behind LiveStore</h2> <p>LiveStore was designed and developed as foundation for <a href="https://overtone.pro/" target="_blank" rel="noopener noreferrer">Overtone</a>, a next-gen music app. To achieve the high-performance requirements of the app, we needed a state management framework that is able to handle the complex data scenarios of the app which started the <a href="https://riffle.systems/" target="_blank" rel="noopener noreferrer">Riffle research project</a> and later became LiveStore.
</p>   </div> <div>  <div> <h2>Get started</h2> <p> Give LiveStore a try. Start with an existing example or add it to your own project. </p>  </div> <div> <h2>Sponsor the project</h2> <p> Become a sponsor and get access to... </p> <ul> <li>  <span>LiveStore devtools</span> </li><li> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path d="M524.5 69.8a1.5 1.5 0 0 0 -.8-.7A485.1 485.1 0 0 0 404.1 32a1.8 1.8 0 0 0 -1.9 .9 337.5 337.5 0 0 0 -14.9 30.6 447.8 447.8 0 0 0 -134.4 0 309.5 309.5 0 0 0 -15.1-30.6 1.9 1.9 0 0 0 -1.9-.9A483.7 483.7 0 0 0 116.1 69.1a1.7 1.7 0 0 0 -.8 .7C39.1 183.7 18.2 294.7 28.4 404.4a2 2 0 0 0 .8 1.4A487.7 487.7 0 0 0 176 479.9a1.9 1.9 0 0 0 2.1-.7A348.2 348.2 0 0 0 208.1 430.4a1.9 1.9 0 0 0 -1-2.6 321.2 321.2 0 0 1 -45.9-21.9 1.9 1.9 0 0 1 -.2-3.1c3.1-2.3 6.2-4.7 9.1-7.1a1.8 1.8 0 0 1 1.9-.3c96.2 43.9 200.4 43.9 295.5 0a1.8 1.8 0 0 1 1.9 .2c2.9 2.4 6 4.9 9.1 7.2a1.9 1.9 0 0 1 -.2 3.1 301.4 301.4 0 0 1 -45.9 21.8 1.9 1.9 0 0 0 -1 2.6 391.1 391.1 0 0 0 30 48.8 1.9 1.9 0 0 0 2.1 .7A486 486 0 0 0 610.7 405.7a1.9 1.9 0 0 0 .8-1.4C623.7 277.6 590.9 167.5 524.5 69.8zM222.5 337.6c-29 0-52.8-26.6-52.8-59.2S193.1 219.1 222.5 219.1c29.7 0 53.3 26.8 52.8 59.2C275.3 311 251.9 337.6 222.5 337.6zm195.4 0c-29 0-52.8-26.6-52.8-59.2S388.4 219.1 417.9 219.1c29.7 0 53.3 26.8 52.8 59.2C470.7 311 447.5 337.6 417.9 337.6z"></path></svg> <span>Discord channel</span> </li><li>  <span>Community</span> </li> </ul>  </div>  </div> <header>  </header>  
</div>]]></description>
        </item>
    </channel>
</rss>