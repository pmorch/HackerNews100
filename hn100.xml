<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 12 Dec 2024 15:30:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Gukesh becomes the youngest chess world champion in history (211 pts)]]></title>
            <link>https://lichess.org/@/Lichess/blog/wcc-2024-round-14-gukesh-becomes-the-youngest-world-champion-in-history/cDggdNZw</link>
            <guid>42398952</guid>
            <pubDate>Thu, 12 Dec 2024 13:29:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lichess.org/@/Lichess/blog/wcc-2024-round-14-gukesh-becomes-the-youngest-world-champion-in-history/cDggdNZw">https://lichess.org/@/Lichess/blog/wcc-2024-round-14-gukesh-becomes-the-youngest-world-champion-in-history/cDggdNZw</a>, See on <a href="https://news.ycombinator.com/item?id=42398952">Hacker News</a></p>
Couldn't get https://lichess.org/@/Lichess/blog/wcc-2024-round-14-gukesh-becomes-the-youngest-world-champion-in-history/cDggdNZw: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Timemap.org – Interactive Map of History (335 pts)]]></title>
            <link>https://www.oldmapsonline.org/en/history/regions</link>
            <guid>42397550</guid>
            <pubDate>Thu, 12 Dec 2024 09:12:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oldmapsonline.org/en/history/regions">https://www.oldmapsonline.org/en/history/regions</a>, See on <a href="https://news.ycombinator.com/item?id=42397550">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><div><p><img alt="Old Maps Online Logo – Explore Historical Maps" fetchpriority="high" width="104.7" height="32.89" decoding="async" data-nimg="1" src="https://www.oldmapsonline.org/images/header_logo_light_mode.svg"><img alt="Old Maps Online Logo – Explore Historical Maps" fetchpriority="high" width="104.7" height="32.89" decoding="async" data-nimg="1" src="https://www.oldmapsonline.org/images/header_logo_dark_mode.svg"></p></div><div><p><span>​</span></p></div></div><ul><li><a href="https://www.oldmapsonline.org/en/project">Project</a></li><li><a href="https://www.oldmapsonline.org/en/community">Community</a></li><li><a href="https://www.oldmapsonline.org/en/news">News</a></li><li><a href="https://www.oldmapsonline.org/en/app">My App</a></li></ul><div><p><a tabindex="0" href="https://www.oldmapsonline.org/en/user/maps">My maps</a></p></div></div><header><div><h6>Regions</h6></div></header><div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div><a href="https://www.oldmapsonline.org/en/history/regions"><p><span>History</span></p></a></div><div><a href="https://www.oldmapsonline.org/en"><p><span>Maps</span></p></a></div><div><a href="https://www.maptiler.com/story/oldmapsonline/" target="_blank"><div><p><img alt="Maptiler" src="https://www.oldmapsonline.org/images/maptiler/icon.svg"></p></div></a></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A ChatGPT clone, in 3000 bytes of C, backed by GPT-2 (2023) (316 pts)]]></title>
            <link>https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html</link>
            <guid>42396372</guid>
            <pubDate>Thu, 12 Dec 2024 05:01:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html">https://nicholas.carlini.com/writing/2023/chat-gpt-2-in-c.html</a>, See on <a href="https://news.ycombinator.com/item?id=42396372">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>
              This program is a dependency-free implementation of GPT-2. It loads
              the weight matrix and BPE file out of the original TensorFlow files,
              tokenizes the input with a simple byte-pair encoder,
              implements a basic linear algebra package with matrix math operations,
              defines the transformer architecture, performs transformer inference,
              and un-tokenizes the output with the BPE decoder.
              All in ~3000 bytes of C.
            </p>
            
      
            <p>
              It's optimized efficiently enough so that GPT-2 Small takes a few
              seconds per reply on any modern machine. To do this I've implemented
              KV caching and an efficient matrix multiplication algorithm,
              with optional OMP parallelism.
            </p>
      
            <p>
              You can then use this to create something like Chat GPT---just so long
              as you don't care about the quality of the output. (It's actually
              pretty terrible output, objectively speaking... But it does run.)
              There are a
              few quirks (especially with handling UTF-8 characters), and running
              the XL size model at long context length can require ~100GB of RAM.
              But if you're just typing with ASCII using GPT2-Small it should run
              just about anywhere.
            </p>
      
            <p>
              I've uploaded <a href="https://github.com/carlini/c-chat-gpt-2">the code to GitHub</a>, so feel free to try and use it there.
            </p>
            
      
            
        
            
            <div id="mine"><p>
            This program is made up of the following main blocks (hover over each to see the coresponding code):        
                <a href="#linalg" id="show0">Basic matrix math library (700 bytes)</a>
                <a href="#matmul" id="show1">Fast matrix multiplication (300 bytes)</a>
                <a href="#nn" id="show2">Neural network layers (300 bytes)</a>
                <a href="#gpt" id="show3">Transformer model (600 bytes)</a>
                <a href="#bpe" id="show5">Byte pair encoding (400 bytes)</a>
                <a href="#z" id="show6">I/O (200 bytes)</a>
                <a href="#loadweight" id="show8">Weight loading (300 bytes)</a>
                <a href="#loadbpe" id="show7">Byte pair encoding loading (300 bytes)</a></p><div id="main">
      <p><span>#include</span><span>&lt;stdio.h&gt;</span></p>
      <p><span>#include</span><span>&lt;stdlib.h&gt;</span></p>
      <p><span>#include</span><span>&lt;string.h&gt;</span></p>
      <p><span>#include</span><span>&lt;math.h&gt;</span></p>
      <p><span>int</span><span> </span><span>U</span><span>,</span><span>C</span><span>,</span><span>K</span><span>,</span><span>c</span><span>,</span><span>d</span><span>,</span><span>S</span><span>,</span><span>zz</span><span>;</span><span>char</span><span>*</span><span>bpe</span><span>;</span><span>typedef</span><span> </span><span>struct</span><span>{</span><span>float</span><span>*</span><span>i</span><span>;</span><span>int</span><span> </span><span>j</span><span>,</span><span>k</span><span>;} </span><span>A</span><span>;</span><span>void</span><span>*</span><span>E</span><span>,*</span><span>n</span><span>;</span><span>A</span><span>*</span><span>f</span><span>;</span><span>FILE</span><span>*</span><span>fp</span><span>;</span></p>
      <p><span>#define</span><span> </span><span>N</span><span>(</span><span>i</span><span>,</span><span>j</span><span>)</span><span>for</span><span>(</span><span>int</span><span> </span><span>i</span><span>=0; i&lt;j; i++)</span></p>
      <p><span>
      <p><span>A</span><span> </span><span>o</span><span>(</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>,</span><span>int</span><span> </span><span>i</span><span>){</span><span>float</span><span>*</span><span>a</span><span>=E;E+=S=4*j*k;memset(a,0,S*i);</span><span>A</span><span> </span><span>R</span><span>={ a,j,k} ;</span><span>return</span><span> R;}</span></p>
      <p><span>#define</span><span> </span><span>I</span><span>(</span><span>R</span><span>,</span><span>B</span><span>)</span><span>A</span><span> </span><span>R</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>float</span><span> </span><span>k</span><span>){ N(i,a.j*a.k){ </span><span>float</span><span> </span><span>b</span><span>=a.i[i]; a.i[i]=B; } </span><span>return</span><span> a; }</span></p>
      <p><span>I</span><span>(l,b/k)</span><span>I</span><span>(q,b+k)</span><span>I</span><span>(u,1./sqrt(</span><span>b</span><span>))</span><span>I</span><span>(z,</span><span>exp</span><span>(</span><span>b</span><span>))</span><span>I</span><span>(r,a.i[(i/a.k)*a.k])</span><span>I</span><span>(P,(i/k&lt;i%(</span><span>int</span><span>)k)?0:exp(b/8))</span><span>I</span><span>(Q,b/2*(1+tanh(.7978845*(b+.044715*b*b*b))))</span></p>
      <p><span>#define</span><span> </span><span>F</span><span>(</span><span>R</span><span>,</span><span>B</span><span>)</span><span>A</span><span> </span><span>R</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>A</span><span> </span><span>b</span><span>){ N(i,a.j*a.k){ a.i[i]=a.i[i]B b.i[i]; } </span><span>return</span><span> a; }</span></p>
      <p><span>F</span><span>(V,+)</span><span>F</span><span>(v,*)</span><span>F</span><span>(H,/)</span><span>F</span><span>(at,+b.i[i%a.k];)</span><span>F</span><span>(mt,*b.i[i%a.k];)</span><span>A</span><span> </span><span>X</span><span>(</span><span>A</span><span> </span><span>a</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.j,a.k,1);N(i,a.j*a.k)R.i[(i/a.k)*a.k]+=a.i[i];r(R,0);</span><span>return</span><span> R;}</span><span>A</span><span> </span><span>p</span><span>(</span><span>A</span><span> </span><span>a</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.k,a.j,1);N(i,a.j*a.k)R.i[i%a.k*a.j+i/a.k]=a.i[i];</span><span>return</span><span> R;}</span></p>
      </span>
      <span>
        <p><span>A</span><span> </span><span>g</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>A</span><span> </span><span>b</span><span>){</span><span>A</span><span> </span><span>R</span><span>=o(a.j,b.j,!c);</span><span>{</span><span>for</span><span>(</span><span>int</span><span> </span><span>i</span><span>=c;i&lt;d;i++){</span><span>for</span><span>(</span><span>int</span><span> </span><span>j</span><span>=0;j&lt;b.j;j+=4){</span><span>for</span><span>(</span><span>int</span><span> </span><span>k</span><span>=0;k&lt;a.k;k+=4){N(k2,4)N(j2,4)R.i[i*b.j+j+j2]+=a.i[i*a.k+k+k2]*b.i[(j+j2)*b.k+k+k2];}}}}</span><span>return</span></p>
      </span>
      <span>
      <p><span> V(o(R.j,R.k,1),R);}</span><span>A</span><span> </span><span>J</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>int</span><span> </span><span>b</span><span>,</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>){</span><span>A</span><span> </span><span>R</span><span>={ a.i+b*j,j,k} ;</span><span>return</span><span> R;}</span><span>A</span><span> </span><span>s</span><span>(</span><span>A</span><span> </span><span>a</span><span>,</span><span>int</span><span> </span><span>i</span><span>){</span><span>A</span><span> </span><span>b</span><span>=V(a,l(X(a),-a.k));</span><span>A</span><span> </span><span>k</span><span>=l(X(v(V(o(b.j,b.k,1),b),b)),b.k-1);</span><span>A</span><span> </span><span>R</span><span>=at(mt(v(V(o(b.j,b.k,1),b),u(q(k,1e-5),0)),f[i+1]),f[i]);</span><span>return</span><span> R;}</span></p>
      <p><span>#define</span><span> </span><span>G</span><span>(</span><span>a</span><span>,</span><span>i</span><span>)at(g(a,f[i+1]),f[i])</span></p>
      </span>
      <span>
        <p><span>A</span><span> </span><span>m</span><span>(</span><span>int</span><span> </span><span>j</span><span>,</span><span>int</span><span> </span><span>k</span><span>){j+=!j;k+=!k;</span><span>A</span><span> </span><span>a</span><span>=o(j,k,1);fread(a.i,S,1,fp);</span><span>return</span><span> p(a);}</span></p>
      </span>
      <span>
        <p><span>int</span><span> </span><span>t</span><span>;</span><span>int</span><span> </span><span>Y</span><span>(</span><span>char</span><span>*</span><span>R</span><span>){</span><span>if</span><span>(!*R)</span><span>return</span><span> 0;</span><span>int</span><span> </span><span>B</span><span>=1e9,</span><span>r</span><span>;N(i,5e4){</span><span>if</span><span>(bpe[999*i]&amp;&amp;strncmp(bpe+999*i,R,S=strlen(bpe+999*i))==0){</span><span>int</span><span> </span><span>k</span><span>=Y(R+S)+i+1e7;</span><span>if</span><span>(k&lt;B){B=k;r=i;}}}t=r;</span><span>return</span><span> B;}</span><span>int</span><span> *</span><span>w</span><span>(</span><span>char</span><span>*</span><span>q</span><span>,</span><span>int</span><span>*</span><span>B</span><span>){</span><span>char</span><span> </span><span>R</span><span>[1000];</span><span>int</span><span> </span><span>i</span><span>=0;</span><span>while</span><span>(q[i]){</span><span>int</span><span> </span><span>j</span><span>=i++;</span><span>while</span><span>(47&lt;q[i]&amp;&amp;q[i]&lt;58||64&lt;q[i]){fflush(stdout);i++;}strcpy(R,q+j);R[i-j]=0;fflush(stdout);</span><span>int</span><span> </span><span>k</span><span>=0;</span><span>while</span><span>(R[k]){Y(R+k);</span><span>char</span><span>*</span><span>M</span><span>=bpe+t*999;k+=strlen(M);*B++=t;}}</span><span>return</span><span> B;}</span></p>
      </span>
      <span>
        <p><span>int</span><span> </span><span>main</span><span>(</span><span>int</span><span> </span><span>S</span><span>,</span><span>char</span><span>**</span><span>D</span><span>){S=D[1][5]+3*D[1][7]+3&amp;3;K=12+4*S+(S&gt;2);U=K*64;C=12*S+12;zz=atoi(D[4]);E=malloc(2LL*U*U*C*zz);</span></p>
      </span>
      <span>
        <p><span>bpe=malloc(1e9);fp=fopen(D[2],</span><span>"r"</span><span>);</span><span>unsigned</span><span> </span><span>char</span><span> </span><span>a</span><span>[S=999],</span><span>b</span><span>[S];N(i,5e4){</span><span>int</span><span> </span><span>k</span><span>=i*S;</span><span>if</span><span>(i&lt;93){bpe[k]=i+33;bpe[k+1]=0;} </span><span>else</span><span> </span><span>if</span><span>(i&gt;254){fscanf(fp,</span><span>"%s %s"</span><span>,a,b);strcat((</span><span>char</span><span>*)a,(</span><span>char</span><span>*)b);</span><span>int</span><span> </span><span>j</span><span>=0;N(i,a[i])bpe[k+j++]=a[i]^196?a[i]:a[++i]-128;bpe[k+j++]=0;} </span><span>else</span><span> </span><span>if</span><span>(i&gt;187){bpe[k]=i-188;bpe[k+1]=0;}}</span><span>int</span><span> </span><span>e</span><span>[1024];d=w(D[3],e)-e;</span><span>int</span><span> </span><span>h</span><span>;N(i,d){</span><span>if</span><span>(e[i]==18861)h=i+1;}printf(</span><span>"AI"</span><span>);N(i,d-h)printf(</span><span>"%s"</span><span>,bpe+e[i+h]*999);</span></p>
      </span>
      <span>
        <p><span>fp=fopen(D[1],</span><span>"r"</span><span>);</span><span>A</span><span>\</span></p>
      <p><span><span>&nbsp;</span></span><span>x</span><span>[999];</span><span>A</span><span>*</span><span>R</span><span>=x;N(i,C){N(j,12)*R++=m(U+U*(j?j^8?j^11?0:3:3:2),U*((j%8==3)+3*(j%8==1)+(j==9)));}*R++=m(U,1);*R++=m(U,1);</span><span>A</span><span> </span><span>QA</span><span>=m(1024,U),</span><span>Z</span><span>=p(m(5e4,U));</span></p>
      </span>
      <span>
        <p><span>while</span><span>(1){</span><span>char</span><span> </span><span>W</span><span>[1000]={ 0} ;</span><span>int</span><span> </span><span>T</span><span>;strcat(W,</span><span>"\nAlice: "</span><span>);printf(</span><span>"\n%s: "</span><span>,bpe+20490*999);fflush(stdout);fgets(W+8,1000,stdin);printf(</span><span>"AI:"</span><span>);strcat(W,</span><span>"\nBob:"</span><span>);d=w(W,e+d)-e;n=E;c=0;</span></p>
      </span>
      <span>
        <p><span>while</span><span>(1){E=n;T=d+32-d%32;c*=!!(d%32);</span><span>A</span><span> </span><span>O</span><span>=o(T,U,1);N(i,d){N(j,U)O.i[i*U+j]=Z.i[e[i]*U+j]+QA.i[j*1024+i];}N(i,C){</span><span>int</span><span> </span><span>y</span><span>;S=0;N(j,10){</span><span>if</span><span>(j==i)y=S;S++;N(k,10*(j&gt;0)){</span><span>if</span><span>(j*10+k&lt;C&amp;&amp;S++&amp;&amp;i==j*10+k)y=S;}}f=x+12*y;</span><span>A</span><span> </span><span>QB</span><span>=p(J(G(s(O,4),0),0,T*3,U));</span><span>A</span><span> </span><span>B</span><span>=o(U,T,1);N(k,K){</span><span>A</span><span> </span><span>L</span><span>=p(J(QB,k*3,64*T,3)),</span><span>a</span><span>=P(g(p(J(L,0,64,T)),p(J(L,T,64,T))),T),</span><span>R</span><span>=p(g(H(a,X(a)),J(L,T*2,64,T)));memcpy(B.i+64*T*k,R.i,64*T*4);}O=V(O,G(p(B),2));O=V(O,G(Q(G(s(O,6),8),0),10));}f=x;O=s(O,12*C);c=0;</span><span>int</span><span> </span><span>S</span><span>=d;d=1;</span><span>A</span><span> </span><span>B</span><span>=g(p(J(O,S-1,U,1)),Z);c=d=S;S=0;N(i,5e4){</span><span>if</span><span>(B.i[i]&gt;B.i[S])S=i;}</span><span>if</span><span>(d==zz){memcpy(e,e+zz/2,S*2);d-=zz/2;c=0;}e[d++]=S;</span></p>
      </span>
      <span>
        <p><span>if</span><span>(bpe[S*999]==10)</span><span>break</span><span>;printf(</span><span>"%s"</span><span>,bpe+S*999);fflush(stdout);}}}</span></p>
        </span>
            </p></div>
      
            </div>
      
            
            
      
            <br>
            <h2>Background: ChatGPT and transformers</h2>
      
            <p>
              In case you've been living under a rock for the past few months,
              ChatGPT is an application where you can talk to a type of machine learning
              model called a "language model" as if it was another person. It responds remarkably well,
              and GPT-4, the latest model that powers ChatGPT, is incredibly impressive.
            </p>
      
            <p>
              This C program implements the behavior of ChatGPT using a much
              weaker model from 2019: GPT-2. Despite being just 2 smaller than GPT-4,
              it has no where near the same capabilities---but it is open source.
              So it has that going for it.
            </p>
      
            <p>
              <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>
              is a type of machine learning model called a <a href="https://arxiv.org/abs/1706.03762">"transformer"</a>.
              These neural networks take a fixed-size sequence of words as input,
              and predict the next word that will occur. By repeating the procedure
              over and over, you can use them to generate arbitrary-length sequences.
            </p>
      
            <p>
              This post isn't meant to be an introduction to all the machine learning
              you'll need to know <i>why</i> a transformer is designed the way it is,
              but the rest of this post will be dedicated to describing how the above
              C code works.
            </p>
      
            <br>
            <h2>Walkthrough the C Code</h2>
      
            <h3 id="linalg">Getting started: Matrix Math (700 bytes)</h3>
      
            <p>
              Seeing as neural networks are just matrix operations. So we're going to need to get
              started by building a matrix library in as few bytes as possible.
            </p>
      
            <p>
              My definition of a matrix is completely minimal:
            </p>
      
      
            <div>
            <p><span>typedef</span><span> </span><span>struct</span><span> {</span></p>
      <p><span><span>&nbsp; </span></span><span>float</span><span>* </span><span>dat</span><span>;</span></p>
            <p><span><span>&nbsp; </span></span><span>int</span><span> </span><span>rows</span><span>, </span><span>cols</span><span>;</span></p>
      <p><span>} Matrix;</span></p>
            </div>
      
            <p>
              We'll begin by by observing that while there are a bunch of different operations
              we'll need to implement, there are basically two "types" of operations"
              </p><ol>
                <li>
                  Matrix-constant operations (e.g., add 7 to each entry of a matrix)
                </li>
                <li>
                  Matrix-matrix operations (e.g., add corresponding matrix entries)
                </li>
              </ol>
            
      
            <p>
              This similarity allows us to use macros to pull out a bunch of the common logic
              into a meta-routine that knows how to operate on, for example, pairs
              of matrices and just leaves the specific operator implementation defined.
            </p>
      
            <p>
              To do this in C, I'll define the function
            </p>
      
      
            <p><span>#define</span><span> </span><span>BINARY</span><span>(</span><span>function</span><span>, </span><span>operation</span><span>)</span></p>
      
            <p>
              as the following:
            </p>
            
            <div>
              <p><span>Matrix FUNCTION(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; a.cols; j++) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>a[i*a.cols + j] = a[i*a.cols + j] OPERATION b[i*a.cols+j];</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span></span><span>return</span><span> a;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              And so for example this lets us just write
            </p>
      
            <div>
              <p><span>BINARY(matrix_elementwise_add, +);</span></p>
      <p><span>BINARY</span><span>(matrix_elementwise_multiply, *);</span></p>
            </div>
      
            <p>
            and have it automatically expand to the full operation that perform
            elementwise addition or multiplication of two matrices. I define a few
            other easy to understand operations as well:
            </p>
      
            <p>
            Now the thing about C's #defines is they're basically just glorified regexs.
            So when we actually run this, what's going to happen is we're going to take
            the line
            </p>
      
            <p><span>a[i*a.cols + j] = a[i*a.cols + j] OPERATION b[i*a.cols+j];</span></p><p>
            
            and expand for the case of multiplication expand it to
            
            </p><p><span>a[i*a.cols + j] = a[i*a.cols + j] * b[i*a.cols+j];</span></p>
      
            <p>
              But this replacement is almost literally just a regular expression replace.
              We could have put anything in place of OPERATION.
              This allows us to define a function like
            </p>
      
            <p><span>BINARY(add_tile, + b.dat[i%a.cols] ; )</span></p>
      
            <p>
              Which at first glance looks rather confusing---what is that semi-colon doing there?---but
              if you just do a regular expression replace on it, you'll see that it expands to
            </p>
      
            <p><span><span>&nbsp; </span>a[i*a.cols + j] = a[i*a.cols + j] + b.dat[i%a.cols] ; </span><span>b</span><span>[i*a.cols+j];</span></p>
      
            <p>
              where because the second expression doesn't do anything this is just equivalent to
            </p>
            
            <p><span>a[i*a.cols + j] = a[i*a.cols + j] + b.dat[i%a.cols] ; </span><span>b</span><span>[i*a.cols+j];</span></p>
      
            <p>
              (TAKE THAT LANGUAGES WITH PROPER MACROS. LISP ISN'T ALWAYS BETTER THAN C!)
            </p>
            
            <h3 id="matmul">Fast matrix multiplication (300 bytes)</h3>
      
            <p>
              The basic implementation of matrix multiplication is entirely straightforward:
              we just implement the naive cubic-time three loops:
              (There's nothing intelligent about my matrix multiplication. If you know how to make
              matrix multiplication fast you can just move along.)
            </p>
      
      
            <div>
              <p><span>Matrix matmul(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = NewMatrix(a.rows, b.rows);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++)</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; b.rows; j++)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; a.cols; k++)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span>out.dat[i * b.rows + j] += a.dat[i * a.cols + k+k2] * b.dat[(j+j2) * b.cols + k];</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
            
            <p>
              Fortunately we can make it much faster with just a few bits of intelligence.
              Because of the way memory and caches work on most computers, it's (much!) faster
              to read and write to the same piece of memory over and over.
            </p>
            
            <div>
              <p><span>Matrix matmul_t_fast(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>Matrix</span><span> </span><span>b</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = NewMatrix(a.rows, b.rows);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; a.rows; i++)</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; b.rows; j += 4)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; a.cols; k += 4)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k2</span><span> = 0; k2 &lt; 4; k2 += 1)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>j2</span><span> = 0; j2 &lt; 4; j2 += 1)</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>out.dat[i * b.rows + j+j2] += a.dat[i * a.cols + k+k2] * b.dat[(j+j2) * b.cols + k+k2];</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              Later we're going to make one more change to the way we do inference and add a new
              parameter to the matrix multiply that instead allows us to only multiply part of Matrix A
              by Matrix B, which is useful when we've already pre-computed part of the product.
            </p>
            
            <h3 id="nn">Neural network layers (300 bytes)</h3>
      
            <p>
              In order to write a transformer I'll need to define a few special neural-network specific layers.
              One of these is the <a href="https://arxiv.org/abs/1606.08415">GELU</a> activation function,
              which you can just think of as magic.
            </p>
            <p><span>UNARY(GELU, b / 2 * (1 + tanh(.7978845 * (b + .044715 * b * b * b))))</span></p>
      
            <p>
              I also implement a function to set the lower-diagonal of a matrix
              (after exponentiating the values). This is useful for what's called <i>causal attention</i>:
              we only want to attend to the past, not the future, and so we set
              the lower diagonal of the attention matrix to zero with this function.
            </p>
      
            <p><span>UNARY(tril, (i/k&lt;i%(</span><span>int</span><span>)k) ? 0 : exp(b/8))</span></p>
      
            <p>
              And finally we need a layer normalization function.
              (Again another piece of magic that you can look up if you want.
              Basically what it does is normalize the mean and variance of each layer.)
            </p>
            <div>
              <p><span>Matrix LayerNorm(</span><span>Matrix</span><span> </span><span>a</span><span>, </span><span>int</span><span> </span><span>i</span><span>) {</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>b</span><span> = add(a, divide_const(sum(a), -a.cols));</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>k</span><span> = divide_const(sum(multiply(</span></p>
      <p><span><span>&nbsp; &nbsp; </span>add(NewMatrix(b.rows,b.cols,1),b), b)), b.cols-1);</span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>out</span><span> = add_tile(multiply_tile(</span></p>
      <p><span><span>&nbsp; &nbsp; </span>multiply(add(NewMatrix(b.rows,b.cols,1),b),</span></p>
      <p><span><span>&nbsp; &nbsp; </span>mat_isqrt(add_const(k, 1e-5),0)), layer_weights[i+1]),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>layer_weights[i]);</span></p>
      
      <p><span><span>&nbsp; </span></span><span>return</span><span> out;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              The final piece of the model is the Linear function that
              just performs a matrix multiplication and adds (with tiling) a bias.
            </p>
      
            <p><span>#define</span><span> </span><span>Linear</span><span>(</span><span>a</span><span>, </span><span>i</span><span>) add_tile(matmul_t_fast(a, layer_weights[i+1]), layer_weights[i])</span></p>
      
            <h3 id="gpt">Transformer architecture (600 bytes)</h3>
      
            <p>
              With all of this out of the way, we can finally implement our transformer
              in just 600 bytes. 
              
            </p>
            
            <div>
              <p><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; NLAYER; i++) {</span></p>
      <p><span><span>&nbsp; </span>layer_weights = weights + 12*permute;</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Compute the keys, queries, and values all at once with a big multiply </span><span></span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>qkv</span><span> = transpose(slice(Linear(LayerNorm(line, 4), 0), 0, T*3, DIM));</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Make space for the output of the computation<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; </span></span><span>Matrix</span><span> </span><span>result</span><span> = NewMatrix(DIM, T, 1);</span></p>
      
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; NHEAD; k++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Split the qkv into each of the heads<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>Matrix</span><span> </span><span>merge</span><span> = transpose(slice(qkv, k*3, 64*T, 3)),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>// perform the product of the queries and keys and then exponentiate </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>a</span><span> = tril(matmul_t_fast(transpose(slice(merge, 0, 64, T)),</span></p>
      <p><span><span>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span>transpose(slice(merge, T, 64, T))), T),</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>// finally multiply the softmax output (a/sum(a)) with the values matrix </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span></span><span>out</span><span> = transpose(matmul_t_fast(divide(a, sum(a)), slice(merge, T*2, 64, T)));</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// and copy the output to the proper location in the result matrix </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; </span>memcpy(result.dat+64*T*k, out.dat, 64*T*4);</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Residual connection </span><span></span></p>
      <p><span><span>&nbsp; </span>line = add(line,Linear(transpose(result), 2));</span></p>
      
      <p><span><span>&nbsp; </span></span><span>// Activation function and residual connection </span><span></span></p>
      <p><span><span>&nbsp; </span>line = add(line, Linear(GELU(Linear(LayerNorm(line, 6), 8), 0), 10));</span></p>
      <p><span>}</span></p>
      
      <p><span>// Reset layer weights so we can do the last layer norm<span>&nbsp;</span></span><span> </span></p>
      <p><span>layer_weights = weights;</span></p>
      <p><span>line = LayerNorm(line, 12*NLAYER);</span></p>
      
      <p><span>Matrix</span><span> </span><span>result</span><span> = matmul_t_fast(transpose(slice(line, tmp-1, DIM, 1)), wte);</span></p>
      
            </div>
            
            <p>
              Now here's a fact that might not be completely obvious about transformer
              inference: once you've called the model to generate one token, you don't
              actually have to re-compute the entire function to generate the next token.
              In fact, you only need to do a small amount of additional work to generate
              each additional token.
            </p>
      
            <p>
              This is because once you've computed the output of the transformer on the
              output of all the tokens up to the Nth token, you can re-use almost all of
              this output to compute the N+1st token (with a little bit more work.)
            </p>
      
            <p>
              To actually implement this, I make all allocations in my code occur
              sequentially within the same block of memory, to guarantee that each
              matrix multiply will always use exactly the same memory. Then, at each
              iteration of the loop, I can just not zero-out the memory before using
              it for the next iteration, and the memory will already contain the
              results of the previous iteration. I just need to run the computation
              for the N+1st row.
            </p>
            
            
            <h3 id="bpe">Byte pair encoding (400 bytes)</h3>
      
            <p>          
              The simplest way to build a language model is on a sequence of words.
              But because the total number of words is essentially unbounded,
              and language models need to have fixed-size inputs,
              it would be necessary to replace sufficiently rare words with a special
              [OUT OF DISTRIBUTION] token. This is no good.
            </p>
      
            <p>
              While a simple “fix” for this would be to just use character-level
              language models that only know about individual letters, this would
              be a problem because it would mean that the model would have to learn
              the meaning of every word from scratch, and also reduces the effective
              context size of the language model by a factor of the average word length.
            </p>
      
            <p>
              So to fix this, language models like GPT-2 work by creating tokens out
              of "word pieces". Some words might be tokens all by them-self, but
              rare words are broken up into smaller pieces. For example, the word
              “nicholas” might be broken up into “nich” “o” “las”.
            </p>
            
            <p>
              The general algorithm for this is rather easy to implement:
              given a word we want to tokenize, we first split it into individual
              characters. Then, we look for pairs of adjacent tokens that should
              be merged, and merge them together. We repeat this until there are
              no more possible merges.
            </p>
      
            <p>
              This algorithm is simple but unfortunately rather hard to implement
              in C because it requires a bunch of allocations, and requires keeping
              track of a tree-like structure of the tokens.
            </p>
      
            <p>
              So instead, we'll turn the rather simple linear time algorithm into a
              potentially exponential time algorithm but save a bunch of code.
              Our basic idea will work like this in C-like pseudocode:
            </p>
      
            <div>
              <p><span>word_tokenize(word) {</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> len(word) == 0 { </span><span>return</span><span> (0, 0); }</span></p>
      <p><span><span>&nbsp; </span>result = (1e9, -1);</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; VOCAB_LEN; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> (is_prefix(bpe[i]), word) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>sub_cost = word_tokenize(word+len(bpe[i]))[0] + i + 1e7;</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>result = min(result, (sub_cost, i));</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span></span><span>return</span><span> result;</span></p>
      <p><span>}</span></p>
            </div>
      
            <p>
              That is, to tokenize a word, we try each possible word in the vocabulary
              to see if it's a prefix of the current word. If so, we try to use this
              as the first token, and then recursively try to tokenize the rest of the
              word. We keep track of the best tokenization we've seen so far (as judged
              by the length, breaking ties by the index of the token in the vocab), and
              return that.
            </p>
              
            
            <h3 id="loadweight">Weight loading (300 bytes)</h3>
      
            <p>
              We're almost done! The last thing we need to do is load the actual weights
              of the neural network off disk. This is actually pretty easy, because
              the weights are stored in a simple binary format that's easy to read
              in C: it's just a completely flat serialization of 32-bit floats.
            </p>
      
            <p>
              The only thing we need to know is how big the various matrices are.
              And fortunately, this is also easy to figure out. Each of the GPT-2
              model sizes have the same architecture, and the weights are saved in the
              same order, so all we need to do is read read the correctly-shaped
              matrices off of disk.
            </p>
      
            <p>
              There's one final annoying thing. The layers of the neural network are
              not stored on disk in the order you might expect, with layer 0 first,
              then layer 1, then layer 2. Instead, the first layer is layer 0, then
              layer 1, and then layer .... TEN! (and then layer 11, and then layer 12.)
              This is because weights are stored when sorted lexicographically.
              And lexicographically, “0” comes before “1”, but “10” comes before
              “2”. So we have to do a bit of work to permute the weights into the
              correct order with the following code
            </p>
      
            <div>
              <p><span>int</span><span> permute;</span></p>
      <p><span>tmp=0;</span></p>
      <p><span>for</span><span> (</span><span>int</span><span> </span><span>j</span><span> = 0; j &lt; 10; j++) {</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> (j == i) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span>permute = tmp;</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>tmp++;</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>k</span><span> = 0; k &lt; 10*(j&gt;0); k++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>if</span><span> (j*10+k &lt; NLAYER &amp;&amp; tmp++ &amp;&amp; i == j*10+k) {</span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>permute = tmp;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span>}</span></p>
            </div>
      
            
            <h3 id="loadbpe">Byte pair encoding loading (300 bytes)</h3>
      
            <p>
              In order to actually perform byte-pair encoding, we need to first load
              the byte-pair encoding vocabulary off disk. In an ideal world we'd
              actually just have a list of all the words in the vocabulary stored
              in some reasonable C-readable format, but because the original file
              was (a) meant for reading in Python, and (b) not meant to make it
              easy to parse in as few bytes as possible, we'll have to do some work here.
            </p>
      
            <p>
              You might expect the file format to just be a list of words one after
              the other, but it's actually instead a list of the byte-pair encodings.
              What this means is instead of being able to read “Hello” as one token,
              the line is “H” “ello” which means we should be merging the tokens
              “H” and “ello” into a single token “Hello”.
            </p>
      
            <p>
              The other challenge is that the file is encoded in smoothing-like
              UTF-8 (but not quite exactly that) for ... reasons.
              All of the printable ascii characters are encoded as themselves,
              but the non-printable characters from 0-31 are encoded as the value
              188+the character. So for example, a space is encoded as the token “Ġ”.
              But now the problem is that the UTF8 encoding of “Ġ” is 0xc4 0xa0
              when on disk, and so when reading it we have to do just some ugly work
              to convert this back to a space.
            </p>
      
            <p>
              And while none of this is actually that hard to do, it still requires
              a fair amount of code which is annoying when you're trying to compress
              everything to be small.
            </p>
      
            <div>
      <p><span>unsigned</span><span> </span><span>char</span><span> a[tmp=999],b[tmp];</span></p>
      <p><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; i &lt; 5e4; i++) {</span></p>
      <p><span><span>&nbsp; </span></span><span>int</span><span> </span><span>k</span><span> = i*tmp;</span></p>
      <p><span><span>&nbsp; </span></span><span>if</span><span> (i &lt; 93) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// The first 92 tokens are just the printable ascii characters </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k] = i + 33;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+1] = 0;</span></p>
      <p><span><span>&nbsp; </span>} </span><span>else</span><span> </span><span>if</span><span> (i &gt; 254) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Ones above 254 are from the BPE file. Load those<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span>fscanf(fp, </span><span>"%s %s"</span><span>, a, b);</span></p>
      <p><span><span>&nbsp; &nbsp; </span>strcat((</span><span>char</span><span>*)a, (</span><span>char</span><span>*)b);</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>int</span><span> </span><span>j</span><span> = 0;</span></p>
      <p><span><span>&nbsp; </span></span><span>for</span><span> (</span><span>int</span><span> </span><span>i</span><span> = 0; a[i]; i++) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// UTF8 encoding makes life hard so handle that here </span><span></span></p>
      <p><span><span>&nbsp; &nbsp; &nbsp; </span>bpe[k+j++] = a[i] ^ 196 ? a[i] : a[++i]-128;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>}</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+j++] = 0;</span></p>
      <p><span><span>&nbsp; </span>} </span><span>else</span><span> </span><span>if</span><span> (i &gt; 187) {</span></p>
      <p><span><span>&nbsp; &nbsp; </span></span><span>// Tokens above 187 are the nonprintable asii character from 0-32<span>&nbsp;</span></span><span> </span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k] = i-188;</span></p>
      <p><span><span>&nbsp; &nbsp; </span>bpe[k+1] = 0;</span></p>
      <p><span><span>&nbsp; </span>}</span></p>
      <p><span>}</span></p>
      </div>        
                        
            
      
            <h2>Conclusion</h2>
            
            <p>
              It's really remarkable how you can distill so many
              decades of progress in machine learning to just a few thousand bytes.
              There is essentially nothing missing here from everything you need to run
              any state-of-the-art neural network (except for the actual model weights).
              While I mostly put this together for fun,
              it's a nice demonstration how <i>simple</i> neural networks actually are.
            </p>
      
            
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[QEMU with VirtIO GPU Vulkan Support (216 pts)]]></title>
            <link>https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f</link>
            <guid>42392802</guid>
            <pubDate>Wed, 11 Dec 2024 20:48:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f">https://gist.github.com/peppergrayxyz/fdc9042760273d137dddd3e97034385f</a>, See on <a href="https://news.ycombinator.com/item?id=42392802">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-qemu-vulkan-virtio-md">
    <article itemprop="text">
<p dir="auto">With its latest reales qemu added the Venus patches so that virtio-gpu now support venus encapsulation for vulkan. This is one more piece to the puzzle towards full Vulkan support.</p>
<p dir="auto">An outdated blog post on <a href="https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/" rel="nofollow">clollabora</a> described in 2021 how to enable 3D acceleration of Vulkan applications in QEMU through the Venus experimental Vulkan driver for VirtIO-GPU with a local development environment. Following up on the outdated write up, this is how its done today.</p>
<p dir="auto"><h2 dir="auto">Definitions</h2><a id="user-content-definitions" aria-label="Permalink: Definitions" href="#definitions"></a></p>
<p dir="auto">Let's start with the brief description of the projects mentioned in the post &amp; extend them:</p>
<ul dir="auto">
<li>QEMU is a machine emulator</li>
<li>VirGL is an OpenGL driver for VirtIO-GPU, available in Mesa.</li>
<li>Venus is an experimental Vulkan driver for VirtIO-GPU, also available in Mesa.</li>
<li>Virglrenderer is a library that enables hardware acceleration to VM guests, effectively translating commands from the two drivers just mentioned to either OpenGL or Vulkan.</li>
<li>libvirt is an API for managing platform virtualization</li>
<li>virt-manager is a desktop user interface for managing virtual machines through libvirt</li>
</ul>
<p dir="auto">Merged Patches:</p>
<ul dir="auto">
<li>2024-08-14 <a href="https://gitlab.freedesktop.org/mesa/mesa/-/commit/087e9a96d13155e26987befae78b6ccbb7ae242b" rel="nofollow">venus: make cross-device optional</a> merged in <a href="https://www.phoronix.com/news/Mesa-24.2-Released" rel="nofollow">mesa 24.2</a></li>
<li>2024-11-25 <a href="https://lore.kernel.org/all/20240726235234.228822-1-seanjc@google.com/" rel="nofollow">KVM: Stop grabbing references to PFNMAP'd pages</a> merged in <a href="https://www.phoronix.com/news/Linux-6.13-KVM" rel="nofollow">linux 6.13</a></li>
<li>2024-11-12 <a href="https://lists.gnu.org/archive/html/qemu-devel/2024-08/msg03288.html" rel="nofollow">Support blob memory and venus on qemu</a> merged in <a href="https://www.phoronix.com/news/QEMU-9.2-Released" rel="nofollow">qemu 9.2.0</a></li>
</ul>
<p dir="auto">Work in progress:</p>
<ul dir="auto">
<li>libvirt <a href="https://gitlab.com/libvirt/libvirt/-/issues/638" rel="nofollow">Add support for more virtio-vga-gl arguments #638</a></li>
<li>virt-manager <a href="https://github.com/virt-manager/virt-manager/issues/362" data-hovercard-type="issue" data-hovercard-url="/virt-manager/virt-manager/issues/362/hovercard">Add support for Venus / Vulkan VirtIO-GPU driver #362</a></li>
</ul>
<p dir="auto"><h2 dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto">Make sure you have the proper version installed on the host:</p>
<ul dir="auto">
<li>linux kernel &gt;= 6.13 built with CONFIG_UDMABUF</li>
<li>working Vulkan and kvm setup</li>
<li>qemu &gt;= 9.2.0</li>
</ul>
<p dir="auto">You can verify this like so:</p>
<pre><code>$ uname -r
6.13.0
$ ls /dev/udmabuf
/dev/udmabuf
$ ls /dev/kvm
/dev/kvm
$ qemu-system-x86_64 --version
QEMU emulator version 9.2.0
Copyright (c) 2003-2024 Fabrice Bellard and the QEMU Project developers
</code></pre>
<p dir="auto">For Vulkan to work you need the proper drivers to be installed for your graphics card. To verfiy your setup, install <code>vulkan-tools</code>:</p>
<pre><code>$ vulkaninfo --summary
==========
VULKANINFO
==========

Vulkan Instance Version: ...
...
$ vkcube
Selected GPU x: ..., type: ...
</code></pre>
<p dir="auto"><h4 dir="auto">Building qemu</h4><a id="user-content-building-qemu" aria-label="Permalink: Building qemu" href="#building-qemu"></a></p>
<p dir="auto">If your distro doesn't (yet) ship and updated version of qemu, you can build it yourself from source:</p>
<pre><code>wget https://download.qemu.org/qemu-9.2.0.tar.xz
tar xvJf qemu-9.2.0.tar.xz
cd qemu-9.2.0
mkdir build &amp;&amp; cd build
../configure --target-list=x86_64-softmmu  \
  --enable-kvm                 \
  --enable-opengl              \
  --enable-virglrenderer       \
  --enable-gtk                 \
  --enable-sdl
make -j4
</code></pre>
<p dir="auto">The configuration step will throgh errors if packages are missing. Check the qemu wiki for further info what to install: <a href="https://wiki.qemu.org/Hosts/Linux" rel="nofollow">https://wiki.qemu.org/Hosts/Linux</a></p>
<p dir="auto"><h2 dir="auto">Create and run an image for QEMU</h2><a id="user-content-create-and-run-an-image-for-qemu" aria-label="Permalink: Create and run an image for QEMU" href="#create-and-run-an-image-for-qemu"></a></p>
<p dir="auto">Create an image &amp; fetch the distro of your choice:</p>
<p dir="auto"><h3 dir="auto">Host</h3><a id="user-content-host" aria-label="Permalink: Host" href="#host"></a></p>
<div dir="auto"><pre>ISO=ubuntu-24.10-desktop-amd64.iso  
wget https://releases.ubuntu.com/oracular/ubuntu-24.10-desktop-amd64.iso  

IMG=ubuntu-24-10.qcow2
qemu-img create -f qcow2 <span>$IMG</span> 16G</pre></div>
<p dir="auto">Run a live version or install the distro</p>
<pre><code>qemu-system-x86_64                                               \
    -enable-kvm                                                  \
    -M q35                                                       \
    -smp 4                                                       \
    -m 4G                                                        \
    -cpu host                                                    \
    -net nic,model=virtio                                        \
    -net user,hostfwd=tcp::2222-:22                              \
    -device virtio-vga-gl,hostmem=4G,blob=true,venus=true        \
    -vga none                                                    \
    -display gtk,gl=on,show-cursor=on                            \
    -usb -device usb-tablet                                      \
    -object memory-backend-memfd,id=mem1,size=4G                 \
    -machine memory-backend=mem1                                 \
    -hda $IMG                                                    \
    -cdrom $ISO                                                  
</code></pre>
<p dir="auto">Adjust the parameters accordingly:</p>
<ul dir="auto">
<li>smp: number of cpu cores</li>
<li>m: RAM</li>
<li>hostmem,size: VRAM</li>
</ul>
<p dir="auto"><h3 dir="auto">Guest</h3><a id="user-content-guest" aria-label="Permalink: Guest" href="#guest"></a></p>
<p dir="auto">Install <code>mesa-utilites</code> and <code>vulkan-tools</code> to test the setup:</p>
<pre><code>$ glxinfo -B
</code></pre>
<pre><code>$ vkcube
Selected GPU x: ..., type: ...
</code></pre>
<p dir="auto">If the deive is <code>llvmpipe</code> somehting is wrong. The device should be <code>virgl (...)</code>.</p>
<p dir="auto"><h4 dir="auto">Troubleshooting</h4><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<ul dir="auto">
<li>(host) add <code>-d guest_errors</code> to show error messages from the guest</li>
<li>(guest) try installing vulkan virtio drivers and mesa</li>
<li>check the original <a href="https://www.collabora.com/news-and-blog/blog/2021/11/26/venus-on-qemu-enabling-new-virtual-vulkan-driver/" rel="nofollow">blog post</a></li>
</ul>
<p dir="auto"><h2 dir="auto">virt-manager</h2><a id="user-content-virt-manager" aria-label="Permalink: virt-manager" href="#virt-manager"></a></p>
<p dir="auto">-- work in progress --</p>
<p dir="auto">Currently this is work in progress, so there is no option to add vulkan support in virt-manager. There are no fields to configure this. Also xml doesnt work, because libvirt doesn't know about these options either, so xml validation fails. There is however an option for <a href="https://libvirt.org/kbase/qemu-passthrough-security.html" rel="nofollow">QEMU command-line passthrough</a> which bypasses the validation.</p>
<p dir="auto">If you setup a default machine with 4G of memory, you can do this:</p>
<div dir="auto"><pre>  &lt;<span>qemu</span><span>:</span><span>commandline</span>&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-device<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>virtio-vga-gl,hostmem=4G,blob=true,venus=true<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-object<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>memory-backend-memfd,id=mem1,size=4G<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-machine<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>memory-backend=mem1<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>-vga<span>"</span></span>/&gt;
    &lt;<span>qemu</span><span>:</span><span>arg</span> <span>value</span>=<span><span>"</span>none<span>"</span></span>/&gt;
  &lt;/<span>qemu</span><span>:</span><span>commandline</span>&gt;</pre></div>
<p dir="auto">Which gives this error:</p>
<pre><code>qemu-system-x86_64: virgl could not be initialized: -1
</code></pre>
<p dir="auto">Changing the number from 4G to 4194304k (same as memory) leds to this error:</p>
<pre><code>qemu-system-x86_64: Spice: ../spice-0.15.2/server/red-qxl.cpp:435:spice_qxl_gl_scanout: condition `qxl_state-&gt;gl_draw_cookie == GL_DRAW_COOKIE_INVALID' failed
</code></pre>
<p dir="auto">to be further investigated.</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Instant macOS install on Proxmox including AMD patches (157 pts)]]></title>
            <link>https://github.com/luchina-gabriel/OSX-PROXMOX</link>
            <guid>42392660</guid>
            <pubDate>Wed, 11 Dec 2024 20:36:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/luchina-gabriel/OSX-PROXMOX">https://github.com/luchina-gabriel/OSX-PROXMOX</a>, See on <a href="https://news.ycombinator.com/item?id=42392660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">OSX-PROXMOX - Run macOS on ANY Computer - AMD &amp; Intel</h2><a id="user-content-osx-proxmox---run-macos-on-any-computer---amd--intel" aria-label="Permalink: OSX-PROXMOX - Run macOS on ANY Computer - AMD &amp; Intel" href="#osx-proxmox---run-macos-on-any-computer---amd--intel"></a></p>
<p dir="auto">Install <code>** FRESH/CLEAN **</code> Proxmox VE v7.0.XX ~ 8.2.XX - Next, Next &amp; Finish (NNF).</p>
<p dir="auto">Open Proxmox Web Console -&gt; Datacenter &gt; NAME OF YOUR HOST &gt; Shell.</p>
<p dir="auto">Copy, paste and execute (code below).</p>
<p dir="auto">Voilà, install macOS! This is really and magic <strong>easiest way</strong>!
<a target="_blank" rel="noopener noreferrer" href="https://github.com/luchina-gabriel/OSX-PROXMOX/blob/main/Artefacts/proxmox-screen.png"><img src="https://github.com/luchina-gabriel/OSX-PROXMOX/raw/main/Artefacts/proxmox-screen.png" alt="overview"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">COPY &amp; PASTE - in shell of Proxmox (for Install or Update this solution)</h2><a id="user-content-copy--paste---in-shell-of-proxmox-for-install-or-update-this-solution" aria-label="Permalink: COPY &amp; PASTE - in shell of Proxmox (for Install or Update this solution)" href="#copy--paste---in-shell-of-proxmox-for-install-or-update-this-solution"></a></p>
<div data-snippet-clipboard-copy-content="/bin/bash -c &quot;$(curl -fsSL https://install.osx-proxmox.com)&quot;"><pre><code>/bin/bash -c "$(curl -fsSL https://install.osx-proxmox.com)"
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">For install EFI Package in macOS, first disable Gatekeeper</h2><a id="user-content-for-install-efi-package-in-macos-first-disable-gatekeeper" aria-label="Permalink: For install EFI Package in macOS, first disable Gatekeeper" href="#for-install-efi-package-in-macos-first-disable-gatekeeper"></a></p>
<div data-snippet-clipboard-copy-content="sudo spctl --master-disable"><pre><code>sudo spctl --master-disable
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Versions of macOS Supported</h2><a id="user-content-versions-of-macos-supported" aria-label="Permalink: Versions of macOS Supported" href="#versions-of-macos-supported"></a></p>
<ul dir="auto">
<li>macOS High Sierra - 10.13</li>
<li>macOS Mojave - 10.14</li>
<li>macOS Catalina - 10.15</li>
<li>macOS Big Sur - 11</li>
<li>macOS Monterey - 12</li>
<li>macOS Ventura - 13</li>
<li>macOS Sonoma - 14</li>
<li>macOS Sequoia - 15</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Versions of Proxmox VE Supported</h2><a id="user-content-versions-of-proxmox-ve-supported" aria-label="Permalink: Versions of Proxmox VE Supported" href="#versions-of-proxmox-ve-supported"></a></p>
<ul dir="auto">
<li>v7.0.XX ~ 8.2.XX</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Opencore version</h2><a id="user-content-opencore-version" aria-label="Permalink: Opencore version" href="#opencore-version"></a></p>
<ul dir="auto">
<li>Oct/2024 - 1.0.2 Added support to macOS Sequoia</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cloud Support (Yes, install your Hackintosh in Cloud Environment)</h2><a id="user-content-cloud-support-yes-install-your-hackintosh-in-cloud-environment" aria-label="Permalink: Cloud Support (Yes, install your Hackintosh in Cloud Environment)" href="#cloud-support-yes-install-your-hackintosh-in-cloud-environment"></a></p>
<ul dir="auto">
<li><a href="https://www.vultr.com/?ref=9035565-8H" rel="nofollow">VultR</a></li>
<li><a href="https://youtu.be/8QsMyL-PNrM" rel="nofollow">Vídeo/Tutorial</a>, please activate captions!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<ul dir="auto">
<li>FOR DEV/STUDENT/TEST ONLY PURPOSES.</li>
<li>I'm not responsible for any problem and/or equipment damage or loss of files.</li>
<li>Always back up everything before any changes to your computer.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto">Since Monterey, your host must have a working TSC (timestamp counter), because otherwise if you give the VM more than one core, macOS will observe the skew between cores and <strong>kernel/memory panic</strong> when it sees time ticking backwards. To check this, on Proxmox run:</p>
<div data-snippet-clipboard-copy-content="dmesg | grep -i -e tsc -e clocksource
...
# for working host must be:
...
clocksource: Switched to clocksource tsc
...

# for broken host could be:
tsc: Marking TSC unstable due to check_tsc_sync_source failed
clocksource: Switched to clocksource hpet"><pre><code>dmesg | grep -i -e tsc -e clocksource
...
# for working host must be:
...
clocksource: Switched to clocksource tsc
...

# for broken host could be:
tsc: Marking TSC unstable due to check_tsc_sync_source failed
clocksource: Switched to clocksource hpet
</code></pre></div>
<p dir="auto">Below is a possible workaround from here: <a href="https://www.nicksherlock.com/2022/10/installing-macos-13-ventura-on-proxmox/comment-page-1/#comment-55532" rel="nofollow">https://www.nicksherlock.com/2022/10/installing-macos-13-ventura-on-proxmox/comment-page-1/#comment-55532</a></p>
<ol dir="auto">
<li>Try to turn off “ErP mode” or any C state power saving modes your BIOS supports and poweroff/poweron device (including physical cable). It could help host OS to init TSC correctly, but no guarantee.</li>
<li>Or try to activate TSC force in GRUB by adding boot flags <code>clocksource=tsc tsc=reliable</code> in the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> and call <code>update-grub</code>. In this case host OS probably could work unstable in some cases.</li>
<li>Check the current TSC by call <code>cat /sys/devices/system/clocksource/clocksource0/current_clocksource</code> must be <code>tsc</code>.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">High Siearra and below installation issues</h3><a id="user-content-high-siearra-and-below-installation-issues" aria-label="Permalink: High Siearra and below installation issues" href="#high-siearra-and-below-installation-issues"></a></p>
<p dir="auto">To solve error <em>The Recovery Server Could Not Be Contacted</em> you need to change the protocol from <code>https://</code> to <code>http://</code>. To do this, follow:</p>
<ul dir="auto">
<li>start installation and get error <em>The Recovery Server Could Not Be Contacted</em>, hold the window with error opened</li>
<li>open Window -&gt; Installer Log</li>
<li>search for the line "Failed to load catalog" -&gt; select line in log windows -&gt; Edit -Copy</li>
<li>close the error message and return to <code>macOS Utilities</code> window</li>
<li>open Utilities -&gt; Terminal, right click -&gt; paste</li>
<li>edit the pasted data, remove everything except URL, like <code>https://blablabla.sucatalog</code></li>
<li>change https -&gt; http</li>
<li>adjust the command to be like: nvram IASUCatalogURL=""</li>
<li>press enter, quit Terminal and try to start installation again</li>
</ul>
<p dir="auto">After this, no additional ISO needed, HighSierra must be installed well from recovey.</p>
<p dir="auto">Here a sample how need to change the error message to the final URL:</p>
<p dir="auto"><code>nIUvram IASUCatalogURL="http://swscan.apple.com/content/catalogs/others/index-10.13-10.12-10.11-10.10-10.9-mountainlion-lion-snowleopard-leopard.merged-1.sucatalog"</code></p>
<p dir="auto">The solution took from here: <a href="https://mrmacintosh.com/how-to-fix-the-recovery-server-could-not-be-contacted-error-high-sierra-recovery-is-still-online-but-broken/" rel="nofollow">https://mrmacintosh.com/how-to-fix-the-recovery-server-could-not-be-contacted-error-high-sierra-recovery-is-still-online-but-broken/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demonstration (in Portuguese/Brazil)</h2><a id="user-content-demonstration-in-portuguesebrazil" aria-label="Permalink: Demonstration (in Portuguese/Brazil)" href="#demonstration-in-portuguesebrazil"></a></p>
<p dir="auto"><a href="https://youtu.be/dil6iRWiun0" rel="nofollow">https://youtu.be/dil6iRWiun0</a></p>
<p dir="auto">* Please use CC with Auto Translate to English for your convenience.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<ul dir="auto">
<li>Opencore/Acidanthera Team</li>
<li>Corpnewt for Applications (ProperTree, genSMBIOS, etc)</li>
<li>Apple for macOS</li>
<li>Proxmox - Excelent and better documentation for Virtualization</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Discord - Universo Hackintosh</h2><a id="user-content-discord---universo-hackintosh" aria-label="Permalink: Discord - Universo Hackintosh" href="#discord---universo-hackintosh"></a></p>
<ul dir="auto">
<li><a href="https://discord.universohackintosh.com.br/" rel="nofollow">Discord</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI Guesses Your Accent (189 pts)]]></title>
            <link>https://start.boldvoice.com/accent-guesser</link>
            <guid>42392088</guid>
            <pubDate>Wed, 11 Dec 2024 19:54:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://start.boldvoice.com/accent-guesser">https://start.boldvoice.com/accent-guesser</a>, See on <a href="https://news.ycombinator.com/item?id=42392088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><div><p><img src="https://start.boldvoice.com/build/_assets/OracleWaveAudioFuzzy-OOCGQGMB.png" alt="OracleWaveAudioFuzzy" width="100%" height="100%"></p></div><div><p>The <span>Accent</span> Oracle</p><p>Do you have an accent when speaking English? I bet I can guess your native language in less than 30 seconds.</p></div></div><div><p><img src="https://start.boldvoice.com/build/_assets/GlobeLanguages-6JQ4ADYP.svg" alt="Globe Languages"></p></div></div><div><p>© 2024 BoldVoice. All rights reserved.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mysterious New Jersey drone sightings prompt call for 'state of emergency' (373 pts)]]></title>
            <link>https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency</link>
            <guid>42391443</guid>
            <pubDate>Wed, 11 Dec 2024 19:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency">https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency</a>, See on <a href="https://news.ycombinator.com/item?id=42391443">Hacker News</a></p>
Couldn't get https://www.theguardian.com/us-news/2024/dec/11/new-jersey-drone-sightings-state-of-emergency: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[2400 phone providers may be shut down by the FCC for failing to stop robocalls (395 pts)]]></title>
            <link>https://docs.fcc.gov/public/attachments/DOC-408083A1.txt</link>
            <guid>42391203</guid>
            <pubDate>Wed, 11 Dec 2024 18:41:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.fcc.gov/public/attachments/DOC-408083A1.txt">https://docs.fcc.gov/public/attachments/DOC-408083A1.txt</a>, See on <a href="https://news.ycombinator.com/item?id=42391203">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[X41 Reviewed Mullvad VPN (394 pts)]]></title>
            <link>https://x41-dsec.de/news/2024/12/11/mullvad/</link>
            <guid>42390768</guid>
            <pubDate>Wed, 11 Dec 2024 18:08:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://x41-dsec.de/news/2024/12/11/mullvad/">https://x41-dsec.de/news/2024/12/11/mullvad/</a>, See on <a href="https://news.ycombinator.com/item?id=42390768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <h2 id="review-of-mullvad-vpn">Review of Mullvad VPN</h2>

<p>X41 performed a white box penetration test with source code access against the <a href="https://mullvad.net/">Mullvad</a> VPN Application. The efforts included formulating a light threat model.</p>

<p>The targets of this test were challenging for the team because of its size, the fact that they run on five different platforms (Linux, Windows, macOS, Android, and iOS), and the regular audits performed by Mullvad VPN. The fact that new vulnerabilities were found in existing code shows that the efforts taken regularly by Mullvad are justified and appropriate for product of such complexity.</p>

<p>It also shows that in mature targets the findings tend to move into domains not under direct control or in direct focus of the application development as can be seen in the findings rooting from specifics of the operating system’s behavior or the interplay of different network layers and protocols.</p>

<p>This is what keeps security audits and tests of mature and hard targets interesting for the team at X41 as well.</p>

<h2 id="results">Results</h2>

<p>A total of six vulnerabilities were discovered during the test by X41.</p>

<p>Overall, the Mullvad VPN Applications appear to have a high security level and are well positioned to protect from the threat model proposed in our report. The use of safe coding and design patterns in combination with regular audits and penetration tests led to a very hardened environment.</p>

<p>The most serious vulnerabilities are considered to be race conditions and temporal safety violations leading to memory corruption issues in the signal handler code. While exploitation of the signal handler code once triggered seems not unlikely, the fact that an attacker first needs to trigger a signal via another fault reduces the severity of the issues. Other vulnerabilities allow leaking information about the identity of a user by network adjacent attackers and to perform side channel attacks that could in specific circumstances reveal which site a client is currently accessing.</p>

<p>The aspect of side channel attacks is mitigated in most parts, except for protocol level attacks that are not within the control of Mullvad VPN AB because they root from a combination of different technologies such as NAT and modern variants of the HTTP protocol. The introduction of obfuscation technologies and proxy  services within the protected VPN is an option for users with higher security and privacy demands.</p>

<p>In conclusion, the client applications exposed a limited number of relevant vulnerabilities. Mullvad VPN AB addressed them swiftly and the fixes were audited to be working properly.</p>

<p>X41 would like to thank Mullvad VPN AB for the nice collaboration and smooth communication throughout the audit!</p>

<h3 id="findings">Findings</h3>

<p>Mullvad’s <a href="https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available">announcement</a> about the audit covers each of the findings and their mitigations. The technical details can be found in our <a href="https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf">report</a>, which we are releasing today.</p>

<h3 id="links">Links</h3>

<p>Full report:<br>
<a href="https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf">https://x41-dsec.de/static/reports/X41-Mullvad-Audit-Public-Report-2024-12-10.pdf</a></p>

<p>Mullvad announcement:<br>
<a href="https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available">https://mullvad.net/en/blog/the-report-for-the-2024-security-audit-of-the-app-is-now-available</a></p>

<p>Mullvad’s previous audits:<br>
<a href="https://github.com/mullvad/mullvadvpn-app/tree/main/audits">https://github.com/mullvad/mullvadvpn-app/tree/main/audits</a></p>

<hr>

<p>If you are interested in working with us on such projects in the future, remote or in-office, have a look at our <a href="https://x41-dsec.de/jobs/">jobs</a> page!</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Azalea Robotics (YC S24) – Baggage-handling robots for airports (117 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42390761</link>
            <guid>42390761</guid>
            <pubDate>Wed, 11 Dec 2024 18:07:11 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42390761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="42390761">
      <td><span></span></td>      <td><center><a id="up_42390761" href="https://news.ycombinator.com/vote?id=42390761&amp;how=up&amp;goto=item%3Fid%3D42390761"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=42390761">Launch HN: Azalea Robotics (YC S24) – Baggage-handling robots for airports</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_42390761">101 points</span> by <a href="https://news.ycombinator.com/user?id=dmillard">dmillard</a> <span title="2024-12-11T18:07:11 1733940431"><a href="https://news.ycombinator.com/item?id=42390761">13 hours ago</a></span> <span id="unv_42390761"></span> | <a href="https://news.ycombinator.com/hide?id=42390761&amp;goto=item%3Fid%3D42390761">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Azalea%20Robotics%20%28YC%20S24%29%20%E2%80%93%20Baggage-handling%20robots%20for%20airports&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=42390761&amp;auth=e0d246a6e8016d06b58f3062ec36f765256d3508">favorite</a> | <a href="https://news.ycombinator.com/item?id=42390761">61&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN! We’re David and John B, cofounders of Azalea Robotics (<a href="https://www.azalearobotics.com/" rel="nofollow">https://www.azalearobotics.com</a>). We build robots to handle passenger baggage in airports. Here are some videos to give you the idea:</p><p>Unedited autonomous ops: <a href="https://www.youtube.com/watch?v=DuJ3ZORnO1o" rel="nofollow">https://www.youtube.com/watch?v=DuJ3ZORnO1o</a></p><p>Teleoperated (sped up, so no sound): <a href="https://www.youtube.com/watch?v=LeK8NQLnYgA" rel="nofollow">https://www.youtube.com/watch?v=LeK8NQLnYgA</a></p><p>The marketing version: <a href="https://www.youtube.com/watch?v=k0SDPm09U6s" rel="nofollow">https://www.youtube.com/watch?v=k0SDPm09U6s</a></p><p>Robotics is in an interesting place right now, with many warehouse automation companies humming along for almost a decade, and a lot of new effort going to full general purpose hardware with humanoids and software via generalist robotics foundation models. We love these efforts (David used to work on one at Google X with Everyday Robots), but we also see a lot of utility in the current wave of robotics planning and perception tech that can enable new use cases today.</p><p>Airlines in the US compete primarily on efficiency and customer loyalty, and baggage handling hits both (John B. has first-hand experience from working on baggage optimization projects at United Airlines). 2% of flights are delayed by baggage errors, leading to downstream network delays. Baggage handling is also a major complaint in customer experience—almost everyone has a horror story of a missing bag, and sometimes people vow never to fly an airline again for losing their belongings. Furthermore, it’s a really dangerous job for employees from a repetitive stress standpoint. EU regulation is coming to reflect this, protecting workers with a maximum number of bags transferred per shift to alleviate back and tendon injuries that are inherent to this job.</p><p>Unfortunately for airlines, passengers don’t package their luggage in nicely uniform cardboard boxes. If they did, then the airlines could benefit directly from the recent takeoff in manipulator tech for warehouses. But airline luggage is way more wacky and irregular. If robots are going to handle it, they need to reason about how to grasp each item, handle its deformability, stack it in a stable way, and do all of this quickly, safely, and reliably.</p><p>This is what we’re tackling at Azalea. We’re bringing our expertise in deformable object manipulation, perception, robot learning, and planning, to this logistical problem.</p><p>We have a few strong bets behind what we’re working on: (1) The hardware to solve this problem has been available or manufacturable for decades, what’s been missing is perception, planning, and control. (2) Cobots, robots designed to operate alongside humans, aren’t enough for safety. To do this task efficiently, you need to move up to 50 kg bags very quickly, which can be dangerous no matter how well the cobots are designed. Light curtains (arrays of lasers that stop a machine when interrupted) and machine cages are the current industrial standard and remain the way to go. (3) Software for generalist robots needs more data than most people today believe, and it will be at least 15 years before deployment: we should focus on specialized problems of economic value.</p><p>Our core technical developments are in a few areas:</p><p>- Grasp synthesis and selection: From visual data only, how can we identify good candidate grasp points and rank them? For this, we use a mix of physical reasoning, heuristics, and a lot of learning from previous data, combined in a single objective function. Furthermore, success must be evaluated as both a successful grasp and continual hold throughout the transfer.</p><p>- Placement planning: How do we lay out luggage in the module we’re loading? There’s a nice ramp-up in difficulty for this problem, from open-loop “divide the world into a grid” approaches, to 3d bin-packing optimization, to reinforcement learning. An interesting aspect of this problem for us is that the bags should be physically stable when the cart starts driving, and lighter, deformable objects shouldn’t be underneath heavy, hard objects. We use a similar mix of physics and learning to model this problem.</p><p>- Fast collision-free planning: Off the shelf planners work great for the most part but can fail in heavily cluttered areas or dynamic scenes. We leverage the fact that we’re always solving a series of similar problems to provide initial guesses for downstream trajectory optimization algorithms. Since each problem is so similar, we can use techniques similar to generative models to propose these initial plans.</p><p>- Mechanical design: The perfect tool to pick up everything checked down a conveyor belt isn’t an easy thing to design. We’re building tools with multiple modes of grasping to handle wide varieties of objects. The videos we linked to are all with suction only – which can be surprisingly powerful! An interesting aspect of autonomy becomes choosing which mode to use when, and how to use it.</p><p>These problems can be deeply interlinked: where you grasp an object depends on what your tooling looks like and informs where you can put it– so a perfect solution would jointly reason about both problems simultaneously. We’re looking forward to getting there as we collect more data and continue our efforts.</p><p>Check out our demo videos above! We have a brand new hardware stack coming soon (and we’ve added a new end effector that we’re keeping hush), but it’s amazing what you can do with pure suction.</p><p>We’re proud of our progress so far but would love to hear your thoughts and feedback. Let us know if you’ve had a particularly bad baggage horror story and/or have personal experience with the industry.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WordPress CEO quits community Slack after court injunction (136 pts)]]></title>
            <link>https://www.404media.co/wordpress-wp-engine-preliminary-injunction/</link>
            <guid>42390709</guid>
            <pubDate>Wed, 11 Dec 2024 18:02:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.404media.co/wordpress-wp-engine-preliminary-injunction/">https://www.404media.co/wordpress-wp-engine-preliminary-injunction/</a>, See on <a href="https://news.ycombinator.com/item?id=42390709">Hacker News</a></p>
Couldn't get https://www.404media.co/wordpress-wp-engine-preliminary-injunction/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[FCC Opens Entire 6 GHz Band to Low Power Device Operations (644 pts)]]></title>
            <link>https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations</link>
            <guid>42390344</guid>
            <pubDate>Wed, 11 Dec 2024 17:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations">https://www.fcc.gov/document/fcc-opens-entire-6-ghz-band-very-low-power-device-operations</a>, See on <a href="https://news.ycombinator.com/item?id=42390344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-main-canvas="">
        
				
	
        <header>
  


<nav aria-labelledby="main-navigation">
  <div id="main-navigation">

          <div>
          
  <p><a href="https://www.fcc.gov/" aria-label="" id="logo">
            <img src="https://www.fcc.gov/themes/custom/fcc/logo.svg" width="175" height="auto" alt="Federal Communications Commission logo">

        
  </a>

  
  </p>


      </div>
    
          <div aria-label="Main Navigation">
        <nav aria-labelledby="browse-by">
          <div role="tablist" id="mainNavbar">
              <ul id="browse-by" role="tablist">
                <li>
                  <a id="nav-category-tab" data-bs-toggle="tab" href="#nav-category" role="tab" aria-controls="nav-category" aria-selected="true" aria-haspopup="true" aria-expanded="true">
                    <p>Browse by</p>
                    <p>category</p>
                    
                  </a>
                </li>
                <li>
                  <a id="nav-bureaus-and-offices-tab" data-bs-toggle="tab" href="#nav-bureaus-and-offices" role="tab" aria-controls="nav-bureaus-and-offices" aria-selected="false" aria-haspopup="true" aria-expanded="false">
                    <p>Browse by</p>
                    <p>bureaus &amp; offices</p>
                    
                  </a>
                </li>
              </ul>
              </div>
        </nav>
      </div>
    
                  
          
  </div>
</nav>
</header>
  
        
  

  <main role="main">
    <div>
          <article>
  
  

    <div>
      <div>
        <ul>
                      <li>
  <div>
    <p>
      Full Title<span>:</span>    </p>
                  <p>FCC Opens Entire 6 GHz Band To Very Low Power Device Operations</p>
              </div>
</li>
                                <li>
        
      
  </li>
                                <li>
  
</li>
                                <li>
  <div>
    <p>
      Description    </p>
                  <p>FCC Opens Entire 6 GHz Band To Very Low Power Device Operations</p>
              </div>
</li>
                                                                                                    </ul>

        
                  <div>
  <h3>Files</h3>
    
  

  </div>
              </div>
      <div>
            <h4>Document Dates</h4>
            <ul>
                              <li>
  <div>
    <p>
      Released On<span>:</span>    </p>
                  <p><time datetime="2024-12-11T12:00:00Z">Dec 11, 2024</time>
</p>
              </div>
</li>
                                                          <li>
  <div>
    <p>
      Adopted Date<span>:</span>    </p>
                  <p><time datetime="2024-12-11T12:00:00Z">Dec 11, 2024</time>
</p>
              </div>
</li>
                                            <li>
  
</li>
                                                                                                </ul>
          </div>
    </div>
</article>


      </div>
  </main>

        
  
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Estimated concentrations of atrazine in agricultural groundwater (143 pts)]]></title>
            <link>https://water.usgs.gov/nawqa/pnsp/features/feature.php</link>
            <guid>42390236</guid>
            <pubDate>Wed, 11 Dec 2024 17:25:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://water.usgs.gov/nawqa/pnsp/features/feature.php">https://water.usgs.gov/nawqa/pnsp/features/feature.php</a>, See on <a href="https://news.ycombinator.com/item?id=42390236">Hacker News</a></p>
Couldn't get https://water.usgs.gov/nawqa/pnsp/features/feature.php: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[OnlyFans models are using AI impersonators to keep up with their DMs (305 pts)]]></title>
            <link>https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/</link>
            <guid>42390210</guid>
            <pubDate>Wed, 11 Dec 2024 17:23:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/">https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/</a>, See on <a href="https://news.ycombinator.com/item?id=42390210">Hacker News</a></p>
Couldn't get https://www.wired.com/story/onlyfans-models-are-using-ai-impersonators-to-keep-up-with-their-dms/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[RISC-V HiFive Premier P550 Development Boards with Ubuntu Now Available (158 pts)]]></title>
            <link>https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu</link>
            <guid>42389532</guid>
            <pubDate>Wed, 11 Dec 2024 16:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu">https://www.sifive.com/blog/hifive-premier-p550-development-boards-with-ubuntu</a>, See on <a href="https://news.ycombinator.com/item?id=42389532">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>With strong initial reviews, the  <a href="https://www.sifive.com/boards/hifive-premier-p550">HiFive Premier P550 Development Boards</a> with pre-installed Ubuntu are in stock and available for purchase at <a href="https://www.arrow.com/en/products/hf106-001/sifive-inc" target="_blank" rel="noopener">Arrow.com</a>. First launched in October, the Yocto-based Early Access edition of the HiFive Premier P550 sold out rapidly as developers eagerly embraced it to test their RISC-V designs on real silicon. Today, we’re excited to announce the general availability of the Premier P550 in both <a href="https://www.arrow.com/en/products/hf106/sifive-inc" target="_blank" rel="noopener">16GB</a> and <a href="https://www.arrow.com/en/products/hf106-001/sifive-inc" target="_blank" rel="noopener">32GB</a> configurations. This release offers even more flexibility and power, featuring pre-installed Ubuntu 24.04 to help developers make the most of their RISC-V projects.</p>
<p><strong>Lower Price, Greater Accessibility</strong>
Due to the boards' initial popularity, we worked closely with our manufacturing partner, ESWIN to ramp up production. As a result of increased production and economies of scale, we’re excited to announce we are able to lower the price to just $399 for the 16GB version and $499 for the 32GB version, making this powerful development board more accessible to a wider range of developers, enthusiasts, and designers.</p>
<p>For those who acted quickly and purchased a board before the price drop, we want to thank you for your support. We’ll be issuing a refund for the price difference to show our appreciation. You will receive an email from Arrow explaining next steps.</p>
<p><strong>Rave Reviews and Feedback</strong>
Early feedback from users has been overwhelmingly positive. It’s been exciting to see how developers and build farms are leveraging the board’s capabilities—from testing software to running video games—pushing the board to its limits. We’d love to hear your thoughts as well. Feel free to send your feedback to HighFiveboards@sifive.com, or share your project with us on social media. Tag us or send in a video to showcase how you're using the HiFive Premier P550.</p>
<p>As we mentioned during the product launch, our goal is to get these boards into the hands of as many developers as possible to help accelerate the growth of the RISC-V ecosystem. Early users have praised the board’s smooth out-of-the-box experience, thorough testing and certification, and premium features. The boards are built with high quality components including powerful Samsung and Micron LPDDR memory, System on Module (SOM) for modularity and upgradeability, and an onboard Baseboard Management Controller ( BMC) offering remote management and control without having physical access to the board, and much more.</p>
<p><strong>Built on Strong Engineering Collaboration</strong>
The HiFive Premier P550 Development Board’s uniqueness stems from the close collaboration between SiFive, ESWIN, and Canonical to ensure the board performs to specification. This strong partnership is helping drive the RISC-V ecosystem forward, and we’re thrilled to see these boards gaining traction.</p>
<p>“We’re excited to see these boards rapidly proliferate into the market,” said Bo Wang, Vice Chairman of <a href="https://www.eswincomputing.com/en/" target="_blank" rel="noopener">ESWIN Computing</a>. “We are pleased to be able to collaborate with SiFive, the industry leader in RISC-V, and we look forward to future products.”</p>
<p>“<a href="https://canonical.com/" target="_blank" rel="noopener">Canonical</a> is deeply committed to RISC-V and creating the best software environment possible for developers coming into this ecosystem. The HiFive Premier P550 is set to be the de facto development platform, and with Ubuntu coming pre-installed on the board we are excited to see the platform used by innovators and developers,” said Gordan Markus, Silicon Alliances Director, Canonical.</p>
<p><strong>A Strategic Investment in RISC-V’s Future</strong>
These boards represent a long-term investment in accelerating the global adoption of RISC-V  and are not intended as a revenue driver.  As the RISC-V ecosystem grows at an exponential pace, having access to actual silicon for hands-on development is essential. We’re committed to supporting this growth, and you can expect many more boards to follow in the months ahead.</p>
<p>We believe this investment benefits you too. With the new, lower price, Ubuntu support, and availability through Arrow.com, there’s never been a better time to dive into RISC-V development. Whether you're building a prototype, exploring new software, or experimenting with hardware, the HiFive Premier P550 offers an unparalleled platform. And with the confidence that it’s built on SiFive’s proven IP, you can be sure you’re working with the best.</p>
<p>We look forward to hearing how you’re using these boards and can’t wait to share more exciting stories from the growing RISC-V community.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pgroll – Zero-downtime, reversible, schema changes for PostgreSQL (new website) (250 pts)]]></title>
            <link>https://pgroll.com/</link>
            <guid>42388973</guid>
            <pubDate>Wed, 11 Dec 2024 15:51:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pgroll.com/">https://pgroll.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42388973">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Trillium TPU Is GA (154 pts)]]></title>
            <link>https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga</link>
            <guid>42388901</guid>
            <pubDate>Wed, 11 Dec 2024 15:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga">https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga</a>, See on <a href="https://news.ycombinator.com/item?id=42388901">Hacker News</a></p>
Couldn't get https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Dear OAuth Providers (318 pts)]]></title>
            <link>https://pilcrowonpaper.com/blog/dear-oauth-providers/</link>
            <guid>42388870</guid>
            <pubDate>Wed, 11 Dec 2024 15:42:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pilcrowonpaper.com/blog/dear-oauth-providers/">https://pilcrowonpaper.com/blog/dear-oauth-providers/</a>, See on <a href="https://news.ycombinator.com/item?id=42388870">Hacker News</a></p>
Couldn't get https://pilcrowonpaper.com/blog/dear-oauth-providers/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini 2.0: our new AI model for the agentic era (874 pts)]]></title>
            <link>https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/</link>
            <guid>42388783</guid>
            <pubDate>Wed, 11 Dec 2024 15:33:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/</a>, See on <a href="https://news.ycombinator.com/item?id=42388783">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
  }">
  
  <div>
          
            <p>Dec 11, 2024</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
  
  <div data-summary-id="ai_summary_2" data-component="uni-ai-generated-summary" data-analytics-module="{
    &quot;event&quot;: &quot;module_impression&quot;,
    &quot;module_name&quot;: &quot;ai_summary&quot;,
    &quot;section_header&quot;: &quot;CTA&quot;
  }">
          <h2>Bullet points</h2>
          <ul>
<li>Google DeepMind introduces Gemini 2.0, a new AI model designed for the "agentic era."</li>
<li>Gemini 2.0 is more capable than previous versions, with native image and audio output and tool use.</li>
<li>Gemini 2.0 Flash is available to developers and trusted testers, with wider availability planned for early next year.</li>
<li>Google is exploring agentic experiences with Gemini 2.0, including Project Astra, Project Mariner, and Jules.</li>
<li>Google is committed to building AI responsibly, with safety and security as key priorities.</li>
</ul>
          
          <p><small>
            Summaries were generated by Google AI. Generative AI is experimental.
          </small>
        </p></div>
</div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/blog_gemini_keyword_header.width-1200.format-webp.webp" fetchpriority="high" alt="Text &quot;Gemini 2.0&quot; in front of a futuristic blue and black abstract background">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div>
        
          
            <div data-component="uni-article-jumplinks" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Jumplinks&quot;,
    &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
  }">
  <nav aria-label="Article Jumplinks">
    <p><span>In this story</span>
    </p>
    
    
    <div>
      <ul id="article-jumplinks__list">
        
        <li>
          <a aria-label="link to A message from our CEO" href="#ceo-message" id="ceo-message-anchor">A message from our CEO</a>
        </li>
        
        <li>
          <a aria-label="link to Introducing Gemini 2.0" href="#gemini-2-0" id="gemini-2-0-anchor">Introducing Gemini 2.0</a>
        </li>
        
        <li>
          <a aria-label="link to Gemini 2.0 Flash" href="#gemini-2-0-flash" id="gemini-2-0-flash-anchor">Gemini 2.0 Flash</a>
        </li>
        
        <li>
          <a aria-label="link to Project Astra" href="#project-astra" id="project-astra-anchor">Project Astra</a>
        </li>
        
        <li>
          <a aria-label="link to Project Mariner" href="#project-mariner" id="project-mariner-anchor">Project Mariner</a>
        </li>
        
        <li>
          <a aria-label="link to Agents for developers" href="#agents-for-developers" id="agents-for-developers-anchor">Agents for developers</a>
        </li>
        
        <li>
          <a aria-label="link to Agents in games" href="#ai-game-agents" id="ai-game-agents-anchor">Agents in games</a>
        </li>
        
        <li>
          <a aria-label="link to Building responsibly" href="#building-responsibly" id="building-responsibly-anchor">Building responsibly</a>
        </li>
        
      </ul>
    </div>
    
  </nav>
</div>
          
          
          <div data-reading-time="true" data-component="uni-article-body">

            
              


<google-read-aloud-player data-analytics-module="{
        &quot;event&quot;: &quot;module_impression&quot;,
        &quot;module_name&quot;: &quot;ai_audio&quot;,
        &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
    }" data-date-modified="2024-12-11T15:30:20.983616+00:00" data-progress-bar-style="half-wave" data-api-key="AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac" data-article-style="style9" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-layout-style="style1" data-highlight-mode="word-over-paragraph" data-highlight-text-color="#000000" data-highlight-word-background="#8AB4F8" data-highlight-paragraph-background="#D2E3FC" data-background="linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)" data-foreground-color="#202124" data-font="600 16px Google Sans, sans-serif" data-box-shadow="0px 1px 3px 1px rgba(60, 64, 67, 0.15)">
</google-read-aloud-player>




            

            
            
<!--article text-->

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="99isd"><b>A note from Google and Alphabet CEO Sundar Pichai:</b></p><p data-block-key="efphd">Information is at the core of human progress. It’s why we’ve focused for more than 26 years on our mission to organize the world’s information and make it accessible and useful. And it’s why we continue to push the frontiers of AI to organize that information across every input and make it accessible via any output, so that it can be truly useful for you.</p><p data-block-key="4oebp">That was our vision when <a href="https://blog.google/technology/ai/google-gemini-ai/">we introduced Gemini 1.0 last December</a>. The first model built to be natively multimodal, Gemini 1.0 and 1.5 drove big advances with multimodality and long context to understand information across text, video, images, audio and code, and process a lot more of it.</p><p data-block-key="2huik">Now millions of developers are building with Gemini. And it’s helping us reimagine all of our products — including all 7 of them with 2 billion users — and to create new ones. <a href="https://notebooklm.google/">NotebookLM</a> is a great example of what multimodality and long context can enable for people, and why it’s loved by so many.</p><p data-block-key="60rf2">Over the last year, we have been investing in developing more agentic models, meaning they can understand more about the world around you, think multiple steps ahead, and take action on your behalf, with your supervision.</p><p data-block-key="ejiii">Today we’re excited to launch our next era of models built for this new agentic era: introducing Gemini 2.0, our most capable model yet. With new advances in multimodality — like native image and audio output — and native tool use, it will enable us to build new AI agents that bring us closer to our vision of a universal assistant.</p><p data-block-key="bh7ok">We’re getting 2.0 into the hands of developers and trusted testers today. And we’re working quickly to get it into our products, leading with Gemini and Search. Starting today our Gemini 2.0 Flash experimental model will be available to all Gemini users. We're also launching a new feature called <a href="https://blog.google/products/gemini/google-gemini-deep-research/">Deep Research</a>, which uses advanced reasoning and long context capabilities to act as a research assistant, exploring complex topics and compiling reports on your behalf. It's available in Gemini Advanced today.</p><p data-block-key="eqmfh">No product has been transformed more by AI than Search. Our AI Overviews now reach 1 billion people, enabling them to ask entirely new types of questions — quickly becoming one of our most popular Search features ever. As a next step, we’re bringing the advanced reasoning capabilities of Gemini 2.0 to AI Overviews to tackle more complex topics and multi-step questions, including advanced math equations, multimodal queries and coding. We started limited testing this week and will be rolling it out more broadly early next year. And we’ll continue to bring AI Overviews to more countries and languages over the next year.</p><p data-block-key="aaa8b">2.0’s advances are underpinned by decade-long investments in our differentiated full-stack approach to AI innovation. It’s built on custom hardware like Trillium, our sixth-generation TPUs. TPUs powered 100% of Gemini 2.0 training and inference, and today Trillium is <a href="https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga">generally available</a> to customers so they can build with it too.</p><div data-block-key="7gbvh"><p>If Gemini 1.0 was about organizing and understanding information, Gemini 2.0 is about making it much more useful. I can’t wait to see what this next era brings.</p><p>-Sundar</p></div><hr></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="99isd">Introducing Gemini 2.0: our new AI model for the agentic era</h2><p data-block-key="7o7kh"><i>By Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind on behalf of the Gemini team</i></p><p data-block-key="ecj6n">Over the past year, we have continued to make incredible progress in artificial intelligence. Today, we are releasing the first model in the Gemini 2.0 family of models: an experimental version of Gemini 2.0 Flash. It’s our workhorse model with low latency and enhanced performance at the cutting edge of our technology, at scale.</p><p data-block-key="6o7oe">We are also sharing the frontiers of our agentic research by showcasing prototypes enabled by Gemini 2.0’s native multimodal capabilities.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="99isd">Gemini 2.0 Flash</h2><p data-block-key="7lrrf">Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
          }">
    

    <p><img alt="A chart comparing Gemini models and their capabilities" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_benchmarks_narrow_light2x.gif">
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="99isd">Our goal is to get our models into people’s hands safely and quickly. Over the past month, we’ve been sharing early, experimental versions of Gemini 2.0, getting great feedback from developers.</p><p data-block-key="964c">Gemini 2.0 Flash is available now as an experimental model to developers via the Gemini API in <a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-exp">Google AI Studio</a> and <a href="https://cloud.google.com/generative-ai-studio">Vertex AI</a> with multimodal input and text output available to all developers, and text-to-speech and native image generation available to early-access partners. General availability will follow in January, along with more model sizes.</p><p data-block-key="a4e0l">To help developers build dynamic and interactive applications, we’re also releasing a new Multimodal Live API that has real-time audio, video-streaming input and the ability to use multiple, combined tools. More information about 2.0 Flash and the Multimodal Live API can be found in our <a href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">developer blog</a>.</p><h3 data-block-key="dsvak">Gemini 2.0 available in Gemini app, our AI assistant</h3><p data-block-key="en32v">Also starting today, <a href="https://gemini.google.com/">Gemini</a> users globally can access a chat optimized version of 2.0 Flash experimental by selecting it in the model drop-down on desktop and mobile web and it will be available in the Gemini mobile app soon. With this new model, users can experience an even more helpful Gemini assistant.</p><p data-block-key="ea033">Early next year, we’ll expand Gemini 2.0 to more Google products.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="ju7ol">Unlocking agentic experiences with Gemini 2.0</h2><p data-block-key="78rfe">Gemini 2.0 Flash’s native user interface action-capabilities, along with other improvements like multimodal reasoning, long context understanding, complex instruction following and planning, compositional function-calling, native tool use and improved latency, all work in concert to enable a new class of agentic experiences.</p><p data-block-key="3k97r">The practical application of AI agents is a research area full of exciting possibilities. We’re exploring this new frontier with a series of prototypes that can help people accomplish tasks and get things done. These include an update to Project Astra, our research prototype exploring future capabilities of a universal AI assistant; the new Project Mariner, which explores the future of human-agent interaction, starting with your browser; and Jules, an AI-powered code agent that can help developers.</p><p data-block-key="c8gij">We’re still in the early stages of development, but we’re excited to see how trusted testers use these new capabilities and what lessons we can learn, so we can make them more widely available in products in the future.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="Fs0t6SdODd8" data-index-id="10" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Gemini 2.0 supercut video" src="https://i.ytimg.com/vi_webp/Fs0t6SdODd8/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/Fs0t6SdODd8/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/Fs0t6SdODd8/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="ju7ol">Project Astra: agents using multimodal understanding in the real world</h2><p data-block-key="fdk7b">Since we introduced <a href="https://deepmind.google/technologies/gemini/project-astra/">Project Astra</a> at I/O, we’ve been learning from trusted testers using it on Android phones. Their valuable feedback has helped us better understand how a universal AI assistant could work in practice, including implications for safety and ethics. Improvements in the latest version built with Gemini 2.0 include:</p><ul><li data-block-key="4arno"><b>Better dialogue:</b> Project Astra now has the ability to converse in multiple languages and in mixed languages, with a better understanding of accents and uncommon words.</li><li data-block-key="1f3oh"><b>New tool use:</b> With Gemini 2.0, Project Astra can use Google Search, Lens and Maps, making it more useful as an assistant in your everyday life.</li><li data-block-key="9f826"><b>Better memory:</b> We’ve improved Project Astra’s ability to remember things while keeping you in control. It now has up to 10 minutes of in-session memory and can remember more conversations you had with it in the past, so it is better personalized to you.</li><li data-block-key="68bh5"><b>Improved latency:</b> With new streaming capabilities and native audio understanding, the agent can understand language at about the latency of human conversation.</li></ul><p data-block-key="4qful">We’re working to bring these types of capabilities to Google products like <a href="http://gemini.google.com/">Gemini</a> app, our AI assistant, and to other form factors like glasses. And we’re starting to expand our trusted tester program to more people, including a small group that will soon begin testing Project Astra on prototype glasses.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="hIIlJt8JERI" data-index-id="13" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Project Astra demo video" src="https://i.ytimg.com/vi_webp/hIIlJt8JERI/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/hIIlJt8JERI/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/hIIlJt8JERI/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Project Mariner: agents that can help you accomplish complex tasks</h2><p data-block-key="b0b0b">Project Mariner is an early research prototype built with Gemini 2.0 that explores the future of human-agent interaction, starting with your browser. As a research prototype, it’s able to understand and reason across information in your browser screen, including pixels and web elements like text, code, images and forms, and then uses that information via an experimental Chrome extension to complete tasks for you.</p><p data-block-key="d0nh0">When evaluated against the <a href="https://arxiv.org/abs/2401.13919">WebVoyager benchmark</a>, which tests agent performance on end-to-end real world web tasks, Project Mariner <a href="http://deepmind.google/technologies/project-mariner">achieved a state-of-the-art result of 83.5%</a> working as a single agent setup.</p><p data-block-key="8mvv5">It’s still early, but Project Mariner shows that it’s becoming technically possible to navigate within a browser, even though it’s not always accurate and slow to complete tasks today, which will improve rapidly over time.</p><p data-block-key="2n0oq">To build this safely and responsibly, we’re conducting active research on new types of risks and mitigations, while keeping humans in the loop. For example, Project Mariner can only type, scroll or click in the active tab on your browser and it asks users for final confirmation before taking certain sensitive actions, like purchasing something.</p><p data-block-key="ch96g">Trusted testers are starting to test Project Mariner using an experimental Chrome extension now, and we’re beginning conversations with the web ecosystem in parallel.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="2XJqLPqHtyo" data-index-id="16" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Mariner demo video" src="https://i.ytimg.com/vi_webp/2XJqLPqHtyo/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/2XJqLPqHtyo/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/2XJqLPqHtyo/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Jules: agents for developers</h2><p data-block-key="216k6">Next, we’re exploring how AI agents can assist developers with Jules — an experimental AI-powered code agent that integrates directly into a GitHub workflow. It can tackle an issue, develop a plan and execute it, all under a developer’s direction and supervision. This effort is part of our long-term goal of building AI agents that are helpful in all domains, including coding.</p><p data-block-key="acjnm">More information about this ongoing experiment can be found in our <a href="https://developers.googleblog.com/en/the-next-chapter-of-the-gemini-era-for-developers/">developer blog post</a>.</p></div>
  

  
    







  
    
        <div data-analytics-module="{
            &quot;module_name&quot;: &quot;Inline Images&quot;,
            &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
          }">
    

    <p>

        
        
          
            <video tabindex="0" autoplay="" loop="" muted="" playsinline="" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Jules_GIF_3D_10_1.mp4" type="video/mp4" title="Animation of Jules coding assistant" alt="Jules">
              Video format not supported
            </video>
          
        
      
      </p>
      
    
      </div>
    
  



  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Agents in games and other domains</h2><p data-block-key="384g8">Google DeepMind has a <a href="https://deepmind.google/discover/blog/agent57-outperforming-the-human-atari-benchmark/">long</a> <a href="https://deepmind.google/research/breakthroughs/alphago/">history</a> of using games to help AI models become better at following rules, planning and logic. Just last week, for example, we introduced <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie 2</a>, our AI model that can create an endless variety of playable 3D worlds — all from a single image. Building on this tradition, we’ve built agents using Gemini 2.0 that can help you navigate the virtual world of video games. It can reason about the game based solely on the action on the screen, and offer up suggestions for what to do next in real time conversation.</p><p data-block-key="b3sa9">We're collaborating with leading game developers like Supercell to explore how these agents work, testing their ability to interpret rules and challenges across a diverse range of games, from strategy titles like “Clash of Clans” to farming simulators like “Hay Day.”</p><p data-block-key="2p0a">Beyond acting as virtual gaming companions, these agents can even tap into Google Search to connect you with the wealth of gaming knowledge on the web.</p></div>
  

  
    
  
    


  <div data-component="uni-article-yt-player" data-page-title="Introducing Gemini 2.0: our new AI model for the agentic era" data-video-id="IKuGNHJBGsc" data-index-id="22" data-type="video" data-analytics-module="{
      &quot;module_name&quot;: &quot;Youtube Video&quot;,
      &quot;section_header&quot;: &quot;undefined&quot;
    }" data-yt-video="module">

      

      <a role="video" tabindex="0">
        <div>
          
            
            <p><img alt="Navi demo video" src="https://i.ytimg.com/vi_webp/IKuGNHJBGsc/default.webp" loading="lazy" data-loading="{
                  &quot;mobile&quot;: &quot;//i.ytimg.com/vi_webp/IKuGNHJBGsc/sddefault.webp&quot;,
                  &quot;desktop&quot;: &quot;https://i.ytimg.com/vi_webp/IKuGNHJBGsc/maxresdefault.webp&quot;
                }"></p>

          
          <svg role="presentation">
  <use xmlns:xlink="http://www.w3.org/1999/xlink" href="/static/blogv2/images/icons.svg?version=pr20241210-1809#mi-keyboard-arrow-right"></use>
</svg>


          <svg role="presentation">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button_no_hole"></use>
            
          </svg>
          <svg role="img">
            
            <use href="/static/blogv2/images/icons.svg?version=pr20241210-1809#yt_video_play_button"></use>
            
          </svg>

          
          
          
          
        </div>
      </a>

      

      
    </div>


  


  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><p data-block-key="of3wa">In addition to exploring agentic capabilities in the virtual world, we’re experimenting with agents that can help in the physical world by applying Gemini 2.0's spatial reasoning capabilities to robotics. While it’s still early, we’re excited about the potential of agents that can assist in the physical environment.</p><p data-block-key="ba0kt">You can learn more about these research prototypes and experiments at <a href="http://labs.google/">labs.google</a>.</p></div>
  

  
    

  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Building responsibly in the agentic era</h2><p data-block-key="6j36d">Gemini 2.0 Flash and our research prototypes allow us to test and iterate on new capabilities at the forefront of AI research that will eventually make Google products more helpful.</p><p data-block-key="fvsj8">As we develop these new technologies, we recognize the responsibility it entails, and the many questions AI agents open up for safety and security. That is why we are taking an exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations.</p><p data-block-key="angf4">For example:</p><ul><li data-block-key="crrpj">As part of our safety process, we’ve worked with our Responsibility and Safety Committee (RSC), our longstanding internal review group, to identify and understand potential risks.</li><li data-block-key="2uku6">Gemini 2.0's reasoning capabilities have enabled major advancements in our AI-assisted red teaming approach, including the ability to go beyond simply detecting risks to now automatically generating evaluations and training data to mitigate them. This means we can more efficiently optimize the model for safety at scale.</li><li data-block-key="81t7m">As Gemini 2.0’s multimodality increases the complexity of potential outputs, we’ll continue to evaluate and train the model across image and audio input and output to help improve safety.</li><li data-block-key="clba0">With Project Astra, we’re exploring potential mitigations against users unintentionally sharing sensitive information with the agent, and we’ve already built in privacy controls that make it easy for users to delete sessions. We’re also continuing to research ways to ensure AI agents act as reliable sources of information and don’t take unintended actions on your behalf.</li><li data-block-key="7r7pa">With Project Mariner, we’re working to ensure the model learns to prioritize user instructions over 3rd party attempts at prompt injection, so it can identify potentially malicious instructions from external sources and prevent misuse. This prevents users from being exposed to fraud and phishing attempts through things like malicious instructions hidden in emails, documents or websites.</li></ul><p data-block-key="f9e42">We firmly believe that the only way to build AI is to be responsible from the start and we'll continue to prioritize making safety and responsibility a key element of our model development process as we advance our models and agents.</p></div>
  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Introducing Gemini 2.0: our new AI model for the agentic era&quot;
         }"><h2 data-block-key="of3wa">Gemini 2.0, AI agents and beyond</h2><p data-block-key="35lej">Today’s releases mark a new chapter for our Gemini model. With the release of Gemini 2.0 Flash, and the series of research prototypes exploring agentic possibilities, we have reached an exciting milestone in the Gemini era. And we’re looking forward to continuing to safely explore all the new possibilities within reach as we build towards AGI.</p></div>
  

  
    














<uni-related-content-tout title="Gemini 2.0: Our latest, most capable AI model yet" cta="See more" summary="See how Gemini 2.0 and our research prototypes work — and how they’ll help make our Google products more helpful." hideimage="False" eyebrow="Collection" fullurl="https://blog.google/products/gemini/google-gemini-ai-collection-2024/" pagetype="collectiondetailpage" isarticlepage="">
  
    <div slot="rct-image-slot">
      
    <figure>
        <picture>
            


    

    
        <source media="(max-resolution: 1.5dppx)" sizes="300px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-300.format-webp.webp 300w">
    
        <source media="(min-resolution: 1.5dppx)" sizes="600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp 600w">
    

    <img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp" alt="gemini social share collection" sizes=" 300px,  600px" srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-300.format-webp.webp 300w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/12-11-24_Collection_Social-Share.width-600.format-webp.webp 600w" data-target="image" loading="lazy">
    


        </picture>
    </figure>


    </div>
  
</uni-related-content-tout>

  

  
    

  
    





<div role="form" aria-label="Sign up to receive weekly news and stories from Google." data-component="uni-subscribe" data-analytics-module="{
    &quot;module_name&quot;: &quot;Newsletter&quot;,
    &quot;section_header&quot;: &quot;Get more stories from Google in your inbox.&quot;
  }">
        
        
        <p>You are already subscribed to our newsletter.</p>
      </div>

  

  


            
            

            
              




            
          </div>
        
      </div>
  </article></div>]]></description>
        </item>
    </channel>
</rss>