<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 24 Apr 2024 23:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[IBM to Acquire HashiCorp, Inc for $6.4 billion (346 pts)]]></title>
            <link>https://newsroom.ibm.com/2024-04-24-IBM-to-Acquire-HashiCorp-Inc-Creating-a-Comprehensive-End-to-End-Hybrid-Cloud-Platform</link>
            <guid>40149136</guid>
            <pubDate>Wed, 24 Apr 2024 20:24:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsroom.ibm.com/2024-04-24-IBM-to-Acquire-HashiCorp-Inc-Creating-a-Comprehensive-End-to-End-Hybrid-Cloud-Platform">https://newsroom.ibm.com/2024-04-24-IBM-to-Acquire-HashiCorp-Inc-Creating-a-Comprehensive-End-to-End-Hybrid-Cloud-Platform</a>, See on <a href="https://news.ycombinator.com/item?id=40149136">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wd_move_content_up">

<p>$6.4 billion acquisition adds suite of leading hybrid and multi-cloud lifecycle management products to help clients grappling with today's AI-driven application growth and complexity</p><p>HashiCorp's capabilities to drive significant synergies across multiple strategic growth areas for IBM, including Red Hat, watsonx, data security, IT automation and Consulting</p><p>As a part of IBM, HashiCorp is expected to accelerate innovation and enhance its go-to-market, growth and monetization initiatives</p><p>Transaction expected to be accretive to Adjusted EBITDA within the first full year, post close, and free cash flow in year two</p>
<p>Apr 24, 2024</p>

			
		
</div><div wd_resize="formatNews" wd_print_url="https://newsroom.ibm.com/2024-04-24-IBM-to-Acquire-HashiCorp-Inc-Creating-a-Comprehensive-End-to-End-Hybrid-Cloud-Platform?printable=1"><p>,  /<a href="http://www.prnewswire.com/" target="_blank">PRNewswire</a>/ -- IBM (NYSE: <a href="http://www.ibm.com/investor" rel="nofollow" target="_blank">IBM</a>) and HashiCorp Inc. (NASDAQ: HCP), a leading multi-cloud infrastructure automation company, today announced they have entered into a definitive agreement under which IBM will acquire HashiCorp for <span>$35</span> per share in cash, representing an enterprise value of <span>$6.4 billion</span>. HashiCorp's suite of products provides enterprises with extensive Infrastructure Lifecycle Management and Security Lifecycle Management capabilities to enable organizations to automate their hybrid and multi-cloud environments. Today's announcement is a continuation of IBM's deep focus and investment in hybrid cloud and AI, the two most transformational technologies for clients today.</p>

<p><a href="https://mma.prnewswire.com/media/2319830/IBM_LOGO_1.html" rel="nofollow" target="_blank"><img alt="IBM Corporation logo. (PRNewsfoto/IBM Corporation)" src="https://mma.prnewswire.com/media/2319830/IBM_LOGO_1.jpg" title="IBM Corporation logo. (PRNewsfoto/IBM Corporation)"> </a></p>

<p>"Enterprise clients are wrestling with an unprecedented expansion in infrastructure and applications across public and private clouds, as well as on-prem environments. The global excitement surrounding generative AI has exacerbated these challenges and CIOs and developers are up against dramatic complexity in their tech strategies," said <span>Arvind Krishna</span>, IBM chairman and chief executive officer. "HashiCorp has a proven track record of enabling clients to manage the complexity of today's infrastructure and application sprawl. Combining IBM's portfolio and expertise with HashiCorp's capabilities and talent will create a comprehensive hybrid cloud platform designed for the AI era."</p>

<p>The rise of cloud-native workloads and associated applications is driving a radical expansion in the number of cloud workloads enterprises are managing. In addition, generative AI deployment continues to grow alongside traditional workloads. As a result, developers are working with increasingly heterogeneous, dynamic, and complex infrastructure strategies. This represents a massive challenge for technology professionals.</p>

<p>HashiCorp's capabilities enable enterprises to use automation to deliver lifecycle management for infrastructure and security, providing a system of record for the critical workflows needed for hybrid and multi-cloud environments. HashiCorp's Terraform is the industry standard for infrastructure provisioning in these environments. HashiCorp's offerings help clients take a cloud-agnostic, and highly interoperable approach to multi-cloud management, and complement IBM's commitment to industry collaboration (including deep and expanding partnerships with hyperscale cloud service providers), developer communities, and open-source hybrid cloud and AI innovation.</p>

<p>"Our strategy at its core is about enabling companies to innovate in the cloud, while providing a consistent approach to managing cloud at scale. The need for effective management and automation is critical with the rise of multi-cloud and hybrid cloud, which is being accelerated by today's AI revolution," said <span>Armon Dadgar</span>, HashiCorp co-founder and chief technology officer. "I'm incredibly excited by today's news and to be joining IBM to accelerate HashiCorp's mission and expand access to our products to an even broader set of developers and enterprises."</p>

<p>"Today is an exciting day for our dedicated teams across the world as well as the developer communities we serve," said <span>Dave McJannet</span>, HashiCorp chief executive officer. "IBM's leadership in hybrid cloud along with its rich history of innovation, make it the ideal home for HashiCorp as we enter the next phase of our growth journey. I'm proud of the work we've done as a standalone company, I am excited to be able to help our customers further, and I look forward to the future of HashiCorp as part of IBM."</p>

<p><b>Transaction Rationale</b></p>

<ul type="disc">
	<li><b>Strong Strategic Fit – </b>The acquisition of HashiCorp by IBM creates a comprehensive end-to-end hybrid cloud platform built for AI-driven complexity. The combination of each company's portfolio and talent will deliver clients extensive application, infrastructure and security lifecycle management capabilities</li>
	<li><b>Accelerates growth in key focus areas – </b>Upon close, HashiCorp is expected to drive significant synergies for IBM, including across multiple strategic growth areas like Red Hat, watsonx, data security, IT automation and Consulting. For example, the powerful combination of Red Hat's Ansible Automation Platform's configuration management and Terraform's automation will simplify provisioning and configuration of applications across hybrid cloud environments. The two companies also anticipate an acceleration of HashiCorp's growth initiatives by leveraging IBM's world-class go-to-market strategy, scale, and reach, operating in more than 175 countries across the globe</li>
	<li><b>Expands Total Addressable Market (TAM) – </b>The acquisition will create the opportunity to deliver more comprehensive hybrid and multi-cloud offerings to enterprise clients. HashiCorp's offerings, combined with IBM and Red Hat, will give clients a platform to automate the deployment and orchestration of workloads across evolving infrastructure including hyperscale cloud service providers, private clouds and on-prem environments. This will enhance IBM's ability to address the total cloud opportunity, which according to IDC had a TAM of <span>$1.1 trillion</span> in 2023, with a compound annual growth rate in the high teens through 2027.<sup>1</sup></li>
	<li><b>Attractive Financial Opportunity – </b>The transaction will accelerate IBM's growth profile over time driven by go-to-market and product synergies. This growth combined with operating efficiencies, is expected to achieve substantial near-term margin expansion for the acquired business. It is anticipated that the transaction will be accretive to Adjusted EBITDA within the first full year, post close, and free cash flow in year two.</li>
</ul>

<p>HashiCorp boasts a roster of more than 4,400 clients, including Bloomberg, Comcast, Deutsche Bank, GitHub, J.P Morgan Chase, Starbucks and Vodafone. HashiCorp's offerings have widescale adoption in the developer community and are used by 85% of the Fortune 500. Their community products across infrastructure and security were downloaded more than 500 million times in HashiCorp's FY2024 and include:</p>

<ul type="disc">
	<li><b>Terraform</b> – provides organizations with a single workflow to provision their cloud, private datacenter, and SaaS infrastructure and continuously manage infrastructure throughout its lifecycle</li>
	<li><b>Vault</b> – provides organizations with identity-based security to automatically authenticate and authorize access to secrets and other sensitive data</li>
	<li><b>Additional products</b> – <i>Boundary</i> for secure remote access;<i> Consul</i> for service-based networking; <i>Nomad</i> for workload orchestration; <i>Packer</i> for building and managing images as code; and <i>Waypoint</i> internal developer platform</li>
</ul>

<p><b>Transaction Details</b></p>

<p>Under the terms of the agreement, IBM will acquire HashiCorp for <span>$35</span> per share in cash, or <span>$6.4 billion</span> enterprise value, net of cash. HashiCorp will be acquired with available cash on hand.</p>

<p>The boards of directors of IBM and HashiCorp have both approved the transaction. The acquisition is subject to approval by HashiCorp shareholders, regulatory approvals and other customary closing conditions.</p>

<p>The Company's largest shareholders and investors, who collectively hold approximately 43% of the voting power of&nbsp;HashiCorp's outstanding common stock, entered into a voting agreement with IBM pursuant to which each has agreed to vote all of their common shares in favor of the transaction and against any alternative transactions.</p>

<p>The transaction is expected to close by the end of 2024.</p>

<p>____________________<br>
<sup>1</sup> The total cloud opportunity is the sum of the cloud-directed spends across Hardware, IT services and SW for Private and Public cloud implementation, sourced from IDC's Worldwide Black Book Live Edition, <span>March 2024</span> (V1 2024)</p>

<p><b>Conference Call Details</b></p>

<p>IBM's regular quarterly earnings conference call is scheduled to begin at <span>5:00 p.m. ET</span>, today. The Webcast may be accessed <a href="https://www.ibm.com/investor/events/earnings-1q24" rel="nofollow" target="_blank">here</a>. Presentation charts will be available shortly before the Webcast.</p>

<p><b>About IBM</b></p>

<p>IBM is a leading provider of global hybrid cloud and AI, and consulting expertise. We help clients in more than 175 countries capitalize on insights from their data, streamline business processes, reduce costs and gain the competitive edge in their industries. Thousands of government and corporate entities in critical infrastructure areas such as financial services, telecommunications and healthcare rely on IBM's hybrid cloud platform and Red Hat OpenShift to affect their digital transformations quickly, efficiently and securely. IBM's breakthrough innovations in AI, quantum computing, industry-specific cloud solutions and consulting deliver open and flexible options to our clients. All of this is backed by IBM's legendary commitment to trust, transparency, responsibility, inclusivity and service. Visit&nbsp;<a href="http://www.ibm.com/" rel="nofollow" target="_blank">www.ibm.com</a>&nbsp;for more information.&nbsp;</p>

<p><b>About HashiCorp</b></p>

<p>HashiCorp&nbsp;is The Infrastructure Cloud™ company, helping organizations automate multi-cloud and hybrid environments with Infrastructure Lifecycle Management and Security Lifecycle Management. HashiCorp&nbsp;offers The Infrastructure Cloud on the HashiCorp&nbsp;Cloud Platform (HCP) for managed cloud services, as well as self-hosted enterprise offerings and community source-available products. The company is headquartered in <span>San Francisco, California</span>. For more information, visit&nbsp;<a href="https://urldefense.proofpoint.com/v2/url?u=http-3A__hashicorp.com&amp;d=DwMFaQ&amp;c=BSDicqBQBDjDI9RkVyTcHQ&amp;r=W9dONwI1yR8TY6fdJkjNwdlDQU2ROvWbv1mlvkWQFLs&amp;m=uOTnwadDTxZ2XD2uC6bDCFPG-9K-Oq5GBVfeuYs3n7-_Z1xbsLo_2585JrS2L788&amp;s=RF6C-_LsY0GUJ3MnM2K_hfDsjYfnI-WCPo4lNlnulgE&amp;e=" rel="nofollow" target="_blank">HashiCorp.com</a>.</p>

<p><b>Press Contacts:</b></p>

<p>IBM:<br>
<span>Tim Davidson</span>, 914-844-7847<br>
<a href="mailto:tfdavids@us.ibm.com" rel="nofollow" target="_blank">tfdavids@us.ibm.com</a></p>

<p>HashiCorp:<br>
<span>Matthew Sherman</span> / <span>Jed Repko</span> / <span>Haley Salas</span> / <span>Joycelyn Barnett</span><br>
<span>Joele Frank</span>, Wilkinson Brimmer Katcher<br>
212-355-4449</p>



<p><i><b>Additional Information and Where to Find It</b></i></p>

<p><i>HashiCorp, Inc. ("HashiCorp"), the members of HashiCorp's board of directors and certain of HashiCorp's executive officers are participants in the solicitation of proxies from stockholders in connection with the pending acquisition of HashiCorp (the "Transaction"). HashiCorp plans to file a proxy statement (the "Transaction Proxy Statement") with the Securities and Exchange Commission (the "SEC") in connection with the solicitation of proxies to approve the Transaction. <span>David McJannet</span>, <span>Armon Dadgar</span>, <span>Susan St. Ledger</span>, <span>Todd Ford</span>, <span>David Henshall</span>, <span>Glenn Solomon</span> and <span>Sigal Zarmi</span>, all of whom are members of HashiCorp's board of directors, and <span>Navam Welihinda</span>, HashiCorp's chief financial officer, are participants in HashiCorp's solicitation. Information regarding such participants, including their direct or indirect interests, by security holdings or otherwise, will be included in the Transaction Proxy Statement and other relevant documents to be filed with the SEC in connection with the Transaction. Additional information about such participants is available under the captions "Board of Directors and Corporate Governance," "Executive Officers" and "Security Ownership of Certain Beneficial Owners and Management" in HashiCorp's definitive proxy statement in connection with its 2023 Annual Meeting of Stockholders (the "2023 Proxy Statement"), which was filed with the SEC on <span>May 17, 2023</span> (and is available at&nbsp;<a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.sec.gov_ix-3Fdoc-3D_Archives_edgar_data_1720671_000114036123025250_ny20008192x1-5Fdef14a.htm&amp;d=DwMGaQ&amp;c=BSDicqBQBDjDI9RkVyTcHQ&amp;r=W9dONwI1yR8TY6fdJkjNwdlDQU2ROvWbv1mlvkWQFLs&amp;m=WIP1VyxuvBGNlzTUaYuNxHpbet03ywNI-NapdbhVOxO7G4LhtR2egnoPZFg5wgFk&amp;s=DAxP0uH2PZLVIrXRiK8JS2ywGdYGNx-kvMHxAEWkP_4&amp;e=" rel="nofollow" target="_blank">https://www.sec.gov/ix?doc=/Archives/edgar/data/1720671/000114036123025250/ny20008192x1_def14a.htm</a>). To the extent that holdings of HashiCorp's securities have changed since the amounts printed in the 2023 Proxy Statement, such changes have been or will be reflected on Statements of Change in Ownership on Form 4 filed with the SEC (which are available at&nbsp;<a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.sec.gov_cgi-2Dbin_browse-2Dedgar-3Faction-3Dgetcompany-26CIK-3D0001720671-26type-3D-26dateb-3D-26owner-3Donly-26count-3D40-26search-5Ftext-3D&amp;d=DwMGaQ&amp;c=BSDicqBQBDjDI9RkVyTcHQ&amp;r=W9dONwI1yR8TY6fdJkjNwdlDQU2ROvWbv1mlvkWQFLs&amp;m=WIP1VyxuvBGNlzTUaYuNxHpbet03ywNI-NapdbhVOxO7G4LhtR2egnoPZFg5wgFk&amp;s=bqavEZgqjNge6kAvbmk0zLhMXTAYmIjA5rzwhQaJDd8&amp;e=" rel="nofollow" target="_blank">https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&amp;CIK=0001720671&amp;type=&amp;dateb=&amp;owner=only&amp;count=40&amp;search_text=</a>). Information regarding HashiCorp's transactions with related persons is set forth under the caption "Related Person Transactions" in the 2023 Proxy Statement. Certain illustrative information regarding the payments to that may be owed, and the circumstances in which they may be owed, to HashiCorp's named executive officers in a change of control of HashiCorp is set forth under the caption "Executive Compensation—Potential Payments upon Termination or Change in Control" in the 2023 Proxy Statement. With respect to Ms. St. Ledger, certain of such illustrative information is contained in the Current Report on Form 8-K filed with the SEC on <span>June 7, 2023</span> (and is available at&nbsp;<a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__www.sec.gov_ix-3Fdoc-3D_Archives_edgar_data_1720671_000162828023021270_hcp-2D20230607.htm&amp;d=DwMGaQ&amp;c=BSDicqBQBDjDI9RkVyTcHQ&amp;r=W9dONwI1yR8TY6fdJkjNwdlDQU2ROvWbv1mlvkWQFLs&amp;m=WIP1VyxuvBGNlzTUaYuNxHpbet03ywNI-NapdbhVOxO7G4LhtR2egnoPZFg5wgFk&amp;s=4QQvgxl7VrRT-9dgZy45Vw0gBhmWz7KH8fzaGjGwkIk&amp;e=" rel="nofollow" target="_blank">https://www.sec.gov/ix?doc=/Archives/edgar/data/1720671/000162828023021270/hcp-20230607.htm</a>).&nbsp;Promptly after filing the definitive Transaction Proxy Statement with the SEC, HashiCorp will mail the definitive Transaction Proxy Statement and a WHITE proxy card to each stockholder entitled to vote at the special meeting to consider the Transaction. STOCKHOLDERS ARE URGED TO READ THE TRANSACTION PROXY STATEMENT (INCLUDING ANY AMENDMENTS OR SUPPLEMENTS THERETO) AND ANY OTHER RELEVANT DOCUMENTS THAT HASHICORP WILL FILE WITH THE SEC WHEN THEY BECOME AVAILABLE BECAUSE THEY WILL CONTAIN IMPORTANT INFORMATION. Stockholders may obtain, free of charge, the preliminary and definitive versions of the Transaction Proxy Statement, any amendments or supplements thereto, and any other relevant documents filed by HashiCorp with the SEC in connection with the Transaction at the SEC's website (<a href="https://urldefense.proofpoint.com/v2/url?u=http-3A__www.sec.gov&amp;d=DwMGaQ&amp;c=BSDicqBQBDjDI9RkVyTcHQ&amp;r=W9dONwI1yR8TY6fdJkjNwdlDQU2ROvWbv1mlvkWQFLs&amp;m=WIP1VyxuvBGNlzTUaYuNxHpbet03ywNI-NapdbhVOxO7G4LhtR2egnoPZFg5wgFk&amp;s=97Mk6TtDEGohehH043KUy50rAX9jXPHlNPxdtGwcYPc&amp;e=" rel="nofollow" target="_blank">http://www.sec.gov</a>). Copies of HashiCorp's definitive Transaction Proxy Statement, any amendments or supplements thereto, and any other relevant documents filed by HashiCorp with the SEC in connection with the Transaction will also be available, free of charge, at HashiCorp's investor relations website (<a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__ir.hashicorp.com_&amp;d=DwMGaQ&amp;c=BSDicqBQBDjDI9RkVyTcHQ&amp;r=W9dONwI1yR8TY6fdJkjNwdlDQU2ROvWbv1mlvkWQFLs&amp;m=WIP1VyxuvBGNlzTUaYuNxHpbet03ywNI-NapdbhVOxO7G4LhtR2egnoPZFg5wgFk&amp;s=zgti-5JjrLH_7Ja9Wdc81ipMwWNQ-4K0YQu2LW5qZnw&amp;e=" rel="nofollow" target="_blank">https://ir.hashicorp.com/</a>), or by emailing HashiCorp's investor relations department (<a href="mailto:ir@hashicorp.com" rel="nofollow" target="_blank">ir@hashicorp.com</a>).</i></p>

<p><i><b>Forward-Looking Statements</b></i></p>

<p><i>Certain statements contained in this communication may be characterized as forward-looking under the Private Securities Litigation Reform Act of 1995. These statements involve a number of risks, uncertainties and other factors that could cause actual results to differ materially.</i></p>

<p><i>Statements in this communication regarding IBM and HashiCorp that are forward-looking may include statements regarding: (i) the Transaction; (ii) the expected timing of the closing of the Transaction; (iii) considerations taken into account in approving and entering into the Transaction; (iv) the anticipated benefits to, or impact of, the Transaction on IBM's and HashiCorp's businesses; and (v) expectations for IBM and HashiCorp following the closing of the Transaction. There can be no assurance that the Transaction will be consummated.</i></p>

<p><i>Risks and uncertainties that could cause actual results to differ materially from those indicated in the forward-looking statements, in addition to those identified above, include: (i) the possibility that the conditions to the closing of the Transaction are not satisfied, including the risk that required approvals from HashiCorp's stockholders for the Transaction or required regulatory approvals to consummate the Transaction are not obtained, on a timely basis or at all; (ii) the occurrence of any event, change or other circumstance that could give rise to a right to terminate the Transaction, including in circumstances requiring HashiCorp to pay a termination fee; (iii) possible disruption related to the Transaction to IBM's and HashiCorp's current plans, operations and business relationships, including through the loss of customers and employees; (iv) the amount of the costs, fees, expenses and other charges incurred by IBM and HashiCorp related to the Transaction; (v) the risk that IBM's or HashiCorp's stock price may fluctuate during the pendency of the Transaction and may decline if the Transaction is not completed; (vi) the diversion of IBM and HashiCorp management's time and attention from ongoing business operations and opportunities; (vii) the response of competitors and other market participants to the Transaction; (viii) potential litigation relating to the Transaction; (ix) uncertainty as to timing of completion of the Transaction and the ability of each party to consummate the Transaction; and (x) other risks and uncertainties detailed in the periodic reports that IBM and HashiCorp filed with the SEC, including IBM's and HashiCorp's respective Annual Reports on Form 10-K.&nbsp; All forward-looking statements in this communication are based on information available to IBM and HashiCorp as of the date of this communication, and, except as required by law, IBM and HashiCorp do not assume any obligation to update the forward-looking statements provided to reflect events that occur or circumstances that exist after the date on which they were made.</i></p>

<p>SOURCE IBM</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM to buy HashiCorp in $6.4B deal (207 pts)]]></title>
            <link>https://www.reuters.com/markets/deals/ibm-buy-hashicorp-64-billion-deal-expand-cloud-software-2024-04-24/</link>
            <guid>40149095</guid>
            <pubDate>Wed, 24 Apr 2024 20:21:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/markets/deals/ibm-buy-hashicorp-64-billion-deal-expand-cloud-software-2024-04-24/">https://www.reuters.com/markets/deals/ibm-buy-hashicorp-64-billion-deal-expand-cloud-software-2024-04-24/</a>, See on <a href="https://news.ycombinator.com/item?id=40149095">Hacker News</a></p>
Couldn't get https://www.reuters.com/markets/deals/ibm-buy-hashicorp-64-billion-deal-expand-cloud-software-2024-04-24/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[I now lack the juice to fuel the bluster to conceal that I am a simpleton (135 pts)]]></title>
            <link>https://lithub.com/i-now-lack-the-juice-to-fuel-the-bluster-to-conceal-that-i-am-a-simpleton-padgett-powell-legend/</link>
            <guid>40148563</guid>
            <pubDate>Wed, 24 Apr 2024 19:32:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lithub.com/i-now-lack-the-juice-to-fuel-the-bluster-to-conceal-that-i-am-a-simpleton-padgett-powell-legend/">https://lithub.com/i-now-lack-the-juice-to-fuel-the-bluster-to-conceal-that-i-am-a-simpleton-padgett-powell-legend/</a>, See on <a href="https://news.ycombinator.com/item?id=40148563">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							        
									<p><span itemprop="articleBody"><p>Forty years ago, Padgett Powell’s erudite coming of age novel <a href="https://bookshop.org/a/132/9781936787722" target="_blank"><em>Edisto</em></a> was released and introduced readers to a 12-year-old literary prodigy named Simons Everson Manigault. Following an acrimonious separation between the “Duchess” and the “Progenitor” (Simons’ parents), the young boy finds himself living in the Manigault&nbsp;summer home in Edisto, South Carolina while his mother descends into alcoholism.</p>
<p>With his domestic reality crumbling to its foundations, Simons spends most of his time at the Baby Grand, a Black nightclub whose patrons take a shine to the young truant. When the Manigault maid’s biracial grandson arrives in Edisto looking for his mother, Simons names him “Taurus” and the name sticks. The unlikely pair develop a friendship, ushering Simons into the world of mid-century American race relations, the sexual revolution, and the infelicitous vortex of confusion that is puberty.</p>
<p>In addition to celebrating <em>Edisto’s</em> milestone anniversary this year, Powell will be releasing <em>Blasphemy and Other Ancestors </em>(Gordon Hill Press), a collaborative book co-authored with myself, Darius James, and Lee Henderson about the tag end of existence and the abasements that memory holds in store for characters living outside of their time.</p>
<p>I corresponded with Powell to discuss the relationship between cornpone and Flann O’Brien, the “rotten taste of America” informing literary culture, and building a career around the “fey interface between believing in the South and making fun of believing in the South.”</p>
<p><strong>*</strong></p>
<p><strong>Jean Marc Ah-Sen: </strong>What was the central attraction of writing <em>Edisto</em>, a bildungsroman that explored Black and white race relations in the South? Why do you think so many writers feel compelled to start their careers writing about emotional and intellectual maturation, and to which characteristics of the coming-of-age novel do you attribute its pride of place among a broadly defined readership?</p>
<span>The attractive characteristic of a young narrator is the absurdity of it and the license of it.</span>
<p><strong>Padgett Powell:</strong> Let me warn us that these questions are too <em>recherche</em> for me. I now lack the juice to fuel the bluster to conceal that I am a simpleton.</p>
<p>A professor in college was roundly pregnant on a Monday, absent Wednesday, and giving her lecture on “The Miller’s Tale” on Friday with the baby on her hip. I thought “What if that little bastard picks this stuff up and knows Chaucer when he’s five?” My own brother had a good mouth on him, there was already the mother in front of me—the novel was there for the writing. The cerebral cogitation was done. Just Strunk &amp; White some sentences and connect them head to tail and throw in everything you’ve ever seen or heard. Done.</p>
<p>You don’t write about “emotional and intellectual maturation.” The attractive characteristic of a young narrator is the absurdity of it and the license of it. Huck Finn is a 14-year-old uneducated antebellum white boy in Mississippi? Huck Finn is Mark Twain being as smart as Mark Twain was. Huck Finn turns him loose. The absurdity of the proposition is like lightning.</p>
<p><strong>JMA: </strong><em>Edisto</em> explored the legacy and aftermath of the reconstruction era all the way into the 1960s. Donald Barthelme in particular praised you for writing about things readers had never heard before between Black and white characters. Did you have a sense that books written about a similar milieu before or around the time of <em>Edisto’s</em> publication were characteristic of a mealy-mouthedness when it came to racial politics?</p>
<p><strong>PP:</strong> I am innocent of anything written along these lines or the spectrum of candor in them. Because I was arrested for my underground newspaper <em>Tough Shit</em> in high school and the principal sent it to my college to dematriculate me from it, and I had as a result narrated the events of my arrest to four eminences at that college, the Dean of Men there put me in a dorm room with the odd Black boy out because he figured I was radical enough to be liberal enough to handle early integration.</p>
<p>“I didn’t want some redneck to eat him up,” he put it to me three weeks into the semester. I said I wouldn’t. Jinx took me to his bar—it’s in the book, verisimilitudinously. I saw some low country Black life behind some doors. We’d had a long-term maid, a complicated woman—in the book as Theenie, verbatim. I saw some mullet fishing in Florida where a woman berated a fellow for his sloth; “You so slow, no wonder your wife left you.” His name was Buckwheat, a name I could not alter or drop, as prudence would have suggested; she called him Wheat, and when he said, “She din’ leave me, she died,” the woman yelled, “The ultimate leff!” If these are things not heard before, it is only because no one has listened and written them down. You could not publish them in America today because of liberal-editing racism.</p>
<p><strong>JMA: </strong>Twelve years after the release of <em>Edisto</em>, you returned to the world of the Manigault family with <em>Edisto Revisited</em>. What were your motivations for dropping back into the land of mullet fishing, moonshine, and professorial mores during Simons’ university years? Did you always have a sense that there was more to Simons’ story that warranted revisiting, or was there some realization that came afterwards that signaled the attractive possibilities of the story continuing in a sequel?</p>
<p><strong>PP:</strong> I had no sense of more to tell, certainly no sense that more was merited, but it was what I could write at the time and I wrote it. Let me lean us up on Flannery O’Connor, our late racist goddesshead: “When I told you to write what is easy I meant what is possible. It is never easy.” If I have misquoted her, it is because my brain has little spots of something in it.</p>
<p><strong>JMA: </strong>Your career has been described as participating in the American Southern literary tradition. Was this an association you felt honored by, or perhaps something that you were suspicious of? Do you think that writing embodying the principles of the American South needs to be constantly evolving, or is it something that needs to be carefully curated, and whose boundaries must be clearly defined, in order for it to endure?</p>
<p><strong>PP:</strong> It might be fun to tear it all up after the Jews Will Not Replace Us boys in their khakis protest, holding copies of <em>Absalom! Absalom!</em> upside down in the style of Trump with his Bible with duped General Milley at his side. It could all go into the big smelter with the Bobbie E. Lee bronze. But then some smartass would invent another plantation house, another confederate widow, another lost utterancer of the not yet lost cause.</p>
<p>I have made a career of dancing without dancing in the fey interface between believing in the South and making fun of believing in the South, which is why no one has ever heard of me. It’s a lame-ass position. The proper term is chicken-shit.</p>
<p>Who would object if clubists wanted to shove you into a cubbyhole with Faulkner and O’Connor and their queer son Tennessee and too straight son Walker and what-litter-is-he-from son Don? Not I. And my God, Barry Hannah got more out of the whiskey oracle than anyone dead. I do not like the sentimental blood-and-grits crowd and I do not like the apotheosis of Story as Panacea, the from-farm-to-porch menu. Cornpone. No.</p>
<p>Southern writing, not often actually defined, means a deep-down knowing that people are beat to shit. An earnest suspicion of earnestness, a recognition and denial of whippedness. I am now spinning cornpone myself. End of the foregoing. Let’s go read some Flann O’Brien. Those brothers are whipped for real.</p>
<p><strong>JMA: </strong>The book that you are perhaps most known for is <a href="https://bookshop.org/a/132/9780061859434" target="_blank"><em>The Interrogative Mood</em>, <em>A </em><em>Novel?</em></a> which is entirely written in the form of meditative questions—“If you were to participate in a spice war, what spice would you fight for?” is my personal favorite. Did you conceive the book as a rascally wedge that could be placed between experimental and commercial fiction? Or was the book perhaps an effort to assert the primacy of artistic questioning over the fatuousness of shopworn opinions?</p>
<p><strong>PP:</strong> You continue to try to flatter. But this is sharp flattery. It moves me to pomposity: experimental fiction means no more or less than fiction whose central thrust in not made-up people doing made-up things. Let’s call that MUPDMUT. With some liberty, Mupdeemut. Experimental fiction may of course have Mupdeemut in it, but not as the thrust of it—something beyond our believing in Mupdeemut is at hand.</p>
<p>When Hulga’s leg is stolen by a Bible salesman, we are to believe it. When our friend Colby has gone too far and is to be hanged, we are not to believe it. We have the pleasure of seeing Colby in his anguish, but we have a larger or smaller pleasure of distraction from the dictate to pretend this horseshit actually happens. That is the “experiment.”</p>
<p>Does the imperative to not believe exceed the pleasure of the imperative to believe? Donald Barthelme believed it did if the writing still contained emotional payoff. O’Connor would have said, did say, to hell with it. “If [the Devil] is only a symbol, to hell with it.” The imperative to believe is at one level rather childish, as in Once upon a time… This is why Coleridge had the wit to call it a “suspension of disbelief,” not precisely an imperative to believe. What an upgrade to be told, “Don’t <em>believe</em> this, you morons.” What you do, mischievously, is believe <em>more</em>. At which point the “experiment” has succeeded.</p>
<p><em>The Int. Mood</em> goes way too far, in Colby terms, and dispenses with Mupdeemut altogether, except for the occasional Jimi Hendrix’s being offered a BLT as he affects to play you a tune. There is no history of intellection in its conception or intent and no prefiguring in its execution (Huck Finn shows you prefigurating for a book like <em>Edisto</em>).</p>
<p>I received an email from a colleague who wanted me to talk to the Dean that opened, “Is it time for us to have a chat with the dean? Are we remembering what was promised us, last spring, at lunch? Are we going to let history repeat itself?” I suffered pique at this and wrote back, “Are your emotions pure? Are your nerves adjustable? How do you stand in relation to the potato? Should it still be Constantinople?”</p>
<p>The pleasure in this was extreme. I thought how funny it would be—Reply All—for her to receive 600 of these questions, and wrote 600 of them, and then could not stop and wrote 142 pages of them. I saw the “rules” immediately, using her model, but exaggerating the forces. Relieve the silly with the grave, the arch with the colloquial, perfect the overt non-sequitur, watch rhythm, let each sentence deliver its impact—a stonking of someone who would presume write your ass with questions as annoying as they were. Marvelous fun, therapeutic because I was fair exercising some deep contours in my shallow brain, and I felt fine every time I wrote a batch of these things, which I began to liken to thousands of redundant missiles like those we have in our nuclear silos.</p>
<p><strong>JMA: </strong>Your prose tends to be voice-driven and characterized by a kind of malapert urbanity. Can you talk about how this stylized way of writing developed, and if it was an artistic response to things you were reading (whether inhospitably or with a deal of enthusiasm)?</p>
<p><strong>PP:</strong> I am ignorant of “malapert” but I do get “urbanity.” Here is as an almost certainly impertinent answer that I do not intend to be impertinent: I learned to write the English I have written by taking three years of Latin, in the putatively desolate educational backwater of Jacksonville, Florida, ending in the tenth grade translating <em>The Aeneid</em>. I was in homeroom sitting with Allen Collins of Lynyrd Skynyrd. We was gettin’ it. We did not know we was gettin’ it.</p>
<p><strong>JMA: </strong>Does the prose voice that you adopt develop in parallel to the thematic concerns that you will tackle in a given work, or does it emerge as a result of other considerations?</p>
<p><strong>PP:</strong> It emerges with no consideration for anything but the next correct word.</p>
<p><strong>JMA:</strong> The destiny of all books is to become unmoored from the time which birthed them, and as new readers discover them, their relationship can become not just tinged, but entirely defined by a sense of presentism. You spent a large part of your career teaching at the University of Florida Creative Writing program, and I’m wondering how you would address this reality when and if it occurred in the classroom? Is there, in point of fact, a “right” and “wrong” way to read?</p>
<p><strong>PP:</strong> We read formative work asking only what might make it better, by which I meant power in the writing. As I came to the end, the students seemed to have been coached toward a new kind of “better” that meant what was less offensive, and the offense was a multi-headed, surprising beast. One student got in trouble with others when he created a character named Phone Ho. I was mystified by all this and got away rather than try to breast the tide.</p>
<p>I was not going to be able to teach writing, if I ever had taught it. I was turned in for use of the phrase “tsunami of inclusivity.” The phrase was examined for “racial content.” It was judged to be empty of racial content. A prior student suggested I use “politicalicity” as in “tsunami of politicalicity” and I paid him $20 for the word, inserted the phrase, and retired. Please see my students Kevin Wilson and Chris Bachelder, and Kevin Canty and Chris Adrian. There are more. These are just my Kevins and my Chrises, as Trump would put it. God are we doomed.</p>
<span>The destiny of all books is to become unmoored from the time which birthed them.</span>
<p><strong>JMA: </strong>Your latest work will be the novelette “The New Book” in our omnibus book <em>Blasphemy and Other Ancestors</em>. Your offering concerns a man of letters taking on an assistant and training him in the righteous arts of romance, but it also features a metafictionally aware narrator playing against a hypothetical reader’s reservations about events unfolding in Florida. I’m curious if this was a way to express frustration with the sensibilities of modern readership or literary criticism?</p>
<p><strong>PP:</strong> I think not. What I recall was writing a fairly comprehensible sketch in the South-satire genre, kind of my schtick, getting tired of my schtick, and for relief sliding into something untenably surreal in which even I could not keep straight what I was talking about. So I shet that thing down. My hero turned into Ted Turner, and Monteagle, Tennessee became the Philippines in WWII, and a girl at Walmart turned into Vanna White and I was doomed.</p>
<p><strong>JMA: </strong>In an industry that routinely heaps indignity after indignity on its practitioners, what has been the most startling development that you have encountered in recent years while releasing your books? Was it an issue arising as a matter of course from past frustrations, or did it spring from some unprecedented corner of the industry?</p>
<p><strong>PP:</strong> It sprang from the unprecedented corner of my own publisher. My book <em>Indigo</em> had been edited and copyedited by two astute, eminent editors, was set to roll, when a “sensitivity editor” was brought in because someone in marketing at the house was not “comfortable representing the book.” (For the record I never saw any evidence that anyone represented the book)</p>
<p>The sensitivity editor, who I suspect was given four times the money I was given for the book, fell to with their sensitivity broadaxe. In a long true account of a dust-up at a restaurant in old Austin, not new Austin, a Black man on my roofing crew came to my defense and knocked out a white restaurant manager, who was at the moment presuming to assault me. Willie had noticed that the manager had Black back-up and felt I should too. “Old Padge need him some brothers too,” he would explain later.</p>
<p>The piece was essentially a portrait of a hero, Willie Ebert Brown, in a terrain of racial relations that had hope in it. The sentence that announced the Black back-up for the manager was this: “A sturdy-looking Black guy came out of the kitchen.” This is choice low fruit for a sensitivity editor. “Objectifying description,” she wrote, “that may invoke associations with slavery.”</p>
<p>I should have desisted publishing the book, but I am a chicken-shit person and I really wanted a book with a beautiful photo of an indigo snake on its cover. My celebration of Willie was thrown out; my invocation of slavery (to which who objects, its absurdity aside?) was one of a hundred other crimes in the piece. Liberal racism had its way: remove racism by removing race.</p>
<p>There is not a person of color in my book except a very positive small tribute to Barack Obama as a tool by which we might argue the French can slow their roll about how racist we are and they aren’t. How that was not deemed racist is a wonder, because it somewhat is. It’s not a wonder: liberal racism is a photo-negative argument. I apologize for this rant. Chicken-shit and now tired too.</p>
<p><strong>JMA: </strong>For decades now, readers and writers alike have speculated about where the future of literature is headed, with some espousing the belief that literary fiction in particular is going the way of opera and ballet. I think it could be argued that anything betraying a high literary sensibility is already beholden to blue-blooded patronage and sponsorship, whether we are talking about arts grants and awards bodies or content subscription models like Substack or Patreon. Do you think that literary fiction can find mass appeal among readers or has its place always been in contradistinction to an upmarket reading sensibility?</p>
<p><strong>PP:</strong> Does “an upmarket reading sensibility” mean people who buy books at the airport? I confess to feeling loose reading this question. Let’s do a loose answer: a really good book, with anomalies here and there, will not sell well to a mass American market. If you make money here, you have done something wrong. It’s the rotten taste of America, the same force that explains a Trump. We have problems way larger than a poor good writer and a successful conman at large in Washington.</p>
</span></p>
									
																		
																		
									<br><hr>
									
							    										
								</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A feature-rich front-end drag-and-drop component library (121 pts)]]></title>
            <link>https://github.com/atlassian/pragmatic-drag-and-drop</link>
            <guid>40147883</guid>
            <pubDate>Wed, 24 Apr 2024 18:25:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/atlassian/pragmatic-drag-and-drop">https://github.com/atlassian/pragmatic-drag-and-drop</a>, See on <a href="https://news.ycombinator.com/item?id=40147883">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">

<p dir="auto"><h2 tabindex="-1" dir="auto">About</h2><a id="user-content-about" aria-label="Permalink: About" href="#about"></a></p>
<p dir="auto">Pragmatic drag and drop is a low level drag and drop toolchain that enables safe and successful usage of the browsers built in drag and drop functionality. Pragmatic drag and drop can be used with any view layer (<a href="https://react.dev/" rel="nofollow"><code>react</code></a>, <a href="https://svelte.dev/" rel="nofollow"><code>svelte</code></a>, <a href="https://vuejs.org/" rel="nofollow"><code>vue</code></a>, <a href="https://angular.io/" rel="nofollow"><code>angular</code></a> and so on). Pragmatic drag and drop is powering some of the biggest products on the web, including <a href="https://trello.com/" rel="nofollow">Trello</a>, <a href="https://www.atlassian.com/software/jira" rel="nofollow">Jira</a> and <a href="https://www.atlassian.com/software/confluence" rel="nofollow">Confluence</a>.</p>
<details>
    <summary>Capabilities</summary>
<p dir="auto">Pragmatic drag and drop consists of a few high level pieces:</p>
<ol dir="auto">
<li><strong>Low level drag and drop behavior</strong></li>
</ol>
<p dir="auto">Pragmatic drag and drop contains a core package, and a number of optional packages, that provide you the pieces to create <em>any</em> drag and drop experience.</p>
<p dir="auto">These pieces are unopinionated about visual language or accessibility, and have no dependency on the Atlassian Design System.</p>
<ul dir="auto">
<li><em>Tiny</em>: ~<code>4.7kB</code> core</li>
<li><em>Incremental</em>: Only use the pieces that you need</li>
<li><em>Headless</em>: Full rendering and style control</li>
<li><em>Framework agnostic</em>: Works with any frontend framework</li>
<li><em>Deferred compatible</em>: Delay the loading the core packages and optional packages in order to further improve page load speeds</li>
<li><em>Flexible</em>: create any experience you want, make any changes you want during a drag operation.</li>
<li><em>Works everywhere</em>: Full feature support in Firefox, Safari, and Chrome, iOS and Android</li>
<li><em>Virtualization support</em>: create any virtual experience you want!</li>
</ul>
<ol start="2" dir="auto">
<li><strong>Optional visual outputs</strong></li>
</ol>
<p dir="auto">We have created optional visual outputs (eg our drop indicator) to make it super fast for us to build consistent Atlassian user experiences. Non Atlassian consumers are welcome to use these outputs, create their own that copy the visual styling, or go a totally different direction.</p>
<ol start="3" dir="auto">
<li><strong>Optional assistive technology controls</strong></li>
</ol>
<p dir="auto">Not all users can achieve pointer based drag and drop experiences. In order to achieve fantastic experiences for assistive technology users, we provide a toolchain to allow you to quickly wire up performant assistive technology friendly flows for any experience.</p>
<p dir="auto">The optional assistive controls we provide are based on the Atlassian Design System. If you do not want to use the Atlassian Design System, you can use our guidelines and substitute our components with your own components, or you can go about accessibility in a different way if you choose.</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is this repository?</h2><a id="user-content-what-is-this-repository" aria-label="Permalink: What is this repository?" href="#what-is-this-repository"></a></p>
<p dir="auto">This repository is currently one way mirror from our internal monorepo that contains all the code for Pragmatic drag and drop.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/2182637/318564281-b45c2dfe-2c54-459e-a3e6-68b2342fe97b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTM5OTk5MDYsIm5iZiI6MTcxMzk5OTYwNiwicGF0aCI6Ii8yMTgyNjM3LzMxODU2NDI4MS1iNDVjMmRmZS0yYzU0LTQ1OWUtYTNlNi02OGIyMzQyZmU5N2IucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDQyNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA0MjRUMjMwMDA2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWQwMTYyZTAzY2ZmNjAzZDhlNzE2OTc3MjYyM2M1OGM2OTU4ZTZlZjY3YWQ5MTgxZGYyYmEzZDk2YTY1NTVkZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.TYJ6omA7YyBaTEUgm2eXaNIyMQNOv_FUCnmLl9LciSA"><img src="https://private-user-images.githubusercontent.com/2182637/318564281-b45c2dfe-2c54-459e-a3e6-68b2342fe97b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTM5OTk5MDYsIm5iZiI6MTcxMzk5OTYwNiwicGF0aCI6Ii8yMTgyNjM3LzMxODU2NDI4MS1iNDVjMmRmZS0yYzU0LTQ1OWUtYTNlNi02OGIyMzQyZmU5N2IucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDQyNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA0MjRUMjMwMDA2WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWQwMTYyZTAzY2ZmNjAzZDhlNzE2OTc3MjYyM2M1OGM2OTU4ZTZlZjY3YWQ5MTgxZGYyYmEzZDk2YTY1NTVkZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.TYJ6omA7YyBaTEUgm2eXaNIyMQNOv_FUCnmLl9LciSA" alt="Diagram of how the mirror works" width="600px"></a>
</p>
<p dir="auto">The intention of this repository is to make public our code, but not to accept code contributions (at this stage). In the future we could explore setting up a two way mirror so that contributions to this repo can also make their way back to our monorepo. You are still welcome to raise issues or suggestions on this repository!</p>
<p dir="auto">All documentation and <code>npm</code> packages are public and available for use by everyone</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Can I use this with my own Design System?</h2><a id="user-content-can-i-use-this-with-my-own-design-system" aria-label="Permalink: Can I use this with my own Design System?" href="#can-i-use-this-with-my-own-design-system"></a></p>
<p dir="auto">Yep! Pragmatic drag and drop as a <a href="https://atlassian.design/components/pragmatic-drag-and-drop/core-package" rel="nofollow">small core package</a>, and then a range of <a href="https://atlassian.design/components/pragmatic-drag-and-drop/optional-package" rel="nofollow">optional packages</a>. Some of the optional packages have dependencies on styling solutions (eg <code>emotion</code>), view libraries (eg <code>react</code>) or on some additional Atlassian outputs (eg <code>@atlaskit/tokens</code>). We have separated out optional packages that have other dependencies so they can be easily swapped with your own pieces that use your own tech stack and visual outputs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Can I use my own design language?</h2><a id="user-content-can-i-use-my-own-design-language" aria-label="Permalink: Can I use my own design language?" href="#can-i-use-my-own-design-language"></a></p>
<p dir="auto">Yep! We have created some design guidelines which embody how we want to achieve drag and drop in our products, and some of those decisions are embodied in some optional packages. However, you are free to use whatever design language you like, including ours!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is <code>@atlaskit</code>?</h2><a id="user-content-what-is-atlaskit" aria-label="Permalink: What is @atlaskit?" href="#what-is-atlaskit"></a></p>
<p dir="auto">The Pragmatic drag and drop packages are published under the <code>@atlaskit</code> namespace on <code>npm</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { draggable } from '@atlaskit/pragmatic-drag-and-drop/element/adapter';"><pre><span>import</span> <span>{</span> <span>draggable</span> <span>}</span> <span>from</span> <span>'@atlaskit/pragmatic-drag-and-drop/element/adapter'</span><span>;</span></pre></div>
<p dir="auto"><code>@atlaskit</code> is the <code>npm</code> namespace that we publish all of our public packages on from inside our internal monorepo. We <em>could</em> look at creating a separate namespace in the future just for Pragmatic drag and drop. If we do that, we'll release some tooling to help folks automatically switch over.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits</h2><a id="user-content-credits" aria-label="Permalink: Credits" href="#credits"></a></p>
<p dir="auto">Made with love by:</p>
<ul dir="auto">
<li><a href="https://twitter.com/alexandereardon" rel="nofollow">Alex Reardon</a></li>
<li><a href="https://twitter.com/DeclanWarn" rel="nofollow">Declan Warn</a></li>
<li><a href="https://twitter.com/lewishealey" rel="nofollow">Lewis Healey</a></li>
<li><a href="https://www.linkedin.com/in/elenimisthos/" rel="nofollow">Eleni Misthos</a></li>
<li><a href="https://soundcloud.com/jessebauer" rel="nofollow">Jesse Bauer</a></li>
<li><a href="https://twitter.com/MitchG23" rel="nofollow">Mitch Gavan</a></li>
<li><a href="https://twitter.com/michaelguitars7" rel="nofollow">Michael Abrahamian</a></li>
<li><a href="https://twitter.com/ReDrUmNZ" rel="nofollow">Tim Keir</a></li>
<li><a href="https://www.linkedin.com/in/gretarit/" rel="nofollow">Greta Ritchard</a></li>
<li><a href="https://www.atlassian.com/" rel="nofollow">Many other folks at Atlassian</a></li>
<li>Logo created by <a href="https://twitter.com/michelleholik" rel="nofollow">Michelle Holik</a> and <a href="https://twitter.com/vojta_holik" rel="nofollow">Vojta Holik</a></li>
</ul>
<p dir="auto">Pragmatic drag and drop stands on the shoulders of giants, including the folks who created the <a href="https://html.spec.whatwg.org/multipage/dnd.html" rel="nofollow">drag and drop specifications</a>, implemented drag and drop in browsers, and the many drag and drop libraries that came before this.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When do we stop finding new music? (133 pts)]]></title>
            <link>https://www.statsignificant.com/p/when-do-we-stop-finding-new-music</link>
            <guid>40147534</guid>
            <pubDate>Wed, 24 Apr 2024 17:50:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.statsignificant.com/p/when-do-we-stop-finding-new-music">https://www.statsignificant.com/p/when-do-we-stop-finding-new-music</a>, See on <a href="https://news.ycombinator.com/item?id=40147534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png" width="624" height="400.92" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:514,&quot;width&quot;:800,&quot;resizeWidth&quot;:624,&quot;bytes&quot;:825644,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3794bcbc-f1c9-48bf-ad20-f1f474f3de1a_800x514.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Say Anything (1989). Credit: 20th Century Studios.</figcaption></figure></div><p>I recently tried Spotify's new DJ feature in which an AI bot curates personalized listening sessions, introducing songs while explaining the intention behind its selections (much like a real-life disc jockey). Every four or five pieces, the bot interjects to set up its next block of music, ascribing a theme to these upcoming works. Here are some of my example introductions:</p><ul><li><p>"Next, we're gonna play some of your favorites from 2016."</p></li><li><p>"Here are some of your favorite indie rock songs from the 2010s."&nbsp; &nbsp;&nbsp;</p></li><li><p>"Up next, we have some music inspired by your love of 2000s hip-hop."</p></li></ul><p>With each DJ interlude, something became increasingly clear: my music taste had barely changed over the course of a decade. Armed with full knowledge of my musical interests, this AI agent had pinpointed my musical paralysis, packaging an algorithmic echo chamber of 2010s indie rock, 2000s pop, Bo Burnham, Blink-182, and Bruce Springsteen. Had my music taste stagnated?&nbsp; &nbsp; &nbsp;</p><p>This minor existential tailspin sent me down a Google rabbit hole—I began frantically researching music paralysis and the science of sonic preference. Was this phenomenon of my own doing or a natural product of aging? Fortunately, the topic of song stagnation has been well-researched, aided by the robust datasets of streaming services.&nbsp;</p><p>So today, we'll explore how our relationship to music changes with age and the developmental phenomena driving our forever-shifting cultural tastes.</p><p>Open-earedness refers to an individual's desire and ability to listen and consider different sounds and musical styling. Research has shown that adolescents exhibit higher levels of open-earedness, with a greater willingness to explore and appreciate diverse musical genres. During these years of sonic exploration, music gets wrapped up in the emotion and identity formation of youth; as a result, the songs of our childhood prove wildly influential over our lifelong music tastes.</p><p><span>A New York Times analysis of Spotify data revealed that </span><a href="https://www.nytimes.com/2018/02/10/opinion/sunday/favorite-songs.html" rel="">our most-played songs often stem from our teenage years, particularly between the ages of 13 and 16</a><span>.</span></p><p>This finding has personal resonance, as I remember my cultural preferences being easily influenced during my pre-teen and early teenage years. For instance, I was twelve when Green Day released their landmark "American Idiot" album, a work that proved monumental in my relationship to music. Listening to the album's titular track felt like a supreme act of rebellion (for a twelve-year-old suburbanite). I was entranced by this song's iconoclastic spirit—could they actually say, "f**k America?" &nbsp; &nbsp; &nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png" width="514" height="279.5934065934066" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:792,&quot;width&quot;:1456,&quot;resizeWidth&quot;:514,&quot;bytes&quot;:2637748,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6143e4a-29e6-4e37-9c9d-91880dd2bb94_1898x1032.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>American Idiot Music Video: Credit: Reprise Records.</figcaption></figure></div><p>But "American Idiot" wasn't a true act of revolution. In fact, the album was produced and promoted by a multinational conglomerate with the intent of packaging seemingly transgressive pop-punk acts for my exact demographic. How was I so thoroughly seduced by this song? And yet, to this day, my visceral reaction to “American Idiot” is still one of euphoria, despite my cynicism. I guess I have no choice but to love this song forever (thanks to pre-teen me).&nbsp;</p><p><span>Indeed, </span><a href="https://today.yougov.com/entertainment/articles/36462-best-decade-for-music-americans-poll-data?redirect_from=%2Ftopics%2Fentertainment%2Farticles-reports%2F2021%2F06%2F16%2Fbest-decade-for-music-americans-poll-data" rel="">YouGov survey data indicates a strong bias toward music from our teenage years</a><span>, a phenomenon that is consistent across generations. Every cohort believes that music was "better back in my day."&nbsp;&nbsp;</span></p><p>Ultimately, cultural preferences are subject to generational relativism, heavily rooted in the media of our adolescence. It's strange how much your 13-year-old self defines your lifelong artistic tastes. At this age, we're unable to drive, vote, drink alcohol, or pay taxes, yet we're old enough to cultivate enduring musical preferences.&nbsp;</p><p>The pervasive nature of music paralysis across generations suggests that the phenomenon's roots go beyond technology, likely stemming from developmental factors. So what changes as we age, and when does open-eardness decline?</p><p><strong><a href="https://www.businessinsider.com/why-we-stop-discovering-new-music-around-age-30-2018-6#:~:text=A%20survey%20from%20music%20streaming%20service%20Deezer,choice%2C%20and%20busy%20with%20work%20and%20children." rel="">Survey</a></strong><a href="https://www.businessinsider.com/why-we-stop-discovering-new-music-around-age-30-2018-6#:~:text=A%20survey%20from%20music%20streaming%20service%20Deezer,choice%2C%20and%20busy%20with%20work%20and%20children." rel="">&nbsp;</a><strong><a href="https://www.businessinsider.com/why-we-stop-discovering-new-music-around-age-30-2018-6#:~:text=A%20survey%20from%20music%20streaming%20service%20Deezer,choice%2C%20and%20busy%20with%20work%20and%20children." rel="">research from European streaming service Deezer indicates that</a></strong><a href="https://www.businessinsider.com/why-we-stop-discovering-new-music-around-age-30-2018-6#:~:text=A%20survey%20from%20music%20streaming%20service%20Deezer,choice%2C%20and%20busy%20with%20work%20and%20children." rel="">&nbsp;</a><strong><a href="https://www.businessinsider.com/why-we-stop-discovering-new-music-around-age-30-2018-6#:~:text=A%20survey%20from%20music%20streaming%20service%20Deezer,choice%2C%20and%20busy%20with%20work%20and%20children." rel="">music discovery peaks at 24</a></strong><span>, with survey respondents reporting increased variety in their music rotation during this time. However, after this age, our ability to keep up with music trends typically declines, with respondents reporting significantly lower levels of discovery in their early thirties. Ultimately,</span><strong>&nbsp;the Deezer study pinpoints 31 as the age when musical tastes start to stagnate.</strong></p><p><span>These findings have been replicated across numerous analyses, including a study of Spotify user data from 2014. Produced from Spotify's internal dataset, this </span><a href="https://skynetandebert.com/2015/04/22/music-was-better-back-then-when-do-we-stop-keeping-up-with-popular-music/" rel="">research explores how tastes deviate from the mainstream with age</a><span>. In this analysis, a contemporary pop star like Dua Lipa would score a 1 (the most popular), and an artist further out of the zeitgeist like Led Zeppelin would rank somewhere in the 200s. The resulting visual is unnerving as we observe our cultural preferences (quite literally) spiral away from the mainstream as we grow older.</span></p><p><strong>This study identifies 33 as the tipping point for sonic stagnation</strong><span>, an age where artistic taste calcifies, increasingly deviating from contemporary works. But wait, there's more. Spotify data indicates that parents stray from the mainstream at an accelerated rate compared to empty nesters—a sort of "parent tax" on one's cultural relevancy.</span></p><p><span>But this stagnation goes beyond the popularity of our music selections; it's also the diversity across these works. From 30 onward, we listen to more music outside the mainstream and </span><a href="https://musicmachinery.com/2014/02/13/age-specific-listening/" rel="">sample fewer artists during streaming sessions</a><span>.</span></p><p>Reading these studies proved an existential body blow because I am 31, apparently on the precipice of becoming a musical dinosaur. I like to think I'm special—that my high-minded dedication to culture makes me an exceptionally unique snowflake—but apparently I'm just like everybody else. I turned 30, and now I'm in a musical rut, content to have an AI bot DJ pacify me with the songs of my youth.&nbsp;</p><p>I used to spend hours researching artists, scrutinizing my CD purchases, and, later, my iTunes selections. Musical exploration was an activity in and of itself; songs were more than background noise. Now, I'm stuck listening to James Blunt's "You're Beautiful" for the 1,000th time. What happened to me?</p><p><span>Music paralysis is the product of both biological trends and practical constraints. Deezer survey respondents who identified as being "in a musical rut" </span><a href="https://www.businessinsider.com/why-we-stop-discovering-new-music-around-age-30-2018-6#:~:text=A%20survey%20from%20music%20streaming%20service%20Deezer,choice%2C%20and%20busy%20with%20work%20and%20children." rel="">cited numerous day-to-day limitations as cause for their stagnation, with the top three reasons being</a><span>:&nbsp;</span></p><ol><li><p>Overwhelmed by the amount of choice available: 19%</p></li><li><p>Having a demanding job: 16%</p></li><li><p>Caring for young children: 11%</p></li></ol><p>This first point regarding the paradox of choice is especially intriguing and would speak to streaming as some sort of societal ill, bombarding us with boundless content. It's easy to condemn Spotify for giving us too many options, but this complaint is likely emblematic of a broader developmental shift.&nbsp;</p><p><span>Context is critical to cultural discovery. An extensive cross-sectional study regarding musical attitudes and preferences from adolescence through middle age found that </span><a href="https://www.researchgate.net/publication/253337104_Music_Through_the_Ages_Trends_in_Musical_Engagement_and_Preferences_From_Adolescence_Through_Middle_Adulthood" rel="">our relationship with music drastically changes over time</a><span>. Surveying over 250,000 individuals, this study found:</span></p><ol><li><p>The degree of importance attributed to music declines with age, even though adults still consider music important.</p></li><li><p>Young people listen to music significantly more than middle-aged adults.</p></li><li><p>Young people listen to music in a wide variety of contexts and settings, whereas adults listen to music primarily in private contexts.</p></li></ol><p>The issue of music discovery does not originate from infinite choice; instead, this problem likely stems from decreased listenership and a waning commitment to exploration. Spending two hours a day combing through iTunes (now Spotify) is impractical. My priorities have changed, my emotional connection to music has changed, and I simply just don't have the time.&nbsp; &nbsp;</p><p>Indeed, this same cross-sectional study revealed that musical preferences are closely related to trends in psychosocial development. In this survey, researchers investigated how tastes vary across five dimensions as we age: intensity, contemporaneous, unpretentiousness, sophistication, and mellowness. The data they collected demonstrates a universality to our forever-changing relationship with music—it's natural to expect a progression in our preferences.&nbsp;</p><p>It's tempting to despair over these results, to accept changing cultural attitudes and the phenomenon of music paralysis as a predetermined truth. At the same time, stagnation is not a certainty. Research suggests that open-eardness and the discovery of new songs can be cultivated. Finding new music is a challenge, but it is achievable with dedicated time and effort. If we avoid the warm complacency of nostalgia, we can recapture our flare for music discovery.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png" width="616" height="369.6" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:720,&quot;width&quot;:1200,&quot;resizeWidth&quot;:616,&quot;bytes&quot;:1302433,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31e0a2b5-3d4c-4565-8ba2-752e410b8d6c_1200x720.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>High Fidelity (2000). Credit: Buena Vista Pictures.</figcaption></figure></div><p><span>My father "likes what he likes": Bruce Springsteen,&nbsp;</span><em>Field of Dreams</em><span>, The Washington Nationals, and consistently reminding me that Fleetwood Mac's&nbsp;</span><em>Rumours</em><span>&nbsp;was made after its bandmates divorced one another. Whenever I point out my dad's stubborn habits, he'll look at me, smile, and quote the immortal wisdom of Popeye: "I am what I am."&nbsp;&nbsp;</span></p><p>When I was younger, I strongly disliked this rationale. Surely, there is no fixed version of who we are. Humans are constantly evolving—perpetually engaged in self-discovery. But maybe this isn't the case for all facets of life.&nbsp; &nbsp;</p><p>The explore-exploit trade-off refers to the dilemma between seeking new information (exploring) and optimizing decisions based on known information (exploiting). Some examples of the explore-exploit trade-off include:&nbsp;</p><ul><li><p><strong>Restaurant selection</strong><span>: Do you find a new restaurant or return to your old haunts?&nbsp;</span></p></li><li><p><strong>Movies</strong><span>: Do you watch something new or re-watch an all-time favorite?&nbsp;&nbsp;</span></p></li><li><p><strong>Career</strong><span>: Should you keep your current job or look for a new one?</span></p></li></ul><p>In the case of music discovery, exploring would consist of finding new songs and subgenres, while exploiting would entail listening to already-beloved tunes.</p><p>The explore-exploit trade-off and an adjacent decision-making puzzle known as the optimal-stopping problem have prompted extensive research and the coining of a shortcut known as the 37% rule. This heuristic suggests we spend the first 37% of available search time exploring our options before settling on a preferred solution or selection.&nbsp;&nbsp;</p><p>In the case of musical preference, the current American lifespan averages 80 years; when we multiply this figure by 37%, we get 30 years—coincidentally, the age at which music tastes stagnate. This back-of-the-envelope math could be interpreted in two ways:&nbsp;</p><ol><li><p><strong>I am going crazy</strong><span>: I see numbers and symbols that don't mean anything. The 37% rule is a vague heuristic that may not even apply to this case, and I am perceiving order from true randomness. </span></p></li><li><p><strong>30 is our optimal stopping point</strong><span>: Despite the 37% rule being a highly generalized heuristic, there is some merit to doubling down on our favorites after a sustained period of searching—a phenomenon that appears to be our default state. We spend 30 years exploring new music, and once we've sampled enough works, we reach an optimal stopping point, comfortable with our rotation of artists and songs.</span></p></li></ol><p>Maybe music paralysis is a feature, not a bug. Running on a never-ending treadmill of cultural exploration may be a recipe for discontent. There is nothing inherently wrong with "liking what you like." Is it my waning music discovery that's making me unhappy or the fact that I've yet to accept this reality?</p><p>Perhaps I should forsake sonic exploration and exploit my love of "American Idiot," 2010s indie rock, 2000s pop, Bo Burnham, Blink-182, and Bruce Springsteen, content to live in an algorithmic echo chamber curated by DJ—my new AI savior.&nbsp;</p><div data-attrs="{&quot;url&quot;:&quot;https://www.statsignificant.com/p/when-do-we-stop-finding-new-music?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="CaptionedButtonToDOM"><p>This post is public so feel free to share it.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.statsignificant.com/p/when-do-we-stop-finding-new-music?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}" data-component-name="ButtonCreateButton"><a href="https://www.statsignificant.com/p/when-do-we-stop-finding-new-music?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div><p><em><span>Want to chat about data and statistics? Have an interesting data project? Just want to say hi? Email </span><a href="http://daniel@statsignificant.com/" rel="">daniel@statsignificant.com</a></em><span>&nbsp; &nbsp;&nbsp; &nbsp;</span><strong>&nbsp;</strong><span>&nbsp; </span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Stainless SDK Generator (158 pts)]]></title>
            <link>https://www.stainlessapi.com/blog/announcing-the-stainless-sdk-generator</link>
            <guid>40146505</guid>
            <pubDate>Wed, 24 Apr 2024 16:34:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stainlessapi.com/blog/announcing-the-stainless-sdk-generator">https://www.stainlessapi.com/blog/announcing-the-stainless-sdk-generator</a>, See on <a href="https://news.ycombinator.com/item?id=40146505">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Stainless generates the official client libraries for OpenAI, Anthropic, Cloudflare, and more. Today, we’re making the Stainless SDK generator available to every developer with a REST API.</p><p>During our private beta, we’ve been able to help millions of developers integrate faster and more reliably with the latest features of some of the world’s most powerful and exciting APIs.</p><p>The Stainless SDK generator accepts an OpenAPI specification and uses it to produce quality SDKs in multiple programming languages. As your API evolves, our automated generator continuously pushes changes, ensuring that your SDKs remain up-to-date—even as you make <a href="https://app.stainlessapi.com/docs/guides/patch-custom-code" target="_blank">arbitrary custom edits</a> to the generated code.</p><p>Here’s a quick, real-world example of the code we can do for you, and how you configure it with Stainless:<br>‍</p><div>



<pre><code>
import Cloudflare from "cloudflare";

async function main() {
  const cloudflare = new Cloudflare();
  const zone = await cloudflare.zones.create({
    account: { id: "xxx" },
    name: "example.com",
    type: "full",
  });
}
</code>
</pre>




<pre><code>
resources:
  zones:
    methods:
      create: post /v1/zones
</code>
</pre>

</div><p><em><br>See the source code for these endpoints in </em><a href="https://github.com/cloudflare/cloudflare-typescript/blob/54faa1bd35ae5f394e37e8ac7f75c36f738baff9/src/resources/zones/zones.ts#L27-L34" target="_blank"><em>TypeScript</em></a><em>, </em><a href="https://github.com/cloudflare/cloudflare-python/blob/77cd2adc0ea0e48fb7419dfe6d48a7a5af78f35c/src/cloudflare/resources/zones/zones.py#L129-L177" target="_blank"><em>Python</em></a><em>, and </em><a href="https://github.com/cloudflare/cloudflare-go/blob/21580812b4a3fb3f215ea1f539c2c14e2fa123bb/zones/zone.go#L50-L61" target="_blank"><em>Go</em></a><em>.</em><br></p><p>‍</p><div><blockquote>“The decision to use Stainless has allowed us to move our focus from building the generation engine to instead building high-quality schemas to describe our services. <p>In the span of a few months, we have gone from inconsistent, manually maintained SDKs to automatically shipping over 1,000 endpoints across three language SDKs with hands-off updates.”</p><p><em>Jacob Bednarz, API Platform Tech Lead, Cloudflare (</em><a href="https://blog.cloudflare.com/lessons-from-building-an-automated-sdk-pipeline" target="blank"><em>see blog post</em></a><em>)</em></p></blockquote></div><p>‍</p><h2><strong>Backstory: scaling SDKs at Stripe</strong></h2><p>From <a href="https://web.archive.org/web/20111213031731/https://stripe.com/" target="_blank">the very first day that stripe.com existed on the internet</a>, SDKs were a big part of the pitch to developers. Today, well over 90% of Stripe developers make well over 90% of requests to the Stripe API through the SDKs. As the front door to the API, the SDKs are how developers think about Stripe—to most people, it’s <code>stripe.charges.create()</code>, not <code>POST /v1/charges</code>.</p><p>Personally, I hadn’t appreciated this until I joined Stripe in 2017—but by then, the growing scope of the Stripe API had exceeded our capacity to build SDKs by hand sustainably. Making manual changes across 7 different programming languages whenever we shipped a new endpoint was toilsome and error-prone.</p><p>Not only that, TypeScript had eaten the world. Now, developers expect typeahead and documentation-on-hover directly in their text editor. Building SDKs that support comprehensive static types in a variety of languages was totally unthinkable without code generation.</p><p>The team had spent over a year exploring existing open-source code generation tools before concluding that none could meet Stripe’s quality standards. Most codegen tools either work in only one language or just use string templating, which leaves you constantly fiddling with issues like trailing commas in Go and invalid indentation in Python. We needed to build our own codegen tool that enabled us to “learn once, write everywhere” and easily produce clean, correct code across all our languages.</p><p>Over a weekend, I hacked together a surprising mashup of JSX and the internals of prettier to enable product developers to quickly template quality code that came well-formatted out of the box. Over the next several months, dozens of engineers helped convert the SDKs to codegen, matching our carefully handcrafted code byte for byte.</p><p>By later that year, I was pairing with a colleague to <a href="https://twitter.com/stripe/status/1222944951853432832" target="_blank">produce the first official TypeScript definitions</a> for the Stripe API.</p><p>‍</p><h2><strong>Scaling SDKs for everybody</strong>‍</h2><p>After I left Stripe, engineers kept asking me how to build great SDKs for their API. I didn’t have a great answer—most companies don’t have several engineer-years lying around to build whole a suite of high-quality code generators spanning a range of popular languages.</p><p>In early 2022, I set out to bootstrap a company, and Lithic became our first customer, making Stainless ramen-profitable from day one. Their head of product had previously been the PM of SDKs at Plaid, where she’d seen firsthand both how valuable SDKs are and how hard it is to codegen decent ones with the openapi-generator.</p><p>Despite the allure of a small, bootstrapped company, I felt bad limiting its impact to the small number of clients I could handle by myself. What’s more, I also got asked how to keep OpenAPI specs up-to-date and valid, how to evolve API versions, how to design RESTful pagination, how to set up API Keys, and a million other problems my old team had already solved at Stripe.</p><p>Eventually it became clear that the world needed a comprehensive developer platform—from docs to request logs to rate-limiting—that could enable REST to live up to its potential.</p><p>Sequoia soon became our first investor, shortly followed by great angels like Cristina Cordova, Guillermo Rauch, Calvin French-Owen, and dozens more.</p><p>There was clearly a huge opportunity to advance the whole REST ecosystem and help a ton of people ship better APIs.</p><p>‍</p><h2><strong>Advancing the API ecosystem</strong></h2><p>APIs are the dendrites of the internet. Literally all internet software connects through APIs—they make up the vast majority of internet traffic.</p><p>Today, the API ecosystem is deeply fragmented:</p><ol role="list"><li>GraphQL. Great for frontends, but not built for server-to-server interactions and hasn’t worked out well for public APIs.</li><li>gRPC. Great for microservices, but doesn’t work for frontends and is unpopular for public APIs.</li><li>REST. Works for frontends, microservices, and public APIs. It’s simple, flexible, and aligned with web standards—but also messy and hard to get right.</li></ol><p>Engineering organizations should be able to use one API technology for everything—their frontends, their microservices, and their public API—and have a good experience everywhere.</p><p>At Stainless, rather than trying to fit GraphQL or gRPC into the square holes they weren’t designed for—or invent some new 15th standard—we are staunch believers that “REST done right” can deliver this vision.</p><p>We want to build great open-source standards and tooling that bring the benefits of GraphQL (types, field selection/expansion, standards) and gRPC (types, speed, versioning) to REST.</p><p>When Stainless REST is realized, you’ll be able to start building a full-stack application using our API layer and have at least as good of a frontend experience as you would have had with GraphQL. When you add a Go microservice, you’ll be able to interconnect with typed clients, efficient packets, and low latency. And then—uniquely—when your biggest customer asks you for an external API, you’ll be able to just say “yes” and change <code>internal(true)</code> to <code>internal(false)</code> instead of rewriting the whole thing.</p><p>Today, our SDK announcement tackles the most salient problem with REST: type safety.</p><p>Our next project is building out a development framework that enables users to ship quality, typesafe REST APIs from any TypeScript backend. With the upcoming Stainless API framework, you declare the shape and behavior of your API in declarative TypeScript code and get an OpenAPI specification, documentation, and typed frontend client without a build step.</p><p>We’re building the framework around REST API design conventions that support rich pagination, consistent errors, field inclusion and selection, and normalized caching on the frontend. These conventions, influenced by best practices at Stripe, can help your team achieve consistent, high-quality results without untold hours of bikeshedding.</p></div><div><h2><strong>Building Stainless</strong></h2><div><blockquote>“The cool part about this is that you can define your API once and get client libraries in every language for free, whether you are an expert in those languages or not. <p> It’s rare that a startup will have people who know Python, Node, Ruby, Rust, Go, Java, etc, etc. But now they can market to all those developers at once.”
</p><p><em>Calvin French-Owen, co-founder, Segment</em></p></blockquote></div><p>Producing a good SDK is more involved than many developers may realize, especially when relying on code generation. The details matter, and it’s not just about pretty code—it’s about making the right choices and balancing some challenging tradeoffs between the characteristics of REST APIs and the idioms of the language at hand.</p><p>Here are a few generic examples:</p><ul role="list"><li>How do you handle response enums in Java? The obvious approach can result in crashes when adding a new variant in the future.</li><li>If your API introduces a union type, how do you express that in Go, given that the language does not have a standard way to express union types?</li><li>If unexpected data comes back from the server—whether due to a beta feature, an edge case, or a bug—how do you expose that data to the user? Should the client library treat it like an error? Is there an idiomatic way to achieve this across every programming language? Finding a good solution requires carefully weighing conflicting type safety and runtime safety considerations.</li><li>Should you automatically retry on 429 or 503 errors? How quickly? What if the API is experiencing a production outage?</li><li>What should you call the method for <code>/v1/invoices/{id}/void</code> in a Java client library? (hint: it can’t be <code>void</code>).</li></ul><p>Note that last problem can’t simply be decided by a machine—it requires context about the rest of the API, and must be decided by a human (even if an LLM can offer a first guess).</p><p>The <code>void</code> endpoint example is obviously an edge case, but the general question of what to name each method and type is as pernicious as it is pedestrian. If an SDK generator infers all names directly from the OpenAPI specification—particularly a specification generated from other sources—users may be confronted with nonsensical types like <code>AccountWrapperConfigurationUnionMember4</code> that raise questions about your company’s overall engineering quality.</p><p>If you ship an SDK without first addressing these issues, you risk locking yourself into design blunders that you may not be able to resolve later without breaking backwards compatibility in ways that are highly disruptive to users.</p><p>We shared the <code>void</code> example above because it is easy to understand, but there are a range of potential pitfalls that are even more subtle and abstruse that inevitably arise in non-trivial APIs. We can build tools to identify such issues, but deciding how to resolve them is often beyond the scope of what can be achieved with automation—even with AI. You need a human being with relevant context and sound judgement to assess the options and make an informed decision.</p><p>From experience, we knew that thoughtful SDK development is a lot more difficult than it seems—auditing every single type name in a typical, medium-sized API requires scanning through tens of thousands of lines of code.</p><p>To enable every developer to ship with the same level of care that we devote to our enterprise clients, we created an SDK Studio that highlights potential problems and makes it easy to quickly scan through all the things you may want to review before shipping a v1:</p><figure><p><img src="https://assets-global.website-files.com/662240109faccc0cefd740ae/66225e04b91b2310f4740bc2_Screenshot_2024-03-27_at_3.49.35_PM.png" loading="lazy" alt=""></p><figcaption>The Stainless SDK Studio</figcaption></figure><p>To start using the Stainless SDK generator, all you need is an OpenAPI specification.</p><p>Within a few minutes, you’ll get alpha SDKs you can publish to package managers—and after a bit of polishing, something you’re proud to release as v1.0.0.</p><p>To get started, check out <a href="https://app.stainlessapi.com/docs/" target="_blank">our documentation</a> or <a href="https://app.stainlessapi.com/login" target="_blank">connect your GitHub account</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US bans TikTok owner ByteDance, will prohibit app in US unless it is sold (110 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/04/biden-signs-bill-to-ban-tiktok-if-chinese-owner-bytedance-doesnt-sell/</link>
            <guid>40146203</guid>
            <pubDate>Wed, 24 Apr 2024 16:17:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/04/biden-signs-bill-to-ban-tiktok-if-chinese-owner-bytedance-doesnt-sell/">https://arstechnica.com/tech-policy/2024/04/biden-signs-bill-to-ban-tiktok-if-chinese-owner-bytedance-doesnt-sell/</a>, See on <a href="https://news.ycombinator.com/item?id=40146203">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Get outta here    —
</h4>
            
            <h2 itemprop="description">Bill gives ByteDance 270 days to sell TikTok or app loses access to US market.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/04/tiktok-800x502.jpg" alt="A TikTok app icon on a phone screen.">
      <figcaption><p>Getty Images | Chesnot </p></figcaption>  </figure>

  




<!-- cache hit 344:single/related:64de479a699749d7c72ce7bf35fcda82 --><!-- empty -->
<p>The Senate last night approved a bill that orders TikTok owner ByteDance to sell the company within 270 days or lose access to the US market. The House had already passed the bill, and President Biden signed it into law today.</p>
<p>The "Protecting Americans From Foreign Adversary Controlled Applications Act" was approved as part of a <a href="https://www.congress.gov/bill/118th-congress/house-bill/815/text">larger appropriations bill</a> that provides aid to Ukraine, Israel, and Taiwan. It passed in a <a href="https://www.senate.gov/legislative/LIS/roll_call_votes/vote1182/vote_118_2_00154.htm">79-18 vote</a>. Biden last night issued a <a href="https://www.whitehouse.gov/briefing-room/statements-releases/2024/04/23/statement-from-president-joe-biden-on-senate-passage-of-the-national-security-package/">statement</a> saying he will sign the appropriations bill into law "as soon as it reaches my desk." He signed the bill into law today, the White House <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2024/04/24/bill-signed-h-r-815/">announced</a>.</p>
<p>The bill classifies TikTok as a "foreign adversary controlled application" and gives the Chinese company ByteDance 270 days to sell it to another entity. Biden can extend the deadline by up to 90 days if a sale is in progress.</p>
<p>TikTok would maintain access to the US market if the president determines that the divestiture "would result in the relevant foreign adversary controlled application no longer being controlled by a foreign adversary." The same divestiture-or-sale requirement would apply to other applications subsequently designated as being controlled by foreign adversaries.</p>
<p>If ByteDance doesn't sell TikTok, app stores in the US would have to drop the app, and Internet hosting services would be prohibited from providing services that enable distribution of TikTok in the US. Companies that violate the prohibition would have to pay civil penalties.</p>                                            
                                                        
<h2>ByteDance will fight law in court</h2>
<p>"Congress is not acting to punish ByteDance, TikTok, or any other individual company," Senate Commerce Committee Chair Maria Cantwell (D-Wash.) said, <a href="https://apnews.com/article/tiktok-ban-congress-bill-1c48466df82f3684bd6eb21e61ebcb8d">according to the Associated Press</a>. "Congress is acting to prevent foreign adversaries from conducting espionage, surveillance, maligned operations, harming vulnerable Americans, our servicemen and women, and our US government personnel."</p>
<p><a href="https://www.reuters.com/world/us/senators-hope-tiktok-will-remain-business-us-under-new-owner-2024-04-23/">Reuters quoted</a> Sen. Ed Markey (D-Mass.) as saying the bill is "really just a TikTok ban" and that "censorship is not who we are as a people. We should not downplay or deny this trade-off." Senator Ron Wyden (D-Ore.) expressed concern that the bill "provides broad authority that could be abused by a future administration to violate Americans' First Amendment rights."</p>
<p>Despite those statements, Markey and Wyden both voted in favor of the appropriations bill that includes the TikTok-inspired law.</p>
<p>ByteDance has said it will file a lawsuit in an attempt to block the law. "This legislation is a clear violation of the First Amendment rights of TikTok's 170 million American users," Michael Beckerman, TikTok's public policy head in the US, <a href="https://arstechnica.com/tech-policy/2024/04/tiktok-ready-to-move-to-the-courts-to-prevent-ban-in-us/">reportedly</a> told staff in a memo after the House vote on Saturday. "We'll continue to fight... This is the beginning, not the end of this long process."</p>
<p>In a <a href="https://twitter.com/TikTokPolicy/status/1783149300471525637">statement</a> today, TikTok said it "will ultimately prevail" in court and that "we have invested billions of dollars to keep US data safe and our platform free from outside influence and manipulation."</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Snowflake Arctic Instruct (128x3B MoE), largest open source model (251 pts)]]></title>
            <link>https://replicate.com/snowflake/snowflake-arctic-instruct</link>
            <guid>40146088</guid>
            <pubDate>Wed, 24 Apr 2024 16:09:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://replicate.com/snowflake/snowflake-arctic-instruct">https://replicate.com/snowflake/snowflake-arctic-instruct</a>, See on <a href="https://news.ycombinator.com/item?id=40146088">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  















<div data-react-placeholder="APIPlayground">
    <div>
        <p>
          <h2>Input</h2>
        </p>
      </div>
    
    <div>
          <p>
            <h2>Output</h2>
          </p>
        </div>
  </div>



















<section id="pricing">
  <a id="performance"></a>

  <h4>Pricing</h4>

  <p>
    This language model is priced by how many input tokens are sent as inputs and how many output tokens are generated.
  </p>

  

  <p>
    Check out
    <a href="https://replicate.com/docs/billing#language-models">our docs</a>
    for more information about how per-token pricing works on Replicate.
  </p>
</section>




<article>
  <p id="readme">
    <h4>Readme</h4>
    
  </p>
  
  <div>
    <h2 id="demo-app">Demo App</h2>
<p>Want to chat with Arctic? <a href="https://arctic.streamlit.app/" rel="nofollow">Try the Streamlit demo app.</a></p>
<h2 id="model-details">Model Details</h2>
<p>Arctic is a dense-MoE Hybrid transformer architecture pre-trained from scratch by the Snowflake AI 
Research Team. We are releasing model checkpoints for both the base and instruct-tuned versions of 
Arctic under an Apache-2.0 license. This means you can use them freely in your own research, 
prototypes, and products. Please see our blog 
<a href="https://www.snowflake.com/blog/arctic-open-and-efficient-foundation-language-models-snowflake" rel="nofollow">Snowflake Arctic: The Best LLM for Enterprise AI — Efficiently Intelligent, Truly Open</a> 
for more information on Arctic and links to other relevant resources such as our series of cookbooks 
covering topics around training your own custom MoE models, how to produce high-quality training data, 
and much more.</p>
<ul>
<li><a href="https://huggingface.co/Snowflake/snowflake-arctic-base/" rel="nofollow">Arctic-Base</a></li>
<li><a href="https://huggingface.co/Snowflake/snowflake-arctic-instruct/" rel="nofollow">Arctic-Instruct</a></li>
<li><a href="https://arctic.streamlit.app/" rel="nofollow">Arctic Demo App</a></li>
</ul>
<p>For the latest details about Snowflake Arctic including tutorials, etc. please refer to our github repo: <a href="https://github.com/Snowflake-Labs/snowflake-arctic" rel="nofollow">https://github.com/Snowflake-Labs/snowflake-arctic</a></p>
<p><strong>Model developers</strong> Snowflake AI Research Team</p>
<p><strong>License</strong> Apache-2.0</p>
<p><strong>Input</strong> Models input text only.</p>
<p><strong>Output</strong> Models generate text and code only.</p>
<p><strong>Model Release Date</strong> April, 24th 2024.</p>
<h2 id="model-architecture">Model Architecture</h2>
<p>Arctic combines a 10B dense transformer model with a residual 128x3.66B MoE MLP resulting in 480B 
total and 17B active parameters chosen using a top-2 gating. For more details about Arctic’s model
Architecture, training process, data, etc. <a href="https://www.snowflake.com/en/data-cloud/arctic/cookbook/" rel="nofollow">see our series of cookbooks</a>.</p>
  </div>
  
</article>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Biden signs TikTok 'ban' bill into law, starting clock for ByteDance to divest (354 pts)]]></title>
            <link>https://www.theverge.com/2024/4/24/24139036/biden-signs-tiktok-ban-bill-divest-foreign-aid-package</link>
            <guid>40145963</guid>
            <pubDate>Wed, 24 Apr 2024 15:59:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/4/24/24139036/biden-signs-tiktok-ban-bill-divest-foreign-aid-package">https://www.theverge.com/2024/4/24/24139036/biden-signs-tiktok-ban-bill-divest-foreign-aid-package</a>, See on <a href="https://news.ycombinator.com/item?id=40145963">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>President Joe Biden signed a foreign aid package that includes a <a href="https://www.theverge.com/2024/4/23/24137638/senate-passes-tiktok-ban-bill-divest-bytedance-foreign-aid">bill that would ban TikTok if China-based parent company ByteDance fails to divest</a> the app within a year.</p></div><p>The divest-or-ban bill is now law, starting the clock for ByteDance to make its move. The company has an initial nine months to sort out a deal, though the president could extend that another three months if he sees progress.</p><p>While just recently the legislation seemed like it would stall out in the Senate after being passed as a standalone bill in the House, political maneuvering helped usher it through to Biden’s desk. The House packaged the TikTok bill — which upped the timeline for divestment from the six months allowed in the earlier version — with foreign aid to US allies, which effectively forced the Senate to consider the measures together. The longer divestment period also seemed to get some lawmakers who were on the fence on board.</p><p>TikTok spokesperson Alex Haurek said in a statement that the company plans to challenge the law in the courts, which could ultimately extend the timeline should the courts delay enforcement pending a resolution. There also remains the question of how China will respond and whether it would let ByteDance sell TikTok and, most importantly, its coveted algorithm that keeps users coming back to the app.</p><p>“As we continue to challenge this unconstitutional ban, we will continue investing and innovating to ensure TikTok remains a space where Americans of all walks of life can safely come to share their experiences, find joy, and be inspired,” Haurek said.&nbsp;</p><p>“Make no mistake, this is a ban,” TikTok CEO Shou Chew said in a video posted on TikTok Wednesday, objecting to some lawmakers’ assertions that they just want to see the platform disconnected from Chinese ownership. “A ban on TikTok and a ban on you and your voice.”</p><p><em><strong>Update, April 24th: </strong>The article has been updated with an official statement from a TikTok spokesperson and its CEO.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VideoGigaGAN: Towards Detail-Rich Video Super-Resolution (186 pts)]]></title>
            <link>https://videogigagan.github.io/</link>
            <guid>40144554</guid>
            <pubDate>Wed, 24 Apr 2024 14:04:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://videogigagan.github.io/">https://videogigagan.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=40144554">Hacker News</a></p>
<div id="readability-page-1" class="page">


  

<div>
          

          

          <p><span><sup>🐢</sup>University of Maryland, College Park</span> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <span><sup><img src="https://videogigagan.github.io/assets/images/adobe.png" width="15"></sup>Adobe Research</span>
          </p>

          
          



          
        </div>


<!-- first row -->
<div id="8xresults">


    <p>
        <h2>8× Upsampling results (128×128→1024×1024)</h2>
    </p>    
    
  
       
    

      <center>
        <p>Our model is able to upsample a video up to 8× with rich details.</p>
      </center>
  </div>

<div id="abstract">
        <h2>Abstract</h2>
        <p>Video super-resolution (VSR) approaches have shown impressive temporal consistency in upsampled videos. 
            However, these approaches tend to generate blurrier results than their image counterparts as they are limited in their generative capability.
            This raises a fundamental question: can we extend the success of a generative image upsampler to the VSR task while preserving the temporal consistency?
            We introduce VideoGigaGAN, a new generative VSR model that can produce videos with high-frequency details and temporal consistency.
            VideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. 
            Simply inflating GigaGAN to a video model by adding temporal modules produces severe temporal flickering.
            We identify several key issues and propose techniques that significantly improve the temporal consistency of upsampled videos.
            Our experiments show that, unlike previous VSR methods, VideoGigaGAN generates temporally consistent videos with more fine-grained appearance details.
            We validate the effectiveness of VideoGigaGAN by comparing it with state-of-the-art VSR models on public datasets and showcasing video results with 8× super-resolution.</p>
        <!-- <div class="column" style="display: flex; justify-content: center;">
          <div class="column has-text-justified" style="flex: 1;  max-width: 45%">
            <image src="assets/images/abstract.svg"/>
          </div>
          <div class="column has-text-justified" style="flex: 1; display: table;">
            <p style="display: table-cell; vertical-align: middle;">
              Our method only takes 15 minutes to optimize a representation from an in-the-wild video and can render novel views at 27 FPS.
              <br><br>
              On the NVIDIA Dataset, our method achieves a rendering quality comparable to state-of-the-art NeRF-based methods but is much faster to train and render. 
              <br><br>
              * The bubble size in the figure indicates the training time (GPU-hours). The training time does not include preprocessing time for all methods.
            </p>
          </div> -->
        </div>

<!-- Paper video. -->

  <div id="spotlight-video">
            <h2>
              <span>Overview: Why is it challenging?</span>
            </h2>
            
        </div>





<div id="method-overview">
        <h2>Method Overview</h2>
        <div>
          <p><img src="https://videogigagan.github.io/assets/images/method_overview.svg"></p><p>Our Video Super-Resolution (VSR) model is built upon the asymmetric U-Net architecture of the image GigaGAN upsampler. 
            To enforce temporal consistency, we first inflate the image upsampler into a video upsampler by adding <span>temporal attention</span> layers into the decoder blocks. 
            We also enhance consistency by incorporating the features from the <span>flow-guided propagation</span> module. 
            To suppress aliasing artifacts, we use <span>Anti-aliasing block</span> in the downsampling layers of the encoder. 
            Lastly, we directly <span>shuttle the high frequency features</span> via skip connection to the decoder layers to compensate for the loss of details in the BlurPool process. 
          </p>
        </div>
      </div>


<div id="ablation">
        <p>
            <h2>Ablation study</h2>
        </p>
        
        <center> 
          <p>
            Strong hallucination capability of image GigaGAN results in temporally flickering artifacts, 
            especially aliasing caused by the artifacted LR input.
          </p>
        </center>
        
        
        
        <!-- Slider for Data -->
        
        
        <p>Slide to switch between different examples</p> 

        <!-- Method Buttons -->
        <p> <br>
            We progressively add components to the base model to handle these artifacts →
          </p>
        
      <div>
        
        <p id="currentMethodDisplay2"> Image GigaGAN (base model) </p>
        <p> GT </p>
      </div>
        
        
       
        
    </div>

<div id="comparison">
        <p>
            <h2>Comparison with previous methods</h2>
        </p>
        

        <center>
          <p>
            Compared to previous models, our models provides a detail-rich result with comparable temporal consistency.
          </p>
          </center><br>

        <!-- Slider for Data -->
        
        
        
        <!-- <div class="columns is-centered has-text-centered">
          <p>Slide to switch between different examples</p>
        </div> -->

        

        

        

        <!-- Method Buttons -->
        
      
      

        

        
    </div>


<div id="4xresults">
      <p>
        <h2>Results on generic videos (128×128→512×512)</h2>
      </p>
      
      
    
    <center>
      <p>
        Our model is able to handle generic videos of different categories.
      </p>
    </center>
</div>


<div id="BibTeX">
    <h2>BibTeX</h2>
    <pre><code>@article{xu2024videogigagan,
      title={VideoGigaGAN: Towards Detail-rich Video Super-Resolution}, 
      author={Yiran Xu and Taesung Park and Richard Zhang and Yang Zhou and Eli Shechtman and Feng Liu and Jia-Bin Huang and Difan Liu},
      year={2024},
      eprint={2404.12388},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
  }</code></pre>
  </div>





  <!-- custom js file  -->
  <!-- <script defer src="./assets/js/fontawesome.all.min.js"></script> -->
  
  
  
  
  
  
  
  
  


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia to Acquire Run:AI (159 pts)]]></title>
            <link>https://blogs.nvidia.com/blog/runai/</link>
            <guid>40144235</guid>
            <pubDate>Wed, 24 Apr 2024 13:36:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.nvidia.com/blog/runai/">https://blogs.nvidia.com/blog/runai/</a>, See on <a href="https://news.ycombinator.com/item?id=40144235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
	<main id="main" role="main">
			<div>
		

<article id="post-71285" data-url="https://blogs.nvidia.com/blog/runai/" data-identifier="71285 https://blogs.nvidia.com/?p=71285" data-title="NVIDIA to Acquire GPU Orchestration Software Provider Run:ai">

	<!-- .entry-header -->

	<!-- META -->
	<!-- .entry-meta -->

	<div>
		<p>To help customers make more efficient use of their AI computing resources, NVIDIA today announced it has entered into a definitive agreement to acquire Run:ai, a Kubernetes-based workload management and orchestration software provider.</p>
<p>Customer AI deployments are becoming increasingly complex, with workloads distributed across cloud, edge and on-premises data center infrastructure.</p>
<p>Managing and orchestrating generative AI, recommender systems, search engines and other workloads requires sophisticated scheduling to optimize performance at the system level and on the underlying infrastructure.</p>
<p>Run:ai enables enterprise customers to manage and optimize their compute infrastructure, whether on premises, in the cloud or in hybrid environments.</p>
<p>The company has built an open platform on <a href="https://www.nvidia.com/en-us/glossary/kubernetes/">Kubernetes</a>, the orchestration layer for modern AI and cloud infrastructure. It supports all popular Kubernetes variants and integrates with third-party AI tools and frameworks.</p>
<p>Run:ai customers include some of the world’s largest enterprises across multiple industries, which use the Run:ai platform to manage data-center-scale GPU clusters.</p>
<p>“Run:ai has been a close collaborator with NVIDIA since 2020 and we share a passion for helping our customers make the most of their infrastructure,” said Omri Geller, Run:ai cofounder and CEO. “We’re thrilled to join NVIDIA and look forward to continuing our journey together.”</p>
<p>The Run:ai platform provides AI developers and their teams:</p>
<ul>
<li aria-level="1">A centralized interface to manage shared compute infrastructure, enabling easier and faster access for complex AI workloads.</li>
<li aria-level="1">Functionality to add users, curate them under teams, provide access to cluster resources, control over quotas, priorities and pools, and monitor and report on resource use.</li>
<li aria-level="1">The ability to pool GPUs and share computing power — from <a href="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/#:~:text=Multi%2DInstance%20GPU%20(MIG)%20expands%20the%20performance%20and%20value,%2C%20cache%2C%20and%20compute%20cores.">fractions of GPUs</a> to multiple GPUs or multiple nodes of GPUs running on different clusters — for separate tasks.</li>
<li aria-level="1">Efficient GPU cluster resource utilization, enabling customers to gain more from their compute investments.</li>
</ul>
<p>NVIDIA will continue to offer Run:ai’s products under the same business model for the immediate future. And NVIDIA will continue to invest in the Run:ai product roadmap as part of <a href="http://www.nvidia.com/dgx-cloud">NVIDIA DGX Cloud</a>, an AI platform co-engineered with leading clouds for enterprise developers, offering an integrated, full-stack service optimized for generative AI.</p>
<p>NVIDIA DGX and DGX Cloud customers will gain access to Run:ai’s capabilities for their AI workloads, particularly for large language model deployments. Run:ai’s solutions are already integrated with <a href="https://www.nvidia.com/en-us/data-center/dgx-platform/">NVIDIA DGX</a>, <a href="https://www.nvidia.com/en-us/data-center/dgx-superpod/">NVIDIA DGX SuperPOD</a>, <a href="https://www.nvidia.com/en-us/data-center/base-command/">NVIDIA Base Command</a>, <a href="https://www.nvidia.com/en-us/gpu-cloud/">NGC</a> containers, and <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software, among other products.</p>
<p>NVIDIA’s accelerated computing platform and Run:ai’s platform will continue to support a broad ecosystem of third-party solutions, giving customers choice and flexibility.</p>
<p>Together with Run:ai, NVIDIA will enable customers to have a single fabric that accesses GPU solutions anywhere. Customers can expect to benefit from better GPU utilization, improved management of GPU infrastructure and greater flexibility from the open architecture.</p>

		<!-- .entry-footer -->

	</div><!-- .entry-content -->
	
<!-- #secondary -->
	

</article><!-- #post-## -->
			<!-- .navigation -->
					</div>
			</main><!-- #main -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Basque Country’s Mondragón Corporation is the largest industrial co-op (256 pts)]]></title>
            <link>https://www.theguardian.com/lifeandstyle/2024/apr/24/in-the-us-they-think-were-communists-the-70000-workers-showing-the-world-another-way-to-earn-a-living</link>
            <guid>40143814</guid>
            <pubDate>Wed, 24 Apr 2024 12:58:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/lifeandstyle/2024/apr/24/in-the-us-they-think-were-communists-the-70000-workers-showing-the-world-another-way-to-earn-a-living">https://www.theguardian.com/lifeandstyle/2024/apr/24/in-the-us-they-think-were-communists-the-70000-workers-showing-the-world-another-way-to-earn-a-living</a>, See on <a href="https://news.ycombinator.com/item?id=40143814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p><span>W</span>hen Marisa Fernández lost her husband to cancer a few years ago, her employers at the Eroski hypermarket went, she says, “above and beyond to help me through the dark days afterwards, rejigging my timetable and giving me time off when I couldn’t face coming in.”</p><p>She had a chance to return the favour recently when the store, in Arrasate-Mondragón in Spain’s Basque Country, was undergoing renovations. Fernández, 58, who started on the cashier desk 34 years ago, and now manages the store’s non-food section, volunteered to work extra shifts over the weekend along with her colleagues to ensure everything was ready for Monday morning. “It’s not just me. Everyone is ready to go the extra mile,” she says.</p><p>Such harmonious employer-worker relations are the stuff of corporate dreams, and they are no accident here: the Eroski retail chain is part of Mondragón Corporation, the largest industrial co-op in the world. As a fully signed-up member, Fernández co-owns part of the supermarket chain that also employs her. “It feels like mine,” she says. “We work hard, but it’s a totally different feeling from working for someone else.”</p><p>That sentiment is echoed by <a href="https://www.mondragon-corporation.com/en/" data-link-name="in body link">Mondragón’s</a> 70,000 other workers. Made up of 81 autonomous co-operatives, the corporation has grown since its creation in 1956 to become a leading force in the Basque economy. Eroski is one of its most conspicuous manifestations, with 1,645 outlets across Spain. In addition to food, the chain has profitable sidelines in white goods, electronics, insurance and holiday bookings.</p><figure id="aff07454-5698-4bb0-9d19-651f202758bf" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-2"><picture><source srcset="https://i.guim.co.uk/img/media/38211391a4b98f8892df00c09a71545690c6b375/0_0_6000_4000/master/6000.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/38211391a4b98f8892df00c09a71545690c6b375/0_0_6000_4000/master/6000.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/38211391a4b98f8892df00c09a71545690c6b375/0_0_6000_4000/master/6000.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/38211391a4b98f8892df00c09a71545690c6b375/0_0_6000_4000/master/6000.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/38211391a4b98f8892df00c09a71545690c6b375/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/38211391a4b98f8892df00c09a71545690c6b375/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Aerial view of a Basque Country supermarket." src="https://i.guim.co.uk/img/media/38211391a4b98f8892df00c09a71545690c6b375/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="296.66666666666663" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Inside an Eroski supermarket in Arrasate-Mondragón.</span> Photograph: Markel Redondo/The Guardian</figcaption></figure><p>More than its economic success, though, Mondragón has become a beacon for the co-operative model, as a more humane and egalitarian way of doing business that puts “people over capital”. Every worker has a stake in the company’s fortunes and a say in how it is run, and receives a share of the profits. But the goal is more about creating “rich societies, not rich people”. That means looking after workers during not only the good times but the tough times, too.</p><p>The lowest point for Maite Aguirrebeitia, for example, came back in 2013, when, after 20 years’ service, the Mondragón co-operative that she and her husband were affiliated to, Fagor Electrodomésticos, filed for bankruptcy. Demand for its ovens and household appliances had plummeted after the 2008 financial crisis and despite help from a Mondragón “solidarity fund”, it never recovered.</p><p>“I felt this overwhelming sense of pain and grief at the time, as if someone close to me had died,” the 56-year-old communications specialist recalls. “Plus we had two kids and bills to pay and so on. The mental stress of it all was huge.”</p><p>Rather than thank the redundant workers for their service and wish them on their way, Mondragón committed to find alternative employment for as many of Fagor’s 1,900 or so workers as it could. After temporary stints in five Mondragón co-operatives in 2022, Aguirrebeitia found a permanent placement with Mondragon Assembly, a manufacturer of equipment for process automation.</p><p>Although it has meant a shift in career – she now works part-time in human resources, and part-time as a receptionist – the security of having a fixed job is a “huge relief”, she says. “I always felt confident that somehow I’d be looked after. I talked to other people who were out of work at the time and they had none of that. They were out on the street, totally alone. If I’d had to compete in the open job market against all the youngsters coming out of university, I’m not sure I’d have ever found another job.”</p><p>Mondragón’s human-centric approach originated far from any business management school. Its roots lie in a socially engaged form of Catholicism that gained ground in the 1940s, during the early years of the Francoist regime. Its initial champion was a Basque-born cleric named José María Arizmendiarrieta, who, in 1941, arrived in the small town of Arrasate-Mondragón, about 30 miles (50km) south-east of Bilbao.</p><figure id="d2208b53-b871-4eb3-8120-678234e365b0" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-3"><picture><source srcset="https://i.guim.co.uk/img/media/5a1477826656c4bf5dc552da7f2886010e556131/0_0_6000_4000/master/6000.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/5a1477826656c4bf5dc552da7f2886010e556131/0_0_6000_4000/master/6000.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/5a1477826656c4bf5dc552da7f2886010e556131/0_0_6000_4000/master/6000.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/5a1477826656c4bf5dc552da7f2886010e556131/0_0_6000_4000/master/6000.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/5a1477826656c4bf5dc552da7f2886010e556131/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/5a1477826656c4bf5dc552da7f2886010e556131/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Statue of a seated José María Arizmendiarrieta. In front of the plinth are colourful flowers." src="https://i.guim.co.uk/img/media/5a1477826656c4bf5dc552da7f2886010e556131/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="296.66666666666663" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>A statue of Mondragón Corporation founder José María Arizmendiarrieta.</span> Photograph: Markel Redondo/The Guardian</figcaption></figure><p>Taking it as his pastoral mission to revitalise the local economy, the diocesan priest set up a technical school for young men. A few years later, he arranged for some of them to take distance-learning degrees in industrial engineering. “After graduating, they all found jobs in conventional companies in the town, but they felt stifled … they wanted more of a say over their destiny, but their employers thought otherwise,” explains Ander Etxeberria, head of Mondragón’s outreach programme.</p><p>With Arizmendiarrieta’s encouragement, five of these first 11 graduates decided in 1955 to set up the now defunct Fagor Electrodomésticos. Seeking a model that reflected their Christian socialist philosophy, they turned to the <a href="https://www.theguardian.com/business/2014/apr/23/rochdale-pioneers-paul-flowers-co-operative-debts" data-link-name="in body link">Rochdale Pioneers</a>, a group of tradespeople from the Lancashire town who, more than a century before, had established the world’s first co-operative. That venture grew to become today’s Co-operative Group, home to the UK’s fifth biggest food retailer and its largest provider of funeral services.</p><p>Mondragón’s founders adopted wholesale many of the Pioneers’ core tenets. In their modern-day headquarters, located in a renovated 14th-century tower with a spectacular mountain backdrop, Etxeberria counts off the group’s 10 “basic principles”. The list ranges from the sovereignty of labour and democratic organisation (one member, one vote), to wage solidarity and “social transformation” – which includes reinvesting surpluses to create new jobs, supporting local charities and community development projects, and strengthening the Basque Country’s Euskara language. Top of the list is voluntary and open membership – namely, the opportunity for everyone to have a personal stake in the enterprise where they work. As an early version of the principles reads: “The first form of elemental justice that we need to practise is to consider each other as free human beings.”</p><figure id="2eb6c58a-b663-4fc0-8789-f6239dd3c5da" data-spacefinder-role="supporting" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-4"><picture><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=380&amp;dpr=2&amp;s=none" media="(min-width: 1300px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1300px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=380&amp;dpr=1&amp;s=none" media="(min-width: 1300px)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=300&amp;dpr=2&amp;s=none" media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=300&amp;dpr=1&amp;s=none" media="(min-width: 980px)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Bearded middle-aged man, wearing a grey blazer and dark shirt." src="https://i.guim.co.uk/img/media/2b367f9256078da1b92282fa6e078feedebc6650/0_0_4000_6000/master/4000.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="667.5" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Ander Etxeberria, head of outreach programmes at the Mondragón Corporation.</span> Photograph: Markel Redondo/The Guardian</figcaption></figure><p>These values hold true into the present, Etxeberria explains. The salary differential between the highest and lowest paid workers in Mondragón, for example, remains about six to one; for the largest 500 listed companies in the US, the <a href="https://www.reuters.com/business/ceo-pay-averaged-167-million-last-year-sp-500-companies-decline-2023-08-03/" data-link-name="in body link">gap is closer to 272 to one.</a> At the year end, members of Mondragón’s co-operatives also decide collectively on whether they should pay themselves bonuses and, if so, how much. This profit-sharing comes in addition to a base pay rate that, on average, is 40% above Spain’s minimum wage.</p><p>Despite its social responsibility credentials, Mondragón remains a competitive business. When Etxeberria presses “play” on an introductory video, the screen shows not pictures of happy workers doing yoga but gleaming industrial facilities and straight-faced technicians in lab coats. Overlaying these images are facts and figures that would have mainstream financiers salivating: €10.6bn (£9.1bn) in annual revenues; a dozen research and development facilities; a global roster of blue-chip clients; and a diversified sector spread – industry, retail, finance and education.</p><p>The same no-nonsense, professional vibe is on show at Fagor Arrasate, a Mondragón affiliate located on one of the many industrial estates that ring Arrasate-Mondragón, a vibrant town of cafe-strewn streets and busy bars. A specialist in metal presses and stamping systems, Fagor Arrasate boasts several hangar-sized workshops full of robotic machinery and giant components ready for export. “Some of the installations we make for customers can be three to four storeys high, so these are massive, multimillion-euro investments,” enthuses Edorta Mendieta, the venture’s marketing manager.</p><p>Pinned to a cork noticeboard beside a busy production line are photos of a recent charity run, a printout of donations to local causes (including €60,000 for a nearby organic food association), and a poster about a forthcoming “women in science” event. In the centre of the board, an A4 sheet of closely printed text gives notice of the co-operative’s general assembly, where next year’s strategy plan will be put to an all-member vote.</p><figure id="d2bfe53e-f89f-443b-a471-97a981705da2" data-spacefinder-role="showcase" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-5"><picture><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=880&amp;dpr=2&amp;s=none" media="(min-width: 1300px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1300px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=880&amp;dpr=1&amp;s=none" media="(min-width: 1300px)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=800&amp;dpr=2&amp;s=none" media="(min-width: 1140px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 1140px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=800&amp;dpr=1&amp;s=none" media="(min-width: 1140px)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=640&amp;dpr=2&amp;s=none" media="(min-width: 980px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 980px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=640&amp;dpr=1&amp;s=none" media="(min-width: 980px)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Mondragón in the Basque Country, where a number of Mondragón Corporation’s factories are based." src="https://i.guim.co.uk/img/media/15bfa8e2bc3f9408a07efc627e71c0242c70f14a/0_434_5272_3163/master/5272.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="266.9831183611533" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Mondragón in the Basque Country, where a number of Mondragón Corporation’s factories are based.</span> Photograph: Markel Redondo/The Guardian</figcaption></figure><p>Mondragón’s collective spirit also offers an edge with innovation. In a process that the movement refers to as “inter-cooperation”, co-operatives within the group frequently swap ideas between themselves and engage in joint research.</p><p>Over the years, many of the best innovations have come from alliances with Mondragón’s homegrown university. Located on a leafy campus in Arrasate-Mondragón, the university was set up with a strong practical bent to both its teaching and its research. So much so, in fact, that the European Commission recently selected it to co-lead a major “dual training” programme aimed at blending academia with business to tackle global challenges such as the climate crisis.</p><p>Mondragón’s approach has proved itself profitable and resilient, so could it become a realistic alternative to the modern corporation?</p><p>It’s a moot point. Despite their worker-first philosophy, the movement’s leaders are reluctant to denounce the wealth-maximising nature of modern market capitalism. The reason is as simple as it is awkward: Mondragón must actively participate in that same capitalist system for its survival.</p><p>This tactic of being “in, but not of” the world of mainstream business has seen the Basque-based movement face charges of double standards. In particular, critics highlight its outsourcing of some of its production to low-wage countries with weaker labour standards, such as China and Mexico. Mondragón argues that it has checks and balances in place to ensure that its foreign business partners uphold workers’ rights, and that keeping costs low is part and parcel of staying competitive. “For us, workers will always come before capital. But capital is still important because without it we cannot fulfil our mission of social transformation,” says Javier Marcos, Mondragón’s director of communications.</p><p>Radical as that mission is, its focus is and always has been primarily on <em>el territorio</em> (the local Basque region); less about rewriting the global economic order and more about improving co-op members’ lives. That said, if others want to copy the Mondragón model, then its doors are always open, insists Etxeberria. In the past month alone, he has hosted large groups of policymakers and business students from the Philippines, Brazil and the US. “They come to see if our approach works in practice,” he says. “They all go back pleasantly surprised, I think.”</p><p>Young people, in particular, are attracted to the notion of business and entrepreneurship going beyond just the pursuit of profit, but they “don’t know the co-operative possibility exists,” says Ana Aguirre, a graduate of Mondragón University. The 33-year-old now co-runs <a href="https://tzbz.coop/en/home/" data-link-name="in body link">Tazebaez</a>, a worker-owned consultancy and education provider that she and eight fellow students created during their bachelor’s degree. For the few who have heard of co-operativism, she adds, most relegate it in the folksy, do-gooder box. “The problem is that it’s portrayed as something to do with charity or [philanthropic] foundations, rather than as a credible business model.”</p><figure id="7f5489aa-c109-499b-a61b-b51c59e09cb6" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-6"><picture><source srcset="https://i.guim.co.uk/img/media/5486d710179b82602e658df0b6cc0f7aa9cc23fe/0_0_6000_4000/master/6000.jpg?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/5486d710179b82602e658df0b6cc0f7aa9cc23fe/0_0_6000_4000/master/6000.jpg?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/5486d710179b82602e658df0b6cc0f7aa9cc23fe/0_0_6000_4000/master/6000.jpg?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/5486d710179b82602e658df0b6cc0f7aa9cc23fe/0_0_6000_4000/master/6000.jpg?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/5486d710179b82602e658df0b6cc0f7aa9cc23fe/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/5486d710179b82602e658df0b6cc0f7aa9cc23fe/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Three young women walking along a walkway with a roof." src="https://i.guim.co.uk/img/media/5486d710179b82602e658df0b6cc0f7aa9cc23fe/0_0_6000_4000/master/6000.jpg?width=445&amp;dpr=1&amp;s=none" width="445" height="296.66666666666663" loading="lazy"></picture></div><figcaption><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Students at the Mondragón University campus.</span> Photograph: Markel Redondo/The Guardian</figcaption></figure><p>Pursuing a co-operative model is far from plain sailing, however. Numerous hurdles exist. For one, membership does not come cheap. To join a co-operative, workers typically put up a one-off payment of about €17,000 each. Plus, just as they are entitled to a share of any profits, so, too, are they liable for any losses.</p><p>Commercial pressures can also prove acute, as Fagor Electrodomésticos’s troubled history shows. The fact that all major investment decisions have to be put to the vote can also make Mondragón’s co-operatives less agile than their conventional competitors. And finding financing can be problematic as the private capital markets are effectively closed to them, admits Fagor Arrasate’s Mendieta: “We can’t incorporate external capital into the co-operative’s share capital because we are governed by the principle of ‘one person, one vote’, which no capitalist investor would accept.”</p><p>In some parts of the world, Mondragón’s approach just looks downright weird. No one bats an eyelid at the co-operative model in countries such as Germany, “but with these ideas in Texas or Kansas, you’re basically a communist,” says Mendieta, only half jokingly.</p><figure id="1b49055b-9c22-4d44-bb7e-6647798c4ab4" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:32,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Best bits: can co-operatives compete with big business?&quot;,&quot;elementId&quot;:&quot;1b49055b-9c22-4d44-bb7e-6647798c4ab4&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/social-enterprise-network/2014/mar/14/co-operatives-compete-big-business&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;display&quot;:2,&quot;theme&quot;:4,&quot;design&quot;:10}}" config="{&quot;renderingTarget&quot;:&quot;Web&quot;,&quot;darkModeAvailable&quot;:false}"></gu-island></figure><p>Across Europe, at least, the co-operative model is widespread. In Norway, for instance, co-ops have a <a href="https://www.nbbl.no/english/" data-link-name="in body link">strong heritage in the social housing sector</a>. Italy’s <a href="https://www.lowimpact.org/posts/why-is-the-co-operative-movement-so-successful-in-emilia-romagna-with-matt-hancock-no-not-that-one" data-link-name="in body link">Emilia-Romagna region</a> boasts a long tradition of industrial co-operatives similar to that of the Basque Country. And, as well as the Co-operative Group, <a href="https://www.consultancy.uk/news/12130/co-operative-businesses-contribute-34-billion-to-uk-economy#:~:text=The%20top%20five%20is%20closed,Co%2Doperative%20and%20First%20Milk." data-link-name="in body link">the UK</a>’s almost 7,000 co-operatives include the mighty John Lewis Partnership, which has a turnover of nearly £10bn. In total, the EU hosts about <a href="https://single-market-economy.ec.europa.eu/sectors/proximity-and-social-economy/social-economy-eu/cooperatives_en" data-link-name="in body link">250,000 co-operatives, providing 5.4m jobs</a>.</p><p>Increasingly, the movement’s footprint is also being seen in company law. Germany has long required <a href="https://worker-participation.eu/legislation/european-company-se/countries-transposition/germany" data-link-name="in body link">corporate boards to have worker representatives</a>, for instance. Similarly, Spanish law allows unemployed people to lump together their unemployment benefit to set up small businesses – known as <a href="https://www.europarl.europa.eu/RegData/etudes/STUD/2016/587300/IPOL_STU(2016)587300_EN.pdf" data-link-name="in body link">Sociedades Laborales</a> – as long as they are majority owned by their employees. The rise in mainstream corporations now talking the language of employee autonomy, horizontal management, dignified wages and similar themes suggests co-operativism is leaving its mark on business company practices if not – yet – on capitalist ownership</p><p>Back in Mondragón’s fort-like headquarters, Etxeberria is quietly confident about the movement’s prospects. Co-operativism, he says, is a little like <em>zirimiri</em> – the Eusakara word for “drizzle”. “It’s the same ideas that keep falling; they’ll settle eventually.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Faer-rs: Linear algebra foundation for the Rust programming language (195 pts)]]></title>
            <link>https://github.com/sarah-ek/faer-rs</link>
            <guid>40143669</guid>
            <pubDate>Wed, 24 Apr 2024 12:41:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sarah-ek/faer-rs">https://github.com/sarah-ek/faer-rs</a>, See on <a href="https://news.ycombinator.com/item?id=40143669">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">faer</h2><a id="user-content-faer" aria-label="Permalink: faer" href="#faer"></a></p>
<p dir="auto"><a href="https://docs.rs/faer" rel="nofollow"><img src="https://camo.githubusercontent.com/54926fb41b51f51d9b0036d79246ce995716617cfcbf55a5e111429fbe7d904c/68747470733a2f2f646f63732e72732f666165722f62616467652e737667" alt="Documentation" data-canonical-src="https://docs.rs/faer/badge.svg"></a>
<a href="https://crates.io/crates/faer" rel="nofollow"><img src="https://camo.githubusercontent.com/fe6554c6f68eaf3ce5f8dbd91150957dfa2a1f5b90613ed557e56c06c3afc6e3/68747470733a2f2f696d672e736869656c64732e696f2f6372617465732f762f666165722e737667" alt="Crate" data-canonical-src="https://img.shields.io/crates/v/faer.svg"></a></p>
<p dir="auto"><code>faer</code> is a Rust crate that implements low level linear algebra routines and a high level wrapper for ease of use, in pure Rust.
The aim is to provide a fully featured library for linear algebra with focus on portability, correctness, and performance.</p>
<p dir="auto">See the <a href="https://faer-rs.github.io/" rel="nofollow">official website</a> and the <a href="https://docs.rs/faer/latest/faer" rel="nofollow">docs.rs</a> documentation for code examples and usage instructions.</p>
<p dir="auto">Questions about using the library, contributing, and future directions can be discussed in the <a href="https://discord.gg/Ak5jDsAFVZ" rel="nofollow">Discord server</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">If you'd like to contribute to <code>faer</code>, check out the list of "good first issue"
issues. These are all (or should be) issues that are suitable for getting
started, and they generally include a detailed set of instructions for what to
do. Please ask questions on the Discord server or the issue itself if anything
is unclear!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Minimum supported Rust version</h2><a id="user-content-minimum-supported-rust-version" aria-label="Permalink: Minimum supported Rust version" href="#minimum-supported-rust-version"></a></p>
<p dir="auto">The current MSRV is Rust 1.67.0.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks</h2><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">The benchmarks were run on an <code>11th Gen Intel(R) Core(TM) i5-11400 @ 2.60GHz</code> with 12 threads.</p>
<ul dir="auto">
<li><code>nalgebra</code> is used with the <code>matrixmultiply</code> backend</li>
<li><code>ndarray</code> is used with the <code>openblas</code> backend</li>
<li><code>eigen</code> is compiled with <code>-march=native -O3 -fopenmp</code></li>
</ul>
<p dir="auto">All computations are done on column major matrices containing <code>f64</code> values.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Matrix multiplication</h2><a id="user-content-matrix-multiplication" aria-label="Permalink: Matrix multiplication" href="#matrix-multiplication"></a></p>
<p dir="auto">Multiplication of two square matrices of dimension <code>n</code>.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4       40ns       41ns      139ns       29ns       17ns
    8       77ns       80ns       63ns      161ns       85ns
   16      189ns      193ns      201ns      363ns      219ns
   32      1.1µs      1.1µs      1.1µs      1.5µs      1.2µs
   64      7.9µs      7.9µs      7.9µs     10.5µs      5.1µs
   96     27.5µs     11.2µs     26.2µs     34.9µs     10.1µs
  128     65.5µs     17.1µs     35.7µs     78.3µs     32.9µs
  192    216.6µs     54.4µs     57.3µs    260.7µs     51.7µs
  256    510.8µs    117.8µs    183.2µs    602.6µs    142.9µs
  384      1.7ms    339.1µs    575.8µs        2ms    327.9µs
  512        4ms    785.6µs      1.3ms      4.7ms        1ms
  640      7.9ms      1.6ms      2.3ms      9.2ms      1.9ms
  768     13.8ms      2.9ms      3.6ms       16ms      3.2ms
  896     22.2ms      4.6ms      6.5ms     25.7ms      5.9ms
 1024     33.9ms      6.6ms      9.7ms     39.1ms      8.3ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4       40ns       41ns      139ns       29ns       17ns
    8       77ns       80ns       63ns      161ns       85ns
   16      189ns      193ns      201ns      363ns      219ns
   32      1.1µs      1.1µs      1.1µs      1.5µs      1.2µs
   64      7.9µs      7.9µs      7.9µs     10.5µs      5.1µs
   96     27.5µs     11.2µs     26.2µs     34.9µs     10.1µs
  128     65.5µs     17.1µs     35.7µs     78.3µs     32.9µs
  192    216.6µs     54.4µs     57.3µs    260.7µs     51.7µs
  256    510.8µs    117.8µs    183.2µs    602.6µs    142.9µs
  384      1.7ms    339.1µs    575.8µs        2ms    327.9µs
  512        4ms    785.6µs      1.3ms      4.7ms        1ms
  640      7.9ms      1.6ms      2.3ms      9.2ms      1.9ms
  768     13.8ms      2.9ms      3.6ms       16ms      3.2ms
  896     22.2ms      4.6ms      6.5ms     25.7ms      5.9ms
 1024     33.9ms      6.6ms      9.7ms     39.1ms      8.3ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Triangular solve</h2><a id="user-content-triangular-solve" aria-label="Permalink: Triangular solve" href="#triangular-solve"></a></p>
<p dir="auto">Solving <code>AX = B</code> in place where <code>A</code> and <code>B</code> are two square matrices of dimension <code>n</code>, and <code>A</code> is a triangular matrix.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4       20ns       19ns      755ns       39ns       65ns
    8      118ns      118ns      1.5µs      308ns      156ns
   16      498ns      502ns      3.3µs      1.5µs      671ns
   32      2.1µs      2.1µs      8.6µs      6.6µs      2.9µs
   64      9.7µs      9.8µs     25.9µs     34.2µs     13.8µs
   96     27.7µs     24.5µs     55.2µs    101.4µs     36.9µs
  128     56.4µs     39.9µs    145.2µs      232µs     81.7µs
  192    167.8µs       92µs    263.6µs    815.5µs    213.6µs
  256    367.7µs      163µs      660µs      1.9ms    488.1µs
  384      1.1ms    317.5µs      1.4ms      7.4ms      1.4ms
  512      2.6ms    662.7µs      3.5ms     17.2ms      3.3ms
  640      4.7ms      1.2ms      5.7ms     33.6ms      5.5ms
  768        8ms      2.3ms      9.4ms     56.2ms      9.3ms
  896     12.3ms      3.6ms     13.6ms     89.3ms       14ms
 1024     18.7ms      5.2ms     20.1ms    131.9ms     22.9ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4       20ns       19ns      755ns       39ns       65ns
    8      118ns      118ns      1.5µs      308ns      156ns
   16      498ns      502ns      3.3µs      1.5µs      671ns
   32      2.1µs      2.1µs      8.6µs      6.6µs      2.9µs
   64      9.7µs      9.8µs     25.9µs     34.2µs     13.8µs
   96     27.7µs     24.5µs     55.2µs    101.4µs     36.9µs
  128     56.4µs     39.9µs    145.2µs      232µs     81.7µs
  192    167.8µs       92µs    263.6µs    815.5µs    213.6µs
  256    367.7µs      163µs      660µs      1.9ms    488.1µs
  384      1.1ms    317.5µs      1.4ms      7.4ms      1.4ms
  512      2.6ms    662.7µs      3.5ms     17.2ms      3.3ms
  640      4.7ms      1.2ms      5.7ms     33.6ms      5.5ms
  768        8ms      2.3ms      9.4ms     56.2ms      9.3ms
  896     12.3ms      3.6ms     13.6ms     89.3ms       14ms
 1024     18.7ms      5.2ms     20.1ms    131.9ms     22.9ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Triangular inverse</h2><a id="user-content-triangular-inverse" aria-label="Permalink: Triangular inverse" href="#triangular-inverse"></a></p>
<p dir="auto">Computing <code>A^-1</code> where <code>A</code> is a square triangular matrix with dimension <code>n</code>.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      162ns      5.2µs      771ns       38ns       65ns
    8      514ns      5.9µs      1.5µs      308ns      156ns
   16      1.6µs      7.7µs      3.4µs      1.5µs      672ns
   32      4.2µs     10.5µs      8.7µs      6.6µs      2.9µs
   64     12.5µs     18.1µs     25.7µs     34.2µs     13.8µs
   96     30.6µs     39.8µs       55µs    101.4µs     36.9µs
  128     42.7µs     51.9µs    144.9µs      232µs     81.6µs
  192      110µs     89.7µs    262.9µs    815.7µs    213.3µs
  256    191.7µs    138.3µs    645.5µs      1.9ms    486.9µs
  384    533.5µs    274.7µs      1.4ms      6.7ms      1.4ms
  512      1.1ms    449.4µs      3.5ms     15.6ms      3.3ms
  640        2ms    861.3µs      5.6ms     30.2ms      5.5ms
  768      3.2ms      1.2ms      9.3ms     51.8ms      9.3ms
  896      4.8ms      1.7ms     13.4ms     81.9ms       14ms
 1024      7.2ms      2.4ms     19.9ms    122.8ms     22.7ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      162ns      5.2µs      771ns       38ns       65ns
    8      514ns      5.9µs      1.5µs      308ns      156ns
   16      1.6µs      7.7µs      3.4µs      1.5µs      672ns
   32      4.2µs     10.5µs      8.7µs      6.6µs      2.9µs
   64     12.5µs     18.1µs     25.7µs     34.2µs     13.8µs
   96     30.6µs     39.8µs       55µs    101.4µs     36.9µs
  128     42.7µs     51.9µs    144.9µs      232µs     81.6µs
  192      110µs     89.7µs    262.9µs    815.7µs    213.3µs
  256    191.7µs    138.3µs    645.5µs      1.9ms    486.9µs
  384    533.5µs    274.7µs      1.4ms      6.7ms      1.4ms
  512      1.1ms    449.4µs      3.5ms     15.6ms      3.3ms
  640        2ms    861.3µs      5.6ms     30.2ms      5.5ms
  768      3.2ms      1.2ms      9.3ms     51.8ms      9.3ms
  896      4.8ms      1.7ms     13.4ms     81.9ms       14ms
 1024      7.2ms      2.4ms     19.9ms    122.8ms     22.7ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cholesky decomposition</h2><a id="user-content-cholesky-decomposition" aria-label="Permalink: Cholesky decomposition" href="#cholesky-decomposition"></a></p>
<p dir="auto">Factorizing a square matrix with dimension <code>n</code> as <code>L×L.T</code>, where <code>L</code> is lower triangular.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4       49ns       49ns      149ns       52ns       43ns
    8      128ns      128ns      329ns       99ns      125ns
   16      408ns      408ns      950ns      412ns      376ns
   32      1.8µs      1.8µs      3.3µs      1.8µs      2.3µs
   64        7µs        7µs     34.6µs     10.5µs        9µs
   96       18µs     18.2µs     70.5µs     31.3µs       21µs
  128     30.1µs     30.4µs    202.2µs     77.4µs     40.3µs
  192     86.4µs     92.7µs    301.3µs    259.8µs    105.2µs
  256    161.7µs    149.4µs    711.5µs    607.4µs    216.6µs
  384    462.9µs    423.9µs      1.2ms      2.1ms    596.5µs
  512      1.1ms    619.5µs      3.8ms      5.4ms      1.3ms
  640      1.9ms      1.3ms      3.3ms     10.4ms      2.2ms
  768      3.3ms      1.8ms      5.4ms     17.9ms      3.7ms
  896        5ms      2.7ms      6.9ms     28.4ms      5.6ms
 1024      7.8ms      3.4ms     14.5ms     41.2ms      8.4ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4       49ns       49ns      149ns       52ns       43ns
    8      128ns      128ns      329ns       99ns      125ns
   16      408ns      408ns      950ns      412ns      376ns
   32      1.8µs      1.8µs      3.3µs      1.8µs      2.3µs
   64        7µs        7µs     34.6µs     10.5µs        9µs
   96       18µs     18.2µs     70.5µs     31.3µs       21µs
  128     30.1µs     30.4µs    202.2µs     77.4µs     40.3µs
  192     86.4µs     92.7µs    301.3µs    259.8µs    105.2µs
  256    161.7µs    149.4µs    711.5µs    607.4µs    216.6µs
  384    462.9µs    423.9µs      1.2ms      2.1ms    596.5µs
  512      1.1ms    619.5µs      3.8ms      5.4ms      1.3ms
  640      1.9ms      1.3ms      3.3ms     10.4ms      2.2ms
  768      3.3ms      1.8ms      5.4ms     17.9ms      3.7ms
  896        5ms      2.7ms      6.9ms     28.4ms      5.6ms
 1024      7.8ms      3.4ms     14.5ms     41.2ms      8.4ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">LU decomposition with partial pivoting</h2><a id="user-content-lu-decomposition-with-partial-pivoting" aria-label="Permalink: LU decomposition with partial pivoting" href="#lu-decomposition-with-partial-pivoting"></a></p>
<p dir="auto">Factorizing a square matrix with dimension <code>n</code> as <code>P×L×U</code>, where <code>P</code> is a permutation matrix, <code>L</code> is unit lower triangular and <code>U</code> is upper triangular.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      103ns       99ns      180ns       77ns       98ns
    8      210ns      217ns      405ns      241ns      278ns
   16      649ns      625ns      1.4µs      859ns      880ns
   32      2.7µs      2.6µs      5.6µs      4.4µs      3.9µs
   64     12.4µs     12.5µs     17.4µs     22.9µs     15.6µs
   96     30.2µs     31.6µs     34.4µs     67.9µs     36.7µs
  128     61.3µs     60.7µs     97.4µs    159.4µs      126µs
  192    163.5µs    187.3µs    182.4µs    527.8µs    425.5µs
  256      352µs    360.9µs    491.1µs      1.3ms    824.9µs
  384    968.8µs    781.3µs    909.5µs      4.5ms      1.9ms
  512      2.1ms      1.5ms      1.5ms     11.1ms      4.3ms
  640      3.8ms      2.2ms      2.2ms     20.7ms      5.6ms
  768      6.2ms      3.2ms      3.4ms     35.8ms      8.6ms
  896      9.5ms      4.6ms      4.7ms     56.1ms     11.4ms
 1024     14.4ms      6.5ms      6.7ms       88ms     17.1ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      103ns       99ns      180ns       77ns       98ns
    8      210ns      217ns      405ns      241ns      278ns
   16      649ns      625ns      1.4µs      859ns      880ns
   32      2.7µs      2.6µs      5.6µs      4.4µs      3.9µs
   64     12.4µs     12.5µs     17.4µs     22.9µs     15.6µs
   96     30.2µs     31.6µs     34.4µs     67.9µs     36.7µs
  128     61.3µs     60.7µs     97.4µs    159.4µs      126µs
  192    163.5µs    187.3µs    182.4µs    527.8µs    425.5µs
  256      352µs    360.9µs    491.1µs      1.3ms    824.9µs
  384    968.8µs    781.3µs    909.5µs      4.5ms      1.9ms
  512      2.1ms      1.5ms      1.5ms     11.1ms      4.3ms
  640      3.8ms      2.2ms      2.2ms     20.7ms      5.6ms
  768      6.2ms      3.2ms      3.4ms     35.8ms      8.6ms
  896      9.5ms      4.6ms      4.7ms     56.1ms     11.4ms
 1024     14.4ms      6.5ms      6.7ms       88ms     17.1ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">LU decomposition with full pivoting</h2><a id="user-content-lu-decomposition-with-full-pivoting" aria-label="Permalink: LU decomposition with full pivoting" href="#lu-decomposition-with-full-pivoting"></a></p>
<p dir="auto">Factorizing a square matrix with dimension <code>n</code> as <code>P×L×U×Q.T</code>, where <code>P</code> and <code>Q</code> are permutation matrices, <code>L</code> is unit lower triangular and <code>U</code> is upper triangular.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      132ns      134ns          -      111ns      164ns
    8      386ns      415ns          -      418ns      493ns
   16      1.7µs      1.7µs          -      2.3µs      2.1µs
   32      5.9µs        6µs          -     14.7µs     12.2µs
   64     25.8µs     25.4µs          -    106.4µs     72.2µs
   96     67.7µs     67.9µs          -    347.3µs    206.3µs
  128    156.4µs    155.2µs          -    819.1µs    460.9µs
  192    463.4µs    460.6µs          -      2.8ms      1.4ms
  256      1.1ms      1.1ms          -      6.6ms      3.3ms
  384      3.8ms      3.8ms          -     22.1ms       11ms
  512     10.1ms      7.9ms          -     53.4ms     27.4ms
  640     17.7ms       12ms          -    102.5ms     50.7ms
  768     31.2ms     17.5ms          -    176.9ms     87.3ms
  896     47.3ms     25.1ms          -      280ms    136.1ms
 1024     76.1ms     33.9ms          -      431ms    207.9ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      132ns      134ns          -      111ns      164ns
    8      386ns      415ns          -      418ns      493ns
   16      1.7µs      1.7µs          -      2.3µs      2.1µs
   32      5.9µs        6µs          -     14.7µs     12.2µs
   64     25.8µs     25.4µs          -    106.4µs     72.2µs
   96     67.7µs     67.9µs          -    347.3µs    206.3µs
  128    156.4µs    155.2µs          -    819.1µs    460.9µs
  192    463.4µs    460.6µs          -      2.8ms      1.4ms
  256      1.1ms      1.1ms          -      6.6ms      3.3ms
  384      3.8ms      3.8ms          -     22.1ms       11ms
  512     10.1ms      7.9ms          -     53.4ms     27.4ms
  640     17.7ms       12ms          -    102.5ms     50.7ms
  768     31.2ms     17.5ms          -    176.9ms     87.3ms
  896     47.3ms     25.1ms          -      280ms    136.1ms
 1024     76.1ms     33.9ms          -      431ms    207.9ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">QR decomposition with no pivoting</h2><a id="user-content-qr-decomposition-with-no-pivoting" aria-label="Permalink: QR decomposition with no pivoting" href="#qr-decomposition-with-no-pivoting"></a></p>
<p dir="auto">Factorizing a square matrix with dimension <code>n</code> as <code>QR</code>, where <code>Q</code> is unitary and <code>R</code> is upper triangular.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      132ns      132ns      758ns      138ns      273ns
    8      345ns      346ns      1.7µs      321ns      777ns
   16      1.1µs      1.1µs      4.8µs      1.3µs      2.2µs
   32      4.4µs      4.4µs     15.3µs      6.9µs      7.4µs
   64     30.5µs     30.1µs     61.7µs     43.4µs     45.2µs
   96     65.2µs     65.2µs    322.4µs    141.3µs     79.1µs
  128    118.4µs    118.3µs    842.4µs    320.9µs    154.3µs
  192    315.3µs    316.1µs      1.6ms      1.1ms    383.7µs
  256    643.8µs    693.4µs      2.8ms      2.4ms    794.6µs
  384      1.9ms      1.7ms      7.6ms      8.1ms      2.1ms
  512      4.1ms        3ms     16.1ms       19ms      4.5ms
  640      7.4ms      4.5ms     22.5ms     36.2ms        8ms
  768     12.2ms      6.6ms     34.7ms     62.1ms     13.2ms
  896     18.6ms      9.2ms     46.3ms     97.7ms     20.4ms
 1024     27.7ms     12.9ms     65.9ms      150ms     30.2ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      132ns      132ns      758ns      138ns      273ns
    8      345ns      346ns      1.7µs      321ns      777ns
   16      1.1µs      1.1µs      4.8µs      1.3µs      2.2µs
   32      4.4µs      4.4µs     15.3µs      6.9µs      7.4µs
   64     30.5µs     30.1µs     61.7µs     43.4µs     45.2µs
   96     65.2µs     65.2µs    322.4µs    141.3µs     79.1µs
  128    118.4µs    118.3µs    842.4µs    320.9µs    154.3µs
  192    315.3µs    316.1µs      1.6ms      1.1ms    383.7µs
  256    643.8µs    693.4µs      2.8ms      2.4ms    794.6µs
  384      1.9ms      1.7ms      7.6ms      8.1ms      2.1ms
  512      4.1ms        3ms     16.1ms       19ms      4.5ms
  640      7.4ms      4.5ms     22.5ms     36.2ms        8ms
  768     12.2ms      6.6ms     34.7ms     62.1ms     13.2ms
  896     18.6ms      9.2ms     46.3ms     97.7ms     20.4ms
 1024     27.7ms     12.9ms     65.9ms      150ms     30.2ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">QR decomposition with column pivoting</h2><a id="user-content-qr-decomposition-with-column-pivoting" aria-label="Permalink: QR decomposition with column pivoting" href="#qr-decomposition-with-column-pivoting"></a></p>
<p dir="auto">Factorizing a square matrix with dimension <code>n</code> as <code>QRP</code>, where <code>P</code> is a permutation matrix, <code>Q</code> is unitary and <code>R</code> is upper triangular.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      167ns      185ns          -      172ns      373ns
    8      430ns      433ns          -      552ns        1µs
   16      1.7µs      1.7µs          -      2.8µs      2.9µs
   32      5.9µs        6µs          -     17.6µs      9.5µs
   64     33.2µs     50.6µs          -    126.9µs     37.9µs
   96     85.6µs    104.7µs          -    421.8µs    104.7µs
  128    182.3µs    209.2µs          -    987.7µs    218.1µs
  192    548.2µs    600.4µs          -      3.3ms    628.1µs
  256      1.3ms      1.4ms          -      7.6ms      1.6ms
  384      4.6ms      3.5ms          -     25.4ms      5.6ms
  512     11.4ms      6.7ms          -       60ms     15.1ms
  640     22.2ms     10.5ms          -    116.2ms     26.6ms
  768     37.7ms     14.8ms          -    199.7ms     46.2ms
  896     60.7ms     20.1ms          -    317.9ms     71.1ms
 1024     90.2ms     30.7ms          -    488.3ms      114ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      167ns      185ns          -      172ns      373ns
    8      430ns      433ns          -      552ns        1µs
   16      1.7µs      1.7µs          -      2.8µs      2.9µs
   32      5.9µs        6µs          -     17.6µs      9.5µs
   64     33.2µs     50.6µs          -    126.9µs     37.9µs
   96     85.6µs    104.7µs          -    421.8µs    104.7µs
  128    182.3µs    209.2µs          -    987.7µs    218.1µs
  192    548.2µs    600.4µs          -      3.3ms    628.1µs
  256      1.3ms      1.4ms          -      7.6ms      1.6ms
  384      4.6ms      3.5ms          -     25.4ms      5.6ms
  512     11.4ms      6.7ms          -       60ms     15.1ms
  640     22.2ms     10.5ms          -    116.2ms     26.6ms
  768     37.7ms     14.8ms          -    199.7ms     46.2ms
  896     60.7ms     20.1ms          -    317.9ms     71.1ms
 1024     90.2ms     30.7ms          -    488.3ms      114ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Matrix inverse</h2><a id="user-content-matrix-inverse" aria-label="Permalink: Matrix inverse" href="#matrix-inverse"></a></p>
<p dir="auto">Computing the inverse of a square matrix with dimension <code>n</code>.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      795ns      7.5µs      534ns       77ns      381ns
    8      2.2µs      8.9µs      995ns      825ns      794ns
   16      5.3µs       12µs      2.9µs        4µs      2.7µs
   32     15.2µs     29.9µs     10.3µs       19µs     10.8µs
   64     49.8µs     66.2µs     40.5µs    101.2µs     45.9µs
   96    127.1µs    122.7µs    182.1µs    285.3µs    119.2µs
  128    199.9µs    172.7µs    314.9µs    661.3µs      341µs
  192      543µs    419.8µs    587.1µs      2.2ms    963.8µs
  256        1ms    668.3µs      1.1ms      5.6ms        2ms
  384      2.9ms      1.4ms      2.4ms     18.7ms      5.1ms
  512      6.2ms      2.6ms      4.6ms     44.2ms     11.9ms
  640     11.5ms      5.5ms      7.2ms       83ms     19.2ms
  768     19.2ms      8.7ms     11.2ms    142.3ms     30.9ms
  896     29.5ms     12.9ms     16.7ms    223.1ms     44.1ms
 1024     43.5ms     18.2ms     23.9ms    347.1ms     68.8ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      795ns      7.5µs      534ns       77ns      381ns
    8      2.2µs      8.9µs      995ns      825ns      794ns
   16      5.3µs       12µs      2.9µs        4µs      2.7µs
   32     15.2µs     29.9µs     10.3µs       19µs     10.8µs
   64     49.8µs     66.2µs     40.5µs    101.2µs     45.9µs
   96    127.1µs    122.7µs    182.1µs    285.3µs    119.2µs
  128    199.9µs    172.7µs    314.9µs    661.3µs      341µs
  192      543µs    419.8µs    587.1µs      2.2ms    963.8µs
  256        1ms    668.3µs      1.1ms      5.6ms        2ms
  384      2.9ms      1.4ms      2.4ms     18.7ms      5.1ms
  512      6.2ms      2.6ms      4.6ms     44.2ms     11.9ms
  640     11.5ms      5.5ms      7.2ms       83ms     19.2ms
  768     19.2ms      8.7ms     11.2ms    142.3ms     30.9ms
  896     29.5ms     12.9ms     16.7ms    223.1ms     44.1ms
 1024     43.5ms     18.2ms     23.9ms    347.1ms     68.8ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Square matrix singular value decomposition</h2><a id="user-content-square-matrix-singular-value-decomposition" aria-label="Permalink: Square matrix singular value decomposition" href="#square-matrix-singular-value-decomposition"></a></p>
<p dir="auto">Computing the SVD of a square matrix with dimension <code>n</code>.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4        2µs      1.9µs        3µs      1.3µs      1.8µs
    8      9.7µs     24.4µs      8.2µs      3.9µs      9.1µs
   16       32µs     57.8µs     25.9µs     16.9µs     49.8µs
   32      107µs    132.1µs     90.3µs     95.9µs      222µs
   64    409.1µs    381.5µs    562.5µs      555µs    987.6µs
   96    903.9µs    913.1µs      1.7ms      1.7ms      2.7ms
  128      1.6ms      1.5ms      2.9ms      4.6ms      4.3ms
  192        4ms        4ms      6.7ms     14.8ms      9.9ms
  256      7.8ms        7ms     11.7ms     47.4ms     17.3ms
  384     20.9ms     15.1ms     25.8ms    121.1ms     42.9ms
  512     45.3ms     28.1ms       52ms    472.1ms     83.9ms
  640       80ms     44.5ms     79.1ms    665.7ms    133.8ms
  768    130.9ms     78.5ms    123.9ms      1.48s    208.9ms
  896    198.4ms    110.9ms    182.8ms      2.11s    295.4ms
 1024    297.8ms      152ms    253.8ms      3.95s    433.6ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4        2µs      1.9µs        3µs      1.3µs      1.8µs
    8      9.7µs     24.4µs      8.2µs      3.9µs      9.1µs
   16       32µs     57.8µs     25.9µs     16.9µs     49.8µs
   32      107µs    132.1µs     90.3µs     95.9µs      222µs
   64    409.1µs    381.5µs    562.5µs      555µs    987.6µs
   96    903.9µs    913.1µs      1.7ms      1.7ms      2.7ms
  128      1.6ms      1.5ms      2.9ms      4.6ms      4.3ms
  192        4ms        4ms      6.7ms     14.8ms      9.9ms
  256      7.8ms        7ms     11.7ms     47.4ms     17.3ms
  384     20.9ms     15.1ms     25.8ms    121.1ms     42.9ms
  512     45.3ms     28.1ms       52ms    472.1ms     83.9ms
  640       80ms     44.5ms     79.1ms    665.7ms    133.8ms
  768    130.9ms     78.5ms    123.9ms      1.48s    208.9ms
  896    198.4ms    110.9ms    182.8ms      2.11s    295.4ms
 1024    297.8ms      152ms    253.8ms      3.95s    433.6ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thin matrix singular value decomposition</h2><a id="user-content-thin-matrix-singular-value-decomposition" aria-label="Permalink: Thin matrix singular value decomposition" href="#thin-matrix-singular-value-decomposition"></a></p>
<p dir="auto">Computing the SVD of a rectangular matrix with shape <code>(4096, n)</code>.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4     73.4µs     73.5µs      311µs    127.5µs     76.7µs
    8    170.8µs    180.7µs    813.8µs    364.3µs    302.3µs
   16    440.4µs      513µs      2.1ms      1.4ms    775.5µs
   32      1.2ms      1.2ms      5.3ms      5.2ms      3.1ms
   64      3.4ms      3.2ms     15.7ms     19.9ms        8ms
   96      6.8ms      5.4ms     30.1ms     44.5ms     17.2ms
  128     11.2ms      8.3ms     47.4ms     79.4ms     30.9ms
  192     23.6ms     16.1ms       63ms    182.2ms     60.7ms
  256     40.7ms     25.5ms       84ms    353.1ms    101.3ms
  384     90.7ms     48.3ms      133ms    904.4ms    219.7ms
  512    164.7ms     80.2ms    303.4ms      2.02s    400.7ms
  640    258.7ms    119.7ms      289ms      3.24s    646.8ms
  768    381.7ms      187ms    440.1ms      5.15s      952ms
  896    532.6ms    252.7ms    550.2ms      7.23s      1.33s
 1024    724.4ms      327ms    849.6ms     10.64s      1.75s"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4     73.4µs     73.5µs      311µs    127.5µs     76.7µs
    8    170.8µs    180.7µs    813.8µs    364.3µs    302.3µs
   16    440.4µs      513µs      2.1ms      1.4ms    775.5µs
   32      1.2ms      1.2ms      5.3ms      5.2ms      3.1ms
   64      3.4ms      3.2ms     15.7ms     19.9ms        8ms
   96      6.8ms      5.4ms     30.1ms     44.5ms     17.2ms
  128     11.2ms      8.3ms     47.4ms     79.4ms     30.9ms
  192     23.6ms     16.1ms       63ms    182.2ms     60.7ms
  256     40.7ms     25.5ms       84ms    353.1ms    101.3ms
  384     90.7ms     48.3ms      133ms    904.4ms    219.7ms
  512    164.7ms     80.2ms    303.4ms      2.02s    400.7ms
  640    258.7ms    119.7ms      289ms      3.24s    646.8ms
  768    381.7ms      187ms    440.1ms      5.15s      952ms
  896    532.6ms    252.7ms    550.2ms      7.23s      1.33s
 1024    724.4ms      327ms    849.6ms     10.64s      1.75s
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hermitian matrix eigenvalue decomposition</h2><a id="user-content-hermitian-matrix-eigenvalue-decomposition" aria-label="Permalink: Hermitian matrix eigenvalue decomposition" href="#hermitian-matrix-eigenvalue-decomposition"></a></p>
<p dir="auto">Computing the EVD of a Hermitian matrix with shape <code>(n, n)</code>.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      1.3µs      1.3µs      1.4µs      675ns        1µs
    8      3.9µs        4µs      6.6µs      2.3µs      3.4µs
   16     13.2µs     13.6µs     25.9µs     10.3µs     12.5µs
   32     50.9µs     51.1µs    167.1µs     50.8µs     49.7µs
   64    223.9µs    217.5µs      1.2ms    293.9µs    211.2µs
   96      519µs    518.2µs      2.6ms      876µs      518µs
  128    931.7µs    885.5µs      5.4ms      1.9ms      1.1ms
  192      2.2ms      2.1ms       16ms      5.8ms      3.1ms
  256      4.1ms      3.5ms     33.9ms     13.2ms      6.6ms
  384     10.5ms      8.8ms    105.5ms     42.7ms     21.2ms
  512     21.9ms     16.5ms      175ms     99.3ms     51.4ms
  640     37.6ms     26.5ms    266.2ms    187.4ms     94.2ms
  768     60.4ms     38.1ms    403.3ms    322.6ms    161.9ms
  896     90.4ms     52.2ms    615.3ms    502.5ms    249.9ms
 1024    132.1ms     68.4ms      909ms    764.1ms      392ms"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      1.3µs      1.3µs      1.4µs      675ns        1µs
    8      3.9µs        4µs      6.6µs      2.3µs      3.4µs
   16     13.2µs     13.6µs     25.9µs     10.3µs     12.5µs
   32     50.9µs     51.1µs    167.1µs     50.8µs     49.7µs
   64    223.9µs    217.5µs      1.2ms    293.9µs    211.2µs
   96      519µs    518.2µs      2.6ms      876µs      518µs
  128    931.7µs    885.5µs      5.4ms      1.9ms      1.1ms
  192      2.2ms      2.1ms       16ms      5.8ms      3.1ms
  256      4.1ms      3.5ms     33.9ms     13.2ms      6.6ms
  384     10.5ms      8.8ms    105.5ms     42.7ms     21.2ms
  512     21.9ms     16.5ms      175ms     99.3ms     51.4ms
  640     37.6ms     26.5ms    266.2ms    187.4ms     94.2ms
  768     60.4ms     38.1ms    403.3ms    322.6ms    161.9ms
  896     90.4ms     52.2ms    615.3ms    502.5ms    249.9ms
 1024    132.1ms     68.4ms      909ms    764.1ms      392ms
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Non Hermitian matrix eigenvalue decomposition</h2><a id="user-content-non-hermitian-matrix-eigenvalue-decomposition" aria-label="Permalink: Non Hermitian matrix eigenvalue decomposition" href="#non-hermitian-matrix-eigenvalue-decomposition"></a></p>
<p dir="auto">Computing the EVD of a matrix with shape <code>(n, n)</code>.</p>
<div data-snippet-clipboard-copy-content="    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      4.8µs      5.1µs      3.5µs          -      3.1µs
    8     15.6µs     16.7µs      9.6µs          -     10.5µs
   16     54.7µs     54.4µs     35.9µs          -     44.4µs
   32    270.7µs    235.6µs    172.6µs          -    199.3µs
   64      1.1ms      1.1ms        1ms          -      1.1ms
   96      2.7ms      2.9ms      5.5ms          -      3.1ms
  128      4.9ms      5.6ms     11.6ms          -      9.2ms
  192     14.4ms     14.3ms     22.4ms          -     26.9ms
  256     24.4ms     26.2ms     49.9ms          -     86.6ms
  384     56.4ms     62.6ms      107ms          -    246.1ms
  512    126.8ms    130.1ms    281.7ms          -    887.6ms
  640    205.8ms    192.6ms    415.6ms          -       1.2s
  768    323.5ms    285.6ms    547.2ms          -      2.84s
  896    438.1ms    375.8ms    704.3ms          -      3.67s
 1024    687.8ms    579.3ms    957.1ms          -         7s"><pre><code>    n       faer  faer(par)    ndarray   nalgebra      eigen
    4      4.8µs      5.1µs      3.5µs          -      3.1µs
    8     15.6µs     16.7µs      9.6µs          -     10.5µs
   16     54.7µs     54.4µs     35.9µs          -     44.4µs
   32    270.7µs    235.6µs    172.6µs          -    199.3µs
   64      1.1ms      1.1ms        1ms          -      1.1ms
   96      2.7ms      2.9ms      5.5ms          -      3.1ms
  128      4.9ms      5.6ms     11.6ms          -      9.2ms
  192     14.4ms     14.3ms     22.4ms          -     26.9ms
  256     24.4ms     26.2ms     49.9ms          -     86.6ms
  384     56.4ms     62.6ms      107ms          -    246.1ms
  512    126.8ms    130.1ms    281.7ms          -    887.6ms
  640    205.8ms    192.6ms    415.6ms          -       1.2s
  768    323.5ms    285.6ms    547.2ms          -      2.84s
  896    438.1ms    375.8ms    704.3ms          -      3.67s
 1024    687.8ms    579.3ms    957.1ms          -         7s
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google delays third-party cookie demise yet again (148 pts)]]></title>
            <link>https://digiday.com/marketing/google-delays-third-party-cookie-demise-yet-again/</link>
            <guid>40143242</guid>
            <pubDate>Wed, 24 Apr 2024 11:51:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digiday.com/marketing/google-delays-third-party-cookie-demise-yet-again/">https://digiday.com/marketing/google-delays-third-party-cookie-demise-yet-again/</a>, See on <a href="https://news.ycombinator.com/item?id=40143242">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-wrapper">
	
<div>
				<div><p> &nbsp;•&nbsp; <span>April 23, 2024&nbsp;&nbsp;•&nbsp;&nbsp;4 min read</span> &nbsp;•</p>








</div>
        <div>
            <p><img width="1030" height="579" src="https://digiday.com/wp-content/uploads/sites/3/2024/03/cookie-sunset-digiday.gif?w=1030&amp;h=579&amp;crop=1" alt="" decoding="async" fetchpriority="high"></p><p>
                        Ivy Liu                    </p>
                            </div>
    </div>
	<div>
				
<p>Google is delaying the end of third-party cookies in its Chrome browser —&nbsp;again. In other unsurprising developments, water remains wet.</p>



<p>The announcement <a href="https://privacysandbox.com/intl/en_us/news/update-on-the-plan-for-phase-out-of-third-party-cookies-on-chrome/">was made on Tuesday</a> ahead of quarterly reports from Google and the ever-watchful U.K. Competition and Markets Authority (CMA), keeping tabs on how this whole situation unfolds.</p>
<div id="piano-meter-offer">


<p>“We recognize that there are ongoing challenges related to reconciling divergent feedback from the industry, regulators and developers, and will continue to engage closely with the entire ecosystem,” according to a statement Google posted on its website for the Privacy Sandbox. “It’s also critical that the CMA has sufficient time to review all evidence including results from industry tests, which the CMA has asked market participants to provide by the end of June. Given both of these significant considerations, we will not complete third-party cookie deprecation during the second half of Q4.”</p>








<p>Google did not outline a more specific timetable beyond hoping for 2025.&nbsp;</p>





<p>This is the third time Google has pushed back its original deadline set in January 2020. Back then, the tech behemoth promised to phase out third-party cookies “within two years” to beef up security for users while surfing the web. But since then, Google’s hit the brakes twice already. And every time, it’s been to give the ad industry more prep time for something that’s been surrounded by a lot of ifs, buts, and maybes. Even at the start of the year, <a href="https://digiday.com/media/four-months-in-heres-the-rundown-of-googles-chrome-cookie-conundrum-so-far/">as Google phased out</a> cookies for one percent of browser traffic, questions loomed over when more significant changes would occur.</p>



<p>With this track record, Google’s latest delay won’t shock many. And even those caught off guard might get a pass. After all, Google had been <a href="https://digiday.com/podcasts/googles-2024-cookie-deprecation-deadline-is-still-on-says-vp-of-global-advertising-dan-taylor/">preaching for months</a> that third-party cookies would vanish from Chrome by the end of 2024.</p>



<p>Despite their assurances, hitting the deadline seemed increasingly unlikely. Especially after the CMA raised 39 “concerns” to be addressed before the plan could proceed back in January. And with the U.K. data watchdog, the Information Commissioner’s Office (ICO), voicing its own reservations earlier this month, the plot thickened. Toss in the fact that Google’s own alternatives to third-party cookies (aforementioned Sandbox) left a lot to be desired, and another delay seemed inevitable.</p>





<p>“We welcome Google’s announcement clarifying the timing of third-party cookie deprecation. This will allow time to assess the results of industry tests and resolve remaining issues,” said a spokesperson from the CMA. “Under the commitments, Google has agreed to resolve our remaining competition concerns before going ahead with third-party cookie deprecation. Working closely with the ICO we expect to conclude this process by the end of 2024.”</p>



<p>For now, Google seems to have next year in mind as the latest end date for its plan to eliminate third-party cookies. </p>



<p>“We remain committed to engaging closely with the CMA and ICO and we hope to conclude that process this year,” Google’s statement read. “Assuming we can reach an agreement, we envision proceeding with third-party cookie deprecation starting early next year.”</p>



<p>Needless to say, the ad industry isn’t holding its breath.</p>



<p>That was certainly true of the&nbsp;46&nbsp;marketers who responded to the latest survey for <a href="https://digiday.com/series/research/">Digiday+ Research</a>, which asked them for their thoughts on the third-party cookie timeline. Of those who responded, 39% said they believe it&nbsp;would&nbsp;be at some point in Q2 2025 or beyond while 26% said they&nbsp;believed&nbsp;Google will get rid of third-party cookies in the Chrome browser before the end of the year.</p>



<p>“It’s unsurprising news given the magnitude of what is happening and the involvement of the CMA, but ultimately still frustrating,” said Wayne Blodwell, founder and CEO of Impact Media, an AI powered attention platform. “However, smart measurement is comfortably the largest way advertisers can create competitive advantage by leaning into durable/non cookie methods such as attention, MMM and econometrics and connecting that to media buying so it really shouldn’t slow down the sophisticated &amp; progressive advertisers anyway.”</p>
</div>		<div>
				








				<p>https://digiday.com/?p=542257</p>
		</div>
				</div><!-- .article-columns-wrapper -->

        <div id="latest_stories">
                                        <h3>
                            <span>More in Marketing</span>
                        </h3>
                                                    
                                    </div>

    <!--
    <div class="top-stories-module">
        <div class="container">
            <div class="module-title">Digiday Top Stories</div>
			        </div>
    </div>
    -->

    

	<!-- Div to serve Piano -->
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta does everything OpenAI should be (366 pts)]]></title>
            <link>https://old.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/</link>
            <guid>40142374</guid>
            <pubDate>Wed, 24 Apr 2024 09:45:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/">https://old.reddit.com/r/MachineLearning/comments/1cbhec7/meta_does_everything_openai_should_be_d/</a>, See on <a href="https://news.ycombinator.com/item?id=40142374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>you continue to not engage with my points and bring up weird arguments</p>

<blockquote>
<p>An LLM is simply an interface to the knowledge (RAG)</p>
</blockquote>

<p>what? what does RAG have to do with this?</p>

<p>an LLM is a tool for fast next-token prediction and as it turns out, next token prediction is a really useful proxy to compress a ton of information into a model that can output it very quickly.</p>

<p>this has safety implications as part of this knowledge is harmful and these models can (and have) been used to act as agents. we have already seen toy examples of where this can all go wrong both in the wild and in research produced by the very labs that make these models (Anthropic, OpenAI). it is really hard to stop these models from acting harmfully and it is impossible to do if the attacker has full access.</p>

<p>as such, we should aim to monitor the usage of these models as their capabilities increase to stop bad actors from taking advantage of them while being undetected, which is something that can easily be done via open source models. therefore, we should restrict the release of open source models when these models become very capable (for some definition of "very").</p>

<p>now, in this argument, where am i going wrong?</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Podlite - a lightweight markup language for organizing knowledge (103 pts)]]></title>
            <link>https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released</link>
            <guid>40141980</guid>
            <pubDate>Wed, 24 Apr 2024 08:35:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released">https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released</a>, See on <a href="https://news.ycombinator.com/item?id=40141980">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><p><img src="https://podlite.org/_next/static/assets/bigtitle-social-release-1.0-6658c8c03e2211a8..png"></p><div><ul><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#7teCRT7J_NCdBW8zdpUvv">What is Podlite?</a></li><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#3VLVT80tANm8AYIXYP_F7">Key Features of Podlite</a></li><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#obLwbA9OhWbqtiPPNof_y">Podlite v1.0 Release Notes</a></li><ul><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#6pqiuJWZFulIRcHJ96kI_">Customization and Extensibility</a></li><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#u66WZFLGoxl81Aql8AZvF">Processing and data handling</a></li><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#4ns4OMojSvtl_LS2hwcCD">Content Structuring and Navigation</a></li><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#W-2U6syW6aX6LITm9CK0O">Document Presentation Enhancements</a></li><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#NWNJV73enOecmRcXdT5LM">Deprecated/removed</a></li></ul><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#PH6OSESHFPlKYdWfFGamr">Summary of Release</a></li><li><a href="https://podlite.org/2024/4/23/1/podlite-specification-v1-0-released#eDPn-NGQl-7E0na2OvrEy">Feedback and Contributions</a></li></ul></div><h2 id="7teCRT7J_NCdBW8zdpUvv">What is Podlite?</h2><p id="rJXFphGmKOE-FcU9msuvl"><a href="https://podlite.org/">Podlite</a>, a lightweight block-oriented markup language that's all about flexibility and ease of use.</p><ul id="zhpYV6pJpRyCJlSUuRLjT"><li id="3tWdeJNN07rddjxheakjq"><p id="CJn2-cCb7h_I3H8Y1Am8t">Unbound by any specific domain, programming language, or concept, Podlite stands out as a universal markup language</p></li><li id="jngGzz-qiVMoNPiNKaCGH"><p id="RzAEBN-5AHccZ_4RWIw0i">In addition, the support for Markdown markup as a standard block adds convenience and allows for the use of familiar syntax for text formatting</p></li><li id="pt_0nSI52fXf-oUHEJgin"><p id="CGZSaW3K1uCd_bxKctvcT">It's perfect for documentation, educational materials, blogging, and much more for organizing knowledge.</p></li><li id="RpO2xZ8SnTJ16At_TpZVo"><p id="77I0nLtmuqii_BewzuAKi">One of the key features of Podlite is its extensibility. This allows for defining unique and domain-specific blocks and expanding the language's functionality according to the requirements of your project.</p></li><li id="MlQfaEhXfCg-LDQBeC1Fr"><p id="s5XnzH7V65krOvOZ54bCd"><a href="https://github.com/podlite/podlite-specs">The Podlite specification</a> is published under the Artistic license 2.0.</p></li></ul><h2 id="3VLVT80tANm8AYIXYP_F7">Key Features of Podlite</h2><ul id="Lc4tmD5D3FnlCwHQBQFfM"><li id="liMtunFXm_uCpUeu3jQrT"><p id="BUybq-yXagVlokn1nNNbM"><strong>Block-based structure:</strong> At the core of Podlite is the concept of "blocks," which are the fundamental units of any document. These blocks can have defined attributes and support the inclusion of rich content.</p></li><li id="meIlb_StoJjni2C8ph38e"><p id="iR4_izTaZd6hOTky2uzPf"><strong>Rich linking capabilities:</strong> Podlite allows blocks, terms, definitions to be linked with various types of relationships, enhancing the interconnectivity and navigability of your documents.</p></li><li id="31Np_-iZ3Q4ClmBM_Tw2_"><p id="mnJx4r9WZlFnlm0TD9LCl"><strong>Support for formulas and Markdown:</strong> Whether you need to insert complex mathematical formulas or prefer the simplicity of Markdown, Podlite has you covered. It seamlessly integrates both, giving you the flexibility to choose how you want to write.</p></li><li id="m2mKUGX-WFwSlMxiiUGcM"><p id="iR0L1oCbbouoHIlHebKTl"><strong>Extensible and customizable:</strong> Extend Podlite with custom blocks and inline codes, tailoring the language to meet your specific needs.</p></li></ul><h2 id="obLwbA9OhWbqtiPPNof_y">Podlite v1.0 Release Notes</h2><p id="fp6sq05Fdj5Jrk3bxG-zn">Podlite has introduced a range of new features designed to enhance the flexibility and usability of the markup language:</p><h3 id="6pqiuJWZFulIRcHJ96kI_">Customization and Extensibility</h3><ul id="8cYbN-_eG2SWA_vkacrab"><li id="PtsVPLjyBu5aqLVdIN0Gg"><p id="Zqu_1yHy1oZXfATRTwQlM"><strong>Refactoring of Custom blocks:</strong> Enhanced customization through refactored named custom blocks.</p></li><li id="AdtQiMFJ5UFLlKksjThz8"><p id="KIuSEz2CgfYgUdxlXelXC"><strong>Inline Markup extension:</strong> <code>M&lt;&gt;</code> now extends inline markup features, allowing for more expressive text formatting.</p></li></ul><h3 id="u66WZFLGoxl81Aql8AZvF">Processing and data handling</h3><ul id="gQanCwTi_ZX-c-cnqIwoY"><li id="ei9Sowj7ZMEaokcWwqBNg"><p id="2z-BKB-F2A9mGKT2rJuPz"><strong>Markdown support:</strong> The <code>=markdown</code> block is now a standard block. Integration of a new markdown mode for the parser.</p></li><li id="Z9wELRk__ybpj02UjHR6U"><p id="ZJZYZqaRcCrStH0ioAfF6"><strong>MIME type specification:</strong> Added <code>:mime-type</code> attribute for <code>=include</code> and <code>=data</code> blocks, facilitating better handling of diverse data types.</p></li><li id="RnMEA6-qjZ5uSidKSBZ-l"><p id="3Gt7nWN9twYQ9kLNIXkfP"><strong>Data schema introduction:</strong> Implemented <code>data:</code> schema for use in <code>=picture</code> and <code>=table</code> blocks, simplifying data embedding.</p></li><li id="rZ4fo6F2TTj1iwE_27zxJ"><p id="rihq_2Ou04qggFsxV6OPG"><strong>Embedding binary data:</strong> Support for embedding binary data directly into documents with <code>:filename</code> and <code>:encoding</code> attributes in <code>=data</code> blocks.</p></li><li id="hCIr9nmFmqBS-j98st4Hq"><p id="IJMoRBTei_RKyaP-hDt0q"><strong>CSV tables:</strong> Capability to render tables directly from CSV files with the enhanced <code>=table</code> block.</p></li></ul><h3 id="4ns4OMojSvtl_LS2hwcCD">Content Structuring and Navigation</h3><ul id="SKOqFR_hSa90oqwSry_v-"><li id="i-cy5IQHowOAc3-_T4lES"><p id="HV0yZtIhoq4mYBB20TmyW"><strong>Selectors:</strong> Introduced selectors for dynamic content interaction and manipulation.</p></li><li id="yn2XiP-qrfYbZfzXi8WHr"><p id="_ZC9_t7BfbmhOlZJUhxNA"><strong>Comprehensive TOC:</strong> Enhanced <code>=toc</code> block for generating detailed tables of contents.</p></li><li id="348U5Cmou9TvGW7BsDPGk"><p id="vgN8IaA3Wf0v4QDZNPBUP"><strong>Including external content:</strong> New <code>=include</code> block for incorporating external files and documents seamlessly.</p></li><li id="b23pZyPK604iEwBb-g1eU"><p id="tXXlrK_JTPFlEOKCWIKVr"><strong>Advanced table formatting:</strong> New <code>=row</code> and <code>=cell</code> blocks with <code>:header</code>, <code>:rowspan</code>, and <code>:colspan</code> attributes for intricate table designs.</p></li><li id="jU8NB2HUgS71sWV1dS6aP"><p id="_vYRun6OrsaRNcB5Ut3er"><strong>Additional attributes for linking:</strong> Addition of <code>:id</code>, <code>:caption</code>, and <code>:lang</code> attributes to enrich document metadata and accessibility.</p></li><li id="r4r1ND8xcM2jRCV8iTR1W"><p id="smVSu9FBSUJSNd5EZSpY9"><strong>Contextual backlinks:</strong> <code>W&lt;&gt;</code> for creating contextual backlinks, enhancing document navigation and reference capabilities.</p></li><li id="qWg3LIIznslPhUO1v_fhx"><p id="j0REjor_x3w3bEpNQr_j7"><strong>Foldable content:</strong> New <code>:folded</code> attribute to toggle visibility of content, and <code>:folded-levels</code> for nested visibility control in table of contents (TOC) blocks.</p></li></ul><h3 id="W-2U6syW6aX6LITm9CK0O">Document Presentation Enhancements</h3><ul id="17b-ixLxtd6KKkbFjj2pA"><li id="0ody8CDukC_en34jt0fIS"><p id="3dYb8PEHBmmVuiP5g3Pwf"><strong>Notification blocks:</strong> Introduced notification blocks with a <code>:notify</code> attribute, improving interactivity within documents.</p></li><li id="y3Z0fdVymsBScLyDQwu79"><p id="3lc05Iu1Syiq93snyFBIR"><strong>Task lists:</strong> Introduction of task lists with the <code>=item</code> block and a <code>:checked</code> attribute to manage to-do items effectively.</p></li><li id="Q4pvdRwG6Opta3Skt9Pk9"><p id="SBFKT9Sx_ejRHprbNjgmj"><strong>Mistake marking:</strong> Introduced <code>O&lt;&gt;</code> for highlighting and marking mistakes, useful in educational and review settings.</p></li><li id="W0pzYoekc_-H0ziOdvX7i"><p id="YUizRT2snKRVUvP0xqSrb"><strong>Text positioning:</strong> New attributes <code>J&lt;&gt;</code> for justifying and <code>H&lt;&gt;</code> for horizontal positioning of text.</p></li><li id="WK4Y_grnRSPAX5M4cPN-u"><p id="PfdZL8bk6fa6ZPspMkZq5"><strong>Emojis:</strong> Added support for emojis with <code>E&lt;&gt;</code>, enriching the expressiveness of documents.</p></li><li id="6rZPD0qdzL0IQw6lJcNW6"><p id="fVTIho589SWCn3IC7sHoM"><strong>Picture insertions:</strong> New features in the <code>=picture</code> block and <code>P&lt;&gt;</code> for improved image handling.</p></li><li id="Yo4v2h9NE2wmkoHdGP8CO"><p id="ekdnk78LkIa2co-YazRVp"><strong>Mathematical formulas:</strong> Enhanced support for complex mathematical formulas with <code>=formula</code> and <code>F&lt;&gt;</code>.</p></li></ul><h3 id="NWNJV73enOecmRcXdT5LM">Deprecated/removed</h3><p id="1FHC7wLJKEzUV8Yd5sO-b">With the introduction of new functionalities, certain features have been deprecated or removed to streamline Podlite:</p><ul id="80Ew63VbHbQKuFYDnOl7B"><li id="S02A7qHs50q831sqYh8vt"><p id="ccivhnXX3krztxySyBoKF"><code>P&lt;toc:&gt;</code>&nbsp;due to&nbsp;<code>=toc</code>&nbsp;block,</p></li><li id="ULJBMYtfm2SeIpgTCphGk"><p id="Af0WEFj8UyHyjFbZTGYcO"><code>P&lt;man:&gt;</code>,&nbsp;<code>P&lt;doc:&gt;</code>&nbsp;due to&nbsp;<code>=include</code>&nbsp;block,</p></li><li id="E8Dyo-2fwkE7yfS3088bN"><p id="MqredMzloFmHkYrs0v326"><code>:margin</code>&nbsp;attribute,&nbsp;<code>=encoding</code>&nbsp;directive, Declarator block been removed due to redundancy.</p></li><li id="QeWARFqjZefHrwIvXDHjC"><p id="VV0bnYpsD4tGoNajNyCWQ">Contextual aliases, the <code>=finish</code> directive, and markers in comments,&nbsp;<code>P&lt;&gt;</code>&nbsp;- placement links,&nbsp;<code>:formatted</code>&nbsp;attribute,&nbsp;<code>:like</code>&nbsp;attribute have been removed due to redundancy and enhancements in other areas.</p></li></ul><h2 id="PH6OSESHFPlKYdWfFGamr">Summary of Release</h2><p id="K2N6_bXLtfkX8A52bNVlw">This release of Podlite introduces significant enhancements aimed at improving the flexibility, customization, and user interaction of the markup language. With the addition of new blocks, attributes, and support for rich media, Podlite is better equipped to handle a variety of documentation needs.</p><p id="tGIBOM8ULz5PQE7NKKo6A">Concurrently, the deprecation and removal of certain features have simplified the toolset, reducing overlap and enhancing user experience. These changes reflect a commitment to the continuous improvement of Podlite, ensuring it remains a versatile and powerful tool for content creators.</p><h2 id="eDPn-NGQl-7E0na2OvrEy">Feedback and Contributions</h2><p id="MqWoIzXF-iMpxeJ2pLR4W">Feedback is encouraged and can be provided through <a href="https://github.com/podlite/podlite-specs/discussions">Github Discussions</a>. This input is crucial for the ongoing development and enhancement of Podlite.</p><p id="UT5hFLXp897qUiHM_VFFp"><strong>Thank You!</strong></p><p id="0yzzIi6cl41JYxm_MbgHI"><strong>Find out more:</strong></p><ul id="BwaJm7prutS0rLWMUByxZ"><li id="D7wy8OtzlWj3s6EcvLm14"><p id="bV4KxksvFdmNv72XYA_5w">Specification : <a href="https://github.com/podlite/podlite-specs">https://github.com/podlite/podlite-specs</a>, <a href="https://podlite.org/specification">[in html format]</a></p></li><li id="Jm83bBDlCM0qBMIjhkyTv"><p id="Bd7xpNRexPtDU-8KGZXaV">Website: <a href="https://podlite.org/">https://podlite.org</a></p></li><li id="-sKDWbZ_uf20YOQeahXT5"><p id="RanuNe4yDCniSoh2qbpaI">Web playground: <a href="https://pod6.in/">https://pod6.in</a></p></li><li id="qD-nW_VZ7OUqU0QGdqk7j"><p id="AVvSxWDkOGgzTaOf5F7lH">Github:  <a href="https://github.com/podlite">https://github.com/podlite</a> ⭐️</p></li></ul><p id="gzd_T1WSQgfk666p6W7a9"><strong>Stay tuned for more updates and happy documenting!</strong></p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sysadmin friendly high speed Ethernet switching (210 pts)]]></title>
            <link>https://blog.benjojo.co.uk/post/sn2010-linux-hacking-switchdev</link>
            <guid>40141967</guid>
            <pubDate>Wed, 24 Apr 2024 08:32:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.benjojo.co.uk/post/sn2010-linux-hacking-switchdev">https://blog.benjojo.co.uk/post/sn2010-linux-hacking-switchdev</a>, See on <a href="https://news.ycombinator.com/item?id=40141967">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h3>Apr 18 2024</h3>

<p><img src="https://blog.benjojo.co.uk/asset/jZtd8Li2b9" alt="A skrillex style logo, but it says mellonx instead"></p>
<p>I’ve been on the lookout for a ethernet switch that I don’t hate, the problem with a lot of higher speed (10G and above) ethernet switches is that they are quite expensive new and if you buy the used then they rarely have many years left ( or none at all ) of software patches.</p>
<p>A lot of the low end market for ethernet switches also have infamously bad software and one of the things that annoys me the most about the networking industry as a whole is that a lot of the cheap equipment has no real way of doing software support yourself.</p>
<p>So, I was very happy to learn that a friend had a Mellanox SN2010 that they were not using and were willing to sell to me. The SN2010 (Or the HP branded SKU that I picked up SN2010M) is a 18xSFP28 (25gbit) and 4xQSFP28 (100gbit) switch that instead of being your typical switch that uses a broadcom chipset for the data plane ( the bit that actually switches the packets ), uses Mellanox’s (now nvidia) own silicon. The massive benefit of this is that the drivers (mlxsw) for the Mellanox chip are open to people who don’t want to pay large volumes of money for a SDK, unlike the broadcom counterparts.</p>
<p>So I took a punt, and bought it.</p>
<p><img src="https://blog.benjojo.co.uk/asset/8TezT7HWPi" alt="A half width 1U height switch, poking out of the top of a bag"></p>
<p>The goal that I now have is to run this relatively cheap (and power efficient at 60W) switch with as close to stock Debian as possible. That way I do not have to lean on any supplier for software updates, and I can upgrade software on the switch for as long as I need it. This does mean however that the bugs will be my responsibility (since there is no <a href="https://community.cisco.com/t5/other-network-architecture-subjects/what-does-tac-do/td-p/2212677">TAC</a> to fall back on).</p>
<p>A lot of the stuff I am going to present will be similar to <a href="https://ipng.ch/s/articles/2023/11/11/mellanox-sn2700.html">Pim’s research for a similar SN2700</a> but I will focus on how I’ve deployed my setup, now that I’ve been running this setup for some time in production without any hitches. Like the SN2700 is a 32x 100G port device, newer, larger and faster versions of these switches are also available.</p>
<p>First though, let’s take a peek…</p>
<h2>Under the hood</h2>
<p><img src="https://blog.benjojo.co.uk/asset/ZB2lDALNjP" alt="The insides of a switch, there are 4 boards, 2 back PSUs, 1 main switch board, and a riser board containing the control plane"></p>
<p>Here we see two reasonably integrated 12V DC power supplies. They are not hot swappable, however in my production experience so far I have not encountered a PSU failure where a hot swap unit would have been useful. So that is not a production concern to me.</p>
<p>In addition the fan tray can easily be swapped around, meaning you can easily transition this from being a Ports-to-Power airflow to Power-to-Ports airflow. I use the switch in Power-to-Ports as it’s easier to access the optical ports from the back of the rack than the front for my use case.</p>
<p>One thing of note while we have the lid open, Inside of the switch is a mystery QSFP connector with no cage, I’m unsure what it is used for, but I’m not willing to risk sticking an optic in it and find out!</p>
<p><img src="https://blog.benjojo.co.uk/asset/E8r0mu32yW" alt="A picture of a QSFP electrical connector without a cage on the side of a switch PCB"></p>
<h2>Debian on a Ethernet Switch</h2>
<p>Because the switch is sold under “open networking” it comes with the <a href="https://en.wikipedia.org/wiki/Open_Network_Install_Environment">ONIE</a> installation system.</p>
<p>My <a href="https://blog.benjojo.co.uk/post/dell-switch-hacking">previous experience with ONIE</a> suggests that it is not going to be incredibly useful for our use case, so we will not be using it, instead opting for a more SN2010 specific method. If you squint, this switch basically looks like a laptop with an absolutely massive network card installed into it:</p>
<p><img src="https://blog.benjojo.co.uk/asset/jWA6ToczpQ" alt="A block diagram of the SN2010"></p>
<p>So we can just install Debian on it as if it was a laptop. So first, we need to get into the firmware/BIOS of the switch.</p>
<p>First you will need (really) a USB Keyboard to get into the BIOS, when the switch boots up mash F7, if you are lucky Ctrl+B will work over the serial console, but do not count on this working.</p>
<p>If you have done this correctly a BIOS password prompt will show up, the BIOS password is typically “admin”</p>
<p>Once you are into the BIOS you can remove the USB keyboard and replace it with a USB drive with a debian netinstall, then using the serial console navigate to the EFI Shell to boot grub from the USB drive.</p>


<p>When you get to grub you need to teach the installer about the serial console, since it will be critical for you to be able to install from, you can do this by adding “console=tty0 console=ttyS0,115200” to the end of the boot options.</p>
<p>Once you have done that, you can proceed with a normal debian install, you will only see one NIC for now, since that is the built in NIC from the Intel Atom SOC that is the “management” ethernet port next to the serial console</p>
<p>Once you have finished installing, you will need to also apply the same “console=tty0 console=ttyS0,115200” into your boot options on first boot, and then set that up to be a permanent grub configuration.</p>
<p>Now that we have a working debian system, we can observe that we only currently have a single NIC, but we do have a mystery PCI device.</p>
<pre><code># lspci
00:00.0 Host bridge: Intel Corporation Atom processor C2000 SoC Transaction Router (rev 03)
00:01.0 PCI bridge: Intel Corporation Atom processor C2000 PCIe Root Port 1 (rev 03)
00:02.0 PCI bridge: Intel Corporation Atom processor C2000 PCIe Root Port 2 (rev 03)
00:03.0 PCI bridge: Intel Corporation Atom processor C2000 PCIe Root Port 3 (rev 03)
00:0b.0 Co-processor: Intel Corporation Atom processor C2000 QAT (rev 03)
00:0e.0 Host bridge: Intel Corporation Atom processor C2000 RAS (rev 03)
00:0f.0 IOMMU: Intel Corporation Atom processor C2000 RCEC (rev 03)
00:13.0 System peripheral: Intel Corporation Atom processor C2000 SMBus 2.0 (rev 03)
00:14.0 Ethernet controller: Intel Corporation Ethernet Connection I354 (rev 03)
00:16.0 USB controller: Intel Corporation Atom processor C2000 USB Enhanced Host Controller (rev 03)
00:17.0 SATA controller: Intel Corporation Atom processor C2000 AHCI SATA2 Controller (rev 03)
00:18.0 SATA controller: Intel Corporation Atom processor C2000 AHCI SATA3 Controller (rev 03)
00:1f.0 ISA bridge: Intel Corporation Atom processor C2000 PCU (rev 03)
00:1f.3 SMBus: Intel Corporation Atom processor C2000 PCU SMBus (rev 03)
01:00.0 Ethernet controller: Mellanox Technologies MT52100
</code></pre>
<p>To allow the switch to command the switch chip and see the other front panel ports we will need to use a kernel that has the mlx-core module, this module is not compiled with the “stock” debian kernels.</p>
<p>This is a case of ensuring the following modules in the kernel build config are set:</p>
<details>
<summary>Click to expand to see kernel options</summary>
<pre><code>CONFIG_NET_IPIP=m
CONFIG_NET_IPGRE_DEMUX=m
CONFIG_NET_IPGRE=m
CONFIG_IPV6_GRE=m
CONFIG_IP_MROUTE_MULTIPLE_TABLES=y
CONFIG_IP_MULTIPLE_TABLES=y
CONFIG_IPV6_MULTIPLE_TABLES=y
CONFIG_BRIDGE=m
CONFIG_VLAN_8021Q=m
CONFIG_BRIDGE_VLAN_FILTERING=y
CONFIG_BRIDGE_IGMP_SNOOPING=y
CONFIG_NET_SWITCHDEV=y
CONFIG_NET_DEVLINK=y
CONFIG_MLXFW=m
CONFIG_MLXSW_CORE=m
CONFIG_MLXSW_CORE_HWMON=y
CONFIG_MLXSW_CORE_THERMAL=y
CONFIG_MLXSW_PCI=m
CONFIG_MLXSW_I2C=m
CONFIG_MLXSW_MINIMAL=y
CONFIG_MLXSW_SWITCHX2=m
CONFIG_MLXSW_SPECTRUM=m
CONFIG_MLXSW_SPECTRUM_DCB=y
CONFIG_LEDS_MLXCPLD=m
CONFIG_NET_SCH_PRIO=m
CONFIG_NET_SCH_RED=m
CONFIG_NET_SCH_INGRESS=m
CONFIG_NET_CLS=y
CONFIG_NET_CLS_ACT=y
CONFIG_NET_ACT_MIRRED=m
CONFIG_NET_CLS_MATCHALL=m
CONFIG_NET_CLS_FLOWE=m
CONFIG_NET_ACT_GACT=m
CONFIG_NET_ACT_MIRRED=m
CONFIG_NET_ACT_SAMPLE=m
CONFIG_NET_ACT_VLAN=m
CONFIG_NET_L3_MASTER_DEV=y
CONFIG_NET_VRF=m
</code></pre>
</details>
<p>If you are not able to compile a kernel yourself, and you can try with my pre-compiled kernels (that come with zero support/security updates/guarantee) here: <a href="https://benjojo.co.uk/fp/mlx-sw-kernel-debs.tar">https://benjojo.co.uk/fp/mlx-sw-kernel-debs.tar</a></p>
<h2>Upgrading the ASIC firmware</h2>
<p>The Linux kernel driver is expecting a specific version of firmware to be running on the switch chip, so after you reboot with the new kernel you might still not have all of the interfaces. You can look in dmesg for something like:</p>
<p><code>[ 7.168728] mlxsw_spectrum 0000:01:00.0: The firmware version 13.1910.622 is incompatible with the driver (required &gt;= 13.2010.1006)</code></p>
<p>We can get these firmware blobs from <a href="https://switchdev.mellanox.com/firmware/">https://switchdev.mellanox.com/firmware/</a>, and extract them to /usr/lib/firmware/mellanox, for example the file path for the above dmesg line should be <code>/usr/lib/firmware/mellanox/mlxsw_spectrum-13.2010.1006.mfa2</code>, once you have put it there you may also want to run <code>update-initramfs -u -k all</code> and reboot and wait (for at least 10 mins) for the driver to automatically upgrade the chip firmware.</p>
<p>If you are running HPE or Catchpoint SKUs of this switch, the kernel driver may fail to upgrade the firmware with something like:</p>
<pre><code>mlxfw: Firmware flash failed: Could not lock the firmware FSM, err (-5)
</code></pre>
<p>If you encounter this try <a href="https://github.com/Mellanox/mstflint">compiling and using the user space tool</a> and running the upgrade manually</p>
<p><code>$ mstfwmanager -d 01:00.0 -i mlxsw_spectrum-13.2000.2308.mfa -f -u</code></p>
<p>If successful, the upgrade should look like:</p>
<pre><code>Device #1:
----------

  Device Type:      Spectrum
  Part Number:      Q9E63-63001_Ax
  Description:      HPE StoreFabric SN2010M 25GbE 18SFP28 4QSFP28 Half Width Switch
  PSID:             HPE0000000025
  PCI Device Name:  01:00.0
  Base MAC:         1c34daaaaa00
  Versions:         Current        Available   

     FW             13.1910.0622   13.2010.1006  
  Status:           Update required

---------
Found 1 device(s) requiring firmware update...
Device #1: Updating FW ...    
[4 mins delay]
Done

Restart needed for updates to take effect.
</code></pre>
<p>Assuming the upgrade succeeds, reboot the switch and you should see a extra 20 network interfaces appear in <code>ip link</code></p>
<p>You can double check your chip versions by running:</p>
<pre><code># devlink dev info
pci/0000:01:00.0:
  driver mlxsw_spectrum
  versions:
      fixed:
        hw.revision A1
        fw.psid HPE0000000025
      running:
        fw.version 13.2010.1006
        fw 13.2010.1006
</code></pre>
<h2>Sensible switch interface names</h2>
<p>You will likely want to apply udev rules to ensure these interfaces are named in a way that makes a bit more sense, otherwise you can physically locate each port by blinking their port LEDs with <code>ethtool -m swp1</code></p>
<p>I use the udev rules from Pim’s guide on the SN2700:</p>
<pre><code># cat &lt;&lt; EOF &gt; /etc/udev/rules.d/10-local.rules
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="mlxsw_spectrum*", \
    NAME="sw$attr{phys_port_name}"
EOF
</code></pre>
<p>Once you reboot your front panel switch interface names should now be swp* interfaces that should match roughly with the numbers on the front.</p>
<p>If you are ever unsure what you are port you are looking at on the CLI you can “eyeball” what port is what by using the port speed indicator from ethtool, for example, a 100G QSFP28 port looks like:</p>
<pre><code>root@bgptools-switch:~# ethtool swp20
Settings for swp20:
    Supported ports: [ FIBRE ]
    Supported link modes:   1000baseKX/Full
                            10000baseKR/Full
                            40000baseCR4/Full
                            40000baseSR4/Full
                            40000baseLR4/Full
                            25000baseCR/Full
                            25000baseSR/Full
                            50000baseCR2/Full
                            100000baseSR4/Full
                            100000baseCR4/Full
                            100000baseLR4_ER4/Full
</code></pre>
<p>These ports can be configured as you would a normal “software” linux router interface, complete with the routing table as well. Except most configuration you are providing to linux is automatically replicated to the ASIC for you. In my case I will use <a href="https://manpages.debian.org/bookworm/ifupdown2/interfaces.5.en.html"><code>ifupdown</code></a> to manage my interface configuration, as it is the easiest for me to debug if it ever goes wrong.</p>
<p>This allows you to have 800Gbits+ of capacity managed by a 4 Core Intel Atom CPU!</p>
<h2>Building the swooter</h2>
<video loop="true" autoplay="true" muted="true"><source src="https://blog.benjojo.co.uk/asset/xr7I3nZsAz" type="video/mp4"></video>
<p>Now that we have a working router, we can just set things up like we would a normal Linux “soft” router, except this swooter (a name I use for a switches that function as IP routers as well) can copy the setup we build inside of Linux, and put it into a data plane capable of multiples of 100 Gbit/s.</p>
<p>For the sake of this post, I will go over the setup I’ve been running in production to show you what this switch has to offer:</p>
<p><img src="https://blog.benjojo.co.uk/asset/YIWXIItk54" alt="A diagram of how I have set up my switch"></p>
<p>I have two uplink ports from my provider, they are VLAN tagged with a DIA/Internet with point to point addresses (/31 for IPv4, /127 for IPv6) that have BGP on them. There are a number of other IP services that are delivered using different VLANs on one of the provider ports.</p>
<p>In my previous setup I would be doing BGP on one of the servers, but the switch can handle both of these BGP sessions. However it’s worth knowing that the switch cannot handle “full” internet BGP tables, but I requested that my provider send IPv4 and IPv6 default routes on both BGP sessions to solve that problem.</p>
<p>In my previous setup, all of my servers sat in a private production VLAN with OSPF coordinating the IP addressing between them. Since I don’t want to change too many things at once. I’ve replicated the same thing by chaining most servers ethernet ports into a newly made “br-rack” Linux Bridge</p>
<p>There is however one server that needs this bridge in a VLAN, however that is fine since you can just make a linux VLAN interface and add that to the bridge and the driver will automatically figure this out.</p>
<p>Using ifupdown language, this is what the port configuration looks like in <code>/etc/network/interfaces</code>:</p>
<pre><code>auto swp4
iface swp4 inet manual
iface swp4 inet6 manual

auto swp4.400 # "Rack LAN"
auto swp4.700 # Service1
auto swp4.701 # Service2

auto br-rack0

iface br-rack0 inet static
        bridge_hw swp5
        bridge-ports swp4.400 swp5 swp6 swp7 swp8 swp9 swp10
        bridge_stp off
        bridge_waitport 0
        bridge_fd 0
        address 185.230.223.xxx/28
        post-up ip l set dev br-rack0 type bridge mcast_snooping 0


iface br-rack0 inet6 static
        dad-attempts 0
        address 2a0c:2f07:4896:xxx/120
</code></pre>
<p>You will want to ensure you have set <code>bridge mcast_snooping 0</code> if you plan on using OSPF, as if you have snooping enabled without extra services running on the switch, multicast traffic (including OSPF) can be disrupted.</p>
<p>You will also want to set <code>bridge_hw</code> to a switch port of your choice. Due to hardware limitations the switch chip has to use 1 range of MAC addresses for things that relate/route to it. So the <code>bridge_hw</code> option just “steals” the MAC address of a port and uses that for the bridge.</p>
<p>At this point you can just configure BGP and OSPF as you normally would, and install/export the routes into the kernel, However since the hardware can only hold around 80,000 routes some care needs to be taken to ensure that you only “install” your own Internal/OSPF routes and your provider BGP default routes.</p>
<p>For example, my own bird config looks like:</p>
<pre><code>protocol kernel {
        merge paths on;
        ipv4 {                        
                export filter {
                        if net ~ [0.0.0.0/0{0,0},185.230.223.0/24{24,32}] || source = RTS_OSPF || source = RTS_OSPF_EXT2 || source = RTS_OSPF_EXT1 then {
                                accept;
                        }
                        reject;
                };
        };
}
</code></pre>
<p>The <code>merge paths on</code> option allows the switch to <a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing">ECMP over routes</a>, useful for your default routes</p>
<pre><code>root@bgptools-switch:~# ip route
default proto bird metric 32 rt_offload
        nexthop via 192.0.2.1 dev swp1.600 weight 1 offload
        nexthop via 192.0.2.2 dev swp2.601 weight 1 offload
198.51.100.0/28 via 185.230.223.xxx dev br-rack0 proto bird metric 32 offload rt_offload
203.0.113.0/24 via 185.230.223.xxx dev br-rack0 proto bird metric 32 offload rt_offload
</code></pre>
<p>It is worth pointing out that you should also setup a sane and sensible SSH policy and firewalling. You could easily just apply the same solution that you use for your servers. Like Salt/Chef/Puppet/Ansible, after all, this is just like a server with a magic NIC in it!</p>
<p>There are also some good linux sysctl options you should set to make your swooter act more like a hardware router is expected to. As per the <a href="https://github.com/Mellanox/mlxsw/wiki/Static-Routing#recommended-sysctl-configuration">mlxsw wiki</a> recommends:</p>
<details>
<summary>Click to expand to see sysctls</summary>
<pre><code># Enable IPv4 and IPv6 forwarding.
net.ipv4.ip_forward=1
net.ipv6.conf.all.forwarding=1
net.ipv6.conf.default.forwarding=1

# Keep IPv6 addresses on an interface when it goes down. This is
# consistent with IPv4.
net.ipv6.conf.all.keep_addr_on_down=1
net.ipv6.conf.default.keep_addr_on_down=1
# Prevent the kernel from routing packets via an interface whose link is
# down. This is not strictly necessary when a routing daemon is used as
# it will most likely evict such routes. In addition, when offloaded,
# such routes will not be considered anyway since the associated neighbour
# entries will be flushed upon the carrier going down, preventing the
# device from determining the destination MAC it should use.
net.ipv4.conf.all.ignore_routes_with_linkdown=1
net.ipv6.conf.all.ignore_routes_with_linkdown=1
net.ipv4.conf.default.ignore_routes_with_linkdown=1
net.ipv6.conf.default.ignore_routes_with_linkdown=1

# Use a standard 5-tuple to compute the multi-path hash.
net.ipv4.fib_multipath_hash_policy=1
net.ipv6.fib_multipath_hash_policy=1
# Generate an unsolicited neighbour advertisement when an interface goes
# down or its hardware address changes.
net.ipv6.conf.all.ndisc_notify=1
net.ipv6.conf.default.ndisc_notify=1

# Do not perform source validation when routing IPv4 packets. This is
# consistent with the hardware data path behavior. No configuration
# is necessary for IPv6.
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
# Do not update the SKB priority from "TOS" field in IP header after
# the packet is forwarded. This applies to both IPv4 and IPv6 packets
# which are forwarded by the device.
net.ipv4.ip_forward_update_priority=0

# Prevent the kernel from generating a netlink event for each deleted
# IPv6 route when an interface goes down. This is consistent with IPv4.
net.ipv6.route.skip_notify_on_dev_down=1
# Use neighbour information when choosing a nexthop in a multi-path
# route. Will prevent the kernel from routing the packets via a
# failed nexthop. This is consistent with the hardware behavior.
net.ipv4.fib_multipath_use_neigh=1

# Increase the maximum number of cached IPv6 routes. No configuration is
# necessary for IPv4.
net.ipv6.route.max_size=16384
# In case the number of non-permanent neighbours in the system exceeds
# this value for over 5 seconds, the garbage collector will kick in.
# Default is 512, but if the system has a larger number of interfaces or
# expected to communicate with a larger number of directly-connected
# neighbours, then it is recommended to increase this value.
net.ipv4.neigh.default.gc_thresh2=8192
net.ipv6.neigh.default.gc_thresh2=8192

# In case the number of non-permanent neighbours in the system exceeds
# this value, the garbage collector will kick in. Default is 1024, but
# if the system has a larger number of interfaces or expected to
# communicate with a larger number of directly-connected neighbours,
# then it is recommended to increase this value.
net.ipv4.neigh.default.gc_thresh3=16384
net.ipv6.neigh.default.gc_thresh3=16384
</code></pre>
</details>
<h2>Hardware assisted firewalling</h2>
<p>Thanks to the switch chip, almost all traffic going through the switch will not be visible to the Debian side of the system. This does mean that you will not be able to use nf/iptables on forwarded traffic, however the switch driver does allow some Linux Traffic Control (tc) rules that use the “<a href="https://man7.org/linux/man-pages/man8/tc-flower.8.html">flower</a>” system to be inserted into hardware, For example:</p>
<pre><code>tc qdisc add dev swp1 clsact

# Rate limit UDP from port swp1 going to a IP address to 10mbit/s
tc filter add dev swp1 ingress protocol ip pref 10 \
        flower skip_sw dst_ip 192.0.2.1 ip_proto udp \
        action police rate 10mbit burst 16k conform-exceed drop/ok

# Drop TCP SYN packets from swp1 going to 192.0.2.2
tc filter add dev swp1 ingress protocol ip pref 20 \
        flower dst_ip 192.0.2.2 ip_proto tcp tcp_flags 0x17/0x02 \
        action drop
</code></pre>
<p>You can monitor the results of these rules using <code>tc -s filter show swp1 ingress</code></p>
<pre><code># tc -s filter show dev swp2 ingress
filter protocol ip pref 10 flower chain 0
filter protocol ip pref 10 flower chain 0 handle 0x1
  eth_type ipv4
  ip_proto udp
  dst_ip 192.0.2.1
  skip_sw
  in_hw in_hw_count 1
        action order 1:  police 0x7 rate 10Mbit burst 16Kb mtu 2Kb action drop overhead 0b
        ref 1 bind 1  installed 3615822 sec used 1 sec
        Action statistics:
        Sent 3447123283 bytes 4481404 pkt (dropped 1920284, overlimits 0 requeues 0)
        Sent software 0 bytes 0 pkt
        Sent hardware 3447123283 bytes 4481404 pkt
        backlog 0b 0p requeues 0
        used_hw_stats immediate
...

</code></pre>
<p>Useful examples of flower rules include:</p>
<pre><code># Target UDP to a IP range
flower skip_sw dst_ip 192.0.2.0/24 ip_proto udp
  
# Target TCP port 80 to any IP
flower skip_sw src_port 80 ip_proto tcp
 
# Target all GRE packets
flower skip_sw ip_proto 47
</code></pre>
<p>You must ensure that you do not put <code>skip_sw</code> in your rule if you plan to drop packets, else your ACL could be bypassed if a packet was engineered to trigger a control plane punt.</p>
<p>I do not know any good utility to manage these rules for you, Instead I have a shell script that applies them on boot using a systemd service.</p>
<h2>Monitoring the split state</h2>
<p>Since there are two sides (the CPU side and the chip side) to this switch, it is useful to monitor both of them, The driver keeps the regular kernel counters in sync with how much the chip is doing for you automatically:</p>
<pre><code># ip -s -h l show dev swp1
24: swp1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 1c:34:da:xx:xx:xx brd ff:ff:ff:ff:ff:ff
    RX:  bytes packets errors dropped  missed   mcast          
         2.01T   4.74G      0       0       0    214M
    TX:  bytes packets errors dropped carrier collsns          
         1.10T   5.78G      0       0       0       0
    altname enp1s0np1
</code></pre>
<p>If you want to just know how much traffic a interfaces has been sent to the Atom CPU, you can run this:</p>
<pre><code># ip -h stats show dev swp1 group offload subgroup cpu_hit
24: swp1: group offload subgroup cpu_hit
    RX:  bytes packets errors dropped  missed   mcast          
         25.3G    264M      0       0       0       0
    TX:  bytes packets errors dropped carrier collsns          
          442M   5.14M      0       0       0       0
</code></pre>
<p>This 25GB here is roughly speaking the BGP traffic that the switch has done with the service provider since boot.</p>
<p>However if you are looking for counters on why data was sent to the CPU, you can run the following to get the counters:</p>
<pre><code># devlink -s trap | grep -v pci/0000:01:00.0 | paste - - - - | grep -v "bytes 0"
  name ttl_value_is_too_small type exception generic true action trap group l3_exceptions            stats:                rx:                  bytes 20084104 packets 227812
...
  name ipv6_ospf type control generic true action trap group ospf            stats:                rx:                  bytes 71177822 packets 591399
  name ipv4_bgp type control generic true action trap group bgp            stats:                rx:                  bytes 1626384210 packets 3005173
  name ipv6_bgp type control generic true action trap group bgp            stats:                rx:                  bytes 2128110217 packets 3538356
...

</code></pre>
<p>Because the network interface counters are automatically synchronised, you can use your normal monitoring tools on your servers, on this switch. My setup is a blend of collectd and prometheus node_exporter, both of these tools work fine:</p>
<p><img src="https://blog.benjojo.co.uk/asset/9gy8MiPB28" alt="A screenshot of a RRD graph showing bandwidth on a switch port"></p>
<h2>Packet Sampling / sflow</h2>
<p>Normal packet sampling methods do not work on this switch, because as mentioned above the CPU side of the switch is nearly totally oblivious to most traffic passing through the switch. However this becomes a problem when you wish to do packet sampling for traffic statistics, or to drive something like <a href="https://fastnetmon.com/">FastNetMon</a> for DDoS detection.</p>
<p>However not all is lost, hsflowd does support the driver’s “<a href="https://man7.org/linux/man-pages/man8/tc-sample.8.html">psample</a>” system for gathering data.</p>
<p>My hsflowd config is as follows:</p>
<pre><code>sflow {
  sampling.10G=10000
  collector { ip=192.0.2.1 UDPPort=6666 }
  psample { group=1 egress=on }
  dent { sw=on switchport=swp.* }
}
</code></pre>
<p>Since hsflowd has an incompatible software licence with most distros, you will have to build it yourself. However I find that once compiled, hsflowd automatically manages the tc rules required for packet samples.</p>
<h2>Alternatives to Nvidia/Mellanox</h2>
<p>I think this is incredibly nice hardware, with even more incredible open source drivers. However I do worry that Nvidia will fall down a similar path to the late <a href="https://en.wikipedia.org/wiki/Nortel">Nortel</a> at this point due to their meteoric rise in an industry that could easily be a bubble. For that reason it is worth calling out that they are not the only vendor with this kind of open source driver functionality.</p>
<p>Arista has a closed source driver like this where you can supplement parts of their “EOS” with your own parts, however it is nowhere near as complete as this. But it does allow you to run bird (or other routing software) on their products if you wish to retain control of the code powering of your routing protocols.</p>
<p>Marvell also apparently has drivers similar to mlxsw, I have yet to personally use such hardware, but Mikrotik is known to use this hardware, but right now has no official (or known) way to “jailbreak” the hardware to run your own software stack.</p>
<p>I hope this changes in the future, as Mikrotik’s hardware price point is very competitive, it’s just the software reliability that always turns me off their products, so having an option to not use RouterOS while keeping their very competitive hardware would be a huge deal.</p>
<h2>Closing thoughts</h2>
<p>I agree with Pim’s conclusion, this switch and its ecosystem is incredible. Good, and acquirable hardware combined with software that you have the power to fix yourself is currently unheard of in the industry, and mellanox delivered it!</p>
<p>This setup has been running without a hitch for bgp.tools for some time now, and I hope to keep it running until I outscale it one day, since I would be surprised if I need to remove it for any other reason.</p>
<p>I’d like to thank <a href="https://ipng.ch/s/articles/2023/11/11/mellanox-sn2700.html">Pim van Pelt for their earlier post on these devices</a> and Basil Filian for helping me figure out a number of quirks of these devices!</p>
<p>If you want to stay up to date with the blog you can use the <a href="https://blog.benjojo.co.uk/rss.xml">RSS feed</a> or you can follow me on Fediverse <a href="https://benjojo.co.uk/u/benjojo">@benjojo@benjojo.co.uk</a>!</p>
<p>Until next time!</p>






</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Piet (382 pts)]]></title>
            <link>https://www.dangermouse.net/esoteric/piet.html</link>
            <guid>40141777</guid>
            <pubDate>Wed, 24 Apr 2024 07:52:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dangermouse.net/esoteric/piet.html">https://www.dangermouse.net/esoteric/piet.html</a>, See on <a href="https://news.ycombinator.com/item?id=40141777">Hacker News</a></p>
<div id="readability-page-1" class="page">

<hr>

<p>
<img src="https://www.dangermouse.net/esoteric/piet/Mondrian.jpg" width="158" height="168" alt="Composition with Red, Yellow and Blue by Piet Mondrian"><br>
<i>Composition with Red,<br>Yellow and Blue</i>.<br>1921, Piet Mondrian.
</p>
<h2>Introduction</h2>
<p>
Piet is a programming language in which programs look like abstract paintings. The language is named after
<a href="http://www.ibiblio.org/wm/paint/auth/mondrian/">Piet Mondrian</a>, who pioneered the field of geometric abstract
art. I would have liked to call the language Mondrian, but
<a href="https://link.springer.com/chapter/10.1007/3-540-45337-7_9">someone beat me to it</a> with a rather mundane-looking
scripting language. Oh well, we can't all be esoteric language writers I suppose.
</p><p>
<i>Notes:</i>
</p>
<ul>
<li>I wrote the Piet specification a long time ago, and the language has taken on a bit of a life of its own, with a small
community of coders writing Piet programs, interpreters, IDEs, and even compilers. I have not written any "authoritative" interpreter, and the
different ones available sometimes interpret the specification slightly differently.</li>
<li>Over the years I have tended to field questions about the spec with "whatever you think makes the most sense", rather than any definitive clarification - thus the slightly different versions out there. I have now added some clarifications to this specification to address some of the questions I have been asked over the years. Hopefully they are sensible and most implementations will already be compliant, but it's possible some do not comply. <i>Caveat emptor</i>.</li>
<li>Some people like to use Piet to set puzzles in various competitions. This web page and the linked resources can help you solve those puzzles, if you have a reasonable grasp of computer coding. If you do not, or it looks too difficult, I suggest asking some of your friends who may be computer programmers to help you. Please do not email to ask me for help. Although I wish you the best in solving your puzzle, I do not have time to help everyone in this situation.</li>
</ul>

<h2>Design Principles</h2>
<ul>
<li>Program code will be in the form of abstract art.</li>
</ul>

<h2>Language Concepts</h2>
<h3>Colours</h3>
<table>
	<tbody><tr>
		<td>#FFC0C0<br>light red</td>
		<td>#FFFFC0<br>light yellow</td>
		<td>#C0FFC0<br>light green</td>
		<td>#C0FFFF<br>light cyan</td>
		<td>#C0C0FF<br>light blue</td>
		<td>#FFC0FF<br>light magenta</td>
	</tr>
	<tr>
		<td>#FF0000<br>red</td>
		<td>#FFFF00<br>yellow</td>
		<td>#00FF00<br>green</td>
		<td>#00FFFF<br>cyan</td>
		<td>#0000FF<br>blue</td>
		<td>#FF00FF<br>magenta</td>
	</tr>
	<tr>
		<td>#C00000<br>dark red</td>
		<td>#C0C000<br>dark yellow</td>
		<td>#00C000<br>dark green</td>
		<td>#00C0C0<br>dark cyan</td>
		<td>#0000C0<br>dark blue</td>
		<td>#C000C0<br>dark magenta</td>
	</tr>
	<tr>
		<td colspan="3">#FFFFFF&nbsp;white</td>
		<td colspan="3"><span color="#FFFFFF">#000000&nbsp;black</span></td>
	</tr>
</tbody></table>
<p>
Piet uses 20 distinct colours, as shown in the table at right.
The 18 colours in the first 3 rows of the table are related cyclically in the following two ways:
</p>
<ul>
<li><b>Hue Cycle:</b> red -&gt; yellow -&gt; green -&gt; cyan -&gt; blue -&gt; magenta -&gt; red
</li><li><b>Lightness Cycle:</b> light -&gt; normal -&gt; dark -&gt; light
</li></ul>
<p>
Note that "light" is considered to be one step "darker" than "dark", and vice versa. White and black do not fall into
either cycle.
</p><p>
Additional colours (such as orange, brown) may be used, though their effect is implementation-dependent. In the simplest
case, non-standard colours are treated by the language interpreter as the same as white, so may be used freely wherever
white is used. (Another possibility is that they are treated the same as black.)
</p>

<h3>Codels</h3>
<p>
Piet code takes the form of graphics made up of the recognised colours. Individual pixels of colour are significant in the
language, so it is common for programs to be enlarged for viewing so that the details are easily visible. In such enlarged
programs, the term "codel" is used to mean a block of colour equivalent to a single pixel of code, to avoid confusion with
the actual pixels of the enlarged graphic, of which many may make up one codel.
</p>

<h3>Colour Blocks</h3>
<p>
The basic unit of Piet code is the colour block. A colour block is a contiguous block of any number of codels of one colour,
bounded by blocks of other colours or by the edge of the program graphic. Blocks of colour adjacent only diagonally are not
considered contiguous. A colour block may be any shape and may have "holes" of other colours inside it, which are not considered
part of the block.
</p>

<h3>Stack</h3>
<p>
Piet uses a <i><a href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)">stack</a></i> for storage of all data values. Data values exist only as integers, though they may be read in or
printed as <a href="https://en.wikipedia.org/wiki/Unicode">Unicode</a> character values with appropriate commands.
</p><p>
The stack is notionally infinitely deep, but implementations may elect to provide a finite maximum stack size. If a finite stack overflows, it should be treated as a runtime error, and handling this will be implementation dependent.
</p>

<h3>Program Execution</h3>
<table>
	<tbody><tr>
		<th>DP</th><th>CC</th><th>Codel chosen</th>
	</tr>
	<tr>
		<td rowspan="2">right</td><td>left</td><td>uppermost</td>
	</tr>
	<tr>
		<td>right</td><td>lowermost</td>
	</tr>
	<tr>
		<td rowspan="2">down</td><td>left</td><td>rightmost</td>
	</tr>
	<tr>
		<td>right</td><td>leftmost</td>
	</tr>
	<tr>
		<td rowspan="2">left</td><td>left</td><td>lowermost</td>
	</tr>
	<tr>
		<td>right</td><td>uppermost</td>
	</tr>
	<tr>
		<td rowspan="2">up</td><td>left</td><td>leftmost</td>
	</tr>
	<tr>
		<td>right</td><td>rightmost</td>
	</tr>
</tbody></table>
<p>
The Piet language interpreter begins executing a program in the colour block which includes the upper left codel of the program.
The interpreter maintains a <em>Direction Pointer</em> (DP), initially pointing to the right. The DP may point either right,
left, down or up. The interpreter also maintains a <em>Codel Chooser</em> (CC), initially pointing left. The CC may point
either left or right. The directions of the DP and CC will often change during program execution.
</p><p>
As it executes the program, the interpreter traverses the colour blocks of the program under the following rules:
</p>
<ol>
<li>The interpreter finds the edge of the current colour block which is furthest in the direction of the DP. (This edge may be disjoint if the block is of a complex shape.)</li>
<li>The interpreter finds the codel of the current colour block on that edge which is furthest to the CC's direction of the DP's direction of travel. (Visualise this as standing on the program and walking in the direction of the DP; see table at right.)</li>
<li>The interpreter travels from that codel into the colour block containing the codel immediately in the direction of the DP.</li>
</ol>
<p>
The interpreter continues doing this until the program terminates.
</p>

<h2>Syntax Elements</h2>
<h3>Numbers</h3>
<p>
Each non-black, non-white colour block in a Piet program represents an integer equal to the number of codels in that block.
Note that non-positive integers cannot be represented, although they can be constructed with operators. When the interpreter
encounters a number, it does not necessarily do anything with it. In particular, it is not automatically pushed on to the
stack - there is an explicit command for that (see below).
</p><p>
The maximum size of integers is notionally infinite, though implementations may implement a finite maximum integer size. An integer overflow is
a runtime error, and handling this will be implementation dependent.
</p>

<h3>Black Blocks and Edges</h3>
<p>
Black colour blocks and the edges of the program restrict program flow. If the Piet interpreter attempts to move into a black block
or off an edge, it is stopped and the CC is toggled. The interpreter then attempts to move from its current block again. If it fails
a second time, the DP is moved clockwise one step. These attempts are repeated, with the CC and DP being changed between alternate
attempts. If after eight attempts the interpreter cannot leave its current colour block, there is no way out and the program terminates.
</p>

<h3>White Blocks</h3>
<p>
White colour blocks are "free" zones through which the interpreter passes unhindered. If it moves from a colour block into a white
area, the interpreter "slides" through the white codels in the direction of the DP until it reaches a non-white colour block.
If the interpreter slides into a black block or an edge, it is considered restricted (see above), otherwise it moves into the
colour block so encountered. Sliding across white blocks into a new colour does not cause a command to be executed (see below). In this way,
white blocks can be used to change the current colour without executing a command, which is very useful for coding loops.
</p><p>
Sliding across white blocks takes the interpreter in a <i>straight line</i> until it hits a coloured pixel or edge. It does not use the procedure
described above for determining where the interpreter emerges from non-white coloured blocks.
</p><p>
<i>Precisely what happens when the interpeter slides across a white block and hits a black block or an edge was not clear in the original
specification. My interpretation follows from a literal reading of the above text:</i>
</p>
<ul>
<li>The interpreter "slides" across the white block in a straight line.</li>
<li>If it hits a restriction, the CC is toggled. Since this results in no difference in where the interpreter is trying to go, the DP is immediately stepped clockwise.</li>
<li>The interpreter now begins sliding from its current white codel, in the new direction of the DP, until it either enters a coloured block or encounters another restriction.</li>
<li>Each time the interpreter hits a restriction while within the white block, it toggles the CC and steps the DP clockwise, then tries to slide again. This process repeats until the interpreter either enters a coloured block (where execution then continues); or until the interpreter begins retracing its route. If it retraces its route entirely within a white block, there is no way out of the white block and execution should terminate.</li>
</ul>

<h3>Commands</h3>
<table>
	<tbody><tr>
		<th>&nbsp;</th><th colspan="3">Lightness change</th>
	</tr>
	<tr>
		<th>Hue change</th><th>None</th><th>1 Darker</th><th>2 Darker</th>
	</tr>
	<tr>
		<th>None</th><td>&nbsp;</td><td>push</td><td>pop</td>
	</tr>
	<tr>
		<th>1 Step</th><td>add</td><td>subtract</td><td>multiply</td>
	</tr>
	<tr>
		<th>2 Steps</th><td>divide</td><td>mod</td><td>not</td>
	</tr>
	<tr>
		<th>3 Steps</th><td>greater</td><td>pointer</td><td>switch</td>
	</tr>
	<tr>
		<th>4 Steps</th><td>duplicate</td><td>roll</td><td>in(number)</td>
	</tr>
	<tr>
		<th>5 Steps</th><td>in(char)</td><td>out(number)</td><td>out(char)</td>
	</tr>
</tbody></table>
<p>
Commands are defined by the transition of colour from one colour block to the next as the interpreter travels through the
program. The number of steps along the Hue Cycle and Lightness Cycle in each transition determine the command executed, as shown
in the table at right. If the transition between colour blocks occurs via a slide across a white block, no command is executed.
The individual commands are explained below.
</p>
<ul>
<li><b>push:</b> Pushes the value of the colour block just exited on to the stack. Note that values of colour blocks are
not automatically pushed on to the stack - this push operation must be explicitly carried out.</li>
<li><b>pop:</b> Pops the top value off the stack and discards it.</li>
<li><b>add:</b> Pops the top two values off the stack, adds them, and pushes the result back on the stack.</li>
<li><b>subtract:</b> Pops the top two values off the stack, calculates the second top value minus the top value,
and pushes the result back on the stack.</li>
<li><b>multiply:</b> Pops the top two values off the stack, multiplies them, and pushes the result back on the stack.</li>
<li><b>divide:</b> Pops the top two values off the stack, calculates the integer division of the second top value by the top value, and pushes the result back on the stack. If a divide by zero occurs, it is handled as an implementation-dependent error, though simply ignoring the command is recommended.</li>
<li><b>mod:</b> Pops the top two values off the stack, calculates the second top value <a href="https://en.wikipedia.org/wiki/Modulo_operation">modulo</a> the top value, and pushes the result back on the stack. The result has the same sign as the divisor (the top value). If the top value is zero, this is a divide by zero error, which is handled as an implementation-dependent error, though simply ignoring the command is recommended. (<i>See note below.</i>)</li>
<li><b>not:</b> Replaces the top value of the stack with 0 if it is non-zero, and 1 if it is zero.</li>
<li><b>greater:</b> Pops the top two values off the stack, and pushes 1 on to the stack if the second top value is
greater than the top value, and pushes 0 if it is not greater.</li>
<li><b>pointer:</b> Pops the top value off the stack and rotates the DP clockwise that many steps (anticlockwise if negative).</li>
<li><b>switch:</b> Pops the top value off the stack and toggles the CC that many times (the absolute value of that many times if negative).</li>
<li><b>duplicate:</b> Pushes a copy of the top value on the stack on to the stack.</li>
<li><b>roll:</b> Pops the top two values off the stack and "rolls" the remaining stack entries to a depth equal to the second value popped, by a number of rolls equal to the first value popped. A single roll to depth <i>n</i> is defined as burying the top value on the stack <i>n</i> deep and bringing all values above it up by 1 place. A negative number of rolls rolls in the opposite direction. A negative depth is an error and the command is ignored. If a roll is greater than an implementation-dependent maximum stack depth, it is handled as an implementation-dependent error, though simply ignoring the command is recommended.</li>
<li><b>in:</b> Reads a value from STDIN as either a number or character, depending on the particular incarnation of this command and pushes it on to the stack. If no input is waiting on STDIN, this is an error and the command is ignored. If an integer read does not receive an integer value, this is an error and the command is ignored.</li>
<li><b>out:</b> Pops the top value off the stack and prints it to STDOUT as either a number or character, depending
on the particular incarnation of this command.</li>
</ul>
<p>
Any operations which cannot be performed (such as popping values when not enough are on the stack) are simply ignored, and processing continues with the next command.
</p><p>
<i>Note on the mod command:</i> In the original specification of Piet the result of a modulo operation with a negative dividend (the second top value popped off the stack) was not explicitly defined. I assumed that everyone would assume that the result of (<i>p</i> mod <i>q</i>) would always be equal to ((<i>p</i> + <i>Nq</i>) mod <i>q</i>) for any integer <i>N</i>. So:
</p>
<ul>
<li>5 mod 3 = 2</li>
<li>2 mod 3 = 2</li>
<li>-1 mod 3 = 2</li>
<li>-4 mod 3 = 2</li>
</ul>
<p>
The mod command is thus identical to <i>floored division</i> in Wikipedia's page on the <a href="https://en.wikipedia.org/wiki/Modulo_operation">modulus operation</a>.
</p>

<h2>Sample Programs and Resources</h2>
<ul>
<li><a href="https://www.dangermouse.net/esoteric/piet/samples.html">Sample programs</a>
</li><li><a href="https://www.dangermouse.net/esoteric/piet/tools.html">Third-party Piet interpreters and development tools</a>
</li></ul>
<p>
<a href="https://www.patreon.com/dmmaus">Support me on Patreon <img src="https://www.dangermouse.net/patreon_16x16.png" width="16" height="16"></a>
</p>
<hr>
<a href="https://www.dangermouse.net/">Home</a> | <a href="https://www.dangermouse.net/esoteric/">Esoteric Programming Languages</a><br>

<i>Last updated: Thursday, 27 September, 2018; 04:00:52 PDT.</i><br>
Copyright © 1990-2022, David Morgan-Mar. <i>dmm@dangermouse.net</i><br>
<i>Hosted by: <a href="http://www.dreamhost.com/rewards.cgi?dmmaus">DreamHost</a></i>




</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Other People’s Problems (141 pts)]]></title>
            <link>https://seths.blog/2024/04/other-peoples-problems/</link>
            <guid>40141463</guid>
            <pubDate>Wed, 24 Apr 2024 06:57:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://seths.blog/2024/04/other-peoples-problems/">https://seths.blog/2024/04/other-peoples-problems/</a>, See on <a href="https://news.ycombinator.com/item?id=40141463">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
						
			
<p>It’s surprisingly easy to be generous and find solutions to our friend’s problems.</p>



<p>Much easier than it is to do it for ourselves. Why?</p>



<p>There are two useful reasons, I think.</p>



<p>FIRST, because we’re unaware of all the real and imaginary boundaries our friends have set up. If it were easy to solve the problem, they probably would have. But they’re making it hard because they have decided that there are people or systems that aren’t worth challenging. Loosening the constraints always makes a problem easier to solve.</p>



<p>And SECOND, because resistance is real. Solving the problem means moving ahead, confronting new, even scarier problems. It might be easier to simply stay where we are, marinating in our stuck.</p>



<p>When we care enough to solve our own problem, we’ll loosen the unloosenable constraints and embrace the new challenges to come.</p>
			

			

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a full Django project, starting with a single file (115 pts)]]></title>
            <link>https://www.mostlypython.com/django-from-first-principles/</link>
            <guid>40140396</guid>
            <pubDate>Wed, 24 Apr 2024 03:59:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mostlypython.com/django-from-first-principles/">https://www.mostlypython.com/django-from-first-principles/</a>, See on <a href="https://news.ycombinator.com/item?id=40140396">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>MP 90: Building a complete Django project from a single file.</p><p><strong>Note: </strong><em>This is the first post in a </em><a href="https://www.mostlypython.com/django-from-first-principles-2/" rel="noreferrer"><em>series</em></a><em> about building a full Django project, starting with a single file. This series will be free to everyone, as soon as each post comes out.</em> <a href="#footnote-1" rel="noreferrer"><sup>1</sup></a></p><p>For a long time, Django has seemed more complex than other web frameworks such as Flask. This reputation is largely based on a somewhat unfair comparison between the "simplest" Django project and the "simplest" Flask project. That comparison often lands in favor of Flask, because Flask's documentation starts off with a single-file project consisting of just five lines of code.</p><p>In this project we'll build a Django project, but we'll take an approach that's much different from most Django tutorials. We'll start with a single file, just like the Flask example, and the single-file demos we see from other frameworks. As we build out the project, we'll add pieces one at a time, only as they're needed. By the time we're finished, we'll have a standard Django project built from the ground up.</p><h2 id="the-source-of-djangos-initial-complexity">The source of Django's (initial) complexity</h2><p>Django's initial complexity stems from the fact that most tutorials tell people to start with these two commands:</p>
<!--kg-card-begin: html-->
<pre data-caption="" data-hl-lines="[]" data-language="sh"><span></span><span>$ </span>django-admin<span> </span>startproject<span> </span>&lt;project-name&gt;<span> </span>.
<span>$ </span>python<span> </span>manage.py<span> </span>startapp<span> </span>&lt;app-name&gt;</pre>
<!--kg-card-end: html-->
<p>After running these two commands, there are twelve files you'll need to make sense of! If you're an experienced Django developer, that automatically-generated project structure is a big help. But if you're new to Django, it's not obvious at all how all these parts fit together.</p><h2 id="an-alternative-approach">An alternative approach</h2><p>Rather than starting with these commands, we'll start with just one file that serves our project's home page. We'll look at all the parts of the file, so the purpose of every aspect of the project is clear from the start. As we progress in building out the project, we'll only expand out into additional files and directories when they're needed.</p><figure><img src="https://www.mostlypython.com/content/images/2024/03/standard_vs_single_django-2-1.png" alt="Output of `tree` for a standard Django project, and a single-file version of the same project." loading="lazy" width="1720" height="1057" srcset="https://www.mostlypython.com/content/images/size/w600/2024/03/standard_vs_single_django-2-1.png 600w, https://www.mostlypython.com/content/images/size/w1000/2024/03/standard_vs_single_django-2-1.png 1000w, https://www.mostlypython.com/content/images/size/w1600/2024/03/standard_vs_single_django-2-1.png 1600w, https://www.mostlypython.com/content/images/2024/03/standard_vs_single_django-2-1.png 1720w" sizes="(min-width: 720px) 720px"><figcaption><span>A standard Django project on the left, before writing any of your own code. There are 12 files that people new to Django need to make sense of. It's possible to start a Django project with just a single file, as shown on the right.</span></figcaption></figure><h2 id="where-well-end-up">Where we'll end up</h2><p>When the project is fully developed, we'll end up with a project structure that's almost identical to what you'd get using the <code>startproject</code> and <code>startapp</code> commands. You probably won't ever use the ground-up approach shown in this series again, but you'll come away from it with a much better understanding of the structure of a typical Django project.</p><h2 id="a-simple-but-nontrivial-project">A simple but nontrivial project</h2><p>A good project for a tutorial series is simple, but nontrivial. This means it should use a database, incorporate user accounts, and feature styling that requires the use of static files.<a href="#footnote-2" rel="noreferrer"><sup>2</sup></a></p><p>In this series we'll build a project called <em>BlogMaker Lite</em>. It's my go-to project for Django discussions, because it ticks all the requirements for a simple but nontrivial project. We'll also deploy the project when we're finished, to prove that the final version of the project would work in production.</p><h2 id="conclusions">Conclusions</h2><p>Django <em>is</em> more complex than some other web frameworks, because it includes a lot more out of the box than minimalist frameworks. By the time you've built out everything your real-world project needs, a typical Django project isn't much more complex than a fully-built project using a different framework.</p><p>While <code>startproject</code> and <code>startapp</code> give experienced Django developers a nice starting point for new projects, people learning about Django don't need to start with the complexity that those two commands introduce. Instead, we can start from a single file that's easier to understand, and build out each new piece as it's needed.</p><p>If you're new to Django, hopefully this series will be a helpful way to see the framework for the first time. If you've been using Django for a while, you'll probably come away with a better understanding of the structure you've already been seeing in your projects. And if you're really experienced with Django, maybe you'll see the framework from a newcomer's eyes once again.</p><p>In the <a href="https://www.mostlypython.com/django-from-first-principles-part-2/" rel="noreferrer">next post</a>, we'll start by writing a single file that serves the project's home page.</p><p><strong>Note:</strong> <em>I'd love feedback throughout this series. If you have any thoughts or reactions, please leave a comment or reach out in </em><a href="https://www.mostlypython.com/contact/" rel="noreferrer"><em>whatever way</em></a><em> works best for you.</em></p>
<!--kg-card-begin: html-->
<hr><p><span>1</span></p>
<!--kg-card-end: html-->
<p>Usually, new series on <em>Mostly Python</em> are only available to paid subscribers for a period of six weeks. This series will be free to everyone as soon as each post comes out. I'm still finishing up the migration from Substack, and it's a little easier to only publish free posts for a little while longer.</p><p>More importantly, I want this series to be available to people in the Django community as soon as each post is published. This series builds on work that people have been sharing at a number of recent DjangoCons, where people have been challenging each other to come up with the shortest working single-file Django project possible.</p><p>If you're curious about some of this prior work, start by watching Carlton Gibson's Djang0Con US 2019 <a href="https://www.youtube.com/watch?v=w9cYEovduWI&amp;ref=mostlypython.com" rel="noreferrer">talk</a>, <em>Using Django as a Micro-Framework</em>. Then take a look at Will Vincent's <a href="https://github.com/wsvincent/django-microframework?ref=mostlypython.com" rel="noreferrer">django-microframework</a> repository; Will has done a nice job of collecting a few different single-file examples. Then check out Paolo Melchiorre's <a href="https://github.com/pauloxnet/uDjango?ref=mostlypython.com" rel="noreferrer">μDjango</a> project, which he demonstrated at DjangoCon US 2023. Andrew Mshar wrote an interesting blog post called <em>Moving from Django Microframework to Django</em>, following up on Paolo's work. Most recently, Andrew Godwin shared his <a href="https://github.com/andrewgodwin/django-singlefile?ref=mostlypython.com" rel="noreferrer">django-singlefile</a> project. This is the first example that seems to be meant for <a href="https://fosstodon.org/deck/@andrew@aeracode.org/112180445225617864?ref=mostlypython.com" rel="noreferrer">real-world use</a>, and I'm curious to see where people go with it.</p><p>While these been interesting exercises, it can quickly turn into a game of code golf. Someone might come up with slightly shorter single-file projects, but many of these are just as hard for newer people to understand as a standard Django project generated from <code>startproject</code> and <code>startapp</code>. I've been thinking for a while that one of the most interesting directions to take this work is building out a full project from one of these single-file examples. This series is my response to seeing everyone else's single-file projects; I hope the people who shared those projects like what they see here. :)</p>
<!--kg-card-begin: html-->
<p><span>2</span></p>
<!--kg-card-end: html-->
<p>It frustrates me to no end seeing hosting platforms use overly simplistic Django projects in their examples. Heroku once pushed a broken build process because the sample project they used for testing didn't even use a database.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maxtext: A simple, performant and scalable Jax LLM (109 pts)]]></title>
            <link>https://github.com/google/maxtext</link>
            <guid>40140002</guid>
            <pubDate>Wed, 24 Apr 2024 03:00:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google/maxtext">https://github.com/google/maxtext</a>, See on <a href="https://news.ycombinator.com/item?id=40140002">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><a href="https://github.com/google/maxtext/actions/workflows/UnitTests.yml"><img src="https://github.com/google/maxtext/actions/workflows/UnitTests.yml/badge.svg" alt="Unit Tests"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">MaxText is a <strong>high performance</strong>, <strong>highly scalable</strong>, <strong>open-source</strong> LLM written in pure Python/Jax and targeting Google Cloud TPUs and GPUs for <strong>training</strong> and <strong>inference</strong>. MaxText achieves <a href="#runtime-performance-results">high MFUs</a> and scales from single host to very large clusters while staying simple and "optimization-free" thanks to the power of Jax and the XLA compiler.</p>
<p dir="auto">MaxText aims to be a launching off point for ambitious LLM projects both in research and production. We encourage users to start by experimenting with MaxText out of the box and then fork and modify MaxText to meet their needs.</p>
<p dir="auto">We have used MaxText to <a href="https://cloud.google.com/blog/products/compute/accurate-quantized-training-aqt-for-tpu-v5e" rel="nofollow">demonstrate high-performance, well-converging training in int8</a> and <a href="https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e" rel="nofollow">scale training to ~51K chips</a>.</p>
<p dir="auto">Key supported features:</p>
<ul dir="auto">
<li>TPUs and GPUs (in preview)</li>
<li>Training and Inference (in preview)</li>
<li>Models: Llama2, Mistral and Gemma</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="https://github.com/google/maxtext/blob/main/getting_started/First_run.md">Getting Started</a></li>
<li><a href="#runtime-performance-results">Runtime Performance Results</a></li>
<li><a href="#comparison-to-alternatives">Comparison To Alternatives</a></li>
<li><a href="#development">Development</a></li>
<li><a href="#features-and-diagnostics">Features and Diagnostics</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">For your first time running MaxText, we provide specific <a href="https://github.com/google/maxtext/blob/main/getting_started/First_run.md">instructions</a>.</p>
<p dir="auto">MaxText supports training and inference of various open models. Follow user guides in the <a href="https://github.com/google/maxtext/blob/main/getting_started">getting started</a> folder to know more.</p>
<p dir="auto">Some extra helpful guides:</p>
<ul dir="auto">
<li><a href="https://ai.google.dev/gemma" rel="nofollow">Gemma</a>: a family of open-weights Large Language Model (LLM) by <a href="https://deepmind.google/" rel="nofollow">Google DeepMind</a>, based on Gemini research and technology. You can run decode and finetuning using <a href="https://github.com/google/maxtext/blob/main/end_to_end/gemma/Run_Gemma.md">these instructions</a>.</li>
<li><a href="https://llama.meta.com/llama2/" rel="nofollow">Llama2</a>: a family of open-weights Large Language Model (LLM) by Meta. You can run decode and finetuning using <a href="https://github.com/google/maxtext/blob/main/getting_started/Run_Llama2.md">these instructions</a>.</li>
</ul>
<p dir="auto">In addition to the getting started guides, there are always other MaxText capabilities that are being constantly being added! The full suite of end-to-end tests is in <a href="https://github.com/google/maxtext/blob/main/end_to_end">end_to_end</a>. We run them with a nightly cadence. They can be a good source for understanding MaxText Alternatively you can see the continuous <a href="https://github.com/google/maxtext/blob/main/.github/workflows/UnitTests.yml">unit tests</a> which are run almost continuously.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Runtime Performance Results</h2><a id="user-content-runtime-performance-results" aria-label="Permalink: Runtime Performance Results" href="#runtime-performance-results"></a></p>
<p dir="auto">More details on reproducing these results can be found in <a href="https://github.com/google/maxtext/blob/main/MaxText/configs/README.md">MaxText/configs/README.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">TPU v5p</h2><a id="user-content-tpu-v5p" aria-label="Permalink: TPU v5p" href="#tpu-v5p"></a></p>
<table>
<thead>
<tr>
<th>No. of params</th>
<th>Accelerator Type</th>
<th>TFLOP/chip/sec</th>
<th>Model flops utilization (MFU)</th>
</tr>
</thead>
<tbody>
<tr>
<td>32B</td>
<td>v5p-128</td>
<td>3.28e+02</td>
<td>71.47%</td>
</tr>
<tr>
<td>64B</td>
<td>v5p-128</td>
<td>3.23e+02</td>
<td>70.31%</td>
</tr>
<tr>
<td>128B</td>
<td>v5p-256</td>
<td>3.15e+02</td>
<td>68.68%</td>
</tr>
<tr>
<td>128B</td>
<td>v5p-512</td>
<td>3.15e+02</td>
<td>68.53%</td>
</tr>
<tr>
<td>256B</td>
<td>v5p-1024</td>
<td>3.16e+02</td>
<td>68.82%</td>
</tr>
<tr>
<td>512B</td>
<td>v5p-1024</td>
<td>2.94e+02</td>
<td>63.99%</td>
</tr>
<tr>
<td>1024B</td>
<td>v5p-2048</td>
<td>2.49e+02</td>
<td>64.05%</td>
</tr>
<tr>
<td>1024B</td>
<td>v5p-4096</td>
<td>2.97e+02</td>
<td>64.80%</td>
</tr>
<tr>
<td>1160B</td>
<td>v5p-7680</td>
<td>2.95e+02</td>
<td>64.27%</td>
</tr>
<tr>
<td>1160B</td>
<td>v5p-12288</td>
<td>3.04e+02</td>
<td>66.23%</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">TPU v5e</h2><a id="user-content-tpu-v5e" aria-label="Permalink: TPU v5e" href="#tpu-v5e"></a></p>
<p dir="auto">For 16B, 32B, 64B, and 128B models. See full run configs in <a href="https://github.com/google/maxtext/blob/main/MaxText/configs/v5e">MaxText/configs/v5e/</a> as <code>16b.sh</code>, <code>32b.sh</code>, <code>64b.sh</code>, <code>128b.sh</code>.</p>
<table>
<thead>
<tr>
<th>Hardware</th>
<th>16B TFLOP/sec/chip</th>
<th>16B MFU</th>
<th>32B TFLOP/sec/chip</th>
<th>32B MFU</th>
<th>64B TFLOP/sec/chip</th>
<th>64B MFU</th>
<th>128B TFLOP/sec/chip</th>
<th>128B MFU</th>
</tr>
</thead>
<tbody>
<tr>
<td>1x v5e-256</td>
<td>120</td>
<td>61.10%</td>
<td>132</td>
<td>66.86%</td>
<td>118</td>
<td>59.90%</td>
<td>110</td>
<td>56.06%</td>
</tr>
<tr>
<td>2x v5e-256</td>
<td>117</td>
<td>59.37%</td>
<td>128</td>
<td>64.81%</td>
<td>112</td>
<td>56.66%</td>
<td>110</td>
<td>55.82%</td>
</tr>
<tr>
<td>4x v5e-256</td>
<td>117</td>
<td>59.14%</td>
<td>126</td>
<td>64.10%</td>
<td>110</td>
<td>55.85%</td>
<td>108</td>
<td>54.93%</td>
</tr>
<tr>
<td>8x v5e-256</td>
<td>115</td>
<td>58.27%</td>
<td>125</td>
<td>63.67%</td>
<td>108</td>
<td>54.96%</td>
<td>104</td>
<td>52.93%</td>
</tr>
<tr>
<td>16x v5e-256</td>
<td>111</td>
<td>56.56%</td>
<td>123</td>
<td>62.26%</td>
<td>105</td>
<td>53.29%</td>
<td>100</td>
<td>50.86%</td>
</tr>
<tr>
<td>32x v5e-256</td>
<td>108</td>
<td>54.65%</td>
<td>119</td>
<td>60.40%</td>
<td>99</td>
<td>50.18%</td>
<td>91</td>
<td>46.25%</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison to Alternatives</h2><a id="user-content-comparison-to-alternatives" aria-label="Permalink: Comparison to Alternatives" href="#comparison-to-alternatives"></a></p>
<p dir="auto">MaxText is heavily inspired by <a href="https://github.com/karpathy/minGPT">MinGPT</a>/<a href="https://github.com/karpathy/nanoGPT">NanoGPT</a>, elegant standalone GPT implementations written in PyTorch and targeting Nvidia GPUs. MaxText is more complex, supporting more industry standard models and scaling to tens of thousands of chips. Ultimately MaxText has an MFU more than three times the <a href="https://twitter.com/karpathy/status/1613250489097027584?cxt=HHwWgIDUhbixteMsAAAA" rel="nofollow">17%</a> reported most recently with that codebase, is massively scalable and implements a key-value cache for efficient auto-regressive decoding.</p>
<p dir="auto">MaxText is more similar to <a href="https://github.com/NVIDIA/Megatron-LM">Nvidia/Megatron-LM</a>, a very well tuned LLM implementation targeting Nvidia GPUs. The two implementations achieve comparable MFUs. The difference in the codebases highlights the different programming strategies. MaxText is pure Python, relying heavily on the XLA compiler to achieve high performance. By contrast, Megatron-LM is a mix of Python and CUDA, relying on well-optimized CUDA kernels to achieve high performance.</p>
<p dir="auto">MaxText is also comparable to <a href="https://github.com/google/paxml">Pax</a>. Like Pax, MaxText provides high-performance and scalable implementations of LLMs in Jax. Pax focuses on enabling powerful configuration parameters, enabling developers to change the model by editing config parameters. By contrast, MaxText is a simple, concrete implementation of various LLMs that encourages users to extend by forking and directly editing the source code.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features and Diagnostics</h2><a id="user-content-features-and-diagnostics" aria-label="Permalink: Features and Diagnostics" href="#features-and-diagnostics"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Collect Stack Traces</h2><a id="user-content-collect-stack-traces" aria-label="Permalink: Collect Stack Traces" href="#collect-stack-traces"></a></p>
<p dir="auto">When running a Single Program, Multiple Data (SPMD) job on accelerators, the overall process can hang if there is any error or any VM hangs/crashes for some reason. In this scenario, capturing stack traces will help to identify and troubleshoot the issues for the jobs running on TPU VMs.</p>
<p dir="auto">The following configurations will help to debug a fault or when a program is stuck or hung somewhere by collecting stack traces. Change the parameter values accordingly in <code>MaxText/configs/base.yml</code>:</p>
<ol dir="auto">
<li>Set <code>collect_stack_trace: True</code> to enable collection of stack traces on faults or when the program is hung. This setting will periodically dump the traces for the program to help in debugging. To disable this, set <code>collect_stack_trace: False</code>.</li>
<li>Set <code>stack_trace_to_cloud: False</code> to display stack traces on console. <code>stack_trace_to_cloud: True</code> will create a temporary file in <code>/tmp/debugging</code> in the TPUs to store the stack traces. There is an agent running on TPU VMs that will periodically upload the traces from the temporary directory to cloud logging in the gcp project. You can view the traces in Logs Explorer on Cloud Logging using the following query:</li>
</ol>
<div data-snippet-clipboard-copy-content="logName=&quot;projects/<project_name>/logs/tpu.googleapis.com%2Fruntime_monitor&quot;
jsonPayload.verb=&quot;stacktraceanalyzer&quot;"><pre><code>logName="projects/&lt;project_name&gt;/logs/tpu.googleapis.com%2Fruntime_monitor"
jsonPayload.verb="stacktraceanalyzer"
</code></pre></div>
<ol start="3" dir="auto">
<li><code>stack_trace_interval_seconds</code> signifies the duration in seconds between each stack trace collection event. Setting <code>stack_trace_interval_seconds: 600</code> will collect the stack traces every 600 seconds (10 minutes).</li>
</ol>
<p dir="auto">Here is the related PyPI package: <a href="https://pypi.org/project/cloud-tpu-diagnostics" rel="nofollow">https://pypi.org/project/cloud-tpu-diagnostics</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ahead of Time Compilation (AOT, tpu-only)</h2><a id="user-content-ahead-of-time-compilation-aot-tpu-only" aria-label="Permalink: Ahead of Time Compilation (AOT, tpu-only)" href="#ahead-of-time-compilation-aot-tpu-only"></a></p>
<p dir="auto">To compile your training run ahead of time, we provide a tool <code>train_compile.py</code>. This tool allows you to compile the main <code>train_step</code> in <code>train.py</code> for target hardware (e.g. a large number of v5e devices) without using the target hardware, and instead you may use only a CPU or a single VM from a different family. This compilation helps with two main goals:</p>
<ul dir="auto">
<li>
<p dir="auto">It will flag any out of memory (OOM) information, such as when the <code>per_device_batch_size</code> is set too high, with an identical OOM stack trace as if it was compiled on the target hardware.</p>
</li>
<li>
<p dir="auto">The ahead of time compilation can be saved and then loaded for fast startup and restart times on the target hardware.</p>
</li>
</ul>
<p dir="auto">The tool <code>train_compile.py</code> is tightly linked to <code>train.py</code> and uses the same configuration file <code>configs/base.yml</code>. Although you don't need to run on a TPU, you do need to install <code>jax[tpu]</code> in addition to other dependencies, so we recommend running <code>setup.sh</code> to install these if you have not already done so.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example AOT 1: Compile ahead of time basics</h3><a id="user-content-example-aot-1-compile-ahead-of-time-basics" aria-label="Permalink: Example AOT 1: Compile ahead of time basics" href="#example-aot-1-compile-ahead-of-time-basics"></a></p>
<p dir="auto">After installing the dependencies listed above, you are ready to compile ahead of time:</p>
<div data-snippet-clipboard-copy-content="# Run the below on a single machine, e.g. a CPU
python3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 compile_topology_num_slices=2 \
global_parameter_scale=16 per_device_batch_size=4"><pre><code># Run the below on a single machine, e.g. a CPU
python3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 compile_topology_num_slices=2 \
global_parameter_scale=16 per_device_batch_size=4
</code></pre></div>
<p dir="auto">This will compile a 16B parameter MaxText model on 2 v5e pods.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example AOT 2: Save compiled function, then load and run it</h3><a id="user-content-example-aot-2-save-compiled-function-then-load-and-run-it" aria-label="Permalink: Example AOT 2: Save compiled function, then load and run it" href="#example-aot-2-save-compiled-function-then-load-and-run-it"></a></p>
<p dir="auto">Here is an example that saves then loads the compiled <code>train_step</code>, starting with the save:</p>
<p dir="auto"><strong>Step 1: Run AOT and save compiled function</strong></p>
<div data-snippet-clipboard-copy-content="# Run the below on a single machine, e.g. a CPU
export LIBTPU_INIT_ARGS=&quot;--xla_enable_async_all_gather=true&quot;
python3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 \
compile_topology_num_slices=2 \
compiled_trainstep_file=my_compiled_train.pickle global_parameter_scale=16 \
per_device_batch_size=4 steps=10000 learning_rate=1e-3"><pre><code># Run the below on a single machine, e.g. a CPU
export LIBTPU_INIT_ARGS="--xla_enable_async_all_gather=true"
python3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 \
compile_topology_num_slices=2 \
compiled_trainstep_file=my_compiled_train.pickle global_parameter_scale=16 \
per_device_batch_size=4 steps=10000 learning_rate=1e-3
</code></pre></div>
<p dir="auto"><strong>Step 2: Run train.py and load the compiled function</strong></p>
<p dir="auto">To load the compiled train_step, you just need to pass <code>compiled_trainstep_file=my_compiled_train.pickle</code> into <code>train.py</code>:</p>
<div data-snippet-clipboard-copy-content="# Run the below on each host of the target hardware, e.g. each host on 2 slices of v5e-256
export LIBTPU_INIT_ARGS=&quot;--xla_enable_async_all_gather=true&quot;
python3 MaxText/train.py MaxText/configs/base.yml run_name=example_load_compile \
compiled_trainstep_file=my_compiled_train.pickle \
global_parameter_scale=16  per_device_batch_size=4 steps=10000 learning_rate=1e-3 \
base_output_directory=gs://my-output-bucket dataset_path=gs://my-dataset-bucket"><pre><code># Run the below on each host of the target hardware, e.g. each host on 2 slices of v5e-256
export LIBTPU_INIT_ARGS="--xla_enable_async_all_gather=true"
python3 MaxText/train.py MaxText/configs/base.yml run_name=example_load_compile \
compiled_trainstep_file=my_compiled_train.pickle \
global_parameter_scale=16  per_device_batch_size=4 steps=10000 learning_rate=1e-3 \
base_output_directory=gs://my-output-bucket dataset_path=gs://my-dataset-bucket
</code></pre></div>
<p dir="auto">In the save step of example 2 above we included exporting the compiler flag <code>LIBTPU_INIT_ARGS</code> and <code>learning_rate</code> because those affect the compiled object <code>my_compiled_train.pickle.</code> The sizes of the model (e.g. <code>global_parameter_scale</code>, <code>max_sequence_length</code> and <code>per_device_batch</code>) are fixed when you initially compile via <code>compile_train.py</code>, you will see a size error if you try to run the saved compiled object with different sizes than you compiled with. However a subtle note is that the <strong>learning rate schedule</strong> is also fixed when you run <code>compile_train</code> - which is determined by both <code>steps</code> and <code>learning_rate</code>. The optimizer parameters such as  <code>adam_b1</code> are passed only as shaped objects to the compiler - thus their real values are determined when you run <code>train.py</code>, not during the compilation. If you do pass in different shapes (e.g. <code>per_device_batch</code>), you will get a clear error message reporting that the compiled signature has different expected shapes than what was input. If you attempt to run on different hardware than the compilation targets requested via <code>compile_topology</code>, you will get an error saying there is a failure to map the devices from the compiled to your real devices. Using different XLA flags or a LIBTPU than what was compiled will probably run silently with the environment you compiled in without error. However there is no guaranteed behavior in this case; you should run in the same environment you compiled in.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Automatically Upload Logs to Vertex Tensorboard</h2><a id="user-content-automatically-upload-logs-to-vertex-tensorboard" aria-label="Permalink: Automatically Upload Logs to Vertex Tensorboard" href="#automatically-upload-logs-to-vertex-tensorboard"></a></p>
<p dir="auto">MaxText supports automatic upload of logs collected in a directory to a Tensorboard instance in Vertex AI. Follow <a href="https://github.com/google/maxtext/blob/main/getting_started/Use_Vertex_AI_Tensorboard.md">user guide</a> to know more.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm giving up on open source (187 pts)]]></title>
            <link>https://nutjs.dev/blog/i-give-up</link>
            <guid>40139837</guid>
            <pubDate>Wed, 24 Apr 2024 02:34:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nutjs.dev/blog/i-give-up">https://nutjs.dev/blog/i-give-up</a>, See on <a href="https://news.ycombinator.com/item?id=40139837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A little over a year ago I wrote a blog post<!-- --> <!-- -->about <a href="https://nutjs.dev/blog/money">open source and why I'm charging money for some of my plugins</a>.<!-- --> <!-- -->Sadly, one year later I've reached a point where I'm just not willing to continue the way I did before.</p><p>So this is my letter of resignation.</p><h2 id="why">Why?</h2><p>Ever since I first started to use Linux I was fascinated by the idea of open source.<!-- --> <!-- -->Almost everything I built on my own is open source, and I'm still contributing to upstream projects I'm using if I<!-- --> <!-- -->encounter things I can improve.</p><p>I sponsored an open source project for the first time over ten years ago when I was still in university, because I<!-- --> <!-- -->always held the belief that if a project is valuable for me, it's worth supporting it.<!-- --> <!-- -->And if I don't have the time to contribute to it myself, I should at least support the people that do.</p><p>Of course, there are people that explicitly do not want any kind of sponsorship, but if they do, I'm happy to help.<!-- --> <!-- -->Working on an open source project is still work, and if you're doing a good job, you should be rewarded for it.<!-- --> <!-- -->I also always believed that if you ever started a project that is valuable for companies, they would support you in<!-- --> <!-- -->return, at least that's the reason why my own company is a monthly sponsor of <a href="https://verdaccio.org/">Verdaccio</a> and<!-- --> <!-- -->why I sponsored maintainers of libraries I'm relying on.</p><p>So, due to these naive believes, I started to work on nut.js under Apache-2.0 license, because I thought that if<!-- --> <!-- -->companies and individuals alike are able to permissively use my software, they would also be willing to support me in<!-- --> <!-- -->return.<!-- --> <!-- -->Now, before you start judging me that I'm only in for the money, wouldn't you agree that it sounds awesome to work on an<!-- --> <!-- -->open source project full time, and still be able to pay your bills?</p><p>Did it work out? No. All I got was complaints.</p><p>In the beginning, people were complaining that the image search plugin was baked into the core of nut.js, and that they<!-- --> <!-- -->were forced to use certain compatible versions of node or Electron.<!-- --> <!-- -->Then they started complaining that the image search plugin was not compatible with Apple Silicon. Which I made clear<!-- --> <!-- -->I would not be able to fix without a machine to test on. So if nobody is willing to lend me a machine or sponsor me, so<!-- --> <!-- -->I could get one myself, it's not going to happen.</p><p>You think anyone made a move? Nope.</p><p>Once I decided to take the investment myself, but charge for the new plugin, I suddenly turned into the greedy asshole<!-- --> <!-- -->that's not giving away everything for free.</p><p>Same goes for companies.<!-- --> <!-- -->Nobody cares about you as long as everything is working smoothly, but as soon as they encounter a problem, guess who<!-- --> <!-- -->comes knocking on my door?</p><p><a href="https://github.com/nut-tree/nut.js/issues/577">This public issue on the nut.js repo</a>, where I'm publicly accused of<!-- --> <!-- -->something that's entirely not true was the final nail in the coffin.</p><p>This has happened several times already.<!-- --> <!-- -->I've been insulted for the things I do with nut.js on Discord, Reddit and now GitHub, but this time, I'm not just<!-- --> <!-- -->sucking it up.</p><p>It may seem to you that open source is great because it's free to use.<!-- --> <!-- -->Truth is, it certainly is not free.<!-- --> <!-- -->Someone is paying a price for it, and if it's not the user, it's the maintainer.</p><p>Everyone's time is valuable, and you may want to spend it wisely.<!-- --> <!-- -->If it's fun to spend time on something, that's great.<!-- --> <!-- -->But if it becomes a burden, it's not fun anymore.</p><p>And if people start insulting you for something you're doing in your free time, it's time to stop.</p><p>Open source is great, but it's not sustainable.<!-- --> <!-- -->We self-sabotaged ourselves over decades, and now we're at a point where it's hard to turn back.<!-- --> <!-- -->Publishing source code for the greater good is a noble cause, but to be honest, I think that over the years, using "open<!-- --> <!-- -->source" has become an excuse to avoid paying for software.</p><p>And who's to blame if something goes wrong? The maintainers, of course.</p><p>I've played this game with nut.js for almost six years, but it's coming to an end now.</p><h2 id="whats-next">What's next?</h2><p>All of my packages around nut.js will cease to exist publicly on npm.<!-- --> <!-- -->Ready-to-use packages will only be available through the private nut.js package registry, which requires an active<!-- --> <!-- -->subscription to be used.</p><p>The GitHub repo will remain public, so if you want to continue using nut.js on your own, you'll have to take care of<!-- --> <!-- -->building, testing and hosting packages yourself.</p><p>If you want to save yourself some time and work, you should grab a license today, because prices will also increase<!-- --> <!-- -->with the release of additional plugins.<!-- --> <!-- -->Existing subscribers will not be affected by this increase.</p><h2 id="will-i-stop-working-on-nut-js-entirely">Will I stop working on nut.js entirely?</h2><p>Definitely not.</p><p>I'll continue to work on nut.js, but updates to the repo will happen with a delay.<!-- --> <!-- -->New features, patches, bug fixes and security updates will be made available to subscribers first.</p><p>As I said, if you want to continue using nut.js, you'll have to take care of building, testing and hosting packages<!-- --> <!-- -->yourself.</p><p>All the best</p><p>Simon</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BeeBase, a programmable relational database with graphical user interface (211 pts)]]></title>
            <link>https://beebase.sourceforge.io</link>
            <guid>40139510</guid>
            <pubDate>Wed, 24 Apr 2024 01:44:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://beebase.sourceforge.io">https://beebase.sourceforge.io</a>, See on <a href="https://news.ycombinator.com/item?id=40139510">Hacker News</a></p>
<div id="readability-page-1" class="page">
    

    <p>Redirection to https://beebase.sourceforge.io/index.php</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Simulating Jupiter (316 pts)]]></title>
            <link>https://emildziewanowski.com/flowfields/</link>
            <guid>40139434</guid>
            <pubDate>Wed, 24 Apr 2024 01:32:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emildziewanowski.com/flowfields/">https://emildziewanowski.com/flowfields/</a>, See on <a href="https://news.ycombinator.com/item?id=40139434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-simplebar="">
                                    <article>
                        <div>
                            <p><h2>Flowfields</h2></p>
                            
<p>Realistic is not necessarily the most convincing. Audio designers know that well and use frozen leeks and watermelons to create sounds of breaking bones and tearing flesh. Chalk shot from slingshot was safer alternative for actual firearms in old westerns. Fake doesn’t have to mean worse, especially when it is hard to tell the difference. With that in mind I will try to create complex flow without using computational fluid dynamics.</p>







<h2>New, Better Jupiter</h2>



<p>Jupiter’s is undeniably beautiful, but it’s safest to admire it from a distance. Characteristic patterns observed on the surface are, in fact, massive storms and Earth-sized cyclones. Jovian winds can reach speeds of hundreds of kilometers per hour, although they may appear quaint from orbit.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/03\/JupiterJuno.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-746&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterJuno-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterJuno-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterJuno-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterJuno-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterJuno-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterJuno-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>I plan to feature a Jupiter-like planet as part of the skybox. While not a unique concept and often seen in the sci-fi genre, I want to take it a step further by animating it. In reality, the motion of the atmosphere is too slow to be noticed. However, I will speed it up substantially to make the movement obvious. This should capture players’ attention and remind them that the action is taking place on a distant, alien world.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/JupiterConcept.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-372&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/JupiterConcept-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/JupiterConcept-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/JupiterConcept-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/JupiterConcept-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/JupiterConcept-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/JupiterConcept-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<h2>Flow</h2>



<p>While I don’t want to create a carbon copy of Jupiter, there is a set of features that immediately come to mind when thinking about gas planet. Hopefully, by recreating these, I will get the visuals reminiscent of Jupiter.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/03\/JupiterFlow.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-625&quot;,&quot;imgStyles&quot;:&quot;width:840px;height:auto&quot;,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterFlow-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterFlow-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterFlow-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterFlow-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterFlow-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterFlow-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>There are three distinct flow patterns I want to replicate :</p>



<ul>
<li>Cyclones: A couple of large, easily identifiable vortices, much like Jupiter’s ‘Great Red Spot.’ Typically elongated, they are often accompanied by a ‘wake’ – a trail of secondary vortices.</li>



<li>Jets: These are linear currents that run parallel to the equator, with easily visible turbulent transition layers.</li>



<li>Storms: Smaller, more volatile, and less defined flow structures that contribute to the texture of the atmosphere.</li>
</ul>



<h2>Animating Fluids</h2>



<p>Recreating water or any other sort of fluid in games is challenging. Computational fluid dynamics is demanding in terms of memory and processing power, but that hasn’t prevented game developers from including water in their games. With a set of clever hacks and workarounds, there is no need for expensive simulation.</p>



<h3>Color Cycling</h3>



<p>Color cycling is a technique with a long history, dating back to systems like the NES. It’s akin to painting by numbers, where colors representing different numbers change each frame. Despite its simplicity, when applied to well-prepared input sprites, it can produce eye-catching effects.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/ContraPaletteCycling.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-322&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/ContraPaletteCycling-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/ContraPaletteCycling-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/ContraPaletteCycling-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/ContraPaletteCycling-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/ContraPaletteCycling-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/ContraPaletteCycling-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>







<h3>Frame-by-Frame Animation</h3>



<p>For a while, the frame-by-frame approach was the standard solution. Each frame was stored as a separate sprite, which demanded a lot of memory. As a result, this method was typically reserved for short animation loops consisting of only a few frames.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/FrameByFrame.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-324&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/FrameByFrame-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/FrameByFrame-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/FrameByFrame-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/FrameByFrame-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/FrameByFrame-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/FrameByFrame-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Limited number of animation frames leads to a ‘choppy’ motion, typical of 90s shooters.</p>







<h3>Texture Scrolling</h3>



<p>As soon as games moved into fully textured 3D environments, texture scrolling became a viable option for creating rivers. This method involves incrementing one of the UV components over time to create the illusion of moving texture. When combined with appropriate geometry and shaders, texture scrolling can yield impressive results and remains a popular choice.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/TexturePanning.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-317&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/TexturePanning-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/TexturePanning-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/TexturePanning-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/TexturePanning-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/TexturePanning-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/TexturePanning-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>While versatile this technique is limited to laminar flow – it is non-trivial to add vortices or any other complex flow patterns.</p>







<h3>Quake UV Distortion</h3>



<p>Id Software, the developer behind Quake, is renowned for its innovations. The water distortion they created is rarely listed among them, but is still worth looking into. This simple formula lends Quake’s lava, water, and portals their distinctive appearance.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/QuakeLava-2.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-352&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/QuakeLava-2-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/QuakeLava-2-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/QuakeLava-2-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/QuakeLava-2-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/QuakeLava-2-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/QuakeLava-2-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Although it’s straightforward to replicate using shaders, the original effect was made without them, relying instead on software rendering.</p>







<h3>Unreal WaterPaint</h3>



<p>WaterPaint is one of the most intriguing methods for simulating fluids in games, yet it’s also one of the most challenging to understand. It appears to generate the surface of the liquid and subsequently uses this information to distort a texture. The system’s complexity borders on overengineering, particularly given that the resulting effect is often overlooked in a fast-paced shooter.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/Waterpaint-2.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-311&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/Waterpaint-2-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/Waterpaint-2-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/Waterpaint-2-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/Waterpaint-2-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/Waterpaint-2-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/Waterpaint-2-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Similarly to Quake this technique also predates shaders. Texture pixels are manipulated by CPU and then sampled just like in the case of normal texture. </p>







<p>The methods mentioned are decades old and, on their own, may not hold up well today. However, they still have value as components within larger, more complex systems.</p>



<h2>Velocity Texture</h2>



<p>Let’s create a “universal” flow shader capable of representing any motion of the fluid. Intuitively, we’ll need two textures: one representing the color of the flowing substance, and another one storing the velocity field. This velocity field texture will be a 2-component 2D texture, with the red component representing the x component of the normalized velocity and the green component representing the y component.</p>



<figure><img loading="lazy" decoding="async" width="1024" height="576" src="https://emildziewanowski.com/wp-content/uploads/2024/03/VelocityFieldPacking-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/03/VelocityFieldPacking-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/03/VelocityFieldPacking-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/03/VelocityFieldPacking-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/03/VelocityFieldPacking-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/03/VelocityFieldPacking-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The velocity field is then used to incrementally modify the texture coordinates of the color texture, in the same manner like in the ‘Texture Scrolling’ described previously. However, in this case, the velocity values vary for each pixel.</p>







<p>The result, while interesting, is rather disappointing as it doesn’t accurately simulate fluid motion. Instead, it is an animated distortion that gradually bends the color texture over time. </p>



<p>The formula worked for simple scrolling because in that case the motion was linear and constant. At each point, the velocity was the same. However, here the velocity is more complex – it is defined per pixel. The correct approach would be to use integration.</p>



<h2>Euler Method</h2>



<p>Let’s simplify the problem. Imagine we have a tiny speck of dust sitting on the surface of water that’s moving. We describe the water’s movement using a texture that shows its velocity. Now, we want to figure out the path this speck of dust would take.</p>



<p>The first solution that comes to mind is to take a small step forward, check and update the velocity, then take another step using the updated velocity, and repeat this process. This is called Explicit Euler Method:</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/ExplicitEulerMethod.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-362&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/ExplicitEulerMethod-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/ExplicitEulerMethod-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/ExplicitEulerMethod-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/ExplicitEulerMethod-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/ExplicitEulerMethod-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/ExplicitEulerMethod-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The Explicit or Forward Euler method is often seen as the most basic and least accurate numerical integration method. The larger the integration step, denoted as “h,” the greater the error, and these errors accumulate over time. Even in the example shown, the integrated path represented by the orange arrows deviates significantly from the particle’s true path, depicted by the gray line. Fortunately this inaccuracy won’t be noticeable in the animation.</p>



<p>The problem is, it’s not easy to transform Euler Method directly into a shader. We need to keep track of the particle’s position after each step, and this is something shader alone cannot do. Position, in form of a deformed color texture, has to be stored in a texture.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/EulerTextureAndShader-1.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-367&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/EulerTextureAndShader-1-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/EulerTextureAndShader-1-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/EulerTextureAndShader-1-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/EulerTextureAndShader-1-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/EulerTextureAndShader-1-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/EulerTextureAndShader-1-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The shader reads the texture storing the color of the fluid and deforms it slightly based on the velocity texture. The resulting deformation is then written back into the color texture. This operation is performed every frame of the animation.</p>



<p>To complicate the problem further, it’s generally not possible to read from and write to the same texture in a fragment shader. Unreal Engine solves this problem with Canvas.</p>



<h2>Canvas Node Setup</h2>



<p>Canvas enables the use of a flow shader (Material) to be drawn into a Render Target texture. What sets Canvas apart is its capability to use the same Render Target in both the input and output of the flow shader, forming a feedback loop. To make this process work, several components are necessary:</p>



<ul>
<li>Render Target Texture: This image stores the state of the flow, or the color of the fluid in our case. It must be created before the animation begins and initial color has to be set.</li>



<li>Rendering Event: The process of updating the animation has to be performed every frame, or at least every frame the animated object is visible.</li>



<li>Flow Material Instance: An instance of the Flow Material is necessary, and it has to be supplied with its own Render Target Texture.</li>
</ul>



<p>Once all these elements are in place, the Rendering Event, which corresponds to one step of fluid simulation, can be achieved using just 3 nodes:</p>



<ul>
<li>Begin Draw Canvas to Render Target</li>



<li>Draw Material</li>



<li>End Draw </li>
</ul>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/CanvasNodeSetup.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-394&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/CanvasNodeSetup-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/CanvasNodeSetup-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/CanvasNodeSetup-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/CanvasNodeSetup-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/CanvasNodeSetup-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/CanvasNodeSetup-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>With everything set up, the resulting animation should look like this:</p>







<p>The initial image is shifted in a more fluid manner, with each pixel moving more continuously. However, it still falls short of resembling the motion of a liquid.</p>



<h2>Improving Velocity Field</h2>



<p>Up until now, we’ve relied on basic smoothed 2D vector noise. While sufficient for testing basic functionality, it not enough to realistically representing fluid flow. Liquids tend to swirl around, forming vortices and other complex patterns, which simple noise cannot approximate effectively.</p>



<p>Fortunately, a mathematical operator, the curl, can be particularly useful here. By applying it to scalar noise, we transform it into a velocity field full of vortices. </p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/GrayscaleToCurl.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-431&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/GrayscaleToCurl-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/GrayscaleToCurl-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/GrayscaleToCurl-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/GrayscaleToCurl-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/GrayscaleToCurl-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/GrayscaleToCurl-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>To describe curl in the simplest way possible – in 2D case curl will create a clockwise flow around areas brighter than the surrounding, and counterclockwise flow around darker areas. I describe curl in more detail in <a href="https://emildziewanowski.com/curl-noise/" data-type="post" data-id="19">Dissecting Curl Noise</a> article.</p>



<p>There are multiple ways to calculate curl. DDX and DDY operators are useful in Shadertoy, where the input is not a static texture but a procedural noise. For more traditional applications like Unreal and Unity, it’s probably better to generate it using image generation software like Substance Designer or Photoshop. Any software capable of generating a normal map from a grayscale image will be helpful here, as converting a normal map to curl is simply a matter of swizzling and inverting channels.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/FlowfieldAuthoring.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-426&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/FlowfieldAuthoring-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/FlowfieldAuthoring-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/FlowfieldAuthoring-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/FlowfieldAuthoring-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/FlowfieldAuthoring-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/FlowfieldAuthoring-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The addition of swirly motion adds a fluid-like quality to the animation, although it still appears somewhat static.</p>







<p>The stationary vortices are the reason behind the artificial appearance. This can be addressed by distorting velocity field using the same function that manipulated lava in Quake.</p>







<p>The flow still lacks the complexity needed to resemble natural motion, but this can be remedied with a more detailed velocity field.</p>



<h2>Mixing</h2>



<p>When the algorithm runs for too long, another problem becomes apparent: mixing. While it’s a desired feature, after a while, it turns the flow colors into a solid mass, devoid of any visual interest. </p>







<p>To remedy that, color can be reintroduced by sampling the initial color texture and blending it with the Render Target texture. This process involves using a point grid mask to mimic pigment dissolving in the fluid.</p>







<p>That solves the issue of mixing, but the pattern of points remains too noticeable. By using a noise texture and applying Quake distortion to it, the effect becomes less conspicuous and more natural.</p>







<p>Jupiter’s appearance is attributed to its water-ammonia clouds, which have a range of compositions and colors. These clouds undergo atmospheric circulation, occasionally pushing layers from below to the surface. This phenomenon results in changes in surface colors and structure over time.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/03\/JupiterComposition.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-599&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterComposition-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterComposition-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterComposition-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterComposition-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterComposition-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/03/JupiterComposition-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>I’ll artificially limit cloud compositions to 3 and assign a texture channel to each. Then, to simulate shifts in composition, I’ll utilize color cycling. In shader terms, the initial color, before being sampled and mixed with the render target, will undergo slight modifications over time. The result may look psychedelic, but ultimately, it will replaced by more natural set of colors. Right now those rainbow patterns serve as an useful placeholder.</p>







<p>The left side displays the initial color, while the right side shows the flow.</p>



<h2>Sharpening</h2>



<p>Another side effect of mixing is the blurring of the texture. As the image becomes progressively smoother, the details are lost, causing the texture to appear low-resolution, which is certainly an undesirable outcome.</p>



<p>The obvious solution is to use sharpening – an operation opposite to blurring. In its simplest form, it samples five pixels – the original one and its four neighbors – and returns their weighted sum. The layout of the pixels with their respective weights is called a kernel.</p>



<p>I will use a slightly different formula, one that isolates the ‘delta’ or change in color. This delta is then multiplied by the strength of sharpening. This approach gives me more control over the effect.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/BasicSharpen.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-397&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpen-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpen-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpen-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpen-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpen-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpen-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The material graph representation might seem daunting at first glance, but it’s essentially the result of repeating the same sampling operation multiple times with different parameters.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/BasicSharpenUnrealNodes-1.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-402&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpenUnrealNodes-1-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpenUnrealNodes-1-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpenUnrealNodes-1-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpenUnrealNodes-1-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpenUnrealNodes-1-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/BasicSharpenUnrealNodes-1-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Sharpen is a separate Canvas rendering operation that follows the animation step.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/03\/SharpenStage.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-615&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/03/SharpenStage-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/03/SharpenStage-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/03/SharpenStage-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/03/SharpenStage-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/03/SharpenStage-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/03/SharpenStage-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Sharpening enhances the details but also introduces stripe artifacts.</p>







<p>This occurs because it is part of the feedback loop. It amplifies the difference between pixels, and the next sharpening step further magnifies the difference. This continues until the color values reach their maximum or minimum value.</p>



<p>The solution to that problem is far from elegant but very effective – clamping the calculated difference. This way, the difference doesn’t increase exponentially and the artifacts have no chance to form.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/ClampedSharpenUnrealNodes.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-406&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/ClampedSharpenUnrealNodes-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/ClampedSharpenUnrealNodes-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/ClampedSharpenUnrealNodes-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/ClampedSharpenUnrealNodes-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/ClampedSharpenUnrealNodes-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/ClampedSharpenUnrealNodes-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Sharpening with clamping:</p>







<p>With that in place, we can consider the whole system complete – we have a set of tools that allow for recreating a wide array of flow types in real-time. Now, to make further improvements, we need to enhance the input data – specifically, the velocity field.</p>



<h2>Flow Patterns</h2>



<p>There are 3 flow patterns that can be easily identified on Jupiter:</p>



<ul>
<li>Cyclones</li>



<li>Jets</li>



<li>Storms</li>
</ul>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/03\/MainCurrentsAndVortices.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large is-resized&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-626&quot;,&quot;imgStyles&quot;:&quot;width:840px;height:auto&quot;,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/03/MainCurrentsAndVortices-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/03/MainCurrentsAndVortices-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/03/MainCurrentsAndVortices-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/03/MainCurrentsAndVortices-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/03/MainCurrentsAndVortices-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/03/MainCurrentsAndVortices-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This list is by no means exhaustive. While there are numerous smaller and less noticeable flow details, many of them can be replicated using the same techniques employed for the main three patterns. </p>



<p>I will attempt to translate these patterns into corresponding velocity fields. This way, the complex flow on Jupiter can be broken down into individual flow components. These components could then be rearranged later to create a new, unique gas giant.</p>



<h2>Creating Flowfields</h2>



<p>As mentioned earlier, in a real-game scenario like Unreal or Unity, it’s not practical to generate the velocity field from scratch. It’s more efficient to generate most of the components in Substance Designer or Photoshop and then combine them in the shader to achieve the desired result. This approach allows us to create complex patterns with no additional costs.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/03\/SubstanceVectorfield.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-621&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/03/SubstanceVectorfield-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/03/SubstanceVectorfield-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/03/SubstanceVectorfield-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/03/SubstanceVectorfield-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/03/SubstanceVectorfield-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/03/SubstanceVectorfield-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>I chose to create velocity textures in Substance Designer due to its flexibility and non-destructive workflow.</p>



<h3>Cyclones</h3>



<p>A cyclone is essentially a large vortex. Creating one involves generating a large blurred black or white dot and passing it through the curl operator. To add more complexity, the result can be combined with another operator – the gradient. This allows the cyclone to either suck in or expel matter, making it more dynamic.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/AnticycloneComponents.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-429&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/AnticycloneComponents-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/AnticycloneComponents-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/AnticycloneComponents-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/AnticycloneComponents-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/AnticycloneComponents-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/AnticycloneComponents-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Relation between <a href="https://emildziewanowski.com/curl-noise/#GradientAndCurlOverlayed" data-type="post" data-id="19">curl and gradient is explained here</a> in more detail.</p>



<p>The velocity field is generated by blending a mixture of curl and gradient operators over the previously created flow pattern.</p>







<p>Substance Designer enables the creation of more complex and detailed velocity fields. In this case, the cyclone flowfield was slightly deformed and elongated, featuring a non-linear speed distribution. Unlike a flowfield generated in a shader from scratch, all these details incur no additional cost – everything is baked into the texture.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/04\/AntycycloneSubstanceGraph.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-794&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/04/AntycycloneSubstanceGraph-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/04/AntycycloneSubstanceGraph-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/04/AntycycloneSubstanceGraph-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/04/AntycycloneSubstanceGraph-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/04/AntycycloneSubstanceGraph-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/04/AntycycloneSubstanceGraph-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<h3>Jets</h3>



<p>The bands around Jupiter are known as belts and zones. Belts consist of darker, warmer clouds, while zones are characterized by brighter clouds made of ice. Strong currents form at the transitions between these bands. These currents, known as jets, run parallel to the equator and alternate in direction. Where two jets meet, the flow becomes turbulent, creating chains of vortices.</p>



<p>Replicating that is relatively simple: blurred stripes represent the laminar flow of the jets, while a curl applied to a series of dots creates vortices. It’s worth noticing the color of the dots; the spin of resulting vortices has to match the direction of the surrounding jets.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/BeltComponents.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-434&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/BeltComponents-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/BeltComponents-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/BeltComponents-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/BeltComponents-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/BeltComponents-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/BeltComponents-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>







<p>Once again, the flowfield generated in Designer exhibits more detail. Transition vortices are more scattered and vary in size. Additionally, jets are slightly disturbed to create a more wavy flow.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/04\/JetsSubstanceGraph.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-796&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/04/JetsSubstanceGraph-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/04/JetsSubstanceGraph-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/04/JetsSubstanceGraph-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/04/JetsSubstanceGraph-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/04/JetsSubstanceGraph-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/04/JetsSubstanceGraph-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<h3>Storms</h3>



<p>“Storms” is the term I used to encompass all the smaller vortices and turbulent streams that accompany the main currents. They are essentially noise, and I will approach creating them in the same way I would create noise.</p>



<p>Noise is typically comprised of multiple layers called octaves. Each subsequent octave contains smaller details and has a diminishing influence. In the case of a velocity field, each layer also has to be animated separately.</p>



<p>Those layers are then blended together to form complex, turbulent motion.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/02\/StormComponents.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-436&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/02/StormComponents-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/02/StormComponents-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/02/StormComponents-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/02/StormComponents-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/02/StormComponents-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/02/StormComponents-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>











<p>Substance Designer features storms gathered into clusters. Two versions of that texture are packed into a single texture and blended using moving masks to simulate quickly shifting currents.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/04\/StormsSubstanceGraph-1.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-798&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/04/StormsSubstanceGraph-1-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/04/StormsSubstanceGraph-1-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/04/StormsSubstanceGraph-1-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/04/StormsSubstanceGraph-1-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/04/StormsSubstanceGraph-1-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/04/StormsSubstanceGraph-1-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>It’s a different approach that results in patches of turbulent flow, as opposed to the uniformly distributed storms generated in Shadertoy. These patches resemble what can be observed on Jupiter more closely.</p>



<h3>Combined</h3>



<p>The division into cyclones, jets, and storms was artificial but proved quite useful for illustrating some of the techniques that can be used to mimic real flowfields. Each flow pattern can be achieved in many different ways, with no single approach that can be described as the “right” one.</p>



<p>To merge all these components together, a simple addition would suffice, but using alpha blending allows for accentuating some features like cyclones and toning down turbulence in certain areas.</p>



<p>At this stage, when all the components are ready, blending them together is more a matter of artistic choice than mathematics. After all, none of the presented techniques have solid grounding in physics – they are just approximations of natural phenomena.</p>







<h2>2010: The Year We Make Contact</h2>



<p>When I started working on the animation of the gas giant, I was really excited about the idea because I naively thought that this was going to be something novel, never tried before. Obviously, I was wrong. Films like ‘Outland’, ‘2010: The Year We Make Contact’, and ‘Contact’ all featured animated Jupiter.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/04\/JupiterInMovies.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-836&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterInMovies-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterInMovies-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterInMovies-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterInMovies-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterInMovies-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterInMovies-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The most interesting portrayal here is the rendition created by Digital Productions for ‘2010: The Year We Make Contact’. The technology behind it is a marvel of CGI, even though it looks like a perfectly executed practical effect.</p>



<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/emildziewanowski.com\/wp-content\/uploads\/2024\/04\/JupiterBreakdown.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large is-style-default&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-837&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1440,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img loading="lazy" decoding="async" width="1024" height="576" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterBreakdown-1024x576.jpg" alt="" srcset="https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterBreakdown-1024x576.jpg 1024w, https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterBreakdown-300x169.jpg 300w, https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterBreakdown-768x432.jpg 768w, https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterBreakdown-1536x864.jpg 1536w, https://emildziewanowski.com/wp-content/uploads/2024/04/JupiterBreakdown-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The basic idea remains largely the same: utilizing a flowfield to deform the initial image. However, the execution differs significantly. While I used Substance Designer to generate flow textures, the team at Digital Productions utilized actual fluid mechanics to simulate the flow. My solution to the problem of mixing was to artificially reintroduce the color, whereas they sidestepped the problem entirely by converting the image into particles.</p>



<p>Remarkably, all of this was accomplished without the aid of modern CGI software or computing power. Instead, it relied on the ingenuity of a team of brilliant engineers and artists, supported by the CRAY X-MP.</p>



<p>The work of Larry Yaeger and Craig Upson is described in greater detail in <a href="https://dl.acm.org/doi/pdf/10.1145/15922.15895" data-type="link" data-id="https://dl.acm.org/doi/pdf/10.1145/15922.15895">Siggraph</a> and <a href="https://archive.org/details/CineFex_1985" data-type="link" data-id="https://archive.org/details/CineFex_1985">Cinefex</a> articles. Additionally, there is a <a href="https://youtu.be/5wuClV-i9r4?si=9uxfC098nNyDezOI" data-type="link" data-id="https://youtu.be/5wuClV-i9r4?si=9uxfC098nNyDezOI">documentary available on YouTube</a>.</p>



<h2>Further Developement</h2>



<p>The presented methods should be sufficient to create convincing-looking flow, but not necessarily a visually appealing planet. Achieving that requires several additional steps:</p>



<ul>
<li>Colors: Currently, the R, G, and B channels represent different substances. Ultimately they will be replaced with a color texture.</li>



<li>UV Mapping: Currently, the texture is just a square; it needs to be wrapped around a sphere. However, I plan to use the method described in <a href="https://emildziewanowski.com/flat-planets/" data-type="post" data-id="108">Flat Planets</a> and apply it to a flat disc.</li>



<li>Shading: Atmosphere is lit differently than a solid, opaque object. A specialized shading model has to be created to complete the effect.</li>
</ul>







<h2 id="Conclusions">Conclusions</h2>



<p>The initial setup required some effort, both in Unreal and in Substance Designer, but once in place, it allowed for easy tweaking and modifications. Since it does not rely on computational fluid dynamics, the motion can be handcrafted, which is both a strength and a challenge. It offers total freedom to create any flow imaginable, but requires the artist to have a basic understanding of fluid dynamics.</p>



<figure><p>
<iframe loading="lazy" title="Flowfields Showcase" width="500" height="281" src="https://www.youtube.com/embed/vEw4XCQkaro?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<p>Most importantly, it can compete with actual fluid simulations while using only a fraction of resources. A full planet with a 1024×1024 texture takes less than 0.5ms to render, which is a modest price for such VFX. </p>




                        </div>
                    </article>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple releases CoreNet, a library for training deep neural networks (435 pts)]]></title>
            <link>https://github.com/apple/corenet</link>
            <guid>40139398</guid>
            <pubDate>Wed, 24 Apr 2024 01:26:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/apple/corenet">https://github.com/apple/corenet</a>, See on <a href="https://news.ycombinator.com/item?id=40139398">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CoreNet: A library for training deep neural networks</h2><a id="user-content-corenet-a-library-for-training-deep-neural-networks" aria-label="Permalink: CoreNet: A library for training deep neural networks" href="#corenet-a-library-for-training-deep-neural-networks"></a></p>
<p dir="auto">CoreNet is a deep neural network toolkit that allows researchers and engineers to train standard and novel small and large-scale models for variety of tasks, including foundation models (e.g., CLIP and LLM), object classification, object detection, and semantic segmentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#whats-new">What's new?</a></li>
<li><a href="#research-efforts-at-apple-using-corenet">Research efforts at Apple using CoreNet</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#directory-structure">Directory Structure</a></li>
<li><a href="#maintainers">Maintainers</a></li>
<li><a href="#contributing-to-corenet">Contributing to CoreNet</a></li>
<li><a href="#license">License</a></li>
<li><a href="#relationship-with-cvnets">Relationship with CVNets</a></li>
<li><a href="#citation">Citation</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's new?</h2><a id="user-content-whats-new" aria-label="Permalink: What's new?" href="#whats-new"></a></p>
<ul dir="auto">
<li><em><strong>April 2024</strong></em>: Version 0.1.0 of the CoreNet library includes
<ul dir="auto">
<li>OpenELM</li>
<li>CatLIP</li>
<li>MLX examples</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Research efforts at Apple using CoreNet</h2><a id="user-content-research-efforts-at-apple-using-corenet" aria-label="Permalink: Research efforts at Apple using CoreNet" href="#research-efforts-at-apple-using-corenet"></a></p>
<p dir="auto">Below is the list of publications from Apple that uses CoreNet:</p>
<ul dir="auto">
<li><a href="https://arxiv.org/abs/2404.14619" rel="nofollow">OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework</a></li>
</ul>

<ul dir="auto">
<li><a href="https://github.com/apple/corenet/blob/main">CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data</a></li>
<li><a href="https://arxiv.org/abs/2303.08983" rel="nofollow">Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement</a></li>
<li><a href="https://arxiv.org/abs/2310.14108" rel="nofollow">CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement</a></li>
<li><a href="https://arxiv.org/abs/2303.14189" rel="nofollow">FastVit: A Fast Hybrid Vision Transformer using Structural Reparameterization</a></li>
<li><a href="https://arxiv.org/abs/2306.00238" rel="nofollow">Bytes Are All You Need: Transformers Operating Directly on File Bytes</a></li>
<li><a href="https://arxiv.org/abs/2206.04040" rel="nofollow">MobileOne: An Improved One millisecond Mobile Backbone</a></li>
<li><a href="https://arxiv.org/abs/2212.10553" rel="nofollow">RangeAugment: Efficient Online Augmentation with Range Learning</a></li>
<li><a href="https://arxiv.org/abs/2206.02680" rel="nofollow">Separable Self-attention for Mobile Vision Transformers (MobileViTv2)</a></li>
<li><a href="https://arxiv.org/abs/2206.02002" rel="nofollow">CVNets: High performance library for Computer Vision, ACM MM'22</a></li>
<li><a href="https://arxiv.org/abs/2110.02178" rel="nofollow">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer, ICLR'22</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You will need Git LFS (instructions below) to run tests and Jupyter notebooks
(<a href="https://jupyter.org/install" rel="nofollow">instructions</a>) in this repository,
and to contribute to it so we recommend that you install and activate it first.</p>
<p dir="auto">On Linux we recommend to use Python 3.10+ and PyTorch (version &gt;= v2.1.0), on
macOS system Python 3.9+ should be sufficient.</p>
<p dir="auto">Note that the optional dependencies listed below are required if you'd like to
make contributions and/or run tests.</p>
<p dir="auto">For Linux (substitute <code>apt</code> for your package manager):</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install git-lfs

git clone git@github.com:apple/corenet.git
cd corenet
git lfs install
git lfs pull
# The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.
python3 -m venv venv &amp;&amp; source venv/bin/activate
python3 -m pip install --editable ."><pre>sudo apt install git-lfs

git clone git@github.com:apple/corenet.git
<span>cd</span> corenet
git lfs install
git lfs pull
<span><span>#</span> The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.</span>
python3 -m venv venv <span>&amp;&amp;</span> <span>source</span> venv/bin/activate
python3 -m pip install --editable <span>.</span></pre></div>
<p dir="auto">To install optional dependencies for audio and video processing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install libsox-dev ffmpeg"><pre>sudo apt install libsox-dev ffmpeg</pre></div>
<p dir="auto">For macOS, assuming you use Homebrew:</p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install git-lfs

git clone git@github.com:apple/corenet.git
cd corenet
cd \$(pwd -P)  # See the note below.
git lfs install
git lfs pull
# The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.
python3 -m venv venv &amp;&amp; source venv/bin/activate
python3 -m pip install --editable ."><pre>brew install git-lfs

git clone git@github.com:apple/corenet.git
<span>cd</span> corenet
<span>cd</span> <span>\$</span>(pwd -P)  <span><span>#</span> See the note below.</span>
git lfs install
git lfs pull
<span><span>#</span> The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment.</span>
python3 -m venv venv <span>&amp;&amp;</span> <span>source</span> venv/bin/activate
python3 -m pip install --editable <span>.</span></pre></div>
<p dir="auto">To install optional dependencies for audio and video processing:</p>

<p dir="auto">Note that on macOS the file system is case insensitive, and case sensitivity
can cause issues with Git. You should access the repository on disk as if the
path were case sensitive, i.e. with the same capitalization as you see when you
list the directories <code>ls</code>. You can switch to such a path with the <code>cd $(pwd -P)</code>
command.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Directory Structure</h2><a id="user-content-directory-structure" aria-label="Permalink: Directory Structure" href="#directory-structure"></a></p>
<p dir="auto">This section provides quick access and a brief description for important CoreNet directories.</p>
<table>
<thead>
<tr>
<th> Description </th>
<th> Quick Access </th>
</tr>
</thead>
<tbody>

<tr> <td> <p dir="auto"><h3 tabindex="-1" dir="auto"> Getting Started </h3><a id="user-content--getting-started-" aria-label="Permalink:  Getting Started " href="#-getting-started-"></a></p> 
Working with the examples is an easy way to get started with CoreNet. 
</td> <td> <pre>└── tutorials
 &nbsp;  ├── <a href="https://github.com/apple/corenet/blob/main/tutorials/train_a_new_model_on_a_new_dataset_from_scratch.ipynb">train_a_new_model_on_a_new_dataset_from_scratch.ipynb</a>
 &nbsp;  ├── <a href="https://github.com/apple/corenet/blob/main/tutorials/guide_slurm_and_multi_node_training.md">guide_slurm_and_multi_node_training.md</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/tutorials/clip.ipynb">clip.ipynb</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/tutorials/semantic_segmentation.ipynb">semantic_segmentation.ipynb</a>
    └── <a href="https://github.com/apple/corenet/blob/main/tutorials/object_detection.ipynb">object_detection.ipynb</a>
</pre> </td> </tr>
<tr> <td> <p dir="auto"><h3 tabindex="-1" dir="auto"> Training Recipes </h3><a id="user-content--training-recipes-" aria-label="Permalink:  Training Recipes " href="#-training-recipes-"></a></p>
CoreNet provides reproducible training recipes, in addition to the pretrained model 
weights and checkpoints for the publications that are listed in <code>projects/</code> directory.
<p dir="auto">Publication project directories generally contain the following contents:</p>
<ul dir="auto">
<li><code>README.md</code> provides documentation, links to the pretrained weights, and citations.</li>
<li><code>&lt;task_name&gt;/&lt;model_name&gt;.yaml</code> provides configuration for reproducing the trainings and evaluations.</li>
</ul>
</td> <td> <pre>└── projects
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/byteformer">byteformer</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/catlip">catlip</a> (*)
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/clip">clip</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/fastvit">fastvit</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/mobilenet_v1">mobilenet_v1</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/mobilenet_v2">mobilenet_v2</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/mobilenet_v3">mobilenet_v3</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/mobileone">mobileone</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/mobilevit">mobilevit</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/mobilevit_v2">mobilevit_v2</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/openelm">openelm</a> (*)
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/range_augment">range_augment</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/projects/resnet">resnet</a>
    └── <a href="https://github.com/apple/corenet/blob/main/projects/vit">vit</a>
<br>
(*) Newly released.
</pre> </td> </tr>
<tr> <td> <p dir="auto"><h3 tabindex="-1" dir="auto"> MLX Examples </h3><a id="user-content--mlx-examples-" aria-label="Permalink:  MLX Examples " href="#-mlx-examples-"></a></p>
MLX examples demonstrate how to run CoreNet models efficiently on Apple Silicon.
Please find further information in the <code>README.md</code> file within the corresponding example directory.
</td> <td> <pre>└──mlx_example
    ├── <a href="https://github.com/apple/corenet/blob/main/mlx_examples/clip">clip</a>
    └── <a href="https://github.com/apple/corenet/blob/main/mlx_examples/open_elm">open_elm</a>
</pre> </td> </tr>
<tr> <td> <p dir="auto"><h3 tabindex="-1" dir="auto"> Model Implementations </h3><a id="user-content--model-implementations-" aria-label="Permalink:  Model Implementations " href="#-model-implementations-"></a></p> 
Models are organized by tasks (e.g. "classification"). You can find all model implementations for each
task in the corresponding task folder. 
<p dir="auto">Each model class is decorated by a
<code>@MODEL_REGISTRY.register(name="&lt;model_name&gt;", type="&lt;task_name&gt;")</code> decorator.
To use a model class in CoreNet training or evaluation,
assign <code>moels.&lt;task_name&gt;.name = &lt;model_name&gt;</code> in the YAML configuration.</p>
</td> <td> <pre>└── corenet
    └── modeling
        └── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/models">models</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/models/audio_classification">audio_classification</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/models/classification">classification</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/models/detection">detection</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/models/language_modeling">language_modeling</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/models/multi_modal_img_text">multi_modal_img_text</a>
            └── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/models/segmentation">segmentation</a>
</pre> </td> </tr>
<tr> <td> <p dir="auto"><h3 tabindex="-1" dir="auto"> Datasets </h3><a id="user-content--datasets-" aria-label="Permalink:  Datasets " href="#-datasets-"></a></p> 
Similarly to the models, datasets are also categorized by tasks.
</td> <td> <pre>└── corenet
    └── data
        └── <a href="https://github.com/apple/corenet/blob/main/corenet/data/datasets">datasets</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/datasets/audio_classification">audio_classification</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/datasets/classification">classification</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/datasets/detection">detection</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/datasets/language_modeling">language_modeling</a>
            ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/datasets/multi_modal_img_text">multi_modal_img_text</a>
            └── <a href="https://github.com/apple/corenet/blob/main/corenet/data/datasets/segmentation">segmentation</a>
</pre> </td> </tr>
<tr> <td> <p dir="auto"><h3 tabindex="-1" dir="auto"> Other key directories </h3><a id="user-content--other-key-directories-" aria-label="Permalink:  Other key directories " href="#-other-key-directories-"></a></p> 
In this section, we have highlighted the rest of the key directories that implement 
classes corresponding to the names that are referenced in the YAML configurations.
</td> <td> <pre>└── corenet
    ├── <a href="https://github.com/apple/corenet/blob/main/corenet/loss_fn">loss_fn</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/corenet/metrics">metrics</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/corenet/optims">optims</a>
    │&nbsp;&nbsp; └── <a href="https://github.com/apple/corenet/blob/main/corenet/optims/scheduler">scheduler</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/corenet/train_eval_pipelines">train_eval_pipelines</a>
    ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data">data</a>
    │&nbsp;&nbsp; ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/collate_fns">collate_fns</a>
    │&nbsp;&nbsp; ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/sampler">sampler</a>
    │&nbsp;&nbsp; ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/text_tokenizer">text_tokenizer</a>
    │&nbsp;&nbsp; ├── <a href="https://github.com/apple/corenet/blob/main/corenet/data/transforms">transforms</a>
    │&nbsp;&nbsp; └── <a href="https://github.com/apple/corenet/blob/main/corenet/data/video_reader">video_reader</a>
    └── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling">modeling</a>
        ├── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/layers">layers</a>
        ├── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/modules">modules</a>
        ├── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/neural_augmentor">neural_augmentor</a>
        └── <a href="https://github.com/apple/corenet/blob/main/corenet/modeling/text_encoders">text_encoders</a>
</pre> </td> </tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Maintainers</h2><a id="user-content-maintainers" aria-label="Permalink: Maintainers" href="#maintainers"></a></p>
<p dir="auto">This code is developed by <a href="https://sacmehta.github.io/" rel="nofollow">Sachin</a>, and is now maintained by Sachin, <a href="https://mchorton.com/" rel="nofollow">Maxwell Horton</a>, <a href="https://www.mohammad.pro/" rel="nofollow">Mohammad Sekhavat</a>, and Yanzi Jin.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Previous Maintainers</h3><a id="user-content-previous-maintainers" aria-label="Permalink: Previous Maintainers" href="#previous-maintainers"></a></p>
<ul dir="auto">
<li><a href="https://farzadab.github.io/" rel="nofollow">Farzad</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing to CoreNet</h2><a id="user-content-contributing-to-corenet" aria-label="Permalink: Contributing to CoreNet" href="#contributing-to-corenet"></a></p>
<p dir="auto">We welcome PRs from the community! You can find information about contributing to CoreNet in our <a href="https://github.com/apple/corenet/blob/main/CONTRIBUTING.md">contributing</a> document.</p>
<p dir="auto">Please remember to follow our <a href="https://github.com/apple/corenet/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">For license details, see <a href="https://github.com/apple/corenet/blob/main/LICENSE">LICENSE</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Relationship with CVNets</h2><a id="user-content-relationship-with-cvnets" aria-label="Permalink: Relationship with CVNets" href="#relationship-with-cvnets"></a></p>
<p dir="auto">CoreNet evolved from CVNets, to encompass a broader range of applications beyond computer vision. Its expansion facilitated the training of foundational models, including LLMs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you find our work useful, please cite the following paper:</p>
<div data-snippet-clipboard-copy-content="@inproceedings{mehta2022cvnets, 
     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, 
     title = {CVNets: High Performance Library for Computer Vision}, 
     year = {2022}, 
     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, 
     series = {MM '22} 
}"><pre><code>@inproceedings{mehta2022cvnets, 
     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, 
     title = {CVNets: High Performance Library for Computer Vision}, 
     year = {2022}, 
     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, 
     series = {MM '22} 
}
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FTC adopts a comprehensive ban on new noncompetes with all workers (180 pts)]]></title>
            <link>https://www.ftc.gov/legal-library/browse/rules/noncompete-rule</link>
            <guid>40138824</guid>
            <pubDate>Wed, 24 Apr 2024 00:01:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ftc.gov/legal-library/browse/rules/noncompete-rule">https://www.ftc.gov/legal-library/browse/rules/noncompete-rule</a>, See on <a href="https://news.ycombinator.com/item?id=40138824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="a0">
  <p>As used in this part:</p><p><em>Business entity</em>&nbsp;means a partnership, corporation, association, limited liability company, or other legal entity, or a division or subsidiary thereof.</p><p><em>Employment</em>&nbsp;means work for a person.</p><p><em>Non-compete clause</em>&nbsp;means:</p><p>(1) A term or condition of employment that prohibits a worker from, penalizes a worker for, or functions to prevent a worker from:</p><p>(i) seeking or accepting work in the United States with a different person where such work would begin after the conclusion of the employment that includes the term or condition; or</p><p>(ii) operating a business in the United States after the conclusion of the employment that includes the term or condition.</p><p>(2) For the purposes of this part 910, term or condition of employment includes, but is not limited to, a contractual term or workplace policy, whether written or oral.</p><p><em>Officer</em> means a president, vice president, secretary, treasurer or principal financial officer, comptroller or principal accounting officer, and any natural person routinely performing corresponding functions with respect to any business entity whether incorporated or unincorporated.</p><p><em>Person</em> means any natural person, partnership, corporation, association, or other legal entity within the Commission’s jurisdiction,&nbsp;including any&nbsp;person acting under color or authority of State law.</p><p><em>Policy-making authority</em>&nbsp;means final authority to make policy decisions that control significant aspects of a business entity or common enterprise and does not&nbsp;include authority limited to advising or exerting influence over such policy decisions or having final authority to make policy decisions for only a subsidiary of or affiliate of a common enterprise.</p><p><em>Policy-making position</em> means a business entity’s president, chief executive officer or the equivalent, any other officer of a business entity who has policy-making authority, or any other natural person who has policy-making authority for the business entity similar to an officer with policy-making authority. An officer of a subsidiary or affiliate of a business entity that is part of a common enterprise who has policy-making authority for the common enterprise may be deemed to have a policy-making position for purposes of this paragraph. A natural person who does not have policy-making authority over a common enterprise may not be deemed to have a policy-making position even if the person has policy-making authority over a subsidiary or affiliate of a business entity that is part of the common enterprise.</p><p><em>Preceding year</em> means a person’s choice among the following time periods: the most recent 52-week year, the most recent calendar year, the most recent fiscal year, or the most recent anniversary of hire year.</p><p><em>Senior executive</em> means a worker who:</p><p>(1) Was in a policy-making position; and</p><p>(2) Received from a person for the employment:</p><p>(i) Total annual compensation of at least $151,164 in the preceding year; or</p><p>(ii) Total compensation of at least $151,164 when annualized if the worker was employed during only part of the preceding year; or</p><p>(iii) Total compensation of at least $151,164 when annualized in the preceding year prior to the worker’s departure if the worker departed from employment prior to the preceding year and the worker is subject to a non-compete clause.&nbsp; &nbsp;</p><p><em>Total annual compensation</em>&nbsp;is based on the worker’s earnings over the preceding year. Total annual compensation may include salary, commissions, nondiscretionary bonuses and other nondiscretionary compensation earned during that 52-week period. Total annual compensation does not include board, lodging and other facilities as defined in 29&nbsp;CFR&nbsp;541.606, and does not include payments for medical insurance, payments for life insurance, contributions to retirement plans and the cost of other similar fringe benefits.</p><p><em>Worker </em>means a natural person who works or who previously worked, whether paid or unpaid, without regard to the worker’s title or the worker’s status under any other State or Federal laws, including, but not limited to, whether the worker is an employee, independent contractor, extern, intern, volunteer, apprentice, or a sole proprietor who provides a service to a person. The term worker includes a natural person who works for a franchisee or franchisor, but does not include a franchisee in the context of a franchisee-franchisor relationship.&nbsp;</p>
</div><div id="a1">
  <p>(a) <em>Unfair methods of competition</em>—(1) <em>Workers other than senior executives</em>. With respect to a worker other than a senior executive, it is an unfair method of competition for a person:</p><p>(i) To enter into or attempt to enter into a non-compete clause;</p><p>(ii) To enforce or attempt to enforce a non-compete clause; or</p><p>(iii) To represent that the worker is subject to a non-compete clause.</p><p>(2) <em>Senior executives</em>. With respect to a senior executive, it is an unfair method of competition for a person:</p><p>(i) To enter into or attempt to enter into a non-compete clause;</p><p>(ii) To enforce or attempt to enforce a non-compete clause entered into after the effective date; or</p><p>(iii) To represent that the senior executive is subject to a non-compete clause, where the non-compete clause was entered into after the effective date.</p><p>(b) <em>Notice requirement for existing non-compete clauses</em>—(1) <em>Notice required</em>. For each existing non-compete clause that it is an unfair method of competition to enforce or attempt to enforce under paragraph (a)(1)(ii) of this section, the person who entered into the non-compete clause with the worker must provide clear and conspicuous notice to the worker by the effective date that the worker’s non-compete clause will not be, and cannot legally be, enforced against the worker.</p><p>(2) <em>Form of notice</em>.&nbsp;The notice to the worker required by paragraph (b)(1) of this section must:</p><p>(i) Identify the person who entered into the non-compete clause with the worker;</p><p>(ii) Be on paper delivered by hand to the worker, or by mail at the worker’s last known personal street address, or by email at an email address belonging to the worker, including the worker’s current work email address or last known personal email address, or by text message at a mobile telephone number belonging to the worker.</p><p>(3) <em>Exception</em>.&nbsp;If a person that is required to provide notice under paragraph (b)(1) of this section has no record of a street address, email address, or mobile telephone number, such person is exempt from the notice requirement in paragraph (b)(1) of this section with respect to such worker.</p><p>(4) <em>Model language</em>.&nbsp;For purposes of paragraph (b)(1) of this section, the following model language constitutes notice to the worker that the worker’s non-compete clause cannot legally be enforced and will not be enforced against the worker.</p><p>Figure 1 to paragraph (b)(4)—Model Language</p><a href="https://www.ftc.gov/system/files/ftc_gov/images/new-rule-image-noncompete-rulev3.png">
<article>
  </article></a><p>(5) <em>Safe harbor</em>. A person complies with the requirement in paragraph (b)(1) of this section if the person provides notice to a worker pursuant to paragraph (b)(4) of this section.</p><p>(6) <em>Optional notice in additional languages</em>.&nbsp;In addition to providing the notice required in paragraph (b)(1) of this section in English, a person is permitted to provide such notice in a language (or in languages) other than English or to include internet links to translations in additional languages. If providing optional notice under this paragraph (b)(6), a person may use any Commission-provided translation of the model language in paragraph (b)(4) of this section.</p>
</div><div id="a2">
  <p>(a) <em>Bona fide sales of business</em>.&nbsp;The requirements of this part 910 shall not apply to a non-compete clause that is entered into by a person pursuant to a bona fide&nbsp;sale of a business entity, of the person’s ownership interest in a business entity, or of all or substantially all of a business entity’s operating assets.</p><p>(b) <em>Existing causes of action</em>. The requirements of this part 910 do not apply where a cause of action related to a non-compete clause accrued prior to the effective date.</p><p>(c) <em>Good faith</em>.&nbsp;It is not an unfair method of competition to enforce or attempt to enforce a non-compete clause or to make representations about a non-compete clause where a person&nbsp;has a good-faith basis to believe that this part 910 is inapplicable.</p>
</div><div id="a3">
  <p>(a) This part 910 will not be construed to annul, or exempt any person from complying with any State statute, regulation, order, or interpretation applicable to a non-compete clause, including, but not limited to, State&nbsp;antitrust and consumer protection laws and State common law, except that this part 910 supersedes such laws to the extent, and only to the extent, that such laws would otherwise&nbsp;permit or authorize a person to engage in conduct that is an unfair method of competition under §&nbsp;910.2(a) or conflict with the notice requirement in §&nbsp;910.2(b).</p><p>(b) Except with respect to laws superseded under paragraph (a) of this section, no provision of this part 910 shall be construed as altering, limiting, or affecting the authority of a State attorney general or any other regulatory or enforcement agency or entity or the rights of a person to bring a claim or regulatory action arising under any State statute, regulation, order, or interpretation, including, but not limited to, State&nbsp;antitrust and consumer protection laws and State common law.</p>
</div><p>If any provision of this part 910 is held to be invalid or unenforceable by its terms, or as applied to any person or circumstance, or stayed pending further agency action, the provision shall be construed so as to continue to give the maximum effect to the provision permitted by law and such invalidity shall not affect the application of the provision to other persons or circumstances or the validity or application of other provisions. If any provision or application of this part is held to be invalid or unenforceable, the provision or application shall be severable from this part 910 and shall not affect the remainder thereof.<strong>&nbsp;</strong></p><div id="a5">
  <p>This part 910 is effective 120 days after date of publication of the final rule.&nbsp;</p><p>PART 915—[Reserved]</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[American Flag Sort (223 pts)]]></title>
            <link>https://xlinux.nist.gov/dads/HTML/americanFlagSort.html</link>
            <guid>40138579</guid>
            <pubDate>Tue, 23 Apr 2024 23:26:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xlinux.nist.gov/dads/HTML/americanFlagSort.html">https://xlinux.nist.gov/dads/HTML/americanFlagSort.html</a>, See on <a href="https://news.ycombinator.com/item?id=40138579">Hacker News</a></p>
Couldn't get https://xlinux.nist.gov/dads/HTML/americanFlagSort.html: Error: connect ECONNREFUSED 129.6.13.19:443]]></description>
        </item>
    </channel>
</rss>