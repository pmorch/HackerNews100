<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 16 Oct 2024 02:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Graphite, a Blender-inspired 2D procedural design Rust app (137 pts)]]></title>
            <link>https://graphite.rs/</link>
            <guid>41853810</guid>
            <pubDate>Tue, 15 Oct 2024 22:32:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://graphite.rs/">https://graphite.rs/</a>, See on <a href="https://news.ycombinator.com/item?id=41853810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<section id="logo">
<p><img src="https://static.graphite.rs/logos/graphite-logotype-color.svg" alt="Graphite Logo">
</p>
</section>


<div id="tagline">
<h2>Your <span>procedural</span> toolbox for 2D content creation</h2>
<p>Graphite is a free, open source vector and raster graphics engine, available now in alpha. Get creative with a nondestructive editing workflow that combines layer-based compositing with node-based generative design.</p>
</div>










<section id="screenshots" data-carousel="" data-carousel-jostle-hint="">
<p><img src="https://static.graphite.rs/content/index/gui-mockup-nodes__6.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')" alt="" data-carousel-image="">
<img src="https://static.graphite.rs/content/index/gui-demo-painted-dreams.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')" alt="" data-carousel-image="">
<img src="https://static.graphite.rs/content/index/gui-demo-node-graph-valley-of-spires.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')" alt="" data-carousel-image="">
<img src="https://static.graphite.rs/content/index/gui-demo-fractal.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')" alt="" data-carousel-image="">
<img src="https://static.graphite.rs/content/index/gui-mockup-nodes__6.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')" alt="" data-carousel-image="">
<img src="https://static.graphite.rs/content/index/gui-demo-painted-dreams.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')" alt="" data-carousel-image="">
</p>


<div>
<p data-carousel-description="">
<a href="https://editor.graphite.rs/#demo/painted-dreams"><em>Painted Dreams</em></a> — Made using nondestructive boolean operations and procedural dot patterns
</p>
<p data-carousel-description="">
<a href="https://editor.graphite.rs/#demo/valley-of-spires"><em>Valley of Spires</em></a> — The layer stack is represented, under the hood, by a node graph (shown fully expanded)
</p>
<p data-carousel-description="">
Mandelbrot fractal filled with a noise pattern, procedurally generated and infinitely scalable
</p>
<p data-carousel-description="">
Coming soon: mockup for the actively in-development raster workflow with new nodes for photo editing
</p>
</div>
</section>



<div id="overview">
<div>

<hr>

<p>Starting life as a vector editor, Graphite is evolving into a generalized, all-in-one graphics toolbox that's built more like a game engine than a conventional creative app. The editor's tools wrap its node graph core, providing user-friendly workflows for vector, raster, and beyond.</p>
</div>
<div>
<h2 id="one-app-to-rule-them-all">One app to rule them all</h2>
<p>Stop jumping between programs— upcoming tools will make Graphite a first-class content creation suite for many workflows, including:</p>
<div>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Graphic Design</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Image Editing</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Motion Graphics</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Digital Painting</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>VFX Compositing</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Desktop Publishing</span>
</p>
</div>
</div>
<div>
<div>
<h2 id="current-features">Current features</h2>
<div>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Vector editing tools</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Procedural workflow for graphic design</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Node-based layers</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Forever free and open source</span>
</p>
</div>
<p>Presently, Graphite is a lightweight offline web app with features primarily oriented around procedural vector graphics editing.</p>
</div>
<div>
<h2 id="upcoming-features">Upcoming features</h2>
<div>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>All-in-one creative tool for all things 2D</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Fully-featured raster manipulation</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Windows/Mac/Linux native apps + web</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Live collaborative editing</span>
</p>
</div>
<p><a href="https://graphite.rs/features#roadmap">Roadmap</a></p>
</div>
</div>
<div>
<h2 id="desktop-first-and-web-ready">Desktop-first and web-ready</h2>
<p>Graphite is designed principally as a desktop-grade professional application that is also accessible in-browser for fast, casual usage.</p>
<p>Where's the download? Desktop apps for Windows, Mac, and Linux should be available later in 2024. Until then, you can <a href="https://support.google.com/chrome/answer/9658361" target="_blank">install it as a PWA</a>.</p>
<p>Developing and maintaining a native app on four platforms is a big task. To not compromise on the optimal desktop experience—which takes longer to do the right way—priorities called for initially supporting just web, the one platform that stays up-to-date and reaches all devices.</p>
<p>Once it's ready to shine, Graphite's code architecture is structured to deliver native performance for your graphically intensive workloads on desktop platforms and very low overhead on the web thanks to WebAssembly and WebGPU, new high-performance browser technologies.</p>
</div>
</div>



<div id="proceduralism">
<div>

<hr>
<p>Graphite is the first and only graphic design package built for procedural editing — where everything is nondestructive.</p>
</div>
<div>

<p>Save hours on tedious alterations and make better creative choices. Graphite lets you iterate rapidly by adjusting node parameters instead of individual elements.</p>
<p>Scatter circles with just a couple nodes...<br>
Want them denser? Bigger? Those are sliders.<br>
Want a different placement area? Just tweak the path.</p>
<p><a href="https://editor.graphite.rs/#demo/red-dress">Open this artwork</a> and give it a try yourself.</p>
</div>
<div>

<p>Nondestructive editing means every decision is tied to a parameter you can adjust later on. Use Graphite to interpolate between any states just by dragging sliders.</p>
<p>Blend across color schemes. Morph shapes before they're scattered around the canvas. The possibilities are endless.</p>
<p><a href="https://editor.graphite.rs/#demo/changing-seasons">Open this artwork</a> and give it a try yourself.</p>
</div>
<div>
<h2 id="geared-for-generative-pipelines">Geared for generative pipelines</h2>
<p>Graphite's representation of artwork as a node graph lets you customize, compose, reuse, share, and automate your own content workflows:</p>
<div>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Pixelation-free infinite zooming and panning of boundless content</span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Modular node-based pipelines for generative AI <em>(soon)</em></span>
</p>
<p><img src="https://static.graphite.rs/icons/icon-atlas-features__2.png" alt="">
<span>Asset pipelines for studio production environments <em>(soon)</em></span>
</p>
</div>
</div>
</div>



<div id="donate">
<h2 id="support-the-mission">Support the mission</h2>
<p>If you aren't paying for your free software, someone else is covering your share. Chip in so Graphite remains sustainable and independent.</p>
<p><a href="https://github.com/sponsors/GraphiteEditor">Donate</a></p>
</div>



<div id="newsletter">

<p>Subscribe to the newsletter for quarterly updates on major development progress. And follow along—or join the conversation—on social media.</p>
<div>
<h2 id="thanks">Thanks!</h2>
<p>You'll receive your first newsletter email with the next major Graphite news.</p>
</div>

</div>



<div id="dive-in">
<h2 id="ready-to-dive-in">Ready to dive in?</h2>
<p>Get started with Graphite by following along to a hands-on quickstart tutorial.</p>
<div>
<p><img data-video-embed="7gjUhl_3X10" src="https://static.graphite.rs/content/index/tutorial-1-youtube.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')" alt="Graphite Tutorial 1 - Hands-On Quickstart">
</p>
</div>

</div>



<div id="recent-news">

<hr>
<div>
<div>
<p><a href="https://graphite.rs/blog/graphite-progress-report-q3-2024/"><img src="https://static.graphite.rs/content/blog/2024-10-15-graphite-progress-report-q3-2024.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')"></a></p><h2><a href="https://graphite.rs/blog/graphite-progress-report-q3-2024/">Graphite progress report (Q3 2024)</a></h2>
<p>Graphite, a new open source 2D procedural graphics editor, has spent July–September building major improvements to performance, node graph organization, nondestructive path editing, a new render engine, and more helpful nodes, amongst over 100 other features and fixes.
This has been the most productive quarter yet in the project's three-year history. Most of our Google Summer of Code student intern projects have already reached their goals, adding to the goodies included in this progress report. All Q3 2024 commits may be viewed in this list and all noteworthy changes are detailed below.
</p>
<p><a href="https://graphite.rs/blog/graphite-progress-report-q3-2024/">Keep reading</a>
</p></div>
<div>
<p><a href="https://graphite.rs/blog/graphite-progress-report-q2-2024/"><img src="https://static.graphite.rs/content/blog/2024-07-31-graphite-progress-report-q2-2024.avif" onerror="this.onerror = null; this.src = this.src.replace('.avif', '.png')"></a></p><h2><a href="https://graphite.rs/blog/graphite-progress-report-q2-2024/">Graphite progress report (Q2 2024)</a></h2>
<p>Graphite, a new open source 2D procedural graphics editor, has spent April–June introducing boolean path operations, a new gradient picker, layer locking, and more improvements.
Overall, editor functionality has been shaping up and becoming an all around useful tool suite, with notable reductions in rough edges for the vector graphics workflow (our initial focus). Raster and raw photo processing workflows are also now in-development by our Google Summer of Code student interns. Node graph quality-of-life improvements centered around tidy node organization are also the focus of the summer work that's underway. These projects are detailed below.
</p>
<p><a href="https://graphite.rs/blog/graphite-progress-report-q2-2024/">Keep reading</a>
</p></div>
</div>
</div>










</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CapibaraZero: A cheap alternative to FlipperZero based on ESP32-S3 (110 pts)]]></title>
            <link>https://capibarazero.github.io/docs/</link>
            <guid>41852821</guid>
            <pubDate>Tue, 15 Oct 2024 20:34:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://capibarazero.github.io/docs/">https://capibarazero.github.io/docs/</a>, See on <a href="https://news.ycombinator.com/item?id=41852821">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__docusaurus_skipToContent_fallback"><main><div><article><center><img src="https://github.com/CapibaraZero/.github/blob/main/logo.png?raw=true" width="300"><h3>A cheap alternative to FlipperZero™ based on Espressif boards</h3></center></article></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All possible plots by major authors (2020) (143 pts)]]></title>
            <link>https://www.the-fence.com/plots-major-authors/</link>
            <guid>41852009</guid>
            <pubDate>Tue, 15 Oct 2024 19:06:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.the-fence.com/plots-major-authors/">https://www.the-fence.com/plots-major-authors/</a>, See on <a href="https://news.ycombinator.com/item?id=41852009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
                    <p>
                We praise canonical authors for their boundless imagination. Then why do all their plots feel the same?            </p>
                
<p><span>anthony trollope</span><br>
Your happiness is put on hold when it transpires your fiancé failed to correctly cash a cheque. This lasts for 130 chapters. Everyone else is ordained.</p>

<p><span>evelyn waugh</span><br>
The spectre of God’s grace haunts your attempts to hang out with people posher than yourself.</p>

<p><span>henry james</span><br>
You declined an invitation, not wishing, to wit, for it to be understood that you might have deliberately allowed yourself to be put at a disadvantage. Now your ruin is certain.</p>

<p><span>graham greene</span><br>
Your only desire is to preserve an inconceivably small piece of your dignity. You are denied this, because of a prior engagement with the British Foreign Office. This novel is called <em>The Way of the Thing</em>.</p>

<p><span>w shakespeare (i)</span><br>
You become King. This turns out to have been a very big mistake.</p>

<p><span>samuel richardson</span><br>
‘Dearest Mama, An alluring yet unvirtuous <em>rake</em> has designs on my Innocence and has placed me in a sack to be <em>transported</em> to Covent Garden. Fortunately the sack is so designed as to allow the writing of several letters.’</p>

<p><span>david foster wallace</span><br>
You suffer from an unbearably acute and inexpressible form of psychic pain and anxiety, the very inexpressibility of which is in and of itself a significant constituent part of this pain. Your name is Quagmire Gesamtkunstwerk.</p>

<p><span>marcel proust</span><br>
The only straight person in Paris has a series of very long lunches.</p>

<p><span>mrs gaskell</span><br>
You are forced to move to Manchester, which is unfortunately full of Mancunians. You have great sympathy for the sufferings of the working poor, but this does not stop you marrying a mill owner.</p>

<p><span>ian mcewan</span><br>
Here is an artist, doing artsy things. Here is a scientist, doing sciencey things. This presents a dichotomy that can only be resolved with a calculated cinematographic finale.</p>

<p><span>e m forster</span><br>
Things were bad, so you left England. Now things are muddled, and only a straight-talking Aunt can save you.</p>

<p><span>cormac mccarthy</span><br>
Nothing can ever be right again. Here’s a horse.</p>

<p><span>p g wodehouse</span><br>
You and all your acquaintances have names better suited to clowns, dogs, or sickening sexual euphemisms. An accidental engagement is contrived to prevent the theft of your aunt’s chef.</p>

<p><span>alan bennett</span><br>
The declining institution on stage represents Britain, because <em>Britain is a declining institution</em>.</p>

<p><span>jane austen</span><br>
Your obligation to make a judicious alliance with an alluring newcomer is constantly pressed upon you by your relations. You despise them all.</p>
<p><img decoding="async" alt="" width="300" height="220" data-img-id="737" data-lazy-function="cropImage" data-src="https://standfirst-thefence-production.imgix.net/uploads/2023/05/authors.png?fit=crop&amp;crop=faces&amp;q=80&amp;auto=compress,format&amp;w=300&amp;h=220" src="https://standfirst-thefence-production.imgix.net/uploads/2023/05/authors.png?fit=crop&amp;crop=faces&amp;q=80&amp;auto=compress,format&amp;w=300&amp;h=220"></p>
<p><span>dan brown</span><br>
Award-winning author Dan Brown has written a complicated role for you with his expensive pen. You are a humanities professor at an Ivy League university, but also, somehow, in mortal peril. Your love interest is picturesque but ill-mannered and French. This is somehow worth several million dollars.</p>

<p><span>agatha christie</span><br>
The extravagant web of lies that is your home life is shattered by twin irritations: a spate of deaths and a Belgian asking irritating questions.</p>

<p><span>zadie smith</span><br>
Shameless recycling of modernist plots, because the novelist is too good at non-fiction to be called out for it.</p>

<p><span>w shakespeare (ii)</span><br>
Very much like the reality <span>tv</span> programme <em>Wife Swap</em>, but set in Italy.</p>

<p><span>iris murdoch</span><br>
An Oxford/ Hampstead sex farce is most profitably understood as a response to Sartre, more’s the pity.</p>

<p><span>ernest hemingway</span><br>
On the journey you drink beer from cold bottles, and peasant’s wine from the big leather sacks the fisherman gave you. When you arrive in the town square, you stop by a café for a bottle of champagne and a bottle of cheap wine. You hate the man you are with. You order more beer. Soon it will be time for lunch.</p>

<p><span>john banville</span><br>
An elderly art historian, lodged in a manor house in rural Ireland, compares the sky to Poussin paintings. For 250 pages.</p>

<p><span>harold pinter</span><br>
You know what Dad.<br>
<em>(pause)</em><br>
I’ve always known you to be a cunt.</p>

<p><span>f scott fitzgerald</span><br>
Ginevra Beauregard and Redmond Ingram (known as Red at Princeton) are honeymooning in the South of France. They are beautiful, clever and rich. For reasons never fully explored, they have resolved to make themselves unhappy.</p>

<p><span>tennessee williams</span><br>
<em>(An angry male protagonist wipes sweat from brow)</em> ‘Someone fix me a goddamn drink. From the icebox, Scarlett, from the goddamn icebox!’</p>

<p><span>oscar wilde</span><br>
To write a comedy of manners once may be regarded as a misfortune; to write several of them begins to look like carelessness.</p>

<p><span>d h lawrence</span><br>
You look upon his bright loins of darkness as they gleam in the Midlands sun. ‘Before we have sex,’ he says, ‘here are several of my opinions.’</p>

<p><span>thomas hardy</span><br>
Lies, lies, misery, lies, suicide, rape, and corn prices.</p>

<p><span>virginia woolf</span><br>
We follow the fortunes of an upper-middle class family as they spend 40 years engaged in artistic hobbies whose sole purpose is to distract them from the existence of a) the actuality of sex and b) the First World War.</p>

<p><span>tom stoppard</span><br>
You are trapped within a play within a play within a play within a GCSE drama syllabus.</p>
<!-- SF Hadrian Popup sf-popup-155 -->

<!-- End Hadrian Popup  -->
<!-- SF Hadrian Popup sf-popup-153 -->

<!-- End Hadrian Popup  -->

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta's open AI hardware vision (191 pts)]]></title>
            <link>https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/</link>
            <guid>41851304</guid>
            <pubDate>Tue, 15 Oct 2024 18:08:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/">https://engineering.fb.com/2024/10/15/data-infrastructure/metas-open-ai-hardware-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=41851304">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>At the Open Compute Project (OCP) Global Summit 2024, we’re showcasing our latest open AI hardware designs with the OCP community.</span></li>
<li aria-level="1"><span>These innovations include a new AI platform, cutting-edge open rack designs, and advanced network fabrics and components.&nbsp;</span></li>
<li aria-level="1"><span>By sharing our designs, we hope to inspire collaboration and foster innovation. If you’re passionate about building the future of AI, we invite you to engage with us and OCP to help shape the next generation of open hardware for AI.</span></li>
</ul>
<p><span>AI has been at the core of the experiences Meta has been delivering to people and businesses for years, including AI modeling innovations to optimize and improve on features like </span><a href="https://ai.meta.com/blog/facebook-feed-improvements-ai-show-more-less/" target="_blank" rel="noopener"><span>Feed</span></a><span> and our </span><a href="https://engineering.fb.com/2024/07/10/data-infrastructure/machine-learning-ml-prediction-robustness-meta/" target="_blank" rel="noopener"><span>ads system</span></a><span>. As we develop and release new, advanced AI models, we are also driven to advance our infrastructure to support our new and emerging AI workloads.</span></p>
<p><span>For example, </span><a href="https://ai.meta.com/blog/meta-llama-3-1/" target="_blank" rel="noopener"><span>Llama 3.1 405B</span></a><span>, Meta’s largest model, is a dense transformer with 405B parameters and a context window of up to 128k tokens. To train a large language model (LLM) of this magnitude, with over 15 trillion tokens, we had to make substantial optimizations to our entire training stack. This effort pushed our infrastructure to operate across more than 16,000 NVIDIA H100 GPUs, making Llama 3.1 405B the first model in the Llama series to be trained at such a massive scale.&nbsp;</span></p>
<p><span>Prior to Llama, our largest AI jobs ran on 128 NVIDIA A100 GPUs. ​But things have rapidly accelerated. ​Over the course of 2023, we rapidly scaled up our training clusters from 1K, 2K, 4K, to eventually 16K GPUs to support our AI workloads. Today, we’re training our models on two</span> <a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/" target="_blank" rel="noopener"><span>24K-GPU clusters</span></a><span>.</span></p>
<p><span>We don’t expect this upward trajectory for AI clusters to slow down any time soon. In fact, we expect the amount of compute needed for AI training will grow significantly from where we are today.</span></p>
<p><span>Building AI clusters requires more than just GPUs. Networking and bandwidth play an important role in ensuring the clusters’ performance. Our systems consist of a tightly integrated HPC compute system and an isolated high-bandwidth compute network that connects all our GPUs and domain-specific accelerators. This design is necessary to meet our injection needs and address the challenges posed by our need for bisection bandwidth.</span></p>
<p><span>In the next few years, we anticipate greater injection bandwidth on the order of a terabyte per second, per accelerator, with equal normalized bisection bandwidth. This represents a growth of more than an order of magnitude compared to today’s networks!</span></p>
<p><span>To support this growth, we need a high-performance, multi-tier, non-blocking network fabric that can utilize modern congestion control to behave predictably under heavy load. This will enable us to fully leverage the power of our AI clusters and ensure they continue to perform optimally as we push the boundaries of what is possible with AI.</span></p>
<p><span>Scaling AI at this speed requires open hardware solutions. Developing new architectures, network fabrics, and system designs is the most efficient and impactful when we can build it on principles of openness. By investing in open hardware, we unlock AI’s full potential and propel ongoing innovation in the field.</span></p>
<h2><span>Introducing Catalina: Open Architecture for AI Infra</span></h2>
<figure id="attachment_21841" aria-describedby="caption-attachment-21841"><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?w=683" alt="" width="456" height="683" srcset="https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png 720w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=611,916 611w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=683,1024 683w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=96,144 96w, https://engineering.fb.com/wp-content/uploads/2050/05/Catalina-Front-Back-2.png?resize=192,288 192w" sizes="(max-width: 992px) 100vw, 62vw"><figcaption id="caption-attachment-21841">Catalina front view (left) and rear view (right).</figcaption></figure>
<p><span>Today, we announced the upcoming release of Catalina, our new high-powered rack designed for AI workloads, to the OCP community. Catalina is based on the <a href="https://nvidianews.nvidia.com/news/nvidia-contributes-blackwell-platform-design-to-open-hardware-ecosystem-accelerating-ai-infrastructure-innovation" target="_blank" rel="noopener">NVIDIA Blackwell platform full rack-scale solution</a>, with a focus on modularity and flexibility. It is built to support the latest NVIDIA GB200 Grace Blackwell Superchip, ensuring it meets the growing demands of modern AI infrastructure.&nbsp;</span></p>
<p><span>The growing power demands of GPUs means open rack solutions need to support higher power capability. With Catalina we’re introducing the Orv3, a high-power rack (HPR) capable of supporting up to 140kW.</span></p>
<p><span>The full solution is liquid cooled and consists of a power shelf that supports a compute tray, switch tray, the Orv3 HPR, the</span> <a href="https://engineering.fb.com/2021/11/09/data-center-engineering/ocp-summit-2021/" target="_blank" rel="noopener"><span>Wedge 400</span></a><span> fabric switch, a management switch, battery backup unit, and a rack management controller.</span></p>
<p><span>We aim for Catalina’s modular design to empower others to customize the rack to meet their specific AI workloads while leveraging both existing and emerging industry standards.</span></p>
<h2><span>The Grand Teton Platform now supports AMD accelerators</span></h2>
<p><img decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?w=916" alt="" width="600" height="436" srcset="https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png 1109w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=916,665 916w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=768,557 768w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=1024,743 1024w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=96,70 96w, https://engineering.fb.com/wp-content/uploads/2024/10/Grand-Teton-AMD-MI300X-Open-small.png?resize=192,139 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>In 2022, we announced </span><a href="https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/" target="_blank" rel="noopener"><span>Grand Teton</span></a><span>, our next-generation AI platform (the follow-up to our Zion-EX platform). Grand Teton is designed with compute capacity to support the demands of memory-bandwidth-bound workloads, such as Meta’s <a href="https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/" target="_blank" rel="noopener">deep learning recommendation models (</a></span><span>DLRMs</span><span>), as well as compute-bound workloads like content understanding.</span></p>
<p><span>Now, we have expanded the Grand Teton platform to support the AMD Instinct MI300X and will be contributing this new version to OCP. Like its predecessors, this new version of Grand Teton</span><span>&nbsp;features a single monolithic system design with fully integrated power, control, compute, and fabric interfaces. This high level of integration simplifies system deployment, enabling rapid scaling with increased reliability for large-scale AI inference workloads.</span></p>
<p><span>In addition to supporting a range of accelerator designs, now including the AMD Instinct MI300x, Grand Teton offers significantly greater compute capacity, allowing faster convergence on a larger set of weights. This is complemented by expanded memory to store and run larger models locally, along with increased network bandwidth to scale up training cluster sizes efficiently.</span></p>
<h2><span>​Open Disaggregated Scheduled Fabric&nbsp;</span></h2>
<p><img loading="lazy" decoding="async" src="https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?w=1024" alt="" width="1024" height="508" srcset="https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png 1871w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=916,454 916w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=768,381 768w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=1024,508 1024w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=1536,762 1536w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=96,48 96w, https://engineering.fb.com/wp-content/uploads/2024/10/OCP-2024-DSF-Meta.png?resize=192,95 192w" sizes="(max-width: 992px) 100vw, 62vw"></p>
<p><span>Developing open, vendor-agnostic networking backend is going to play an important role going forward as we continue to push the performance of our AI training clusters. Disaggregating our network allows us to work with vendors from across the industry to design systems that are innovative as well as scalable, flexible, and efficient.</span></p>
<p><span>Our new <a href="https://engineering.fb.com/2024/10/15/data-infrastructure/open-future-networking-hardware-ai-ocp-2024-meta/" target="_blank" rel="noopener">Disaggregated Scheduled Fabric (DSF)</a> for our next-generation AI clusters offers several advantages over our existing switches. By opening up our network fabric we can overcome limitations in scale, component supply options, and power density. DSF is powered by the open</span><a href="https://github.com/opencomputeproject/SAI" target="_blank" rel="noopener"><span> OCP-SAI</span></a><span> standard and</span><a href="https://engineering.fb.com/2018/09/04/data-infrastructure/research-in-brief-building-switch-software-at-scale-and-in-the-open/" target="_blank" rel="noopener"><span> FBOSS</span></a><span>, Meta’s own network operating system for controlling network switches. It also supports an open and standard Ethernet-based RoCE interface to endpoints and accelerators across several GPUS and NICS from several different vendors, including our partners at NVIDIA, Broadcom, and AMD.</span></p>
<p><span>In addition to DSF, we have also developed and built new 51T fabric switches based on Broadcom and Cisco ASICs. Finally, we are sharing our new FBNIC, a new NIC module that contains our first Meta-design network ASIC. In order to meet the growing needs of our AI&nbsp;</span></p>
<h2><span>Meta and Microsoft: Driving Open Innovation Together</span></h2>
<p><span>Meta and Microsoft have a long-standing partnership within OCP, beginning with the development of the </span><a href="https://www.opencompute.org/documents/switch-abstraction-interface-ocp-specification-v0-2-pdf" target="_blank" rel="noopener"><span>Switch Abstraction Interface (SAI)</span></a><span> for data centers in 2018. Over the years together, we’ve contributed to key initiatives such as the </span><a href="https://www.opencompute.org/blog/new-open-accelerator-infrastructure-oai-sub-project-to-launch-within-the-ocp-server-project" target="_blank" rel="noopener"><span>Open Accelerator Module (OAM)</span></a><span> standard and SSD standardization, showcasing our shared commitment to advancing open innovation.</span></p>
<p><span>Our current </span><a href="https://azure.microsoft.com/en-us/blog/accelerating-industry-wide-innovations-in-datacenter-infrastructure-and-security/" target="_blank" rel="noopener"><span>collaboration focuses on Mount Diablo</span></a><span>, a new disaggregated power rack. It’s a cutting-edge solution featuring a scalable 400 VDC unit that enhances efficiency and scalability. This innovative design allows more AI accelerators per IT rack, significantly advancing AI infrastructure. We’re excited to continue our collaboration through this contribution.</span></p>
<h2><span>The open future of AI infra</span></h2>
<p><a href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/" target="_blank" rel="noopener"><span>Meta is committed to open source AI</span></a><span>. We believe that open source will put the benefits and opportunities of AI into the hands of people all over the word.&nbsp;</span></p>
<p><span>AI won’t realize its full potential without collaboration. We need open software frameworks to drive model innovation, ensure portability, and promote transparency in AI development. We must also prioritize open and standardized models so we can leverage collective expertise, make AI more accessible, and work towards minimizing biases in our systems.​</span></p>
<p><span>Just as important, we also need open AI hardware systems. These systems are necessary for delivering the kind of high-performance, cost-effective, and adaptable infrastructure necessary for AI advancement.​</span></p>
<p><span>We encourage anyone who wants to help advance the future of AI hardware systems to engage with the OCP community. By addressing AI’s infrastructure needs together, we can unlock the true promise of open AI for everyone.​</span></p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel and AMD form advisory group to reshape x86 ISA (113 pts)]]></title>
            <link>https://www.theregister.com/2024/10/15/intel_amd_x86_future/</link>
            <guid>41851104</guid>
            <pubDate>Tue, 15 Oct 2024 17:50:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/10/15/intel_amd_x86_future/">https://www.theregister.com/2024/10/15/intel_amd_x86_future/</a>, See on <a href="https://news.ycombinator.com/item?id=41851104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>The shape of the x86 instruction set architecture (ISA) is evolving. On Tuesday, Intel and AMD announced the formation of an ecosystem advisory group intended to drive greater consistency between the brands' x86 implementations.</p>
<p>Intel and AMD have been co-developing the x86-64 instruction for decades. But while end user workloads have enjoyed cross-compatibility between the two chipmakers' products, this has been far from universal.</p>
<p>"x86 is the de facto standard. It's a strong ecosystem, but it's one that really Intel and AMD have co-developed in a way, but an arm's length, and you know, that has caused some inefficiencies and some drift in portions of the ISA over time." AMD EVP of datacenter solutions Forrest Norrod said during a press briefing ahead of the announcement.</p>

    

<p>The introduction of advanced vector extensions (AVX) is the most obvious example of where compatibility across Intel and AMD platforms hasn't always been guaranteed.</p>

        


        

<p>For many years, those who wanted to take advantage of fat 512-bit vector registers have been limited to Intel platforms. In fact, AMD lacked support for AVX-512 until the <a target="_blank" href="https://www.theregister.com/2022/11/10/amd_96core_epyc/">launch</a> of Zen 4 in 2022, and even then it only supported it by double pumping a 256-bit data path. It wasn't until this year's Zen 5 <a target="_blank" href="https://www.theregister.com/2024/07/15/amd_spills_the_beans_on/">launch</a> the House of Zen added support for a full 512-bit data path.</p>
<p>Going forward, Intel, AMD, and their industry partners aim to avoid this kind of inconsistency by converging around a more uniform implementation. To support this goal, the duo has solicited the help of Broadcom, Dell, Google, HPE, HP, Lenovo, Meta, Microsoft, Oracle, Red Hat, as well as individuals, including Linux kernel-dev Linus Torvalds and Epic's Tim Sweeney.</p>

        

<p>This advisory group will be tasked with reshaping the x86 ISA to improve cross-compatibility, simplify software development, and address changing demands around emerging technologies.</p>
<p>"We'll have, not only will we have the benefits of performance, flexibility and compatibility across hardware, we'll have it across software, operating systems and a variety of services," Intel EVP of datacenter and AI group Justin Hotard told us.</p>
<p>"I think this will actually enable greater choice in the fundamental products, but reduce the friction of being able to choose from those choices," echoed Norrod.</p>

        

<p>However, it'll be some time before we see the group's influence realized in products. Norrod emphasized that silicon development can take months if not years. As such it's "not something that's going to reflect into products, I don't believe, in the next year or so."</p>
<p>For end users, the benefits are numerous as in theory taking advantage of either Intel or AMD's products will require less specialization, something we're sure the hyperscalers will appreciate.</p>
<p>For the long-time rivals, however, the change could have major implications for the future development of the architecture. While the two chipmakers have caught up with each other on vector extensions, Intel still has its advanced matrix extensions (AMX) for CPU-based AI inference acceleration.</p>
<p>It remains to be seen whether these extensions will be phased out or if some version of them will eventually make their way into AMD's Epyc and Ryzen processors. We have no doubt that either team's SoC designers would relish the opportunity to reclaim all that die area currently consumed by the NPU.</p>
<p>"I don't think we want to commit to 'we're going to support this or not support this' in a time frame. But I think the intent is we want to support things consistently," Hotard said.</p>
<p>While Norrod and Hotard declined to comment on specific changes coming to x86, recent developments, particularly on Intel's side, give us some idea of the ISA's trajectory.</p>
<p>In June, Intel published an update to its proposed <a target="_blank" rel="nofollow" href="https://www.intel.com/content/www/us/en/developer/articles/technical/envisioning-future-simplified-architecture.html">x86S spec</a>, a stripped down version of the ISA free of legacy bloat — most notably 32-bit and 16-bit execution modes. As we understand it, 32-bit code would still be able to run, albeit in a compatibility mode.</p>
<p>There's also the AVX10 spec that we <a target="_blank" href="https://www.theregister.com/2023/08/15/avx10_intel_interviews/">looked at</a> last year, which made many of AVX512's more attractive functions. Under the new spec, AVX10 compatible chips will, for the most part, share a common feature set — including 32 registers, k-masks, and FP16 support — and minimally support 256 bit wide registers.</p>
<p>AVX10 is important for Intel which has transitioned to a dual-stack Xeon roadmap with P-and E-core CPUs, like Granite Rapids and Sierra Forest, the latter of which lacks support for AVX512.</p>
<ul>

<li><a href="https://www.theregister.com/2024/10/10/amd_epyc_turin/">AMD pumps Epyc core count to 192, clocks up to 5 GHz with Turin debut</a></li>

<li><a href="https://www.theregister.com/2024/10/15/samsung_hb3me/">Samsung's HBM3E has been a disaster, but there's a path back</a></li>

<li><a href="https://www.theregister.com/2024/09/24/intel_xeon_6p/">With Granite Rapids, Intel is back to trading blows with AMD</a></li>

<li><a href="https://www.theregister.com/2024/09/19/sifive_ai_accelerator/">SiFive expands from RISC-V cores for AI chips to designing its own full-fat accelerator</a></li>
</ul>
<p>AMD's dense Zen C-cores don't suffer from this limitation, but can be switched to a double pumped 256-bit data path to achieve lower power AVX512 support. Whether Intel will push ahead with AVX10 or borrow AMD's implementation under the newly formed advisory group is another unknown, but given enough time, we can expect the two chipmakers to coalesce around a common implementation whether it be AVX, AMX or something else.</p>
<p>That's assuming, of course, that Intel and AMD can agree on how to address industry needs.</p>
<p>With that said, a more consistent ISA could help stave off the growing number of Arm-compatible CPUs finding homes in cloud datacenters. While the exact cores used by these chips may differ — most use Arm's Neoverse cores, but some, like Ampere have developed their own — most are using either the older ARMv8 or ARMv9 ISAs, ensuring that with few exceptions code developed on one should run without issue on the other. ®</p>                                


                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sqlite3 WebAssembly (392 pts)]]></title>
            <link>https://sqlite.org/wasm/doc/trunk/index.md</link>
            <guid>41851051</guid>
            <pubDate>Tue, 15 Oct 2024 17:45:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sqlite.org/wasm/doc/trunk/index.md">https://sqlite.org/wasm/doc/trunk/index.md</a>, See on <a href="https://news.ycombinator.com/item?id=41851051">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>This site is home to the documentation for the SQLite project's
WebAssembly- and JavaScript-related APIs, which enable the use of
<a href="https://sqlite.org/">sqlite3</a> in modern WASM-capable browsers. These components
were initially released for public beta with version 3.40 and will
tentatively be made API-stable with the 3.41 release, pending
community feedback.</p>

<p>Disclaimer: this site requires a modern, JavaScript-capable browser
for full functionality. This site uses client-side storage for storing
certain browsing preferences (like the bright/dark mode toggle) but
does not store any user information server-side, except for logged-in
developers. The only user-level details this site shares with any
other systems are the public SCM-related details of this site's own
developers.</p>

<h2>Site Overview</h2>
<!-- https://unicode.org/emoji/charts/full-emoji-list.html -->

<p><a href="https://sqlite.org/wasm/doc/trunk/about.md"><strong>About</strong> the sqlite3 WASM subproject</a>:</p>

<ul>
<li>📰 <a href="https://sqlite.org/wasm/doc/trunk/news.md"><strong>Project news</strong></a></li>
<li>💬 <a href="https://sqlite.org/wasm/doc/trunk/faq.md"><strong>Frequently Asked Questions</strong></a></li>
<li>🚧 <a href="https://sqlite.org/wasm/doc/trunk/todo.md"><strong>TODOs</strong></a> and (un)planned features</li>
<li>☎ <strong>Technical support</strong> is provided, and feedback gladly accepted,
via <a href="https://sqlite.org/forum/"><strong>the sqlite forum</strong></a>. Those with <a href="https://sqlite.org/prosupport.html">commercial SQLite
support contracts</a> may use their usual
support channels.</li>
</ul>

<p><strong>Making use of this project:</strong></p>

<ul>
<li><p>👣 The <strong><a href="https://sqlite.org/wasm/doc/trunk/demo-123.md">three-step HOWTO</a></strong> demonstrates how to
include and run the sqlite3 WASM module and its associated
JavaScript APIs.</p></li>
<li><p>💾 <strong><a href="https://sqlite.org/download.html">Downloads</a></strong> are available via
the main project downloads page.</p>

<ul>
<li>📸 <strong><a href="https://sqlite.org/wasm/uv/snapshot.html">Prerelease snapshots</a></strong> are
updated from time to time</li>
<li>📦 <strong><a href="https://sqlite.org/wasm/doc/trunk/npm.md">npm module</a></strong></li>
</ul></li>
<li><p>🛠 <a href="https://sqlite.org/wasm/doc/trunk/building.md"><strong>Building sqlite3 WASM</strong></a> and its associated JS
code.</p>

<ul>
<li><a href="https://sqlite.org/wasm/doc/trunk/emscripten.md">Emscripten</a> build specifics</li>
</ul></li>
<li><p>📇 <a href="https://sqlite.org/wasm/doc/trunk/api-index.md">The <strong>API index</strong></a> describes the various API
variants and how to load and access them.</p>

<ul>
<li>🧑‍🍳 <a href="https://sqlite.org/wasm/doc/trunk/cookbook.md"><strong>Cookbook</strong></a> of recipes for client-level code</li>
<li>💾 <a href="https://sqlite.org/wasm/doc/trunk/persistence.md"><strong>Persistent storage</strong> options</a></li>
<li>🤓 Using <a href="https://sqlite.org/wasm/doc/trunk/c-structs.md"><strong>C-structs</strong></a> in the JS API</li>
<li>🧱 Creating <a href="https://sqlite.org/wasm/doc/trunk/vtab.md"><strong>virtual tables</strong></a> and table-valued
functions in JS</li>
<li>💣 <a href="https://sqlite.org/wasm/doc/trunk/api-changes.md"><strong>API changes</strong></a> which <em>might</em> affect
clients</li>
<li>🔭 The <a href="https://sqlite.org/wasm/doc/trunk/module-symbols.html"><strong>module symbols app</strong></a>
generates lists of sqlite3 API symbols from the JavaScript/WASM module.</li>
</ul></li>
</ul>

<p><strong>About this documentation:</strong></p>

<ul>
<li>📜 <a href="https://sqlite.org/wasm/doc/trunk/license.md"><strong>License</strong></a></li>
<li>🚧 <a href="https://sqlite.org/wasm/doc/trunk/doc-maintenance.md"><strong>Doc and repository maintenance</strong></a></li>
</ul>



<h2>In the Wild</h2>
<p>Third-party projects known to be using this project include (in order
of their addition to this list)...</p>

<ul>
<li><a href="https://github.com/nalgeon/sqlime">SQLime</a> provides a database
browser interface.</li>
<li><a href="https://github.com/evoluhq/evolu">Evolu</a> Evolu is a local-first platform
designed for privacy, ease of use, and no vendor lock-in.</li>
<li><a href="https://github.com/mandolyte/sqlitenext">SQLiteNext</a> provides a demo of
integrating this project with next.js.</li>
<li><a href="https://github.com/overtone-app/sqlite-wasm-esm">sqlite-wasm-esm</a> demonstrates
how to use this project with the Vite toolchain.</li>
<li><a href="https://www.npmjs.com/package/sqlite-wasm-http">sqlite-wasm-http</a>
provides an SQLite VFS with read-only access to databases which are
served directly over HTTP.</li>
</ul>



<h2>Related Works</h2>
<p>(In the order in which we became aware of them...)</p>

<ul>
<li>Alon Zakai's <a href="https://github.com/sql-js/sql.js"><strong>sql.js</strong></a> is the
first known direct usage of sqlite3 in a web browser, <a href="https://github.com/kripken/sql.js/commit/cebd80648dbd369b34804c5a00b4d0bddc1cbf05">dating back to
2012</a>,
not counting WebSQL (which was a native-level feature and has long
since been removed from most browsers).</li>
<li>Roy Hashimoto's
<a href="https://github.com/rhashimoto/wa-sqlite"><strong>wa-sqlite</strong></a> is home to
the first known implementation of <a href="https://sqlite.org/wasm/doc/trunk/persistence.md">OPFS storage</a>
of sqlite3 databases.</li>
<li>James Long's
<a href="https://github.com/jlongster/absurd-sql"><strong>absurd-js</strong></a>
demonstrates storing sqlite3 databases inside IndexedDB databases.</li>
<li><a href="https://supabase.com/blog/postgres-wasm"><strong>postgres-wasm</strong></a> runs a
<a href="https://www.postgresql.org/">Postgres database server</a> in a browser.</li>
<li><a href="https://fossil.wanderinghorse.net/r/jaccwabyt"><strong>Jaccwabyt</strong></a> is a
small JS library for manipulating WASM-hosted C structs via JS code,
created specifically to support the <a href="https://sqlite.org/wasm/doc/trunk/persistence.md#vfs-opfs">OPFS
sqlite3_vfs</a> implementation in this
project. This project embeds a copy but does not expose it to client
applications.</li>
<li><a href="https://github.com/sagemathinc/cowasm"><strong>CoWasm</strong></a> is
"Collaborative WebAssembly for Servers and Browsers". <a href="https://cowasm.sh/">Their demo
app</a> includes a WASM build of the sqlite3 shell
application.</li>
<li><a href="https://sqlite.org/forum/forumpost/ca6139791c"><strong>Evan Brass's
build</strong></a> uses a <a href="https://github.com/WebAssembly/wasi-sdk">WASI
SDK</a> build, instead of Emscripten, and demonstrates some
novel features which this project's WASM build does not.</li>
</ul>

<h2>Third-party Documentation and Articles</h2>
<p>The following links reference articles and documentation published
about SQLite WASM by third parties:</p>

<!-- , listed in order of their addition here: -->

<ul>
<li><a href="https://developer.chrome.com/blog/from-web-sql-to-sqlite-wasm/">Porting WebSQL to OPFS</a>,
by the Google Chrome dev team</li>
<li>The VMWare OCTO team writes about <a href="https://wasmlabs.dev/articles/sqlite-wasi-support/">building SQLite3 for
WASI</a></li>
</ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The C23 edition of Modern C is now available for free (326 pts)]]></title>
            <link>https://gustedt.wordpress.com/2024/10/15/the-c23-edition-of-modern-c/</link>
            <guid>41850017</guid>
            <pubDate>Tue, 15 Oct 2024 16:06:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gustedt.wordpress.com/2024/10/15/the-c23-edition-of-modern-c/">https://gustedt.wordpress.com/2024/10/15/the-c23-edition-of-modern-c/</a>, See on <a href="https://news.ycombinator.com/item?id=41850017">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<p><a href="#content">
			Skip to content		</a></p><!-- .site-header -->

		<div id="content">
	<main id="main">
		
<article id="post-4215">
	<!-- .entry-header -->

	
	
	<div>
		
<p>The C23 edition of Modern C is now available for free download from</p>



<p><a href="https://hal.inria.fr/hal-02383654">https://hal.inria.fr/hal-02383654</a></p>



<div><p>This new edition has been the occasion to overhaul the presentation in many places, but its main purpose is the update to the new C standard, <a href="https://www.iso.org/standard/82075.html">C23</a>. The goal was to publish this new edition of Modern C at the same time as the new C standard goes through the procedure of ISO publication. The closest approximation of the contents of the new standard in a publically available document can be found <a href="https://www.open-std.org/JTC1/SC22/WG14/www/docs/n3220.pdf">here</a>. New releases of major compilers already implement most of the new features that it brings.</p>
<p>Among the most noticeable changes and additions that we handle are those for integers: there are new bit-precise types coined <code>_BitInt(N)</code>, new C library headers <code>(for arithmetic with overflow check) and</code> (for bit manipulation), possibilities for 128 bit types on modern architectures, and substantial improvements for enumeration types. Other new concepts in C23 include a <code>nullptr</code> constant and its underlying type, syntactic annotation with attributes, more tools for type generic programming such as type inference with <code>auto</code> and <code>typeof</code>, default initialization with <code>{}</code>, even for variable length arrays, and <code>constexpr</code> for named constants of any type. Furthermore, new material has been added, discussing compound expressions and lambdas, so-called “internationalization”, a comprehensive approach for program failure.</p>
<p>Also added has been an appendix and a temporary include header for an easy transition to C23 on existing platforms, that will allow you to start off with C23 right away.</p>
<p>Manning’s early access program (MEAP) for the new edition is still open at</p>
<p><a href="https://www.manning.com/books/modern-c-third-edition" rel="nofollow">https://www.manning.com/books/modern-c-third-edition</a></p>
<p>Unfortunately they were not yet able to tell me when their version of the C23 edition will finally be published.</p>
</div>





			
				</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article><!-- #post-4215 -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
	</main><!-- .site-main -->

	
</div><!-- .site-content -->

		<!-- .site-footer -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple introduces iPad mini built for Apple Intelligence (184 pts)]]></title>
            <link>https://www.apple.com/newsroom/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/</link>
            <guid>41849058</guid>
            <pubDate>Tue, 15 Oct 2024 14:35:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/">https://www.apple.com/newsroom/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/</a>, See on <a href="https://news.ycombinator.com/item?id=41849058">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        <div>
                    
                    
                        <span>PRESS RELEASE</span>
                    
                    
                        <span>October 15, 2024</span>
                    
                    
                </div>

        <div>
                
                
                
                    <h2>
                        
    
        Apple introduces powerful new iPad&nbsp;mini built for Apple&nbsp;Intelligence
    

                    </h2>
                
            </div>

        <div>
                
                
                    The ultraportable iPad mini is more capable and versatile than ever with the powerful A17 Pro chip and support for Apple&nbsp;Pencil&nbsp;Pro
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, A hand holds up the new iPad mini.">
        <div>
             
              
              <div>
                The new iPad mini delivers powerful performance, incredible value, and the full iPad experience in an ultraportable design.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-hero-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-hero-241015_big" aria-label="Download media, A hand holds up the new iPad mini."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><strong><span>CUPERTINO, CALIFORNIA</span> </strong>Apple today introduced the new <a href="https://www.apple.com/ipad-mini/" target="_blank">iPad mini</a>, supercharged by the A17 Pro chip and Apple Intelligence, the easy-to-use personal intelligence system that understands personal context to deliver intelligence that is helpful and relevant while protecting user privacy. With a beloved ultraportable design, the new iPad mini is available in four gorgeous finishes, including a new blue and purple, and features the brilliant 8.3-inch Liquid Retina display. A17 Pro delivers a huge performance boost for even the most demanding tasks, with a faster CPU and GPU, a 2x faster Neural Engine than the previous-generation iPad mini,<sup>1</sup> and support for Apple Intelligence. The versatility and advanced capabilities of the new iPad mini are taken to a whole new level with support for Apple Pencil Pro, opening up entirely new ways to be even more productive and creative. The 12MP wide back camera supports Smart HDR 4 for natural-looking photos with increased dynamic range, and uses machine learning to detect and scan documents right in the Camera app.
</div>
                 
             
                 <div>The new iPad mini features all-day battery life and brand-new experiences with iPadOS 18. Starting at just $499 with 128GB — double the storage of the previous generation — the new iPad mini delivers incredible value and the full iPad experience in an ultraportable design. Customers can pre-order the new iPad mini today, with availability beginning Wednesday, October 23.
</div>
                 
             
                 <div>“There is no other device in the world like iPad mini, beloved for its combination of powerful performance and versatility in our most ultraportable design. iPad mini appeals to a wide range of users and has been built for Apple Intelligence, delivering intelligent new features that are powerful, personal, and private,” said Bob Borchers, Apple’s vice president of Worldwide Product Marketing. “With the powerful A17 Pro chip, faster connectivity, and support for Apple Pencil Pro, the new iPad mini delivers the full iPad experience in our most portable design at an incredible value.”
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="ipad-mini-color-lineup">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-dda6f49a0116ad3180b87abe06bdc92c" href="#gallery-dda6f49a0116ad3180b87abe06bdc92c" data-ac-gallery-trigger="gallery-dda6f49a0116ad3180b87abe06bdc92c"><span>The new iPad mini is shown in the new blue color.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-e62275c2c4c5d7f444000d658260ec13" href="#gallery-e62275c2c4c5d7f444000d658260ec13" data-ac-gallery-trigger="gallery-e62275c2c4c5d7f444000d658260ec13"><span>The new iPad mini is shown in the new purple color.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-6cbaea15325c06057bb774c55151379a" href="#gallery-6cbaea15325c06057bb774c55151379a" data-ac-gallery-trigger="gallery-6cbaea15325c06057bb774c55151379a"><span>The new iPad mini is shown in starlight.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-63a6d5c11433b9d5e71fec71d0db9fbf" href="#gallery-63a6d5c11433b9d5e71fec71d0db9fbf" data-ac-gallery-trigger="gallery-63a6d5c11433b9d5e71fec71d0db9fbf"><span>The new iPad mini is shown in space gray.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-dda6f49a0116ad3180b87abe06bdc92c" aria-labelledby="gallery-dotnav-dda6f49a0116ad3180b87abe06bdc92c" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:new-blue">
                                
                                <div>
                                    <div>The new iPad mini comes in two gorgeous new finishes — blue and purple — in addition to starlight and space gray.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-blue-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-blue-241015_inline" aria-label="Download media, The new iPad mini is shown in the new blue color."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-e62275c2c4c5d7f444000d658260ec13" aria-labelledby="gallery-dotnav-e62275c2c4c5d7f444000d658260ec13" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:new-purple">
                                
                                <div>
                                    <div>The new iPad mini comes in two gorgeous new finishes — blue and purple — in addition to starlight and space gray.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-purple-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-purple-241015_inline" aria-label="Download media, The new iPad mini is shown in the new purple color."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-6cbaea15325c06057bb774c55151379a" aria-labelledby="gallery-dotnav-6cbaea15325c06057bb774c55151379a" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:starlight">
                                
                                <div>
                                    <div>The new iPad mini comes in two gorgeous new finishes — blue and purple — in addition to starlight and space gray.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-starlight-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-starlight-241015_inline" aria-label="Download media, The new iPad mini is shown in starlight."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-63a6d5c11433b9d5e71fec71d0db9fbf" aria-labelledby="gallery-dotnav-63a6d5c11433b9d5e71fec71d0db9fbf" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:space-gray">
                                
                                <div>
                                    <div>The new iPad mini comes in two gorgeous new finishes — blue and purple — in addition to starlight and space gray.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-space-gray-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-space-gray-241015_inline" aria-label="Download media, The new iPad mini is shown in space gray."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>A17 Pro Unlocks Powerful Performance</strong>
</h2>
                 
             
                 <div>The new iPad mini gets a major update with A17 Pro, delivering incredible performance and power efficiency in an ultraportable design. A17 Pro is a powerful chip that unlocks a number of improvements over A15 Bionic in the previous-generation iPad mini. With a 6-core CPU — two performance cores and four efficiency cores — A17 Pro delivers a 30 percent boost in CPU performance.<sup>1</sup> A17 Pro also brings a boost in graphics performance with a 5-core GPU, delivering a 25 percent jump over the previous generation.<sup>1</sup> A17 Pro brings entirely new experiences — including pro apps used by designers, pilots, doctors, and others — and makes it faster than ever for users to edit photos, dive into more immersive AR applications, and more. The new iPad mini brings true-to-life gaming with hardware-accelerated ray tracing — which is 4x faster than software-based ray tracing — as well as support for Dynamic Caching and hardware-accelerated mesh shading. From creating engaging content faster than ever in Affinity Designer, to playing demanding, graphics-intensive AAA games like Zenless Zone Zero, users can take the powerful performance and ultraportable iPad mini anywhere.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, A still from the game Zenless Zone Zero on iPad mini.">
        <div>
             
              
              <div>
                The A17 Pro chip makes playing demanding, graphics-intensive AAA games like Zenless Zone Zero easier than ever while on the go.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-Zenless-Zone-Zero-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-Zenless-Zone-Zero-241015_big" aria-label="Download media, A still from the game Zenless Zone Zero on iPad mini."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Built for Apple Intelligence</strong>
</h2>
                 
             
                 <div>With the power of the A17 Pro chip, the new iPad mini delivers support for Apple Intelligence. Deeply integrated into iPadOS 18, Apple Intelligence harnesses the power of Apple silicon and Apple-built generative models to understand and create language and images, take action across apps, and draw from personal context to simplify and accelerate everyday tasks. Many of the models that power Apple Intelligence run entirely on device, and Private Cloud Compute offers the ability to flex and scale computational capacity between on-device processing and larger, server-based models that run on dedicated Apple silicon servers.
</div>
                 
             
                 <div>The first set of Apple Intelligence features will be available in U.S. English this month through a free software update with iPadOS 18.1, and available for iPad with A17 Pro or M1 and later. Apple Intelligence delivers experiences that are delightful, intuitive, easy to use, and specially designed to help users do the things that matter most to them:<sup>2</sup>
</div>
                 
             
                 <div><ul>
<li>With <strong>Writing Tools</strong>, users can refine their words by rewriting, proofreading, and summarizing text nearly everywhere they write, including Mail, Notes, Pages, and third-party apps.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, Apple Intelligence Writing Tools are shown on iPad mini.">
                <div>
                         
                            
                            <div>
                                With systemwide Writing Tools, available with the first set of Apple Intelligence features in iPadOS 18, users can rewrite, proofread, and summarize text nearly everywhere they write.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-Apple-Intelligence-Writing-Tools-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-Apple-Intelligence-Writing-Tools-241015_inline" aria-label="Download media, Apple Intelligence Writing Tools are shown on iPad mini."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <div><ul>
<li><strong>Siri</strong> becomes more deeply integrated into the system experience and gets a new design with an elegant glowing light that wraps around the edge of the screen when active on iPad. With richer language-understanding capabilities, communicating with Siri is more natural and flexible. Siri can follow along when users stumble over their words, can maintain context from one request to the next, and now, users can type to Siri. Siri also has extensive product knowledge to answer questions about features on iPad and other Apple devices.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>In Photos, the <strong>Memories</strong> feature now enables users to create the movies they want to see by simply typing a description, and with the new <strong>Clean Up</strong> tool, they can identify and remove distracting objects in the background of a photo — without accidentally altering the subject.</li>
</ul>
</div>
                 
             
                 <div>Additional Apple Intelligence features will be rolling out over the next several months:
</div>
                 
             
                 <div><ul>
<li><strong>Image Playground </strong>allows users to create playful images in moments.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Image Playground tools are shown in the Freeform app.">
        <div>
             
              
              <div>
                With Image&nbsp;Playground, users can create playful images in seconds in apps like Freeform.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-Apple-Intelligence-Image-Playground-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-Apple-Intelligence-Image-Playground-241015_big" aria-label="Download media, Image Playground tools are shown in the Freeform app."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <div><ul>
<li><strong>Image Wand </strong>is a new tool in the Apple Pencil tool palette that can transform a rough sketch into a polished image.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Emoji will be taken to an entirely new level with the ability to create original <strong>Genmoji</strong> by simply typing a description, or by selecting a photo of a friend or family member.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li><strong>Siri</strong> will be able to draw on a user’s personal context to deliver intelligence that is tailored to them. It will also gain onscreen awareness to understand and take action with users’ content, as well as take hundreds of new actions in and across Apple and third-party apps.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>With <strong>ChatGPT </strong>integrated into experiences within iPadOS 18, users have the option to access its<strong> </strong>expertise, as well as its image- and document-understanding capabilities, within Siri and Writing Tools without needing to jump between tools. And privacy protections are built in so&nbsp;a user’s IP address is obscured, and OpenAI won’t store requests. Users can access ChatGPT for free without creating an account, and ChatGPT’s data-use policies apply for those who choose to connect their account.</li>
</ul>
</div>
                 
             
                 <h2><strong>Even Faster Connectivity</strong>
</h2>
                 
             
                 <div>With faster wireless and wired connectivity, users can do even more on iPad mini while on the go. The new iPad mini supports Wi-Fi 6E, which delivers up to twice the performance than the previous generation,<sup>3</sup> so users can download files, play games online, and stream movies even faster. Wi-Fi + Cellular models with 5G allow users to access their files, communicate with peers, and back up their data in a snap while on the go. Cellular models of the new iPad mini are activated with eSIM, a more secure alternative to a physical SIM card, allowing users to quickly connect and transfer their existing plans digitally, and store multiple cellular plans on a single device. Customers can easily get connected to wireless data plans on the new iPad mini in over 190 countries and regions around the world without needing to get a physical SIM card from a local carrier. The USB-C port is now up to 2x faster than the previous generation, with data transfers up to 10Gbps, so importing large photos and videos is even quicker.
</div>
                 
             
                 <h2><strong>Incredible Camera Experience</strong>
</h2>
                 
             
                 <div>Great cameras, along with the incredibly portable form factor of iPad mini, enable powerful mobile workflows. The 12MP wide back camera delivers gorgeous photos, and<strong> </strong>with Smart HDR 4, they will be even more detailed and vivid. Utilizing the powerful 16-core Neural Engine, the new iPad mini uses artificial intelligence (AI) to automatically identify documents right in the Camera app and can use the new True Tone flash to remove shadows from the document. The 12MP Ultra Wide front-facing camera in portrait orientation, with support for Center Stage, is great for all the ways customers use iPad mini.
</div>
                 
             
         </div>
 

    
    
    


    
        
        
        
        
            <figure aria-label="Media, A close-up of the back cameras on the new iPad mini.">
                <div>
                         
                            
                            <div>
                                An updated 12MP wide back camera delivers even more versatility on the new iPad mini.
                            </div>
                        
                        
                        
                        
                        <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-camera-detail-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-camera-detail-241015_inline" aria-label="Download media, A close-up of the back cameras on the new iPad mini."></a>
                    </div>
            </figure>
        
    










    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Magical Capabilities with Apple Pencil Pro</strong>
</h2>
                 
             
                 <div><a href="https://www.apple.com/apple-pencil/" target="_blank">Apple Pencil Pro</a> unlocks magical capabilities and powerful interactions, turning iPad mini into a sketchbook users can take anywhere. Apple Pencil Pro can sense a user’s squeeze, bringing up a tool palette to quickly switch tools, line weights, and colors, all without interrupting the creative process. A custom haptic engine delivers a light tap that provides confirmation when users squeeze, double-tap, or snap to a Smart Shape for a remarkably intuitive experience. Users can roll Apple Pencil Pro for precise control of the tool they’re using. Rotating the barrel changes the orientation of shaped pen and brush tools, just like pen and paper, and with Apple Pencil hover, users can visualize the exact orientation of a tool before making a mark. Apple Pencil Pro features support for Find My, and pairs, charges, and is stored through a new magnetic interface on the new iPad mini. iPad mini also supports Apple Pencil (USB-C), ideal for note taking, sketching, annotating, journaling, and more, at a great value.
</div>
                 
             
         </div>
 

    
    
    






  
    
    
    
    
      <figure aria-label="Media, Apple Pencil Pro draws on the new iPad mini in Procreate.">
        <div>
             
              
              <div>
                With advanced features like squeeze, barrel roll, and haptic feedback, Apple Pencil Pro makes creating a masterpiece in apps like Procreate more intuitive than ever.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-Apple-Pencil-Pro-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-Apple-Pencil-Pro-241015_big" aria-label="Download media, Apple Pencil Pro draws on the new iPad mini in Procreate."></a>
          </div>
      </figure>
    
  






    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>iPadOS 18 Brings Powerful and Intelligent New Features</strong>
</h2>
                 
             
                 <div>In addition to the groundbreaking capabilities of Apple Intelligence, <a href="https://www.apple.com/newsroom/2024/09/ipados-18-is-now-available-taking-ipad-to-the-next-level/" target="_blank">iPadOS 18</a> brings powerful features that enhance the iPad experience, making it more versatile and intelligent than ever. iPadOS also has advanced frameworks like Core ML that make it easy for developers to tap into the Neural Engine to deliver powerful AI features right on device.
</div>
                 
             
                 <div><ul>
<li>Designed for the unique capabilities of iPad, <strong>Calculator </strong>delivers an entirely new way to use Apple Pencil to solve expressions, as well as basic and scientific calculators with a new history function and unit conversions. With <strong>Math Notes</strong>, users are now able to type mathematical expressions or write them out to see them instantly solved in handwriting like their own. They can also create and use variables, and add an equation to insert a graph. Users can also access their Math Notes in the Notes app, and use all of the math functionality in any of their other notes.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>In the <strong>Notes </strong>app, handwritten notes become more fluid, flexible, and easy to read with <strong>Smart Script</strong> and the power of Apple Pencil. Smart Script unleashes powerful new capabilities for users editing handwritten text, allowing them to easily add space, or even paste typed text in their own handwriting. And as users write with Apple Pencil, their handwriting will be automatically refined in real time to be smoother, straighter, and more legible.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>With new <strong>Audio Recording </strong>and <strong>Transcription</strong>, iPad can capture a lecture or conversation, and transcripts are synced with the audio, so users can search for an exact moment in the recording.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>New levels of customization come to iPad, and users have even more options to express themselves through the <strong>Home Screen</strong> with app icons and widgets that can be placed in any open position. App icons and widgets can take on a new look with a dark or tinted effect, and users can make them appear larger to create the experience that is perfect for them. <strong>Control Center </strong>has been redesigned to provide easier access to many of the things users do every day, delivering quick access to new groups of a user’s most-utilized controls. Users can even organize new controls from third-party apps in the redesigned Control Center.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The <strong>Photos </strong>app<strong> </strong>receives its biggest update ever, bringing users powerful new tools that make it easier to find what they are looking for with a simplified and customizable app layout that takes advantage of the larger display on iPad and helps users browse by themes without having to organize content into albums.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Users have new ways to stay connected and express themselves in <strong>Messages</strong>, with all-new animated text effects, redesigned Tapbacks, and the ability to schedule messages to send at a later time.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="ipados-18-on-ipad-mini">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-474fa0b40a8b1252aea7e9ab85cc7989" href="#gallery-474fa0b40a8b1252aea7e9ab85cc7989" data-ac-gallery-trigger="gallery-474fa0b40a8b1252aea7e9ab85cc7989"><span>The new Math Notes feature is shown on iPad mini.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-c4e4bd28e81aef6239ea581c1b7658d7" href="#gallery-c4e4bd28e81aef6239ea581c1b7658d7" data-ac-gallery-trigger="gallery-c4e4bd28e81aef6239ea581c1b7658d7"><span>The new Smart Script feature is shown on iPad mini.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-de3df090c9be30d1fe53d1f0b94d9236" href="#gallery-de3df090c9be30d1fe53d1f0b94d9236" data-ac-gallery-trigger="gallery-de3df090c9be30d1fe53d1f0b94d9236"><span>The redesigned Photos app is shown on iPad mini.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-474fa0b40a8b1252aea7e9ab85cc7989" aria-labelledby="gallery-dotnav-474fa0b40a8b1252aea7e9ab85cc7989" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:math-notes">
                                
                                <div>
                                    <div>Math Notes in the Calculator app brings an intuitive way to do math, allowing users to type or write out math to evaluate expressions and assign variables.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-Math-Notes-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-Math-Notes-241015_inline" aria-label="Download media, The new Math Notes feature is shown on iPad mini."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-c4e4bd28e81aef6239ea581c1b7658d7" aria-labelledby="gallery-dotnav-c4e4bd28e81aef6239ea581c1b7658d7" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:smart-script">
                                
                                <div>
                                    <div>The powerful Smart Script handwriting tools can refine handwritten notes and even paste typed text in one’s own handwriting.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-Smart-Script-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-Smart-Script-241015_inline" aria-label="Download media, The new Smart Script feature is shown on iPad mini."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-de3df090c9be30d1fe53d1f0b94d9236" aria-labelledby="gallery-dotnav-de3df090c9be30d1fe53d1f0b94d9236" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:redesigned-photos">
                                
                                <div>
                                    <div>Taking advantage of the larger display on iPad, the Photos app receives its biggest redesign yet.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2024/10/apple-introduces-powerful-new-ipad-mini-built-for-apple-intelligence/article/Apple-iPad-mini-redesigned-Photos-241015.zip" download="" data-analytics-title="download image - Apple-iPad-mini-redesigned-Photos-241015_inline" aria-label="Download media, The redesigned Photos app is shown on iPad mini."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Better for the Environment</strong>
</h2>
                 
             
                 <div>The new iPad mini is designed with the environment in mind, including 100 percent recycled aluminum in the enclosure, 100 percent recycled rare earth elements in all magnets, and 100 percent recycled gold plating and tin soldering in multiple printed circuit boards. The new iPad mini meets Apple’s high standards for energy efficiency, and is free of mercury, brominated flame retardants, and PVC. The packaging is 100 percent fiber-based, bringing Apple closer to its goal to remove plastic from all packaging by 2025.
</div>
                 
             
                 <div>Today, Apple is carbon neutral for global corporate operations and, as part of its ambitious Apple 2030 goal, plans to be carbon neutral across its entire carbon footprint by the end of this decade.
</div>
                 
             
         </div>
 

    
    
    


     
     
    
    
        <div>
             
                 
                 
             
                 <div><ul>
<li>Customers can pre-order the new iPad mini starting today, October 15, on <a href="https://www.apple.com/store/" target="_blank">apple.com/store</a>, and in the Apple Store app in 29 countries and regions, including the U.S. It will begin arriving to customers, and will be in Apple Store locations and Apple Authorized Resellers, starting Wednesday, October&nbsp;23.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Available in blue, purple, starlight, and space gray, the new iPad mini starts at&nbsp;<strong>$499</strong>&nbsp;(U.S.) for the Wi-Fi model, and&nbsp;<strong>$649&nbsp;</strong>(U.S.) for the Wi-Fi + Cellular model.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The new iPad mini starts with 128GB of storage — double the storage of the previous generation. The new iPad mini is also available in 256GB and 512GB configurations.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>For education, the new iPad mini starts at<strong> $449 </strong>(U.S.). Education pricing is available to current and newly accepted college students and their parents, as well as faculty, staff, and home-school teachers of all grade levels. For more information, visit <a href="https://www.apple.com/us-hed/shop/" target="_blank">apple.com/us-hed/shop</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple Pencil Pro is compatible with the new iPad mini. It is available for <strong>$129</strong> (U.S.), and <strong>$119</strong> (U.S.) for education. Apple Pencil (USB-C) is available for<strong> $79 </strong>(U.S.),<strong> </strong>and<strong> $69 </strong>(U.S.)<strong> </strong>for education.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>The new Smart Folio, available in charcoal gray, light violet, denim, and sage, is<strong> $59</strong> (U.S.).</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Apple offers great ways to save on the latest iPad. Customers can trade in their current iPad and get credit toward a new one by visiting the <a href="https://www.apple.com/store/" target="_blank">Apple Store online</a>, the Apple Store app, or an Apple Store location. To see what their device is worth and for terms and conditions, customers can visit <a href="https://www.apple.com/shop/trade-in/" target="_blank">apple.com/shop/trade-in</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Customers in the U.S. who shop at Apple using Apple Card can pay monthly at 0 percent APR when they choose to check out with Apple Card Monthly Installments, and they’ll get 3 percent Daily Cash back — all up front. More information — including details on eligibility, exclusions, and Apple Card terms — is available at <a href="https://www.apple.com/apple-card/monthly-installments/" target="_blank">apple.com/apple-card/monthly-installments</a>.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>AppleCare+ for iPad provides unparalleled service and support. This includes unlimited incidents of accidental damage, battery service coverage, and 24/7 support from the people who know iPad best.</li>
</ul>
</div>
                 
             
                 <div><ul>
<li>Every customer who buys directly from Apple Retail gets access to Personal Setup. In these guided online sessions, a Specialist can walk customers through setup, or focus on features that help them make the most of their new device.</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    





    
    
    <div>
            <ol>
<li>Testing conducted by Apple in September 2024 using preproduction iPad mini (A17 Pro) and production iPad mini (6th generation) units. Tested with Affinity Photo 2 v2.5.5.2636 using the built-in benchmark version 25000. Performance tests are conducted using specific iPad units and reflect the approximate performance of iPad mini.</li>
<li>Apple Intelligence will be available as a free software update for iPad with A17 Pro or M1 and later with device and Siri language set to U.S. English. The first set of features will be available in beta this month with iPadOS 18.1 with more features rolling out in the months to come. Later this year, Apple Intelligence will add support for localized English in Australia, Canada, New Zealand, South Africa, and the U.K. In the coming year, Apple Intelligence will expand to more languages, like Chinese, English (India), English (Singapore), French, German, Italian, Japanese, Korean, Portuguese, Spanish, Vietnamese, and others.</li>
<li>Wi‑Fi 6E available in countries and regions where supported.</li>
</ol>

        </div>



    
    
    






    

















		
		
			
























		
		

</article>



</section>
</main>


	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mothbox 4.0 (174 pts)]]></title>
            <link>https://digital-naturalism-laboratories.github.io/Mothbox/</link>
            <guid>41848804</guid>
            <pubDate>Tue, 15 Oct 2024 14:10:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://digital-naturalism-laboratories.github.io/Mothbox/">https://digital-naturalism-laboratories.github.io/Mothbox/</a>, See on <a href="https://news.ycombinator.com/item?id=41848804">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content"><main><h2 id="mothbox-40"> <a href="#mothbox-40" aria-labelledby="mothbox-40"></a> Mothbox 4.0</h2><p>The Mothbox is a low-cost, <a href="https://digital-naturalism-laboratories.github.io/Mothbox/docs/about/specs/">high-performance insect monitor</a>. It features a power efficient and lightweight design meant to help field biologists deploy it in the depths of the jungles, and its low-cost nature means you can build one to study the biodiversity at your home!</p><p>All the physical designs, electronics schematics, Pi Scripts, and insect-IDing Artificial Intelligence are <strong>provided free and open source</strong>, so you can build, share and improve on these designs yourself!</p><p>See the <a href="https://digital-naturalism-laboratories.github.io/Mothbox/docs/about/specs/">full specifications of what it can do here.</a></p><p><img src="https://github.com/user-attachments/assets/cf7da6c8-2a7d-40a8-8872-6f9987c43082" alt="PXL_20240720_054408351 MP-EDIT"></p><h2 id="why-study-insects-for-conservation"> <a href="#why-study-insects-for-conservation" aria-labelledby="why-study-insects-for-conservation"></a> Why Study Insects for Conservation?</h2><p>Insect populations can be used as an ultra high resolution sensor for changes in environments.</p><h3 id="insects-especially-moths-and-beetles-are-hyperdiverse"> <a href="#insects-especially-moths-and-beetles-are-hyperdiverse" aria-labelledby="insects-especially-moths-and-beetles-are-hyperdiverse"></a> Insects (Especially Moths and Beetles) are Hyperdiverse</h3><p>Of all life on earth (including bacteria) there are about 2 million species scientists have described and <a href="https://ourworldindata.org/how-many-species-are-there#:~:text=In%20the%20chart%2C%20we%20see,reptiles%2C%20and%20over%206%2C000%20mammals.">half of the species are insects!</a> What’s more, if you look at <strong>just Moths</strong> there are about 144,000 species, meaning <strong>about 1 in every 14 species in the world is a moth!</strong> For reference there are only about 11,000 bird species, and only 6500 species of mammals (and half are bats and rodents).</p><p>Using creatures with a super diverse gene pool allows us to make sophisticated insights into the health of different areas. Combining the activity of thousands of species of insects with other data like climate, acoustic, or soil analysis, can provide deep analysis into how environments can be repaired.</p><h3 id="offer-highly-localized-insights"> <a href="#offer-highly-localized-insights" aria-labelledby="offer-highly-localized-insights"></a> Offer Highly Localized Insights</h3><p>Because they tend to have short lives and often limited ranges, insects can provide super localized data. Moths, in particular, are great indicators of overall biodiversity because they are often host-specific as caterpillars. This means they only feed on one or a limited number of plant species. So, by monitoring moths, you are getting a proxy of the local plant community.</p><p>For comparison, think about something like a Jaguar, which is a rare find and scientifically valuable to see on a wildlife camera, but they have large ranges and long lives. The presence of a Jaguar does not necessarily tell us much about the health of the environment where it was spotted. It could simply be travelling across disturbed farmland to escape from a destroyed habitat elsewhere.</p><p>Having an insect sensor (like a Mothbox), however, that could compare the activity of thousands of different species of insects can highlight differences in environmental health in areas just a couple kilometers apart.</p><h3 id="they-are-easy-to-attract"> <a href="#they-are-easy-to-attract" aria-labelledby="they-are-easy-to-attract"></a> They are Easy to Attract</h3><p>Humans have long ago discovered that many nocturnal insects like Moths seem to be attracted to lights. We now know this is because <a href="https://www.nature.com/articles/s41467-024-44785-3">bright lights disorient their natural steering mechanism</a>. Scientists have used bright artificial lights for over a century now to take censuses of insect populations. The problem with this type of “Mothlighting” is that it can be incredibly time consuming (a scientist has to hang out all night and note which insects are visiting). Instead we have an automated device that does all this work for you!</p><h2 id="what-it-does"> <a href="#what-it-does" aria-labelledby="what-it-does"></a> What it Does</h2><p>The Mothbox stays in an ultra low-power state until your schedule tells it which nights to wake up. Then it triggers an insect lure (usually a bright UV light) and then takes ultra-high resolution photos of the visitors it attracts. <img src="https://github.com/user-attachments/assets/cb7f6a03-849b-4b2a-99b9-9580aa245816" alt="PXL_20240607_025924783 NIGHT-EDIT"></p><p>Next we created some open-source AI scripts that process these insects for you. First, a trained YOLO v8 detects where all the insects are in the images and crops them out.</p><p><img src="https://github.com/Digital-Naturalism-Laboratories/Mothbox/assets/742627/ec1a50ce-38bf-4bb3-b8b6-752ba1801050" width="48%"> <img src="https://github.com/user-attachments/assets/3d0936ce-e89c-411a-8529-a932acc6e9c8" width="48%"></p><p>Then we use a special version of BioCLIP with a user interface to help you automatically group and ID the insects to different taxonomic levels! <img src="https://github.com/user-attachments/assets/93728753-8a70-4686-b493-1e3de177627e" alt="image"></p><h2 id="build-it-yourself"> <a href="#build-it-yourself" aria-labelledby="build-it-yourself"></a> Build it Yourself!</h2><p><img src="https://github.com/user-attachments/assets/f37ec4d5-4761-4eab-b0ae-179b05b3d1cf" alt="PXL_20240810_155421647 MP"></p><p>This Mothbox documentation will provide you with documentation for how to source, build, program, and use your mothbox!</p><p><a href="https://digital-naturalism-laboratories.github.io/Mothbox/docs/building">Get started building your mothbox!</a></p><p>After following these guides, you should be able to make your own set of mothboxes to conduct your own biodiversity studies!</p><p><img src="https://github.com/Digital-Naturalism-Laboratories/Mothbox/assets/742627/2e1cacf2-35dd-48b0-83c7-29b5320fa36c" width="45%"> <img src="https://github.com/user-attachments/assets/2416c098-b080-4014-b972-3acb9e366aa6" width="45%"></p><h2 id="mothbeam"> <a href="#mothbeam" aria-labelledby="mothbeam"></a> Mothbeam</h2><p>We are also building an open-source, portable low cost light for mothlighting, the Mothbeam! Some early documentation for <a href="https://digital-naturalism-laboratories.github.io/Mothbox/docs/building/attractor/#internal-mothbeam">making your own Mothbeam is here.</a> <img src="https://github.com/Digital-Naturalism-Laboratories/Mothbox/assets/742627/fab3c9ac-f879-4768-abee-1e61d8d63172" alt="Untitled"></p><p><img src="https://github.com/user-attachments/assets/5201d9bb-20fc-43ae-8e99-69f6ac24126e" alt="PXL_20240718_190826331 MP"></p></main><hr></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Life expectancy rise in rich countries slows down: took 30 years to prove (101 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-024-03244-1</link>
            <guid>41848482</guid>
            <pubDate>Tue, 15 Oct 2024 13:39:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-024-03244-1">https://www.nature.com/articles/d41586-024-03244-1</a>, See on <a href="https://news.ycombinator.com/item?id=41848482">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    
                        <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-024-03244-1/d41586-024-03244-1_27700178.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-024-03244-1/d41586-024-03244-1_27700178.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="A Japanese baby being carried on his grandmother's back." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-024-03244-1/d41586-024-03244-1_27700178.jpg">
  <figcaption>
   <p><span>What chance do children born since 2010 have of living to 100? Less than 2% for men and about 5% for women.</span><span>Credit: Skye Hohmann/Alamy</span></p>
  </figcaption>
 </picture>
</figure><p>Put aside the hype about the growing number of us who are likely to make it to 100, because the rise in human life expectancy might actually be slowing down. At least, according to a study that analysed mortality data for ten countries or regions over the past three decades<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>.</p><p>“There are limits to how far out we can push out the envelope of human survival,” says study co-author S. Jay Olshansky, an epidemiologist at the University of Illinois Chicago. “If you live long enough, you run up against the biological process of ageing.”</p><p>The era of what he calls radical life extension is over, he argues. Some researchers, however, disagree and point out that medical science could yet find a way to push age limits.</p><p>Advances in public health and medicine during the twentieth century increased human life expectancy to about three years per decade. But Olshansky and others have long argued that this rate of improvement is not sustainable, despite more-optimistic forecasts that predict most children born in the twenty-first century would live for a 100 years or more<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup>. But this is difficult to confirm, because the only way to be sure is to wait for enough people to die, or not.</p><p>With his colleagues, Olshansky first published the idea<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup> that human life expectancy has a finite limit in 1990. “We have waited 30 years to test this,” he says. “And we now have definitive evidence that the limited lifespan hypothesis is correct.”</p><p>That evidence is based on the numbers of reported deaths in parts of the world with some of the highest current life expectancies, including Hong Kong, Japan, South Korea, Australia, France, Italy, Switzerland, Sweden, the United States and Spain. The analysis looked at the period of 1990 to 2019, to avoid the distorting impact of the COVID-19 pandemic.</p><p>The team found that the rate of improvement in life expectancy in the decade of 2010–19 had dropped below that seen between 1990 and 2000. People were still living longer, but not by as much. In fact, in every population except those of Hong Kong and South Korea, the decadal rise in life expectancy decelerated to below two years.</p><p>Overall, the study found that children born since 2010 have a relatively small chance of living to 100 (5.1% chance for women and 1.8% chance for men). The most likely cohort to see a full century are women in Hong Kong, with a 12.8% chance.</p><h2>Can we overcome ageing?</h2><p>It’s clear that further extension of the average lifespan is difficult because that would require researchers to find treatments for illnesses that affect older people, says Dmitri Jdanov, a demographer at the Max Planck Institute for Demographic Research in Rostock, Germany. Jdanov wrote, together with his colleague Domantas Jasilionis, an <a href="https://www.nature.com/articles/s43587-024-00722-z" data-track="click" data-label="https://www.nature.com/articles/s43587-024-00722-z" data-track-category="body text link">accompanying commentary on the paper</a>. Both articles are published in <i>Nature Aging</i> today.</p><p>But Jdanov thinks that Olshansky is too pessimistic about possible progress. “Although making another leap might be difficult, the rapid development of new technologies may lead to an unexpected health revolution,” he says.</p><p>A century ago, few researchers would have thought that child mortality could be reduced considerably, he says. But advances in vaccines, education and public health have since slashed the rate from more than 20% in 1950 to less than 4% now.</p><p>“If we cannot imagine something, it doesn’t mean that it’s impossible,” Jdanov says.</p><p>The study also revealed what Olshansky calls a “shocking” decline in the average life expectancy in the United States in the decade starting in 2010 — a trend seen in such a long-lived population only after extreme events, such as war, since 1900. The decline in the United States is driven by increasing numbers of deaths because of conditions such as diabetes and heart disease in people aged roughly 40 to 60.</p><p>“It tells you that something pretty negative is happening among some subgroups of the population to drag the average down, because the wealthier, more highly educated subgroups are actually doing better,” Olshansky says.</p>
                    
                </div><div id="references" aria-labelledby="Bib1"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ol data-track-component="outbound reference" data-track-context="references section"><li data-counter="1."><p id="ref-CR1">Olshansky, S. J., Willcox, B. J., Demetrius, L. &amp; Beltrán-Sánchez, H. <i>Nature Aging</i> https://doi.org/10.1038/s43587-024-00702-3 (2024).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1038/s43587-024-00702-3" data-track-item_id="10.1038/s43587-024-00702-3" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs43587-024-00702-3" aria-label="Article reference 1" data-doi="10.1038/s43587-024-00702-3">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Nature%20Aging&amp;doi=10.1038%2Fs43587-024-00702-3&amp;publication_year=2024&amp;author=Olshansky%2CS.%20J.&amp;author=Willcox%2CB.%20J.&amp;author=Demetrius%2CL.&amp;author=Beltr%C3%A1n-S%C3%A1nchez%2CH.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="2."><p id="ref-CR2">Christensen, K., Doblhammer, G., Rau, R. &amp; Vaupel, J. V. <i>Lancet</i> <b>374</b>, 1196–1208 (2009).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/S0140-6736(09)61460-4" data-track-item_id="10.1016/S0140-6736(09)61460-4" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS0140-6736%2809%2961460-4" aria-label="Article reference 2" data-doi="10.1016/S0140-6736(09)61460-4">Article</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Lancet&amp;doi=10.1016%2FS0140-6736%2809%2961460-4&amp;volume=374&amp;pages=1196-1208&amp;publication_year=2009&amp;author=Christensen%2CK.&amp;author=Doblhammer%2CG.&amp;author=Rau%2CR.&amp;author=Vaupel%2CJ.%20V.">
                    Google Scholar</a>&nbsp;
                </p></li><li data-counter="3."><p id="ref-CR3">Olshansky, S. J., Carnes, B. A. &amp; Cassel, C. <i>Science</i> <b>250</b>, 634–640 (1990).</p><p><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1126/science.2237414" data-track-item_id="10.1126/science.2237414" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1126%2Fscience.2237414" aria-label="Article reference 3" data-doi="10.1126/science.2237414">Article</a>&nbsp;
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=2237414" aria-label="PubMed reference 3">PubMed</a>&nbsp;
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=&amp;journal=Science&amp;doi=10.1126%2Fscience.2237414&amp;volume=250&amp;pages=634-640&amp;publication_year=1990&amp;author=Olshansky%2CS.%20J.&amp;author=Carnes%2CB.%20A.&amp;author=Cassel%2CC.">
                    Google Scholar</a>&nbsp;
                </p></li></ol><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/d41586-024-03244-1?format=refman&amp;flavour=references">Download references</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built the most over-engineered Deal With It emoji generator (586 pts)]]></title>
            <link>https://emoji.build/deal-with-it-generator/</link>
            <guid>41848150</guid>
            <pubDate>Tue, 15 Oct 2024 13:05:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://emoji.build/deal-with-it-generator/">https://emoji.build/deal-with-it-generator/</a>, See on <a href="https://news.ycombinator.com/item?id=41848150">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I 3D scanned the tunnels inside the Maya Pyramid Temples at Copan (238 pts)]]></title>
            <link>https://mused.com/guided/158/temple-26-and-excavation-tunnels-copan-ruinas/</link>
            <guid>41848099</guid>
            <pubDate>Tue, 15 Oct 2024 12:57:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mused.com/guided/158/temple-26-and-excavation-tunnels-copan-ruinas/">https://mused.com/guided/158/temple-26-and-excavation-tunnels-copan-ruinas/</a>, See on <a href="https://news.ycombinator.com/item?id=41848099">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-josh-anim-name="fadeIn" data-josh-delay="300ms"><p>Since the 1930’s, archaeologists have tunneled into the acropolis at Copan to understand the many phases of construction throughout its history. </p>
<p>With investigations now mostly complete, the tunnels measure close to 4 kilometers in cumulative length, and have uncovered early stelae, plaster facades and tombs that have taught us much about what the acropolis was like before its final phase. </p>
<p>A Harvard-led conservation plan in on-going collaboration with the Instituto Hondureño de Antropología e Historia for these tunnels created a digital 3D model of the tunnel system and addresses ways to preserve the valuable architectural heritage within, taking into account the humid climate, seasonal changes to the water table, visitor access, and risk of collapse.</p>
<p>U</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Asterinas: OS kernel written in Rust and providing Linux-compatible ABI (177 pts)]]></title>
            <link>https://github.com/asterinas/asterinas</link>
            <guid>41847640</guid>
            <pubDate>Tue, 15 Oct 2024 12:01:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/asterinas/asterinas">https://github.com/asterinas/asterinas</a>, See on <a href="https://news.ycombinator.com/item?id=41847640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/asterinas/asterinas/blob/main/docs/src/images/logo_en.svg"><img src="https://github.com/asterinas/asterinas/raw/main/docs/src/images/logo_en.svg" alt="asterinas-logo" width="620"></a><br>
    A secure, fast, and general-purpose OS kernel written in Rust and compatible with Linux<br>
    <a href="https://github.com/asterinas/asterinas/actions/workflows/test_osdk.yml"><img src="https://github.com/asterinas/asterinas/actions/workflows/test_osdk.yml/badge.svg?event=push" alt="Test OSDK"></a>
    <a href="https://github.com/asterinas/asterinas/actions/workflows/test_asterinas.yml"><img src="https://github.com/asterinas/asterinas/actions/workflows/test_asterinas.yml/badge.svg?event=push" alt="Test Asterinas"></a>
    <a href="https://asterinas.github.io/benchmark/" rel="nofollow"><img src="https://github.com/asterinas/asterinas/actions/workflows/benchmark_asterinas.yml/badge.svg" alt="Benchmark Asterinas"></a>
    <br>
</p>
<p dir="auto">English | <a href="https://github.com/asterinas/asterinas/blob/main/README_CN.md">中文版</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introducing Asterinas</h2><a id="user-content-introducing-asterinas" aria-label="Permalink: Introducing Asterinas" href="#introducing-asterinas"></a></p>
<p dir="auto">Asterinas is a <em>secure</em>, <em>fast</em>, and <em>general-purpose</em> OS kernel
that provides <em>Linux-compatible</em> ABI.
It can serve as a seamless replacement for Linux
while enhancing <em>memory safety</em> and <em>developer friendliness</em>.</p>
<ul dir="auto">
<li>
<p dir="auto">Asterinas prioritizes memory safety
by employing Rust as its sole programming language
and limiting the use of <em>unsafe Rust</em>
to a clearly defined and minimal Trusted Computing Base (TCB).
This innovative approach,
known as <a href="https://asterinas.github.io/book/kernel/the-framekernel-architecture.html" rel="nofollow">the framekernel architecture</a>,
establishes Asterinas as a more secure and dependable kernel option.</p>
</li>
<li>
<p dir="auto">Asterinas surpasses Linux in terms of developer friendliness.
It empowers kernel developers to
(1) utilize the more productive Rust programming language,
(2) leverage a purpose-built toolkit called <a href="https://asterinas.github.io/book/osdk/guide/index.html" rel="nofollow">OSDK</a> to streamline their workflows,
and (3) choose between releasing their kernel modules as open source
or keeping them proprietary,
thanks to the flexibility offered by <a href="#License">MPL</a>.</p>
</li>
</ul>
<p dir="auto">While the journey towards a production-grade OS kernel can be challenging,
we are steadfastly progressing towards our goal.
Currently, Asterinas only supports x86-64 VMs.
However, <a href="https://asterinas.github.io/book/kernel/roadmap.html" rel="nofollow">our aim for 2024</a> is
to make Asterinas production-ready on x86-64 VMs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Get yourself an x86-64 Linux machine with Docker installed.
Follow the three simple steps below to get Asterinas up and running.</p>
<ol dir="auto">
<li>Download the latest source code.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/asterinas/asterinas"><pre>git clone https://github.com/asterinas/asterinas</pre></div>
<ol start="2" dir="auto">
<li>Run a Docker container as the development environment.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -it --privileged --network=host --device=/dev/kvm -v $(pwd)/asterinas:/root/asterinas asterinas/asterinas:0.9.2"><pre>docker run -it --privileged --network=host --device=/dev/kvm -v <span><span>$(</span>pwd<span>)</span></span>/asterinas:/root/asterinas asterinas/asterinas:0.9.2</pre></div>
<ol start="3" dir="auto">
<li>Inside the container, go to the project folder to build and run Asterinas.</li>
</ol>

<p dir="auto">If everything goes well, Asterinas is now up and running inside a VM.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The Book</h2><a id="user-content-the-book" aria-label="Permalink: The Book" href="#the-book"></a></p>
<p dir="auto">See <a href="https://asterinas.github.io/book/" rel="nofollow">The Asterinas Book</a> to learn more about the project.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Asterinas's source code and documentation primarily use the
<a href="https://github.com/asterinas/asterinas/blob/main/LICENSE-MPL">Mozilla Public License (MPL), Version 2.0</a>.
Select components are under more permissive licenses,
detailed <a href="https://github.com/asterinas/asterinas/blob/main/.licenserc.yaml">here</a>. For the rationales behind the choice of MPL, see <a href="https://asterinas.github.io/book/index.html#licensing" rel="nofollow">here</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Mermaid ASCII Diagrams (154 pts)]]></title>
            <link>https://mermaid-ascii.art/</link>
            <guid>41847407</guid>
            <pubDate>Tue, 15 Oct 2024 11:30:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mermaid-ascii.art/">https://mermaid-ascii.art/</a>, See on <a href="https://news.ycombinator.com/item?id=41847407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <form id="input" hx-post="/generate" hx-target="#result" hx-trigger="click from:.example-buttons">
                
                <div>
                        <p><label for="xPadding">xPadding:</label></p><p>3</p>
                    </div>
                <div>
                        <p><label for="yPadding">yPadding:</label></p><p>3</p>
                    </div>
                
            </form>
            
            <hr>
            <p>There are alternative forms of generating mermaid graphs in ASCII using this same Git repository.</p>
            <h2>cUrl request with data</h2>
            <div>
                <pre><code>curl https://mermaid-ascii.art/generate -d mermaid="graph LR\nABC --&gt; DEF"</code></pre>
            </div>
            <h2>Run Golang in CLI</h2>
            <div>
                <pre><code>git clone https://github.com/AlexanderGrooff/mermaid-ascii
cd mermaid-ascii
go run main.go -f test.mermaid</code></pre>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Short films by Lillian F. Schwartz (1927-2024) (102 pts)]]></title>
            <link>http://lillian.com/films/</link>
            <guid>41847239</guid>
            <pubDate>Tue, 15 Oct 2024 10:58:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://lillian.com/films/">http://lillian.com/films/</a>, See on <a href="https://news.ycombinator.com/item?id=41847239">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Lillian Schwartz, resident artist and consultant at Bell Laboratories (New Jersey), 1969-2002. During the 70s and 80s Schwartz developed a catalogue of visionary techniques for the use of the computer system by artists. Her formal explorations in abstract animation involved the marriage of film, computers and music in collaboration with such luminaries as computer musicians Jean-Claude Risset, Max Mathews, Vladimir Ussachevsky, Milton Babbit, and Richard Moore. Schwartz’s films have been shown and won awards at the Venice Biennale, Zagreb, Cannes, The National Academy of Television Arts and Sciences, and nominated and received Emmy nominations and awards.</p><p>Her work has been exhibited at, and is owned by, The Museum of Modern Art (New York), The Metropolitan Museum of Art (New York), The Whitney Museum of American Art (New York), The Moderna Museet (Stockholm), Centre Beauborg (Paris), Stedlijk Museum of Art (Amsterdam), and the Grand Palais Museum (Paris). Lumen has collaborated with Lillian Schwartz and curator Gregory Kurcewicz to compile a touring package of these important works. “A Beautiful Virus Inside the Machine” features animations restored to video. “The Artist and the Computer”, 1976, 10 mins is a documentary about her work. Produced by Larry Keating for AT&amp;T, “The Artist and the Computer is an excellent introductory informational film that dispels some of the ‘mystery’ of computer-art technology, as it clarifies the necessary human input of integrity, artistic sensibilities, and aesthetics. Ms. Schwartz’s voice over narration explains what she hoped to accomplish in the excerpts from a number of her films and gives insight into the artist’s problems and decisions.” – John Canemaker</p><div>
<h2><iframe src="http://player.vimeo.com/video/32222751" width="500" height="375" frameborder="0" allowfullscreen="allowfullscreen"></iframe></h2>
<p>New pixel-editing techniques extends the psychology of perception and won Schwartz an Emmy. Commissioned by The Museum of Modern Art. Thanks to Richard C. Voss and Sig Handleman at IBM Yorktown, NY. Funded by IBM.</p>
<p><strong>1978 “NEWTONIAN I” – 4 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56943959" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
An illusion of 3 dimensions is achieved by a blending of mathematics and physics to carry the spectator through a new range of audio and visual dynamics. The illusion is further enhanced by moving objects through space such that they are covered and uncovered in encounters with other objects, an expert use of color and a unique musical score by Jean-Claude Risset. University of Marseilles, Opera House – Sidney Australia.</p>
<p><strong>1978 “NEWTONIAN II” – 5 1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/57013999?title=0&amp;byline=0&amp;portrait=0" width="500" height="375" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
This film is strongly rooted in its underlying mathematical structure which forms the basis for the images. The music by Jean Claude Risset is integral to the creation of this concert of space and time. First World Animated Film Festival in Varna, Bulgaria (1979).</p>
<p><strong>1977 “L’OISEAU” – 4 Min 55 Sec.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56943958" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by F. Richard Moore. A single bird in flight is transformed, enhanced and interpreted so as to present a unique visual experience. From its original inception in a 128 frame black-and-white sequence it evolves by programmed reflection, inversion, magnification, color transformation and time distortion into the final restructured film as art. Director’s Choice and Purchase Award – Sinking Creek 1978; Zagreb ’78 International Animation Film Festival.</p>
<p><strong>1977 “BAGATELLES” – 4 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/38997801" width="500" height="331" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by Webern; synthesized by computer by Max V. Mathews and Elizabeth Cohen. Crystal growth by Charles Miller. Animated paints by Lillian Schwartz. Abstract images of frame-by-frame animation with subtle growing effects of crystals are enhanced by polarized colors. IRCAM, Paris.</p>
<p><strong>1976 “PICTURES FROM A GALLERY” – 6 Min 33 Sec.</strong></p>
<p><iframe src="http://player.vimeo.com/video/59694963" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p><iframe src="https://player.vimeo.com/video/111135300" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by Albert Miller. Picture-processed photos from the artist-filmmaker’s family. Faces are abstracted in a divisionistic manner. “… one of the great motion pictures of our time. While embracing the full range of human activity from cradle to old age, the production illuminates with deep feeling the many elements of present-day technology in filmmaking and the expanded cinema. It is truly a work of genius.” – John W. L. Russell, International Media Coordinator, USIA. Awards – Golden Eagle-Cine 1976; Grenoble Film Festival Award 1976, International Women’s Film Festival 1976. Cannes Film Festival.</p>
<p><strong>1976 “LA SPIRITATA” – 4 Min. 20 Sec.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56931283?title=0&amp;byline=0&amp;portrait=0" width="500" height="375" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music “Canzoni per sonar a quattro” by Giovanni Gabrieli, performed by Elizabeth Cohen, Max Mathews, and Gerard Schwarz. Images generated by computer.</p>
<p><strong>1976 “FANTASIES” – 5 Min. 15 Sec.</strong></p>
<p><iframe src="http://player.vimeo.com/video/38999416" width="500" height="331" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Computer generated images used as counterpoint to music “Fantasia &amp; In Nomine” by John Ward, performed by Elizabeth Cohen, Max Mathews, and Gerard Schwarz.</p>
<p><strong>1976 “EXPERIMENTS” – 7 Min.</strong></p>
<p>Music “Quartet in F” by Maurice Ravel, performed by Max Mathews. Subtly colored images combining microphotography and computer generated images with unique editing sequences that propel the viewer into a spiral-like endless vortex. Three music tracks were produced by the Groove System – a computer-controlled sound synthesizer.</p>
<p><strong>1975 “KINESIS” – 4-1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/54309109" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by Albert Miller. Escher-like images stepping through the frames to the music of a jazz group. Delightful–shows a depth in the imagery not accomplished by computer before. Oberhausen 1976.</p>
<p><strong>1975 “COLLAGE” – 5 1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/39002913" width="500" height="331" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by Joe Olive. A swift moving assortment of moving images. Filmed from a color TV monitor that was computer controlled.</p>
<p><strong>1975 “ALAE” – 5 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/53112794" width="500" height="367" frameborder="0"></iframe><br>
Music by F. Richard Moore. “The most fascinating use of the computer in filmmaking that I have seen recently is in L. Schwartz’ ALAE. Beginning with footage of sea birds in flight, the film image is then optically scanned and transformed by the computer. The geometric overlay on live random motion has the effect of creating new depth, a third dimension. Our perception of the birds’ forms and movements is heightened by the abstract pattern outlining them. Even if you suffer from the delusion that if you’ve seen one computer film, you’ve seen them all, give this stunning, original film a chance. Should you remain unconvinced, ALAE is a good example of the fact that computer technology seems destined to play an important role in animation.” – Jana Varlejs, Cine-Opsis, \f2Wilson Library Bull.\f1, March 1976. Whitney Museum of American Art.</p>
<p><strong>1974 “METAMORPHOSIS” – 8 Min. 15 Sec.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56932892?title=0&amp;byline=0&amp;portrait=0" width="500" height="375" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music Symphony in D Major by Salieri. “As expert hands in the complex techniques of integrating the computer and animation, L. Schwartz makes fascinating use of exotic, flowing forms, colors and electronic music in ‘Metamorphosis’.” – A. H. Weiler, N. Y. Times. “Schwartz’ METAMORPHOSIS is a complex study of evolving lines, planes, and circles, all moving at different speeds, and resulting in subtle color changes. The only computer-generated work on the program, it transcends what many of us have come to expect of such film with its subtle variations and significant use of color.” – Catherine Egan, Sight Lines, Vol. 8, No. 4, Summer 1975. Sinking Creek-1974; 1975 American Film Festival “Film as Art”. A three screen production.</p>
<p><strong>1974 “GALAXIES” – 4-1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56893953?title=0&amp;byline=0&amp;portrait=0" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by F. Richard Moore. Computer-simulated disk galaxies that are superimposed and twirl through space in beautiful colors at different speeds. Computer – Dr. Frank Hohl. NASA, Langley Air Force Base. Recent screening at the Museum of Modern Art, New York, December 10, 2012.</p>
<p><strong>1974 “MAYAN” – 7 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/57014001?title=0&amp;byline=0&amp;portrait=0" width="500" height="375" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
This tape combines live-images filmed in the Yucatan with output from the Paik video-synthesizer ribboned with computer-generated images. Post-production completed at WNET, Channel 13 as part of the Artist-in- Residence program.</p>
<p><strong>1974 “MIRAGE” – 5 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/39001677" width="500" height="331" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by Max Mathews. Filmed directly from color television controlled by computer programs. Beautifully flowing shapes that overlap and intertwine.</p>
<p><strong>1974 “METATHESIS” – 3 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/57014000?title=0&amp;byline=0&amp;portrait=0" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Changing parameters on mathematical equations. “As expert hands in the complex techniques of integrating the computer and animation, L. Schwartz makes fascinating use of exotic, flowing forms, colors and electronic music in METATHESIS.”- A. H. Weiler, \f2N. Y. Times\f1 Whitney Museum of American Art, and the Kennedy Center in Washington.</p>
<p><strong>1973 “INNOCENCE” – 2-1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/39001085" width="500" height="331" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by Emmanuel Ghent. Computer generated music and visuals films directly from a color TV monitor.</p>
<p><strong>1973 “PAPILLONS” – 4 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/65476276?title=0&amp;byline=0&amp;portrait=0" width="500" height="375" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Mathematical functions are the basis for a science film on contour plots and an art film. Both are shown simultaneously at a two screen production for an IEEE conference in NYC. Beauty in Science &amp; Art. Recent screening at the Museum of Modern Art, New York, December 10, 2012.</p>
<p><strong>1972 “MATHOMS” – 2-1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56932890?title=0&amp;byline=0&amp;portrait=0" width="500" height="375" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by F. Richard Moore. A playful concoction of computer produced images, a few hand-animated scenes and shots of lab equipment. Made largely from left-overs from scientific research. Whitney Museum of American Art.</p>
<p><strong>1972 “ENIGMA” – 4 min. 20 sec.</strong></p>
<p><iframe src="http://player.vimeo.com/video/54111878" width="500" height="367" frameborder="0"></iframe><br>
Copyright © 1972, 1973, 2003 Lillian Schwartz. All rights reserved.</p>
<p>“Lines and rectangles are the geometric shapes basic to ENIGMA, a computer graphics film full of subliminal and persistent image effects. In a staccato rhythm, the film builds to a climax by instantly replacing one set of shapes with another, each set either changing in composition and color or remaining for a moment to vibrate strobiscopically and then change.” – The Booklist. Awards: Foothills-1972; Kenyon-1973; 16 mm. de Montreal; 5th Annual Monterey Film Festival; 2nd Los Angeles International Film Festival; Nat. Acad. of TV Arts &amp; Sciences; Spec. Effect (72). Recent screening at the Museum of Modern Art, New York, December 10, 2012.</p>
<p><strong>1972 “GOOGOLPLEX” – 5-1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/54309111" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Extended editing techniques based on Land’s experiments affect the viewer’s sensory perceptions. “This viewer also found ‘Googolplex’, the six-minute computer-made compilation of swiftly-moving patterns … to be inventive, eye-catching examples of technical professionalism.” – A.\ H. Weiler, N. Y. Times. Lincoln Center Film Festival. Recent screening at the Museum of Modern Art, New York, December 10, 2012.</p>
<p><strong>1972 “APOTHEOSIS” – 4-1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/53112792" width="500" height="367" frameborder="0"></iframe><br>
Music by F. Richard Moore. “Apotheosis, which is developed from images made in the radiation treatment of human cancer, is the most beautiful and the most subtly textured work in computer animation I have seen.” – Roger Greenspun, N. Y. Times Award Foothills-1973. Recent screening at the Museum of Modern Art, New York, December 10, 2012.</p>
<p><strong>1972 “MUTATIONS” – 7-1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56932889?title=0&amp;byline=0&amp;portrait=0" width="500" height="375" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
“The changing dots, ectoplasmic shapes and electronic music of L. Schwartz’s ‘Mutations’ which has been shot with the aid of computers and lasers, makes for an eye-catching view of the potentials of the new techniques.” – A. H. Weiler, N. Y. Times. Music by Jean-Claude Risset–commissioned by Office de Radiodiffusion-Television Francaise. Golden Eagle-Cine 1973; Red Ribbon award – Special Effects – National Academy of Television Arts &amp; Sciences; Cannes Film Festival, 1974. Recent screening at the Museum of Modern Art, New York, December 10, 2012.&nbsp;shown at festiva musica acoustica. mutation exhibited in a concert of works by JEAN-CLAUDE RISSET. OCT 2013</p>
<p><strong>1972 “AFFINITIES” – 4-1/2 Min.</strong></p>
<p>Beethoven’s variations on Mozart’s “la ci darem la Mano” synthesized on computer by F. Richard Moore. A ballet of squares and octagons in many forms, exhibiting a variety of geometric and sometimes sensuous interactions. Whitney Museum of American Art.</p>
<p><strong>1972 “MIS-TAKES” – 3-1/2 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/39002900" width="500" height="331" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Music by Max Mathews. A colorful collage, with a subtle ecology theme, made largely from footage from trial runs of programs used for many of the other films.</p>
<p><strong>1971 “OLYMPIAD” – 3 Min. 20 Sec.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56556015?title=0&amp;byline=0&amp;portrait=0" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Copyright © 1971, 1973, 2003 Lillian Schwartz. All rights reserved.</p>
<p>Study in motion based on Muybridge’s photographs of man-running. “Figures of computer stylized athletes are seen in brilliant hues chasing each other across the screen. Images are then reversed and run across the screen in the other direction; then images are flopped until athletes are running in countless ways … not unlike a pack of humanity on a football field.” Bob Lehmann, Today’s Film-maker magazine. Lincoln Center Animation Festival of the 5th New York Film Festival. Recent screening at the Museum of Modern Art, New York, December 10, 2012.</p>
<p><strong>1971 “UFOs” – 3 Min</strong><br>
<iframe src="http://player.vimeo.com/video/56482927?title=0&amp;byline=0&amp;portrait=0" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>Copyright © 1971, 2013, 2015 Lillian F. Schwartz</p>
<p>Invented first 2D/3D Film without Pixel Shifting.</p>
<p>UFOs involved developing special color filters used on anoptical bench in reshooting original rolls, but to heighten the color saturation exponentially, black frames were inter-edited so that the eyes’ cones were constantly refreshed, the saturation never diminished, and there was a shift in the brain’s alpha rhythm so that observers had conflicting reations. A film breakthrough that literally created an individualized viewing experience.</p>
<p>Music by Emmanuel Ghent. “UFOs proves that computer animation–once a rickety and gimmicky device–is now progressing to the state of an art. The complexity of design and movement, the speed and rhythm, the richness of form and motion, coupled with stroboscopic effects is unsettling. Even more ominously, while design and action are programmed by humans, the ‘result’ in any particular sequence is neither entirely predictable … being created at a rate faster and in concatenations more complex than eye and mind can follow or initiate.” – Amos Vogel, Village Voice. Awards: Ann Arbor-1971; International award-Oberhausen, 1972; 2nd Los Angeles International Film Festival; Museum of Modern Art collection; Commissioned by AT&amp;T. Recent screening at the Museum of Modern Art, New York, December 10, 2012.</p>
<p><strong>1970 “PIXILLATION” – 4 Min.</strong></p>
<p><iframe src="http://player.vimeo.com/video/56480534?title=0&amp;byline=0&amp;portrait=0" width="500" height="367" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br>
Copyright © 1970, 2013 Lillian F. Schwartz All rights reserved.</p>
<p>PIXILLATION occurred at a time when the computer system was linear in time and space; Programs did not yet control pixels as moving, malleable palettes so Lillian F. Schwartz only coded a few lines of computer-generated black and white texture that she intermixed with colored hand animation. She developed an editing technique so that colors between the two media were usually matched, but sometimes mismatched to permit the eye to see an even more saturated color arrangement. The film can be viewed in either 2D or 3D without pixel shifting although one must wear chroma-depth glasses.</p>
<p>“With computer-produced images and Moog-synthesized sound Pixillation is in a sense an introduction to the electronics lab. But its forms are always handsome, its colors bright and appealing, its rhythms complex and inventive.” – Roger Greenspun, N. Y. Times. Golden Eagle-Cine 1971. Moog sound by Gershon Kingsley; Version III: pulls the viewer into a primal experience. Awards:Red Ribbon Award for Special Effects from The National Academy of Television, Arts &amp; Sciences; The Smithsonian Institution and The United States Department of Commerce, Travel Services for Man &amp; His World at the Montreal Expo, ’71; collection The Museum of Modern Art. Commissioned by AT&amp;T. Recent screening at the Museum of Modern Art, New York, December 10, 2012.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Web Browser Engineering (658 pts)]]></title>
            <link>https://browser.engineering/index.html</link>
            <guid>41846780</guid>
            <pubDate>Tue, 15 Oct 2024 09:42:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browser.engineering/index.html">https://browser.engineering/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41846780">Hacker News</a></p>
<div id="readability-page-1" class="page">


<header>


<a href="https://twitter.com/browserbook">Twitter</a> ·
<a href="https://browserbook.substack.com/">Blog</a> ·
<a href="https://patreon.com/browserengineering">Patreon</a> ·
<a href="https://github.com/browserengineering/book/discussions">Discussions</a>
</header>




<nav id="toc">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#part-1-loading-pages" id="toc-part-1-loading-pages">Part
1: Loading Pages</a></li>
<li><a href="#part-2-viewing-documents" id="toc-part-2-viewing-documents">Part 2: Viewing Documents</a></li>
<li><a href="#part-3-running-applications" id="toc-part-3-running-applications">Part 3: Running
Applications</a></li>
<li><a href="#part-4-modern-browsers" id="toc-part-4-modern-browsers">Part 4: Modern Browsers</a></li>
</ul>
</nav>

<p>Web browsers are ubiquitous, but how do they work? This book
explains, building a basic but complete web browser, from networking to
JavaScript, in a couple thousand lines of Python.</p>

<div>
<figure>
<img src="https://browser.engineering/im/cover.jpg" alt="The cover for Web Browser Engineering, from Oxford University Press">

</figure>
<section id="pre-order-web-browser-engineering">
<h2>Pre-order <em>Web Browser Engineering</em></h2>
<p><em>Web Browser Engineering</em> will be published by Oxford
University Press before the end of the year. To get it as soon as it’s
out, <a href="https://global.oup.com/academic/product/web-browser-engineering-9780198913863">pre-order
now!</a></p>
</section>
</div>
<p>Follow this book’s <a href="https://browserbook.substack.com/archive">blog</a> or <a href="https://twitter.com/browserbook">Twitter</a> for updates. You can
also talk about the book with others in our <a href="https://github.com/browserengineering/book/discussions">discussion
forum</a>.</p>
<p>If you are enjoying the book, consider supporting us on <a href="https://patreon.com/browserengineering">Patreon</a>.</p>
<p>Or just <a href="mailto:author@browser.engineering">send us an
email</a>!</p>
<section id="introduction">
<h2>Introduction</h2>
<ol type="1">
<li><a href="https://browser.engineering/preface.html">Preface</a></li>
<li><a href="https://browser.engineering/intro.html">Browsers and the Web</a></li>
<li><a href="https://browser.engineering/history.html">History of the Web</a></li>
</ol>
</section>
<h2 id="part-1-loading-pages">Part 1: Loading Pages</h2>
<ol type="1">
<li><a href="https://browser.engineering/http.html">Downloading Web Pages</a><br>
URLs and HTTP requests</li>
<li><a href="https://browser.engineering/graphics.html">Drawing to the Screen</a><br>
Creating windows and drawing to a canvas</li>
<li><a href="https://browser.engineering/text.html">Formatting Text</a><br>
Word wrapping and line spacing</li>
</ol>
<h2 id="part-2-viewing-documents">Part 2: Viewing Documents</h2>
<ol start="4" type="1">
<li><a href="https://browser.engineering/html.html">Constructing an HTML Tree</a><br>
Parsing and fixing HTML</li>
<li><a href="https://browser.engineering/layout.html">Laying Out Pages</a><br>
Inline and block layout</li>
<li><a href="https://browser.engineering/styles.html">Applying Author Styles</a><br>
Parsing and applying CSS</li>
<li><a href="https://browser.engineering/chrome.html">Handling Buttons and Links</a><br>
Hyperlinks and browser chrome</li>
</ol>
<h2 id="part-3-running-applications">Part 3: Running Applications</h2>
<ol start="8" type="1">
<li><a href="https://browser.engineering/forms.html">Sending Information to Servers</a><br>
Form submission and web servers</li>
<li><a href="https://browser.engineering/scripts.html">Running Interactive Scripts</a><br>
Changing the DOM and reacting to events</li>
<li><a href="https://browser.engineering/security.html">Keeping Data Private</a><br>
Cookies and logins, XSS and CSRF</li>
</ol>
<h2 id="part-4-modern-browsers">Part 4: Modern Browsers</h2>
<ol start="11" type="1">
<li><a href="https://browser.engineering/visual-effects.html">Adding Visual Effects</a><br>
Blending, clipping, and compositing</li>
<li><a href="https://browser.engineering/scheduling.html">Scheduling Tasks and Threads</a><br>
The event loop and the rendering pipeline</li>
<li><a href="https://browser.engineering/animations.html">Animating and Compositing</a><br>
Smooth animations using the GPU</li>
<li><a href="https://browser.engineering/accessibility.html">Making Content Accessible</a><br>
Keyboard input, zooming, and the accessibility tree</li>
<li><a href="https://browser.engineering/embeds.html">Supporting Embedded Content</a><br>
Images, iframes, and scripting</li>
<li><a href="https://browser.engineering/invalidation.html">Reusing Previous Computation</a><br>
Invalidation, editing, and correctness</li>
</ol>













</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Pumpkin – A Modern Minecraft server written in Rust (285 pts)]]></title>
            <link>https://github.com/Snowiiii/Pumpkin</link>
            <guid>41846636</guid>
            <pubDate>Tue, 15 Oct 2024 09:18:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Snowiiii/Pumpkin">https://github.com/Snowiiii/Pumpkin</a>, See on <a href="https://news.ycombinator.com/item?id=41846636">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<p dir="auto"><h2 tabindex="-1" dir="auto">Pumpkin</h2><a id="user-content-pumpkin" aria-label="Permalink: Pumpkin" href="#pumpkin"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Snowiiii/Pumpkin/actions/workflows/rust.yml/badge.svg"><img src="https://github.com/Snowiiii/Pumpkin/actions/workflows/rust.yml/badge.svg" alt="CI"></a>
<a href="https://discord.gg/wT8XjrjKkf" rel="nofollow"><img src="https://camo.githubusercontent.com/044987ec7a0aa910e2c5480a2fce200ff9fa8269a4a2533a2a5c87e06ba3a866/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313236383539323333373434353937383139332e7376673f6c6162656c3d266c6f676f3d646973636f7264266c6f676f436f6c6f723d66666666666626636f6c6f723d373338394438266c6162656c436f6c6f723d364137454332" alt="Discord" data-canonical-src="https://img.shields.io/discord/1268592337445978193.svg?label=&amp;logo=discord&amp;logoColor=ffffff&amp;color=7389D8&amp;labelColor=6A7EC2"></a>
<a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/dc0c67e5ec43ed955dd30f906d640e079aecd82f8be64ef1d7153ca8e6afc894/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63757272656e745f76657273696f6e2d312e32312e312d626c7565"><img src="https://camo.githubusercontent.com/dc0c67e5ec43ed955dd30f906d640e079aecd82f8be64ef1d7153ca8e6afc894/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f63757272656e745f76657273696f6e2d312e32312e312d626c7565" alt="Current version)" data-canonical-src="https://img.shields.io/badge/current_version-1.21.1-blue"></a></p>
</div>
<p dir="auto"><a href="https://snowiiii.github.io/Pumpkin/" rel="nofollow">Pumpkin</a> is a Minecraft server built entirely in Rust, offering a fast, efficient,
and customizable experience. It prioritizes performance and player enjoyment while adhering to the core mechanics of the game.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/71594357/357392192-7e2e865e-b150-4675-a2d5-b52f9900378e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkwMDY1MDIsIm5iZiI6MTcyOTAwNjIwMiwicGF0aCI6Ii83MTU5NDM1Ny8zNTczOTIxOTItN2UyZTg2NWUtYjE1MC00Njc1LWEyZDUtYjUyZjk5MDAzNzhlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE1VDE1MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI0Nzc3NzBlYzI3NzQwMjg0MzJlZjY2YmM4YzBmOGFkZDY0YjExZTFiYzNhMDRjNThkMzg3MGMyOWUzODlmMDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.mLiSktRWZz3v9ppfunfxn0kkOzFN9jmIRh5VoXxUJs4"><img src="https://private-user-images.githubusercontent.com/71594357/357392192-7e2e865e-b150-4675-a2d5-b52f9900378e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjkwMDY1MDIsIm5iZiI6MTcyOTAwNjIwMiwicGF0aCI6Ii83MTU5NDM1Ny8zNTczOTIxOTItN2UyZTg2NWUtYjE1MC00Njc1LWEyZDUtYjUyZjk5MDAzNzhlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEwMTUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMDE1VDE1MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWI0Nzc3NzBlYzI3NzQwMjg0MzJlZjY2YmM4YzBmOGFkZDY0YjExZTFiYzNhMDRjNThkMzg3MGMyOWUzODlmMDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.mLiSktRWZz3v9ppfunfxn0kkOzFN9jmIRh5VoXxUJs4" alt="image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">What Pumpkin wants to achieve</h2><a id="user-content-what-pumpkin-wants-to-achieve" aria-label="Permalink: What Pumpkin wants to achieve" href="#what-pumpkin-wants-to-achieve"></a></p>
<ul dir="auto">
<li><strong>Performance</strong>: Leveraging multi-threading for maximum speed and efficiency.</li>
<li><strong>Compatibility</strong>: Supports the latest Minecraft server version and adheres to vanilla game mechanics.</li>
<li><strong>Security</strong>: Prioritizes security by preventing known exploits.</li>
<li><strong>Flexibility</strong>: Highly configurable with the ability to disable unnecessary features.</li>
<li><strong>Extensibility</strong>: Provides a foundation for plugin development.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What Pumpkin will not</h2><a id="user-content-what-pumpkin-will-not" aria-label="Permalink: What Pumpkin will not" href="#what-pumpkin-will-not"></a></p>
<ul dir="auto">
<li>Be a drop-in replacement for vanilla or other servers</li>
<li>Be compatible with plugins or mods for other servers</li>
<li>Function as a framework for building a server from scratch.</li>
</ul>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Pumpkin is currently under heavy development.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features (WIP)</h2><a id="user-content-features-wip" aria-label="Permalink: Features (WIP)" href="#features-wip"></a></p>
<ul>
<li> Configuration (toml)</li>
<li> Server Status/Ping</li>
<li> Login</li>
<li>Player Configuration
<ul>
<li> Registries (biome types, paintings, dimensions)</li>
<li> Server Brand</li>
<li> Server Links</li>
<li> Set Resource Pack</li>
<li> Cookies</li>
</ul>
</li>
<li>World
<ul>
<li> World Joining</li>
<li> Player Tab-list</li>
<li> World Loading</li>
<li> Entity Spawning</li>
<li> Chunk Loading</li>
<li> World Generation</li>
<li> Chunk Generation</li>
<li> World Borders</li>
<li> World Saving</li>
</ul>
</li>
<li>Player
<ul>
<li> Player Skins</li>
<li> Player Client brand</li>
<li> Player Teleport</li>
<li> Player Movement</li>
<li> Player Animation</li>
<li> Player Inventory</li>
<li> Player Combat</li>
</ul>
</li>
<li>Server
<ul>
<li> Plugins</li>
<li> Query</li>
<li> RCON</li>
<li> Inventories</li>
<li> Particles</li>
<li> Chat</li>
<li> Commands</li>
</ul>
</li>
<li>Proxy
<ul>
<li> Velocity</li>
</ul>
</li>
</ul>
<p dir="auto">Check out our <a href="https://github.com/users/Snowiiii/projects/12/views/3">Github Project</a> to see current progress</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to run" href="#how-to-run"></a></p>
<p dir="auto">See <a href="https://snowiiii.github.io/Pumpkin/about/quick-start.html" rel="nofollow">https://snowiiii.github.io/Pumpkin/about/quick-start.html</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">Contributions are welcome! See <a href="https://github.com/Snowiiii/Pumpkin/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Docs</h2><a id="user-content-docs" aria-label="Permalink: Docs" href="#docs"></a></p>
<p dir="auto">The Documentation of Pumpkin can be found at <a href="https://snowiiii.github.io/Pumpkin/" rel="nofollow">https://snowiiii.github.io/Pumpkin/</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Communication</h2><a id="user-content-communication" aria-label="Permalink: Communication" href="#communication"></a></p>
<p dir="auto">Consider joining our <a href="https://discord.gg/wT8XjrjKkf" rel="nofollow">discord</a> to stay up-to-date on events, updates, and connect with other members.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Funding</h2><a id="user-content-funding" aria-label="Permalink: Funding" href="#funding"></a></p>
<p dir="auto">If you want to fund me and help the project, Check out my <a href="https://github.com/sponsors/Snowiiii">GitHub sponsors</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Thanks</h2><a id="user-content-thanks" aria-label="Permalink: Thanks" href="#thanks"></a></p>
<p dir="auto">A big thanks to <a href="https://wiki.vg/" rel="nofollow">wiki.vg</a> for providing valuable information used in the development of this project.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: MynaUI Icons – 1180 Beautifully Crafted Open Source Icons (102 pts)]]></title>
            <link>https://mynaui.com/icons</link>
            <guid>41846539</guid>
            <pubDate>Tue, 15 Oct 2024 09:03:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mynaui.com/icons">https://mynaui.com/icons</a>, See on <a href="https://news.ycombinator.com/item?id=41846539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Not affiliated with Figma, TailwindCSS or shadcn/ui.</p><nav><a href="https://mynaui.com/legal">Legal</a><a target="_blank" rel="noreferrer noopener" title="X (Twitter)" href="https://twitter.com/praveenjuge/"><svg width="24" height="24" fill="none" stroke="currentColor" stroke-width="1.5" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><path d="m19 4-5.93 6.93M5 20l5.93-6.93m0 0 5.795 6.587c.19.216.483.343.794.343h1.474c.836 0 1.307-.85.793-1.435L13.07 10.93m-2.14 2.14L4.214 5.435C3.7 4.85 4.17 4 5.007 4h1.474c.31 0 .604.127.794.343l5.795 6.587"></path></svg></a><a target="_blank" rel="noreferrer noopener" title="Dribbble" href="https://dribbble.com/praveenjuge/"><svg width="24" height="24" fill="none" stroke="currentColor" stroke-width="1.5" viewBox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><path d="M12 21a9 9 0 1 0 0-18 9 9 0 0 0 0 18"></path><path d="M3.07 10.875c1.7.102 6.2.195 9.08-1.035s5.358-3.492 6.208-4.21"></path><path d="M8.625 3.654c1.409 1.3 4.482 4.61 5.625 7.896 1.143 3.286 1.566 7.326 1.827 8.476"></path><path d="M21 12c-1.313 0-4.936-.495-8.178.928-3.522 1.547-6.072 3.946-7.184 5.438"></path></svg></a></nav></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Drawing Graphics on Apple Vision with the Metal Rendering API (132 pts)]]></title>
            <link>https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api</link>
            <guid>41845646</guid>
            <pubDate>Tue, 15 Oct 2024 06:44:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api">https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api</a>, See on <a href="https://news.ycombinator.com/item?id=41845646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Drawing Graphics on Apple Vision with the Metal Rendering API</h2><a id="user-content-drawing-graphics-on-apple-vision-with-the-metal-rendering-api" aria-label="Permalink: Drawing Graphics on Apple Vision with the Metal Rendering API" href="#drawing-graphics-on-apple-vision-with-the-metal-rendering-api"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ol dir="auto">
<li><a href="#introduction">Introduction</a>
<ol dir="auto">
<li><a href="#why-write-this-article">Why Write This Article?</a></li>
<li><a href="#metal">Metal</a></li>
<li><a href="#compositor-services">Compositor Services</a></li>
</ol>
</li>
<li><a href="#creating-and-configuring-a-layerrenderer">Creating and configuring a <code>LayerRenderer</code></a>
<ol dir="auto">
<li><a href="#variable-rate-rasterization-foveation">Variable Rate Rasterization (Foveation)</a></li>
<li><a href="#organising-the-metal-textures-used-for-presenting-the-rendered-content">Organising the Metal Textures Used for Presenting the Rendered Content</a></li>
</ol>
</li>
<li><a href="#vertex-amplification">Vertex Amplification</a>
<ol dir="auto">
<li><a href="#preparing-to-render-with-support-for-vertex-amplification">Preparing to Render with Support for Vertex Amplification</a></li>
<li><a href="#enabling-vertex-amplification-for-a-render-pass">Enabling Vertex Amplification for a Render Pass</a></li>
<li><a href="#specifying-the-viewport-mappings-for-both-render-targets">Specifying the Viewport Mappings for Both Render Targets</a></li>
<li><a href="#computing-the-view-and-projection-matrices-for-each-eye">Computing the View and Projection Matrices for Each Eye</a></li>
<li><a href="#adding-vertex-amplification-to-our-shaders">Adding Vertex Amplification to our Shaders</a>
<ol dir="auto">
<li><a href="#vertex-shader">Vertex Shader</a></li>
<li><a href="#fragment-shader">Fragment Shader</a></li>
</ol>
</li>
</ol>
</li>
<li><a href="#updating-and-encoding-a-frame-of-content">Updating and Encoding a Frame of Content</a>
<ol dir="auto">
<li><a href="#rendering-on-a-separate-thread">Rendering on a Separate Thread</a></li>
<li><a href="#fetching-a-next-frame-for-drawing">Fetching a Next Frame for Drawing</a></li>
<li><a href="#getting-predicted-render-deadlines">Getting Predicted Render Deadlines</a></li>
<li><a href="#updating-our-app-state-before-rendering">Updating Our App State Before Rendering</a></li>
<li><a href="#waiting-until-optimal-rendering-time">Waiting Until Optimal Rendering Time</a></li>
<li><a href="#frame-submission-phase">Frame Submission Phase</a></li>
</ol>
</li>
<li><a href="#supporting-both-stereoscopic-and-non-vr-display-rendering">Supporting Both Stereoscopic and non-VR Display Rendering</a>
<ol dir="auto">
<li><a href="#two-rendering-paths-layerrendererframedrawable-vs-mtkview">Two Rendering Paths. <code>LayerRenderer.Frame.Drawable</code> vs <code>MTKView</code></a></li>
<li><a href="#adapting-our-vertex-shader">Adapting our Vertex Shader</a></li>
</ol>
</li>
<li><a href="#gotchas">Gotchas</a>
<ol dir="auto">
<li><a href="#cant-render-to-a-smaller-resolution-pixel-buffer-when-foveation-is-enabled">Can't Render to a Smaller Resolution Pixel Buffer when Foveation is Enabled</a></li>
<li><a href="#postprocessing">Postprocessing</a></li>
<li><a href="#true-camera-position">True Camera Position</a></li>
<li><a href="#apple-vision-simulator">Apple Vision Simulator</a></li>
</ol>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">At the time of writing, Apple Vision Pro has been available for seven months, with numerous games released and an increasing number of developers entering this niche. When it comes to rendering, most opt for established game engines like Unity or Apple's high-level APIs like RealityKit. However, there's another option that's been available since the beginning: building your own rendering engine using the Metal API. Though challenging, this approach offers full control over the rendering pipeline, down to each byte and command submitted to the GPU on each frame.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong>: visionOS 2.0 enables rendering graphics with the Metal API and compositing them in <strong>mixed</strong> mode with the user’s surroundings, captured by the device's cameras. This article focuses on developing Metal apps for fully immersive mode, though passthrough rendering will be discussed at the end. At the time of Apple Vision Pro release, visionOS 1.0 allowed for rendering with the Metal API in <strong>immersive</strong> mode only.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why Write This Article?</h3><a id="user-content-why-write-this-article" aria-label="Permalink: Why Write This Article?" href="#why-write-this-article"></a></p>
<p dir="auto">Mainly as a recap to myself of all I have learned. I used all of this while building <a href="https://rayquestgame.com/" rel="nofollow">RAYQUEST</a>, my first game for Apple Vision. I am not going to present any groundbreaking techniques or anything that you can not find in Apple documentation and official examples, aside from some gotchas I discovered while developing my game. In fact, I'd treat this article as an additional reading to the Apple examples. Read them first or read this article first. I will link to Apple's relevant docs and examples as much as possible as I explain the upcomming concepts.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Metal</h3><a id="user-content-metal" aria-label="Permalink: Metal" href="#metal"></a></p>
<p dir="auto">To directly quote Apple <a href="https://developer.apple.com/metal/" rel="nofollow">docs</a>:</p>
<blockquote>
<p dir="auto">Metal is a modern, tightly integrated graphics and compute API coupled with a powerful shading language that is designed and optimized for Apple platforms. Its low-overhead model gives you direct control over each task the GPU performs, enabling you to maximize the efficiency of your graphics and compute software. Metal also includes an unparalleled suite of GPU profiling and debugging tools to help you improve performance and graphics quality.</p>
</blockquote>
<p dir="auto">I will not focus too much on the intristics of Metal in this article, however will mention that the API is mature, well documented and with plenty of tutorials and examples. I personally find it <strong>very nice</strong> to work with. If you want to learn it I suggest you read <a href="https://www.kodeco.com/books/metal-by-tutorials/v4.0" rel="nofollow">this book</a>. It is more explicit than APIs such as OpenGL ES, there is more planning involved in setting up your rendering pipeline and rendering frames, but is still very approachable and more beginner friendly then, say, Vulkan or DirectX12. Furthermore, Xcode has high quality built-in Metal profiler and debugger that allows for inspecting your GPU workloads and your shader inputs, code and outputs.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Compositor Services</h3><a id="user-content-compositor-services" aria-label="Permalink: Compositor Services" href="#compositor-services"></a></p>
<p dir="auto">Compositor Services is a visionOS-specific API that bridges your SwiftUI code with your Metal rendering engine. It enables you to encode and submit drawing commands directly to the Apple Vision displays, which include separate screens for the left and right eye.</p>
<p dir="auto">At the app’s initialization, Compositor Services automatically creates and configures a <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer" rel="nofollow"><code>LayerRenderer</code></a> object to manage rendering on Apple Vision during the app lifecycle. This configuration includes texture layouts, pixel formats, foveation settings, and other rendering options. If no custom configuration is provided, Compositor Services defaults to its standard settings. Additionally, the <code>LayerRenderer</code> supplies timing information to optimize the rendering loop and ensure efficient frame delivery.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Creating and configuring a <code>LayerRenderer</code></h2><a id="user-content-creating-and-configuring-a-layerrenderer" aria-label="Permalink: Creating and configuring a LayerRenderer" href="#creating-and-configuring-a-layerrenderer"></a></p>
<p dir="auto">In our scene creation code, we need to pass a type that adopts the <a href="https://developer.apple.com/documentation/compositorservices/compositorlayerconfiguration" rel="nofollow"><code>CompositorLayerConfiguration</code></a> protocol as a parameter to our scene content. The system will then use that configuration to create a <code>LayerRenderer</code> that will hold information such as the pixel formats of the final color and depth buffers, how the textures used to present the rendered content to Apple Vision displays are organised, whether foveation is enabled and so on. More on all these fancy terms a bit later. Here is some boilerplate code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="struct ContentStageConfiguration: CompositorLayerConfiguration {
  func makeConfiguration(capabilities: LayerRenderer.Capabilities, configuration: inout LayerRenderer.Configuration) {
      // Specify the formats for both the color and depth output textures that Apple Vision will create for us.
      configuration.depthFormat = .depth32Float
      configuration.colorFormat = .bgra8Unorm_srgb

      // TODO: we will adjust the rest of the configuration further down in the article.
  }
}

@main
struct MyApp: App {
  var body: some Scene {
    WindowGroup {
      ContentView()
    }
    ImmersiveSpace(id: &quot;ImmersiveSpace&quot;) {
      CompositorLayer(configuration: ContentStageConfiguration()) { layerRenderer in
         // layerRenderer is what we will use for rendering, frame timing and other presentation info in our engine
      }
    }
  }
}"><pre><span>struct</span> <span>ContentStageConfiguration</span><span>:</span> <span>CompositorLayerConfiguration</span> <span>{</span>
  <span>func</span> makeConfiguration<span>(</span>capabilities<span>:</span> <span>LayerRenderer</span><span>.</span><span>Capabilities</span><span>,</span> configuration<span>:</span> <span>inout</span> <span>LayerRenderer</span><span>.</span><span>Configuration</span><span>)</span> <span>{</span>
      <span>// Specify the formats for both the color and depth output textures that Apple Vision will create for us.</span>
      configuration<span>.</span>depthFormat <span>=</span> <span>.</span>depth32Float
      configuration<span>.</span>colorFormat <span>=</span> <span>.</span>bgra8Unorm_srgb

      <span>// TODO: we will adjust the rest of the configuration further down in the article.</span>
  <span>}</span>
<span>}</span>

<span>@<span>main</span></span>
<span>struct</span> <span>MyApp</span><span>:</span> <span>App</span> <span>{</span>
  <span>var</span> <span><span>body</span></span><span>:</span> <span>some</span> <span>Scene</span> <span>{</span>
    <span>WindowGroup</span> <span>{</span>
      <span>ContentView</span><span>(</span><span>)</span>
    <span>}</span>
    <span>ImmersiveSpace</span><span>(</span>id<span>:</span> <span>"</span><span>ImmersiveSpace</span><span>"</span><span>)</span> <span>{</span>
      <span>CompositorLayer</span><span>(</span>configuration<span>:</span> <span>ContentStageConfiguration</span><span>(</span><span>)</span><span>)</span> <span>{</span> layerRenderer <span>in</span>
         <span>// layerRenderer is what we will use for rendering, frame timing and other presentation info in our engine</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Variable Rate Rasterization (Foveation)</h3><a id="user-content-variable-rate-rasterization-foveation" aria-label="Permalink: Variable Rate Rasterization (Foveation)" href="#variable-rate-rasterization-foveation"></a></p>
<p dir="auto">Next thing we need to set up is whether to enable support for <strong>foveation</strong> in <code>LayerRenderer</code>. Foveation allows us to render at a higher resolution the content our eyes gaze directly at and render at a lower resolution everything else. That is very beneficial in VR as it allows for improved performance.</p>
<p dir="auto">Apple Vision does eye-tracking and foveation automatically for us (in fact, it is not possible for developers to access the user's gaze <strong>at all</strong> due to security concerns). We need to setup our <code>LayerRenderer</code> to support it and we will get it "for free" during rendering. When we render to the <code>LayerRenderer</code> textures, Apple Vision will automatically adjust the resolution to be higher at the regions of the textures we directly gaze at. Here is the previous code that configures the <code>LayerRenderer</code>, updated with support for foveation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="func makeConfiguration(capabilities: LayerRenderer.Capabilities, configuration: inout LayerRenderer.Configuration) {
   // ...

   // Enable foveation
   let foveationEnabled = capabilities.supportsFoveation
   configuration.isFoveationEnabled = foveationEnabled
}"><pre><span>func</span> makeConfiguration<span>(</span>capabilities<span>:</span> <span>LayerRenderer</span><span>.</span><span>Capabilities</span><span>,</span> configuration<span>:</span> <span>inout</span> <span>LayerRenderer</span><span>.</span><span>Configuration</span><span>)</span> <span>{</span>
   <span>// ...</span>

   <span>// Enable foveation</span>
   <span>let</span> <span>foveationEnabled</span> <span>=</span> capabilities<span>.</span>supportsFoveation
   configuration<span>.</span>isFoveationEnabled <span>=</span> foveationEnabled
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Organising the Metal Textures Used for Presenting the Rendered Content</h3><a id="user-content-organising-the-metal-textures-used-for-presenting-the-rendered-content" aria-label="Permalink: Organising the Metal Textures Used for Presenting the Rendered Content" href="#organising-the-metal-textures-used-for-presenting-the-rendered-content"></a></p>
<p dir="auto">We established we need to render our content as two views to both Apple Vision left and right displays. We have three options when it comes to the organization of the textures' layout we use for drawing:</p>
<ol dir="auto">
<li><a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/layout/dedicated" rel="nofollow"><code>LayerRenderer.Layout.dedicated</code></a> - A layout that assigns a separate texture to each rendered view. Two eyes - two textures.</li>
<li><a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/layout/shared" rel="nofollow"><code>LayerRenderer.Layout.shared</code></a> - A layout that uses a single texture to store the content for all rendered views. One texture big enough for both eyes.</li>
<li><a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/layout/layered" rel="nofollow"><code>LayerRenderer.Layout.layered</code></a> - A layout that specifies each view’s content as a slice of a single 3D texture with two slices.</li>
</ol>
<p dir="auto">Which one should you use? Apple official examples use <code>.layered</code>. Ideally <code>.shared</code> or <code>.layered</code>, as having one texture to manage results in fewer things to keep track of, less commands to submit and less GPU context switches. Some important to Apple Vision rendering techniques such as vertex amplification do not work with <code>.dedicated</code>, which expects a separate render pass to draw content for each eye texture, so it is best avoided.</p>
<p dir="auto">Let's update the configuration code once more:</p>
<div dir="auto" data-snippet-clipboard-copy-content="func makeConfiguration(capabilities: LayerRenderer.Capabilities, configuration: inout LayerRenderer.Configuration) {
   // ...

   // Set the LayerRenderer's texture layout configuration
   let options: LayerRenderer.Capabilities.SupportedLayoutsOptions = foveationEnabled ? [.foveationEnabled] : []
   let supportedLayouts = capabilities.supportedLayouts(options: options)

   configuration.layout = supportedLayouts.contains(.layered) ? .layered : .shared
}"><pre><span>func</span> makeConfiguration<span>(</span>capabilities<span>:</span> <span>LayerRenderer</span><span>.</span><span>Capabilities</span><span>,</span> configuration<span>:</span> <span>inout</span> <span>LayerRenderer</span><span>.</span><span>Configuration</span><span>)</span> <span>{</span>
   <span>// ...</span>

   <span>// Set the LayerRenderer's texture layout configuration</span>
   <span>let</span> <span>options</span><span>:</span> <span>LayerRenderer</span><span>.</span><span>Capabilities</span><span>.</span><span>SupportedLayoutsOptions</span> <span>=</span> foveationEnabled ? <span>[</span><span>.</span>foveationEnabled<span>]</span> <span>:</span> <span>[</span><span>]</span>
   <span>let</span> <span>supportedLayouts</span> <span>=</span> capabilities<span>.</span><span>supportedLayouts</span><span>(</span>options<span>:</span> options<span>)</span>

   configuration<span>.</span>layout <span>=</span> supportedLayouts<span>.</span><span>contains</span><span>(</span><span>.</span>layered<span>)</span> ? <span>.</span>layered <span>:</span> <span>.</span>shared
<span>}</span></pre></div>
<p dir="auto">That takes care of the basic configuration for <code>LayerRenderer</code> for rendering our content. We set up our textures' pixel formats, whether to enable foveation and the texture layout to use for rendering. Let's move on to rendering our content.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Vertex Amplification</h3><a id="user-content-vertex-amplification" aria-label="Permalink: Vertex Amplification" href="#vertex-amplification"></a></p>
<p dir="auto">Imagine we have a triangle we want rendered on Apple Vision. A triangle consists of 3 vertices. If we were to render it to a "normal" non-VR display we would submit 3 vertices to the GPU and let it draw them for us. On Apple Vision we have two displays. How do we go about it? A naive way would be to submit two drawing commands:</p>
<ol dir="auto">
<li>Issue draw command <strong>A</strong> to render 3 vertices to the left eye display.</li>
<li>Issue draw command <strong>B</strong> to render the same 3 vertices again, this time for the right eye display.</li>
</ol>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Rendering everything twice and issuing double the amount of commands is required if you have chosen <code>.dedicated</code> texture layout when setting up your <code>LayerRenderer</code>.</p>
</blockquote>
<p dir="auto">This is not optimal as it doubles the commands needed to be submited to the GPU for rendering. A 3 vertices triangle is fine, but for more complex scenes with even moderate amounts of geometry it becomes unwieldly very fast. Thankfully, Metal allows us to submit the 3 vertices once for both displays via a technique called <strong>Vertex Amplification</strong>.</p>
<p dir="auto">Taken from this great <a href="https://developer.apple.com/documentation/metal/render_passes/improving_rendering_performance_with_vertex_amplification" rel="nofollow">article</a> on vertex amplification from Apple:</p>
<blockquote>
<p dir="auto">With vertex amplification, you can encode drawing commands that process the same vertex multiple times, one per render target.</p>
</blockquote>
<p dir="auto">That is very useful to us because one "render target" from the quote above translates directly to one display on Apple Vision. Two displays for the left and right eyes - two render targets to which we can submit the same 3 vertices once, letting the Metal API "amplify" them for us, for free, with hardware acceleration, and render them to both displays <strong>at the same time</strong>. Vertex Amplification is not used only for rendering to both displays on Apple Vision and has its benefits in general graphics techniques such as Cascaded Shadowmaps, where we submit one vertex and render it to multiple "cascades", represented as texture slices, for more adaptive and better looking realtime shadows.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Preparing to Render with Support for Vertex Amplification</h4><a id="user-content-preparing-to-render-with-support-for-vertex-amplification" aria-label="Permalink: Preparing to Render with Support for Vertex Amplification" href="#preparing-to-render-with-support-for-vertex-amplification"></a></p>
<p dir="auto">But back to vertex amplification as means for efficient rendering to both Apple Vision displays. Say we want to render the aforementioned 3 vertices triangle on Apple Vision. In order to render anything, on any Apple device, be it with a non-VR display or two displays set-up, we need to create a <a href="https://developer.apple.com/documentation/metal/mtlrenderpipelinedescriptor" rel="nofollow"><code>MTLRenderPipelineDescriptor</code></a> that will hold all of the state needed to render an object in a single render pass. Stuff like the vertex and fragment shaders to use, the color and depth pixel formats to use when rendering, the sample count if we use MSAA and so on. In the case of Apple Vision, we need to explicitly set the <code>maxVertexAmplificationCount</code> property when creating our <code>MTLRenderPipelineDescriptor</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let pipelineStateDescriptor = MTLRenderPipelineDescriptor()
pipelineStateDescriptor.vertexFunction = vertexFunction
pipelineStateDescriptor.fragmentFunction = fragmentFunction
pipelineStateDescriptor.maxVertexAmplificationCount = 2
// ..."><pre><span>let</span> <span>pipelineStateDescriptor</span> <span>=</span> <span>MTLRenderPipelineDescriptor</span><span>(</span><span>)</span>
pipelineStateDescriptor<span>.</span>vertexFunction <span>=</span> vertexFunction
pipelineStateDescriptor<span>.</span>fragmentFunction <span>=</span> fragmentFunction
pipelineStateDescriptor<span>.</span>maxVertexAmplificationCount <span>=</span> <span>2</span>
<span>// ...</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Enabling Vertex Amplification for a Render Pass</h4><a id="user-content-enabling-vertex-amplification-for-a-render-pass" aria-label="Permalink: Enabling Vertex Amplification for a Render Pass" href="#enabling-vertex-amplification-for-a-render-pass"></a></p>
<p dir="auto">We now have a <code>MTLRenderPipelineDescriptor</code> that represents a graphics pipeline configuration with vertex amplification enabled. We can use it to create a render pipeline, represented by <a href="https://developer.apple.com/documentation/metal/mtlrenderpipelinestate" rel="nofollow"><code>MTLRenderPipelineState</code></a>. Once this render pipeline has been created, the call to render this pipeline needs to be encoded into list of per-frame commands to be submitted to the GPU. What are examples of such commands? Imagine we are building a game with two objects and on each frame we do the following operations:</p>
<ol dir="auto">
<li>Set the clear color before rendering.</li>
<li>Set the viewport size.</li>
<li>Set the render target we are rendering to.</li>
<li>Clear the contents of the render target with the clear color set in step 1.</li>
<li>Set <code>MTLRenderPipelineState</code> for object A as active</li>
<li>Render object A.</li>
<li>Set <code>MTLRenderPipelineState</code> for object B as active</li>
<li>Render object B.</li>
<li>Submit all of the above commands to the GPU</li>
<li>Write the resulting pixel values to some pixel attachment</li>
</ol>
<p dir="auto">All of these rendering commands represent a single <strong>render pass</strong> that happens on each frame while our game is running. This render pass is configured via <a href="https://developer.apple.com/documentation/metal/mtlrenderpassdescriptor" rel="nofollow"><code>MTLRenderPassDescriptor</code></a>. We need to configure the render pass to use foveation and output to two render targets simultaneously.</p>
<ol dir="auto">
<li>Enable foveation by supplying a <a href="https://developer.apple.com/documentation/metal/mtlrasterizationratemap" rel="nofollow"><code>rasterizationRateMap</code></a> property to our <code>MTLRenderPassDescriptor</code>. This property, represented by <a href="https://developer.apple.com/documentation/metal/mtlrasterizationratemap" rel="nofollow"><code>MTLRasterizationRateMap</code></a> is created for us behind the scenes by the Compositor Services. We don't have a direct say in its creation. Instead, we need to query it. On each frame, <code>LayerRenderer</code> will supply us with a <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/frame" rel="nofollow"><code>LayerRenderer.Frame</code></a> object. Among other things, <code>LayerRenderer.Frame</code> holds a <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable" rel="nofollow"><code>LayerRenderer.Drawable</code></a>. More on these objects later. For now, we need to know that this <code>LayerRenderer.Drawable</code> object holds not only the textures for both eyes we will render our content into, but also an array of <code>MTLRasterizationRateMap</code>s that hold the foveation settings for each display.</li>
<li>Set the amount of render targets we will render to by setting the <a href="https://developer.apple.com/documentation/metal/mtlrenderpassdescriptor/1437975-rendertargetarraylength" rel="nofollow"><code>renderTargetArrayLength</code></a> property. Since we are dealing with two displays, we set it to 2.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="// Get the current frame from Compositor Services
guard let frame = layerRenderer.queryNextFrame() else {
   return
}

// Get the current frame drawable
guard let drawable = frame.queryDrawable() else {
   return
}

let renderPassDescriptor = MTLRenderPassDescriptor()
// ...

// both eyes ultimately have the same foveation settings. Let's use the left eye MTLRasterizationRateMap for both eyes
renderPassDescriptor.rasterizationRateMap = drawable.rasterizationRateMaps.first
renderPassDescriptor.renderTargetArrayLength = 2"><pre><span>// Get the current frame from Compositor Services</span>
guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
   <span>return</span>
<span>}</span>

<span>// Get the current frame drawable</span>
guard <span>let</span> drawable <span>=</span> frame<span>.</span><span>queryDrawable</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
   <span>return</span>
<span>}</span>

<span>let</span> <span>renderPassDescriptor</span> <span>=</span> <span>MTLRenderPassDescriptor</span><span>(</span><span>)</span>
<span>// ...</span>

<span>// both eyes ultimately have the same foveation settings. Let's use the left eye MTLRasterizationRateMap for both eyes</span>
renderPassDescriptor<span>.</span>rasterizationRateMap <span>=</span> drawable<span>.</span>rasterizationRateMaps<span>.</span>first
renderPassDescriptor<span>.</span>renderTargetArrayLength <span>=</span> <span>2</span></pre></div>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Turning on foveation prevents rendering to a pixel buffer with smaller resolution than the device display. Certain graphics techniques allow for rendering to a lower resolution pixel buffer and upscaling it before presenting it or using it as an input to another effect. That is a performance optimisation. Apple for example has the <a href="https://developer.apple.com/documentation/metalfx" rel="nofollow">MetalFX</a> upscaler that allows us to render to a smaller pixel buffer and upscale it back to native resolution. That is not possible when rendering on visionOS with foveation enabled due to the <code>rasterizationRateMaps</code> property. That property is set internally by Compositor Services when a new <code>LayerRenderer</code> is created based on whether we turned on the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/configuration-swift.struct/isfoveationenabled" rel="nofollow"><code>isFoveationEnabled</code></a> property in our layer configuration. We don't have a say in the direct creation of the <code>rasterizationRateMaps</code> property. We can not use smaller viewport sizes when rendering to our <code>LayerRenderer</code> textures that have predefined rasterization rate maps because the viewport dimensions will not match. We can not change the dimensions of the predefined rasterization rate maps.</p>
<p dir="auto">With foveation disabled you <strong>can</strong> render to a pixel buffer smaller in resolution than the device display. You can render at, say, 75% of the native resolution and use MetalFX to upscale it to 100%. This approach works on Apple Vision.</p>
</blockquote>
<p dir="auto">Once created with the definition above, the render pass represented by a <a href="https://developer.apple.com/documentation/metal/mtlrendercommandencoder" rel="nofollow"><code>MTLRenderCommandEncoder</code></a>. We use this <code>MTLRenderCommandEncoder</code> to encode our <strong>rendering</strong> commands from the steps above into a <a href="https://developer.apple.com/documentation/metal/mtlcommandbuffer" rel="nofollow"><code>MTLCommandBuffer</code></a> which is submitted to the GPU for execution. For a given frame, after these commands have been issued and submitted by the CPU to the GPU for encoding, the GPU will execute each command in correct order, produce the final pixel values for the specific frame, and write them to the final texture to be presented to the user.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> A game can and often does have multiple render passes per frame. Imagine you are building a first person racing game. The main render pass would draw the interior of your car, your opponents' cars, the world, the trees and so on. A second render pass will draw all of the HUD and UI on top. A third render pass might be used for drawing shadows. A fourth render pass might render the objects in your rearview mirror and so on. All of these render passes need to be encoded and submitted to the GPU on each new frame for drawing.</p>
</blockquote>
<p dir="auto">It is important to note that the commands to be encoded in a <code>MTLCommandBuffer</code> and submitted to the GPU are not only limited to rendering. We can submit "compute" commands to the GPU for general-purpose non-rendering work such as fast number crunching via the <a href="https://developer.apple.com/documentation/metal/mtlcomputecommandencoder" rel="nofollow"><code>MTLComputeCommandEncoder</code></a> (modern techniques for ML, physics, simulations, etc are all done on the GPU nowadays). Apple Vision internal libraries for example use Metal for all the finger tracking, ARKit environment recognition and tracking and so on. However, let's focus only on the rendering commands for now.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Specifying the Viewport Mappings for Both Render Targets</h4><a id="user-content-specifying-the-viewport-mappings-for-both-render-targets" aria-label="Permalink: Specifying the Viewport Mappings for Both Render Targets" href="#specifying-the-viewport-mappings-for-both-render-targets"></a></p>
<p dir="auto">We already created a render pass with vertex amplification enabled. We need to instruct Metal on the correct viewport offsets and sizes for each render target before we render. We need to:</p>
<ol start="2" dir="auto">
<li>Specify the view mappings that hold per-output offsets to a specific render target and viewport.</li>
<li>Specify the viewport sizes for each render target.</li>
</ol>
<p dir="auto">The viewport sizes and view mappings into each render target depend on our textures' layout we specified when creating the <code>LayerRenderer</code> configuration used in Compositor Services earlier in the article. We should <strong>never</strong> hardcode these values ourselves. Instead, we can query this info from the current frame <code>LayerRenderer.Drawable</code>. It provides the information and textures we need to draw into for a given frame of content. We will explore these objects in more detail later on, but the important piece of information is that the <code>LayerRenderer.Drawable</code> we just queried will give us the correct viewport sizes and view mappings for each render target we will draw to.</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Get the current frame from Compositor Services
guard let frame = layerRenderer.queryNextFrame() else {
   return
}

// Get the current frame drawable
guard let drawable = frame.queryDrawable() else {
   return
}

// Creates a MTLRenderCommandEncoder
guard let renderEncoder = commandBuffer.makeRenderCommandEncoder(descriptor: renderPassDescriptor) else {
  return
}

// Query the current frame drawable view offset mappings for each render target
var viewMappings = (0 ..< 2).map {
   MTLVertexAmplificationViewMapping(
     viewportArrayIndexOffset: UInt32($0),
     renderTargetArrayIndexOffset: UInt32($0)
   )
}
// Set the number of amplifications and the correct view offset mappings for each render target
renderEncoder.setVertexAmplificationCount(2, viewMappings: &amp;viewMappings)

let viewports = drawable.views.map { $0.textureMap.viewport }
renderEncoder.setViewports(viewports)

// Encode our rendering commands into the MTLRenderCommandEncoder

// Submit the MTLRenderCommandEncoder to the GPU for execution"><pre><span>// Get the current frame from Compositor Services</span>
guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
   <span>return</span>
<span>}</span>

<span>// Get the current frame drawable</span>
guard <span>let</span> drawable <span>=</span> frame<span>.</span><span>queryDrawable</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
   <span>return</span>
<span>}</span>

<span>// Creates a MTLRenderCommandEncoder</span>
guard <span>let</span> renderEncoder <span>=</span> commandBuffer<span>.</span><span>makeRenderCommandEncoder</span><span>(</span>descriptor<span>:</span> renderPassDescriptor<span>)</span> <span>else</span> <span>{</span>
  <span>return</span>
<span>}</span>

<span>// Query the current frame drawable view offset mappings for each render target</span>
<span>var</span> <span>viewMappings</span> <span>=</span> <span>(</span><span>0</span> <span>..&lt;</span> <span>2</span><span>)</span><span>.</span><span>map</span> <span>{</span>
   <span>MTLVertexAmplificationViewMapping</span><span>(</span>
     viewportArrayIndexOffset<span>:</span> <span>UInt32</span><span>(</span>$0<span>)</span><span>,</span>
     renderTargetArrayIndexOffset<span>:</span> <span>UInt32</span><span>(</span>$0<span>)</span>
   <span>)</span>
<span>}</span>
<span>// Set the number of amplifications and the correct view offset mappings for each render target</span>
renderEncoder<span>.</span><span>setVertexAmplificationCount</span><span>(</span><span>2</span><span>,</span> viewMappings<span>:</span> <span>&amp;</span>viewMappings<span>)</span>

<span>let</span> <span>viewports</span> <span>=</span> drawable<span>.</span>views<span>.</span><span>map</span> <span>{</span> $0<span>.</span>textureMap<span>.</span>viewport <span>}</span>
renderEncoder<span>.</span><span>setViewports</span><span>(</span>viewports<span>)</span>

<span>// Encode our rendering commands into the MTLRenderCommandEncoder</span>

<span>// Submit the MTLRenderCommandEncoder to the GPU for execution</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Computing the View and Projection Matrices for Each Eye</h4><a id="user-content-computing-the-view-and-projection-matrices-for-each-eye" aria-label="Permalink: Computing the View and Projection Matrices for Each Eye" href="#computing-the-view-and-projection-matrices-for-each-eye"></a></p>
<p dir="auto">Okay, we created our <code>LayerRenderer</code> that holds the textures we will render to, enabled foveation, and have vertex amplification enabled. Next we need to compute the correct view and projection matrices <strong>for each eye</strong> to use during rendering. If you have done computer graphics work or used a game engine like Unity, you know that usually we create a virtual camera that sits somewhere in our 3D world, is oriented towards a specific direction, has a specific field of view, a certain aspect ratio, a near and a far plane and other attributes. We use the view and projection matrix of the camera to transform a vertex's 3D position in our game world to clip space, which in turn is is further transformed by the GPU to finally end up in device screen space coordinates.</p>
<p dir="auto">When rendering to a non-VR screen, it is up to us, as programmers, to construct this virtual camera and decide what values all of these properties will have. Since our rendered objects' positions are ultimately presented on a 2D screen that we look at from some distance, these properties do not have to be "physically based" to match our eyes and field of view. We can go crazy with really small range of field of view, use a portrait aspect ratio, some weird projection ("fish eye") and so on for rendering. Point being is that, in non-VR rendering, we are given leeway on how to construct the camera we use depending on the effect and look we are trying to achieve.</p>
<p dir="auto">When rendering on Apple Vision we can not set these camera properties or augment them manually in any way as it might cause sickness. Changing the default camera properties will result in things looking "weird", and not matching our eyes (remember the initial eye setup you had to do when you bought your Apple Vision?). I can't imagine Apple being okay with publishing apps that augment the default camera projections as they might break the immersion, feel "off" and make the product look crappy.</p>
<p dir="auto">My point is that we have to use the projection and view matrices given to us by Apple Vision. We are trying to simulate a world in immersive mode or mix our content with the real environment in mixed mode. It should feel natural to the user, as if she is not even wearing a device. We should not downscale the field of view, change the aspect ratio or mess with any other settings.</p>
<p dir="auto">So on each frame, we need to query 2 view matrices representing each eye's position and orientation in the physical world. Similarly, we need to query 2 perspective projection matrices that encode the <strong>correct</strong> aspect, field of view, near and far planes for each eye from the current frame <code>LayerRenderer.Drawable</code>. Each eye's "view" is represented by <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/view" rel="nofollow"><code>LayerRenderer.Drawable.View </code></a>. Compositor Services provides a distinct view for each eye, i.e. each display. It is up to us to obtain these 4 matrices on each frame from both the left and right eye <code>LayerRenderer.Drawable.View</code> and use them to render our content to both of the displays. These 4 matrices are:</p>
<ol dir="auto">
<li>Left eye view matrix</li>
<li>Left eye projection matrix</li>
<li>Right eye view matrix</li>
<li>Right eye projection matrix</li>
</ol>
<p dir="auto"><h5 tabindex="-1" dir="auto">View Matrices</h5><a id="user-content-view-matrices" aria-label="Permalink: View Matrices" href="#view-matrices"></a></p>
<p dir="auto">These matrices represent each eye's position and orientation <strong>with regards to the world coordinate space</strong>. As you move around your room the view matrices will change. Shorter people will get different view matrices then tall people. You sitting on a couch and looking to the left will produce different view matrices than you standing up and looking to the right.</p>
<p dir="auto">Obtaining the view matrices for both eyes is a 3 step process:</p>
<ol dir="auto">
<li>Obtain Apple Vision view transform <strong>pose</strong> matrix that indicates the device position and orientation in the world coordinate system.</li>
</ol>
<p dir="auto">This is global and not tied to a specific eye. It has nothing do to with Compositor Services or the current frame's <code>LayerRenderer.Drawable</code>. Instead, to obtain it, we need to use ARKit and more specifically the visionOS-specific <a href="https://developer.apple.com/documentation/arkit/worldtrackingprovider" rel="nofollow"><code>WorldTrackingProvider</code></a>, which is a source of live data about the device pose and anchors in a person’s surroundings. Here is some code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// During app initialization
let worldTracking = WorldTrackingProvider()
let arSession = ARKitSession()

// During app update loop
Task {
  do {
    let dataProviders: [DataProvider] = [worldTracking]
    try await arSession.run(dataProviders)
  } catch {
    fatalError(&quot;Failed to run ARSession&quot;)
  }
}

// During app render loop
let deviceAnchor = worldTracking.queryDeviceAnchor(atTimestamp: time)

// Query Apple Vision world position and orientation anchor. If not available for some reason, fallback to an identity matrix
let simdDeviceAnchor = deviceAnchor?.originFromAnchorTransform ?? float4x4.identity"><pre><span>// During app initialization</span>
<span>let</span> <span>worldTracking</span> <span>=</span> <span>WorldTrackingProvider</span><span>(</span><span>)</span>
<span>let</span> <span>arSession</span> <span>=</span> <span>ARKitSession</span><span>(</span><span>)</span>

<span>// During app update loop</span>
<span>Task</span> <span>{</span>
  <span>do</span> <span>{</span>
    <span>let</span> <span>dataProviders</span><span>:</span> <span>[</span><span>DataProvider</span><span>]</span> <span>=</span> <span>[</span>worldTracking<span>]</span>
    <span><span>try</span></span> <span>await</span> arSession<span>.</span><span>run</span><span>(</span>dataProviders<span>)</span>
  <span>}</span> <span>catch</span> <span>{</span>
    <span>fatalError</span><span>(</span><span>"</span><span>Failed to run ARSession</span><span>"</span><span>)</span>
  <span>}</span>
<span>}</span>

<span>// During app render loop</span>
<span>let</span> <span>deviceAnchor</span> <span>=</span> worldTracking<span>.</span><span>queryDeviceAnchor</span><span>(</span>atTimestamp<span>:</span> time<span>)</span>

<span>// Query Apple Vision world position and orientation anchor. If not available for some reason, fallback to an identity matrix</span>
<span>let</span> <span>simdDeviceAnchor</span> <span>=</span> deviceAnchor<span><span>?</span></span><span>.</span>originFromAnchorTransform <span>??</span> float4x4<span>.</span>identity</pre></div>
<p dir="auto"><code>simdDeviceAnchor</code> now holds Apple Vision head transform pose matrix.</p>
<ol start="2" dir="auto">
<li>Obtain the eyes' local transformation matrix</li>
</ol>
<p dir="auto">These matrices specify the position and orientation of the left and right eyes <strong>releative</strong> to the device's pose. Just like any eye-specific information, we need to query it from the current frame's <code>LayerRenderer.Drawable</code>. Here is how we obtain the left and right eyes local view matrices:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewLocalMatrix = drawable.views[0].transform
let rightViewLocalMatrix = drawable.views[1].transform"><pre><span>let</span> <span>leftViewLocalMatrix</span> <span>=</span> drawable<span>.</span><span>views</span><span>[</span><span>0</span><span>]</span><span>.</span>transform
<span>let</span> <span>rightViewLocalMatrix</span> <span>=</span> drawable<span>.</span><span>views</span><span>[</span><span>1</span><span>]</span><span>.</span>transform</pre></div>
<ol start="3" dir="auto">
<li>Multiply the device pose matrix by each eye local transformation matrix to obtain each eye view transform matrix in the world coordinate space.</li>
</ol>
<p dir="auto">To get the final world transformation matrix for each eye we multiply the matrix from step 1. by both eyes' matrices from step 2:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewWorldMatrix = (deviceAnchorMatrix * leftEyeLocalMatrix.transform).inverse
let rightViewWorldMatrix = (deviceAnchorMatrix * rightEyeLocalMatrix.transform).inverse"><pre><span>let</span> <span>leftViewWorldMatrix</span> <span>=</span> <span>(</span>deviceAnchorMatrix <span>*</span> leftEyeLocalMatrix<span>.</span>transform<span>)</span><span>.</span>inverse
<span>let</span> <span>rightViewWorldMatrix</span> <span>=</span> <span>(</span>deviceAnchorMatrix <span>*</span> rightEyeLocalMatrix<span>.</span>transform<span>)</span><span>.</span>inverse</pre></div>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Pay special attention to the <code>.inverse</code> part in the end! That is because Apple Vision expects us to use a reverse-Z projection. This is especially important for passthrough rendering with Metal on visionOS 2.0.</p>
</blockquote>
<p dir="auto">Hopefully this image illustrates the concept:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api/blob/main/vision-pro-matrices.png"><img src="https://github.com/gnikoloff/drawing-graphics-on-apple-vision-with-metal-rendering-api/raw/main/vision-pro-matrices.png" alt="Apple Vision eye matrices illustrated"></a></p>
<p dir="auto">To recap so far, let's refer to the 4 matrices needed to render our content on Apple Vision displays. We already computed the first two, the eyes world view transformation matrices, so let's cross them out from our to-do list:</p>
<ol dir="auto">
<li><del>Left eye view matrix</del></li>
<li><del>Right eye view matrix</del></li>
<li>Left eye projection matrix</li>
<li>Right eye projection matrix</li>
</ol>
<p dir="auto">Two more projection matrices to go.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Left and Right Eyes Projection Matrices</h5><a id="user-content-left-and-right-eyes-projection-matrices" aria-label="Permalink: Left and Right Eyes Projection Matrices" href="#left-and-right-eyes-projection-matrices"></a></p>
<p dir="auto">These two matrices encode the perspective projections for each eye. Just like any eye-specific information, they very much rely on Compositor Services and the current <code>LayerRenderer.Frame</code>. How do we go about computing them?</p>
<p dir="auto">Each <code>LayerRenderer.Drawable.View</code> for both eyes gives us a property called <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/view/4082271-tangents" rel="nofollow"><code>.tangents</code></a>. It represents the values for the angles you use to determine the planes of the viewing frustum. We can use these angles to construct the volume between the near and far clipping planes that contains the scene’s visible content. We will use these tangent values to build the perspective projection matrix for each eye.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> The <code>.tangents</code> property is in fact deprecated on visionOS 2.0 and should not be used in new code. To obtain correct projection matrices for a given eye, one should use the new Compositor Services' <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/computeprojection(convention:viewindex:)" rel="nofollow"><code>.computeProjection</code></a> method. I will still cover doing it via the <code>.tangents</code> property for historical reasons.</p>
</blockquote>
<p dir="auto">Let's obtain the tangent property for both eyes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewTangents = drawable.views[0].tangents
let rightViewTangents = drawable.views[0].tangents"><pre><span>let</span> <span>leftViewTangents</span> <span>=</span> drawable<span>.</span><span>views</span><span>[</span><span>0</span><span>]</span><span>.</span>tangents
<span>let</span> <span>rightViewTangents</span> <span>=</span> drawable<span>.</span><span>views</span><span>[</span><span>0</span><span>]</span><span>.</span>tangents</pre></div>
<p dir="auto">We will also need to get the near and far planes to use in our projections. They are the same for both eyes. We can query them from the current frame's <code>LayerRenderer.Drawable</code> like so:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let farPlane = drawable.depthRange.x
let nearPlane = drawable.depthRange.y"><pre><span>let</span> <span>farPlane</span> <span>=</span> drawable<span>.</span>depthRange<span>.</span>x
<span>let</span> <span>nearPlane</span> <span>=</span> drawable<span>.</span>depthRange<span>.</span>y</pre></div>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Notice that the far plane is encoded in the <code>.x</code> property, while the near plane is in the <code>.y</code> range. That is, and I can not stress it enough, because Apple expects us to use reverse-Z projection matrices.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> At the time of writing this article, at least on visionOS 1.0, the far plane (<code>depthRange.x</code>) in the reverse Z projection is actually positioned at negative infinity. I am not sure if this is the case in visionOS 2.0. Not sure why Apple decided to do this. Leaving it as-is at infiity will break certain techniques (for example subdividing the viewing frustum volume into subparts for Cascaded Shadowmaps). In RAYQUEST I actually artifically overwrite and cap this value at something like -500 before constructing my projection matrices. Remember what I said about never overwriting the default projection matrix attributes Apple Vision gives you? Well, I did it only in this case. It works well for immersive space rendering. I can imagine however that overwriting any of these values is a big no-no for passthrough rendering on visionOS 2.0 (which has a different way of constructing projection matrices for each eye alltogether via the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/computeprojection(convention:viewindex:)" rel="nofollow"><code>.computeProjection</code></a>).</p>
</blockquote>
<p dir="auto">Now that we have both <code>tangents</code> for each eye, we will utilise Apple's <a href="https://developer.apple.com/documentation/spatial" rel="nofollow">Spatial</a> API. It will allow us to create and manipulate 3D mathematical primitives. What we are interested in particular is the <a href="https://developer.apple.com/documentation/spatial/projectivetransform3d" rel="nofollow"><code>ProjectiveTransform3D</code></a> that will allow us to obtain a perspective matrix for each eye given the tangents we queried earlier. Here is how it looks in code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewProjectionMatrix = ProjectiveTransform3D(
  leftTangent: Double(leftViewTangents[0]),
  rightTangent: Double(leftViewTangents[1]),
  topTangent: Double(leftViewTangents[2]),
  bottomTangent: Double(leftViewTangents[3]),
  nearZ: depthRange.x,
  farZ: depthRange.y,
  reverseZ: true
)

let rightViewProjectionMatrix = ProjectiveTransform3D(
  leftTangent: Double(rightViewTangents[0]),
  rightTangent: Double(rightViewTangents[1]),
  topTangent: Double(rightViewTangents[2]),
  bottomTangent: Double(rightViewTangents[3]),
  nearZ: depthRange.x,
  farZ: depthRange.y,
  reverseZ: true
)"><pre><span>let</span> <span>leftViewProjectionMatrix</span> <span>=</span> <span>ProjectiveTransform3D</span><span>(</span>
  leftTangent<span>:</span> <span>Double</span><span>(</span><span>leftViewTangents</span><span>[</span><span>0</span><span>]</span><span>)</span><span>,</span>
  rightTangent<span>:</span> <span>Double</span><span>(</span><span>leftViewTangents</span><span>[</span><span>1</span><span>]</span><span>)</span><span>,</span>
  topTangent<span>:</span> <span>Double</span><span>(</span><span>leftViewTangents</span><span>[</span><span>2</span><span>]</span><span>)</span><span>,</span>
  bottomTangent<span>:</span> <span>Double</span><span>(</span><span>leftViewTangents</span><span>[</span><span>3</span><span>]</span><span>)</span><span>,</span>
  nearZ<span>:</span> depthRange<span>.</span>x<span>,</span>
  farZ<span>:</span> depthRange<span>.</span>y<span>,</span>
  reverseZ<span>:</span> true
<span>)</span>

<span>let</span> <span>rightViewProjectionMatrix</span> <span>=</span> <span>ProjectiveTransform3D</span><span>(</span>
  leftTangent<span>:</span> <span>Double</span><span>(</span><span>rightViewTangents</span><span>[</span><span>0</span><span>]</span><span>)</span><span>,</span>
  rightTangent<span>:</span> <span>Double</span><span>(</span><span>rightViewTangents</span><span>[</span><span>1</span><span>]</span><span>)</span><span>,</span>
  topTangent<span>:</span> <span>Double</span><span>(</span><span>rightViewTangents</span><span>[</span><span>2</span><span>]</span><span>)</span><span>,</span>
  bottomTangent<span>:</span> <span>Double</span><span>(</span><span>rightViewTangents</span><span>[</span><span>3</span><span>]</span><span>)</span><span>,</span>
  nearZ<span>:</span> depthRange<span>.</span>x<span>,</span>
  farZ<span>:</span> depthRange<span>.</span>y<span>,</span>
  reverseZ<span>:</span> true
<span>)</span></pre></div>
<p dir="auto">And that's it! We have obtained all 4 matrices needed to render our content. The global view and projection matrix for the left and the right eyes.</p>
<p dir="auto">Armed with these 4 matrices we can now move on to writing our shaders for stereoscoping rendering.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Adding Vertex Amplification to our Shaders</h4><a id="user-content-adding-vertex-amplification-to-our-shaders" aria-label="Permalink: Adding Vertex Amplification to our Shaders" href="#adding-vertex-amplification-to-our-shaders"></a></p>
<p dir="auto">Usually when rendering objects we need to supply a pair of shaders: the vertex and fragment shader. Let's focus on the vertex shader first.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Vertex Shader</h5><a id="user-content-vertex-shader" aria-label="Permalink: Vertex Shader" href="#vertex-shader"></a></p>
<p dir="auto">If you have done traditional, non-VR non-stereoscoping rendering, you know that you construct a virtual camera, position and orient it in the world and supply it to the vertex shader which in turn multiplies each vertex with the camera view and projection matrices to turn it from local space to clip space. If you made it this far in this article, I assume you have seen this in your shader language of choice:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
   matrix_float4x4 projectionMatrix;
   matrix_float4x4 viewMatrix;
   // ...
} CameraEyeUniforms;

typedef struct {
   float4 vertexPosition [[attribute(0)]];
} VertexIn;

typedef struct {
  float4 position [[position]];
  float2 texCoord [[shared]];
  float3 normal [[shared]];
} VertexOut;

vertex VertexOut myVertexShader(
   Vertex in [[stage_in]],
   constant CameraEyeUniforms &amp;camera [[buffer(0)]]
) {
   VertexOut out = {
      .position = camera.projectionMatrix * camera.viewMatrix * in.vertexPosition,
      .texCoord = /* compute UV */,
      .normal = /* compute normal */
   };
   return out;
}

fragment float4 myFragShader() {
   return float4(1, 0, 0, 1);
}"><pre><span>typedef</span> <span>struct</span> {
   matrix_float4x4 projectionMatrix;
   matrix_float4x4 viewMatrix;
   <span><span>//</span> ...</span>
} CameraEyeUniforms;

<span>typedef</span> <span>struct</span> {
   float4 vertexPosition [[attribute(<span>0</span>)]];
} VertexIn;

<span>typedef</span> <span>struct</span> {
  float4 position [[position]];
  float2 texCoord [[shared]];
  float3 <span>normal</span> [[shared]];
} VertexOut;

vertex VertexOut <span>myVertexShader</span>(
   Vertex in [[stage_in]],
   constant CameraEyeUniforms &amp;camera [[buffer(<span>0</span>)]]
) {
   VertexOut out = {
      .<span>position</span> = camera.<span>projectionMatrix</span> * camera.<span>viewMatrix</span> * in.<span>vertexPosition</span>,
      .<span>texCoord</span> = <span><span>/*</span> compute UV <span>*/</span></span>,
      .<span>normal</span> = <span><span>/*</span> compute normal <span>*/</span></span>
   };
   <span>return</span> out;
}

fragment float4 <span>myFragShader</span>() {
   <span>return</span> <span>float4</span>(<span>1</span>, <span>0</span>, <span>0</span>, <span>1</span>);
}</pre></div>
<p dir="auto">This vertex shader expects a single pair of matrices - the view matrix and projection matrix.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Take a look at the <code>VertexOut</code> definition. <code>texCoord</code> and <code>normal</code> are marked as <code>shared</code>, while <code>position</code> is not. That's because the position values will change depending on the current vertex amplification index. Both eyes have a different pair of matrices to transform each vertex with. The output vertex for the left eye render target will have different final positions than the output vertex for the right eye.
I hope this makes clear why <code>texCoord</code> and <code>normal</code> are <code>shared</code>. The values are <strong>not</strong> view or projection dependent. Their values will always be uniforms across different render targets, regardless of with which eye are we rendering them. For more info check out this <a href="https://developer.apple.com/documentation/metal/render_passes/improving_rendering_performance_with_vertex_amplification" rel="nofollow">article</a>.</p>
</blockquote>
<p dir="auto">Remember we have two displays and two eye views on Apple Vision. Each view holds it's own respective view and projection matrices. We need a vertex shader that will accept 4 matrices - a view and projection matrices for each eye.</p>
<p dir="auto">Let's introduce a new struct:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
   CameraEyeUniforms camUniforms[2];
   // ...
} CameraBothEyesUniforms;"><pre><span>typedef</span> <span>struct</span> {
   CameraEyeUniforms camUniforms[<span>2</span>];
   <span><span>//</span> ...</span>
} CameraBothEyesUniforms;</pre></div>
<p dir="auto">We treat the original <code>CameraUniforms</code> as a single eye and combine both eyes in <code>camUniforms</code>. With that out of the way, we need to instruct the vertex shader which pair of matrices to use exactly. How do we do that? Well, we get a special <code>amplification_id</code> property as input to our shaders. It allows us to query the index of which vertex amplification are we currently executing. We have two amplifications for both eyes, so now we can easily query our <code>camUniforms</code> array! Here is the revised vertex shader:</p>
<div dir="auto" data-snippet-clipboard-copy-content="vertex VertexOut myVertexShader(
   ushort ampId [[amplification_id]],
   Vertex in [[stage_in]],
   constant CameraBothEyesUniforms &amp;bothEyesCameras [[buffer(0)]]
) {
   constant CameraEyeUniforms &amp;camera = bothEyesCameras.camUniforms[ampId];
   VertexOut out = {
      .position = camera.projectionMatrix * camera.viewMatrix * in.vertexPosition;
   };
   return out;
}"><pre>vertex VertexOut <span>myVertexShader</span>(
   <span>ushort</span> ampId [[amplification_id]],
   Vertex in [[stage_in]],
   constant CameraBothEyesUniforms &amp;bothEyesCameras [[buffer(<span>0</span>)]]
) {
   constant CameraEyeUniforms &amp;camera = bothEyesCameras.<span>camUniforms</span>[ampId];
   VertexOut out = {
      .<span>position</span> = camera.<span>projectionMatrix</span> * camera.<span>viewMatrix</span> * in.<span>vertexPosition</span>;
   };
   <span>return</span> out;
}</pre></div>
<p dir="auto">And that's it! Our output textures and render commands have been setup correctly, we have obtained all required matrices and compiled our vertex shader with support for vertex amplification.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Fragment Shader</h5><a id="user-content-fragment-shader" aria-label="Permalink: Fragment Shader" href="#fragment-shader"></a></p>
<p dir="auto">I will skip on the fragment shader code for brevity sake. However will mention a few techniques that require us to know the camera positions and / or matrices in our fragment shader code:</p>
<ol dir="auto">
<li>Lighting, as we need to shade pixels based on the viewing angle between the pixel and the camera</li>
<li>Planar reflections</li>
<li>Many post-processing effects</li>
<li>Non-rendering techniques such as Screen Space Particle Collisions</li>
</ol>
<p dir="auto">In all these cases, we need the two pair of view + projection matrices for each eye. Fragment shaders also get the <code>amplification_id</code> property as input, so we can query the correct matrices in exactly the same way as did in the vertex shader above.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Compute shaders <strong>do not</strong> get an <code>[[amplification_id]]</code> property. That makes porting and running view-dependent compute shaders harder when using two texture views in stereoscoping rendering. When adopting well established algorithms you may need to rethink them to account for two eyes and two textures.</p>
</blockquote>
<p dir="auto">All that's left to do is...</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Updating and Encoding a Frame of Content</h2><a id="user-content-updating-and-encoding-a-frame-of-content" aria-label="Permalink: Updating and Encoding a Frame of Content" href="#updating-and-encoding-a-frame-of-content"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rendering on a Separate Thread</h3><a id="user-content-rendering-on-a-separate-thread" aria-label="Permalink: Rendering on a Separate Thread" href="#rendering-on-a-separate-thread"></a></p>
<p dir="auto">Rendering on a separate thread is recommended in general but especially important on Apple Vision. That is because, during rendering, we will pause the render thread to wait until the optimal rendering time provided to us by Compositor Services. We want the main thread to be able to continue to run, process user inputs, network and so on in the meantime.</p>
<p dir="auto">How do we go about this? Here is some code:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@main
struct MyApp: App {
  var body: some Scene {
    WindowGroup {
      ContentView()
    }
    ImmersiveSpace(id: &quot;ImmersiveSpace&quot;) {
      CompositorLayer(configuration: ContentStageConfiguration()) { layerRenderer in
         let engine = GameEngine(layerRenderer)
         engine.startRenderLoop()
      }
    }
  }
}

class GameEngine {
   private var layerRenderer: LayerRenderer

   public init(_ layerRenderer: LayerRenderer) {
      self.layerRenderer = layerRenderer

       layerRenderer.onSpatialEvent = { eventCollection in
         // process spatial events
       }
   }

   public func startRenderLoop() {
      let renderThread = Thread {
         self.renderLoop()
       }
       renderThread.name = &quot;Render Thread&quot;
       renderThread.start()
   }

   private func renderLoop() {
      while true {
        if layerRenderer.state == .invalidated {
          print(&quot;Layer is invalidated&quot;)
          return
        } else if layerRenderer.state == .paused {
          layerRenderer.waitUntilRunning()
          continue
        } else {
          autoreleasepool {
            // render next frame here
            onRender()
          }
        }
      }
   }

   private func onRender() {
      // ...
   }
}"><pre><span>@<span>main</span></span>
<span>struct</span> <span>MyApp</span><span>:</span> <span>App</span> <span>{</span>
  <span>var</span> <span><span>body</span></span><span>:</span> <span>some</span> <span>Scene</span> <span>{</span>
    <span>WindowGroup</span> <span>{</span>
      <span>ContentView</span><span>(</span><span>)</span>
    <span>}</span>
    <span>ImmersiveSpace</span><span>(</span>id<span>:</span> <span>"</span><span>ImmersiveSpace</span><span>"</span><span>)</span> <span>{</span>
      <span>CompositorLayer</span><span>(</span>configuration<span>:</span> <span>ContentStageConfiguration</span><span>(</span><span>)</span><span>)</span> <span>{</span> layerRenderer <span>in</span>
         <span>let</span> <span>engine</span> <span>=</span> <span>GameEngine</span><span>(</span>layerRenderer<span>)</span>
         engine<span>.</span><span>startRenderLoop</span><span>(</span><span>)</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>

<span>class</span> <span>GameEngine</span> <span>{</span>
   <span>private</span> <span>var</span> <span><span>layerRenderer</span></span><span>:</span> <span>LayerRenderer</span>

   <span>public</span> <span>init</span><span>(</span>_ layerRenderer<span>:</span> <span>LayerRenderer</span><span>)</span> <span>{</span>
      <span>self</span><span>.</span>layerRenderer <span>=</span> layerRenderer

       layerRenderer<span>.</span>onSpatialEvent <span>=</span> <span>{</span> eventCollection <span>in</span>
         <span>// process spatial events</span>
       <span>}</span>
   <span>}</span>

   <span>public</span> <span>func</span> startRenderLoop<span>(</span><span>)</span> <span>{</span>
      <span>let</span> <span>renderThread</span> <span>=</span> <span>Thread</span> <span>{</span>
         <span>self</span><span>.</span><span>renderLoop</span><span>(</span><span>)</span>
       <span>}</span>
       renderThread<span>.</span>name <span>=</span> <span>"</span><span>Render Thread</span><span>"</span>
       renderThread<span>.</span><span>start</span><span>(</span><span>)</span>
   <span>}</span>

   <span>private</span> <span>func</span> renderLoop<span>(</span><span>)</span> <span>{</span>
      while true <span>{</span>
        if layerRenderer<span>.</span>state <span>==</span> <span>.</span>invalidated <span>{</span>
          <span>print</span><span>(</span><span>"</span><span>Layer is invalidated</span><span>"</span><span>)</span>
          <span>return</span>
        <span>}</span> <span>else</span> if layerRenderer<span>.</span>state <span>==</span> <span>.</span>paused <span>{</span>
          layerRenderer<span>.</span><span>waitUntilRunning</span><span>(</span><span>)</span>
          continue
        <span>}</span> <span>else</span> <span>{</span>
          <span>autoreleasepool</span> <span>{</span>
            <span>// render next frame here</span>
            <span>onRender</span><span>(</span><span>)</span>
          <span>}</span>
        <span>}</span>
      <span>}</span>
   <span>}</span>

   <span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
      <span>// ...</span>
   <span>}</span>
<span>}</span></pre></div>
<p dir="auto">We start a separate thread that on each frame checks the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/state-swift.enum" rel="nofollow"><code>LayerRenderer.State</code></a> property. Depending on this property value, it may skip the current frame, quit the render loop entirely or draw to the current frame. The main thread is unaffected and continues running other code and waits for spatial events.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Fetching a Next Frame for Drawing</h3><a id="user-content-fetching-a-next-frame-for-drawing" aria-label="Permalink: Fetching a Next Frame for Drawing" href="#fetching-a-next-frame-for-drawing"></a></p>
<p dir="auto">Remember all the code we wrote earlier that used <code>LayerRenderer.Frame</code>? We obtained the current <code>LayerRenderer.Drawable</code> from it and queried it for the current frame view and projection matrices, view mappings and so on. This <code>LayerRenderer.Frame</code> is obviously different across frames and we have to constantly query it before using it and encoding draw commands to the GPU. Let's expand upon the <code>onRender</code> method from the previous code snippet and query the next frame for drawing:</p>
<div dir="auto" data-snippet-clipboard-copy-content="class GameEngine {
   // ...
   private func onRender() {
      guard let frame = layerRenderer.queryNextFrame() else {
         print(&quot;Could not fetch current render loop frame&quot;)
         return
       }
   }
}"><pre><span>class</span> <span>GameEngine</span> <span>{</span>
   <span>// ...</span>
   <span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
      guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
         <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
         <span>return</span>
       <span>}</span>
   <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Getting Predicted Render Deadlines</h3><a id="user-content-getting-predicted-render-deadlines" aria-label="Permalink: Getting Predicted Render Deadlines" href="#getting-predicted-render-deadlines"></a></p>
<p dir="auto">We need to block our render thread until the optimal rendering time to start the submission phase given to us by Compositor Services. First we will query this optimal rendering time and use it later. Let's expand our <code>onRender</code> method:</p>
<div dir="auto" data-snippet-clipboard-copy-content="private func onRender() {
   guard let frame = layerRenderer.queryNextFrame() else {
      print(&quot;Could not fetch current render loop frame&quot;)
      return
   }
   guard let timing = frame.predictTiming() else {
      return
   }
}"><pre><span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
   guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
      <span>return</span>
   <span>}</span>
   guard <span>let</span> timing <span>=</span> frame<span>.</span><span>predictTiming</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>return</span>
   <span>}</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Updating Our App State Before Rendering</h3><a id="user-content-updating-our-app-state-before-rendering" aria-label="Permalink: Updating Our App State Before Rendering" href="#updating-our-app-state-before-rendering"></a></p>
<p dir="auto">Before doing any rendering, we need to update our app state. What do I mean by this? We usually have to process actions such as:</p>
<ol dir="auto">
<li>User input</li>
<li>Animations</li>
<li>Frustum Culling</li>
<li>Physics</li>
<li>Enemy AI</li>
<li>Audio</li>
</ol>
<p dir="auto">These tasks can be done on the CPU or on the GPU via compute shaders, it does not matter. What does matter is that we need to process and run all of them <strong>before rendering</strong>, because they will dictate what and how exactly do we render. As an example, if you find out that two enemy tanks are colliding during this update phase, you may want to color them differently during rendering. If the user is pointing at a button, you may want to change the appearance of the scene. Apple calls this the <strong>update phase</strong> in their docs btw.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> All of the examples above refer to non-rendering work. However we <strong>can</strong> do rendering during the update phase!
The important distinction that Apple makes is whether we rely on the device anchor information during rendering. It is important to do only rendering work that <strong>does not</strong> depend on the device anchor in the update phase. Stuff like shadow map generation would be a good candidate for this. We render our content from the point of view of the sun, so the device anchor is irrelevant to us during shadowmap rendering.
Remember, it still may be the case that we have to wait until optimal rendering time and skip the current frame. We do not have reliable device anchor information <strong>yet</strong> during the update phase.</p>
</blockquote>
<p dir="auto">We mark the start of the update phase by calling Compositor Service's <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/frame/startupdate()" rel="nofollow"><code>startUpdate</code></a> method. Unsurprisingly, after we are done with the update phase we call <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/frame/endupdate()" rel="nofollow"><code>endUpdate</code></a> that notifies Compositor Services that you have finished updating the app-specific content you need to render the frame. Here is our updated render method:</p>
<div dir="auto" data-snippet-clipboard-copy-content="private func onRender() {
   guard let frame = layerRenderer.queryNextFrame() else {
      print(&quot;Could not fetch current render loop frame&quot;)
      return
   }
   guard let timing = frame.predictTiming() else {
      return
   }

   frame.startUpdate()

   // do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here

   frame.endUpdate()
}"><pre><span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
   guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
      <span>return</span>
   <span>}</span>
   guard <span>let</span> timing <span>=</span> frame<span>.</span><span>predictTiming</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>return</span>
   <span>}</span>

   frame<span>.</span><span>startUpdate</span><span>(</span><span>)</span>

   <span>// do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here</span>

   frame<span>.</span><span>endUpdate</span><span>(</span><span>)</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Waiting Until Optimal Rendering Time</h3><a id="user-content-waiting-until-optimal-rendering-time" aria-label="Permalink: Waiting Until Optimal Rendering Time" href="#waiting-until-optimal-rendering-time"></a></p>
<p dir="auto">We already queried and know the optimal rendering time given to us by Compositor Services. After wrapping up our update phase, we need to block our render thread until the optimal time to start the submission phase of our frame. To block the thread we can use Compositor Service's <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/clock" rel="nofollow"><code>LayerRenderer.Clock</code></a> and more specifically its <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/clock/wait(until:tolerance:)" rel="nofollow"><code>.wait()</code></a> method:</p>
<div dir="auto" data-snippet-clipboard-copy-content="private func onRender() {
   guard let frame = layerRenderer.queryNextFrame() else {
      print(&quot;Could not fetch current render loop frame&quot;)
      return
   }
   guard let timing = frame.predictTiming() else {
      return
   }

   frame.startUpdate()

   // do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here

   frame.endUpdate()

   // block the render thread until optimal input time
   LayerRenderer.Clock().wait(until: timing.optimalInputTime)
}"><pre><span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
   guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
      <span>return</span>
   <span>}</span>
   guard <span>let</span> timing <span>=</span> frame<span>.</span><span>predictTiming</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>return</span>
   <span>}</span>

   frame<span>.</span><span>startUpdate</span><span>(</span><span>)</span>

   <span>// do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here</span>

   frame<span>.</span><span>endUpdate</span><span>(</span><span>)</span>

   <span>// block the render thread until optimal input time</span>
   <span>LayerRenderer</span><span>.</span><span>Clock</span><span>(</span><span>)</span><span>.</span><span>wait</span><span>(</span>until<span>:</span> timing<span>.</span>optimalInputTime<span>)</span>
<span>}</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Frame Submission Phase</h3><a id="user-content-frame-submission-phase" aria-label="Permalink: Frame Submission Phase" href="#frame-submission-phase"></a></p>
<p dir="auto">We have ended our update phase and waited until the optimal rendering time. It is time to start the <strong>submission phase</strong>. <strong>That</strong> is the right time to query the device anchor information, compute the correct view and projection matrices and submit any view-related drawing commands with Metal (basically all the steps we did in the "Vertex Amplification" chapter of this article).</p>
<p dir="auto">Once we have submitted all of our drawing and compute commands to the GPU, we end the frame submission. The GPU will take all of the submited commands and execute them for us.</p>
<div dir="auto" data-snippet-clipboard-copy-content="private func onRender() {
   guard let frame = layerRenderer.queryNextFrame() else {
      print(&quot;Could not fetch current render loop frame&quot;)
      return
   }
   guard let timing = frame.predictTiming() else {
      return
   }

   frame.startUpdate()

   // do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here

   frame.endUpdate()

   LayerRenderer.Clock().wait(until: timing.optimalInputTime)

   frame.startSubmission()

   // we already covered this code, query device anchor position and orientation in physical world
   let deviceAnchor = worldTracking.queryDeviceAnchor(atTimestamp: time)
   let simdDeviceAnchor = deviceAnchor?.originFromAnchorTransform ?? float4x4.identity

   // submit all of your rendering and compute related Metal commands here

   // mark the frame as submitted and hand it to the GPU
   frame.endSubmission()
}"><pre><span>private</span> <span>func</span> onRender<span>(</span><span>)</span> <span>{</span>
   guard <span>let</span> frame <span>=</span> layerRenderer<span>.</span><span>queryNextFrame</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>print</span><span>(</span><span>"</span><span>Could not fetch current render loop frame</span><span>"</span><span>)</span>
      <span>return</span>
   <span>}</span>
   guard <span>let</span> timing <span>=</span> frame<span>.</span><span>predictTiming</span><span>(</span><span>)</span> <span>else</span> <span>{</span>
      <span>return</span>
   <span>}</span>

   frame<span>.</span><span>startUpdate</span><span>(</span><span>)</span>

   <span>// do your game's physics, animation updates, user input, raycasting and non-device anchor related rendering work here</span>

   frame<span>.</span><span>endUpdate</span><span>(</span><span>)</span>

   <span>LayerRenderer</span><span>.</span><span>Clock</span><span>(</span><span>)</span><span>.</span><span>wait</span><span>(</span>until<span>:</span> timing<span>.</span>optimalInputTime<span>)</span>

   frame<span>.</span><span>startSubmission</span><span>(</span><span>)</span>

   <span>// we already covered this code, query device anchor position and orientation in physical world</span>
   <span>let</span> <span>deviceAnchor</span> <span>=</span> worldTracking<span>.</span><span>queryDeviceAnchor</span><span>(</span>atTimestamp<span>:</span> time<span>)</span>
   <span>let</span> <span>simdDeviceAnchor</span> <span>=</span> deviceAnchor<span><span>?</span></span><span>.</span>originFromAnchorTransform <span>??</span> float4x4<span>.</span>identity

   <span>// submit all of your rendering and compute related Metal commands here</span>

   <span>// mark the frame as submitted and hand it to the GPU</span>
   frame<span>.</span><span>endSubmission</span><span>(</span><span>)</span>
<span>}</span></pre></div>
<p dir="auto">And that's it! To recap: we have to use a dedicated render thread for drawing and Compositor Services' methods to control its execution. We are presented with two phases: update and submit. We update our app state in the update phase and issue draw commands with Metal in the submit phase.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supporting Both Stereoscopic and non-VR Display Rendering</h2><a id="user-content-supporting-both-stereoscopic-and-non-vr-display-rendering" aria-label="Permalink: Supporting Both Stereoscopic and non-VR Display Rendering" href="#supporting-both-stereoscopic-and-non-vr-display-rendering"></a></p>
<p dir="auto">As you can see, Apple Vision requires us to <strong>always</strong> think in terms of two eyes and two render targets. Our rendering code, matrices and shaders were built around this concept. So we have to write a renderer that supports "traditional" non-VR and stereoscoping rendering simultaneously. Doing so however requires some careful planning and inevitably some preprocessor directives in your codebase.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Two Rendering Paths. <code>LayerRenderer.Frame.Drawable</code> vs <code>MTKView</code></h3><a id="user-content-two-rendering-paths-layerrendererframedrawable-vs-mtkview" aria-label="Permalink: Two Rendering Paths. LayerRenderer.Frame.Drawable vs MTKView" href="#two-rendering-paths-layerrendererframedrawable-vs-mtkview"></a></p>
<p dir="auto">On Apple Vision, you configure a <code>LayerRenderer</code> at init time and the system gives you <code>LayerRenderer.Frame.Drawable</code> on each frame to draw to. On macOS / iOS / iPadOS and so on, you create a <code>MTKView</code> and a <code>MTKViewDelegate</code> that allows you to hook into the system resizing and drawing updates. In both cases you present your rendered content to the user by drawing to the texture provided by the system for you. How would this look in code? How about this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="open class Renderer {
   #if os(visionOS)
      public var currentDrawable: LayerRenderer.Drawable
   #else
      public var currentDrawable: MTKView
   #endif

   private func renderFrame() {
      // prepare frame, run animations, collect user input, etc

      #if os(visionOS)
         // prepare a two sets of view and projection matrices for both eyes
         // render to both render targets simultaneously 
      #else
         // prepare a view and projection matrix for a single virtual camera
         // render to single render target
      #endif

      // submit your rendering commands to the GPU for rendering
   }
}"><pre><span>open</span> <span>class</span> <span>Renderer</span> <span>{</span>
   <span>#if os(visionOS)</span>
      <span>public</span> <span>var</span> <span><span>currentDrawable</span></span><span>:</span> <span>LayerRenderer</span><span>.</span><span>Drawable</span>
   <span>#else</span>
      <span>public</span> <span>var</span> <span><span>currentDrawable</span></span><span>:</span> <span>MTKView</span>
   <span>#endif</span>

   <span>private</span> <span>func</span> renderFrame<span>(</span><span>)</span> <span>{</span>
      <span>// prepare frame, run animations, collect user input, etc</span>

      <span>#if os(visionOS)</span>
         <span>// prepare a two sets of view and projection matrices for both eyes</span>
         <span>// render to both render targets simultaneously </span>
      <span>#else</span>
         <span>// prepare a view and projection matrix for a single virtual camera</span>
         <span>// render to single render target</span>
      <span>#endif</span>

      <span>// submit your rendering commands to the GPU for rendering</span>
   <span>}</span>
<span>}</span></pre></div>
<p dir="auto">By using preprocessor directives in Swift, we can build our project for different targets. This way we can have two render paths for stereoscoping (Apple Vision) and normal 2D rendering (all other Apple hardware).</p>
<p dir="auto">It should be noted that the 2D render path will omit all of the vertex amplification commands we prepared earlier on the CPU to be submitted to the GPU for drawing. Stuff like <code>renderEncoder.setVertexAmplificationCount(2, viewMappings: &amp;viewMappings)</code> and <code>renderEncoder.setViewports(viewports)</code> is no longer needed.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adapting our Vertex Shader</h3><a id="user-content-adapting-our-vertex-shader" aria-label="Permalink: Adapting our Vertex Shader" href="#adapting-our-vertex-shader"></a></p>
<p dir="auto">The vertex shader we wrote earlier needs some rewriting to support non-Vertex Amplified rendering. That can be done easily with <a href="https://developer.apple.com/documentation/metal/using_function_specialization_to_build_pipeline_variants" rel="nofollow">Metal function constants</a>. Function constants allow us to compile one shader binary and then conditionally enable / disable things in it when using it to build render or compute pipelines. Take a look:</p>
<div dir="auto" data-snippet-clipboard-copy-content="typedef struct {
   matrix_float4x4 projectionMatrix;
   matrix_float4x4 viewMatrix;
} CameraUniforms;

typedef struct {
   CameraEyeUniforms camUniforms[2];
} CameraBothEyesUniforms;

typedef struct {
  float4 position [[position]];
} VertexOut;

constant bool isAmplifiedRendering [[function_constant(0)]];
constant bool isNonAmplifiedRendering = !isAmplifiedRendering;

vertex VertexOut myVertexShader(
   ushort ampId                                    [[amplification_id]],
   constant CameraUniforms &amp;camera                 [[buffer(0), function_constant(isNonAmplifiedRendering]],
   constant CameraBothEyesUniforms &amp;cameraBothEyes [[buffer(1), function_constant(isAmplifiedRendering)]]
) {
   if (isAmplifiedRendering) {
      constant CameraEyeUniforms &amp;camera = bothEyesCameras.uniforms[ampId];
      out.position = camera.projectionMatrix * camera.viewMatrix * vertexPosition;
   } else {
      out.position = camera.projectionMatrix * camera.viewMatrix * vertexPosition;
   }
   return out;
}

fragment float4 myFragShader() {
   return float4(1, 0, 0, 1);
}"><pre><span>typedef</span> <span>struct</span> {
   matrix_float4x4 projectionMatrix;
   matrix_float4x4 viewMatrix;
} CameraUniforms;

<span>typedef</span> <span>struct</span> {
   CameraEyeUniforms camUniforms[<span>2</span>];
} CameraBothEyesUniforms;

<span>typedef</span> <span>struct</span> {
  float4 position [[position]];
} VertexOut;

constant <span>bool</span> isAmplifiedRendering [[function_constant(<span>0</span>)]];
constant <span>bool</span> isNonAmplifiedRendering = !isAmplifiedRendering;

vertex VertexOut <span>myVertexShader</span>(
   <span>ushort</span> ampId                                    [[amplification_id]],
   constant CameraUniforms &amp;camera                 [[buffer(<span>0</span>), function_constant(isNonAmplifiedRendering]],
   constant CameraBothEyesUniforms &amp;cameraBothEyes [[buffer(<span>1</span>), function_constant(isAmplifiedRendering)]]
) {
   <span>if</span> (isAmplifiedRendering) {
      constant CameraEyeUniforms &amp;camera = bothEyesCameras.<span>uniforms</span>[ampId];
      out.<span>position</span> = camera.<span>projectionMatrix</span> * camera.<span>viewMatrix</span> * vertexPosition;
   } <span>else</span> {
      out.<span>position</span> = camera.<span>projectionMatrix</span> * camera.<span>viewMatrix</span> * vertexPosition;
   }
   <span>return</span> out;
}

fragment float4 <span>myFragShader</span>() {
   <span>return</span> <span>float4</span>(<span>1</span>, <span>0</span>, <span>0</span>, <span>1</span>);
}</pre></div>
<p dir="auto">Our updated shader supports both flat 2D and stereoscoping rendering. All we need to set the <code>isAmplifiedRendering</code> function constant when creating a <code>MTLRenderPipelineState</code> and supply the correct matrices to it.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> It is important to note that even when rendering on Apple Vision you may need to render to a flat 2D texture. One example would be drawing shadows, where you put a virtual camera where the sun should be, render to a depth buffer and then project these depth values when rendering to the main displays to determine if a pixel is in shadow or not. Rendering from the Sun point of view in this case does not require multiple render targets or vertex amplification. With our updated vertex shader, we can now support both.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Gotchas</h2><a id="user-content-gotchas" aria-label="Permalink: Gotchas" href="#gotchas"></a></p>
<p dir="auto">I have hinted at some of these throughout the article, but let's recap them and write them down together.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Can't Render to a Smaller Resolution Pixel Buffer when Foveation is Enabled</h3><a id="user-content-cant-render-to-a-smaller-resolution-pixel-buffer-when-foveation-is-enabled" aria-label="Permalink: Can't Render to a Smaller Resolution Pixel Buffer when Foveation is Enabled" href="#cant-render-to-a-smaller-resolution-pixel-buffer-when-foveation-is-enabled"></a></p>
<p dir="auto">Turning on foveation prevents rendering to a pixel buffer with smaller resolution than the device display. Certain graphics techniques allow for rendering to a lower resolution pixel buffer and upscaling it before presenting it or using it as an input to another effect. That is a performance optimisation. Apple for example has the <a href="https://developer.apple.com/documentation/metalfx" rel="nofollow">MetalFX</a> upscaler that allows us to render to a smaller pixel buffer and upscale it back to native resolution. That is not possible when rendering on visionOS with foveation enabled due to the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/drawable/rasterizationratemaps" rel="nofollow"><code>rasterizationRateMaps</code></a> property. That property is set internally by Compositor Services when a new <code>LayerRenderer</code> is created based on whether we turned on the <a href="https://developer.apple.com/documentation/compositorservices/layerrenderer/configuration-swift.struct/isfoveationenabled" rel="nofollow"><code>isFoveationEnabled</code></a> property in our layer configuration. We don't have a say in the direct creation of the <code>rasterizationRateMaps</code> property. We can not use smaller viewport sizes sizes when rendering to our <code>LayerRenderer</code> textures that have predefined rasterization rate maps because the viewport dimensions will not match. We can not change the dimensions of the predefined rasterization rate maps.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Postprocessing</h3><a id="user-content-postprocessing" aria-label="Permalink: Postprocessing" href="#postprocessing"></a></p>
<p dir="auto">Many Apple official examples use compute shaders to postprocess the final scene texture. Implementing sepia, vignette and other graphics techniques happen at the postprocessing stage.</p>
<p dir="auto">Using compute shaders to write to the textures provided by Compositor Services' <code>LayerRenderer</code> is not allowed. That is because these textures do not have the <a href="https://developer.apple.com/documentation/metal/mtltextureusage/1515854-shaderwrite" rel="nofollow"><code>MTLTextureUsage.shaderWrite</code></a> flag enabled. We can not enable this flag post factum ourselves, because they were internally created by Compositor Services. So for postprocessing we are left with spawning fullscreen quads for each display and using fragment shader to implement our postprocessing effects. That is allowed because the textures provided by Compositor Services <strong>do</strong> have the <a href="https://developer.apple.com/documentation/metal/mtltextureusage/1515701-rendertarget" rel="nofollow"><code>MTLTextureUsage.renderTarget</code></a> flag enabled. That is the case with the textures provided by <code>MTKView</code> on other Apple hardware btw.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">True Camera Position</h3><a id="user-content-true-camera-position" aria-label="Permalink: True Camera Position" href="#true-camera-position"></a></p>
<p dir="auto">Remember when we computed the view matrices for both eyes earlier?</p>
<div dir="auto" data-snippet-clipboard-copy-content="let leftViewWorldMatrix = (deviceAnchorMatrix * leftEyeLocalMatrix.transform).inverse
let rightViewWorldMatrix = (deviceAnchorMatrix * rightEyeLocalMatrix.transform).inverse"><pre><span>let</span> <span>leftViewWorldMatrix</span> <span>=</span> <span>(</span>deviceAnchorMatrix <span>*</span> leftEyeLocalMatrix<span>.</span>transform<span>)</span><span>.</span>inverse
<span>let</span> <span>rightViewWorldMatrix</span> <span>=</span> <span>(</span>deviceAnchorMatrix <span>*</span> rightEyeLocalMatrix<span>.</span>transform<span>)</span><span>.</span>inverse</pre></div>
<p dir="auto">These are 4x4 matrices and we can easily extract the translation out of them to obtain each eye's world position. Something like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="extension SIMD4 {
  public var xyz: SIMD3<Scalar> {
    get {
      self[SIMD3(0, 1, 2)]
    }
    set {
      self.x = newValue.x
      self.y = newValue.y
      self.z = newValue.z
    }
  }
}

// SIMD3<Float> vectors representing the XYZ position of each eye
let leftEyeWorldPosition = leftViewWorldMatrix.columns.3.xyz
let rightEyeWorldPosition = rightViewWorldMatrix.columns.3.xyz"><pre><span>extension</span> <span>SIMD4</span> <span>{</span>
  <span>public</span> <span>var</span> <span><span>xyz</span></span><span>:</span> <span>SIMD3</span><span>&lt;</span><span>Scalar</span><span>&gt;</span> <span>{</span>
    <span>get</span> <span>{</span>
      <span>self</span><span>[</span><span>SIMD3</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>)</span><span>]</span>
    <span>}</span>
    <span>set</span> <span>{</span>
      <span>self</span><span>.</span>x <span>=</span> newValue<span>.</span>x
      <span>self</span><span>.</span>y <span>=</span> newValue<span>.</span>y
      <span>self</span><span>.</span>z <span>=</span> newValue<span>.</span>z
    <span>}</span>
  <span>}</span>
<span>}</span>

<span>// SIMD3&lt;Float&gt; vectors representing the XYZ position of each eye</span>
<span>let</span> <span>leftEyeWorldPosition</span> <span>=</span> leftViewWorldMatrix<span>.</span>columns<span>.</span><span>3</span><span>.</span>xyz
<span>let</span> <span>rightEyeWorldPosition</span> <span>=</span> rightViewWorldMatrix<span>.</span>columns<span>.</span><span>3</span><span>.</span>xyz</pre></div>
<p dir="auto">So what is the true camera position? We might need it in our shaders, to implement certain effects, etc.</p>
<p dir="auto">Since the difference between them is small, we can just pick the left eye and use its position as the unified camera world position.</p>
<div dir="auto" data-snippet-clipboard-copy-content="let cameraWorldPosition = leftEyeWorldPosition"><pre><span>let</span> <span>cameraWorldPosition</span> <span>=</span> leftEyeWorldPosition</pre></div>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong>: Why do we have to pick the left eye and not the right one? Xcode simulator uses the left eye to render, while the right one is ignored.</p>
</blockquote>
<p dir="auto">Or we can take their average:</p>
<div dir="auto" data-snippet-clipboard-copy-content="let cameraWorldPosition = (leftEyeWorldPosition + rightEyeWorldPosition) * 0.5"><pre><span>let</span> <span>cameraWorldPosition</span> <span>=</span> <span>(</span>leftEyeWorldPosition <span>+</span> rightEyeWorldPosition<span>)</span> <span>*</span> 0.5</pre></div>
<p dir="auto">I use this approach in my code.</p>
<blockquote>
<p dir="auto"><strong><em>NOTE</em></strong> Using this approach breaks in Xcode's Apple Vision simulator! The simulator renders the scene for just the left eye. You will need to use the <code>#if targetEnvironment(simulator)</code> preprocessor directive to use only the <code>leftEyeWorldPosition</code> when running your code in the simulator.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Apple Vision Simulator</h3><a id="user-content-apple-vision-simulator" aria-label="Permalink: Apple Vision Simulator" href="#apple-vision-simulator"></a></p>
<p dir="auto">First of all, the Simulator renders your scene only for the left eye. It simply ignores the right eye. All of your vertex amplification code will work just fine, but the second vertex amplification will be ignored.</p>
<p dir="auto">Secondly, it also lacks some features (which is the case when simulating other Apple hardware as well). MSAA for example is not allowed so you will need to use the <code>#if targetEnvironment(simulator)</code> directive and implement two code paths for with MSAA and without.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Retreat to Muskworld (137 pts)]]></title>
            <link>https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/</link>
            <guid>41845596</guid>
            <pubDate>Tue, 15 Oct 2024 06:34:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/">https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/</a>, See on <a href="https://news.ycombinator.com/item?id=41845596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img loading="lazy" data-attachment-id="277" data-permalink="https://niedermeyer.io/2024/10/11/the-retreat-to-muskworld/screenshot-2024-10-11-174447/" data-orig-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png" data-orig-size="847,644" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot 2024-10-11 174447" data-image-description="" data-image-caption="" data-medium-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=300" data-large-file="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=847" tabindex="0" role="button" width="847" height="644" src="https://niedermeyer.io/wp-content/uploads/2024/10/screenshot-2024-10-11-174447.png?w=847" alt="A Tesla Cybercab driving past a small town movie set, during Tesla's We, Robot event"><figcaption><em>A totally real robotaxi, driving in an extremely normal town</em></figcaption></figure>



<p>Almost eight years ago, Elon Musk announced that every Tesla made from that moment forward would be capable of Level 5 autonomous driving with nothing more than a software update. It was a pivotal moment in Tesla’s history, committing the company to not just succeed as an electric automaker, but solve one of the most ambitious AI and robotics challenges possible. To create confidence in that staggering aspiration, Tesla released a video of a Model X driving around Palo Alto autonomously to the Rolling Stones’ “Paint it Black,” claiming that the driver behind the wheel was only there “for legal purposes.”</p>



<p>Eight long and hype-filled years later, Tesla is still looking for ways to build confidence in its ability to deliver a “general solution to self-driving” through hype and spectacle, even as companies like Waymo deliver <a href="https://techcrunch.com/2024/08/20/waymo-is-now-giving-100000-robotaxi-rides-week/">the reality of 100,000 driverless taxi rides per week</a>. Rather than meeting the competitive challenge from Waymo with real driverless rides on real public streets, Tesla’s latest ploy for credibility sees the firm retreating ever deeper into fantasy, building what can only be described as a temporary theme park on a movie studio lot for its first ever “driverless” demonstration.</p>



<p>This contrast is instructive. The “Paint It Black” video of eight years ago was no more “real” or “fake” than yesterday’s “We, Robot” demonstration, but at least it had the pretense of reality: it depicted a real car on real roads. Tesla’s latest spectacle likely cost orders of magnitude more to produce, but it didn’t even purport to show any actual real-world capability. The entire thing was pure fantasy, in a contained fantasy world, built on a movie theater lot that exists for the sole purpose of producing such spectacles.</p>



<p>This trajectory, from simulating future capability on public roads to creating a fantasy world for fantasy cars to show off fantasy capabilities, should worry Tesla’s supporters. We can already see Musk retreating into a misinformation-fueled fantasy world every day on Twitter, and the jarring divisiveness of the Cybertruck suggests that his runaway ego is already making Tesla’s products less palatable. If Musk’s retreat into a self-soothing fantasy bubble is also making his hype game less effective, and the 8% drop in Tesla’s stock price suggests that it is, his most important skill set is on the line.</p>



<p>Of course, with Wall Street analysts almost universally declaring themselves “underwhelmed,” it has to be asked: what else could they have possibly expected? Did they really believe that now, after eight years of empty hype, fake statistics, and blown deadlines, Tesla would actually start providing credible evidence to back the litany of bullshit? Having made no effort to explain his chronic inability to meet (let alone stop making) his self-imposed deadlines, and facing no real consequences for nearly a decade of what is either unprecedented public delusion or deception, why on earth would Elon Musk make a serious play for credibility now?</p>



<p>Having drawn a poor hand eight years ago, <a href="https://niedermeyer.io/2024/04/22/no-more-rebuys-mr-musk/">Elon Musk is playing poker the only way he knows how: going all-in on every hand</a>. This strategy has created a confidence game of unprecedented proportions in our financial markets, as every gambler in the casino wants to put a chip on the guy going all-in and winning every time, and only the $13 billion of hung debt for the Twitter deal suggests his hot streak might ever end. All Musk has to do to keep the music playing is to project confidence, which is infinitely easier to do in a studio lot setpiece than out on public roads.</p>



<p>For everyone not locked into this financial-cognitive nightmare, it’s hard to imagine anyone seriously believing that a night of delusional Disney Adult cringe might actually inflate Tesla’s stock beyond the current ~$680 billion valuation. Given that Tesla’s “Full Self-Driving” is already the subject of a multi-year <a href="https://apnews.com/article/tesla-investigations-justice-department-musk-self-driving-29a68864f75c9fabbd04f7a87d169444">federal investigation</a> into <a href="https://www.reuters.com/business/autos-transportation/tesla-autopilot-probe-us-prosecutors-focus-securities-wire-fraud-2024-05-08/">securities and wire fraud</a>, We, Robot’s blatant fantasy-mongering is downright shocking. If anything, showing a low-speed, closed-course theme park ride in order to build confidence around Tesla’s progress toward actual real-world driverless capability is almost too childish to call a fraud. </p>



<p>Ultimately, Musk’s increasingly-degenerate gambling run is slouching toward one last big coinflip: the 2024 presidential election. With <a href="https://www.nytimes.com/2024/10/11/us/politics/elon-musk-donald-trump-pennsylvania.html">Musk going “all-in” on Donald Trump</a>, and <a href="https://nypost.com/2024/10/07/us-news/elon-musk-suggests-hell-be-thrown-in-prison-if-harris-beats-trump-if-he-loses-im-fed/">musing that he will end up in a prison cell if Kamala Harris is elected</a>, it’s clear that his main political issue is his freedom to keep rolling over his endless confidence game without legal consequences. If Trump wins and delivers Musk the impunity he craves, the line between amusement park fantasy and $700 billion self-driving juggernaut will all but disappear, and we will all find ourselves living in Muskworld’s house of mirrors.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Superstitious Users and the FreeBSD Logo (108 pts)]]></title>
            <link>https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html</link>
            <guid>41845427</guid>
            <pubDate>Tue, 15 Oct 2024 06:10:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html">https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006642.html</a>, See on <a href="https://news.ycombinator.com/item?id=41845427">Hacker News</a></p>
<div id="readability-page-1" class="page">
   
<!--htdig_noindex-->
    <b>Brett Glass</b> 
    <a href="mailto:freebsd-chat%40freebsd.org?Subject=Superstitious%20users%20and%20the%20FreeBSD%20logo&amp;In-Reply-To=" title="Superstitious users and the FreeBSD logo">brett at lariat.net
       </a><br>
    <i>Wed Nov 30 02:30:04 UTC 2011</i>
    <ul>
        <li>Previous message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006641.html">Query about about freebsd-chat digestt, Vol 398, Issue 1
</a></li>
        <li>Next message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006643.html">Superstitious users and the FreeBSD logo
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/date.html#6642">[ date ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/thread.html#6642">[ thread ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/subject.html#6642">[ subject ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/author.html#6642">[ author ]</a>
         </li>
       </ul>
    <hr>  
<!--/htdig_noindex-->
<!--beginarticle-->
<pre>Everyone:

I just got a call from the owner of a hotel for which we provide 
hotspot service. She says that a guest spotted the "Powered by 
FreeBSD" logo at the bottom of the login page, and was offended; 
the guest was convinced that either we or the hotel management 
"worshipped the Devil" and refused to stay at the hotel unless the 
logo was removed. The owner could make no headway by explaining 
that the besneakered mascot was a cartoon character and was a 
daemon, not the Devil. And she feared upsetting the guest even more 
if she said that large portions of the same software are inside 
every Mac and iPad. The hotel stands to lose more than $1000 if the 
guest, who had originally planned to stay for a long period, moves out.

One of our tech support people also got a call directly from the 
hotel guest, who claimed that having the logo on the page 
constituted "abuse." The guest also claimed to be "losing money" 
because she wouldn't use the hotspot if there was a "devil" on the 
splash page. He didn't even realize what she was talking about at 
first.... He couldn't imagine why on Earth this person was calling 
him and going on about devils.

Attempts at misguided religious censorship notwithstanding, I don't 
want to see one of my  ISP's customers lose business. And I'd like 
to keep a FreeBSD logo on our hotspot page. Is there artwork that 
doesn't include horned creatures that might offend the ignorant or 
superstitious?

--Brett Glass

</pre>


<!--endarticle-->
<!--htdig_noindex-->
    <hr>
    <ul>
        <!--threads-->
	<li>Previous message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006641.html">Query about about freebsd-chat digestt, Vol 398, Issue 1
</a></li>
	<li>Next message: <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/006643.html">Superstitious users and the FreeBSD logo
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/date.html#6642">[ date ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/thread.html#6642">[ thread ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/subject.html#6642">[ subject ]</a>
              <a href="https://lists.freebsd.org/pipermail/freebsd-chat/2011-November/author.html#6642">[ author ]</a>
         </li>
       </ul>

<hr>
<a href="http://lists.freebsd.org/mailman/listinfo/freebsd-chat">More information about the freebsd-chat
mailing list</a><br>
<!--/htdig_noindex-->

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cheating alleged after men's world conker champion found with steel chestnut (397 pts)]]></title>
            <link>https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut</link>
            <guid>41844545</guid>
            <pubDate>Tue, 15 Oct 2024 03:12:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut">https://www.theguardian.com/sport/2024/oct/14/cheating-alleged-after-mens-world-conker-champion-found-with-steel-chestnut</a>, See on <a href="https://news.ycombinator.com/item?id=41844545">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The World Conker Championships is investigating cheating allegations after the men’s winner was found to have a steel chestnut in his pocket.</p><p>David Jakins won the annual title in Southwick, <a href="https://www.theguardian.com/uk-news/northamptonshire" data-link-name="in body link" data-component="auto-linked-tag">Northamptonshire</a>, on Sunday for the first time after competing since 1977.</p><p>But the 82-year-old was found to have a metal replica in his pocket when he was searched by organisers after his victory.</p><p>The retired engineer has denied using the metal variety in the tournament.</p><p>Jakins was responsible for drilling and inserting strings into other competitors’ chestnuts as the competition’s top judge, known as the “King Conker”.</p><p>Alastair Johnson-Ferguson, who lost in the men’s final against Jakins, said he suspected “foul play”, the Telegraph reported.</p><p>The 23-year-old said: “My conker disintegrated in one hit, and that just doesn’t happen … I’m suspicious of foul play and have expressed my surprise to organisers.”</p><p>Kelci Banschbach, 34, from Indianapolis, defeated the men’s champion in the grand final to become the first American to win the competition. More than 200 people took part.</p><p>Jakins said: “I was found with the steel conker in my pocket, but I only carry [it] around with me for humour value and I did not use it during the event.</p><p>“Yes, I did help prepare the conkers before the tournament. But this isn’t cheating or a fix, and I didn’t mark the strings.”</p><p>St John Burkett, a spokesperson for the World Conker Championships, said the cheating claims were being investigated.</p><p>“Allegations of foul play have been received that somehow King Conker swapped his real conker for the metal one later found in his pocket.</p><p>“Players select conkers from a sack before each round.</p><p>“There are also suggestions that King Conker had marked the strings of harder nuts. We can confirm he was involved in drilling and lacing the nuts before the event.</p><p>“We are investigating.”</p><p>More than 2,000 conkers had been prepared prior to the event.</p></div></div>]]></description>
        </item>
    </channel>
</rss>