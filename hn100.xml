<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 01 Feb 2024 16:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Damn Small Linux 2024 (148 pts)]]></title>
            <link>https://www.damnsmalllinux.org/</link>
            <guid>39215846</guid>
            <pubDate>Thu, 01 Feb 2024 13:47:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.damnsmalllinux.org/">https://www.damnsmalllinux.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39215846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <tbody><tr>
      <td>
	<header>
        	<img id="logo" src="https://www.damnsmalllinux.org/img/logo.png" alt="Damn Small Linux">
        	
	</header>      
	 <br>

<nav>
    <a href="https://www.damnsmalllinux.org/">Home</a>
    <a href="https://www.damnsmalllinux.org/2024-download.html">Download DSL 2024</a>
    <a href="https://www.damnsmalllinux.org/forums">DSL Forums</a>
    <a href="https://www.damnsmalllinux.org/support.html">Support DSL</a>
    <a href="https://www.damnsmalllinux.org/old-index.html">Historic DSL</a>
</nav>

        <p>
          The New DSL 2024 has been reborn as a compact Linux distribution tailored for low-spec x86 computers. It packs a lot of applications into a small package. All the applications are chosen for their functionality, small size, and low dependencies. DSL 2024 also has many text-based applications that make it handy to use in a term window or TTY.
        </p>
	



        <p>
          DSL 2024 currently only ships with two window managers: Fluxbox and JWM. Both are lightweight, fairly intuitive, and easy to use.
        </p>
        <p>
          DSL has three X-based web browsers:
          </p><ul>
	    <li><strong>BadWolf</strong> (light, security-oriented, fully HTML5 compatible)</li>
            <li><strong>Dillo</strong> (super-light GUI browser)</li>
            <li><strong>Links2</strong> (text and light GUI browser)</li>
          </ul>
        
        <p>
          For office applications, DSL has:
          </p><ul>
            <li><strong>AbiWord</strong> word processor</li>
            <li><strong>Gnumeric</strong> spreadsheets</li>
            <li><strong>Sylpheed</strong> email client</li>
            <li><strong>Zathura</strong> PDF viewer</li>
          </ul>
        
        <p>
          For multimedia applications:
          </p><ul>
            <li><strong>MPV</strong> (video and audio)</li>
            <li><strong>XMMS</strong> (a lightweight audio player)</li>
          </ul>
        
        <p>
           Other applications:
         </p><ul> 
          <li><strong>mtPaint</strong> (graphics editing)</li>
          <li><strong>gFTP</strong> (FTP, SFTP, and SCP)</li>
          <li><strong>Leafpad</strong> (quick editing)</li>
          <li><strong>zzzFM</strong> (file manager lifted straight out of antiX)</li>
         </ul>
        
        <p>
          There are three GUI-based games picked because they are fun and relatively light.
        </p>
        <p>
          DSL 2024 is also loaded up with a whole bunch of handy term-based applications:
          </p><ul>
            <li><strong>Ranger</strong> file manager</li>
            <li><strong>VisiData</strong> a powerful CSV and spreadsheet tool</li>
            <li><strong>FZF</strong> fuzzy finder</li>
            <li><strong>Tmux</strong> terminal multiplexer</li>
            <li><strong>Mutt</strong> email client</li>
            <li><strong>Cmus</strong> music player</li>
            <li><strong>CDW</strong> CD burner</li>
            <li><strong>Htop</strong>, an interactive process viewer</li>
            <li><strong>SurfRaw</strong> (with wrapper) to search from the term</li>
            <li><strong>Weather App</strong></li>
            <li><strong>MPV</strong> video/audio player with wrapper</li>
            <li><strong>Vim</strong> and <strong>Nano</strong> for editing</li>
            <li>Five term-based games</li>
            <li>Two term-compatible web browsers: <strong>W3M</strong> and <strong>Links2</strong></li>
            <li>...and much more</li>
          </ul>
        
        <p>
          <strong>Why make a new DSL after all these years?</strong>
        </p>
        <p>
          Creating the original DSL, a versatile 50MB distribution, was a lot of fun and one of the things I am most proud of as a personal accomplishment. However, as a concept, it was in the right place at the right time, and the computer industry has changed a lot since then. While it would be possible to make a bootable Xwindows 50MB distribution today, it would be missing many drivers and have only a handful of very rudimentary applications. People would find such a distribution a fun toy or something to build upon, but it would not be usable for the average computer user out of the gate.
        </p>
        <p>
          Meanwhile, in 2024, nearly everyone has abandoned the sub-700MB size limit to run on computers old enough to not have a DVD and cannot boot off of a USB drive. This is completely understandable because applications, the kernel, and drivers have all mushroomed in their space requirements. Hats off to Puppy Linux for staying one of the few that still offer a full desktop environment in a small size.
        </p>
        <p>
          The new goal of DSL is to pack as much usable desktop distribution into an image small enough to fit on a single CD, or a hard limit of 700MB. This project is meant to service older computers and have them continue to be useful far into the future. Such a notion sits well with my values. I think of this project as my way of keeping otherwise usable hardware out of landfills.
        </p>
        <p>
          As with most things in the GNU/Linux community, this project continues to stand on the shoulders of giants. I am just one guy without a CS degree, so for now, this project is based on antiX 23 i386. AntiX is a fantastic distribution that I think shares much of the same spirit as the original DSL project. AntiX shares pedigree with MEPIS and also leans heavily on the geniuses at Debian. So, this project stands on the shoulders of giants. In other words, DSL 2024 is a humble little project!
        </p>
        <p>
          Though it may seem comparably ridiculous that 700MB is small in 2024 when DSL was 50MB in 2002, I’ve done a lot of hunting to find small footprint applications, and I had to do some tricks to get a workable desktop into the 700MB limit. To get the size down, the ISO only has en_US, en_BG, en_CA, and en_AU locales enabled, and I had to strip the source codes, many man pages, and documentation out. I do provide a download script that will restore all the missing files, and so far, it seems to be working well.
        </p>
        <p>
          Unlike the original DSL, this version has apt fully enabled. So if there is anything you feel is missing, it is very simple to get it installed. I also made an effort to leave as much of the antiX goodness enabled as possible. However, it must be said that DSL is a derivative work but also a reductive work. Some things from antiX may be broken or missing. If you find a bug, it is likely my fault.
        </p>
	<h2>Thank you section:</h2>

	<p>Thank you Debian and antiX for doing all the heavy lifting.</p>

	<p>Thank you <a href="http://www.ibiblio.org/"><img src="http://www.ibiblio.org/hosted/images/sm_hosted_trans.gif" width="80" height="24" alt="hosted by ibiblio"></a> for mirroring the DSL downloads all these years.</p>

	<p>Thank you GPedde at DeviantArt for the beautiful <a href="https://www.deviantart.com/gpedde/art/Maintenance-165229479">wallpaper</a>.

	</p><p>Finally, thank you to the users of DSL for your <a href="https://www.damnsmalllinux.org/forums">feedback</a> and <a href="https://www.damnsmalllinux.org/support.html">support</a>.</p>
      
      </td>
    </tr>
  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The VAE Used for Stable Diffusion Is Flawed (168 pts)]]></title>
            <link>https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/</link>
            <guid>39215242</guid>
            <pubDate>Thu, 01 Feb 2024 12:25:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/">https://old.reddit.com/r/StableDiffusion/comments/1ag5h5s/the_vae_used_for_stable_diffusion_1x2x_and_other/</a>, See on <a href="https://news.ycombinator.com/item?id=39215242">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h5><strong>Short summary for those who are technically inclined:</strong></h5>

<p>CompVis fucked up the KL divergence loss on the KL-F8 VAE that is used by SD1.x, SD2.x, SVD, DALL-E 3, and probably other models.  As a result, the latent space created by it has a massive KL divergence and is smuggling global information about the image through a few pixels.  If you are thinking of using it for training a new, trained-from-scratch foundation model, <strong>don't!</strong> (for the less technically inclined this <strong>does not</strong> mean switch out your VAE for your LoRAs or finetunes, you absolutely do not have the compute power to change the model to a whole new latent space, that would require effectively a full retrain's worth of training.)  SDXL is not subject to this issue because it has its own VAE, which as far as I can tell is trained correctly and does not exhibit the same issues.</p>

<h3>What is the VAE?</h3>

<p>A Variational Autoencoder, in the context of a latent diffusion model, is the eyes and the paintbrush of the model. It translates regular pixel-space images into latent images that are constructed to encode as much of the information about those images as possible into a form that is smaller and easier for the diffusion model to process.</p>

<p>Ideally, we want this "latent space" (as an alternative to <em>pixel space</em>) to be robust to noise (since we're using it with a <em>denoising</em> model), we want latent pixels to be very spatially related to the RGB pixels they represent, and most importantly of all, we want the model to be able to (mostly) accurately reconstruct the image from the latent.  Because of the first requirement, the VAE's encoder doesn't output just a tensor, it outputs a probability distribution that we then sample, and training with samples from this distribution helps the model to be less fragile if we get things a little bit wrong with operations on latents.  For the second requirement, we use Kullback-Leibler (KL) divergence as part of our loss objective: when training the model, we try to push it towards a point where the KL divergence between the latents and a standard Gaussian distribution is minimal -- this effectively ensures that the model's distribution trends toward being roughly equally certain about what each individual pixel should be.  For the third, we simply decode the latent and use any standard reconstruction loss function (LDM used LPIPS and L1 for this VAE).</p>

<h3>What is going on with KL-F8?</h3>

<p>First, I have to show you what a <em>good</em> latent space looks like.  Consider this image: <a href="https://i.imgur.com/DoYf4Ym.jpeg">https://i.imgur.com/DoYf4Ym.jpeg</a></p>

<p>Now, let's encode it using the SDXL encoder (after downscaling the image to shortest side 512) and look at the <em>log variance</em> of the latent distribution (please ignore the plot titles, I was testing something else when I discovered this): <a href="https://i.imgur.com/Dh80Zvr.png">https://i.imgur.com/Dh80Zvr.png</a></p>

<p>Notice how there are some lines, but overall the log variance is fairly consistent throughout the latent.  Let's see how the KL-F8 encoder handles this: <a href="https://i.imgur.com/pLn4Tpv.png">https://i.imgur.com/pLn4Tpv.png</a></p>

<p>This obviously looks very different in many ways, but the most important part right now is that black dot (hereafter referred to as the "black hole").  It's not a brain tumor, though it does look like one, and might as well be the machine-learning equivalent of one.  It's a spot where the VAE is trying to smuggle <strong>global information about the image</strong> through latent space.  This is exactly the problem that KL-divergence loss is supposed to prevent.  Somehow, it didn't.  I suspect this is due to underweighting of the KL loss term.</p>

<h3>What are the implications?</h3>

<p>Somewhat subtle, but significant.  Any latent diffusion model using this encoder is having to do a lot of extra work to get around the bad latent space.</p>

<p>The easiest one to demonstrate, is that the latent space is very fragile in the area of the black hole: <a href="https://i.imgur.com/8DSJYPP.png">https://i.imgur.com/8DSJYPP.png</a></p>

<p>In this image, I overwrote the mean of the latent distribution with random noise in a 3x3 area centered on the black hole, and then decoded it.  I then did the same on another 3x3 area as a control and decoded it.  The right side images are the difference between the altered and unaltered images.  <strong>Altering the latents at the black hole region makes changes across the whole image.</strong>  Altering latents anywhere else causes strictly local changes.  What we would want is strictly local changes.</p>

<p>The most substantial implication of this, is that these are the rules that the Stable Diffusion or other denoiser model has to play by, because this is the latent space it is aligned to.  So, of course, it learns to construct latents that smuggle information: <a href="https://i.imgur.com/WJsWG78.png">https://i.imgur.com/WJsWG78.png</a></p>

<p>This image was constructed by measuring the mean absolute error between the reconstruction of an unaltered latent and one where a <strong>single</strong> latent pixel was zeroed out.  Bright regions are ones where it is smuggling information.</p>

<p>This presents a number of huge issues for a denoiser model, because these latent pixels have a huge impact on the whole image and yet are treated as equal.  The model also has to spend a ton of its parameter space on managing this.</p>

<p>You can reproduce the effects on Stable Diffusion yourself using this code:</p>

<pre><code>import torch
from diffusers import StableDiffusionPipeline
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from copy import deepcopy

pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, safety_checker=None).to("cuda")
pipe.vae.requires_grad_(False)
pipe.unet.requires_grad_(False)
pipe.text_encoder.requires_grad_(False)

def decode_latent(latent):
    image = pipe.vae.decode(latent / pipe.vae.config.scaling_factor, return_dict=False)
    image = pipe.image_processor.postprocess(image[0], output_type="np", do_denormalize=[True] * image[0].shape[0])
    return image[0]

prompt = "a photo of an astronaut riding a horse on mars"

latent = pipe(prompt, output_type="latent").images

original_image = decode_latent(latent)

plt.imshow(original_image)
plt.show()

divergence = np.zeros((64, 64))
for i in tqdm(range(64)):
    for j in range(64):
        latent_pert = deepcopy(latent)
        latent_pert[:, :, i, j] = 0
        md = np.mean(np.abs(original_image - decode_latent(latent_pert)))
        divergence[i, j] = md

plt.imshow(divergence)
plt.show()
</code></pre>

<h3>What is the prognosis?</h3>

<p>Still investigating this!  But I wanted to disclose this sooner rather than later, because I am confident in my findings and what they represent.</p>

<p>SD 1.x, SD 2.x, SVD, and DALL-E 3 (kek) and probably other models are likely affected by this.  You can't just switch them over to another VAE like SDXL's VAE without what might as well be a full retrain.</p>

<p>Let me be clear on this before going any further:  <strong>These models demonstrably work fine.  If it works, it works, and they work.</strong>  This is more of a discussion of the limits and if/when it is worth jumping ship to another model architecture.  I love model necromancy though, so let's talk about salvaging them.</p>

<p>Firstly though, if you are thinking of making a new, trained-from-scratch foundation model with the KL-F8 encoder, <strong>don't!</strong>  Probably tens of millions of dollars of compute have already gone towards models using this flawed encoder, don't add to that number!  At the very least, resume training on it and crank up that KL divergence loss term until the model behaves!  Better yet, do what Stability did and train a new one on a dataset that is better than OpenImages.</p>

<p>I think there is a good chance that the VAE could be fixed without altering the overall latent space too much, which would allow salvaging existing models.  Recall my comparison in that second to last image: even though the VAE was smuggling global features, the reconstruction still looked mostly fine without the smuggled features.  Training a VAE encoder would normally be an extremely bad idea if your expectation is to use the VAE on existing models aligned to it, because you'll be changing the latent space and the model will not be aligned to it anymore.  But if deleting the black hole doesn't destroy the image (which is the case here), it may very well be possible to tune the VAE to no longer smuggle global features while keeping the latent space at least similar enough to where existing models can be made compatible with it with at most a significantly shorter finetune than would normally be needed.  It may also be the case that you can already define a latent image within the decoder's space that is a close reconstruction of a given original without the smuggled features, which would make this task significantly easier.  Personally, I'm not ready to give up on SD1.5 until I have tried this and conclusively failed, because frankly rebuilding all existing tooling would suck, and model necromancy is fun, so I vote model necromancy!  This all needs actual testing though.</p>

<p>I suspect it may be possible to mitigate some of the effects of this within SD's training regimen by somehow scaling reconstruction loss on the latent image by the log variance of the latent.  The black hole is very well defined by the log variance: the VAE is <em>very certain</em> about what those pixels should be compared to other pixels and they accordingly have much more influence on the image that is reconstructed.  If we take the log variance as a proxy for the impact a given pixel has on the model, maybe you can better align the training objective of the denoiser model with the actual impact on latent reconstruction.  This is purely theoretical and needs to be tested first.  Maybe don't do this until I get a chance to try to fix the VAE, because that would just be further committing the model to the existing shitty latent space.</p>

<p>Failing this, people should recognize the limits of SD1.x and move to a new architecture.  It's over a year old, and this field moves fast.  Preferably one that still doesn't require a 3090 to run, please, I have one but not everyone does and what made SD1.5 so well supported was the fact that it could be run and trained on a much broader variety of hardware (being able to train a model in a decent amount of time with less than an A100-80GB would also be great too).  There are a lot of exciting new architectural changes proposed lately with things like Hourglass Diffusion Transformers and the new Karras paper from December to where a much, much better model with a similar compute footprint is certainly possible.  And we knew that SD1.5 would be fully obsolete one day.</p>

<p>I would like to thank my friends who helped me recognize and analyze this problem, and I would also like to thank the Glaze Team, because I accidentally discovered this while analyzing latent images perturbed by Nightshade and wouldn't have found it without them, because I guess nobody else ever had a reason to inspect the log variance of the latent distributions created by the VAE.  I'm definitely going to be performing more validation on models I try to use in my projects from now on after this, Jesus fucking Christ.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: filippo.io/mlkem768 – Post-Quantum Cryptography for the Go Ecosystem (172 pts)]]></title>
            <link>https://words.filippo.io/dispatches/mlkem768/</link>
            <guid>39214743</guid>
            <pubDate>Thu, 01 Feb 2024 11:10:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://words.filippo.io/dispatches/mlkem768/">https://words.filippo.io/dispatches/mlkem768/</a>, See on <a href="https://news.ycombinator.com/item?id=39214743">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p><a href="https://pkg.go.dev/filippo.io/mlkem768?ref=words.filippo.io">filippo.io/mlkem768</a> is a pure-Go implementation of ML-KEM-768 optimized for correctness and readability. ML-KEM (formerly known as Kyber, renamed because we can’t have nice things) is a <a href="https://en.wikipedia.org/wiki/Post-quantum_cryptography?ref=words.filippo.io">post-quantum</a> key exchange mechanism in the process of being standardized by NIST and adopted by most of the industry.</p>
<p>The package amounts to <a href="https://github.com/FiloSottile/mlkem768/blob/main/mlkem768.go?ref=words.filippo.io">~500 lines of code</a>, plus 200 lines of comments, and 650 lines of tests. It has no dependencies except for golang.org/x/crypto/sha3. It’s meant for <a href="https://go.dev/cl/550215?ref=words.filippo.io">upstreaming</a> into the Go standard library (initially as an internal-only package used in an opt-in crypto/tls experiment) and was designed to provide high security assurance through ease of review, simplicity, and thorough testing.</p>
<p>I livecoded part of its development <a href="https://twitch.tv/filosottile?ref=words.filippo.io">on Twitch</a>, and you can watch <a href="https://www.youtube.com/watch?v=MyB7A93C-V0&amp;ref=words.filippo.io">the replay on YouTube</a>.</p>
<p>Unlike most other implementations, this code was not ported from the reference pq-crystals library, but written from scratch not having ever closely read other codebases. This was an intentional exercise in spec validation, to show it is possible to produce an interoperable implementation from the specification alone.</p>
<p>The <a href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.203.ipd.pdf?ref=words.filippo.io">FIPS 203 document</a> turned out to be an excellent implementation guide, with detailed pseudo-code, exhaustive definitions, and consistent type information. (This is something I would like to ask of any large specification document: define your types and use them and denote them!) To make the code both easier to review and better as a learning resource, function and variable names, and even operation ordering, are carefully picked to mirror the FIPS specification.</p>
<p>The specification actually requires fairly limited math background, but to facilitate the work of implementers, I wrote up <a href="https://words.filippo.io/dispatches/kyber-math/">Enough Polynomials and Linear Algebra to Implement Kyber</a>.</p>
<p>Beyond that, the only parts left as an exercise to the reader were</p>
<ol>
<li>implementing arithmetic modulo the prime 3329;</li>
<li>concretely implementing the compress and decompress functions mapping values [0, 3329) to and from [0, 2ᵈ); and</li>
<li>ensuring constant time operations.</li>
</ol>
<p>Modulo arithmetic was reasonably easy, as we all collectively learned a lot about finite field arithmetic through years of RSA and elliptic curve implementations. The small prime actually makes the task feel unnaturally simple.</p>
<p><a href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.203.ipd.pdf?ref=words.filippo.io#equation.4.5">Compression and decompression</a> turned out to be the most difficult part of the project. The specification defines them in abstract terms as fractions and rounding rules—“just” compute (2ᵈ/q)·x or (q/2ᵈ)·y and round to the closest integer—but in practice we need to implement them with constant time arithmetic and bitwise operations! In my <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/n8dsN_aMsa8/m/NHO2lNyWAAAJ?ref=words.filippo.io">public comments</a> I pointed out that having each implementation figure out a strategy is risky and redundant. I was more correct than I thought: it turned out that <a href="https://github.com/pq-crystals/kyber/commit/dda29cc63af721981ee2c831cf00822e69be3220?ref=words.filippo.io">the reference implementation and ~every implementation ported from it used a division</a> which depending on compiler optimizations and platform might result in a DIV instruction, which is variable-time even when the divisor is fixed. This package was unaffected, because it used <a href="https://www.nayuki.io/page/barrett-reduction-algorithm?ref=words.filippo.io">Barrett reduction</a> from the start, like BoringSSL.</p>
<p>You can read <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/n8dsN_aMsa8/m/NHO2lNyWAAAJ?ref=words.filippo.io">the rest of my formal public comments</a> on the pqc-forum mailing list.</p>
<p>Readability was a major goal of the implementation, and it was pursued <s>even</s> especially for complex functions like compression and decompression. A readable implementation has two purposes: first, it allows effective review, both during the code review process and later by interested researchers, improving security; second, it serves as an educational resource for the next generation of maintainers and cryptography engineers (or curious nerds). Reading the Go cryptography standard library is how I got started on the path that led me here, so it is especially important to me to preserve and improve it as a learning resource. It’s obviously subjective, but I believe this to be the most understandable public ML-KEM/Kyber implementation. Compare for example <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/mlkem768.go?ref=words.filippo.io#L394-L442">our compression/decompression functions</a> with <a href="https://github.com/pq-crystals/kyber/blob/11d00ff1f20cfca1f72d819e5a45165c1e0a2816/ref/poly.c?ref=words.filippo.io#L9-L112">the reference implementation</a>.</p>
<p>Sometimes improving readability and reviewability means making code longer and less reusable: for example for ML-KEM-768 we need to serialize 1-, 4-, 10-, and 12-bit integers in a packed format. A universal 1-to-12 bit encoder and decoder is a pretty gnarly piece of code to write correctly, but each of those four sizes are actually pretty easy to write a dedicated encoder/decoder for.<sup><a href="#fn1" id="fnref1">[1]</a></sup> This is why we have <code>ringCompressAndEncode1/4/10</code> etc. instead of a single universal function. This also made it easy to work some special required checks into the 12-bit decoder.</p>
<p>This, by the way, was only possible because we targeted ML-KEM-768 specifically, or we’d have had to implement 5- and 11-bit encodings, as well. ML-KEM is specified at three security levels (-512, -768, and -1024). However, the Kyber team recommends using -768 over -512 for a more conservative security margin against novel cryptanalysis, while -1024 exists only for the same reasons 256-bit security levels exist: compliance and blind <a href="https://www.imperialviolet.org/2014/05/25/strengthmatching.html?ref=words.filippo.io">strength matching</a>. Most protocols being tested or standardized coalesced around ML-KEM-768, so targeting only that improves not only readability, but also security (because there are fewer moving parts), and performance (because we can optimize allocation sizes, iteration counts, and encoding algorithms) at little to no cost.</p>
<p>After readability, testing is the main component in this package’s high security assurance strategy. Besides checking that key generation, encapsulation, and decapsulation round-trip correctly, and maintaining a test coverage of 95%+, we</p>
<ul>
<li>ensure interoperability with <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/testdata/vectors.json?ref=words.filippo.io">test vectors obtained from NIST and other implementations</a>;</li>
<li>exhaustively test every input combination for base field arithmetic operations (addition, subtraction, and multiplication modulo 3329) against expected values computed trivially with variable-time operations;</li>
<li>exhaustively test compression and decompression against math/big.Rat (contributed by David Buchanan);</li>
<li>test that pre-computed constants match their definition;</li>
<li>check that incorrect lengths (both long and short) cause the appropriate error for every input of every function;</li>
<li>run an extensive set of reusable test vectors we developed (see below);</li>
<li>run test vectors <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/aCAX-2QrUFw/m/hy5gwcESAAAJ?ref=words.filippo.io">provided by Sophie Schmieg</a> which will be eventually included in <a href="https://github.com/google/wycheproof?ref=words.filippo.io">Wycheproof</a>.</li>
</ul>
<p>Our test vectors are designed to be reusable by other implementations, and <a href="https://c2sp.org/CCTV/ML-KEM?ref=words.filippo.io">are published as part of the CCTV project</a> along with detailed <a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#intermediate-values">intermediate values</a> for testing and debugging each intermediate step and partial algorithm, which we used during development. There are different sets of tests vectors, each designed to reach different edge cases.</p>
<ul>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#bad-encapsulation-keys">Negative test vectors</a> provide invalid encapsulation keys, where the coefficients are higher than 3329. These were often requested, since all the test vectors from the Kyber and NIST teams are for regular, correct inputs. These vectors individually test every value from 3329 to 2¹²-1 and every coefficient location, sharing the remaining coefficients so they compress from 1–3 MiB down to 12–28 KiB.</p>
</li>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#unlucky-ntt-sampling-vector">“Unlucky” vectors</a> require an unusually large number of XOF reads. Kyber samples a matrix from a portion of public keys<sup><a href="#fn2" id="fnref2">[2]</a></sup> with <em>rejection sampling</em>: it gets a random value between 0 and 2 ¹²-1 and checks if it’s less than 3329, if not, it tries again. The amount of bytes needed to sample a matrix depends on how lucky you get with the sampling, and that’s a random function of the public key component. These vectors are regular public keys and require reading more than 575 bytes from the SHAKE-128 XOF in SampleNTT, which would ordinarily happen with probability 2⁻³⁸. Sophie’s vectors were bruteforced further, and require up to 591 bytes.</p>
<blockquote>
<p>At this point I would like to thank our detection and response team for not killing my job(s) hashing vast amounts of random seeds and looking for zeroes in the output. — Sophie Schmieg</p>
</blockquote>
</li>
<li>
<p>Special vectors <a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#strcmp-vectors">fail if strcmp is used</a> in ML-KEM.Decaps. In ML-KEM.Decaps the ciphertext is compared with the output of K-PKE.Encrypt for implicit rejection. If an implementation were to use <code>strcmp()</code> for that comparison it would fail to reject some ciphertexts if a zero byte terminates the comparison early. This one I hope is going to sit as a silent trap for years—who would use <code>strcmp()</code> in cryptographic code—and then ruthlessly kill a vulnerability, because of course someone will.</p>
</li>
<li>
<p><a href="https://github.com/C2SP/CCTV/blob/main/ML-KEM/README.md?ref=words.filippo.io#accumulated-pq-crystals-vectors">Accumulated vectors</a> (derived from the reference pq-crystals implementation) allow testing randomly reachable edge cases without checking in large amounts of data. The reference implementation of Kyber includes a <code>test_vectors.c</code> program that generates 300MB of random vectors. I had no intention of checking in the output or compiling C, but since they are just randomly generated vectors, we can regenerate them in our tests from the deterministic RNG (SHAKE-128 with an empty input) and check they hash to an expected value. We can even take it further, and produce hashes for a million random tests, beyond the 10k they generate.</p>
</li>
</ul>
<p>I am happy to report that none of the tests, many introduced after completion of the implementation, identified any issues in filippo.io/mlkem768. There is <a href="https://github.com/C2SP/CCTV/issues/7?ref=words.filippo.io">at least one reported instance</a> of the negative vectors identifying a defect in a major implementation, though.</p>
<p>Performance is not a primary goal (neither of this package nor of <a href="https://go.dev/design/cryptography-principles?ref=words.filippo.io">the Go cryptography packages</a>) but the package needs to be fast enough to be useful. Thankfully, ML-KEM is pretty fast, to the point that this simple implementation is competitive with our assembly-optimized P-256 and X25519 implementations.</p>
<p>To compare apples to apples, note that we need to compare the whole operation that each side needs to perform for key establishment: for ECDH, two scalar multiplications (one of them by the fixed base point); for KEMs, key generation and decapsulation on one side, and encapsulation on the other. ECDH is symmetrical, ML-KEM key establishment is not.</p>
<p>The ECDH benchmarks below already include the two scalar multiplications, while the mlkem768 benchmarks are split as key generation and decapsulation under “Alice” and encapsulation under “Bob”. Since decapsulation includes a full encryption (to check the resulting ciphertext matches the input), Alice takes a lot longer than Bob: the latter does an encryption, while the former does an encryption, a decryption, and a key generation.</p>
<p>All in all, “Bob” is as fast as our X25519 or P-256, while “Alice” takes less than twice. Compared to some of the fastest ML-KEM implementations out there (BoringSSL and libcrux), this package takes approximately double the time. For such a simple and unoptimized implementation, this is more than satisfactory.</p>
<pre><code>goos: darwin
goarch: arm64
cpu: Intel(R) Core(TM) i5-7400 CPU @ 3.00GHz
pkg: crypto/ecdh
                         │   sec/op    │
ECDH/P256-8                49.43µ ± 0%
ECDH/X25519-8              77.46µ ± 0%

pkg: filippo.io/mlkem768
                         │   sec/op    │
RoundTrip/Alice-8          109.4µ ± 0%
RoundTrip/Bob-8            56.19µ ± 0%

goos: linux
goarch: amd64
pkg: crypto/ecdh
                         │   sec/op    │
ECDH/P256-4                78.88µ ± 1%
ECDH/X25519-4              115.6µ ± 2%

pkg: filippo.io/mlkem768
                         │   sec/op    │
RoundTrip/Alice-4          223.8µ ± 2%
RoundTrip/Bob-4            114.7µ ± 1%
</code></pre>
<p>The performance wasn’t entirely free. In general, I followed high-performance Go programming patterns, trying for example to minimize heap allocations. Next, I <a href="https://go.dev/cl/544817/2?ref=words.filippo.io">reworked the x/crypto/sha3 package</a> so it could be used without any heap allocation thanks to <a href="https://words.filippo.io/efficient-go-apis-with-the-inliner/">the mid-stack inlining trick</a>. However, I haven’t merged those changes yet and they are not included in the benchmarks above, because they have a negative effect on Apple M2 processors. No idea why yet.</p>
<pre><code>goos: darwin
goarch: arm64
pkg: filippo.io/mlkem768
                  │   sec/op    │   sec/op     vs base                │
RoundTrip/Alice-8   109.4µ ± 0%   121.3µ ± 1%  +10.91% (p=0.000 n=10)
RoundTrip/Bob-8     56.19µ ± 0%   59.94µ ± 2%   +6.66% (p=0.000 n=10)

goos: linux
goarch: amd64
                  │   sec/op    │   sec/op     vs base               │
RoundTrip/Alice-4   223.8µ ± 2%   218.6µ ± 1%  -2.32% (p=0.000 n=10)
RoundTrip/Bob-4     114.7µ ± 1%   109.5µ ± 0%  -4.57% (p=0.000 n=10)
</code></pre>
<p>The one successful optimization was complaining about the confusing result above on the Gophers Slack <code>#performance</code> channel, which sniped Josh Bleecher Snyder into contributing <a href="https://github.com/FiloSottile/mlkem768/pulls?q=is%3Apr+author%3Ajosharian&amp;ref=words.filippo.io">a couple changes</a> :)</p>
<p>There is some low hanging fruit still: key generation and decapsulation both sample a matrix from the same value, and since the two are usually done sequentially on the Alice side, the matrix could be stored saving around 10% time. There might be an opportunity to save a copy in the sha3 read path, too. After that, it’s a matter of optimizing the field implementation.</p>
<p>If you got this far, you might want to follow me on Bluesky at <a href="https://bsky.app/profile/filippo.abyssdomain.expert?ref=words.filippo.io">@filippo.abyssdomain.expert</a> or on Mastodon at <a href="https://abyssdomain.expert/@filippo?ref=words.filippo.io">@filippo@abyssdomain.expert</a>.</p>
<h2 id="bonus-track-using-a-ml-kem-implementation-as-kyber-v3">Bonus track: using a ML-KEM implementation as Kyber v3</h2>
<p>NIST made a few small changes to the Round 3 submission of Kyber. They are summarized in Section 1.3 of the FIPS draft.</p>
<p>However, there are a few experimental protocols defined in terms of Kyber v3 (or “draft00”), including the main deployed PQ TLS key exchange. Do we have to make a separate package to support them?</p>
<p>Luckily, no we don’t.</p>
<p>One change adds some validation for an edge case (non-canonical coefficient encodings in public keys) that was undefined in Kyber. Honest implementations will not produce such keys, so we can reject them as specified in the FIPS draft. It will make it possible to fingerprint our implementation as Kyber-on-ML-KEM but will be otherwise harmless.</p>
<p>One change removed a hashing step applied to CSPRNG input. Since those bytes are random, it’s impossible for any party to tell the difference.</p>
<p>The final change is the major one, and the trickiest. The ciphertext used to be hashed into the shared secret. This difference would prevent interoperability. However, the mixing happens as an additional key derivation, which was entirely removed in ML-KEM, which instead returns the value K as-is. This means we can run ML-KEM to generate the shared secret K and then apply</p>
<pre><code>SHAKE-256(K || c)[:32]
</code></pre>
<p>to generate the Kyber shared secret. No need to break the ML-KEM abstraction.</p>
<p>There’s one wrinkle: both Kyber and ML-KEM perform implicit rejection in Decapsulate by hashing a secret with the ciphertext and returning that as the shared secret. If we do the key derivation above on top of ML-KEM, we’ll hash the ciphertext twice for implicit rejections. That’s ok, because the output of implicit rejection is unpredictable by design, not an interoperation target.</p>
<h2 id="the-picture">The picture</h2>
<p>In Berlin there's an old closed airport, <a href="https://en.wikipedia.org/wiki/Berlin_Tempelhof_Airport?ref=words.filippo.io">Tempelhof</a>, which is now a public park. Walking down the taxiways (pictured) or along the centrelines of the 09L/27R and 09R/27L crossed-out runways is kinda unsettling, at least for me. ("Should I be speaking with Ground or Tower? Can I enter this runway?") Fun fact, in 2010 a single-engine plane forgot to switch fuel tank and did an emergency landing on 27L. Closed runways are the best bad places to land, after all.</p>
<p><img src="https://words.filippo.io/content/images/2024/01/news---1--2--1.jpeg" alt="A cement taxiway pictured at sunset, from the middle of the yellow centreline. The airport terminal is visible on the horizon, and a patch of grass on the left." loading="lazy"></p>
<p>This work was funded by a Google <a href="https://bughunters.google.com/about/rules/5891381450768384/open-source-security-subsidies-rules?ref=words.filippo.io">Open Source Security Subsidy</a> and by my awesome clients—<a href="https://www.sigsum.org/?ref=words.filippo.io">Sigsum</a>, <a href="https://www.latacora.com/?ref=words.filippo.io">Latacora</a>, <a href="https://interchain.io/?ref=words.filippo.io">Interchain</a>, <a href="https://smallstep.com/?ref=words.filippo.io">Smallstep</a>, <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, <a href="https://goteleport.com/?ref=words.filippo.io">Teleport</a>, and <a href="https://tailscale.com/?ref=words.filippo.io">Tailscale</a>—who, through our retainer contracts, get face time and unlimited access to advice on Go and cryptography.</p>
<p>Here are a few words from some of them!</p>
<p>Latacora — <a href="https://www.latacora.com/blog/2023/12/22/case-for-password-hashing/?ref=words.filippo.io">We wrote about password hashing with delegation</a>, a somewhat less known password hashing primitive. It's a PBKDF with a special property, that allows offloading hashing computation to a potentially untrusted server. In this blog post, we describe this primitive and discuss its applicability in the context of End-to-End Encrypted (E2EE) backup systems.</p>
<p>Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. <a href="https://goteleport.com/identity-governance-security/?utm=filippo&amp;ref=words.filippo.io">Teleport Identity Governance &amp; Security</a> is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.</p>
<p>Ava Labs — We at <a href="https://www.avalabs.org/?ref=words.filippo.io">Ava Labs</a>, maintainer of <a href="https://github.com/ava-labs/avalanchego?ref=words.filippo.io">AvalancheGo</a> (the most widely used client for interacting with the <a href="https://www.avax.network/?ref=words.filippo.io">Avalanche Network</a>), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.</p>
<hr>
<section>
<ol>
<li id="fn1"><p>The minimum common multiple of 1/4/10/12 with 8 is less than 64, so we can pack a few values in a uint64, and then serialize that. <a href="https://github.com/FiloSottile/mlkem768/blob/344d5ee2c575ca84613bbb119c1d8b1ef6699ea5/mlkem768.go?ref=words.filippo.io#L522-L627">The result</a> is IMHO pretty readable. <a href="#fnref1">↩︎</a></p>
</li>
<li id="fn2"><p>IIUC the matrix could have been hardcoded but is instead derived from a seed in the key instead to bypass debate on how the hardcoded matrix was generated, and any backdoor concerns. My somewhat spicy opinion is that we’ll come to see this as a mistake, and a case of <a href="https://words.filippo.io/dispatches/parameters/">failing to define parameters</a>. If the matrix was hardcoded ML-KEM would be faster and simpler. For example, there would be no need for these tests at all, and <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/s-C-zIAeKfE/m/n9yqLja7AQAJ?ref=words.filippo.io">the matrix derivation typo in the spec draft</a> couldn’t have happened. Maybe some deployments can just specify and use a profile of ML-KEM that fixes the matrix seed. <a href="#fnref2">↩︎</a></p>
</li>
</ol>
</section>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU chip goal 'unrealistic' says ASML CEO (109 pts)]]></title>
            <link>https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/</link>
            <guid>39213180</guid>
            <pubDate>Thu, 01 Feb 2024 06:11:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/">https://www.electronicsweekly.com/news/business/eu-chip-goal-totally-unrealistic-2024-01/</a>, See on <a href="https://news.ycombinator.com/item?id=39213180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					                                
					
<p>Investments in fab by TSMC, Bosch, NXP and Infineon are “good for the European car industry but it’s not enough. It’s not enough at all,” says Wennink, “if you want to get to 20%, you just have to calculate how much you need to build here.”</p>
<p><img decoding="async" loading="lazy" src="https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-150x150.jpg" alt="EU chip goal ‘totally unrealistic’ says ASML CEO" width="150" height="150" srcset="https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-150x150.jpg 150w, https://static.electronicsweekly.com/wp-content/uploads/sites/16/2014/08/Broadwell_14nm_Wafer_White_p-70x70.jpg 70w" sizes="(max-width: 150px) 100vw, 150px">In 2022, Kurt Sievers, CEO of NXP stated: “We have calculated that we would need €500 billion investment in Europe to reach the 20 percent market share goal.”</p>
<p>According to SEMI, China will have 8.6 million 8 inch equivalent wpm capacity this year, Taiwan will have 5.7 million wpm, Korea will have 5.1 million wpm, Japan will have 4.7 million wpm, the USA will have 3.1 million wpm, Europe will have 2.7 million wpm, and S.E.Asia will&nbsp;have 1.7 million wpm.</p>
<p>So Europe has roughly 8% of the industry measured by capacity and has roughly 8% of the market measured by value.</p>
<p>To up that to 20% in a world where capacity is currently growing at about 6% a year would need about a dozen new fabs to be built and in full volume production by 2030.</p>
                                    
										
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Marlon Brando Was a Secret Tech Geek and Photoshop Ace (2015) (123 pts)]]></title>
            <link>https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html</link>
            <guid>39213150</guid>
            <pubDate>Thu, 01 Feb 2024 06:04:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html">https://www.yahoo.com/entertainment/marlon-brando-was-a-secret-tech-geek-and-photoshop-124061824672.html</a>, See on <a href="https://news.ycombinator.com/item?id=39213150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Marlon Brando: Actor, activist… special effects tech geek?<br></p><p>Though the late star was considered by many to be the greatest actor of the 20th century, Brando always had a love-hate relationship with his craft, often remarking that it was a <a href="http://www.vanityfair.com/news/2005/03/brando200503" rel="nofollow noopener" target="_blank" data-ylk="slk:silly profession for a grown adult;elm:context_link;itc:0;sec:content-canvas">silly profession for a grown adult</a>. He was so frustrated by acting that, in the late 1980s, he embraced a new technology that promised to keep him on the big screen while simultaneously lightening his workload.</p><p>In the new documentary <i>Listen to Me Marlon</i>, director Stevan Riley stitches together intimate home footage that the actor took over the years and combines it with rare interviews to portray Brando as a complicated and volatile soul. But the actor still had a spark for life before his death at age 80 in 2004, and he was fascinated with visual effects, as you can see in this exclusive clip above from the documentary, which you can watch above. The footage was originally intended for an experimental film called <i>Software</i>, which Brando tried to make in the 1990s — more on that later — and features Brando, then in his seventies, delivering a monologue from <i>Macbeth</i>, his face rendered in blue pixels. <br></p><p>The technology in the clip, known as “digital equivalence,” involves creating a photo-realistic digital double of an actor that can look, move, and sound just like the original. It had long been a holy grail of the computer-graphics industry, and when Brando found out about its potential, he partnered with Scott Billups, a veteran cinematographer and graphics pioneer, in the pursuit of perfecting the technology.</p><p>“He dug technology — he was always had the latest system,” Billups, who contributed the footage to the new doc, told Yahoo Movies. “He was always upgraded. He had some new Macs before I did — and I was a Mac developer.”</p><p>The two met in the mid 1970s, but didn’t begin working together in earnest until the late 1980s, when Billups was conducting digital experiments with an actor who lived near Brando’s Hollywood estate (Billups declined to identify the thespian by name). While that neighbor was unnerved by the idea of creating a digital double, Brando eagerly volunteered to be the next test-subject.</p><p>“The friend was freaked, and thought it would kill acting and kill us all and put us out of business, but the test models worked, and you knew who was [on screen]. So Marlon was like, ‘I’m next, I’m next,’” Billups said. They worked closely with Stephen and Lloyd Addleman, brothers who <a href="https://books.google.com/books?id=xlP1uoxU1nEC&amp;pg=PA14&amp;lpg=PA14&amp;dq=Stephen+and+Lloyd+Addleman+cyberware&amp;source=bl&amp;ots=gEmbqlYhVv&amp;sig=f10haQ9tsaT8n3H14OQnqaXdfUY&amp;hl=en&amp;sa=X&amp;ved=0CDMQ6AEwBGoVChMI0YHO2tnYxgIViowNCh3jmgSw#v=onepage&amp;q=Stephen%20and%20Lloyd%20Addleman%20cyberware&amp;f=false" rel="nofollow noopener" target="_blank" data-ylk="slk:invented an early version of the 3D scanner hardware;elm:context_link;itc:0;sec:content-canvas">invented an early version of the 3D scanner hardware</a>, which Billups likened to an advanced supermarket grocery scanner and a spinning chair.</p><p>“The concept was for [the actor] to sit in this chair and get really still, and the scanner would go around you in 360 degrees [in] just under a minute,” Billups explained. “Sometimes the scanner would freeze up, so we’d spin the chair. You put somebody like Marlon in a chair and try to get an even spin on him — well, he’s really not the kind of guy who likes to sit still for long.”</p><figure><div><p><img alt="image" src="https://s.yimg.com/ny/api/res/1.2/4o8McHDwlF0pyFFynGH2yA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTcyMA--/http://40.media.tumblr.com/214f05b1313f97c892af69e1eacfa8bb/tumblr_inline_nrna90Gdb21t2ear7_540.jpg" data-src="https://s.yimg.com/ny/api/res/1.2/4o8McHDwlF0pyFFynGH2yA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTcyMA--/http://40.media.tumblr.com/214f05b1313f97c892af69e1eacfa8bb/tumblr_inline_nrna90Gdb21t2ear7_540.jpg"></p></div></figure><p><i>Brando, David Addleson and Billups in the late ‘80s (Stephen Addleson) </i></p><p>Billups and Brando wound up doing four different tests of the technology throughout the late ‘80s and early ‘90s; one such test focused on replacing the actor in a film he probably shouldn’t have made anyway. “He was doing <i>Island of Doctor Moreau</i>,” Billups remembered, “and he said he could do half of that script digitally.”</p><p>That 1996 sci-fi movie, which co-starred Val Kilmer, was a notorious bomb, but the digital equivalence tests did lead to the duo working on an even more off-beat project: An adaptation of Rudy Rucker’s <a href="http://www.amazon.com/Software-Rudy-Rucker/dp/0380701774" rel="nofollow noopener" target="_blank" data-ylk="slk:Philip K. Dick Award-winning cyberpunk novel Software;elm:context_link;itc:0;sec:content-canvas">Philip K. Dick Award-winning cyberpunk novel <i>Software</i></a>, on which Billups and Brando began pre-production in 1997.</p><p>They set up shop at a company called House of Moves in Santa Monica, building a motion capture studio where they hosted Brando’s A-list friends, including Faye Dunaway. (Dennis Hopper was also attached to the film early on.) “We were scanning them and doing mo-cap — we were going to do this whole movie with digital equivalents,” Billups said. “You could do a movie with a week’s worth of work instead of sitting on set bored out of their minds for a couple months.”</p><p>Billups’ version of <i>Software</i> ultimately fell apart due to disagreements with producers and financiers; it was awfully expensive to attempt to make a film with such an ambitious and risky technology.</p><p>In the photo above, you can see Brando sitting in the lab. Below, we have scans of some of the early tests that Billups and Brando did together; the actor was required to speak — with exaggerated expression —every vowel and syllable, so that they could later generate a fully digital character that made the same exact facial movements. The result is found in the video at the top of this story.</p><figure><div><p><img alt="image" src="https://s.yimg.com/ny/api/res/1.2/deXKktli0lzDktsXE.kCGw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTkyMQ--/http://40.media.tumblr.com/1a54a4f0aa630d8c7b90b9455b5cfe98/tumblr_inline_nrna90lH0f1t2ear7_540.jpg" data-src="https://s.yimg.com/ny/api/res/1.2/deXKktli0lzDktsXE.kCGw--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTkyMQ--/http://40.media.tumblr.com/1a54a4f0aa630d8c7b90b9455b5cfe98/tumblr_inline_nrna90lH0f1t2ear7_540.jpg"></p></div></figure><p><i>Some early scans of Brando’s face</i></p><p>The collapse of <i>Software</i> didn’t sink the actor’s intense interest in technology; as the new century dawned, he pursued not just high-grade experimental electronics, but consumer-level tech, as well.<br></p><p>“Marlon was a closet geek. Well, not even in the closet — he was big into it,” Billups explained. “He was an ace with Photoshop. He would take people’s pictures and put them in places they never were. He’d say, ‘Remember when we were in so-and-so?’ and you’d see people trying to remember that situation.”</p><p>The trick to successfully confusing his friends, Billups explained, was being subtle about it. “He would maneuver it so wasn’t totally unbelievable; he would create an alternate reality,” Billups remembered. “He’d say, ‘I’ve got a picture of it right here, you’re looking at a picture of you at the Statue of Liberty.’ He would do this to all kinds of people, it was hilarious.”</p><p>Brando invested heavily in his hardware, always going after the latest computer systems — often owning them before Billups, who still makes his living in the visual effects industry. The actor was even an early user of CoSa After Effects, the post-production program used for green screen and other effects work, which was eventually sold to Adobe.</p><p>Billups remembers Brando first talking about potentially using motion capture and digital equivalence to perform his role as Jor-El in the original <i>Superman</i> movie, which came out in 1978. It’s taken almost 40 years for effects artists and engineers to realize the technology he was pursuing; this summer, audiences will see aged-down versions of Arnold Schwarzenegger and Michael Douglas in <i>Terminator Genisys</i> and <i>Ant-Man</i>, respectively. The tech isn’t perfect yet, but the facial scan used in those two blockbusters is a tremendous leap forward from where it was when Brando died in 2004.</p><p>In general, Brando would have loved how far technology has come, and it’s easy to imagine the actor being enamored with social media. Brando was a fan of the <i>Myst</i> PC game franchise, and was active in early forms of social media.</p><p>“He loved the anonymity of the web and ability to interact with people outside of his skin,” Billups recalled. “His online persona was that of Chinese exchange student and he handled it masterfully. Not in any skeezy way, but he could pull things out of people that might not have realized of themselves. He was quite masterful at this.”<br></p><p>His email address was just as curious: Brando went by the name BrainFlakes.</p><p>“He was the cleverest son of a bitch you’d ever met,” Billups added. “He was a really sweet guy.”</p><p><i>Listen to Me Marlon</i> will be released at New York’s Film Forum on July 29, LA’s Landmark Theater on July 31, and rollout nationwide after that. It will also air on Showtime in the fall.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lockbox: forward proxy for making third party API calls (113 pts)]]></title>
            <link>https://github.com/mkjt2/lockbox</link>
            <guid>39211857</guid>
            <pubDate>Thu, 01 Feb 2024 01:26:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mkjt2/lockbox">https://github.com/mkjt2/lockbox</a>, See on <a href="https://news.ycombinator.com/item?id=39211857">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:mkjt2/lockbox" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="EOO0ylnXEFjsZr9K3F_V5PBjpz5xD8MVMbkriUntE_HlXJ8wB5woh5jR8FOpnl7EkTLZAL1PeqZ7lxYgR4l75w" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="mkjt2/lockbox" data-current-org="" data-current-owner="mkjt2" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=sgPRTMgz8ku95KMqhkt%2Bc5eH7iEFcXSQxOL162YxiBpQ1kQrDjn3vBito38AHpDzRdv%2BfTPg5%2BaBLmRF0Pcdx1ZJClGV%2F%2BuaF2H7f6%2F7zqTci5qJgiHlNZ%2Fa4kplETiyAkg4d4c6G5ryHzqxAuzhzVz4loJTZl89EmFrvg%2Bs%2BVBmZrCIxGqvmrfiBMhE40Xbde1B9tvTWvm%2BhGWJfR2itHPAB2NtRP2ml%2FXGDGDOXtfz0fHZgo%2BC%2Bs61usHR1tBxFahTfOdkSGvGYK0%2F2N0xWZM0GHAt6dicoIASJxGCrCRWhsaveA4vUZtdHxuqAuX6%2Bw%2Bp5JKh2VqJoErsWSESzW0BD9bCSmMgBP6fFF38Kz5DflH94796roj%2FcpGZDqLbwKbg1kTfIMhW9IGrXNtrhgBxm5LdCB8loKrQtHF6PERztP%2BfLO%2FLvccQDo16WNaRpzpzBy7zJDRAU2%2B7m3SyaT7Kk9eXiWO%2BXXcMAHd%2FGZBPZFER92%2FMjIkXTN%2BZcO9e%2Fuzq6mwJ62stPw%3D%3D--SRlzcLraTcY3Xf9d--UDI9lK0l9MZklwNbxKg2hw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=mkjt2%2Flockbox" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/mkjt2/lockbox&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="99513d7c9ca4e79466e8e86a7454ba13acb003d591c1d874fb5a07f69253f529" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making a PDF that's larger than Germany (622 pts)]]></title>
            <link>https://alexwlchan.net/2024/big-pdf/</link>
            <guid>39210507</guid>
            <pubDate>Wed, 31 Jan 2024 22:47:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexwlchan.net/2024/big-pdf/">https://alexwlchan.net/2024/big-pdf/</a>, See on <a href="https://news.ycombinator.com/item?id=39210507">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"> <article>   <p>I was browsing social media this morning, and I saw a claim I’ve seen go past a few times now – that there’s a maximum size for a PDF document:</p>  <p>Some version of this has been floating around the Internet <a href="https://twitter.com/hassmanm/status/303086542">since 2007</a>, probably earlier. This tweet is pretty emblematic of posts about this claim: it’s stated as pure fact, with no supporting evidence or explanation. We’re meant to just accept that a single PDF can only cover about half the area of Germany, and we’re not given any reason why 381&nbsp;kilometres is the magic limit.</p> <p>I started wondering: has anybody made a PDF this big? How hard would it be? Can you make a PDF that’s even bigger?</p> <p>A few years ago I did some <a href="https://codegolf.stackexchange.com/a/48938/13285">silly noodling into PostScript</a>, the precursor to PDF, and it was a lot of fun. I’ve never actually dived into the internals of PDF, and this seems like a good opportunity.</p> <p>Let’s dig in.</p> <h2 id="where-does-the-claim-come-from">Where does the claim come from?</h2> <p>These posts are often accompanied by a “well, actually” where people in the replies explain this is a limitation of a particular PDF reader app, not a limitation of PDF itself. They usually link to something like <a href="https://en.wikipedia.org/wiki/PDF#:~:text=Page%20dimensions%20are%20not%20limited%20by%20the%20format%20itself">the Wikipedia article for PDF</a>, which explains:</p> <blockquote> <p>Page dimensions are not limited by the format itself. However, Adobe Acrobat imposes a limit of 15&nbsp;million by 15&nbsp;million inches, or 225 trillion in<sup>2</sup> (145,161 km<sup>2</sup>).<a href="https://en.wikipedia.org/wiki/PDF#cite_note-pdf-ref-1.7-2"><sup>[2]</sup></a></p> </blockquote> <p>If you follow the reference link, you find the <a href="https://web.archive.org/web/20081001170454/https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf">specification for PDF&nbsp;1.7</a>, where an appendix item explains in more detail (emphasis mine):</p> <blockquote> <p>In PDF versions earlier than PDF&nbsp;1.6, the size of the default user space unit is fixed at 1/72&nbsp;inch. In Acrobat viewers earlier than version 4.0, the minimum allowed page size is 72 by 72&nbsp;units in default user space (1 by 1&nbsp;inch); the maximum is 3240 by 3240&nbsp;units (45 by 45&nbsp;inches). In Acrobat versions 5.0 and later, the minimum allowed page size is 3 by 3&nbsp;units (approximately 0.04 by 0.04&nbsp;inch); the maximum is 14,400 by 14,400&nbsp;units (200 by 200&nbsp;inches).</p> <p>Beginning with PDF&nbsp;1.6, the size of the default user space unit may be set with the UserUnit entry of the page dictionary. <strong>Acrobat 7.0 supports a maximum UserUnit value of 75,000, which gives a maximum page dimension of 15,000,000&nbsp;inches (14,400 * 75,000 * 1 ⁄ 72).</strong> The minimum UserUnit value is 1.0 (the default).</p> </blockquote> <p>15&nbsp;million inches is exactly 381&nbsp;kilometres, matching the number in the original tweet. And although this limit first appeared in PDF&nbsp;1.6, it’s “version 7” of Adobe Acrobat. This is probably where the original claim comes from.</p> <p>What if we make a PDF that exceeds these “maximum” values?</p> <h2 id="the-inner-structure-of-pdfs">The inner structure of PDFs</h2> <p>I’ve never dived into the internals of a PDF document – I’ve occasionally glimpsed some bits in a hex editor, but I’ve never really understood how they work. If I’m going to be futzing around for fun, this is a good opportunity to learn how to edit the PDF directly, rather than going through a library.</p> <p>I found <a href="https://help.callassoftware.com/a/798383-how-to-create-a-simple-pdf-file">a good article</a> which explains the internal structure of a PDF, and combined with asking ChatGPT a few questions, I was able to get enough to write some simple files by hand.</p> <p>I know that PDFs support a huge number of features, so this is probably a gross oversimplification, but this is the mental picture I created:</p> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 280 560" role="img" aria-labelledby="svg_pdf_layout" style="width: 300px;"> <defs> <marker id="arrowhead" markerWidth="7" markerHeight="4.9" refX="0" refY="2.45" orient="auto"> <polygon points="0 0, 7 2.45, 0 4.9"></polygon> </marker> </defs> <path d="M 230 470 L 270 470 L 270 350 L 250 350" marker-end="url(#arrowhead)"></path> <rect x="50" y="10" width="180" height="40"></rect> <text x="140" y="30">%PDF-1.6</text> <rect x="50" y="70" width="180" height="240"></rect> <text x="140" y="85" text-anchor="middle" dominant-baseline="middle" font-size="10pt">objects</text> <rect x="80" y="100" width="120" height="40"></rect> <text x="140" y="120">object 1</text> <rect x="80" y="150" width="120" height="40"></rect> <text x="140" y="170">object 2</text> <path d="M 140 195 L 140 245"></path> <rect x="80" y="250" width="120" height="40"></rect> <text x="140" y="270">object N</text> <path d="M 50 350 L 10 350 L 10 270 L 62 270" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 220 L 62 220" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 170 L 62 170" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 120 L 62 120" marker-end="url(#arrowhead)"></path> <rect x="50" y="330" width="180" height="40"></rect> <text x="140" y="350">xref</text> <rect x="50" y="390" width="180" height="40"></rect> <text x="140" y="410">trailer</text> <rect x="50" y="450" width="180" height="40"></rect> <text x="140" y="470">startxref</text> <rect x="50" y="510" width="180" height="40"></rect> <text x="140" y="530">%%EOF</text> <title id="svg_pdf_layout">tbc</title></svg> <p>The start and end of a PDF file are always the same: a version number (<code>%PDF-1.6</code>) and an end-of-file marker (<code>%%EOF</code>).</p> <p>After the version number comes a long list of objects. There are lots of types of objects, for all the various things you can find in a PDF, including the pages, the text, and the graphics.</p> <p>After that list comes the <code>xref</code> or cross-reference table, which is a lookup table for the objects. It points to all the objects in the file: it tells you that object 1 is 10&nbsp;bytes after the start, object 2 is after 20&nbsp;bytes, object 3 is after 30&nbsp;bytes, and so on. By looking at this table, a PDF reading app knows how many objects there are in the file, and where to find them.</p> <p>The <code>trailer</code> contains some metadata about the overall document, like the number of pages and whether it’s encrypted.</p> <p>Finally, the <code>startxref</code> value is a pointer to the start of the <code>xref</code> table. This is where a PDF reading app starts: it works from the end of the file until it finds the <code>startxref</code> value, then it can go and read the <code>xref</code> table and learn about all the objects.</p> <p>With this knowledge, I was able to write my first PDF by hand. If you save this code into a file named <code>myexample.pdf</code>, it should open and show a page with a red square in a PDF reading app:</p><pre><code><span>%PDF-1.6

% The first object.  The start of every object is marked by:
%
%     &lt;object number&gt; &lt;generation number&gt; obj
%
% (The generation number is used for versioning, and is usually 0.)
%
% This is object 1, so it starts as `1 0 obj`.  The second object will
% start with `2 0 obj`, then `3 0 obj`, and so on.  The end of each object
% is marked by `endobj`.
%
% This is a "stream" object that draws a shape.  First I specify the
% length of the stream (54&nbsp;bytes).  Then I select a colour as an
% RGB value (`1 0 0 RG` = red), then I set a line width (`5 w`) and
% finally I give it a series of coordinates for drawing the square:
%
%     (100, 100) ----&gt; (200, 100)
%                          |
%     [s = start]          |
%         ^                |
%         |                |
%         |                v
%     (100, 200) &lt;---- (200, 200)
%</span>
1 0 obj
&lt;&lt;
	/Length 54
&gt;&gt;
stream
1 0 0 RG
5 w
100 100 m
200 100 l
200 200 l
100 200 l
s
endstream
endobj

<span>% The second object.
%
% This is a "Page" object that defines a single page.  It contains a
% single object: object 1, the red square.  This is the line `1 0 R`.
%
% The "R" means "Reference", and `1 0 R` is saying "look at object number 1
% with generation number 0" -- and object 1 is the red square.
%
% It also points to a "Pages" object that contains the information about
% all the pages in the PDF -- this is the reference `3 0 R`.</span>
2 0 obj
&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	/MediaBox [0 0 300 300]
	/Contents 1 0 R
&gt;&gt;
endobj

<span>% The third object.
%
% This is a "Pages" object that contains information about the different
% pages.  The `2 0 R` is reference to the "Page" object, defined above.</span>
3 0 obj
&lt;&lt;
	/Type /Pages
	/Kids [2 0 R ]
	/Count 1
&gt;&gt;
endobj

<span>% The fourth object.
%
% This is a "Catalog" object that provides the main structure of the PDF.
% It points to a "Pages" object that contains information about the
% different pages -- this is the reference `3 0 R`.</span>
4 0 obj
&lt;&lt;
	/Type /Catalog
	/Pages 3 0 R
&gt;&gt;
endobj

<span>% The xref table.  This is a lookup table for all the objects.
%
% I'm not entirely sure what the first entry is for, but it seems to be
% important.  The remaining entries correspond to the objects I created.</span>
xref
0 4
0000000000 65535 f
0000000851 00000 n
0000001396 00000 n
0000001655 00000 n
0000001934 00000 n

<span>% The trailer.  This contains some metadata about the PDF.  Here there
% are two entries, which tell us that:
%
%   - There are 4 entries in the `xref` table.
%   - The root of the document is object 4 (the "Catalog" object)
%</span>
trailer
&lt;&lt;
	/Size 4
	/Root 4 0 R
&gt;&gt;

<span>% The startxref marker tells us that we can find the xref table 2196&nbsp;bytes
% after the start of the file.</span>
startxref
2196

<span>% The end-of-file marker.
%%EOF</span></code></pre><p>I played with this file for a while, just doing simple things like adding extra shapes, changing how the shapes appeared, and putting different shapes on different pages. I tried for a while to get text working, but that was a bit beyond me.</p> <p>It quickly became apparent why nobody writes PDFs by hand – it got very fiddly to redo all the lookup tables! But I’m glad I did it; manipulating all the PDF objects and their references really helped me feel like I understand the basic model of PDFs. I opened some “real” PDFs created by other apps, and they have many more objects and types of object – but now I could at least follow some of what’s going on.</p> <p>With this newfound ability to edit PDFs by hand, how can I create monstrously big ones?</p> <h2 id="changing-the-page-size-mediabox-and-userunit">Changing the page size: /MediaBox and /UserUnit</h2> <p>Within a PDF, the size of each page is set on the individual “Page” objects – this allows different pages to be different sizes. We’ve already seen this once:</p><pre><code>&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	<span>/MediaBox [0 0 300 300]</span>
	/Contents 1 0 R
&gt;&gt;</code></pre><p>Here, the <code>MediaBox</code> is setting the width and height of the page – in this case, a square of 300 × 300&nbsp;units. The default unit size is 1/72&nbsp;inch, so the page is 300 × 72 = 4.17&nbsp;inches. And indeed, if I open this PDF in Adobe Acrobat, that’s what it reports:</p> <picture> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.avif 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.avif 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.webp 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.webp 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.png 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.png 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.avif 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.avif 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.webp 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.webp 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.png 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.png 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/png"> <img src="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.png" alt="Screenshot of Acrobat’s ‘Document Properties’ panel, showing the page size of 4.17 x 4.17 in." width="724"> </picture> <p>By changing the <code>MediaBox</code> value, we can make the page bigger. For example, if we change the value to <code>600 600</code>, Acrobat says it’s now <code>8.33 x 8.33 in</code>. Nice!</p> <p>We can increase it all the way to <code>14400 14400</code>, the max allowed by Acrobat, and then it says the page is now <code>200.00 x 200.00in</code>. (You <a href="https://alexwlchan.net/images/2024/acrobat-error.png">get a warning</a> if you try to push past that limit.)</p> <p>But 200&nbsp;inches is far short of 381&nbsp;kilometres – and that’s because we’re using the default unit of 1/72&nbsp;inch. We can increase the unit size by adding a <code>/UserUnit</code> value. For exaple, setting the value to 2 will double the page in both dimensions:</p><pre><code>&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	<span>/MediaBox [0 0 14400 14400]</span>
	<span>/UserUnit 2</span>
	/Contents 1 0 R
&gt;&gt;</code></pre><p>And now Acrobat reports the size of the page as <code>400.00 x 400.00 in</code>.</p> <p>If we crank it all the way up to the maximum of <code>UserUnit 75000</code>, Acrobat now reports the size of our page as <code>15,000,000,000.00 x 15,000,000,000.00 in</code> – 381 km along both sides, matching the original claim. If you’re curious, you can <a href="https://alexwlchan.net/files/2024/biggest.pdf">download the PDF</a>.</p> <p>If you try to create a page with a larger size, either by increasing the <code>MediaBox</code> or <code>UserUnit</code> values, Acrobat just ignores it. It keeps saying that the size of a page is 15 billion inches, even if the page metadata says it’s higher. (And if you increase the <code>UserUnit</code> past <code>75000</code>, this happens silently – there’s no warning or error to suggest the size of the page is being capped.)</p> <p>This probably isn’t an issue – I don’t think the <code>UserUnit</code> value is widely used in practice. I found <a href="https://stackoverflow.com/a/59927201/1558022">one Stack Overflow answer</a> saying as such, and I couldn’t find any examples of it online. The builtin macOS Preview.app doesn’t even support it – it completely ignores the value, and treats all PDFs as if the unit size is 1/72&nbsp;inch.</p> <p>But unlike Acrobat, the Preview app doesn’t have an upper limit on what we can put in <code>MediaBox</code>. It’s perfectly happy for me to write a width which is a 1 followed by twelve 0s:</p> <picture> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.avif 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.avif 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.webp 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.webp 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.png 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.png 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.avif 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.avif 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.webp 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.webp 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.png 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.png 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/png"> <img src="https://alexwlchan.net/images/2024/preview-megawide_1x.png" alt="Screenshot of Preview’s Document inspector, showing the page size of 352777777777.78 x 10.59 cm." width="366"> </picture> <p>If you’re curious, that width is approximately the distance between the Earth and the Moon. I’d have to get my ruler to check, but I’m pretty sure that’s larger than Germany.</p> <p>I could keep going. And I did. Eventually I ended up with a PDF that Preview claimed is larger than the entire universe – approximately 37 trillion light years square. Admittedly it’s mostly empty space, but so is the universe. If you’d like to play with that PDF, you can <a href="https://alexwlchan.net/files/2024/universe.pdf">get it here</a>.</p> <p>Please don’t try to print it.</p> </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MobileDiffusion: Rapid text-to-image generation on-device (236 pts)]]></title>
            <link>https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</link>
            <guid>39210458</guid>
            <pubDate>Wed, 31 Jan 2024 22:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html">https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</a>, See on <a href="https://news.ycombinator.com/item?id=39210458">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5966553114967673984">
<p><span>Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML</span>

</p><p>
Text-to-image <a href="https://arxiv.org/abs/2006.11239">diffusion models</a> have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., <a href="https://stability.ai/news/stable-diffusion-public-release">Stable Diffusion</a>, <a href="https://openai.com/research/dall-e">DALL·E</a>, and <a href="https://imagen.research.google/">Imagen</a>). While recent advancements in inference solutions on <a href="https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html">Android</a> via MediaPipe and <a href="https://github.com/apple/ml-stable-diffusion">iOS</a> via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.
</p> <p>
To that end, in “<a href="https://arxiv.org/abs/2311.16567">MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices</a>”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment.
</p>




<p>Rapid text-to-image generation on-device.</p>





<h2>Background</h2>


<p>
The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires <a href="https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html">iterative denoising</a> to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature.
</p>
<p>
The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (e.g., <a href="https://arxiv.org/abs/2206.00927">DPM</a>) or distillation techniques (e.g., <a href="https://arxiv.org/abs/2202.00512">progressive distillation</a>, <a href="https://arxiv.org/abs/2303.01469">consistency distillation</a>), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> and <a href="https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.">Adversarial Diffusion Distillation</a>, even reduce to a single necessary step. 
</p>
<p>
However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (e.g., <a href="https://snap-research.github.io/SnapFusion/">SnapFusion</a>). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures.
</p>





<h2>MobileDiffusion</h2>


<p>
Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model's architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion’s <a href="https://arxiv.org/abs/2112.10752">UNet architecture</a>. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion.
</p>
<p>
The design of MobileDiffusion follows that of <a href="https://arxiv.org/abs/2112.10752">latent diffusion models</a>. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use <a href="https://arxiv.org/abs/2103.00020">CLIP-ViT/L14</a>, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. 
</p>




<h3>Diffusion UNet</h3>


<p>
As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (e.g., data, optimizer) to study the effects of different architectures.
</p>
<p>
In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of <a href="https://arxiv.org/abs/2301.11093">UViT</a> architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. 
</p>





<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s915/image4.png"><img data-original-height="249" data-original-width="915" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png"></a></td></tr><tr><td>Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.</td></tr></tbody></table>




<p>
Convolution blocks, in particular <a href="https://arxiv.org/abs/1512.03385">ResNet</a> blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is <a href="https://arxiv.org/abs/1704.04861">separable convolution</a>. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance.
</p>
<p>
In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of <a href="https://arxiv.org/pdf/2110.12894.pdf">FLOPs</a> (floating-point operations) and number of parameters. 
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png"><img data-original-height="742" data-original-width="1200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png"></a></td></tr><tr><td>Comparison of some diffusion UNets.</td></tr></tbody></table>





<h3>Image decoder</h3>


<p>
In addition to the UNet, we also optimized the image decoder. We trained a <a href="https://arxiv.org/abs/2012.03715">variational autoencoder</a> (VAE) to encode an <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a> image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size.  To further enhance efficiency, we design a lightweight decoder architecture by pruning the original’s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our <a href="https://arxiv.org/abs/2311.16567">paper</a>.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png"><img data-original-height="789" data-original-width="1124" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png"></a></td></tr><tr><td>VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).</td></tr></tbody></table>



<br>

<table>
  <tbody><tr>
   <td><b>Decoder</b>
   </td>
   <td><b>&nbsp;&nbsp;#Params (M)&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;PSNR↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;SSIM↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;LPIPS↓&nbsp;&nbsp;</b>
   </td>
  </tr>
  <tr>
   <td><b>SD</b>
   </td>
   <td>49.5
   </td>
   <td>26.7
   </td>
   <td>0.76
   </td>
   <td>0.037
   </td>
  </tr>
  <tr>
   <td><b>Ours</b>
   </td>
   <td>39.3
   </td>
   <td>30.0
   </td>
   <td>0.83
   </td>
   <td>0.032
   </td>
  </tr>
  <tr>
   <td><b>Ours-Lite&nbsp;&nbsp;&nbsp;&nbsp;</b>
   </td>
   <td>9.8
   </td>
   <td>30.2
   </td>
   <td>0.84
   </td>
   <td>0.032
   </td>
  </tr>
</tbody></table>
<br>





<h3>One-step sampling</h3>


<p>
In addition to optimizing the model architecture, we adopt a <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN hybrid</a> to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (e.g., <a href="https://arxiv.org/abs/2301.09515">StyleGAN-T</a>, <a href="https://arxiv.org/abs/2303.05511">GigaGAN</a>) confront similar complexities, resulting in highly intricate and expensive training.
</p>
<p>
To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training.
</p>
<p>
The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations.  
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s960/image7.jpg"><img data-original-height="576" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg"></a></td></tr><tr><td>Illustration of DiffusionGAN fine-tuning.</td></tr></tbody></table>




<h2>Results</h2>


<p>
Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling. With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1.png"><img data-original-height="1296" data-original-width="1728" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png"></a></td></tr><tr><td>Images generated by our MobileDiffusion</td></tr></tbody></table>



<p>
We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices.
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png" imageanchor="1"><img data-original-height="742" data-original-width="1184" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png"></a></td></tr><tr><td>Latency measurements (<b>s</b>) on mobile devices.</td></tr></tbody></table>




<h2>Conclusion</h2>


<p>
With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google’s <a href="https://ai.google/responsibility/responsible-ai-practices/">responsible AI practices</a>.
</p>




<h2>Acknowledgments</h2>


<p>
<em>We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.</em>
</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visual Analysis of Binary Files (200 pts)]]></title>
            <link>http://binvis.io/#/</link>
            <guid>39210436</guid>
            <pubDate>Wed, 31 Jan 2024 22:39:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://binvis.io/#/">http://binvis.io/#/</a>, See on <a href="https://news.ycombinator.com/item?id=39210436">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[I don't want anything your AI generates (203 pts)]]></title>
            <link>https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/</link>
            <guid>39210126</guid>
            <pubDate>Wed, 31 Jan 2024 22:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/">https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/</a>, See on <a href="https://news.ycombinator.com/item?id=39210126">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).</p><div><p>I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).</p><p>I can't trust the answers it provides or the text it generates. It's not a replacement for search, it simply makes search worse.</p><p>The images it generates are, at best, a polished regression to the mean. If you want custom art, pay an artist.</p><p>I want to talk to a person, not a chatbot. The chatbot wastes time while you wait for a person that can actually help.</p><p>I don't want music recommendations from something that can't appreciate or understand music. Human recommendations will always be better.</p><p>I don't want AI mediating social interactions that it cannot and does not understand (though it may appear to). If I'm weary of too much volume on any social platform or in any news feed, I'll cut back on what I'm following.</p><p>If you're having AI attend a meeting for you, it probably wasn't that important. If you're having AI write your email, it probably wasn't that important. If it's screening job candidates for you, you're missing quality candidates.</p><p>I don't like the idea of it being trained on anything I've written or created<sup><a href="#fn1" id="fnref1">[1]</a></sup>.</p><p>I'll concede that Copilot is better than traditional autocomplete, but that feels like a pretty low bar<sup><a href="#fn2" id="fnref2">[2]</a></sup>.</p><p>These tools will improve<sup><a href="#fn3" id="fnref3">[3]</a></sup>. The cost to the environment will increase, it will help companies deskill jobs and muddy public discourse. The "benefits" it provides to end users are, at best, dubious — though everyone responsible for creating it will most certainly enrich themselves.</p><p>This all portends a future I can't imagine wanting or even being interested in.</p><hr><section><ol><li id="fn1"><p>Although consuming that may well be to its detriment. Nonetheless, my <code>robots.txt</code> file reflects this stance. <a href="#fnref1">↩︎</a></p></li><li id="fn2"><p>My first experience with Copilot chat was it getting an answer wrong, apologizing when that was pointed out and then repeating the same or similar, but still incorrect, answer. <a href="#fnref2">↩︎</a></p></li><li id="fn3"><p>I don't think this is a good thing. <a href="#fnref3">↩︎</a></p></li></ol></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek Coder: Let the Code Write Itself (198 pts)]]></title>
            <link>https://deepseekcoder.github.io/</link>
            <guid>39209814</guid>
            <pubDate>Wed, 31 Jan 2024 21:43:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepseekcoder.github.io/">https://deepseekcoder.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39209814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
            DeepSeek Coder comprises a series of code language models trained from scratch on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens.
            We provide various sizes of the code model, ranging from 1B to 33B versions.
            Each model is pre-trained on repo-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, resulting in foundational models (DeepSeek-Coder-Base).
            We further fine-tune the base model with 2B tokens of instruction data to get instruction-tuned models, namedly DeepSeek-Coder-Instruct.
          </p>
          <!-- <p>
            For coding capabilities, DeepSeek-Coder-Base achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.
            And Compared to GPT35-turbo, DeepSeek-Coder-Instruct demonstrates superior performance in human evaluation while maintaining comparable performance in MBPP.
          </p> -->
          <ul>
            <li>
              Pretrained on <b>2 Trillion</b> tokens over more than 80 programming languages.
            </li>
            <li>
              Various model sizes (<b>1.3B</b>, <b>5.7B</b>, <b>6.7B</b> and <b>33B</b>) to support different requirements.
            </li>
            <li>
              A window size of <b>16K window </b>size, supporting <b>project-level</b> code completion and infilling.
            </li>
            <li>
              <b>State-of-the-Art</b> performance among open code models. 
            </li>
            <li>
              <b>Open source and free for research and commercial use</b>.
            </li>
          </ul>
          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generating Fantasy Maps (2016) (158 pts)]]></title>
            <link>http://mewo2.com/notes/terrain/</link>
            <guid>39209687</guid>
            <pubDate>Wed, 31 Jan 2024 21:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://mewo2.com/notes/terrain/">http://mewo2.com/notes/terrain/</a>, See on <a href="https://news.ycombinator.com/item?id=39209687">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">
    
    





<p><img src="http://mewo2.com/deserts/map.png" alt="An example map from the generator" width="60%"></p>
<p>These are some notes on how I generate the maps for my Twitter bot
<a href="https://twitter.com/unchartedatlas">@unchartedatlas</a>, which is based on a generator I originally produced
during <a href="https://github.com/dariusk/NaNoGenMo-2015/issues/156">NaNoGenMo 2015</a>. There's JavaScript code for the generator
on Github <a href="https://github.com/mewo2/terrain">here</a>, and the original messy Python generator code can be
seen <a href="https://github.com/mewo2/deserts">here</a>.</p>
<p>You may also be interested in this <a href="http://mewo2.com/notes/naming-language/">companion piece</a>, which describes
the placename generation.</p>
<h3>Inspiration</h3>
<p>I wanted to make maps that look like something you'd find at the back of one of
the cheap paperback fantasy novels of my youth. I always had a fascination with
these imagined worlds, which were often much more interesting than whatever
luke-warm sub-Tolkien tale they were attached to.</p>
<p>At the same time, I wanted to play with terrain generation with a physical
basis. There are loads of articles on the internet which describe terrain
generation, and they almost all use some variation on a fractal noise approach,
either directly (by adding layers of noise functions), or indirectly (e.g.
through midpoint displacement). These methods produce lots of fine detail, but
the large-scale structure always looks a bit off. Features are attached in
random ways, with no thought to the processes which form landscapes. I wanted
to try something a little bit different.</p>
<p>There are a few different stages to the generator. First we build up a
height-map of the terrain, and do things like routing water flow over the
surface. Then we can render the 'physical' portion of the map. Finally we can
place cities and 'regions' on the map, and place their labels.</p>
<h3>Grids</h3>
<p>To represent the heightmap, first we need a grid of points. Although it can be
simpler to work on a regular square grid, I much prefer to work on an irregular
set of points for something like this. With a regular grid, it's very easy to
run into weird artifacts, and you often have to do a lot of postprocessing to
hide the effects of the grid. If you use an irregular grid, then there are a
few things which are more complicated, but the structure of the grid helps to
give the map a rough, organic feel, and you never have to worry about nasty
linear artifacts in the finished product.</p>
<p>The approach I use is the same as in <a href="http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/">this article</a>, which is one of the
better references out there on how to do non-fractal terrain generation. I
won't go into too much detail here because that article explains it very
clearly, with lots of diagrams.</p>
<p>I start by selecting points at random within the map. These points tend to be a
bit clumpy and uneven, so I use <a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd relaxation</a> to improve the point
set. For speed, I only use one iteration of this process, but you can repeat it
as many times as you like. There are rapidly diminishing returns after a few
iterations though.</p>
<p>All of the calculations are actually carried out on the 'dual points' of the
original point set, which correspond to the corners of the Voronoi polygons.
This has the advantage that the number of neighbours per node is fixed at
three, which helps in some parts of the code.</p>
<p>Note: this shows 256 (2<sup>8</sup>) points, to make viewing easier, but the real generator uses 16,384 (2<sup>14</sup>) points. I have a programmer's superstitions about always using powers of 2, which are more pleasing to the spirit of the machine.</p>

<h3>Rough outlines</h3>
<p>One of the difficulties of creating landscapes in a realistic way is that real
landscapes aren't created all at once. Instead, they evolve from earlier
landscapes, which in turn evolved from even earlier landscapes, and so on back
for billions of years. There's no good way to simulate this process in a
reasonable amount of time, so we need to cheat slightly.</p>
<p>Rather than an infinite regress of older landscapes, I start with a simple
'proto-landscape', built with geometric primitives. This lets me control the
broad outlines of the terrain, while leaving the details for the more physical
processes to fill in later.</p>
<p>Some useful primitives which we can add together:</p>
<ul>
<li>Constant slope - if you want to pretend this is physically motivated, think of it as tectonic uplift on one side of the map</li>
<li>Cone shapes - these can be islands or mountains, or if inverted, lakes or seas</li>
<li>Rounded blobs - these make better hills, and can be scattered all around to make a noisy surface</li>
</ul>
<p>We also have a few operations which are handy:</p>
<ul>
<li>Normalize - rescale the heights to lie in the range 0-1</li>
<li>Round - normalize, then take the square root of the height value, to round off the tops of hills</li>
<li>Relax - replace each height value with the average of its neighbours, to smooth the surface</li>
<li>Set sea level - translate the heightmap up or down so that a particular quantile is at zero</li>
</ul>
<p>The particular sequence of primitives and operations used can be varied to produce different kinds of landscape, such as coastlines, islands and mountain ranges.</p>
<p>Note: the black line indicates the zero contour, which we treat as 'sea level'. Also, this map uses 4,096 (2<sup>12</sup>) points, for speed.</p>

<h3>Erosion</h3>
<p>The results of this process can be a little bit on the blobby side, which means
they rarely look good on their own. We want to scuff them up a bit, so they
look more like real landscapes. We do this by applying an erosion operation.</p>
<p>In most of the world, by far the largest influence on the shape of landforms is
fluvial (water-based) erosion. Water flows downhill, carrying sediment along
with it, carving out valleys and river basins. This is a massively complex
phenomenon, and modelling it correctly is a very active research area, but we
can get a long way by sketching a simple version of the process.</p>
<p>We need to start by tracing the routes that water would take over the grid. For
each grid point, we say that water flows to its lowest neighbour, and so on
down until we reach the edge of the map. This gives a map of water flow.</p>
<p>There's an obvious problem when we reach gridpoints which are lower than all of
their neighbours. Do we route the water back uphill? This will probably lead to
cycles in the water system, which are trouble. Instead, we want to fill in
these gaps (often called sinks or depressions), so that the water always runs
downhill all the way to the edge.</p>
<p>It's easy to see how to fill in a single gridpoint, but as the depression gets
bigger, and possibly links up with other depressions, the number of possible
cases multiplies enormously. Luckily, there's an algorithm for filling
depressions, called the Planchon-Darboux algorithm.</p>
<div>

<h4>Aside: the Planchon-Darboux algorithm</h4>

<p>The algorithm works by finding the lowest surface with the following two properties:</p>

<ul>
<li>The surface is everywhere at least as high as the input surface</li>
<li>Every non-edge point has a neighbour which is lower than it</li>
</ul>

<p>To calculate this, we start with an infinitely high surface everywhere
except on the edge, where we use the original heights. Then, on each iteration,
we find points which have a neighbour which is lower than them, and set their
height to their original height, or the height of their lowest neighbour (plus
a small amount), whichever is higher. We halt when we can go a full iteration
without changing any point.</p>

<p>There are various ways of speeding up this algorithm, mostly by tweaking the
order in which points are visited. For more details, and a proof of
correctness, you can read <a href="http://horizon.documentation.ird.fr/exl-doc/pleins_textes/pleins_textes_7/sous_copyright/010031925.pdf">the original paper</a>.</p>

</div>

<p>With the water routing calculated, we can work out how much water is flowing
through each point. I assume that rainfall is constant across the whole map,
and iterate through the points in descending order, passing the rainfall, plus
the accumulated water flux, from each point to its 'downhill point'. This gives
a map of water flux, which usually converges into a nice branching river
structure, with lots of small streams feeding a larger central channel.</p>
<p>To calculate erosion, I combine the water flux with the slope at each point, as
calculated based on the triangle of its neighbours. The exact formula I use is
the product of the slope with the square root of the water flux. This isn't
necessarily very physical, but it does give nice-looking results. I also add a
small term which is proportional to the slope squared. This prevents deep
gorges from forming, which might be physically realistic, but don't look good
in the graphical style I've chosen.</p>
<p>I find it's very important to cap the erosion rate, otherwise strange things
can happen. A little goes a very long way with this. Also, erosion always
lowers the surface, so it usually helps to drop the sea level afterwards to match.</p>
<p>A final tweak to the heightmap is to smooth out the coastlines slightly. The
erosion tends to produce quite rough terrain, which becomes tiny islands when
cut off by sea level. A few of these can look good, but too many just looks
messy. I repeatedly apply a filter where points which are below sea level, but
a majority of whose neighbours are above sea level, get pulled up, and vice
versa for points which are above sea level and have undersea neighbours. A
couple of repeats of this produces a much cleaner coastline.</p>


<h3>Rendering terrain</h3>
<p>Now comes the question of drawing the map (at least the physical portion). The
easy part is the coastline - we've been doing this already. It's just a matter
of drawing line segments where the heightmap crosses zero. There's not a lot
extra to do about this.</p>
<p>The next component is the rivers. We don't want to display the entire drainage
network, because that would cover the whole map. Instead, we only show the
drainage from points with above a certain threshold of water flux. By
connecting these points to their downstream neighbours, we can trace out the
river paths.</p>
<p>One problem with this approach is that the rivers tend to zigzag from grid
point to grid point, rather than following a smooth path. To solve this, I
relax the points in the middle of the path towards their upstream and
downstream neighbours (keeping the top and bottom fixed, so that intersections
work properly). This smooths things out beautifully.</p>
<p>The final part of this is the shading on the sides of hills, which helps
indicate the topography. It is a central principle of cartography that we tend
to interpret maps as though viewing the terrain from the bottom of the map,
looking towards the top. So we want to draw strokes which go up and right if
the terrain slopes upwards from left to right, and down and right if the
terrain slopes downwards. Similarly, the strokes on the 'near' side of hills
should be longer than those on the 'far' side.</p>
<p>For each grid point, I calculate the slope, and ignore the point if it is less
than a random threshold. For points which pass this test, I draw a short stroke
with slope proportional to the horizontal component of the heightmap slope,
with a small modifier for the vertical component. If the stroke would be too
steep, I split it into several shorter strokes, at the maximum slope, drawn at
random around the point.</p>


<h3>Cities, borders</h3>
<p>Now that we have the 'physical' portion of the map sorted, we can move to
looking at the 'political'. We want to place cities and towns on the map in
feasible-looking locations. At the same time, we want the cities to be spread
out enough that we can put labels on them without worrying too much about
overlap.</p>
<p>To place a city, I generate a score for each point, which is a combination of three things:</p>
<ul>
<li>Water flux - we want cities to be preferentially located on rivers, so high water flux gets a bonus</li>
<li>Distance from other cities - we want cities to be spread out, so penalize locations which are too close to an existing city</li>
<li>Distance from the edge of the map - the other two criteria alone tend to push cities to the map edge, which isn't ideal, so penalize locations too close to the edge</li>
</ul>
<p>Every time I add a city, I choose the point with the highest score, then
recalculate the scores for all points. I make a distinction between cities,
which have a 'region' associated with them, and towns, which don't. The cities
are placed first, but otherwise there's no distinction in the code.</p>
<p>The next step is to mark out the regions. We want the borders between regions
to seem fairly reasonable, following natural borders like rivers and mountain
ranges. The way I approach this is to expand regions outwards from each city,
so that each region consists of the points which are 'closest' to its city,
according to a particular distance measure. This distance measure is calculated
by adding up the cost of the route, based on these criteria:</p>
<ul>
<li>Horizontal distance</li>
<li>Slope - uphill is much cheaper than downhill, so regions expand until they hit the top of ridges, then stop</li>
<li>Water flux - crossing a river is expensive</li>
<li>Shorelines - there is a large penalty for going from land to water (or vice versa), and a smaller penalty for travelling by water</li>
</ul>
<p>Finally, the borders are drawn, using the same smoothing technique as the rivers.</p>


<h3>Placing labels</h3>
<p>At this point we can start naming things, using the process described in <a href="http://mewo2.com/notes/naming-language/">these
notes</a>. I generate names for cities, towns and regions, using a
consistent language for the whole map.</p>
<p>The very last part of the process is to place the labels, avoiding overlaps,
obscured cities, labels going off the edge of the map, etc. This sounds easy,
but it really really isn't. Ideally, some sort of intelligent layout algorithm
would place all the labels, rearranging as necessary. Instead, I have a few
hundred lines of spaghetti code which seems to get the right answer most of the
time, but which is packed full of magic numbers.</p>
<p>In rough outline, what happens is this: the labels are placed in the order 1.
cities, 2. towns, 3. regions. For cities and towns, there are four possible
slots for the label, above, below, and to each side of the marker. Each label
is placed, attempting to avoid the following overlaps (in rough order of
importance):</p>
<ul>
<li>City markers</li>
<li>Other city labels</li>
<li>The map edge</li>
<li>Borders</li>
<li>Coastlines</li>
<li>Rivers</li>
</ul>
<p>Obviously it's not usually possible to avoid all of these, so the least bad
solution is chosen.</p>
<p>For regions, there's a bit more freedom in where to place the label, but the
labels are also bigger, and the map is more cluttered at this point. The
scoring system rates positions based on proximity to the center of the region,
as well as being over land, and penalties for all the overlaps mentioned
before.</p>
<p>I wanted to make a nice interactive example for this, but trying to separate out the label placement code made me feel physically unwell. Sorry about that.</p>

<p>So that's the algorithm, at least in rough outline. The actual code running on
the bot has a few other little bits and pieces, which honestly don't do much,
but removing them is more trouble than it's worth when the code is working as
it stands. The JavaScript code behind this page is available <a href="https://github.com/mewo2/terrain">on
GitHub</a>, and if you're really really brave you can look at the original
<a href="https://github.com/mewo2/deserts">Python code</a> which was written while I was figuring all this out.</p>
<p>There's obviously lots that could be done to improve this. Erosion is just one
process, and the next obvious thing to add would be fluvial deposition, which
would allow for flood plains, river deltas, etc to form. If you wanted more
realistic mountains, then glacial processes would be worth looking at. Volcanic
stuff could also be fun.</p>
<p>On the graphical side, it could be fun to try to sketch more interesting
textures on the map, such as forests or fields. Or you could fill the oceans
with sea monsters and lost ships. If you're really brave, you could even look
at labelling more features, like mountain ranges and rivers.</p>
<p>As always, if you do any of these things, or make anything else interesting
with this code or these ideas, please get in touch and let me know.</p>
<p>If you would like to buy a print of one of these maps, please visit <a href="https://shop.mewo2.com/">my web shop</a>.</p>

    <hr><p>
    
    If you've enjoyed this piece, please consider <a href="https://www.patreon.com/mewo2">contributing on Patreon</a> so I can do more things like this.
    
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comcast reluctantly agrees to stop its misleading "10G Network" claims (198 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/</link>
            <guid>39209469</guid>
            <pubDate>Wed, 31 Jan 2024 21:10:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/">https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/</a>, See on <a href="https://news.ycombinator.com/item?id=39209469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      10G or not 10G    —
</h4>
            
            <h2 itemprop="description">Comcast said it will drop "Xfinity 10G Network" brand name after losing appeal. </h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/01/comcast-10g-800x528.jpg" alt="A Comcast router/modem gateway.">
      <figcaption><p>Comcast</p></figcaption>  </figure>

  




<!-- cache hit 181:single/related:5927e4533beb5ac868f3f9b9c0ce65dd --><!-- empty -->
<p>Comcast has reluctantly agreed to discontinue its "Xfinity 10G Network" brand name after losing an appeal of a ruling that found the marketing term was misleading. It will keep using the term 10G in other ways, however.</p>
<p>Verizon and T-Mobile both challenged Comcast's advertising of 10G, a term used by cable companies since it was unveiled in January 2019 by industry lobby group NCTA-The Internet &amp; Television Association. We <a href="https://arstechnica.com/information-technology/2019/01/envious-of-5g-hype-cable-cos-unveil-potentially-confusing-10g-trademark/">wrote in 2019</a> that the cable industry's 10G marketing was likely to confuse consumers and seemed to be a way of countering 5G hype generated by wireless companies.</p>
<p>10G doesn't refer to the 10th generation of a technology. It is a reference to potential 10Gbps broadband connections, which would be much faster than the actual speeds on standard cable networks today.</p>
<p>The challenges lodged against Comcast marketing were filed with the advertising industry's self-regulatory system run by BBB National Programs. BBB's National Advertising Division (NAD) <a href="https://arstechnica.com/tech-policy/2023/10/comcast-should-stop-advertising-slower-speeds-as-10g-industry-group-says/">ruled against Comcast</a> in October 2023, but Comcast appealed to the National Advertising Review Board (NARB).</p>
<p>The NARB <a href="https://bbbprograms.org/media-center/dd/comcast-10g-xfinity">announced its ruling</a> today, agreeing with the NAD that "Comcast should discontinue use of the term 10G, both when used in the name of the service itself ('Xfinity 10G Network') as well as when used to describe the Xfinity network. The use of 10G in a manner that is not false or misleading and is consistent with the panel decision is not precluded by the panel recommendations."</p>
<h2>“Comcast will discontinue brand name”</h2>
<p>Comcast agreed to make the change in an advertiser's statement that it provided to the NARB. "Although Comcast strongly disagrees with NARB's analysis and approach, Comcast will discontinue use of the brand name 'Xfinity 10G Network' and will not use the term '10G' in a manner that misleadingly describes the Xfinity network itself," Comcast said.</p>
<p>Comcast said it disagrees with "the recommendation to discontinue the brand name" because the company "makes available 10Gbps of Internet speed to 98 percent of its subscribers upon request." But those 10Gbps speeds aren't available in Comcast's typical service plans and require a fiber-to-the-home connection instead of a standard cable installation.</p>                                            
                                                        
<p>The Comcast "Gigabit Pro" fiber connection that provides 10Gbps speeds costs $299.95 a month plus a $19.95 modem lease fee. It also requires a $500 installation charge and a $500 activation charge.</p>
<p>Comcast said it may still use 10G in ways that are less likely to confuse consumers. "Consistent with the panel's recommendation... Comcast reserves the right to use the term '10G' or 'Xfinity 10G' in a manner that does not misleadingly describe the Xfinity network itself," the company said.</p>
<p>When contacted by Ars, a Comcast spokesperson said, "We disagree with the decision but are pleased that we have confirmed our continued use of 10G in advertising."</p>
<h2>Comcast claims “not supported”</h2>
<p>The NARB said the "recent availability of 10G speeds through [the Gigabit Pro] service tier does not support the superior speed claim (or a 10Gbps claim) for the Xfinity network as a whole." As the NARB noted, there is an "absence" of data showing how many Comcast customers actually use that service.</p>
<p>The NARB also said that 10G is misleading because of the implied comparison to 5G wireless networks.&nbsp;"The NARB panel concluded that 10G expressly communicates at a minimum that users of the Xfinity network will experience significantly faster speeds than are available on 5G networks," the announcement of the ruling said. "This express claim is not supported because the record does not contain any data comparing speeds experienced by Xfinity network users with speeds experienced by subscribers to 5G networks."</p>
<p>As the NAD has <a href="https://bbbprograms.org/media-center/dd/comcast-10g">previously stated</a>, 10G is more of an "aspirational" term rather than something that's offered over today's cable networks. Over the past five years, the NCTA has been using the term 10G to describe just about any improvement to cable networks, regardless of the actual speeds.</p>
<p>The NCTA coincidentally issued a <a href="https://www.ncta.com/whats-new/10g-five-years-on-the-future-is-even-closer">press release</a> yesterday hailing the fifth anniversary of its first 10G announcement. "Five years on, the future is even closer... Here in 2024, the promise of 10G is becoming more and more of a reality," the NCTA said.</p>
<p>The announcement listed some examples of multi-gigabit (but not 10-gigabit) cable speeds, some of which were only achieved in lab testing or demos. NCTA claimed that "10G can change lives" and that the "10G platform will facilitate the next great technological advancements in the coming decades, ensuring fast, reliable, and safe networks continue to power the American economy."</p>
<p>For all of you cable broadband users, just remember to ignore "10G" in cable-company marketing and check the actual speeds you're paying for.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FBI confirms it issued remote kill command to blow out Volt Typhoon's botnet (104 pts)]]></title>
            <link>https://www.theregister.com/2024/01/31/volt_typhoon_botnet/</link>
            <guid>39209352</guid>
            <pubDate>Wed, 31 Jan 2024 21:00:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/01/31/volt_typhoon_botnet/">https://www.theregister.com/2024/01/31/volt_typhoon_botnet/</a>, See on <a href="https://news.ycombinator.com/item?id=39209352">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>China's Volt Typhoon attackers used "hundreds" of outdated Cisco and NetGear routers infected with malware in an attempt to break into US critical infrastructure facilities, according to the Justice Department.</p>
<p>On Tuesday <a target="_blank" href="https://www.theregister.com/2024/01/30/fbi_china_volt/">news broke</a> that the Feds had blocked the malicious network that was set up on end-of-life, US-based small office/home office routers. Now more details have come out about how an FBI team infiltrated the attack and harvested the key data before remotely wiping the KV Botnet, according to four warrants (<a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDIsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDIxL2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.4LRHVZyUDYfMnRWC7s_-GUsAOhl4SewCG7GXGj4tAX8/s/3078670442/br/236285728656-l">5018</a>, <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDMsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDE2L2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.8vmYc17z664gSNfykf8ncM2yRyRspFYF6SCIHiTZhb8/s/3078670442/br/236285728656-l">5530</a>, <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDQsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDA2L2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.d-xmdG8esN2ZFRwqLXHNvwIo6ZpVS-2HQ5js3wwfJo8/s/3078670442/br/236285728656-l">5451</a> and <a target="_blank" rel="nofollow" href="https://lnks.gd/l/eyJhbGciOiJIUzI1NiJ9.eyJidWxsZXRpbl9saW5rX2lkIjoxMDUsInVyaSI6ImJwMjpjbGljayIsInVybCI6Imh0dHBzOi8vd3d3Lmp1c3RpY2UuZ292L29wYS9tZWRpYS8xMzM2NDExL2RsP2lubGluZT0mdXRtX21lZGl1bT1lbWFpbCZ1dG1fc291cmNlPWdvdmRlbGl2ZXJ5IiwiYnVsbGV0aW5faWQiOiIyMDI0MDEzMS44OTQzMDk5MSJ9.xD5SrU9r17bUh7t5QaI4TWGGTTTlIfn6D8a3QecwFJY/s/3078670442/br/236285728656-l">5432</a>) filed by the FBI in the Southern District Court of Texas last month and released today.</p>
<p>"China's hackers are targeting American civilian critical infrastructure, pre-positioning to cause real-world harm to American citizens and communities in the event of conflict," FBI Director Christopher Wray said in a <a target="_blank" rel="nofollow" href="https://www.justice.gov/usao-sdtx/pr/us-government-disrupts-botnet-peoples-republic-china-used-conceal-hacking-critical">statement</a>. "Volt Typhoon malware enabled China to hide as they targeted our communications, energy, transportation, and water sectors."</p>

    

<p>The Feds claim the Middle Kingdom keyboard warriors downloaded a virtual private network module to the vulnerable routers and set up an encrypted communication channel to control the botnet and hide their illegal activities. Specifically: Volt Typhoon used the US-based routers and IP addresses to <a target="_blank" href="https://www.theregister.com/2024/01/31/critical_infrastructure_hacking/">target</a> US critical infrastructure, we're told.</p>
<ul>

<li><a href="https://www.theregister.com/2024/01/30/fbi_china_volt/">US shorts China's Volt Typhoon crew targeting America's criticals</a></li>

<li><a href="https://www.theregister.com/2023/05/25/china_volt_typhoon_attacks/">Five Eyes and Microsoft accuse China of attacking US infrastructure again</a></li>

<li><a href="https://www.theregister.com/2024/01/31/critical_infrastructure_hacking/">We know nations are going after critical systems, but what happens when crims join in?</a></li>

<li><a href="https://www.theregister.com/2024/01/31/ivanti_patches_zero_days/">Ivanti releases patches for VPN zero-days, discloses two more high-severity vulns</a></li>
</ul>
<p>The warrants allowed law enforcement to remotely install software on the routers to search for, and then seize or copy, information about the illicit activity before wiping the malware from the compromised devices.</p>
<p>To do this — and to limit the cops' search to routers infected with the botnet — the FBI sent specific KV Botnet commands to compromised routers to collect "non-content information about those nodes," according to the warrants.</p>

        

<p>This includes the IP address, port numbers used by infected routers to communicate with other nodes, as well as IP addresses and ports used by each node's parent, and data on the command-and-control nodes.</p>
<p>"A router that is not infected by the KV Botnet malware would not receive or respond to this command," court documents claim.&nbsp;</p>

        

<p>The Feds, along with foreign agency partners in Five Eyes nations, <a target="_blank" href="https://www.theregister.com/2023/05/25/china_volt_typhoon_attacks/">first warned</a> about this threat in May 2023.</p>
<p>Also today, the US Cybersecurity Agency and FBI <a target="_blank" rel="nofollow" href="https://www.cisa.gov/news-events/alerts/2024/01/31/cisa-and-fbi-release-secure-design-alert-urging-manufacturers-eliminate-defects-soho-routers">issued an alert</a> urging manufacturers to eliminate defects in SOHO router web management interfaces. This, according to the agencies, includes automating update capabilities, locating the web management interface on LAN-side ports, and requiring a manual override to remove security settings. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Moved My Blog from IPFS to a Server (301 pts)]]></title>
            <link>https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html</link>
            <guid>39208673</guid>
            <pubDate>Wed, 31 Jan 2024 20:07:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html">https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html</a>, See on <a href="https://news.ycombinator.com/item?id=39208673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><span>Written by</span>
    
        Neiman
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2024-01-31 00:00:33 +0100">January 31, 2024</time>
    
  </p>

  
  

  <p>It’s safe to say I was a pioneer of IPFS + ENS websites. When I set up my first ENS+IPFS website in March 2019 there were no more than 15 others. Between 2019 to 2022 I co-built an IPFS+ENS browser extension (Almonit), an IPFS+ENS search engine (Esteroids), and of course, my personal blog was available only in IPFS+ENS.</p>

<p>But today I moved my blog back to a server, and I’d like to discuss why.</p>

<p>What got me excited about peer-to-peer websites like IPFS is that, theoretically, the more visitors a website has, the more robust, censorship-resistant, and scalable it is. Again, theoretically.</p>

<p>Do you know how popular torrent files seem to live forever? I wanted the same but for websites. I imagined a website that is hard to ddos (Robust), difficult to block (Censorship-resistant), and the more readers it has, the faster it is to use it since some readers help to spread the content (Scalable).</p>

<p>I imagined a website with a big “Pin Me” button (pinning in IPFS is like seeding in BitTorrent). If a reader presses the button they wil help serve the website.</p>

<p>In practice this didn’t work out really, and for several reasons.</p>

<ol>
  <li>
    <p>IPFS users mostly don’t run their own nodes or software. Instead, they use gateways. It’s an educated guess I’m making based on what I see in the community, and based on the fact that it’s quite an inconvenience to run your own IPFS node. But even if you do run your own node, the fact you access a website doesn’t mean you pin it. Not at all.</p>

    <p>This is a huge difference from BitTorrent where the only way to get content is to run your own software, and when you download something you also share it, by default.</p>

    <p>Hence, most readers will not help share the website, but even for the ones who will there are still extra complications:</p>
  </li>
  <li>
    <p>Websites are dynamic objects. Their content is being updated all the time. If you just pin the content of the current version of a website, that’s not much help.</p>

    <p>What most IPFS websites do is use a name system that points to the latest version of its content. It’s usually either IPNS, the internal name system of IPFS, or ENS, Ethereum Name System. But IPFS doesn’t include yet an easy command to always pin the latest content of IPNS, and if someone uses ENS, it means that whoever pins it also needs to listen to Ethereum blockchain events, a huge extra challenge on its own to do without a centralized service.</p>
  </li>
  <li>
    <p>To make things worse, it’s actually quite hard to get IPFS content to be available in the browser in a reliable way!</p>

    <p>For example, I wanted my IPFS blog to be available to all major gateways, all IPFS nodes, Brave browser (which supports IPFS natively), and js-libp2p &amp; helia (the js libraries of IPFS). I didn’t find a reliable way to achieve that on my own.</p>

    <p><strong>Long rant:</strong> I pinned content from my own server and played forever with settings and definitions, but couldn’t get the content to be available everywhere. Worse was with Helia, where I just couldn’t manage to get my content accessible from Helia within the browser without connecting directly to my own node. But then what’s p2p network about it?</p>

    <p>I found out that there’s a service, <a href="https://cid.contact/">cid.contact</a>, called a “Content Routing” service. It’s written in the “about section” of cid.contact it’s related to Filecoin, but for some reason, it holds routing data for IPFS, which as far as I know is a different network. The address cid.contact is hard-coded into Helia’s code in the version I used at least, and it was clear that if a content is indexed by cid.contact, then it’s reachable almost everywhere, but if it’s not indexed - it’s not reachable always.</p>

    <p>I couldn’t figure out how to index my content in cid.contact. Honestly, I’m not sure I wanted to. Because what’s the point? it seems to just add a dependency on a centralized service. I could try to run my own indexer and define it in my website, but again, centralization. What’s the economic model for these indexers? Which actors do we expect to run them in the long term?</p>

    <p>The text in cid.contact says that at the current size of IPFS, it’s unreasonable to expect the DHT to handle routing efficiently on its own. This kind of makes sense, but what’s the alternative to that, that doesn’t break up the technology pros?</p>
  </li>
</ol>

<p>By now I got tired of the constant struggle for my IPFS blog to function well. At least for a short while, I want a simple, classic working solution. The blog you’re reading now is built with Jekyll and is hosted on my own 10$ server.</p>

<p>don’t get me wrong, I’m still an IPFS fanboy. It’s a great project managed very well. It just doesn’t fit a personal blog needs yet.</p>

<p>That said, It’s difficult to follow the constant development and innovation of IPFS or Filecoin without this becoming a day job. Did I miss some trivial solution or a recent innovation? If yes, let me know. There are no comments here yet, but I am available via old-style email (neiman@hackerspace.pl), or in Mastodon (@neiman@mastodon.social).</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fighting Infomania: Why 80% of Your Reading Is a Waste of Time (2016) (102 pts)]]></title>
            <link>https://www.nateliason.com/blog/infomania</link>
            <guid>39208629</guid>
            <pubDate>Wed, 31 Jan 2024 20:03:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nateliason.com/blog/infomania">https://www.nateliason.com/blog/infomania</a>, See on <a href="https://news.ycombinator.com/item?id=39208629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I returned to college Junior Fall dissatisfied. I’d just finished a summer internship doing management consulting with a government contracting firm, and in the process, realized that working in a company wasn’t for me.</p><p>The original plan had been to leverage it into a better internship somewhere else, and eventually a full-time offer, but with how much I disliked the work I knew something had to change.</p><p>Then I hit a snag: I wanted to get into startups or lifestyle businesses, but how was I supposed to&nbsp;<em>get there</em>?</p><p>With most corporate work you can go the traditional route of school -&gt; internship -&gt; job, but skill-based jobs at smaller companies don’t have such a clear route. And this lack of a clear route is what stops many students from going after&nbsp;<a href="https://www.nateliason.com/fun-work/">fun work</a>.</p><p>Faced with not knowing the way forward, or where to begin with “starting a startup,” I began reading&nbsp;<em>everything</em>.</p><p>Over the next 6 months, I&nbsp;<a href="https://www.nateliason.com/can-easily-read-100-books-year/" target="_blank">read 30+ books</a>&nbsp;on entrepreneurship, startups, marketing, “growth hacking,” and everything tangentially related I could find. And that doesn’t include the countless blog posts, articles, reddit threads, and whatever else I could get my hands on.</p><p>A good plan, right? No, 80% of it was a waste of time,&nbsp;<strong>and most people make the same mistake with how they consume information every day</strong>.</p><h2>Just in Time vs Just in Case</h2><p>Toyota’s ability to kill it in the car market came from their (then) revolutionary manufacturing process:&nbsp;<a href="https://en.wikipedia.org/wiki/Just-in-time_manufacturing" target="_blank">just in time manufacturing</a>.</p><p>While Detroit and other car companies would pump out cars “just in case” they were purchased, Toyota would make them “just in time” for their orders. This saved them millions of dollars by minimizing unused inventory, and the cost savings allowed them to speed up their manufacturing process significantly with new technology.</p><p>What Toyota realized was that by creating cars just in time, they could move faster, respond to market changes quicker, and only make cars that they knew they needed.</p><p>You should approach tactical knowledge the same way.</p><p>Sidebar: There’s an important distinction here between&nbsp;tactical&nbsp;knowledge (e.g. how to do search engine optimization), and&nbsp;philosophical&nbsp;knowledge (e.g. understanding our tendency towards&nbsp;biased thinking). The first, tactical knowledge, is what we’re concerned with overdosing on.</p><p>The school model focuses on just in case knowledge. You take classes and study textbooks that cram knowledge into you hoping that one day you might find it useful. But that’s not how&nbsp;<a href="https://www.nateliason.com/self-education/" target="_blank">learning works in the real world</a>.</p><h2>Where We Make “Just In Case” Mistakes in the Real World</h2><p>When I was working on&nbsp;<a href="https://www.nateliason.com/life-after-sumome/" target="_blank">marketing at SumoMe</a>, I never followed any marketing blogs. I didn’t check my Feedly when I got to work, didn’t look on GrowthHackers or Inbound to see what was trending, and didn’t have any newsletters I was subscribed to.</p><p>Conventional wisdom says that you should follow what people in your industry are talking about tactics-wise, but it’s just noise. You should have the strategy and tactics that you’re working on, and then you should execute on them. Constantly listening to and checking what other people are doing doesn’t help you stay focused–it just makes you question yourself.</p><p>The exception to this is if you can find a site that publishes articles very infrequently but where they are very high quality (for example,&nbsp;<a href="http://backlinko.com/" target="_blank">Backlinko</a>&nbsp;for SEO).</p><h3>The Problem of Frequency</h3><p>As a general rule, the more frequently a site publishes about tactics (marketing, personal finance, weight loss, etc.), the&nbsp;<em>less</em>&nbsp;you should listen to it.</p><p>No topic is sufficiently complex that you need new information on executing on it every day. Getting in shape requires doing a few very simple things every day for months, not finding a new 13 minute 6 step workout every day so you can have a butt like today’s hot celebrity.</p><p>That’s part of the reason I’ve deliberately avoided having a narrow theme to my writing. You don’t need an entire site on&nbsp;<a href="https://www.nateliason.com/last-longer-in-bed/" target="_blank">lasting longer in bed</a>&nbsp;or&nbsp;<a href="https://www.nateliason.com/5-day-water-fast-health-benefits/" target="_blank">water fasting</a>, you just need one or a couple really good articles.</p><h3>The Problem of Recency</h3><p>Then, there’s the issue with new articles. We tend towards&nbsp;<em>neomania:</em>&nbsp;overly focusing on the new and shiny, when new and shiny things tend to be the quickest to go and the least likely to be valuable.</p><p>A safer bet is to see what content has stood the test of time, following the&nbsp;<em>Lindy Rule.&nbsp;</em>Anything that has been around for 50 years will probably be around for another 50 years, but something that’s been around a few days has no proof of staying power.</p><p>Will people be reading Aristotle in another 2,000 years? Probably. Will they be reading this week’s pop-business book in another 10 years? Probably not.</p><h2>Why We Do It</h2><p>There are three reasons why we feel compelled to constantly consume knowledge about things we’re working on, and I’ll explain the problems with each.</p><h3>1. Habit</h3><p>We carry that “just in case” habit with us from school, and feel like we’re not doing our job if we’re not front-loading knowledge.</p><p>But it should be clear the fault here now. Just because we were taught that way doesn’t mean it’s the best way, it’s simply the way things have always been done.</p><h3>2. History</h3><p>Prior to 20 years ago, you wanted to err on the side of just in case knowledge since getting new information was so difficult.</p><p>That’s not the case anymore. Anything you could possibly want to learn you could figure out the basics of in an afternoon with a WiFi connection. You don’t have to worry about front loading everything because you’ll hardly ever be in a situation where you can’t look up the answers.</p><h3>3. Fauxductivity</h3><p>Then there’s the fact that doing it makes us feel like we’re being productive when we’re really just slacking off, a phenomenon I’m going to call “Fauxductivity.”</p><p>Any time you’re doing something that feels productive&nbsp;but doesn’t directly impact your most important goal, you’re being fauxductive. That includes bingeing on just in case knowledge, as well as checking email,&nbsp;<a href="https://www.nateliason.com/news-waste-time/" target="_blank">reading the news</a>, trying productivity tools, organizing your desktop, etc.</p><h2>1 Rule to Fight Infomania</h2><p>Here’s your new rule for information, especially blogs and popular non-fiction books:</p><p><em><strong>If it doesn’t answer a specific question you’re currently asking, cover philosophical knowledge, or entertain you, then don’t read it.</strong></em></p><p>Unfollow the industry blogs, stop reading the newspaper, don’t go on site aggregators, quit it. It’s all fauxductivity and wasting your time with useless information that will only cause&nbsp;<a href="https://www.nateliason.com/decision-fatigue/" target="_blank">decision paralysis</a>, self-doubt, and decreased confidence in your existing decisions.</p><p>When you have a specific question (e.g. how do I grow my Instagram following) that’s when you start digging through the blogs and industry material. But only until you have enough information to answer your question and formulate a plan. Don’t fill up your RSS with every social media and Instagram blog out there hoping you stumble on something else tomorrow.</p><p>If you can do this, you’ll spend significantly less time reading things that you never use, and you’ll get more information and mileage out of what you do read.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Some blind hackers are bridging IRC to LMMs running locally (186 pts)]]></title>
            <link>https://2mb.codes/~cmb/ollama-bot/</link>
            <guid>39208451</guid>
            <pubDate>Wed, 31 Jan 2024 19:50:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://2mb.codes/~cmb/ollama-bot/">https://2mb.codes/~cmb/ollama-bot/</a>, See on <a href="https://news.ycombinator.com/item?id=39208451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>This is a rudimentary IRC bot that communicates with a local instance
of <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a>.</p>
<h3 id="installation"><a href="#installation" rel="nofollow noopener">#</a>Installation</h3>
<p>You'll need to have <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a> installed in order to
actually use the bot.  Instructions for doing that are out of scope for
this document.  Then, build the bot from source:</p>
<pre><code>git clone 'https://git.2mb.codes/~cmb/ollama-bot
cd ollama-bot
go build
</code></pre>
<p>Put the ollama-bot binary on your <code>$PATH</code>.
We run this on a Void Linux host.  Runit services for both the bot and
ollama itself are included in the ollama-bot repository, along with some
instructions for getting it running under runit.  If you use something else,
you're on your own, but of course, patches are welcome.</p>
<h3 id="configuration"><a href="#configuration" rel="nofollow noopener">#</a>Configuration</h3>
<p>The program searches for its configuration file in one of several places,
in the following order, and uses the first file it finds:</p>
<pre><code>~/.config/ollama-bot/ollama-bot.toml
/usr/local/etc/ollama-bot/ollama-bot.toml
/etc/ollama-bot/ollama-bot.toml
</code></pre>
<p>The configuration file should contain one or more network definitions.
I.E., it is an array of tables using toml notation.  Look at the file
<code>config.toml.sample</code> for a sample configuration.</p>
<h3 id="usage"><a href="#usage" rel="nofollow noopener">#</a>Usage</h3>
<p>Once the bot is connected to one or more IRC networks, you can converse
with it, either by highlighting it in a channel, or by sending it a private
message.  For the most part, you just send it free-form text, and it will
respond with free-form text.  However, it also accepts various commands,
preceded by an exclamation mark <code>!</code>.  Send <code>!help</code> for a list of commands.
Use of the commands is best demonstrated by an example chat transcript.</p>
<h4 id="chat-transcript"><a href="#chat-transcript" rel="nofollow noopener">#</a>Chat Transcript</h4>
<pre><code>&lt;chris&gt; ai-bot: !help
&lt;ai-bot&gt; 
&lt;ai-bot&gt; This bot supports the following commands:
&lt;ai-bot&gt; !describe URL [model] -- Describe image at URL, optionally choosing model
&lt;ai-bot&gt; !switch-text-model -- Start a conversation using a different text model
&lt;ai-bot&gt; !list-text-models -- List models usable for text conversation
&lt;ai-bot&gt; !list-image-models -- List models usable for image description
&lt;ai-bot&gt; !help -- This command.
&lt;chris&gt; ai-bot: !list-image-models
&lt;ai-bot&gt; This bot supports the following image models: llava, llava:13b, bakllava
&lt;chris&gt; ai-bot: !list-text-models
&lt;ai-bot&gt; This bot supports the following text models: mistral:instruct, llama2, llama2-uncensored
&lt;chris&gt; ai-bot: !describe https://the-brannons.com/chris-and-bird.jpg llava:13b
&lt;ai-bot&gt;  The image features a bearded man with a bird perched on his shoulder. The bird is small, likely a parrot or a similar type of pet bird. The man appears to be in a relaxed setting, possibly sitting down while enjoying the company with his feathered companion.
&lt;chris&gt; ai-bot: What kind of beard does he have?
&lt;ai-bot&gt;  While I cannot precisely describe the man's beard style without more specific information, it is safe to say that he has some form of facial hair on his face. This could range from a full beard to a goatee or even just a five o'clock shadow.
# Use the default model, bakllava.  This resets conversation state.
&lt;chris&gt; ai-bot: !describe https://the-brannons.com/chris-and-bird.jpg
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The image features a bearded man with a bird perched on his shoulder. The man has an odd facial expression, almost as if he is looking into the camera. The bird appears to be a parrot and is sitting close to the man's ear.
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The scene takes place in a room furnished with a couch and a chair. The couch is positioned in the background, while the chair occupies a smaller portion of the frame on the left side.
&lt;chris&gt; ai-bot: Tell me about his beard?
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The bearded man in the image has an odd facial expression, making his appearance quite distinctive. His beard seems to be well-groomed and likely represents a fashionable or stylish choice in his personal grooming.
&lt;chris&gt; ai-bot: What do you mean by odd facial expression?
&lt;ai-bot&gt; 
&lt;ai-bot&gt; By odd facial expression, I refer to the man's facial features looking unnatural, exaggerated, or distorted. This may include an awkward smile, squinting, or an unusual or contorted countenance. The phrase "what the heck" implies that the person's appearance seems out of the ordinary or unexpected, which adds a humorous and quirky touch to the image.
# Switch back to text chat, selecting a model and resetting conversation state:
&lt;chris&gt; ai-bot: !switch-to-text-model llama2-uncensored
&lt;ai-bot&gt; Text model switched.
&lt;chris&gt; ai-bot: Somebody on the Internet told me that I can believe everything I read on the Internet?  Were they full of shit?
&lt;ai-bot&gt; Unfortunately, no. This statement is not true. There are many websites and sources on the internet that provide incorrect or misleading information. It's important to be careful when using any online source for information and to verify it by checking multiple sources or contacting a reliable expert.
&lt;chris&gt; ai-bot: But Wikipedia is always reliable!
&lt;ai-bot&gt; Wikipedia is an excellent resource, but it's not infallible. Like any encyclopedia, it can be edited and updated by anyone with access to the website. While most of the information on Wikipedia is accurate, there are some inaccuracies and errors that may exist due to human error or deliberate vandalism. It's always a good idea to cross-reference information from different sources before relying solely on any one source.
&lt;chris&gt; ai-bot: Could you tell me about some known examples of errors in Wikipedia?  Otherwise, I'm just gonna assume you're full of it.
&lt;ai-bot&gt; Sure! One well-known example of an error in Wikipedia is the article on Abraham Lincoln. The entry stated that Lincoln was the first president to receive a patent for his invention - which was actually issued after he left office. Another example is the article on the Apollo 11 mission, which claimed that Neil Armstrong's quote "One small step" was misquoted from an interview with President John F. Kennedy. However, NASA confirmed that the quote used in Wikipedia w
&lt;ai-bot&gt; as
&lt;ai-bot&gt; accurate and had been recorded during the mission.
</code></pre>
<h3 id="why"><a href="#why" rel="nofollow noopener">#</a>Why?</h3>
<p>Because most of the people in the group behind this are hard-core free software
people to some degree or other.  We want to see access to AI liberated and
democratized, I.E., free as in freedom.  And almost as importantly, it sounded
like fun when we were bored on a Friday afternoon.</p>
<h3 id="legal"><a href="#legal" rel="nofollow noopener">#</a>Legal</h3>
<p>Licensed as GPL-2 to match the license of <code>irc-go</code>.</p>
<h3 id="thanks"><a href="#thanks" rel="nofollow noopener">#</a>Thanks</h3>
<p>The group behind ollama-bot consists of Kirk, Tyler, Mike, and Chris, plus the peanut gallery on IRC.</p>
<p>We use the <a href="https://git.zx2c4.com/irc-go" rel="nofollow noopener">irc-go</a> package from Jason Donenfeld.
This seems to have been an excellent choice.</p>
<p>This project would not be possible without <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a>.</p>
<p>Code hosting is graciously provided by the fine people at <a href="https://2mb.solutions/" rel="nofollow noopener">2MB Solutions</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral CEO confirms 'leak' of new open source AI model nearing GPT4 performance (351 pts)]]></title>
            <link>https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/</link>
            <guid>39208213</guid>
            <pubDate>Wed, 31 Jan 2024 19:32:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/">https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=39208213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-01-31T18:44:56+00:00" datetime="2024-01-31T18:44:56+00:00">January 31, 2024 10:44 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="420" src="https://venturebeat.com/wp-content/uploads/2024/01/cfr0z3n_overhead_view_of_the_eiffel_tower_rising_from_a_landsca_d374695a-bca7-4db7-abcd-d30ef5fd0e04.png?fit=750%2C420&amp;strip=all" alt="Overhead view of Eiffel tower in a Paris made of circuit boards."></p><p><span>Credit: VentureBeat made with Midjourney V6</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary" role="main">

			<article id="post-2932274">
				<div>
					<p>The past few days have been a wild ride for the growing open source AI community — even by its fast-moving and freewheeling standards.</p>



<p>Here’s the quick chronology: on or about January 28, a user with the handle “Miqu Dev” <a href="https://huggingface.co/miqudev/miqu-1-70b" target="_blank" rel="noreferrer noopener">posted a set of files on HuggingFace</a>, the leading open source AI model and code sharing platform, that together comprised a seemingly new open source large language model (LLM) labeled “miqu-1-70b.”</p>



<p>The HuggingFace entry, which is still up at the time of this article’s posting, noted that new LLM’s “Prompt format,” how users interact with it, was the same as <a href="https://mistral.ai/">Mistral</a>, the <a href="https://www.ft.com/content/cf939ea4-d96c-4908-896a-48a74381f251" target="_blank" rel="noreferrer noopener">well-funded open source Parisian AI company</a> behind <a href="https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/" target="_blank" rel="noreferrer noopener">Mixtral 8x7b</a>, viewed by many to be the top performing open source LLM presently available, a fine-tuned and retrained version of Meta’s Llama 2. </p>



<h2 id="h-posted-on-4chan">Posted on 4chan</h2>



<p>The same day, an anonymous user on 4chan (possibly “Miqu Dev”) posted a <a href="https://boards.4chan.org/g/thread/98696032#p98697258" target="_blank" rel="noreferrer noopener">link to the miqu-1-70b files on 4chan</a>, the notoriously longstanding haven of online memes and toxicity, where users began to notice it.</p>



<p>Some took to X, Elon Musk’s social network formerly known as Twitter, to share the discovery of the model and what appeared to be its exceptionally high performance at common LLM tasks (measured by tests known as benchmarks), approaching the previous leader, OpenAI’s GPT-4 on the <a href="https://eqbench.com/" target="_blank" rel="noreferrer noopener">EQ-Bench</a>. </p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Whatever Miqu is, it has some sort of special sauce. It gets an 83.5 on EQ-Bench (evaluated locally), surpassing *every other LLM in the world except GPT-4*. EQ-Bench has a 0.97 correlation w/ MMLU, and a 0.94 correlation w/ Arena Elo. It *beats* Mistral Medium – at Q4_K_M. I… <a href="https://t.co/0gOOPjxjPD">pic.twitter.com/0gOOPjxjPD</a></p>— N8 Programs (@N8Programs) <a href="https://twitter.com/N8Programs/status/1752441060133892503?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Might be late but I am now 100% convinced that Miqu is the same model that's accessible as Mistral-Medium on Perplexity Labs. It was plausible that it knows standard puzzles, but there ain't no way in Hell a pranker has tuned it to identically phrase the responses in Russian too. <a href="https://t.co/zZMcpspXch">pic.twitter.com/zZMcpspXch</a></p>— Teortaxes▶️ (@teortaxesTex) <a href="https://twitter.com/teortaxesTex/status/1752427812466593975?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<h2 id="h-mistral-quantized">Mistral quantized?</h2>



<p>Machine learning (ML) researchers took notice on <a href="https://www.linkedin.com/posts/maxime-labonne_new-open-source-llm-competes-with-gpt-activity-7158414579671199744-MFfA/" target="_blank" rel="noreferrer noopener">LinkedIn</a>, as well. </p>



<div><p><em>“Does ‘miqu’ stand for MIstral QUantized? We don’t know for sure, but this quickly became one of, if not the best open-source LLM,”</em> wrote Maxime Labonne, an ML scientist at JP Morgan &amp; Chase, one of the world’s largest banking and financial companies.<em> “Thanks to @152334H, we also now have a good unquantized version of miqu here: <a href="https://lnkd.in/g8XzhGSM">https://lnkd.in/g8XzhGSM</a></em></p><p><em>The investigation continues. Meanwhile, we might see fine-tuned versions of miqu outperforming GPT-4 pretty soon.</em>“</p></div>



<p><a href="https://huggingface.co/docs/optimum/concept_guides/quantization" target="_blank" rel="noreferrer noopener">Quantization</a> in ML refers to a technique used to make it possible to run certain AI models on less powerful computers and chips by replacing specific long numeric sequences in a model’s architecture with shorter ones. </p>



<p>Users speculated “Miqu” might be a new Mistral model being covertly “leaked” by the company itself into the world — especially since Mistral is known for <a href="https://venturebeat.com/ai/mistral-ai-bucks-release-trend-by-dropping-torrent-link-to-new-open-source-llm/" target="_blank" rel="noreferrer noopener">dropping new models and updates without fanfare</a> through esoteric and technical means — or perhaps an employee or customer gone rouge.</p>



<h2 id="h-confirmation-from-the-top">Confirmation from the top</h2>



<p>Well, today it appears we finally have confirmation of the latter of those possibilities: Mistral co-founder and CEO Arthur Mensch took to X to clarify: <em>“An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly…</em></p>



<p><em>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got access to our entire cluster — the pretraining finished on the day of Mistral 7B release. We’ve made good progress since — stay tuned!<a href="https://twitter.com/arthurmensch/status/1752737462663684344/history"></a>“</em></p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><div lang="en" dir="ltr"><p>An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly.</p><p>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got…</p></div>— Arthur Mensch (@arthurmensch) <a href="https://twitter.com/arthurmensch/status/1752737462663684344?ref_src=twsrc%5Etfw">January 31, 2024</a></blockquote>
</div></figure>



<p>Hilariously, Mensch also appears to have taken to the illicit HuggingFace post not to demand a takedown, but leaving a comment that the poster “might consider attribution.”</p>



<figure></figure>



<p>Still, with Mensch’s note to “stay tuned!” it appears that not only is Mistral training a version of this so-called “Miqu” model that approaches GPT-4 level performance, but it may, in fact, match or exceed it, if his comments are to be interpreted generously. </p>



<h2 id="h-a-pivotal-moment-in-open-source-ai-and-beyond">A pivotal moment in open source AI and beyond?</h2>



<p>That would be a watershed moment not just for open source generative AI but the entire field of AI and computer science: <a href="https://openai.com/research/gpt-4" target="_blank" rel="noreferrer noopener">since its release back in March 2023</a>, GPT-4 has remained the most powerful and highest performing LLM in the world by most benchmarks. Not even any of <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">Google’s presently available, long-rumored Gemini models</a> have been able to eclipse it — yet (according to some measures, the current Gemini models are <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">actually worse than the older OpenAI GPT-3.5 mode</a>l). </p>



<p>The release of an open source GPT-4 class model, which would presumably be functionally free to use, would likely place enormous competitive pressure on OpenAI and its subscription tiers, especially as more enterprises look to open source models, or a mixture of open source and closed source, to power their applications, <a href="https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples/">as VentureBeat’s founder and CEO Matt Marshall recently reported</a>. OpenAI may retain the edge with its faster GPT-4 Turbo and GPT-4V (vision), but the writing on the wall is pretty clear: the open source AI community is catching up fast. Will OpenAI have enough of a head start, and a metaphorical “moat” with its <a href="https://venturebeat.com/ai/openai-launches-gpt-store-but-revenue-sharing-is-still-to-come/">GPT Store</a> and other features, to remain in the top spot for LLMs? </p>
<p><strong>VentureBeat's mission</strong> is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=bottomBoilerplate" data-type="URL" data-id="/newsletters/">Discover our Briefings.</a></p><!-- Boilerplate CSS for "after" -->				</div><!-- .article-content -->

									
				
			</article><!-- #post-2932274 .article-wrapper -->


		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building an early warning system for LLM-aided biological threat creation (108 pts)]]></title>
            <link>https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation</link>
            <guid>39207291</guid>
            <pubDate>Wed, 31 Jan 2024 18:15:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation">https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation</a>, See on <a href="https://news.ycombinator.com/item?id=39207291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><em>Note: As part of our </em><a href="http://openai.com/safety/preparedness" rel="noopener noreferrer" target="_blank"><em>Preparedness Framework</em></a><em>, we are investing in the development of improved evaluation methods for AI-enabled safety risks. We believe that these efforts would benefit from broader input, and that methods-sharing could also be of value to the AI risk research community. To this end, we are presenting some of our early work—today, focused on biological risk. We look forward to community feedback, and to sharing more of our ongoing research.&nbsp;</em></p></div><p><strong>Background. </strong>As OpenAI and other model developers build more capable AI systems, the potential for both beneficial and harmful uses of AI will grow. One potentially harmful use, highlighted by researchers and policymakers, is the ability for AI systems to assist malicious actors in creating biological threats (e.g., see <a href="https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/" rel="noopener noreferrer" target="_blank">White House 2023</a>, <a href="https://www.washingtontimes.com/news/2022/sep/12/ai-powered-biological-warfare-biggest-issue-former/" rel="noopener noreferrer" target="_blank">Lovelace 2022</a>, <a href="https://www.vox.com/future-perfect/23820331/chatgpt-bioterrorism-bioweapons-artificial-inteligence-openai-terrorism" rel="noopener noreferrer" target="_blank">Sandbrink 2023</a>). In one discussed hypothetical example, a malicious actor might use a highly-capable model to develop a step-by-step protocol, troubleshoot wet-lab procedures, or even autonomously execute steps of the biothreat creation process when given access to tools like <a href="https://www.theguardian.com/science/2022/sep/11/cloud-labs-and-remote-research-arent-the-future-of-science-theyre-here" rel="noopener noreferrer" target="_blank">cloud labs</a> (see <a href="https://www.nti.org/analysis/articles/the-convergence-of-artificial-intelligence-and-the-life-sciences/" rel="noopener noreferrer" target="_blank">Carter et al., 2023</a>). However, assessing the viability of such hypothetical examples was limited by insufficient evaluations and data.</p><p>Following our recently shared <a href="https://cdn.openai.com/openai-preparedness-framework-beta.pdf" rel="noopener noreferrer" target="_blank">Preparedness Framework</a>, we are developing methodologies to empirically evaluate these types of risks, to help us understand both where we are today and where we might be in the future. Here, we detail a new evaluation which could help serve as one potential “tripwire” signaling the need for caution and further testing of biological misuse potential. This evaluation aims to measure whether models could meaningfully increase malicious actors’ access to dangerous information about biological threat creation, compared to the baseline of existing resources (i.e., the internet).</p><p>To evaluate this, we conducted a study with 100 human participants, comprising (a) 50 biology experts with PhDs and professional wet lab experience and (b) 50 student-level participants, with at least one university-level course in biology. Each group of participants was randomly assigned to either a control group, which only had access to the internet, or a treatment group, which had access to GPT-4 in addition to the internet. Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation.<span><sup><span>[^1]</span></sup><!----></span> To our knowledge, this is the largest to-date human evaluation of AI’s impact on biorisk information.</p><p><strong>Findings.<em> </em></strong>Our study assessed uplifts in performance for participants with access to GPT-4 across five metrics (accuracy, completeness, innovation, time taken, and self-rated difficulty) and five stages in the biological threat creation process (ideation, acquisition, magnification, formulation, and release). We found mild uplifts in accuracy and completeness for those with access to the language model. Specifically, on a 10-point scale measuring accuracy of responses, we observed a mean score increase of 0.88 for experts and 0.25 for students compared to the internet-only baseline, and similar uplifts for completeness (0.82 for experts and 0.41 for students). However, the obtained effect sizes were not large enough to be statistically significant, and our study highlighted the need for more research around what performance thresholds indicate a meaningful increase in risk. Moreover, we note that information access alone is insufficient to create a biological threat, and that this evaluation does not test for success in the physical construction of the threats.</p><p>Below, we share our evaluation procedure and the results it yielded in more detail. We also discuss several methodological insights related to capability elicitation and security considerations needed to run this type of evaluation with frontier models at scale. We also discuss the limitations of statistical significance as an effective method of measuring model risk, and the importance of new research in assessing the meaningfulness of model evaluation results.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kando: The Cross-Platform Pie Menu (138 pts)]]></title>
            <link>https://github.com/kando-menu/kando</link>
            <guid>39206966</guid>
            <pubDate>Wed, 31 Jan 2024 17:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kando-menu/kando">https://github.com/kando-menu/kando</a>, See on <a href="https://news.ycombinator.com/item?id=39206966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:kando-menu/kando" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="ZjgF1FIoJ3Gm7wxtqnKw8WzmM6JDZjaI7SjhtwmwHx649CPXFc08BGilSfuY00Y5ms6f1tyLljUiyYpRUrrTKQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="kando-menu/kando" data-current-org="kando-menu" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=FCltbRivJMkrLPIMV7rr6Q2kg6xiv2TeI%2BTOV043onkp1lbR8qELatdabL0UHFNXyqqrj49KAqn6E5syczmB18ZESFRFMDU5n4wISzeG%2FeqN3%2BYM4cWXFkbuacAy1pOr25L2%2BJq8QeJPCvoEyXiyl4t1wvfJzah5y0oqwUQYsp7KNGlAccxSwMWiMSpwDnQkShsEYHIxSa5a9V%2B7S2mpQbe42DE%2B%2BGgKuBERiRvnVhXjtqlJF12PTgc5hTDN9sC%2Fwx9B8izY3%2FqC2EDzIX%2BS0XYmjIzWUBGLn%2BKUq4QWSRByush9himQxNT4UmXbxhShw2bPveHXDx0FPPRaGkgIvVwL9WWSGNRuLtdJkMestC9lfCEUbWMeLGwmLgUCYSO9dmHTyVII7C%2F9nWtshvzQfPEuC9jayF6h%2Fpn%2BYh0FbTDmEp0rR2Gdxx5mQiqmXc%2F%2FBH6tBKbO2SgNVY24PyQG55kvu6DZcAaZLDGDG59BS2f6oAN6sUlNwwzLvBsHl3PCWRhg49cWDPOltTLTKxA%3D--5%2FaNfF%2F%2BXue6MB0N--HRT%2BZRbFbvbxVReIj5Hpjw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=kando-menu%2Fkando" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/kando-menu/kando&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="71bbb3ccf2095432fdae0dfca361a935a2c104a0221d8343d9f88b26f6fcb68c" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hearing aids slow cognitive decline in people at high risk (157 pts)]]></title>
            <link>https://www.nih.gov/news-events/nih-research-matters/hearing-aids-slow-cognitive-decline-people-high-risk</link>
            <guid>39206911</guid>
            <pubDate>Wed, 31 Jan 2024 17:47:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nih.gov/news-events/nih-research-matters/hearing-aids-slow-cognitive-decline-people-high-risk">https://www.nih.gov/news-events/nih-research-matters/hearing-aids-slow-cognitive-decline-people-high-risk</a>, See on <a href="https://news.ycombinator.com/item?id=39206911">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content">
                <h2>You are here</h2>          
                <div about="/news-events/nih-research-matters/hearing-aids-slow-cognitive-decline-people-high-risk" typeof="sioc:Item foaf:Document" role="main">

  
  <p><span property="dc:date" datatype="xsd:dateTime" content="2023-08-08T00:00:00-04:00">August 8, 2023</span></p><div><div><h3>At a Glance</h3>

<ul>
	<li>Hearing aids reduced the rate of cognitive decline in older adults at high risk of dementia by almost 50% over a three-year period.</li>
	<li>Treating hearing loss may be a safe way to lower the risk of dementia in vulnerable populations.</li>
</ul></div><p>As the world population ages, the number of people living with dementia and other types of cognitive impairment continues to rise. Safe and affordable interventions to prevent or slow age-related cognitive decline are greatly needed.</p>

<p>Studies have found an association between hearing loss and the development of dementia in older adults. Research also suggests that using hearing aids to treat hearing loss may help slow the development of cognitive problems. But potential links between hearing loss treatment and improved cognition hadn’t been tested in a large, randomized study.</p>

<p>To close this gap, an NIH-funded research team co-led by Dr. Frank Lin from Johns Hopkins University enrolled nearly 1,000 adults, ages 70 to 84, into a clinical trial. They compared the rate of cognitive decline over a three-year period between people who did and didn’t receive hearing aids.</p>

<p>The researchers recruited participants with substantial hearing loss from two different study populations. About 250 healthy older adults came from a long-term study of heart health at four sites in the U.S. Another 739 people were newly recruited from the surrounding communities at the same four sites. Participants recruited from the heart-health study had more risk factors for developing dementia, including being older and having faster rates of cognitive decline than those from the general community.</p>

<p>The researchers randomly assigned participants to one of two interventions. About half received hearing aids and instruction in how to use them. The other half were assigned to a health education program focused on promoting healthy aging. Both groups received follow-up visits every six months to reinforce the training.</p>

<p>Before the start of the study and annually for three years, all participants took a battery of tests to measure cognitive functioning. The results were published on July 17, 2023, in <em>The Lancet</em>.</p>

<p>People who received the hearing aids reported substantial improvement in communication abilities over the course of the study. Those in the health-education group, as expected, did not experience improvement in hearing and communication.</p>

<p>In the main analysis of all study participants, the researchers saw no difference in the rate of change in cognitive functioning between people who received the hearing aids and those who didn’t.</p>

<p>However, when the analysis focused on people from the heart-health study, who had a higher risk of dementia, the benefit of the hearing aids was substantial. Those who received hearing aids had an almost 50% reduction in the rate of cognitive decline compared with people in the health-education group.</p>

<p>“Hearing loss is very treatable in later life, which makes it an important public health target to reduce risk of cognitive decline and dementia,” Lin says.</p>

<p>The researchers are continuing to follow the study participants to see how changes in cognition develop over time. They’re also looking at brain scans and data on social engagement to better understand how protecting hearing may prevent cognitive decline in vulnerable older adults.</p>

<p>“Until we know more,” Lin adds, “we recommend for general health and well-being that older adults have their hearing checked regularly and any hearing issues properly addressed.”</p>

<p>—by Sharon Reynolds</p></div><h2>Related Links</h2><ul><li><a href="https://www.nih.gov/news-events/nih-research-matters/diagnosing-hidden-hearing-loss" target="_top">Diagnosing Hidden Hearing Loss</a></li><li><a href="https://www.nih.gov/news-events/nih-research-matters/healthy-diets-linked-lower-risk-hearing-loss-women" target="_top">Healthy Diets Linked to Lower Risk of Hearing Loss in Women</a></li><li><a href="https://www.nih.gov/news-events/nih-research-matters/protein-involved-hearing-loss-recovery" target="_top">Protein Involved in Hearing Loss Recovery</a></li><li><a href="https://newsinhealth.nih.gov/2018/05/struggling-hear" target="_top">Struggling to Hear? Tiny Devices Can Keep You Connected</a></li><li><a href="https://www.nidcd.nih.gov/health/age-related-hearing-loss" target="_top">Age-Related Hearing Loss (Presbycusis)</a></li><li><a href="https://www.nidcd.nih.gov/health/hearing-aids" target="_top">Hearing Aids</a></li><li><a href="https://www.nia.nih.gov/health/topics/hearing-and-hearing-loss" target="_top">Hearing and Hearing Loss</a></li><li><a href="https://www.nia.nih.gov/health/alzheimers" target="_top">Alzheimer's Disease &amp; Related Dementias</a></li><li><a href="https://www.nia.nih.gov/health/cognitive-health-and-older-adults" target="_top">Cognitive Health and Older Adults</a></li></ul><p><strong>References:&nbsp;</strong><a href="https://pubmed.ncbi.nlm.nih.gov/37478886/">Hearing intervention versus health education control to reduce cognitive decline in older adults with hearing loss in the USA (ACHIEVE): a multicentre, randomised controlled trial.</a> Lin FR, Pike JR, Albert MS, Arnold M, Burgard S, Chisolm T, Couper D, Deal JA, Goman AM, Glynn NW, Gmelin T, Gravens-Mueller L, Hayden KM, Huang AR, Knopman D, Mitchell CM, Mosley T, Pankow JS, Reed NS, Sanchez V, Schrack JA, Windham BG, Coresh J; ACHIEVE Collaborative Research Group. <em>Lancet</em>. 2023 Jul 17:S0140-6736(23)01406-X. doi: 10.1016/S0140-6736(23)01406-X. Online ahead of print. PMID: 37478886.</p><p><strong>Funding:&nbsp;</strong>NIH’s National Institute on Aging (NIA) and National Heart, Lung, and Blood Institute (NHLBI); Eleanor Schwartz Charitable Foundation.</p>  
  
</div>
                
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Testing how hard it is to cheat with ChatGPT in interviews (225 pts)]]></title>
            <link>https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews</link>
            <guid>39206731</guid>
            <pubDate>Wed, 31 Jan 2024 17:35:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews">https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews</a>, See on <a href="https://news.ycombinator.com/item?id=39206731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>ChatGPT has revolutionized work as we know it. From helping small businesses automate their administrative tasks to coding entire React components for web developers, its usefulness is hard to overstate.</p>
<p>At interviewing.io, we've been thinking a lot about how ChatGPT will change technical interviewing. <strong>One big question is: Does ChatGPT make it easy to cheat in interviews?</strong> To decide for yourself, check out this 45-second video. In it, an engineer gets ChatGPT to show him exactly how to respond to an interviewer’s questions.</p>
<a href="https://www.tiktok.com/@kagehiromitsuyami/video/7296566544602123566">
<blockquote cite="https://www.tiktok.com/@kagehiromitsuyami/video/7296566544602123566" data-video-id="7296566544602123566"> </blockquote> 
</a>
<p>ChatGPT integration to help you cheat in interviews!</p>
<p>Crazy, right? Initial responses to cheating software like this have been pretty in line with what you’d expect:</p>

<p>It seems clear that ChatGPT can assist people during their interviews, but we wanted to know:</p>
<ul>
<li>How much can it help?</li>
<li>How easy is it to cheat (and get away with it)?</li>
<li>Will companies that ask LeetCode questions need to make significant changes to their interview process?</li>
</ul>
<p>To answer these questions, we recruited some of our professional interviewers and users for a cheating experiment! Below, we’ll share everything we discovered and explain what it means for you. As a little preview, just know this: companies need to change the types of interview questions they are asking—immediately.</p>
<h2>The experiment</h2>
<p>interviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. We have thousands of professional interviewers in our ecosystem, and hundreds of thousands of engineers have used our platform to prepare for interviews.<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></p>
<h3>Interviewers</h3>
<p>Interviewers came from our pool of professional interviewers. They were broken into three groups, with each group asking a different type of question.  <strong>The interviewers had no idea that the experiment was about ChatGPT or cheating; we told them that "[this] research study aims to understand the trends in the predictability of an interviewer’s decisions over time – especially when asking standard vs. non-standard interview questions."</strong></p>
<p>These were the three question types:</p>
<ol>
<li><strong>Verbatim LeetCode questions</strong>: questions pulled directly from LeetCode at the interviewer's discretion with no modifications to the question.</li>
</ol>
<p>Example: The <a href="https://leetcode.com/problems/sort-colors/">Sort Colors</a> LeetCode question is asked exactly as it is written.</p>
<ol start="2">
<li><strong>Modified LeetCode questions</strong>: questions pulled from LeetCode and then modified to be similar to the original but still notably different from it.</li>
</ol>
<p>Example: The <a href="https://leetcode.com/problems/sort-colors/">Sort Colors</a> question above but modified to have four integers (0,1,2,3) instead of just three integers (0,1,2) in the input.</p>
<ol start="3">
<li><strong>Custom questions</strong>: questions that aren’t directly tied to any question that exists online.</li>
</ol>
<p>Example: You are given a log file with the following format:
- <code>&lt;username&gt;: &lt;text&gt; - &lt;contribution score&gt;</code>
- Your task is to identify the user who represents the median level of engagement in a conversation. Only consider users with a contribution score greater than 50%. Assume that the number of such users is odd, and you need to find the one right in the middle when sorted by their contribution scores. Given the file below, the correct answer is SyntaxSorcerer.</p>
<div><pre><code>LOG FILE START
<span>NullPointerNinja:</span> <span>"who's going to the event tomorrow night?"</span> - <span>100%</span>
<span>LambdaLancer:</span> <span>"wat?"</span> - <span>5%</span>
<span>NullPointerNinja:</span> <span>"the event which is on 123 avenue!"</span> - <span>100%</span>
<span>SyntaxSorcerer:</span> <span>"I'm coming! I'll bring chips!"</span> - <span>80%</span>
<span>SyntaxSorcerer:</span> <span>"and something to drink!"</span> - <span>80%</span>
<span>LambdaLancer:</span> <span>"I can't make it"</span> - <span>25%</span>
<span>LambdaLancer:</span> <span>"🙁"</span> - <span>25%</span>
<span>LambdaLancer:</span> <span>"I really wanted to come too!"</span> - <span>25%</span>
<span>BitwiseBard:</span> <span>"I'll be there!"</span> - <span>25%</span>
<span>CodeMystic:</span> <span>"me too and I'll brink some dip"</span> - <span>75%</span>
LOG FILE <span>END</span>
</code></pre></div>
<p>For more information about question types and about how we designed this experiment, please read the <a href="https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit">Interviewer Experiment Guidelines</a> doc that we shared with participating interviewers.</p>
<h3>Interviewees</h3>
<p>Interviewees came from our pool of active users and were invited to participate in a short survey. We selected interviewees who:</p>
<ul>
<li>Were actively looking for a job in today's market</li>
<li>Had 4+ years of experience and were applying to senior-level positions</li>
<li>Rated their “ChatGPT while coding” familiarity as moderate to high</li>
<li>Identified themselves as someone who thought they could cheat in an interview without getting caught</li>
</ul>
<p>This selection helped us skew the candidates toward people who could feasibly cheat in an interview, had the motivation to do so, and were already reasonably familiar with ChatGPT and coding interviews.</p>
<p><strong>We told interviewees that they had to use ChatGPT in the interview, and the goal was to test their ability to cheat with ChatGPT.</strong> They were also told not to try to pass the interview with their own skills — the point was to rely on ChatGPT.</p>
<p>We ended up conducting 37 interviews overall, 32 of which we were able to use (we had to remove 5 because participants didn’t follow directions):</p>
<ul>
<li>11 with the “verbatim” treatment</li>
<li>9 with the “modified” treatment</li>
<li>12 with the “custom” treatment</li>
</ul>
<p>A quick disclaimer. Because our platform allows for anonymity, our interviews have audio but no video. We’re anonymous because we want to create a safe space for our users to fail and learn quickly without judgment. It’s great for our users, but we acknowledge that not having video in these interviews makes our experiment less realistic. In a real interview, you will be on camera with a job on the line, which makes cheating harder — but does not eliminate it (watch the TikTok above if you disagree!).</p>
<p>After the interviews, both interviewers and interviewees had to complete an exit survey. We asked interviewees about the difficulties of using ChatGPT during the interview, and interviewers were given multiple chances to express concerns about the interview — we wanted to see how many interviewers would flag their interviews as problematic and report that they suspected cheating.</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewee_Collage_Survey_88a5b0a095.jpg" alt="Post-survey interviewee questions" width="600" height="500"></p><p>Post-survey interviewee questions</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewer_Collage_Survey_a0e0cf80fb.jpg" alt="Post-survey interviewer questions" width="600" height="500"></p><p>Post-survey interviewer questions</p>
<p>We had no idea what would happen in this experiment, but we assumed that if half the candidates that cheated got away with it and passed the interview, it would be a telling result for our industry.</p>
<h2>Results</h2>
<p>After removing interviews where participants did not follow instructions<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>, we got the following results. Our control was how candidates performed in interviewing.io mock interviews outside the study: 53%.<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> Note that most mock interviews on our platform are LeetCode-style questions, which makes sense because that's primarily what FAANG companies ask. We'll come back to this in a moment.</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Pass_rate_by_question_type_when_trying_to_cheat_with_Chat_GPT_c20c87712e.png" alt="Pass Rate by Question Type with the Control group at 53%, the Verbatim group at 73%, the Modified group at 67%, and the Custom group at 25%" width="650"></p><p>"Verbatim" questions passed significantly more often, compared to both our platform average and to "custom" questions. "Verbatim" and "modified" questions were not statistically significantly differnt from each other. "Custom" questions had a significantly lower pass rate than any of the other groups.</p>
<h3>“Verbatim” questions</h3>
<p>Predictably, the verbatim group performed the best, passing 73% of their interviews. Interviewees reported that they got the perfect solution from ChatGPT.</p>
<p>The most notable comment from the post-interview survey for this group is below — we think it is particularly telling of what was going on in many of the interviewers’ minds:</p>
<blockquote>
<p><em>“It's tough to determine if the candidate breezed through the question because they're actually good or if they've heard this question before. Normally, I add 1-2 unique twists to the problem to ascertain the difference.”</em></p>
</blockquote>
<p>Normally, this interviewer would have followed up with a modified question to get more signal, so let’s examine the “modified” group next to see if interviewers actually got more signal by adding a twist to their questions.</p>
<h3>“Modified” questions</h3>
<p>Remember, this group may have had a LeetCode question given to them, which was standard but modified in a way that was not directly available online. This means ChatGPT couldn’t have had a direct answer to this question. Hence, the interviewees were much more dependent on ChatGPT's actual problem-solving abilities than its ability to regurgitate LeetCode tutorials.</p>
<p>As predicted, the results for this group weren’t too different from the “verbatim” group, with 67% of candidates passing their interviews. As it turns out, this difference was not statistically significantly different from the "verbatim" group, i.e., “modified” and “verbatim” are essentially the same. This result suggests that ChatGPT can handle minor modifications to questions without much trouble. Interviewees did notice, however, that it took more prompting to get ChatGPT to solve the modified questions. As one of our interviewees said:</p>
<blockquote>
<p><em>“Questions that are lifted directly from LeetCode were no problem at all. A follow-up question that was not so much directly LeetCode-style was much harder to get ChatGPT to answer.”</em></p>
</blockquote>
<h3>“Custom” questions</h3>
<p>As expected, the “custom” question group had the lowest pass rate, with only 25% of candidates passing. <strong>Not only is it statistically significantly smaller than the other two treatment groups, it's significantly lower than the control! When you ask candidates fully custom questions, they perform worse than they do when they're not cheating (and getting asked LeetCode-style questions)!</strong></p>
<p>Note that this number, when initially calculated, was marginally higher, but after reviewing the custom questions in detail, we discovered a problem with this question type we hadn’t anticipated, which had skewed the results minorly toward a higher pass rate. Read the section below called "Companies: Change the questions you are asking immediately!" to find out what that problem was.</p>
<h2>No one was caught cheating!</h2>
<p>In our experiment, interviewers were not aware that the interviewees were being asked to cheat. As you recall, after each interview, we had interviewers complete a survey in which they had to describe how confident they were in their assessments of candidates.</p>
<p><strong>Interviewer confidence in the correctness of their assessments was high, with 72% saying they were confident in their hiring decision.</strong> One interviewer felt so strongly about an interviewee's performance that they concluded we should invite them to be an interviewer on the platform!</p>
<blockquote>
<p><em>“The candidate performed very well and demonstrated knowledge of a strong Amazon L6 (Google L5) SWE... and could also be considered to be an interviewer/mentor on interviewing.io.”</em></p>
</blockquote>
<p>That is a lot of confidence after just one interview — probably too much!</p>
<p>We’ve long known that <a href="https://interviewing.io/blog/own-interview-performance">engineers are bad at gauging their own performance</a>, so perhaps it shouldn’t come as a shock to find that interviewers also overestimate the effectiveness of the questions that they ask.</p>
<p>Of the interviewers who were not confident in their hiring choice (28%), we asked them why. This was the frequency distribution of their reasons.</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewer_confidence_61b40b9ce1.png" alt="Interviewers weren't confident in their assessment because the candidate gave suboptimal solutions, missed edge cases, had messy code, was inarticulate, or occasionally had other niche issues, but no interviewer was called out cheating as a concern" width="650"></p><p>Note that cheating isn’t mentioned anywhere!</p>
<p>Most interviewers concerned about their hiring decision expressed specific reasons for their lack of confidence. These issues typically included suboptimal solutions, missed edge cases, messy code, or poor communication. We specifically included an “Other Issue” category to see if they would express a concern that the interviewee was cheating, but digging deeper revealed only minor grievances like “personality issues” and “they need to speed up their coding.”</p>
<p>In addition to having this opportunity to call out cheating, interviewers were prompted three additional times to note any other concerns they had with the interview, including free-form text boxes and several multiple-choice questions with options to explain their concerns.</p>
<p>When an interviewee bombed because they didn’t understand the ChatGPT response, the interviewer chalked up the interviewee’s odd behavior and stilted responses to a lack of practice — not cheating. One interviewer thought the candidate's problem-solving was fine but commented that they were slow and needed to consider edge cases more carefully.</p>
<blockquote>
<p><em>“Candidate did not seem prepared for any LeetCode questions."</em></p>
</blockquote>
<blockquote>
<p><em>“Candidate's approach lacked clarity, and they jumped into the coding too early.”</em></p>
</blockquote>
<blockquote>
<p><em>“The candidate was not prepared to tackle even the most basic coding questions on LeetCode.”</em></p>
</blockquote>
<blockquote>
<p><em>“Good problem solving in general, but the candidate needs to be faster at coding and identifying critical edge cases.”</em></p>
</blockquote>
<p>So, who reported concerns about cheating? And who got caught?</p>
<p><strong>As it turns out, not a single interviewer mentioned concerns about any of the candidates cheating!</strong></p>
<p>We were stunned to discover that interviewers reported no suspicions of cheating, and interestingly, interviewees were largely confident that they were getting away with it, too. 81% reported no concerns about being caught, 13% thought they might have tipped off the interviewer, and an astonishingly small 6% of participants thought the interviewer suspected them of cheating.</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewee_worry_level_6d1445e631.png" alt="81% of interviewees were not worried, 13% were slightly worried, and just 6% were very worried about being caught" width="650"></p><p>Interviewees were mostly sure that their cheating went undetected</p>
<p>The candidates who worried they were caught did have abnormal comments from the interviewers in the post-survey analysis, but they still were not suspected of cheating. <strong>To summarize, most candidates thought they were getting away with cheating — and they were right!</strong></p>
<h2>Companies: Change the questions you are asking immediately!</h2>
<p>The obvious conclusion from these results is that <strong>companies need to start asking custom questions immediately, or they are at serious risk of candidates cheating during interviews (and ultimately not getting useful signal from their interviews)!</strong></p>
<p>ChatGPT has made verbatim questions obsolete; anyone relying on them will be naively leaving their hiring processes up to chance. Hiring is <a href="https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring">already tricky enough</a> without worrying about cheating. If you’re part of a company that uses verbatim LeetCode questions, please share this post internally!</p>
<p>Using custom questions isn’t just a good way to prevent cheating. It filters out candidates who have memorized a bunch of LeetCode solutions (as you saw, our custom question pass rate was significantly lower than our control). It also meaningfully improves candidate experience, which makes people way more likely to want to work for you. A while ago, we did an <a href="https://interviewing.io/blog/best-technical-interviews-common">analysis of what makes good interviewers good</a>. Not surprisingly, asking good questions was one of the hallmarks, and our best-rated interviewers were the ones who tended to ask custom questions! Question quality was extremely significant in our study, regarding whether the candidate wanted to move forward with the company. It was much more important than the company’s brand strength, which mattered for getting candidates in the door but didn’t matter relative to question quality once they were in process.</p>
<p>As some of our interviewees said…</p>
<blockquote>
<p><em>“Always nice to get questions that are more than just plain algorithms.”</em></p>
</blockquote>
<blockquote>
<p><em>“I liked the question — it takes a relatively simple algorithms problem (build and traverse a tree) and adds some depth. I also liked that the interviewer connected the problem to a real product at [Redacted], which made it feel less like a toy problem and more like a pared-down version of a real problem.”</em></p>
</blockquote>
<blockquote>
<p><em>“This is my favorite question that I’ve encountered on this site. It was one of the only ones that seemed to have real-life applicability and was drawn from a real (or potentially real) business challenge. And it also nicely wove in challenges like complexity, efficiency, and blocking.”</em></p>
</blockquote>
<p>One more somewhat subtle piece of advice for companies who decide to move to more custom questions. You might be tempted to take verbatim LeetCode questions and change up the wording or some of the window dressing. That makes sense, because it’s certainly easier than coming up with questions from scratch. Unfortunately, that doesn’t work.</p>
<p>As we mentioned earlier, we discovered in this experiment that just because a question looks like a custom question, doesn’t mean it is one. Questions can appear custom and still be identical to an existing LeetCode question. <strong>When making questions to ask candidates, it isn’t enough to obscure an existing problem.</strong> You need to ensure that the problem has unique inputs and outputs to be effective at stopping ChatGPT from recognizing it!</p>
<p>The questions that interviewers ask are confidential, and we cannot share the exact questions that our interviewers used in the experiment. However, we can give you an indicative example.  Below is a “custom question” with this critical flaw, which is easy for ChatGPT to beat:</p>
<div><pre><code><span>For</span> her birthday, Mia received a mysterious box containing numbered cards 
<span>and</span> a note saying, <span>"Combine two cards that add up to 18 to unlock your gift!"</span> 
Help Mia find the right pair <span>of</span> cards <span>to</span> reveal her surprise.

<span>Input:</span> An array <span>of</span> integers (the numbers <span>on</span> the cards), <span>and</span> the target sum (<span>18</span>). 
arr = [<span>1</span>, <span>3</span>, <span>5</span>, <span>10</span>, <span>8</span>], target = <span>18</span>

<span>Output:</span> The indices <span>of</span> the two cards that add up <span>to</span> the target sum. 
<span>In</span> this <span>case</span>, [<span>3</span>, <span>4</span>] because index <span>3</span> <span>and</span> <span>4</span> add <span>to</span> <span>18</span> (<span>10</span>+<span>8</span>).
</code></pre></div>
<p>Did you spot the issue? While this question appears “custom” at first glance, its objective is identical to the popular <a href="https://leetcode.com/problems/two-sum/">TwoSum</a> question: finding two numbers that sum to a given target. The inputs and outputs are identical; the only thing “custom” about the question is the story added to the problem.</p>
<p>Seeing that this is identical to known problems, it shouldn’t be a surprise to learn that ChatGPT does well on questions that have inputs and outputs identical to existing known problems — even when they have a unique story added to them.</p>
<h3>How to actually create good custom questions</h3>
<p>One thing we’ve found incredibly useful for coming up with good, original questions is to start a shared doc with your team where every time someone solves a problem they think is interesting, no matter how small, they jot down a quick note. These notes don’t have to be fleshed out at all, but they can be the seeds for unique interview questions that give candidates insight into the day-to-day at your company. Turning these disjointed seeds into interview questions takes thought and effort — you have to prune a lot of the details and distill the essence of the problem into something that doesn’t take the candidate a lot of work/setup to grok. You’ll also likely have to iterate on these home-grown questions a few times before you get them right — but the payoff can be huge.</p>
<p>To be clear, we’re not advocating the removal of data structures and algorithms from technical interviews. DS&amp;A questions have gotten a bad reputation because of bad, unengaged interviewers and because of companies lazily rehashing LeetCode problems, many of them bad, which have nothing to do with their work. In the hands of good interviewers, those questions are powerful and useful. If you use the approach above, you’ll be able to come up with new data structure &amp; algorithmic questions that have a practical foundation and component that will engage candidates and get them excited about the work you’re doing.</p>
<p>You’ll also be moving our industry forward. It’s not OK that memorizing a bunch of LeetCode questions gives candidates an edge in today’s interview process, nor is it OK that interviews have gotten to a state where cheating starts to look like a rational choice. The solution is more work on the employer’s part to come up with better questions. Let’s all do it together.</p>
<h2>Real talk for job seekers</h2>
<p>All right, now, for all of you who are actively looking for work, listen up! Yes, a subset of your peers will now be using ChatGPT to cheat in interviews, and at companies that ask LeetCode questions (sadly, many of them), those peers will have an edge… for a short while.</p>
<p><strong>Right now, we’re in a liminal state where companies’ processes have not caught up to reality. They will, soon enough, either by moving away from using verbatim LeetCode questions entirely (which will be a boon for our entire industry) or by returning to in-person onsites (which will make cheating largely impossible past the technical screen) or both.</strong></p>
<p>It sucks that other candidates cheating is another thing to worry about in an <a href="https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews">already difficult climate</a>, but we cannot, in good conscience, endorse cheating to “level the playing field.”</p>
<p>In addition, interviewees who used ChatGPT uniformly reported how much more difficult the interview was to complete while juggling the AI.</p>
<p>Below, you can view one interviewee stumbling through their time complexity analysis after giving a perfect answer to an interview question. The interviewer is confused as the interviewee scrambles to explain how they got to their incorrect time complexity (secretly provided by ChatGPT).</p>
<p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" height="360px" src="https://www.youtube.com/embed/jtcCK0yr9Bg" title="Interviewee stumbles after ChatGPT feeds them a wrong answer" width="640px"></iframe></p>
<p>While no one was caught during the study, their cameras were off, and cheating was still difficult for many of our skilled candidates, as evidenced by this clip.</p>
<p>Ethics aside, cheating is difficult, stressful, and not entirely straightforward to implement. Instead, we advise investing that effort into practice, which will serve you well once companies change their processes, which hopefully should be soon. Ultimately, we hope the advent of ChatGPT will be the catalyst that finally moves our industry’s interview standards away from grinding and memorizing to actually testing engineering ability.</p>
<div><p><img alt="Author avatar" title="Michael Mroczka" loading="lazy" width="150" height="150" decoding="async" data-nimg="1" srcset="https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FMike_Headshot_actual_size_2990c50d7a.png&amp;w=256&amp;q=75 1x, https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FMike_Headshot_actual_size_2990c50d7a.png&amp;w=384&amp;q=75 2x" src="https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FMike_Headshot_actual_size_2990c50d7a.png&amp;w=384&amp;q=75"></p><div><p><span>Michael Mroczka</span></p><p>Michael Mroczka, an ex-Google SWE, is one of the highest-rated mentors at interviewing.io and primarily works on the Dedicated Coaching program. He has a decade of coaching experience, having personally helped 100+ engineers get into Google and other desirable tech companies. After receiving multiple offers from tech companies early in his career, he enjoys teaching others proven techniques to pass technical interviews.</p>
<p>He also sometimes writes technical content for interviewing.io (like this piece) and was one of the authors of interviewing.io’s <a href="https://interviewing.io/guides/system-design-interview">A Senior Engineer's Guide to the System Design Interview</a>.</p></div></div>
<p><em>Special thanks to Dwight Gunning and Liz Graves for their help with this experiment. And of course a big thank you to all the interviewees and interviewers who participated!</em></p>
<h2>Footnotes:</h2>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>To be an interviewer on our platform, you have to have at least 4 years of experience and have conducted at least 20 interviews on behalf of a FAANG or FAANG-adjacent company). <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Five interviews needed to be removed because they did not meaningfully use ChatGPT. In two instances, the interviewee was familiar with the question and chose to solve the problem themselves. In one interview, the interviewee wanted to just try the question on their own and didn't prompt ChatGPT, ignoring our instructions. The last two interviews were "custom" interview questions that were problematic for reasons we’ll outline later in this article. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>This is a higher passthrough rate than you'd see in the wild. We think it comes down to two factors: selection bias and pre-interview prep. The users who invest in interview prep are a specific, self-selected slice of the population. Moreover, many of our users practice on their own before practicing with a human. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sega AI (278 pts)]]></title>
            <link>https://www.smspower.org/SegaAI/Index</link>
            <guid>39206529</guid>
            <pubDate>Wed, 31 Jan 2024 17:22:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smspower.org/SegaAI/Index">https://www.smspower.org/SegaAI/Index</a>, See on <a href="https://news.ycombinator.com/item?id=39206529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wikitext">
<p><span>Sega AI Computer (セガＡＩコンピューター)</span>
</p>
<div id="tocid1">
<ul><li><a href="#Introduction">Introduction</a>
</li><li><a href="#SystemSpecs">System Specs</a>
<ul><li><a href="#TouchSurface">Touch surface</a>
</li><li><a href="#AudioSpecs">Audio Specs</a>
</li><li><a href="#SoftwareStorage">Software Storage</a>
</li></ul></li><li><a href="#ArtificialIntelligence">Artificial Intelligence?</a>
</li><li><a href="#WeEventuallyFoundOne">We eventually found one...</a>
</li><li><a href="#AnUsuallyLong19861989LifeCycle">An usually long 1986-1989 life cycle?</a>
</li><li><a href="#WhoWorkedOnTheSegaAIComputer">Who worked on the Sega AI Computer?</a>
</li><li><a href="#Emulation">Emulation</a>
<ul><li><a href="#InstructionsToRunMAME">Instructions to run MAME</a>
</li><li><a href="#Controls">Controls</a>
</li><li><a href="#KnownEmulationBugsAsOfJanuary2024">Known emulation bugs as of January 2024</a>
</li></ul></li><li><a href="#SoftwareTitles">Software Titles</a>
<ul><li><a href="#WhatWeFoundSoFar">What we found so far</a>
</li><li><a href="#WhatWeHaventFoundSoFar">What we haven't found so far</a>
</li></ul></li><li><a href="#Downloads">Downloads</a>
<ul><li><a href="#SystemRoms">System Roms</a>
</li><li><a href="#SoftwareRomsAmpOverlay">Software Roms &amp; Overlay</a>
</li><li><a href="#MAMEReadyRomsFolder">MAME ready Roms folder</a>
</li><li><a href="#TapesRecordings">Tapes recordings</a>
</li><li><a href="#HardwareManuals">Hardware Manuals</a>
</li><li><a href="#SoftwareScans">Software Scans</a>
</li><li><a href="#FlyersAmpAds">Flyers &amp; Ads</a>
</li><li><a href="#Photos">Photos</a>
</li><li><a href="#MagazineScans">Magazine Scans</a>
</li><li><a href="#TechnicalDocumentations">Technical documentations</a>
</li></ul></li><li><a href="#Links">Links</a>
<ul><li><a href="#OurBigSharedFolder">Our big shared folder</a>
</li><li><a href="#ExternalResources">External resources</a>
</li><li><a href="#SiteSections">Site sections</a>
</li><li><a href="#ForumThreads">Forum threads</a>
</li></ul></li><li><a href="#ResearchCredits">Research credits</a>
<ul><li><a href="#Index">Credits</a>
</li><li><a href="#SpecialThanks">Special Thanks</a>
</li><li><a href="#AboutSMSPower">About SMS Power!</a>
</li></ul></li></ul></div>
<h2><a name="Introduction" id="Introduction"></a> Introduction</h2>
<p>Around late 1986, Sega released the “Sega AI Computer”. <strong>This is one of Sega’s least well known and rarest systems.</strong> Not much is known about this system apart from a small amount of information in Japanese and American flyers and press articles. The information we have is still piecemeal and may be partly inaccurate.
</p>
<p><strong>Today we are making public, for the first time: all system roms extracted from the Sega AI Computer, data dumps from 26 my-cards and 14 tapes, many scans and photographs, and in collaboration with <a href="https://www.mamedev.org/" rel="nofollow">MAME developers</a>, an early working MAME driver allowing this computer to be emulated.</strong>
</p>
<p>The majority of these software titles had zero information about them on the internet prior to us publishing them: no screenshots, no photos or scans of actual software. Considering the elusive nature of this machine, it is possible that some games have never been seen or completed by anyone outside of their original development teams.
</p>
<p>We hope that this release will be interesting to obscure game and computer historians and hobbyists alike. We will further amend it over time by releasing extra scans, hopefully improving emulation and publishing/discovering new information.
</p>
<p><img src="https://www.smspower.org/uploads/SegaAI/SegaAI-Cover2.jpg" alt="" title=""><br>Extract from a US ad (1986)</p>
<hr>
<h2><a name="SystemSpecs" id="SystemSpecs"></a> System Specs</h2>
<ul><li>Year: 1986
</li><li>CPU: 16-bit NEC V20 at 5 Mhz
</li><li>ROM: 512 KB total (as actually found in system)
<ul><li>2x64 KB system ROM (“Operating System, Prolog”)
</li><li>1x128 KB character ROM
</li><li>2x128 KB speech ROM
</li></ul></li><li>RAM: 128 KB
</li><li>Software on Sega My Cards (128 KB to 256 KB).
</li><li>Software on Audio Cassettes.
</li><li>Video : Yamaha V9938 (Resolution 256x212) with 64 KB VRAM.
</li><li>Sound: SN76489 PSG
</li><li>Inputs: Touch surface with overlays, 8-directions pad + 3 buttons.
</li><li>Inputs: Microphone input.
</li><li>Inputs/Outputs: a RS232 Centronics port
</li><li>Cassette Drive : 9600bps.
</li><li>Optional Keyboard peripheral.
</li><li>Optional Sound Box peripheral with a YM2151 FM chip.
</li></ul><p>An early flyer suggest the possible planning or existence of: a BASIC programming ROM Card, a Disk Drive extension, a Bridge Unit featuring an 8-bit CPU, joystick peripherals and a microphone input.
</p>
<p><img src="https://www.smspower.org/uploads/SegaAI/SegaAI-Peripherals.jpg" alt="" title=""><br>Extract from a 1986 Japanese pamphlet</p>
<p>Quick comparisons with other pieces of hardware:
</p><ul><li>The Sega SC-3000 (1983) used a Z80 8-bit CPU, 2 KB of RAM, 16 KB of RAM and no system ROM.
</li><li>The Sega Master System (1985+) increased this to 8 KB of RAM and included a better video chip.
</li><li>The video portion of Sega AI Computer satisfies MSX2 specs.
</li></ul><h3><a name="TouchSurface" id="TouchSurface"></a> Touch surface</h3>
<p>The system has a large touch surface. Most software titles are shipped with an overlay, providing a custom touch interface for each title. Some titles use the touch surface without an overlay to provide drawing capabilities.
</p>
<h3><a name="AudioSpecs" id="AudioSpecs"></a> Audio Specs</h3>
<p>The system is capable of playing <strong>PSG audio</strong>, and <strong>FM audio</strong> when equipped with the Sound Box extension. It is also equipped with a <strong>“speech synthesiser”</strong> in the form of a chip capable of decoding ADPCM data. One of the 128 KB ROM contains speech samples for the 46 common sounds of the Japanese language, the other 128 KB ROM contains entire sentences which are used by the system. In addition, the system is also able to seek and play audio from the cassette drive.
</p>
<p>So up to 4 types of audio can be mixed together: <em>“I tried out a sampling of the software with the Sound Box, and many of them produced FM music instead of simple PSG without the box. All FM is mixed in with everything else, including the PSG, which is still used for rhythm hits; the speech chip, which is used in "Popo's Adventure" ("take a big hamburger" it says, eg.); and the sound from the cassette, which is mostly narration and hints.”</em>.
</p>

<h3><a name="SoftwareStorage" id="SoftwareStorage"></a> Software Storage</h3>
<p>The system can boot software from small-sized cards similar to Master System and PC Engine cards. Unlike Master System-era cards, which could carry a maximum of 32 KB of data, the Sega AI Computer cards can carry 128 KB and 256 KB of data.
</p>
<p>The 128 KB cards have their whole memory space mapped through card pinouts (17 pins). The 256 KB cards have a simple Master System-style bank-switching mapper embedded.
</p>
<p>The system can also boot software from its cassette drive. Sega AI Computer cassettes are stereo, with one channel dedicated to storing encoded data, and the other to storing regular audio. This allows certain loading operations to be playing background music during the load, including snippets from Vivaldi’s Four Seasons.
</p>
<h2><a name="ArtificialIntelligence" id="ArtificialIntelligence"></a> Artificial Intelligence? </h2>
<p><span><img src="https://www.smspower.org/uploads/SegaAI/CloseUp-01.jpg" alt="" title=""></span></p>
<p>Documents describe it as a full-featured computer with an educational twist. The system itself sports markings with a promising <em>“SEGA PROLOG…. Bringing you into the world of artificial intelligence”</em>.
</p>
<p>Effectively all the software we found so far is educational and mostly aimed at kids. The system hardware was definitely not used to its maximum potential, although later 1988-1989 software titles are of higher quality than earlier ones. A US prototype suggests that some form of LISP exists, but we weren’t able to get access to it yet.
</p>
<p>The Prolog interpreter appears to be used by existing software to allow some form of natural language processing. It doesn't seem accessible to the end-user.
</p>

<p>One of the most important English sources is a July 24, 1986 article from “Electronics” magazine, which we are reproducing here in its entirety:
</p>
<div><p>“CHILDREN'S COMPUTER BRINGS AI INTO THE HOME
</p></div><div><p>TOKYO Recent advances in logic-programming languages and processor and peripheral chips are making artificial-intelligence applications practical in the low-end home computer market. The first product that will seek to prove this point is the AI Computer, which Sega Enterprises Ltd. of Tokyo will start selling in Japan next month for $547.
</p></div><div><p>The Sega AI Computer is built to run programs written in the Prolog AI language. It is strongly oriented toward computer-aided-instruction, especially for children, rather than the catchall hobbyist-to-professional target of other home computers. Perhaps its simplest AI application is a personal diary program that can be used two ways: in a simple word-processing mode for children with some writing facility and in a prompt mode.
</p></div><div><p>In the prompt mode, the child is asked about his or her activities during the day and replies with one- and two-word answers. The computer program then writes a grammatically correct diary entry based on those replies. In more advanced CAI applications, the computer is more flexible than previous systems. It can parse a user's natural-language inputs and evaluate the person's ability level. It can then proceed to material of appropriate difficulty, rather than simply advancing one level at a time.
</p></div><div><p>The success of the AI Computer will depend almost totally on the software available for it, and Sega has already forged ties with the educational community to develop courseware for 3- to 8- year-olds. Its partner in this endeavor is general trader Marubeni Corp.'s Visual Information Section, Corporate Development Department, Tokyo, which is working with various educational organizations to create the programs.
</p></div><div><p>Sega plans to work also with Linguaphone Institute (Japan) Ltd., Tokyo, to develop English-language instruction programs. Courseware in English should be ready next year, and the Sega AI machine then will be introduced in the U. S.
</p></div><div><p>Rather than employing the Basic-language interpreters of most personal computers, Sega's AI machine uses a run-time Prolog-language interpreter residing in 128-K bytes of read-only memory. The Prolog interpreter is for running applications only — it cannot be used for programming. The company chose the Prolog AI language because of its ability to handle unformatted input and to parse natural-language input. Prolog is not especially suitable for driving displays and controlling peripherals, so Prolog functions call up fast, efficient assembly-language subroutines for these tasks.
</p></div><div><p>ENHANCED MATH. Sega Prolog was developed jointly with CSK Research Institute, the AI lab of software house Computer Services Corp., Tokyo. In addition to the assembly-language calls, the language's mathematical performance was enhanced for the AI Computer.
</p></div><div><p>The computer's hardware is designed for high performance at low cost. It is built under contract by Nippon Gakki Co., Shizuoka, a leading maker of MSX- compatible computers and electronic musical instruments. The microprocessor is NEC Corp.'s V-20, an enhanced version of the Intel 8088 implemented in CMOS. The Prolog interpreter is stored in 128-K bytes of read-only memory, and main memory is 128K bytes of dynamic random-access memory. Video memory is 64-K bytes of DRAM, expandable to 128-K bytes. A Centronics compatible printer interface is standard. The system's power supply has sufficient capacity to run an optional 3 1/2-in. floppy-disk drive. The machine also comes with a tape recorder for digital input or natural-language audio output, and an eight-direction cursor controller.
</p></div><div><p>The Sega Computer has a tablet on its sloping upper surface that takes overlays for various applications. One that will be used in many children's programs is a Japanese-language touchpad, which includes all phonetic-syllabary characters along with variations annotated with one of two standard marks. The technique was designed to meet the needs of young users, who might not be adept at using a traditional Japanese keyboard where the operator strikes character and mark-only keys in succession to obtain the desired syllable. For older children and adults, the computer also provides the new Japanese industrial-standard keyboard,, which fits on top of the tablet to save desk space. Applications programs are usually supplied as plug-in ROM cards with a capacity off up to 128-K bytes, although the built-in cassette tape recorder and optional 3 1/2-in floppy disk drive can also be used.
</p></div><div><p>ROM versions of Basic and Logo are available for users who want to write programs in those languages. A I28-K RAM expansion kit and a I28-K RAM card are available for disk-based applications or languages that require extra memory.
</p></div><p>-Charles L Cohen
</p><p>Electronics/ July 24, 1986“
</p><h2><a name="WeEventuallyFoundOne" id="WeEventuallyFoundOne"></a> We eventually found one...</h2>
<p>In September 2014, more than a decade after the last public sighting, a user of Yahoo! Auctions in Japan put a Sega AI Computer unit for sale along with 15 software titles, all listed separately. With help from a few generous individuals and associations, we decided to buy everything and work on it. Since that time, several other units have appeared on Yahoo Auctions, but rarely ever came with software again. We however slowly managed to gather more software from other sources. At the end of 2022, another meaningful set of auctions including a boxed keyboard and 10 earlier iterations of known software were sold, which we also managed to acquire a year later from the initial buyer.
</p>
<p>The hardware and software acquired in the first batch were stamped with mention of the Aomori Minami Hoikuen (青森南保育園) in Tokyo. The Sega AI Computer appears to have been mostly sold to Japanese schools. It is unknown how many were manufactured, how many were sold, and if Sega had further plans to market the machine to a wider audience. The December 1986 and January 1987 issues of “Copel 21”, a Japanese science magazine for kids, included an order form to purchase the Sega AI Computer for 87500 yens, and the Kumon Wonderschool set in 17 installments of 9990 yens.
</p>

<h2><a name="AnUsuallyLong19861989LifeCycle" id="AnUsuallyLong19861989LifeCycle"></a> An usually long 1986-1989 life cycle?</h2>
<p>Many software titles have a date printed in their manual. They range from August 1st, 1986 to May 1st, 1989. There are two versions of the My Card for “AI Enikki”: one dated 1986 and one dated 1989. At least 10 titles were released in 1986 on cassettes, then re-released in 1987-1988 on cards. In both instances, the later software versions have been updated and are more advanced.
</p>
<p>The presumed software release timespan (at least 33 months between August 1986 and May 1989) is particularly odd considering this is an extremely rare and unknown system. Why would Sega update and re-release software? Did they do an early confidential launch, then decided to go back to the drawing board to improve software, only to do a similarly confidential relaunch? Did Sega have contracts in place with customers e.g. schools to keep on delivering software for a certain time? We're not sure!
</p>
<p>The ad and order form from COPEL 21 December 1986 issue suggest that at least it was possible to purchase the Sega AI Computer from that time.
</p>
<h2><a name="WhoWorkedOnTheSegaAIComputer" id="WhoWorkedOnTheSegaAIComputer"></a> Who worked on the Sega AI Computer?</h2>
<p><em>Caution: Information here is extremely piecemeal and may be inaccurate.</em>
</p>
<ul><li>The project was seemingly led by Sega R&amp;D Dept 6 (第6研究開発部).
</li><li>Masami Ishikawa (Sega) is among the people who worked on the Sega AI Computer project.
</li><li>Mika Okada (Sega) is among the people who worked on art for games.
</li><li>Very likely many other people were involved but we don’t have many names for now.
</li><li>Some games were developed by Marubeni Corp (according to “Electronics” July 1986 article).
</li><li>The “Pinpon Pasokon” series of 3 software are credited to a company called Kamiya.
</li><li>Sega Prolog was developed partly by the CSK Research Institute (CRI). An early prototype unit aimed at market evaluation in the USA seemingly makes many references to CRI, but we didn’t get access to this prototype.
</li><li>Sega worked with Linguaphone Institute to develop the English-language learning software.
</li><li>Hardware was built by Nippon Gakki Co., Shizuoka (according to “Electronics” July 1986 article).
</li><li>Spotted in the data following a simple search for ASCII strings:
<ul><li>One system ROM contains the following ASCII string: <code>“07/19-1986”.</code>
</li><li>The Sound Box ROM contains the following ASCII strings: <code>“Programmed by SHUN ARAI. Produced by YASUHIGE KOBAYASHI. Special Thanks: KAMIYA STUDIO &amp; PHILOS” “R&amp;D No. 6 SOUND BOARD v1.1 87/08/12 SEGA”</code>
</li></ul></li></ul><p>Black Squirrel thoughts: <em>“It is likely a CSK product with Sega branding. CSK was Sega's parent company at the time (they bought Sega in 1983) and seem to have used the AI Computer as an attempt to get into the educational market. CSK also had subsidiaries producing hardware and software - CRI (CSK Research Institute) seems to be responsible for "Sega Prolog", the closest this thing gets to an operating system. I don't know if this means CRI were secretly producing the software on cards and cassettes under the Sega name, or if there were genuine Sega people involved (I noticed a credit for Shun Arai on the sound board - he was a Sega man). Perhaps they thought kids would engage more if they saw a Sega logo in 1986 rather than a CSK/CRI one?”</em>
</p>
<p><em>“Sega's use of the term "AI" or "artificial intelligence" is a bit loose, even for 1986. It's a far cry from the likes of ChatGPT - I think the Sega AI Computer is giving the illusion of intelligence by recognising and responding in real human languages (English? Japanese? Both?) rather than 20 GOTO 10. The selling point seems to be "it's not BASIC, it's not assembly, it's Prolog!"”</em>
</p>
<p><em>“The promotional material is hinting at modems and printers and microphones and mysterious boxes - not sure how much of that came to pass. There's talk of an "8-bit CPU board" - maybe that's where MSX2 compatibility comes in? I don't know what the plan was there - perhaps it was a fallback if AI Computer software didn't take off.”</em>
</p>
<p><em>“It does have some legacy though: starting with the Mega Drive, games on Sega platforms were required to have a ROM header: <a href="https://segaretro.org/ROM_header" rel="nofollow">https://segaretro.org/ROM_header</a>. If it was a game, it was designated "GM", but educational software was designated "AI". ”</em>
</p>
<h2><a name="Emulation" id="Emulation"></a> Emulation</h2>
<p><img src="https://www.smspower.org/uploads/SegaAI/SegaAI-BootScreen.png" alt="" title=""></p>
<p>A MAME driver has been developed by Wilbert Pol and Fabio Priuli, based on hardware research by Chris Covell. Code has been merged, and 2024/01/31 release will partial emulation of the Sega AI Computer. If you want to try it today:
</p>
<ul><li><a href="https://www.mamedev.org/release.html" rel="nofollow">MAME Downloads</a>.
</li><li><a href="https://www.mamedev.org/?p=533" rel="nofollow">MAME 0.262 release note</a>.
</li><li>Download our <a href="#MAMEReadyRomsFolder">MAME ready roms folder</a> and unzip to <code>roms/segaai/</code>
</li></ul><h3><a name="InstructionsToRunMAME" id="InstructionsToRunMAME"></a> Instructions to run MAME</h3>
<ul><li>From the UI:
<ul><li>Run MAME and browse to AI system.
</li><li>Press TAB to access configuration menu.
</li></ul></li><li>From the command-line:
<ul><li>Copy system roms, games and overlay scans from xxxx.zip into roms/segai/ folder (use use "-rompath /path/to/extract/to/segaai" command-line parameter).
</li><li>Run with <code>“mame -nodebug -window segaai -card XXXX”</code> where “XXXX” stands for a game identifier in MAME’s format.
</li><li>To enable the Sound Box emulation, add the <code>“-exp soundbox”</code> flag.
</li><li>Multi-cards games may be booted from the second card using <code>"-card XXXX:card2"</code>, e.g. <code>"-card okeihana:card2"</code>.
</li></ul></li></ul><p>The game identifiers in MAME’s format are: <code>aienikki, alicewor, andersen, arabiann, cinderel, columbus, cosmictr, eigogame, eigoohan, gulliver, henshin, mozartac, oceanfan, okeihana, pinponmm, pinponmr, pinponnm, ranranme, robinson, runrunmu, tantanrh, wakuwaku</code>.
</p>
<h3><a name="Controls" id="Controls"></a> Controls</h3>
<ul><li>Because the system has both a D-Pad and an emulated touch surface, it is recommended that you reconfigure MAME inputs.
</li><li>Press TAB and go to 'Input Settings'-&gt;'Input Assignments (this system')
</li><li>You can leave P1 Up/Down/Left/Right mapped to Keyboard arrow keys.
</li><li>Change Touchpad X Analog Inc/Dec to e.g. D and A, and Touchpad Y Analog Inc/Dec to e.g. S and W.
</li><li>The controls becomes:
<ul><li>Touchpad: move cursor with WASD, press with SHIFT.
</li><li>D-Pad: Arrow keys
</li><li>Grey button: Space
</li><li>RL/PL: Ctrl, Alt
</li></ul></li></ul><h3><a name="KnownEmulationBugsAsOfJanuary2024" id="KnownEmulationBugsAsOfJanuary2024"></a> Known emulation bugs as of January 2024:</h3>
<ul><li>Speech emulation seems to occasionally refer to samples from the wrong ROM.
</li><li>Tape drive is not emulated yet.
</li><li>Keyboard is not emulated yet.
</li><li>Likely other emulation bugs and issues.
</li></ul><h2><a name="SoftwareTitles" id="SoftwareTitles"></a> Software Titles</h2>
<h3><a name="WhatWeFoundSoFar" id="WhatWeFoundSoFar"></a> What we found so far</h3>
<p>See our site listings (every game has its own page):
</p><ul><li><a href="https://www.smspower.org/Games-Releases/AI-JP" rel="nofollow">Our list of known Sega AI Computer unique releases</a>
</li><li><a href="https://www.smspower.org/Games/AI-All" rel="nofollow">Our list of known Sega AI Computer unique titles</a>.
</li><li><a href="https://www.smspower.org/Scans/AI-All" rel="nofollow">Our list of Sega AI Computer related scans</a>.
</li></ul><p>All software we found so far is mostly in the Japanese language and tends to be text-heavy, making them difficult to play for non-Japanese readers. We hope that our scans and screenshots can still give you a sense of what they are.
</p>
<p><strong>AI Enikki</strong>
</p><ul><li><a href="https://www.smspower.org/Games/AIEnikki-AI">AI Enikki</a> (ＡＩえにっき) (early card version, later card version)
</li></ul>
<p><strong>Kumon Wonderschool (two sets)</strong>
</p>
<p>Two sets exists: an earlier tape-only set, and a later card-only set.
Only the card versions are emulated so far. The tape version were released 1-2 years before and appear to be different software with different contents.
</p>

<ul><li><a href="https://www.smspower.org/Games/AliceWorld-AI">Alice World</a> (ありす・わーるど) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/RobinsonLand-AI">Robinson Land</a> (ろびんそん・らんど) (early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/CosmicTrain-AI">Cosmic Train</a> (こすみっく・とれいん) (early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/CinderellaLabyrinth-AI">Cinderella Labyrinth</a> (しんでれら・らびりんす) (early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/GulliverPocket-AI">Gulliver Pocket</a> (がりばー・ぽけっと) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/MozartAcademy-AI">Mozart Academy</a> (もーつぁると・あかでみー) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/ArabianNight-AI">Arabian Night</a> (あらびあん・ないと) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/AndersenDream-AI">Andersen Dream</a> (あんでるせん・どりーむ) (early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/OceanFantasy-AI">Ocean Fantasy</a> (おーしゃん・ふぁんたじー) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/ColumbusMap-AI">Columbus Map</a> (ころんぶす・まっぷ) early tape version, later card version)
</li></ul><table><tbody><tr><td><img src="https://www.smspower.org/uploads/Screenshots/AliceWorld-AI-Card-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/AliceWorld-AI-Card-04.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/AliceWorld-AI-Card-11.png" alt="" title=""></td></tr>
<tr><td>Alice World [Card version]</td><td>Alice World [Card version]</td><td>Alice World [Card version]</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/CosmicTrain-AI-Card-02.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/CosmicTrain-AI-Card-14.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/AndersenDream-AI-Card-TitleScreen.png" alt="" title=""></td></tr>
<tr><td>Cosmic Train [Card version]</td><td>Cosmic Train [Card version]</td><td>Andersen Dream [Card version]</td><td>&nbsp;</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/ArabianNight-AI-Card-01.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/CinderellaLabyrinth-AI-Card-02.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/ColumbusMap-AI-Card-08.png" alt="" title=""></td></tr>
<tr><td>Arabian Night [Card version]</td><td>Cinderella Labyrinth [Card version]</td><td>Columbus Map [Card version]</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/GulliverPocket-AI-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/GulliverPocket-AI-03.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/MozartAcademy-AI-Card-06.png" alt="" title=""></td><td>&nbsp;</td></tr>
<tr><td>Gulliver Pocket [Card version]</td><td>Gulliver Pocket [Card version]</td><td>Mozart Academy [Card version]</td><td>&nbsp;</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/OceanFantasy-AI-Card-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/OceanFantasy-AI-Card-08.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/RobinsonLand-AI-Card-TitleScreen.png" alt="" title=""></td></tr>
<tr><td>Ocean Fantasy [Card version]</td><td>Ocean Fantasy [Card version]</td><td>Robinson Land [Card version]]</td></tr>
</tbody></table>
<p><strong>Ongaku Wonder School</strong>
</p><ul><li><a href="https://www.smspower.org/Games/RunrunMusic-AI">Runrun Music</a> (るんるん・みゅーじっく) (card)
</li><li><a href="https://www.smspower.org/Games/TantanRhythm-AI">Tantan Rhythm</a> (たんたん・りずむ) (card)
</li><li><a href="https://www.smspower.org/Games/RanranMelody-AI">Ranran Melody</a> (らんらん・めろでぃー) (card)
</li></ul>
<p><strong>English Wonder School</strong>
</p><ul><li><a href="https://www.smspower.org/Games/FolkAndFairyTales-AI">Folks &amp; Fairy Tales - Eigo de Hanashi</a> (Folk &amp; Fairy Tales - えいごでおはなし) (two cards + two tapes)
</li><li><a href="https://www.smspower.org/Games/PoposAdventure-AI">Popo Adventure's - Eigo de Game</a> (Popo's Adventure - えいごでげーむ) (two cards + two tapes)
</li></ul><table><tbody><tr><td><img src="https://www.smspower.org/uploads/Screenshots/PoposAdventure-AI-PopoNoYume-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/PoposAdventure-AI-PopoNoYume-02.png" alt="" title=""></td><td>&nbsp;</td></tr>
<tr><td>Popo's Adventure: Popo no Yume</td><td>Popo's Adventure: Popo no Yume</td><td>&nbsp;</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/FolkAndFairyTales-AI-EigoDeOhanashi1-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/FolkAndFairyTales-AI-EigoDeOhanashi1-01.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/FolkAndFairyTales-AI-EigoDeOhanashi1-02.png" alt="" title=""></td></tr>
<tr><td>Folk &amp; Fairy Tales</td><td>Folk &amp; Fairy Tales</td><td>Folk &amp; Fairy Tales</td></tr>
</tbody></table>
<p><strong>Surasura Moji Wonder School</strong>
</p><ul><li><a href="https://www.smspower.org/Games/HenshinKanji-AI">Henshin Kanji</a> (へんしんかんじ) (card)
</li><li><a href="https://www.smspower.org/Games/OkeikoHanamaruAiueo-AI">Okeiko/Hanamaru Aiueo</a> (おけいこ・はなまる　あいえうお) (two cards)
</li><li><a href="https://www.smspower.org/Games/WakuWakuABCTo123-AI">Waku Waku ABC To 123</a> (わくわくＡＢＣと１２３) (card)
</li></ul>
<p><strong>Pinpon Pasokon</strong>
</p><ul><li><a href="https://www.smspower.org/Games/PinponNumbers-AI">Pinpon Numbers</a> (ピンポン　ナンバーズ) (card)
</li><li><a href="https://www.smspower.org/Games/PinponMusicRhythm-AI">Pinpon Music Rhythm</a> (ピンポン　ミュージック　リズム) (card)
</li><li><a href="https://www.smspower.org/Games/PinponMusicMelody-AI">Pinpon Music Melody</a> (ピンポン　ミュージック　メロディー) (card)
</li></ul>
<h3><a name="WhatWeHaventFoundSoFar" id="WhatWeHaventFoundSoFar"></a> What we haven't found so far</h3>
<p>AI Enikki: Cassette version was spotted in an auction, we couldn’t acquire it.
</p>
<p>Edison Labo: mentioned in Kumon Wonder School Japanese flyer, with screenshots.
</p>
<p>Mentions of other titles at <a href="http://park21.wakwak.com/~suka/sega/catalog/AIcom/AI.html" rel="nofollow">SukaSega's AI page</a> (exact source unknown)
</p><ul><li>Edison’s Labo (エジソンラボ)
</li><li>Lincoln Freedom (リンカーン・フリーダム)
</li><li>Grimm House (グリム・ハウス)
</li><li>Holmes Mystery (ホームズ・ミステリー)
</li><li>Anne Diary (アンネ・ダイアリー)
</li><li>Columbus Egg (コロンブス・エッグ) → Likely same as “Columbus Map”
</li><li>Takara Jima Pirates (宝島・パイレーツ)
</li><li>Space Fantasy (スペース・ファンタジー)
</li><li>Safari Fantasy (サファリ・ファンタジー)
</li><li>Mothers Book (マザーズブック)
</li></ul><p><a href="#MagazineScans">COPEL 21 December 1986 and January 1987 issues</a> contains an ad showing 18 boxes (likely mockups) with readable names:
</p><ul><li>Alice World (アリス　ワールド）→ Released
</li><li>Robinson World (ロビンソン　ワールド）→ Released as “Robinson Land”
</li><li>Cinderella World (シンデレラ ワールド）→ Released as “Cinderella Labyrinth”
</li><li>Aesop World (イソップ ワールド）
</li><li>Hansel &amp; Gretel (へんセル＆グレーテル　ワールド）
</li><li>Goku World (ゴクウ　ワールド）
</li><li>Taka Jima World (高島　ワールド）
</li><li>Helen Keller World (ヘレン ケラー ワールド）
</li><li>Nightingale World（ナイチンゲール ワールド）
</li><li>Jeanne d’Arc World (ジャンヌ・ダルク　ワールド）
</li><li>Edison World (エジソン　ワールド）→ Later referred to as “Edison’s Labo”
</li><li>Lincoln World (リンカーン ワールド）→ Later referred to as “Lincoln Freedom”
</li><li>Space World (スペース（宇宙）ワールド）
</li><li>Safari World (サファリ（？？）ワールド）
</li><li>Robot World (ロボット　ワールド）
</li><li>Mother World 1 (マザー ワールド１）
</li><li>Mother World 2 (マザー ワールド２）
</li><li>Mother World 3 (マザー ワールド３）
</li></ul><p>It is unlikely that those titles were all finished or released, but we have screenshots for “Edison’s Labo” in a flyer so this one was likely developed.
</p>
<h2><a name="Downloads" id="Downloads"></a> Downloads</h2>
<h3><a name="SystemRoms" id="SystemRoms"></a> System Roms</h3>
<p>The system dumps amount to a total of 590 KB of uncompressed data.
</p>
<ul><li><strong><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20System%20Roms.zip">Sega AI Computer - System Roms.zip</a></strong> (382 KB)
<ul><li>AI-E000.bin: system EPROM 0, marked 8/24 (64 kb)
</li><li>AI-F000.bin: system EPROM 1, marked 7/21 (64 kb), contains string: <code>7/19/86</code>
</li><li>AI-MPR-7619.bin: speech ROM 0 (128 kb)
</li><li>AI-MPR-7620.bin: speech ROM 1 (128 kb)
</li><li>AI-MPR-7689.bin: character ROM (128 kb)
</li><li>AI-SND-AI2002.bin: sound box EPROM (128 kb), contains strings: <code>Programed by SHUN ARAI.Produced by YASUHIGE KOBAYASHI.Special Thanks KAMIYA STUDIO &amp; PHILOS [...] R&amp;D No.6SOUND BOARD V1.187/08/12 SEGA</code>
</li></ul></li></ul><h3><a name="SoftwareRomsAmpOverlay" id="SoftwareRomsAmpOverlay"></a> Software Roms &amp; Overlay</h3>
<p>The 26 Sega My Card dumps amount to a total of 4.2 MB of uncompressed data.
We also scan all overlays at had at our disposal.
</p>
<ul><li><strong><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%2026%20Cards%20Roms.zip">Sega AI Computer - 26 Cards Roms.zip</a></strong> (1941 kB)
</li><li><strong><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%2026%20Overlay%20Scans.zip">Sega AI Computer - 26 Overlay Scans.zip</a></strong> (10 MB)
</li></ul><h3><a name="MAMEReadyRomsFolder" id="MAMEReadyRomsFolder"></a> MAME ready Roms folder</h3>
<p>As MAME use specific naming convention for its files, we provide a readily usable folder:
</p><ul><li><strong><a href="https://www.smspower.org/uploads/SegaAI/MAME%20ready%20roms%20folder%20for%20Sega%20AI%20Computer%20no%20tapes.zip">MAME ready roms folder for Sega AI Computer (no tapes).zip</a></strong> (10 MB)
</li></ul><h3><a name="TapesRecordings" id="TapesRecordings"></a> Tapes recordings</h3>
<p>Important: Very large data. The 14 cassettes (16 sides in total) currently amount to about ~1.2 GB of data. Please don't download unless you have use for this: the cassette system is NOT emulated a the time of writing, thus the data from our cassette dumps was not yet certified as “correct” (as in, passing the Sega AI Computer loading routines and probable checksum checks). According to early research, tapes are using a FSK modulation scheme, with a theoretical maximum bit-rate of 9600 BPS which is very high for its time. Actual bit-rate used seems at least two times lower. We are currently releasing 48 Khz, 24-bit stereo lossless FLAC. It is probable that once emulated, we can settle on a compact format for the data segments, or can tell what level of audio compression is adequate for loading in a real system and emulators.
</p>
<p>Tape downloads are available from our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Our Big Shared Folder</a>.
</p>
<ul><li>Alice World [Tape version] (Tape).flac (~5 minutes)
</li><li>Andersen Dream [Tape version] (Tape).flac (~9 minutes)
</li><li>Arabian Night [Tape version] (Tape).flac (~10 minutes)
</li><li>Cinderella Labyrinth [Tape version] (Tape).flac (~5 minutes)
</li><li>Columbus Map [Tape version] (Tape).flac (~5 minutes)
</li><li>Cosmic Train [Tape version] (Tape).flac (~9 minutes)
</li><li>Folk and Fairy Tales - Eigo de Ohanashi 1 (Tape Side A).flac (~15 minutes)
</li><li>Folk and Fairy Tales - Eigo de Ohanashi 1 (Tape Side B).flac (~15 minutes)
</li><li>Folk and Fairy Tales - Eigo de Ohanashi 2 (Tape Side A).flac (~16 minutes)
</li><li>Folk and Fairy Tales - Eigo de Ohanashi 2 (Tape Side B).flac (~16 minutes)
</li><li>Gulliver Pocket [Tape version] (Tape).flac (~5 minutes)
</li><li>Mozart Academy [Tape version] (Tape).flac (~5 minutes)
</li><li>Ocean Fantasy [Tape version] (Tape).flac (~11 minutes)
</li><li>Popo's Adventure - Eigo de Game 1 - Popo no Yume (Tape).flac (~16 minutes)
</li><li>Popo's Adventure - Eigo de Game 2 - Popoland no Himitsu (Tape).flac (~18 minutes)
</li><li>Robinson Land [Tape version] (Tape).flac (~5 minutes)
</li></ul><h3><a name="HardwareManuals" id="HardwareManuals"></a> Hardware Manuals</h3>
<ul><li><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20Main%20Unit%20Manual.pdf">Sega AI Computer - Main Unit Manual.pdf</a> (15 MB)
</li><li><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20Sound%20Box%20Manual.pdf">Sega AI Computer - Sound Box Manual.pdf</a> (9 MB)
</li><li><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20Keyboard%20Manual.pdf">Sega AI Computer - Keyboard Manual.pdf</a> (15 MB)
</li></ul>
<h3><a name="SoftwareScans" id="SoftwareScans"></a> Software Scans</h3>
<ul><li>See <a href="https://www.smspower.org/Scans/AI-All" rel="nofollow">Our list of Sega AI Computer related scans</a>.
</li></ul><h3><a name="FlyersAmpAds" id="FlyersAmpAds"></a> Flyers &amp; Ads</h3>
<ul><li><a href="https://www.smspower.org/uploads/SegaAI/Flyer%20-%20CAI-1%201986-04%20Japan.pdf">Flyer - CAI-1 (1986-04) (Japan).pdf</a> (26 MB) (courtesy of <a href="https://sudden-desu.net/" rel="nofollow">Sudden-Desu</a>)
</li><li><a href="https://www.smspower.org/uploads/SegaAI/Flyer%20-%20Sega%20AI%20Computer%20Japan.pdf">Flyer - Sega AI Computer (Japan).pdf</a> (20 MB) (courtesy of <a href="https://sudden-desu.net/" rel="nofollow">Sudden-Desu</a>)
</li><li>see our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Big Shared Folder</a> for many more.
</li></ul>

<h3><a name="Photos" id="Photos"></a> Photos</h3>
<p>See our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Big Shared Folder</a> for:
</p><ul><li>about 150 photos from all past known auctions/sales of Sega AI Computer contents.
</li><li>close up photos of Sega AI Main Unit
</li><li>close up photos of Sega AI Main Unit board
</li><li>close up photos of Sega AI Keyboard
</li><li>close up photos of Sega AI Sound Box board
</li><li>photos of software packaging
</li><li>archived webpages about an unreleased US prototype
</li><li>and more...
</li></ul>



<h3><a name="MagazineScans" id="MagazineScans"></a> Magazine Scans</h3>
<ul><li><a href="https://www.smspower.org/uploads/SegaAI/COPEL%2021%201986-12%20Article%20amp%20Ad%20Japan.pdf">COPEL 21 - 1986-12 - Article and Ad (Japan).pdf</a> (30 MB)
</li><li>see our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Big Shared Folder</a> for many more.
</li></ul><h3><a name="TechnicalDocumentations" id="TechnicalDocumentations"></a> Technical documentations</h3>
<ul><li><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20Misc%20Technical%20Docs%202024-01-29.zip">Sega AI Computer - Misc Technical Docs (2024-01-29).zip</a> (122 KB) various notes and schematics by Chris Covell.
</li><li>Also see <a href="https://www.smspower.org/forums/15244-SegaAIComputerReverseEngineeringThread" rel="nofollow">Reverse engineering thread</a>.
</li></ul><h2><a name="Links" id="Links"></a> Links</h2>
<h3><a name="OurBigSharedFolder" id="OurBigSharedFolder"></a> Our big shared folder</h3>
<ul><li>Our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Big Shared Folder</a> has about 700 files, 1.6 GB of data.
</li></ul><h3><a name="ExternalResources" id="ExternalResources"></a> External resources</h3>
<ul><li><a href="http://park21.wakwak.com/~suka/sega/catalog/AIcom/AI.html" rel="nofollow">http://park21.wakwak.com/~suka/sega/catalog/AIcom/AI.html</a>
</li><li><a href="https://segaretro.org/Sega_AI_Computer" rel="nofollow">https://segaretro.org/Sega_AI_Computer</a>
</li><li><a href="https://archive.org/search?query=%22%E3%82%BB%E3%82%ACAI%22&amp;sin=TXT" rel="nofollow">https://archive.org/search?query=%22%E3%82%BB%E3%82%ACAI%22&amp;sin=TXT</a>
</li></ul><h3><a name="SiteSections" id="SiteSections"></a> Site sections</h3>
<ul><li><a href="https://www.smspower.org/Games-Releases/AI-JP" rel="nofollow">https://www.smspower.org/Games-Releases/AI-JP</a>
</li><li><a href="https://www.smspower.org/Games/AI-All" rel="nofollow">https://www.smspower.org/Games/AI-All</a>
</li><li><a href="https://www.smspower.org/Scans/AI-All" rel="nofollow">https://www.smspower.org/Scans/AI-All</a>
</li></ul><h3><a name="ForumThreads" id="ForumThreads"></a> Forum threads</h3>
<ul><li><a href="https://www.smspower.org/forums/15004-SegaAIComputerSoftware" rel="nofollow">https://www.smspower.org/forums/15004-SegaAIComputerSoftware</a>
</li><li><a href="https://www.smspower.org/forums/15244-SegaAIComputerReverseEngineeringThread" rel="nofollow">https://www.smspower.org/forums/15244-SegaAIComputerReverseEngineeringThread</a>
</li><li><a href="https://www.smspower.org/forums/19976-SegaAIComputerFirstDumpsSystemRomsScansRecordingsMore" rel="nofollow">https://www.smspower.org/forums/19976-SegaAIComputerFirstDumpsSystemRomsScansRecordingsMore</a>
</li></ul><h2><a name="ResearchCredits" id="ResearchCredits"></a> Research credits</h2>
<h3><a name="Index" id="Index"></a> Credits</h3>
<ul><li>Chris Covell (<a href="https://www.chrismcovell.com/" rel="nofollow">https://www.chrismcovell.com</a>) did the majority of the early hardware reverse engineering, documentation and software dumping.
</li><li>Wilbert Pol and Fabio Priuli went on to create a MAME driver. Early work was actually done in 2015. As we uncovered new software and received it in late 2023, we decided to resume work on this release and Wilbert took care of further emulation work. A first version of the driver was merged into MAME’s codebase on January 23, 2024 (<a href="https://github.com/mamedev/mame/pull/11747" rel="nofollow">pull request</a>).
</li><li>Omar Cornut acquired hardware and software, dumped software cards and tapes acquired later, scanned contents, researched information and organized other work.
</li><li>Charles MacDonald helped build a custom My Card adapter with configurable pinouts to facilitate dumping later finds.
</li><li>Game Preservation Society received the hardware and made the first pass on trying to dump the data. It was found that the Sega AI cards share the pinouts of Sega MyCards used for the SG-1000 II and Master System. A first set of (incomplete) dumps were made which would help moving emulation forward until the unit and software were passed to Chris.
</li></ul>
<h3><a name="SpecialThanks" id="SpecialThanks"></a> Special Thanks</h3>
<ul><li>Thanks to Odin Ozdil for helping acquire extra software and scanning the Sega AI Computer manual.
</li><li>Thanks to Masato Ozaki, BEEP Shop &amp; Takayuki Komabayashi for helping me acquire extra software.
</li><li>Thanks to Black Squirrel and Gorefestor for help writing this article.
</li><li>Thanks to KailoKyra for scan processing, assistance with tape dumping &amp; more.
</li><li>Thanks to Balrog for assistance with tape dumping.
</li><li>Thanks to <a href="http://park21.wakwak.com/~suka/sega" rel="nofollow">SukaSega</a> for many early scans and research.
</li><li>Thanks to BKK for general research.
</li><li>Thanks to <a href="https://sudden-desu.net/" rel="nofollow">Sudden-desu</a> for high-resolution scans of two Japanese flyers.
</li><li>Thanks to Maxim for general support, corrections, web work &amp; many more.
</li><li>Thanks to generous donors helping buying the first batch: initial purchase received funds from: Adrien Duchemole, Bock, Chris Covell, David Papazian (Mobigame), Frank Cifaldi (Video Game History Foundation), Kusfo, Game Preservation Society, GlobZ, Gregg "Greggman" Tavares, Olivier L, Vitorio Miliano.
</li></ul><h3><a name="AboutSMSPower" id="AboutSMSPower"></a> About SMS Power!</h3>
<ul><li>Active since 1997, <a href="https://www.smspower.org/" rel="nofollow">SMS Power!</a> has been researching, gathering and publishing all sort of information and data related to Sega 8-bit systems. We unveiled hundreds of prototypes, and have an active community of hobbyists contributors creating maps, soundtracks, game modifications, translations and more. You may if you can <a href="https://www.smspower.org/Home/Donate" rel="nofollow">Donate to facilitate purchasing prototypes and rare software</a>.
</li></ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLaVA-1.6: Improved reasoning, OCR, and world knowledge (168 pts)]]></title>
            <link>https://llava-vl.github.io/blog/2024-01-30-llava-1-6/</link>
            <guid>39206375</guid>
            <pubDate>Wed, 31 Jan 2024 17:13:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://llava-vl.github.io/blog/2024-01-30-llava-1-6/">https://llava-vl.github.io/blog/2024-01-30-llava-1-6/</a>, See on <a href="https://news.ycombinator.com/item?id=39206375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <!-- for mathjax support -->





<p>In October 2023, we released <a href="https://arxiv.org/abs/2310.03744">LLaVA-1.5</a> with a simple and efficient design along with great performance on a benchmark suite of 12 datasets. It has since served as the foundation of many comprehensive studies of data, model, and capabilities of large multimodal models (LMM), and has enabled various new applications.</p>

<p><strong>Today, we are thrilled to present LLaVA-1.6, with improved reasoning, OCR, and world knowledge. LLaVA-1.6 even exceeds Gemini Pro on several benchmarks.</strong></p>

<p>Compared with LLaVA-1.5, LLaVA-1.6 has several improvements:</p>

<ol>
  <li><strong>Increasing the input image resolution</strong> to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.</li>
  <li><strong>Better visual reasoning and OCR capability</strong> with an improved visual instruction tuning data mixture.</li>
  <li><strong>Better visual conversation for more scenarios</strong>, covering different applications. Better world knowledge and logical reasoning.</li>
  <li><strong>Efficient deployment and inference</strong> with <a href="https://github.com/sgl-project/sglang">SGLang</a>.</li>
</ol>

<p>Along with performance improvements, <strong>LLaVA-1.6 maintains the minimalist design and data efficiency of LLaVA-1.5</strong>. It re-uses the pretrained connector of LLaVA-1.5, and still uses less than 1M visual instruction tuning samples. The largest 34B variant finishes training in ~1 day with 32 A100s. <strong>Code, data, model will be made publicly available.</strong></p>

<h2 id="open-source-release">Open-Source Release</h2>

<p>We open-source the LLaVA-1.6 to facilitate future development of LMM in the community. Code, data, model will be made publicly available.</p>

<ul>
  <li><a href="https://llava.hliu.cc/" target="_blank">Demo</a></li>
  <li><a href="https://github.com/haotian-liu/LLaVA" target="_blank">Code</a> (Training code coming soon)</li>
  <li><a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v16" target="_blank">Model</a></li>
  <li>Data: Coming soon.</li>
</ul>

<h2 id="results">Results</h2>

<p><span>Open-Source</span>
  <span>Proprietary</span>
</p>

<table>
  <tbody><tr><th>Data (PT)</th><th>Data (IT)</th><th>Model</th><th>MMMU (val)</th><th>Math-Vista</th><th>MMB-ENG</th><th>MMB-CN</th><th>MM-Vet</th><th>LLaVA-Wild</th><th>SEED-IMG</th></tr>
  <tr><td>N/A</td><td>N/A</td><td>GPT-4V</td><td>56.8</td><td>49.9</td><td>75.8</td><td>73.9</td><td>67.6</td><td>-</td><td>71.6</td></tr>
  <tr><td>N/A</td><td>N/A</td><td>Gemini Ultra</td><td>59.4</td><td>53</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr><td>N/A</td><td>N/A</td><td>Gemini Pro</td><td>47.9</td><td>45.2</td><td>73.6</td><td>74.3</td><td>64.3</td><td>-</td><td>70.7</td></tr>
  <tr><td>1.4B</td><td>50M</td><td>Qwen-VL-Plus</td><td>45.2</td><td>43.3</td><td>-</td><td>-</td><td>55.7</td><td>-</td><td>65.7</td></tr>
  <tr><td>1.5B</td><td>5.12M</td><td>CogVLM-30B</td><td>32.1</td><td>-</td><td>-</td><td>-</td><td>56.8</td><td>-</td><td>-</td></tr>
  <tr><td>125M</td><td>~1M</td><td>Yi-VL-34B</td><td>45.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr><td>558K</td><td>665K</td><td>LLaVA-1.5-13B</td><td>36.4</td><td>27.6</td><td>67.8</td><td>63.3</td><td>36.3</td><td>72.5</td><td>68.2</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-34B</td><td>51.1</td><td>46.5</td><td>79.3</td><td>79</td><td>57.4</td><td>89.6</td><td>75.9</td></tr>
</tbody></table>

<details>
<summary><b><em>For more results, please unfold to see expanded tables</em></b></summary>

<i>More benchmarks will be added soon.</i>

<p><span>$^\dagger$specialist</span>
  <span>Grey: supervised finetuned</span>
  <span>$^*$training <u>image</u> observed</span>
  <span>Normal text: zero-shot</span>
</p>

<table>
  <tbody><tr><th>Model</th><th>VQAv2</th><th>GQA</th><th>VisWiz</th><th>TextVQA</th><th>ScienceQA</th></tr>
  <tr><td>GPT-4V</td><td>77.2</td><td>-</td><td>-</td><td>78.0</td><td>-</td></tr>
  <tr><td>Gemini Ultra</td><td>77.8</td><td>-</td><td>-</td><td>82.3</td><td>-</td></tr>
  <tr><td>Gemini Pro</td><td>71.2</td><td>-</td><td>-</td><td>74.6</td><td>-</td></tr>
  <tr><td>PALI-X</td><td>86.0$^\dagger$</td><td>-</td><td>70.9$^\dagger$</td><td>71.4$^\dagger$</td><td>-</td></tr>
  <tr><td>CogVLM-30B</td><td>83.4 (84.7$^\dagger$)</td><td>65.2$^\dagger$</td><td>76.4$^\dagger$</td><td>68.1 (69.3$^\dagger$)</td><td>92.7$^\dagger$</td></tr>
  <tr><td>LLaVA-1.5-13B</td><td>80</td><td>63.3</td><td>53.6</td><td>61.3$^*$</td><td>71.6</td></tr>
  <tr><td>LLaVA-1.6-Vicuna-7B</td><td>81.8</td><td>64.2</td><td>57.6</td><td>64.9</td><td>70.1</td></tr>
  <tr><td>LLaVA-1.6-Vicuna-13B</td><td>82.8</td><td>65.4</td><td>60.5</td><td>67.1</td><td>73.6</td></tr>
  <tr><td>LLaVA-1.6-Mistral-7B</td><td>82.2</td><td>64.8</td><td>60.0</td><td>65.7</td><td>72.8</td></tr>
  <tr><td>LLaVA-1.6-34B</td><td>83.7</td><td>67.1</td><td>63.8</td><td>69.5</td><td>81.8</td></tr>
</tbody></table>


<table>
  <tbody><tr><th>Data (PT)</th><th>Data (IT)</th><th>Model</th><th>MMMU (val)</th><th>MMMU (test)</th><th>MathVista</th><th>MMB-ENG</th><th>MMB-CN</th><th>MM-Vet</th><th>LLaVA-Wild</th><th>SEED-IMG</th><th>MME</th><th>POPE</th></tr>
  <tr><td>N/A</td><td>N/A</td><td>GPT-4V</td><td>56.8</td><td>55.7</td><td>49.9</td><td>75.8</td><td>73.9</td><td>67.6</td><td>-</td><td>71.6</td><td>-</td><td>-</td></tr>
  <tr><td>N/A</td><td>N/A</td><td>Gemini Ultra</td><td>59.4</td><td>-</td><td>53</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr><td>N/A</td><td>N/A</td><td>Gemini Pro</td><td>47.9</td><td>-</td><td>45.2</td><td>73.6</td><td>74.3</td><td>64.3</td><td>-</td><td>70.7</td><td>-</td><td>-</td></tr>
   <tr><td>1.4B</td><td>50M</td><td>Qwen-VL-Plus</td><td>45.2</td><td>40.8</td><td>43.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr> 
  <tr><td>1.5B</td><td>5.12M</td><td>CogVLM-30B</td><td>32.1</td><td>30.1</td><td>-</td><td>-</td><td>-</td><td>56.8</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr><td>558K</td><td>665K</td><td>LLaVA-1.5-13B (336$^2$)</td><td>36.4</td><td>33.6</td><td>27.6</td><td>67.8</td><td>63.3</td><td>36.3</td><td>72.5</td><td>68.2</td><td>1531/295</td><td>85.93</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-Vicuna-7B (672$^2$)</td><td>35.8</td><td>-</td><td>34.6</td><td>67.4</td><td>60.6</td><td>43.9</td><td>81.6</td><td>70.2</td><td>1519/332</td><td>86.53</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-Mistral-7B (672$^2$)</td><td>35.3</td><td>-</td><td>37.7</td><td>68.7</td><td>61.2</td><td>47.3</td><td>83.2</td><td>72.2</td><td>1498/321</td><td>86.73</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-13B (672$^2$)</td><td>36.2</td><td>-</td><td>35.3</td><td>70</td><td>64.4</td><td>48.4</td><td>87.3</td><td>71.9</td><td>1575/326</td><td>86.23</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-34B (672$^2$)</td><td>51.1</td><td>44.7</td><td>46.5</td><td>79.3</td><td>79</td><td>57.4</td><td>89.6</td><td>75.9</td><td>1631/397</td><td>87.73</td></tr>
</tbody></table>

</details>



<p>✨ Highlights:</p>
<ul>
  <li><strong>SoTA Performance!</strong> LLaVA-1.6 achieves the best performance compared with open-source LMMs such as <a href="https://github.com/THUDM/CogVLM" target="_blank">CogVLM</a> or <a href="https://huggingface.co/01-ai/Yi-VL-34B" target="_blank">Yi-VL</a>. Compared with commercial ones, it catches up to Gemini Pro and outperforms <a href="https://huggingface.co/spaces/Qwen/Qwen-VL-Plus" target="_blank">Qwen-VL-Plus</a> on selected benchmarks.</li>
  <li><strong>Zero-shot Chinese capability</strong>. LLaVA-1.6’s Chinese capability is an emerging zero-shot capability (i.e., only English multimodal data is considered). Its performance on Chinese multimodal scenarios is surprisingly good, e.g., SoTA on MMBench-CN.</li>
  <li><strong>Low Training Cost</strong>. LLaVA-1.6 is trained with 32 GPUs for ~1 day, with 1.3M data samples in total. The compute / training data cost is 100-1000 times smaller than others.</li>
</ul>

<h3 id="qualitative-results">Qualitative Results</h3>

<details>
<summary><b>Mark Zuckerberg talks about Llama-3 and 600K H100s</b></summary>

<div>

  <h4>User: Explain this to me in detail.</h4>

  <p><img width="90%" src="https://llava-vl.github.io/blog/assets/images/llava-1-6/zuck_llama3.jpg">

</p></div>

<div>

  <h4>LLaVA-1.6-34B</h4>

  <p>The image you've provided appears to be a screenshot of a social media post, specifically from Instagram, featuring a man speaking and a text overlay. The man in the image is wearing a black t-shirt and appears to be in the middle of a conversation or presentation. The text overlay is a quote from a post by a user named "zuck," which is presumably a reference to Mark Zuckerberg, the CEO of Meta (formerly known as Facebook).</p>

  <p>The text overlay reads:</p>

  <p>"Some updates on our AI efforts. Our long term vision is to build general intelligence, open source it responsibly, and make it widely available so everyone can benefit. We're bringing our two major AI research efforts (FAIR and GenAI) closer together to support this. We're currently training our next-gen model Llama 3, and we're building massive compute infrastructure to support our future roadmap, including 35k H100s by the end of this year — and overall almost 600k H100s equivalents of compute if you include other GPUs. Also really excited about our progress building new AI-centric computing devices like Ray Ban Meta smart glasses. Lots more to come."</p>

  <p>This text provides insights into Meta's artificial intelligence (AI) initiatives. It mentions that the company's long-term goal is to develop general intelligence and make it accessible to everyone. The post also discusses the integration of Meta's two AI research efforts, FAIR (Facebook AI Research) and GenAI, to support this goal.</p>

  <p>The text also mentions the training of a next-generation AI model called Llama 3 and the construction of massive compute infrastructure to support Meta's future plans. It highlights the company's investment in NVIDIA's H100 GPUs, with a total of 35,000 H100s by the end of the year and almost 600,000 H100s equivalents of compute if other GPUs are included.</p>

  <p>Lastly, the post expresses excitement about Meta's progress in developing AI-centric computing devices, specifically Ray Ban Meta smart glasses. The post concludes with a statement that there is more to come, suggesting that Meta has additional AI-related announcements or developments in the pipeline.</p>

  <p>The image of the man speaking could be from a video or a live stream where he is discussing these AI updates. The context of the image suggests that it is part of a larger conversation or announcement related to Meta's AI strategy and advancements.</p>

</div>

</details>

<details>
<summary><b>When to leave to pickup based on flight info?</b></summary>

<div>

  <h4>User: I need to pick up my wife. I live in San Jose. When should I leave?</h4>

  <p><img width="90%" src="https://llava-vl.github.io/blog/assets/images/llava-1-6/ewr_sfo_flight.png">

</p></div>

<div>

  <h4>LLaVA-1.6-34B</h4>

  <p>Based on the information provided in the image, the flight is scheduled to arrive at 11:51 AM at San Francisco International Airport (SFO). If you live in San Jose, you should consider the travel time between San Jose and San Francisco, which is approximately 45-60 minutes depending on traffic conditions.</p>

  <p>To ensure you have enough time to pick up your wife, you should leave San Jose no later than 11:00 AM to account for traffic and any unexpected delays. However, it's always a good idea to leave earlier than necessary to ensure you arrive on time.</p>

</div>

</details>


<h2 id="detailed-technical-improvement">Detailed Technical Improvement</h2>

<p>We detail the findings we have and improvements we make to LLaVA-1.5. <em>More implementation details will be released and documented here in the coming days.</em></p>

<h3 id="1-dynamic-high-resolution">(1) Dynamic High Resolution</h3>

<p>We design our model at high resolution with an aim to <strong>preserve its data efficiency</strong>.
When provided with high-resolution images and representations that preserve these details, the model’s capacity to perceive intricate details in an image is significantly improved. It reduces the model hallucination that conjectures the imagined visual content when confronted with low-resolution images. Our ‘AnyRes’ technique is designed to accommodate images of various high resolutions. We employ a grid configuration of $\{2 \times 2, 1 \times \{2,3,4\}, \{2,3,4\} \times 1\}$, balancing performance efficiency with operational costs. See our <a href="https://static.hliu.cc/files/llava/improved_llava.pdf">updated LLaVA-1.5 technical report</a> for more details.</p>

<p>
  <img width="90%" src="https://llava-vl.github.io/blog/assets/images/llava-1-6/high_res_arch_v2.png">
  <br>
  <em>Illustration of dynamic high resolution scheme: a grid configuration of $\\{2 \times 2\\}$</em>
</p>

<h3 id="2-data-mixture">(2) Data Mixture</h3>

<ul>
  <li><strong>High-quality User Instruct Data</strong>. Our definition of high-quality visual instruction-following data hinges on two principal criteria: First, the diversity of task instructions, ensuring adequately represent a broad spectrum of user intents that are likely to be encountered in real-world scenarios, particularly during the model’s deployment phase. Second, the superiority of responses is critical, with the objective of soliciting favorable user feedback. To achieve this, we consider two data sources: (1) Existing GPT-V data. <a href="https://huggingface.co/datasets/laion/gpt4v-dataset">LAION-GPT-V</a> and <a href="https://sharegpt4v.github.io//">ShareGPT-4V</a>. (2) To further facilitate better visual conversation for more scenarios, we collect a small 15K visual instruction tuning dataset covering different applications. The instructions and images come from <a href="https://llava-vl.github.io/">LLaVA demo</a>, which are real-world users requests. We carefully filter samples that may have privacy concerns or are potentially harmful, and generate the response with GPT-4V.</li>
  <li><strong>Multimodal Document/Chart Data</strong>. (1) We remove <a href="https://textvqa.org/textcaps/">TextCaps</a> from our training data as we realize that TextCaps uses the same set of training images as <a href="https://textvqa.org/">TextVQA</a>. This allows us to better understand the zero-shot OCR capability of our model when evaluating TextVQA during development. To maintain and further improve our model’s OCR capability, we replace TextCaps with DocVQA and SynDog-EN. (2) Motivated by <a href="https://huggingface.co/Qwen/Qwen-VL">Qwen-VL-7B-Chat</a>, we further add ChartQA, DVQA, and AI2D for better chart and diagram understanding.</li>
</ul>

<h3 id="3-scaling-llm-backbone">(3) Scaling LLM backbone</h3>

<p>In addition to Vicuna-1.5 (<a href="https://huggingface.co/lmsys/vicuna-7b-v1.5">7B</a> and <a href="https://huggingface.co/lmsys/vicuna-13b-v1.5">13B</a>), we consider more LLMs, including <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a> and <a href="https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B">Nous-Hermes-2-Yi-34B</a>. These LLMs possess nice properties, flexible commercial use terms, strong bilingual support and larger language model capacity. It allows LLaVA to support a wider spectrum of users and more scenarios in the community. The LLaVA recipe works well with various LLMs, and scales up smoothly with the LLM up to 34B.</p>

<h2 id="model-card">Model Card</h2>

<table>
  <tbody><tr><th colspan="2">Name</th><th>LLaVA-1.6-7B</th><th>LLaVA-1.6-13B</th><th>LLaVA-1.6-34B</th></tr>
  <tr><th rowspan="4">Model Size</th><td>Total</td><td><b>7.06B</b></td><td><b>13.35B</b></td><td><b>34.75B</b></td></tr>
  <tr><td>Vision Encoder</td><td>303.5M</td><td>303.5M</td><td>303.5M</td></tr>
  <tr><td>Connector</td><td>21M</td><td>31.5M</td><td>58.7M</td></tr>
  <tr><td>LLM</td><td>6.74B</td><td>13B</td><td>34.39B</td></tr>
  <tr><th colspan="2">Resolution</th><td colspan="3">336 x [(2,2), (1,2), (2,1), (1,3), (3,1), (1,4), (4,1)]</td></tr>
  <tr><th>Stage-1</th><th>Training Data</th><td colspan="3">558K</td></tr>
  <tr><th></th><th>Trainable Module</th><td colspan="3">Connector</td></tr>
  <tr><th>Stage-2</th><th>Training Data</th><td colspan="3">760K</td></tr>
  <tr><th></th><th>Trainable Module</th><td colspan="3">Full model</td></tr>
  <tr><th colspan="2">Compute (#GPU x #Hours)</th><td>8x20</td><td>16x24</td><td>32x30</td></tr>
  <tr><th colspan="2">Training Data (#Samples)</th><td colspan="3">1318K</td></tr>
</tbody></table>

<h2 id="team">Team</h2>



<h2 id="acknowledgement">Acknowledgement</h2>

<ul>
  <li>A16Z Open Source AI Grants Program.</li>
  <li>We thank Lianmin Zheng, Ying Sheng, Shiyi Cao for the integration of LLaVA to SGLang.</li>
  <li>This work was supported in part by NSF CAREER IIS2150012, Microsoft Accelerate Foundation Models Research, and Institute of Information &amp; communications Technology Planning &amp; Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training).</li>
</ul>

<h2 id="citation">Citation</h2>

<div><pre><code><span>@misc</span><span>{</span><span>liu2024llava16</span><span>,</span>
    <span>title</span><span>=</span><span>{LLaVA-1.6: Improved reasoning, OCR, and world knowledge}</span><span>,</span>
    <span>url</span><span>=</span><span>{https://llava-vl.github.io/blog/2024-01-30-llava-1-6/}</span><span>,</span>
    <span>author</span><span>=</span><span>{Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae}</span><span>,</span>
    <span>month</span><span>=</span><span>{January}</span><span>,</span>
    <span>year</span><span>=</span><span>{2024}</span>
<span>}</span>

<span>@misc</span><span>{</span><span>liu2023improvedllava</span><span>,</span>
      <span>title</span><span>=</span><span>{Improved Baselines with Visual Instruction Tuning}</span><span>,</span> 
      <span>author</span><span>=</span><span>{Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae}</span><span>,</span>
      <span>publisher</span><span>=</span><span>{arXiv:2310.03744}</span><span>,</span>
      <span>year</span><span>=</span><span>{2023}</span><span>,</span>
<span>}</span>

<span>@misc</span><span>{</span><span>liu2023llava</span><span>,</span>
      <span>title</span><span>=</span><span>{Visual Instruction Tuning}</span><span>,</span> 
      <span>author</span><span>=</span><span>{Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae}</span><span>,</span>
      <span>publisher</span><span>=</span><span>{NeurIPS}</span><span>,</span>
      <span>year</span><span>=</span><span>{2023}</span><span>,</span>
<span>}</span>
</code></pre></div>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Differ: Tool for testing and validating transformed programs (111 pts)]]></title>
            <link>https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/</link>
            <guid>39205784</guid>
            <pubDate>Wed, 31 Jan 2024 16:33:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/">https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/</a>, See on <a href="https://news.ycombinator.com/item?id=39205784">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">

			
				
<article id="post-106148">
	<!-- .entry-header -->

	<div>
		<p><em>By Michael Brown</em></p>
<p>We recently released a new differential testing tool, called DIFFER, for finding bugs and soundness violations in transformed programs. DIFFER combines elements from differential, regression, and fuzz testing to help users find bugs in programs that have been altered by software rewriting, debloating, and hardening tools. We used DIFFER to evaluate 10 software debloating tools, and it discovered debloating failures or soundness violations in 71% of the transformed programs produced by these tools.</p>
<p>DIFFER fills a critical need in post-transformation software validation. Program transformation tools usually leave this task entirely to users, who typically have few (if any) tools beyond regression testing via existing unit/integration tests and fuzzers. These approaches do not naturally support testing transformed programs against their original versions, which can allow subtle and novel bugs to find their way into the modified programs.</p>
<p>We’ll provide some background research that motivated us to create DIFFER, describe how it works in more detail, and discuss its future.</p>
<p>If you prefer to go straight to the code, check out <a href="https://github.com/trailofbits/differ" target="_blank" rel="noopener">DIFFER on GitHub</a>.</p>
<p><a href="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?ssl=1"><img fetchpriority="high" decoding="async" data-attachment-id="106163" data-permalink="https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/differ-tool-design-2/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?fit=1000%2C820&amp;ssl=1" data-orig-size="1000,820" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Differ Tool Design (2)" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?fit=300%2C246&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?fit=690%2C566&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?resize=690%2C566&amp;ssl=1" alt="" width="690" height="566" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?w=1000&amp;ssl=1 1000w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?resize=300%2C246&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?resize=768%2C630&amp;ssl=1 768w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></a></p>
<h2>Background</h2>
<p>Software transformation has been a hot research area over the past decade and has primarily been motivated by the need to secure legacy software. In many cases, this must be done without the software’s source code (binary only) because it has been lost, is vendor-locked, or cannot be rebuilt due to an obsolete build chain. Among the more popular research topics that have emerged in this area are binary lifting, recompiling, rewriting, patching, hardening, and debloating.</p>
<p>While tools built to accomplish these goals have demonstrated some successes, they carry significant risks. When compilers lower source code to binaries, they discard contextual information once it is no longer needed. Once a program has been lowered to binary, the contextual information necessary to safely modify the original program generally cannot be fully recovered. As a result, tools that modify program binaries directly may inadvertently break them and introduce new bugs and vulnerabilities.</p>
<p>While DIFFER is application-agnostic, we originally built this tool to help us find bugs in programs that have had unnecessary features removed with a debloating tool (e.g., <a href="https://github.com/michaelbrownuc/CARVE" target="_blank" rel="noopener">Carve</a>, <a href="https://github.com/ashish-gehani/Trimmer" target="_blank" rel="noopener">Trimmer</a>, <a href="https://github.com/cxreet/razor" target="_blank" rel="noopener">Razor</a>). In general, software debloaters try to minimize a program’s attack surface by removing unnecessary code that may contain latent vulnerabilities or be reused by an attacker using code-reuse exploit patterns. Debloating tools typically perform an analysis pass over the program to map features to the code necessary to execute them. These mappings are then used to cut code that corresponds to features the user doesn’t want. However, these cuts will likely be imprecise because generating the mappings relies on imprecise analysis steps like binary recovery. As a result, new bugs and vulnerabilities can be introduced into debloated programs during cutting, which is exactly what we have designed DIFFER to detect.</p>
<h2>How does DIFFER work?</h2>
<p>At a high level, DIFFER (shown in figure 1) is used to test an unmodified version of the program against one or more modified variants of the program. DIFFER allows users to specify seed inputs that correspond to both unmodified and modified program behaviors and features. It then runs the original program and the transformed variants with these inputs and compares the outputs. Additionally, DIFFER supports template-based mutation fuzzing of these seed inputs. By providing mutation templates, DIFFER can maximize its coverage of the input space and avoid missing bugs (i.e., false negatives).</p>
<p>DIFFER expects to see the same outputs for the original and variant programs when given inputs that correspond to unmodified features. Conversely, it expects to see different outputs when it executes the programs with inputs corresponding to modified features. If DIFFER detects unexpected matching, differing, or crashing outputs, it reports them to the user. These reports help the user identify errors in the modified program resulting from the transformation process or its configuration.</p>
<div id="attachment_106156"><p><a href="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?ssl=1"><img decoding="async" aria-describedby="caption-attachment-106156" data-attachment-id="106156" data-permalink="https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/differ-overview/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?fit=1999%2C1480&amp;ssl=1" data-orig-size="1999,1480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="DIFFER Overview" data-image-description="" data-image-caption="<p>Figure 1: Overview of DIFFER</p>
" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?fit=300%2C222&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?fit=690%2C511&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=690%2C511&amp;ssl=1" alt="" width="690" height="511" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?w=1999&amp;ssl=1 1999w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=300%2C222&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=1024%2C758&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=768%2C569&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=1536%2C1137&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=1200%2C888&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></a></p><p id="caption-attachment-106156">Figure 1: Overview of DIFFER</p></div>
<p>When configuring DIFFER, the user selects one or more comparators to use when comparing outputs. While DIFFER provides many built-in comparators that check basic outputs such as return codes, console text, and output files, more advanced comparators are often needed. For this purpose, DIFFER allows users to add custom comparators for complex outputs like packet captures. Custom comparators are also useful for reducing false-positive reports by defining allowable differences in outputs (such as timestamps in console output). Our open-source release of DIFFER contains many useful comparator implementations to help users easily write their own comparators.</p>
<p>However, DIFFER does not and cannot provide formal guarantees of soundness in transformation tools or the modified programs they produce. Like other dynamic analysis testing approaches, DIFFER cannot exhaustively test the input space for complex programs in the general case.</p>
<h2>Use case: evaluating software debloaters</h2>
<p>In a recent <a href="https://arxiv.org/abs/2312.13274" target="_blank" rel="noopener">research study</a> we conducted in collaboration with our friends at GrammaTech, we used DIFFER to evaluate debloated programs created by 10 different software debloating tools. We used these tools to remove unnecessary features from 20 different programs of varying size, complexity, and purpose. Collectively, the tools created 90 debloated variant programs that we then validated with DIFFER. DIFFER discovered that 39 (~43%) of these variants still had features that debloating tools failed to remove. Even worse, DIFFER found that 25 (~28%) of the variants either crashed or produced incorrect outputs in retained features after debloating.</p>
<p>By discovering these failures, DIFFER has proven itself as a useful post-transformation validation tool. Although this study was focused on debloating transformations, we want to emphasize that DIFFER is general enough to test other transformation tools such as those used for software hardening (e.g., CFI, stack protections), translation (e.g., C-to-Rust transformers), and surrogacy (e.g., ML surrogate generators).</p>
<h2>What’s next?</h2>
<p>With DIFFER now available as open-source software, we invite the security research community to use, extend, and help maintain DIFFER via pull requests. We have several specific improvements planned as we continue to research and develop DIFFER, including the following:</p>
<ul>
<li>Support running binaries in Docker containers to reduce environmental burdens.</li>
<li>Add new built-in comparators.</li>
<li>Add support for targets that require superuser privileges.</li>
<li>Support monitoring multiple processes that make up distributed systems.</li>
<li>Add runtime comparators (via instrumentation, etc.) for “deep” equivalence checks.</li>
</ul>
<h3>Acknowledgements</h3>
<p>This material is based on work supported by the Office of Naval Research (ONR) under Contract No. N00014-21-C-1032. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the ONR.</p>

			</div><!-- .entry-content -->

	
</article><!-- #post-106148 -->
						<!-- #nav-below -->
		
					<!-- #comments .comments-area -->

			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Y Combinator CEO Garry Tan's online rant spurs threats to supes, police reports (429 pts)]]></title>
            <link>https://missionlocal.org/2024/01/y-combinator-ceo-garry-tans-online-rant-spurs-threat-to-supe-police-reports/</link>
            <guid>39205676</guid>
            <pubDate>Wed, 31 Jan 2024 16:26:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://missionlocal.org/2024/01/y-combinator-ceo-garry-tans-online-rant-spurs-threat-to-supe-police-reports/">https://missionlocal.org/2024/01/y-combinator-ceo-garry-tans-online-rant-spurs-threat-to-supe-police-reports/</a>, See on <a href="https://news.ycombinator.com/item?id=39205676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
		
<p>At least two San Francisco supervisors received a threatening letter at their homes on Tuesday and two more filed police reports after Garry Tan, the CEO of Y Combinator and a heavy campaign donor to efforts to oust progressive politicians, <a href="https://missionlocal.org/2024/01/garry-tan-death-wish-sf-supervisors/">posted online that seven supervisors should die a slow death</a>.</p>



<p>Tan wrote “Die slow motherfuckers” in reference to Supervisors Aaron Peskin, Connie Chan, Myrna Melgar, Shamann Walton, Hillary Ronen, Dean Preston and Ahsha Safaí in a <a href="https://missionlocal.org/2024/01/garry-tan-death-wish-sf-supervisors/">late-night rant Saturday</a>. During his online tirade, Tan <a href="https://missionlocal.org/2024/01/garry-tan-death-wish-sf-supervisors/">posted photos of his private liquor stash, and indicated to a fellow Twitter-user that he was inebriated</a>. </p>



<p>The “die slow, motherfucker” line was a reference to a Tupac Shakur song, and Tan later apologized. That 1996 song, “<a href="https://www.youtube.com/watch?v=41qC3w3UUkU">Hit ‘Em Up</a>,” escalated the simmering East Coast-West Coast rap rivalry into a lethal feud; Shakur was gunned down three months after its release.&nbsp;</p>




<p>The named supervisors did not take the incident lightly:<a href="https://www.sfchronicle.com/sf/article/sf-garry-tan-x-threatening-peskin-police-report-18637813.php"> Peskin and Chan</a> on Tuesday <a href="https://www.sfchronicle.com/sf/article/sf-garry-tan-x-threatening-peskin-police-report-18637813.php">filed police reports</a>, Melgar and Safaí have pledged to do the same and both Peskin and Preston today received a letter to their homes citing Tan’s diatribe and wishing death upon them and their families.</p>



<p>When asked if she felt personally threatened by Tan’s behavior, Chan responded “Seeing what my colleague[s] received in the mail? Yes, absolutely. I have a 10-year-old. I do not tell people where my child attends school.”</p>



<p><a href="https://www.sfchronicle.com/sf/article/sf-garry-tan-x-threatening-peskin-police-report-18637813.php">The <em>San Francisco Chronicle</em> first reported </a>the hate mail sent to Preston and the police reports filed by Chan and Peskin. Peskin at 8:40 p.m. confirmed to Mission Local that he, too, received a threatening letter at his home. The envelope was emblazoned with a smiling portrait of Tan.</p>



<p>“Garry Tan is right!” the letter sent to Peskin, Preston and, perhaps, others read. “I wish a slow and painful death for you and your loved ones.”</p>



<figure><img decoding="async" width="853" height="640" src="https://missionloca.s3.amazonaws.com/mission/wp-content/uploads/2024/01/IMG_0761-853x640.jpeg" alt="Gary Tan is the right poster."><figcaption>The letter received today by Supervisors Aaron Peskin and Dean Preston</figcaption></figure>



<p>Preston has in particular drawn the venom of Tan: Preston is the lone democratic socialist on the Board of Supervisors, an odious position to Tan, and Tan has donated $5,000 to Preston’s opponent and <a href="https://sfstandard.com/2023/07/31/dump-dean-preston-pac-growsf-declares-war-sf-supervisor-election/">pledged</a> another $50,000 to the effort to unseat him.</p>




<p>Bizarrely, the letter to Preston and Peskin concluded: “This mail was sent to communicate a political opinion. No threats were intended.”</p>



<p>The board’s five Jewish members —&nbsp;Peskin, Ronen, Melgar, Preston and Rafael Mandelman — <a href="https://sfstandard.com/2023/10/20/sickening-antisemitic-postcard-mailed-to-homes-of-5-san-francisco-supervisors/">in October received antisemitic postcards</a> at their homes. Peskin said multiple supervisors have received as many as four more antisemitic letters or postcards since then. </p>



<p>He noted that the exact phrase “This mail was sent to communicate a political opinion. No threats were intended” was written on one of the disturbing communiques he received earlier.&nbsp;</p>



<p>Chan says she received a letter in December at her home accusing her of “white genocide.” Chan, who is Chinese, is married to a white man and has&nbsp;a mixed-race child.&nbsp;</p>



<p>While she first thought to “brush off” Tan’s comments, Chan says she was initially unaware how tied in he is politically. She then learned that Tan has hosted numerous moderate supervisor candidates at his home, including her major challenger, Marjan Philhour.</p>




<p>Chan, in her police report, notes that she had filed a prior report last year against Philhour’s staffer, Forrest Liu, whom she accuses of twice physically threatening her — publicly and in person.</p>



<p>After realizing the association of Tan to Philhour to Liu, she said “it then dawned on me that these unhinged individuals who have wished me death online as well as physically threaten me are connected, it became alarming to me that the threats are potentially violent and imminent.” </p>



<p>Liu did not respond to requests for comment. </p>



<p>Tan has indeed hosted San Francisco moderate political figures at his Mission District home, including District Attorney Brooke Jenkins, Philhour and District 9 candidate Trevor Chandler.</p>



<p>Philhour and Chandler both criticized their backer Tan in light of his comments.</p>



<p>“Threats to one’s life or safety have no place in political discourse,” Philhour said.</p>



<p>“It is never okay to advocate violence, be it seriously or in jest,” said Chandler.&nbsp;</p>



<p>Jenkins did not respond to requests for comment.</p>



<p>Peskin today asked the City Attorney’s Office to look into requiring public disclosures from recipients of political donations from “purveyors of hate and violence.” </p>



<p>“When individuals in our society, especially and most concerning those with money,<br>power and influence, call for public servants to be tortured or killed, it is crossing a bright line — and must not be condoned by society,” Peskin said today in Board chambers. “To ignore it is dangerous and a disservice to democracy.”</p>



<p>But tying this potential legislation to the message Tan communicated to his 408,000 Twitter followers would appear to be a serious legal challenge: Half a dozen lawyers and judges told Mission Local that, however ill-advised, Tan’s comments do not rise to the legal definition of a death threat.&nbsp;</p>



<p><a href="https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode=PEN&amp;sectionNum=422.">Under Penal Code 422</a>, a person making a criminal threat must harbor “specific intent that the statement, made verbally, in writing, or by means of an electronic communication device, is to be taken as a threat…”&nbsp;</p>



<p>“It is offensive, but it is speech protected by the First Amendment,” said Berkeley School of Law dean Erwin Chemerinsky. “It does not meet the standard for incitement.”&nbsp;</p>



<p>Tan is a well-heeled donor for San Francisco’s moderate causes and candidates. He sits on the board of Grow SF, a political pressure group favoring moderate causes and candidates and targeting progressives. Tan gave more than $100,000 to the <a href="https://missionlocal.org/2022/05/explore-almost-9-million-donated-for-and-against-boudin-recall/">2022 campaign to recall then-District Attorney Chesa Boudin</a>. He gave at least $20,000 to the <a href="https://missionlocal.org/2022/01/who-is-funding-the-school-board-recall/">2021 school board recall</a>, too.</p>



<p>Tan could not be reached for comment. Neither Y Combinator nor Grow SF immediately responded to requests for comment.</p>


	<div data-posts="" data-current-post-id="581806">
							<h2>
					<span>See Also</span>
				</h2>
						
	<article data-post-id="581443">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

				</div>
		</div></div>]]></description>
        </item>
    </channel>
</rss>