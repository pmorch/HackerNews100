<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 27 Dec 2025 21:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Nvidia's $20B Antitrust Loophole (Not an Acquisition) (212 pts)]]></title>
            <link>https://ossa-ma.github.io/blog/groq</link>
            <guid>46403559</guid>
            <pubDate>Sat, 27 Dec 2025 17:42:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ossa-ma.github.io/blog/groq">https://ossa-ma.github.io/blog/groq</a>, See on <a href="https://news.ycombinator.com/item?id=46403559">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p><span>AI</span><span>Research</span><span>Opinion/Mild Hating</span><span>Geopolitical Circlejerk</span><span>Venture Capital</span></p><article><p>"You're taking on a giant. What gives you the audacity?"</p>
<p>On November 5th, 2025, Groq CEO Jonathan Ross was asked why he was even bothering to challenge Nvidia. He didn't blink:</p>
<p>"I think that was a polite way to ask why in the world are we competing with Nvidia, so we're not. Competition is a waste of money; competition fundamentally means you are taking something someone else is doing and trying to copy it. You're wasting R&amp;D dollars trying to do the exact same thing they've done instead of using them to differentiate."</p>
<p>49 days later, Nvidia paid $20 billion for Groq's assets and hired Ross along with his entire executive team.</p>
<p>Except this wasn't actually an acquisition, at least not in the traditional sense. Nvidia paid $20 billion for Groq's IP and people, but explicitly did NOT buy the company.
Jensen Huang's statement was surgical: "While we are adding talented employees to our ranks and licensing Groq's IP, we are not acquiring Groq as a company."</p>
<p>That phrasing is the entire story. Because what Nvidia carved out of the deal tells you everything about why this happened.</p>
<p>Forget the AI doomer takes about a bubble forming, lets look into the actual reasons.</p>
<h2>What Nvidia Actually Bought (And What It Didn't)</h2>
<p>Nvidia acquired:</p>
<ul>
<li>All of Groq's intellectual property and patents</li>
<li>Non-exclusive licensing rights to Groq's inference technology</li>
<li>Jonathan Ross (CEO), Sunny Madra (President), and the entire senior leadership team</li>
</ul>
<p>Nvidia explicitly did NOT buy:</p>
<ul>
<li>GroqCloud (the cloud infrastructure business)</li>
</ul>
<p>GroqCloud continues as an independent company under CFO Simon Edwards.
This is Nvidia's largest acquisition ever (previous record was Mellanox at $7B in 2019), and they structured it to leave the actual operating business behind. That doesn't happen by accident.</p>
<h2>LPU vs TPU vs CPU: Why SRAM Matters</h2>
<p>To understand why Nvidia paid anything for Groq, you need to understand the architectural bet Ross made when he left Google.</p>
<p><strong>CPUs and GPUs</strong> are built around external DRAM/HBM (High Bandwidth Memory). Every compute operation requires shuttling data between the processor and off-chip memory. This works fine for general-purpose computing, but for inference workloads, that constant round-trip creates latency and energy overhead. GPUs evolved from graphics rendering, so they're optimized for parallel training workloads, not sequential inference.</p>
<p><strong>TPUs (Google's Tensor Processing Units)</strong> reduce some of this overhead with larger on-chip buffers and a systolic array architecture, but they still rely on HBM for model weights and activations. They're deterministic in execution but non-deterministic in memory access patterns.</p>
<p><strong>LPUs (Groq's Language Processing Units)</strong> take a different approach: massive on-chip SRAM instead of external DRAM/HBM. The entire model (for models that fit) lives in SRAM with 80 TB/s of bandwidth and 230 MB capacity per chip. No off-chip memory bottleneck. No dynamic scheduling. The architecture is entirely deterministic from compilation to execution. You know exactly what happens at each cycle on each chip at each moment.</p>
<p>This creates massive advantages for inference:</p>
<ul>
<li>Llama 2 7B: 750 tokens/sec (2048 token context)</li>
<li>Llama 2 70B: 300 tokens/sec (4096 token context)</li>
<li>Mixtral 8x7B: 480 tokens/sec (4096 token context)</li>
</ul>
<p>And 10x better energy efficiency because you're not constantly moving data across a memory bus.</p>
<p><img src="https://ossa-ma.github.io/images/sota_tps_chart.png" alt="SOTA model tokens/sec throughput on GPU inference">
<em>Compare this to SOTA model tokens/sec throughput on GPU inference</em></p>
<p>Serious trade off though, only 14GB of SRAM per rack means you can't run Llama 3.1 405B. And LPUs can't train models at all. This is an inference-only architecture with limited model size support.</p>
<p>But here's what makes this interesting: if DRAM/HBM prices continue climbing (DRAM has tripled in a year I should've gone all in DRAM in Jan I'm done with indexes), and if inference becomes the dominant AI workload (which it is), SRAM-based architectures become economically compelling despite the size limitations. Most production AI applications aren't running 405B models. They're running 7B-70B models that need low latency and high throughput.</p>
<h2>The $13.1B Premium and the Non-Traditional Structure</h2>
<p>Groq raised $750 million in September 2025 at a post-money valuation of $6.9 billion. Three months later on Xmas Eve, Nvidia paid $20 billion through a "non-exclusive licensing agreement" that acquired all IP and talent while explicitly NOT buying the company.</p>
<p>That's a $13.1 billion premium (3x the September valuation) for a company valued at 40x target revenue (double Anthropic's recent 20x multiple) with slashed projections (The Information reported Groq cut over $1B from 2025 revenue forecasts).</p>
<p>The structure is the story. Traditional M&amp;A would trigger:</p>
<ul>
<li>CFIUS review (given Saudi contracts)</li>
<li>Antitrust scrutiny (Nvidia's dominant position)</li>
<li>Shareholder votes and disclosure requirements</li>
<li>Multi-year regulatory review</li>
</ul>
<p>Non-exclusive licensing bypasses all of it. No acquisition means no CFIUS review. "Non-exclusive" means no monopoly concerns (anyone can license Groq's tech). No shareholder votes, minimal disclosure.</p>
<p>But in practice: Nvidia gets the IP (can integrate before anyone else), the talent (Ross + team can't work for competitors now), and the elimination of GroqCloud (will likely die without IP or leadership). The "non-exclusive" label is legal fiction. When you acquire all the IP and hire everyone who knows how to use it, exclusivity doesn't matter.</p>
<p>The question isn't just why Nvidia paid $13.1B more than market rate for technology they could build themselves (they have the PDK, volume, talent, infrastructure, and cash). The question is why they structured it this way.</p>
<p><strong>The $13.1B premium bought five things a traditional acquisition couldn't:</strong></p>
<ol>
<li>
<p><strong>Regulatory arbitrage</strong>: Non-exclusive licensing avoids years of antitrust review. Structure the deal as IP licensing + talent acquisition, and regulators have no grounds to block it. This alone is worth billions in time and certainty.</p>
</li>
<li>
<p><strong>Neutralizing Meta/Llama</strong>: The April 2025 partnership gave Groq distribution to millions of developers. If Llama + Groq became the default open-source inference stack, Nvidia's ecosystem gets commoditized. Kill the partnership before it scales.</p>
</li>
<li>
<p><strong>Eliminating GroqCloud without inheriting Saudi contracts</strong>: Nvidia has invested in other cloud providers. GroqCloud was a competitor. Traditional acquisition would mean inheriting $1.5B worth of contracts to build AI infrastructure for Saudi Arabia, triggering CFIUS scrutiny. The carve-out kills GroqCloud while avoiding geopolitical entanglement.</p>
</li>
<li>
<p><strong>Political access</strong>: Chamath makes ~$2B (Social Capital's ~10% stake). Sacks looks good (major AI deal under his watch as AI Czar). Nvidia gets favorable regulatory treatment from the Trump administration. Timing it for Christmas Eve ensures minimal media scrutiny of these connections.</p>
</li>
<li>
<p><strong>Preempting other acquirers</strong>: Ross invented Google's TPU. If Google had acquired Groq and brought Ross back, they'd have the original TPU creator plus LPU IP. Amazon and Microsoft were also exploring alternatives to Nvidia GPUs. Better to pay $20B now than compete with Ross at Google/Amazon/Microsoft later.</p>
</li>
</ol>
<p><strong>Two speculative reasons worth considering:</strong></p>
<ol start="6">
<li>
<p><strong>Blocking Google/Amazon/Microsoft from partnering with Groq</strong>: Both are developing custom AI chips (Trainium, Maia). If either had hired Ross + team or licensed Groq's tech, Nvidia's inference dominance faces a real challenger. If Google had acquired Groq and brought Ross back, they'd have the original TPU inventor plus LPU IP.</p>
</li>
<li>
<p><strong>Chiplet integration for future products</strong>: Nvidia might integrate LPU as a chiplet alongside GPUs in Blackwell or future architectures. Having Ross's team makes that possible. You can't integrate IP you don't own, and you can't build it without the people who invented it.</p>
</li>
</ol>
<p>That's how business works when regulation hasn't caught up to structural innovation. Nvidia paid $6.9B for technology and $13.1B to solve everything else using a deal structure that traditional antitrust can't touch.</p>
<h2>The Saudi Arabia Problem</h2>
<p>In February 2025, Saudi Arabia committed $1.5 billion to expand Groq's Dammam data center. The publicly stated goal was supporting SDAIA's ALLaM, Saudi Arabia's Arabic large language model. The actual goal was Vision 2030: positioning the Kingdom as an AI superpower.</p>
<p>Groq built the region's largest inference cluster in eight days in December 2024. From that Dammam facility, GroqCloud serves "nearly four billion people regionally adjacent to the KSA." This isn't API access. This is critical AI infrastructure for a nation-state, funded by the Public Investment Fund, processing inference workloads at national scale.</p>
<p>According to Ross in the Series E announcement, Groq powers Humain's services including the Humain chat product and supported OpenAI's GPT-OSS model release in Saudi Arabia. Groq operates 13 facilities across the US, Canada, Europe, and the Middle East. Ross noted that capacity expanded more than 10% in the month before the funding announcement and all of that capacity was already in use. Customers were asking for more capacity than Groq could satisfy.</p>
<p>That creates a CFIUS (Committee on Foreign Investment in the United States) problem. A U.S. chip company, venture-backed by American investors, building sovereign AI capability for Saudi Arabia. If Nvidia had acquired GroqCloud outright, they would inherit those contracts and the regulatory scrutiny that comes with them. Foreign investment reviews, export control questions, congressional inquiries about why an American company is providing cutting-edge AI to a Middle Eastern monarchy.</p>
<p>By carving out GroqCloud, Nvidia gets the technology and the talent without the geopolitical mess. The Saudi contracts stay with Edwards and the independent GroqCloud entity. Clean separation. No CFIUS entanglement.</p>
<h2>Who Actually Gets Paid (And Who Gets Left Behind)</h2>
<p>The Financial Times reported that "despite the loss of much of its leadership team, Groq said it will continue to operate as an independent company." That's corporate speak for: executives and VCs are cashing out while regular employees watch the company they built get hollowed out.</p>
<p>Here's how the $20B probably breaks down (we'll never know the exact numbers since Groq is private and this isn't a traditional acquisition):</p>
<p><strong>Who definitely wins:</strong></p>
<p><strong>VCs (Chamath, BlackRock, Neuberger Berman, Deutsche Telekom, etc.)</strong>: They own equity in Groq Inc. Depending on how the deal is structured, they get paid based on their ownership percentage. Social Capital's ~10% stake (after dilution) is worth $1.6-2.4B. BlackRock, Neuberger Berman, and other Series E investors get their cut. They're protected regardless of structure.</p>
<p><strong>Executives joining Nvidia (Ross, Madra, senior leadership)</strong>: They get:</p>
<ul>
<li>Retention packages from Nvidia (likely massive given the $20B deal size)</li>
<li>Sign-on bonuses</li>
<li>Accelerated vesting of any unvested Groq equity</li>
<li>New Nvidia equity compensation packages</li>
<li>Their existing Groq equity gets paid out at the $20B valuation</li>
</ul>
<p>Jensen Huang's email to Nvidia staff (obtained by the FT) said they're "adding talented employees to our ranks." When you're talent important enough to be mentioned in a $20B deal, you're getting paid.</p>
<p><strong>Who might get paid (depending on deal structure):</strong></p>
<p><strong>Regular Groq employees with vested equity</strong>: This is where it gets murky. There are three possible scenarios:</p>
<p><strong>Scenario 1: The IP licensing fee goes to Groq Inc.</strong>
If the $20B (or a significant portion) is structured as a licensing fee paid to Groq Inc. for the IP rights, that money gets distributed to all shareholders based on ownership percentage. Employees with vested stock options or RSUs get their pro-rata share. This is the best case for employees.</p>
<p>Example: Engineer with 0.01% fully vested equity gets $2M ($20B × 0.01%). Not bad for an engineer who's been there since 2018-2020.</p>
<p><strong>Scenario 2: Most of the $20B goes to retention packages</strong>
If the deal is structured so that the bulk of the money goes to retention/hiring packages for Ross, Madra, and the senior team joining Nvidia, with a smaller licensing fee to Groq Inc., employees get less. Maybe the split is $15B retention, $5B licensing fee. Now that same engineer with 0.01% gets $500K instead of $2M.</p>
<p><strong>Scenario 3: The IP licensing is separate from talent acquisition</strong>
Nvidia pays Groq Inc. for the IP (say $5-7B, roughly the Sept 2024 valuation), and separately pays Ross + team retention packages directly. Regular employees get their share of the IP licensing fee only. That same engineer might get $500-700K.</p>
<p><strong>The critical question</strong>: Is the $20B figure the total cost to Nvidia (including retention packages), or is it just the IP licensing fee? If it's total cost and most goes to retention, regular employees get scammed.</p>
<p><strong>Who definitely gets done over:</strong></p>
<p><strong>Employees staying at GroqCloud</strong>: These are the people who:</p>
<ul>
<li>Weren't important enough to be hired by Nvidia</li>
<li>Have equity tied to GroqCloud's future value</li>
<li>Just watched their CEO, President, and entire engineering leadership leave</li>
<li>Are now working for a company with no IP rights, no technical leadership, and no future</li>
</ul>
<p>Their equity is worthless. GroqCloud will wind down over 12-18 months. They'll either get laid off or jump ship to wherever they can land. They built the LPU architecture, contributed to the compiler stack, supported the infrastructure, and got nothing while Chamath made $2B.</p>
<h2>The All-In Connection</h2>
<p>This gets messier when you look at who was involved. Chamath Palihapitiya, through Social Capital, led Groq's initial $10 million investment in 2017 at a $25 million pre-money valuation. Social Capital secured 28.57% of the company and a board seat for Chamath.</p>
<p>David Sacks, Chamath's co-host on the All-In podcast, became Trump's AI and Crypto Czar in late 2024. In July 2025, Sacks co-authored "America's AI Action Plan," a White House strategy document positioning AI as a matter of national security. The plan called for exporting "the full AI technology stack to all countries willing to join America's AI alliance" while preventing adversarial nations from building independent AI capabilities.</p>
<p>Two months later at the All-In Summit in September 2025, Tareq Amin (CEO of HUMAIN, Saudi Arabia's state-backed AI company) presented Groq as "the American AI stack in action." This was seven months after the $1.5B Saudi deal.</p>
<p>Sunny Madra, Groq's President and COO, was actively promoting the All-In narrative during this period. He appeared on the All-In podcast in March 2024 to provide a "Groq update" and joined Sacks on "This Week in Startups" in November 2023. When Anthropic raised AI safety regulation concerns in October 2025, Madra publicly sided with Sacks, suggesting "one company is causing chaos for the entire industry" and echoing Sacks's accusation that Anthropic was engaged in "regulatory capture."</p>
<p>So you have Sacks pushing an "America First" AI policy from the White House while Chamath's portfolio company (where Madra is President) is building AI infrastructure for Saudi Arabia. Then Groq gets presented at the All-In Summit as an example of American AI leadership. Three months later, announced on Christmas Eve when media coverage is minimal, Nvidia pays $20 billion to clean up the geopolitical contradiction.</p>
<p>Chamath walks away with $1.6B to $2.4B. Sacks gets a major AI deal under his watch. Nvidia gets favorable regulatory treatment and eliminates multiple problems. The timing ensures minimal scrutiny of these connections.</p>
<h2>Chamath's $2 Billion Win vs. His SPAC Graveyard</h2>
<p>After dilution from raising $1.7 billion across Series C, D, and E rounds, Social Capital's stake in Groq was probably 8-12% by the time of the Nvidia deal. At a $20 billion exit, that's $1.6 billion to $2.4 billion.</p>
<p><img src="https://ossa-ma.github.io/images/victorian_chamath.jpg" alt="Chamath after using you as exit liquidity and bankrolling it into a 200x win for himself">
<em>Chamath after using you as exit liquidity and bankrolling it into a 200x win for himself</em></p>
<p>Why do I hate Chamath?</p>
<p>Let's look at the sh he dumped on retail with his abysmal SPAC track record:</p>
<ul>
<li>IPOA (Virgin Galactic): -98.5% since inception</li>
<li>IPOB (Opendoor): -62.9% (was -95% before a brief spike)</li>
<li>IPOC (Clover Health): -74.4%</li>
<li>IPOE (SoFi): +46% (still underperformed S&amp;P 500's +86%)</li>
</ul>
<p>Chamath personally dumped $213 million of Virgin Galactic stock before it crashed, using PIPE structures that let him exit while retail investors stayed locked up. In October 2025, when launching a new SPAC, he posted a warning telling retail investors not to buy it: "these vehicles are not ideal for most retail investors."</p>
<p>The Groq bet was classic venture capital: concentrated bet on an exceptional founder (Jonathan Ross, the engineer who invented Google's TPU) building non-obvious technology. Social Capital's 2017 internal memo projected a "High" exit scenario of $3.2 billion. They landed within range despite dilution.</p>
<p>But retail investors never got access to deals like Groq. They got Virgin Galactic. LOL.</p>
<h2>To Conclude</h2>
<p>Nvidia paid $20 billion for a company valued at $6.9 billion three months earlier, structured the deal to avoid traditional M&amp;A oversight, killed the cloud business without inheriting Saudi contracts, and enriched the exact people (Chamath, Sacks) who spent the last year promoting "American AI leadership" while cutting deals with foreign governments. The employees who built the technology either got hired by Nvidia or have been utterly shafted.</p>
<p>This was fun to look into. If you have any questions or comments, <a href="mailto:ossamachaib.cs@gmail.com">shout me</a>.</p>
<hr>
<h2>Sources</h2>
<ul>
<li><a href="https://www.cnbc.com/2025/12/24/nvidia-buying-ai-chip-startup-groq-for-about-20-billion-biggest-deal.html">Nvidia Acquiring Groq for $20B</a></li>
<li><a href="https://techcrunch.com/2025/12/24/nvidia-acquires-ai-chip-challenger-groq-for-20b-report-says/">Nvidia-Groq Deal Structure</a></li>
<li><a href="https://www.bloomberg.com/news/articles/2024-09-03/ai-chip-startup-groq-raises-750-million-at-6-9-billion-value">Groq Series E: $750M at $6.9B Valuation</a></li>
<li><a href="https://www.techpowerup.com/319286/groq-lpu-ai-inference-chip-is-rivaling-major-players-like-nvidia-amd-and-intel">Groq LPU Architecture</a></li>
<li><a href="https://www.webpronews.com/the-deterministic-bet-how-groqs-lpu-is-rewriting-the-rules-of-ai-inference-speed/">Groq Deterministic Inference</a></li>
<li><a href="https://groq.com/blog/saudi-arabia-announces-1-5-billion-expansion-to-fuel-ai-powered-economy-with-groq">Saudi Arabia $1.5B Groq Investment</a></li>
<li><a href="https://www.datacenterdynamics.com/en/news/groq-secures-15bn-from-saudi-arabia-to-expand-ai-inference-infrastructure-in-the-region/">Groq Dammam Data Center</a></li>
<li><a href="https://fortune.com/2025/07/23/all-in-podcast-trump-ai-action-plan-david-sacks-jd-vance-jensen-huang/">America's AI Action Plan</a></li>
<li><a href="https://www.whitehouse.gov/articles/2025/07/white-house-unveils-americas-ai-action-plan/">White House AI Action Plan</a></li>
<li><a href="https://www.benzinga.com/markets/ipos/25/08/47213886/chamath-palihapitiya-returns-to-spacs-past-18-deals-investments-average-loss-of-14">Chamath SPAC Performance</a></li>
<li><a href="https://finance.yahoo.com/news/chamath-palihapitiya-crumbling-spac-empire-100748788.html">Chamath Virgin Galactic Stock Dump</a></li>
<li><a href="https://techcrunch.com/2025/10/01/chamath-warns-retail-investors-to-avoid-his-new-spac/">Chamath SPAC Warning</a></li>
<li><a href="https://groq.com/newsroom/meta-and-groq-collaborate-to-deliver-fast-inference-for-the-official-llama-api">Meta-Groq Partnership</a></li>
<li><a href="https://groq.com/newsroom/groq-powers-humain-one--real-time-ai-operating-system-for-enterprise">HUMAIN Groq Partnership</a></li>
<li><a href="https://podcasts.apple.com/us/podcast/e169-elon-sues-openai-apples-decline-tiktok-ban-bitcoin/id1502871393?i=1000648540648">Sunny Madra All-In Appearance</a></li>
<li><a href="https://techcrunch.com/2025/10/21/anthropic-ceo-claps-back-after-trump-officials-accuse-firm-of-ai-fear-mongering/">David Sacks Anthropic Criticism</a></li>
</ul></article></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Janet Jackson had the power to crash laptop computers (2022) (192 pts)]]></title>
            <link>https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994</link>
            <guid>46403291</guid>
            <pubDate>Sat, 27 Dec 2025 17:16:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994">https://devblogs.microsoft.com/oldnewthing/20220816-00/?p=106994</a>, See on <a href="https://news.ycombinator.com/item?id=46403291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="single-wrapper">
    
    <article data-clarity-region="article" id="post-106994">
        <div data-bi-area="body_article" data-bi-id="post_page_body_article">
            <p>A colleague of mine shared a story from Windows XP product support. A major computer manufacturer discovered that playing the music video for Janet Jackson’s “<a href="https://en.wikipedia.org/wiki/Rhythm_Nation">Rhythm Nation</a>” would crash certain models of laptops. I would not have wanted to be in the laboratory that they must have set up to investigate this problem. Not an artistic judgement.</p>
<p>One discovery during the investigation is that playing the music video also crashed some of their competitors’ laptops.</p>
<p>And then they discovered something extremely weird: Playing the music video on one laptop caused a laptop sitting nearby to crash, even though that other laptop wasn’t playing the video!</p>
<p>What’s going on?</p>
<p>It turns out that the song contained one of the natural resonant frequencies for the model of 5400 rpm laptop hard drives that they and other manufacturers used.</p>
<p>The manufacturer worked around the problem by adding a custom filter in the audio pipeline that detected and removed the offending frequencies during audio playback.</p>
<p>And I’m sure they put a digital version of a “Do not remove” sticker on that audio filter. (Though I’m worried that in the many years since the workaround was added, nobody remembers why it’s there. Hopefully, their laptops are not still carrying this audio filter to protect against damage to a model of hard drive they are no longer using.)</p>
<p>And of course, no story about natural resonant frequencies can pass without a reference to <a href="https://www.history.com/this-day-in-history/tacoma-narrows-bridge-collapses"> the collapse of the Tacoma Narrows Bridge in 1940</a>.¹</p>
<p><b>Related</b>: <a href="https://www.youtube.com/watch?v=tDacjrSCeq4"> Shouting in the Datacenter</a>.</p>
<p><b>Bonus chatter</b>: <a href="https://twitter.com/WindowsDocs/status/1558114944738103297"> Video version of this story</a> and a <a href="https://twitter.com/WindowsDocs/status/1558115433449852929"> Twitter poll</a>.</p>
<p>Also, <a href="https://twitter.com/osterman">Larry Osterman</a> had a similar experience with <a href="https://twitter.com/osterman/status/1558676353196494850"> a specific game that crashed a prototype PC</a>.</p>
<p><b>Follow-up</b>: <a href="https://devblogs.microsoft.com/oldnewthing/20220920-00/?p=107201"> Janet Jackson had the power to crash laptop computers, follow-up</a>.</p>
<p>¹ <b>Follow-up 2</b>: Yes, I know that the Tacoma Narrows Bridge collapse was not the result of resonance, but I felt I had to drop the reference to forestall the “You forgot to mention the Tacoma Narrows Bridge!” comments.</p>
        </div><!-- .entry-content -->

        <!-- AI Disclaimer -->
            </article>
    
</div><div><!-- Author section -->
            <h2>Author</h2>
            <div><div><p><img src="https://devblogs.microsoft.com/oldnewthing/wp-content/uploads/sites/38/2019/02/RaymondChen_5in-150x150.jpg" alt="Raymond Chen"></p></div><p>Raymond has been involved in the evolution of Windows for more than 30 years. In 2003, he began a Web site known as The Old New Thing which has grown in popularity far beyond his wildest imagination, a development which still gives him the heebie-jeebies. The Web site spawned a book, coincidentally also titled The Old New Thing (Addison Wesley 2007). He occasionally appears on the Windows Dev Docs Twitter account to tell stories which convey no useful information.</p></div>        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gpg.fail (184 pts)]]></title>
            <link>https://gpg.fail</link>
            <guid>46403200</guid>
            <pubDate>Sat, 27 Dec 2025 17:05:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gpg.fail">https://gpg.fail</a>, See on <a href="https://news.ycombinator.com/item?id=46403200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><!----> <p><span>"in the hurry of leaving i forgot the sites src at home, sorry, had to rewrite the whole thing. expect a nicer site by tomorrow. im patching as we speak."</span> <br> <span>-  crackticker (&lt;- to blame)</span></p> <ol><!--[--><li><a href="https://gpg.fail/detached">Multiple Plaintext Attack on Detached PGP Signatures in GnuPG</a></li><li><a href="https://gpg.fail/filename">GnuPG Accepts Path Separators and Path Traversals in Literal Data "Filename" Field</a></li><li><a href="https://gpg.fail/formfeed">Cleartext Signature Plaintext Truncated for Hash Calculation</a></li><li><a href="https://gpg.fail/malleability">Encrypted message malleability checks are incorrectly enforced causing plaintext recovery attacks</a></li><li><a href="https://gpg.fail/memcpy">Memory Corruption in ASCII-Armor Parsing</a></li><li><a href="https://gpg.fail/minisign">Trusted comment injection (minisign)</a></li><li><a href="https://gpg.fail/notdash">Cleartext Signature Forgery in the NotDashEscaped header implementation in GnuPG</a></li><li><a href="https://gpg.fail/notsoclear">OpenPGP Cleartext Signature Framework Susceptible to Format Confusion</a></li><li><a href="https://gpg.fail/noverify">GnuPG Output Fails To Distinguish Signature Verification Success From Message Content</a></li><li><a href="https://gpg.fail/nullbyte">Cleartext Signature Forgery in GnuPG</a></li><li><a href="https://gpg.fail/polyglot">Radix64 Line-Truncation Enabling Polyglot Attacks</a></li><li><a href="https://gpg.fail/sha1">GnuPG may downgrade digest algorithm to SHA1 during key signature checking</a></li><li><a href="https://gpg.fail/trust">GnuPG Trust Packet Parsing Enables Adding Arbitrary Subkeys</a></li><li><a href="https://gpg.fail/trustcomment">Trusted comment Injection (minisign)</a></li><!--]--></ol> <h2>VOD:</h2> <!----><!----></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia just paid $20B for a company that missed its revenue target by 75% (183 pts)]]></title>
            <link>https://blog.drjoshcsimmons.com/p/nvidia-just-paid-20-billion-for-a</link>
            <guid>46403041</guid>
            <pubDate>Sat, 27 Dec 2025 16:47:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.drjoshcsimmons.com/p/nvidia-just-paid-20-billion-for-a">https://blog.drjoshcsimmons.com/p/nvidia-just-paid-20-billion-for-a</a>, See on <a href="https://news.ycombinator.com/item?id=46403041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>If you’ve heard “Grok” thrown around lately, you’re probably thinking of Elon’s chatbot from xAI. We’re not talking about that one. That model isn’t particularly good, but its whole value prop is being politically incorrect so you can get it to say edgy things.</p><p>The company Nvidia bought is Groq (with a Q). Totally different beast.</p><p><span>If you’ve used any high quality LLM, you’ve noticed it takes </span><em>a while to generate a response.</em><span> Especially for something rapid fire like a conversation, you want high quality AND speed. But speed is often what gets sacrificed. There’s always that “thinking... gathering my notes... taking some time to form the best response” delay.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!7ppM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!7ppM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 424w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 848w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 1272w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!7ppM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png" width="1456" height="777" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:777,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3954485,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://blog.drjoshcsimmons.com/i/182706966?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!7ppM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 424w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 848w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 1272w, https://substackcdn.com/image/fetch/$s_!7ppM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a099433-4de1-45a1-9804-cfcede6b28da_5064x2704.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>My default Zellij layout so I can parallelize Claude Code tasks.</figcaption></figure></div><p><span>Groq specialized in hardware and software that makes this way faster. They created a new type of chip called an LPU (Language Processing Unit). It’s based on an ASIC, an application specific integrated circuit. If that’s confusing, don’t worry about it. It’s just </span><strong>a processor that does a specific type of task really well.</strong></p><p>So imagine you’re talking to Gemini and it takes a couple seconds to respond. Now imagine it responded instantly, like 10 or 100 times faster. That’s the problem Groq was solving.</p><p>To go one level deeper on LPUs versus GPUs (the processors most LLMs run on, typically Nvidia cards): those GPU calculations have to access a lot of things in memory. Nvidia’s chips depend on HBM, high bandwidth memory. But LPUs use something called SRAM that’s much faster to reference.</p><p>Think about it like this. Your wife has a grocery list for you. You go to the store but forget the list. Every time you’re in an aisle, you have to call her on the phone: “Hey, do I need anything from the bread aisle?” Get the bread. Put the phone down. Go to the next aisle. “Hey, do I need anything from canned goods?” And so on through produce, meat, pick up the beer, check out, get home. Very inefficient.</p><p>Groq’s approach is like you just took the list to the store. Get to a new aisle, look at the list. Next aisle, look at the list. Much faster than a phone call.</p><p>That’s the key difference. Nvidia GPUs are phoning out every time they hit a new aisle. Groq’s LPUs mean the shopper has the list in their pocket.</p><p>Groq’s main offering is GroqCloud. An engineer like me isn’t going to go out and buy an LPU (I don’t even know if they’re commercially available). What I’m going to do is, if I need lightning fast response in an application I’m building, I’ll use GroqCloud. That inference happens at an extremely fast rate, running on LPUs in a data center somewhere.</p><p>Their value prop is: fast, cheap, low energy.</p><p>Where they’ve been falling short is they mostly use open source models. Llama, Mistral, GPT-OSS (OpenAI’s open source offering). These are decent models, but nowhere near the quality of something like Anthropic’s Opus 4.5 or Gemini 3 Pro.</p><p>Groq positioned themselves for hardcore use cases where milliseconds matter. One of their big industry fits is real time data analysis for Formula 1. When your driver’s out there on the track, you don’t have time to send a query to Gemini and wait 20 to 30 seconds to figure out if you should pit this guy for new tires this lap. You want something like Groq which does pretty good analysis, really really really fast.</p><p>This is insider baseball you’re going to miss from mainstream news headlines. The media is paying attention to Grok (Elon’s chatbot), not Groq (the chip company). This isn’t a conspiracy, it’s just that the only people aware of Groq are software developers and tech nuts like me.</p><p>This is a canary in the coal mine for worse things to come in 2026.</p><p>About a year ago, Groq announced a $1.5 billion infrastructure investment deal with Saudi Arabia. They also secured a $750 million Series D funding round. These are crazy multiples even for a software company that’s somewhat leveraged in hardware. Bubble level projections.</p><p>To visualize $1.5 billion: if you cashed that check out in $100 bills and stacked them one on top of another, it would reach a five story building. For ordinary plebeians like us, at the average US salary of around $75K, you’d need to work 20,000 years to earn that.</p><p>At that time, the company was valued at $2 billion. Hello bubble.</p><p><span>Then in maybe one of the best rug pulls of all time, in July they quietly changed their revenue projections to $500 million.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-182706966" href="https://blog.drjoshcsimmons.com/p/nvidia-just-paid-20-billion-for-a#footnote-1-182706966" target="_self" rel="">1</a></span><span> A 75% cut in four months. I’ve never seen anything like that since the 2008 financial crisis.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://www.youtube.com/watch?v=2po-s2yOCcg&quot;,&quot;text&quot;:&quot;Prefer to Watch the Video&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.youtube.com/watch?v=2po-s2yOCcg" rel=""><span>Prefer to Watch the Video</span></a></p><p>This was a company valued at $2 billion, enough that the government of Saudi Arabia was investing at that valuation. Then they took a 75% haircut four months later without anything major happening.</p><p>If it can happen to Groq, who else can it happen to? Nvidia? Intel?</p><p>The rumors started flying on Christmas Eve. Confirmed the 26th: Nvidia will be buying Groq, their key product line, and key personnel including the CEO, for $20 billion.</p><p><strong>Let’s walk through that again:</strong></p><ul><li><p><strong>February: $2 billion valuation</strong></p></li><li><p><strong>July: Down to $500 million</strong></p></li><li><p><strong>December: Nvidia buys them for $20 billion after they took a 75% haircut</strong></p></li></ul><p>What is going on? This is a bubble.</p><p>The only explanation is this is a fear purchase. Groq was promising faster, cheaper, more efficient, less electricity use for their chips. But they couldn’t scale fast enough to compete with the vendor lock in and buddy system Nvidia has going.</p><p>Nvidia’s buying them with their insanely inflated war chest. They don’t want a chunk taken out of their market share. They can’t afford to take that chance. So it’s like they’re just saying: “Shut up, take the $20 billion, walk away from this project.”</p><p>This is a sign that in order to succeed, Nvidia needs a monopoly on the market. Otherwise they would not pay ten times the company’s valuation that was then decreased by 75%. This is a desperate move to consolidate the market into “you have to go with us.”</p><p>Saudi Arabia didn’t keep that $1.5 billion sitting around. They redirected it to Nvidia and AMD instead. Nvidia still gets paid ofc.</p><p><span>Smaller competitors like Cerebras and Inflection, doing things in Groq’s space or exploring different architectures for AI inference, are canceling IPOs, dropping like flies, seeking emergency funding. The chatter I’m hearing from VCs and friends in that world? </span><strong>Ain’t nobody buying it right now.</strong></p><p>Google made their own chip. Microsoft and Amazon are racing to make competition chips that run on less electricity, are more efficient, faster. But no matter what anybody does, the market is consolidating around the Nvidia monopoly for AI hardware. Engineers like me and architects at large enterprises are trying to escape this. Once they consolidate enough of the market, they can set their price for chips and usage. If you don’t own them, you go through Oracle or some cloud computing service, and they can charge whatever they want because there will be no competitors. Even a competitor having a rough time but getting some traction? They just buy them out for $20 billion because with this monopoly going, that’s pocket change.</p><p>$20 billion is a rounding error to Nvidia.</p><p><strong>The AI infrastructure boom ran on one large errant assumption: that power is cheap and plentiful. </strong><span>That assumption was very, very wrong.</span></p><p><span>We’ve seen parts of the power grid fail, go unmaintained. </span><a href="https://www.palladiummag.com/2023/06/01/complex-systems-wont-survive-the-competence-crisis/" rel="">There’s a great article on Palladium called “The Competency Crisis” that explains some of what’s going on in the US right now.</a><span> Electricity is now the bottleneck. It’s the constraint. Too expensive or you can’t get enough of it. Who’s paying these costs? The tech companies aren’t. Trump is meeting with Jensen Huang, with Sam Altman. He hasn’t been over my place lately. He hasn’t invited you to the White House to talk about how you can’t afford groceries and eggs cost three times what they did a few years ago.</span></p><p>No.</p><p><strong>You and I are going to pay to subsidize the electricity constraints.</strong></p><p>US data centers are using about 4% of all electricity right now. If growth continues at the same rate, in ten years they’ll use 9%. Almost one tenth of all electricity generated in the US.</p><p>I’m not much of an environmentalist. But even someone like me, pretty jaded to some amount of waste because of industrialization, something like this actually makes my stomach turn. In places with lots of data centers, AI heavy regions, they’re experiencing about a 250% increase in energy costs compared to just five years ago. That’s huge. Even compared to grocery costs, which are out of control. At least with food you have alternatives. Red meat too expensive? Buy chicken. Chicken too expensive? Buy eggs. But electricity? You have to run electricity. It’s a public utility. You can’t just “use a lot less” when your bill goes up 2.5x.</p><p>Let me walk you through what happens. Some business development guy from Oracle wants to build a new data center in Rural America. A year before it’s even built, they meet with city officials, maybe the governor. They grease the right people. Get legislation passed with the utility companies that says they’ll pay a preferential rate for electricity because they’re going to use a shit ton.</p><p>They’re Oracle or Nvidia, they’re good for it. They pay five years upfront. Then the utility decides what to do with the rest of the electricity. The grid is strained. They want everyone else to use less. They can’t just tell them to use less, so they keep raising the price until naturally you just can’t afford to use more.</p><p>You turn the lights off. Turn the TV off. Get those LED bulbs that disrupt your sleep. Do everything to keep electricity costs down. But you and I are left holding the bag. The data center folks aren’t paying for that, they paid upfront at a preferential rate.</p><p>Senate Democrats are allegedly investigating this. I’ll believe it when I see it. These tech companies have their hooks so far into influential politicians. It’s a vote grab. I’d be happy to be wrong about that, but I know I’m not going to be.</p><p>I talked about this in my last piece on the AI bubble. This computer communism that’s going on, pricing you out of personal computing, keeping it all in the family of these tech companies. But with the Groq deal confirmed, it’s gone one step further. Nvidia is not just selling chips anymore. They are lending money to customers who buy the chips. They are artificially inflating demand.</p><p>It’s like if I run a small grocery store and I need to show good signal to investors that I’m bringing cash in the door. So I go downtown during farmer’s market and give everybody a $20 voucher to use at my store. I take a wholesale hit when they come in and buy stuff. But what I can show is: “Hey, people are coming in and spending money. Look at the revenue. Give me more venture capital money.”</p><p>This can’t work infinitely, for the same reason any perpetual motion machine can’t work.</p><p><span>Back in September, Nvidia announced a $100 billion investment in OpenAI. Coming through in $10 billion tranches over time. And it’s not equity, it’s lease agreements for Nvidia’s chips. Literally paying them to pay themselves.There’s probably some tax shenanigans going on there since they’re typically offering $xxx in </span><em>leases of their chips</em><span> to the company they’re “lending” to. Assuming they do this in part because the depreciation on the physical asset (the chips) can be written off on their taxes somehow. They’re essentially incentivizing another person to use them with money. Even OpenAI’s CFO has admitted, quote: “Most of the money will go back to Nvidia.”</span></p><p>They’re playing both sides. In the OpenAI case, they’re financing software that uses their chips. But they’re also getting their hooks into data centers.</p><p>CoreWeave: they have something like 7% share in the company. Worth $3 billion. That’s funded by GPU revenue from CoreWeave.</p><p>Lambda: another data center operator. That’s a $1.3 billion spending deal. Renting Nvidia’s own chips back from them.</p><p>They’ve pledged to invest £2 billion across British startups, which of course are going to go back to Nvidia chips one way or another.</p><p>In 2024, they invested about $1 billion across startups and saw a $24 billion return in chip spend. They 24x’d their billion dollar investment in one year. Nvidia has more power than the Fed right now. More power than the president over the economy. They have their hand on the knob of the economy. They can choose how fast or slow they want it to go. If the Nvidia cash machine stops printing, if they stop funding startups, data centers, hardware companies, software companies, that whole part of the economy slows way down and maybe crashes if investors get spooked.</p><p>I’m waiting for somebody to blow the whistle on this. I’m not a finance guy, so it’s strange I’m even talking about it. But their entire success story for the next couple years hinges on their $100 billion investment in OpenAI, which they’re expecting to bring back about $300 billion in chip purchases.</p><p><span>It’s vendor financing. It’s sweetening the pot on your own deals. </span><strong>I cannot believe more people are not talking about this.</strong></p><p>OpenAI, the leader of this space, the company whose CEO Sam Altman is invited to the White House numerous times, probably has a direct line to Trump, a lot of the economy hinges on this guy’s strategy, opinions, and public statements.</p><p><strong>And he runs a company that is not profitable. </strong><span>Actually insane if you think about it. All he’s done with that company, from an economics point of view, is rack up debt. Spent more than he’s earned.</span></p><p>By that metric, I’m richer than Sam Altman. Not in net worth. But if I consider myself a business and the fact that I bring in any salary at all, even a dollar a year, that would make me earn more than Sam Altman, who has only lost money. In the next few years, they’re expected to burn something like $75 billion a year. Just set that money on fire. I have a credit card with no preset spending limit, but I assume if I run up a $75 billion charge it’s going to get denied. By their projections, they think they’ll become profitable around 2029/2030, and they need $200 billion in annual revenue to offset their debts and losses.</p><p>To visualize $200 billion: if you cashed that out in $100 bills and stacked them, it would reach halfway to the International Space Station. That’s how much they’d have to make every single year to just be profitable. Not be a massively successful company. Just to not spend more than they earn.</p><ul><li><p>2024: Spent $5 billion, earned $3.7 billion. Spending $1.35 for every dollar earned.</p></li><li><p>2025: Set $8 billion on fire (after profits).</p></li><li><p>2028 (projected): Will lose $74 billion that year. Just lose it.</p></li><li><p>Cumulative losses through 2029: $143 billion.</p></li></ul><p>They’d need to make $200 billion in a year to offset that. Are they including interest? I have no idea how these things work, but in simple terms: they’re spending a lot more money than they make.</p><p>Groq was kind of a one off because Nvidia panic bought the competition. But they also need to figure out how to get prices down or they can’t keep this money machine moving.</p><p>Groq is probably one of the last companies that caught a good lifeboat off a sinking ship.</p><p>What we’re going to see this year: it’ll start small, but get major. A company first, then multiple larger ones, that had a 2025 valuation, will go to raise. They’ll do a 409A evaluation. A bunch of smart analysts will say what it’s worth to investors. And you’re going to see the valuation drop. They won’t be able to raise money.</p><p>Then the shit is really going to start to hit the fan.</p><p>The dominoes will start falling. That’s probably what kicks off the actual pop, and it’s imminent. Any day now.</p><p>Part of the big gamble they’ve sold investors is: we’ve got to replace you. The worker. That’s why we need to spend so much money, go into debt. It’ll all be worth it because then we won’t have to pay people to do these jobs anymore.</p><p>We’re going to continue to see massive labor displacement. To a degree this is a shell game, an investor illusion. What these larger enterprise companies are hoping to do: cut a lot of folks, have big layoffs, say it’s because AI is replacing the jobs.</p><p>In some cases they’re right. Data entry, for example. I don’t mean to be mean if you do data entry for a living, but there are models very good at that now. You need someone to manage the work, spot check it. But it’s kind of a job AI can do.</p><p>Like how grocery stores have automatic checkout lines now with one person monitoring six or eight of them. So some of that’s real. A lot of it isn’t though.</p><p>They cut a bunch of American workers under the guise that AI’s replacing workers. A lot of these megacorp execs are actually convinced of it. Americans are expensive, especially tech workers.</p><p>Then they see: damn, maybe we could have cut some, but not as many. We got greedy. Now our services are failing in production. AWS in Northern Virginia is flaky, going down again. That just happened, by the way, direct result of these layoffs.</p><p>So instead they think, “We’ll look globally! Spin up a campus in Hyderabad!” Pay them way less. The cost of living is less there, they expect less. Bring them over on H-1B when needed. I’ve written about the H-1B program. This is nothing against the individuals on that program. I’ve worked with very talented H-1Bs, and some very inferior ones, just like American citizens.</p><p>But the corporate sleight of hand is something like this, we can get those H-1B visas, and they’re not going to ask for pesky stuff like Sunday off for church. We can put them on call 24/7 and they can’t say no because if they do, we kick them back to their country. Same thing that’s happened with migrant labor in farming over the past century. Even if it doesn’t look like it on the surface, corporations know that they can pay H-1B employees less than American citizens. If a US citizen and H-1B recipient both make $120k but the H-1B works double the hours because they have no room to push back and are under threat of being sent home, they are making 50% less than the American per-hour.</p><p>Amazon/AWS: 14,000 laid off.</p><p>Microsoft: 15,000 total just in 2025.</p><p>Salesforce: My favorite one. 4,000 customer support roles cut. And the CEO is gloating about it in interviews. So great that he can replace workers!</p><p><a href="https://timesofindia.indiatimes.com/technology/tech-news/after-laying-off-4000-employees-and-automating-with-ai-agents-salesforce-executives-admit-we-were-more-confident-about-/articleshow/126121875.cms" rel="">Now Salesforce is admitting that they fucked up. They cut too many people. </a></p><p><span>I’ve been on the other side of this when I was an engineering director at a Fortune 500 company. They were </span><em>neurotic</em><span> about tracking AI use. Spending an exorbitant amount of money on shitty AI tools. Like tools from a year ago. GPT-4o in the GPT-5 era, in the Anthropic dominance era. More or less useless.</span></p><p>Not only would they monitor all usage across employees, specifically who’s using it how much, they could see every single message being sent to the AI. So theoretically you could check somebody’s queries and do a performance evaluation based on where you perceive them to be.</p><p>Pretty creepy.</p><p>They’re using yesterday’s tools because of regulation and compliance, blowing an absurd amount of money. The CEO just sees a lot going out the door: “I thought these were supposed to save money, what’s going on here?” So his lieutenants have to get a grip on it, monitor everything. Even at Amazon this is being included in performance reviews, how much they’re using AI.</p><p>That’s why if you’re a software developer wondering why your boss is on you about using AI tools. They’re probably getting pressure from their bosses. I want my engineers to be as productive as possible. I think AI is probably part of that tool belt for everyone at this point. But is tracking it really the best way? I’m going to gauge performance on metrics, how you interacted with the team, what you shipped. It puts the cart before the horse to say you’re paid by how much you use these AI tools. If I’m a developer, I’m just spinning up a script to send lorem ipsum text 24 hours a day, get maximum ratings because I used GPT-3.5 the most.</p><p>MIT did a study in summer 2025. They’re saying 95% of companies report zero measurable ROI on AI products.</p><p>Actually not that crazy if you consider that a lot of them did layoffs and subbed in AI. That’s just going according to plan in my estimation.</p><p>They estimate about $30 to $40 billion has been spent on enterprise AI. That’s the money your JPMorgan Chase is spending for their engineers to use Claude Code or whatever dated tool they have access to.</p><p>The market heard that signal. When that study came out:</p><ul><li><p>Nvidia crashed 3.5%</p></li><li><p>Palantir down 10%</p></li><li><p>Nasdaq in general down 1.2%</p></li></ul><p>That last one isn’t encouraging for an AI bubble pop because it indicates this is a big part of the economy.</p><p>January through February: Maybe down valuations, just a very flat market without much growth.</p><p>Q1 to Q2: We’ll start to see a couple businesses, or maybe one major one at first like a domino starting to fall, not able to raise capital at their 2025 valuation. We’ll see valuations go down. VCs will be like: “Not touching it, not giving them more money, cutting our losses.”</p><p>Then timing a little more indeterminate but these things will happen quickly in succession:</p><ul><li><p>Credit markets start to tighten</p></li><li><p>Debt refinancing pressure builds up in the system</p></li><li><p>Nvidia revises its revenue guidance to something at least vaguely linked to reality</p></li></ul><p>And that’s when the big reckoning begins.</p><p>I want to be clear. AI is not going anywhere. It’s going to continue being a mainstream part of the world. I like a lot of these tools, I think they’re very helpful.</p><p>But the talking heads have really promised us the world. It’s clear the technology cannot deliver above and beyond what it’s doing now.</p><p>We’ve seen progress of these models slow at an exponential rate of slowing over the past few releases. Each release is better, but the gap between the current release and the last release is much smaller than it used to be.</p><p>Because of that, a lot of these AI companies are going to survive, but their valuations are going to get giga slashed.</p><p>Valuations of $500 billion for OpenAI, $42 billion for Anthropic are unsustainable. We’re going to actually see them become unsustainable in 2026 as they’re eventually cut. Smaller companies will face those slashes first. But it’s coming for the major AI labs as well.</p><p>This is good news, honestly. This AI hype has really turned tech into something different these days, different than it used to be. While I don’t feel AI is going anywhere, I do feel we’ll get a little more back to normal once this bubble pops and resolves.</p><p><em>What do you think? Drop a comment below. Subscribe if you found this interesting.</em></p></div></article></div><div><div id="discussion"><h4>Discussion about this post</h4></div><div><h3>Ready for more?</h3></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Floor796 (344 pts)]]></title>
            <link>https://floor796.com/</link>
            <guid>46401612</guid>
            <pubDate>Sat, 27 Dec 2025 13:13:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://floor796.com/">https://floor796.com/</a>, See on <a href="https://news.ycombinator.com/item?id=46401612">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Apple releases open-source model that instantly turns 2D photos into 3D views (345 pts)]]></title>
            <link>https://github.com/apple/ml-sharp</link>
            <guid>46401539</guid>
            <pubDate>Sat, 27 Dec 2025 12:58:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/apple/ml-sharp">https://github.com/apple/ml-sharp</a>, See on <a href="https://news.ycombinator.com/item?id=46401539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Sharp Monocular View Synthesis in Less Than a Second</h2><a id="user-content-sharp-monocular-view-synthesis-in-less-than-a-second" aria-label="Permalink: Sharp Monocular View Synthesis in Less Than a Second" href="#sharp-monocular-view-synthesis-in-less-than-a-second"></a></p>
<p dir="auto"><a href="https://apple.github.io/ml-sharp/" rel="nofollow"><img src="https://camo.githubusercontent.com/cafdcb5612b1ab28528d47af9245604f8f7b0792562c7c5151fff90340a3d6cc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50726f6a6563742d506167652d677265656e" alt="Project Page" data-canonical-src="https://img.shields.io/badge/Project-Page-green"></a>
<a href="https://arxiv.org/abs/2512.10685" rel="nofollow"><img src="https://camo.githubusercontent.com/dba21084a03f7471ad5ab1cbe4b2eeb9c6c4333dde6dae06cd437bc9b9163cf7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f61725869762d323531322e31303638352d6233316231622e737667" alt="arXiv" data-canonical-src="https://img.shields.io/badge/arXiv-2512.10685-b31b1b.svg"></a></p>
<p dir="auto">This software project accompanies the research paper: <em>Sharp Monocular View Synthesis in Less Than a Second</em>
by <em>Lars Mescheder, Wei Dong, Shiwei Li, Xuyang Bai, Marcel Santos, Peiyun Hu, Bruno Lecouat, Mingmin Zhen, Amaël Delaunoy,
Tian Fang, Yanghai Tsin, Stephan Richter and Vladlen Koltun</em>.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/ml-sharp/blob/main/data/teaser.jpg"><img src="https://github.com/apple/ml-sharp/raw/main/data/teaser.jpg" alt=""></a></p>
<p dir="auto">We present SHARP, an approach to photorealistic view synthesis from a single image. Given a single photograph, SHARP regresses the parameters of a 3D Gaussian representation of the depicted scene. This is done in less than a second on a standard GPU via a single feedforward pass through a neural network. The 3D Gaussian representation produced by SHARP can then be rendered in real time, yielding high-resolution photorealistic images for nearby views. The representation is metric, with absolute scale, supporting metric camera movements. Experimental results demonstrate that SHARP delivers robust zero-shot generalization across datasets. It sets a new state of the art on multiple datasets, reducing LPIPS by 25–34% and DISTS by 21–43% versus the best prior model, while lowering the synthesis time by three orders of magnitude.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">We recommend to first create a python environment:</p>
<div data-snippet-clipboard-copy-content="conda create -n sharp python=3.13"><pre><code>conda create -n sharp python=3.13
</code></pre></div>
<p dir="auto">Afterwards, you can install the project using</p>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt"><pre><code>pip install -r requirements.txt
</code></pre></div>
<p dir="auto">To test the installation, run</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Using the CLI</h2><a id="user-content-using-the-cli" aria-label="Permalink: Using the CLI" href="#using-the-cli"></a></p>
<p dir="auto">To run prediction:</p>
<div data-snippet-clipboard-copy-content="sharp predict -i /path/to/input/images -o /path/to/output/gaussians"><pre><code>sharp predict -i /path/to/input/images -o /path/to/output/gaussians
</code></pre></div>
<p dir="auto">The model checkpoint will be downloaded automatically on first run and cached locally at <code>~/.cache/torch/hub/checkpoints/</code>.</p>
<p dir="auto">Alternatively, you can download the model directly:</p>
<div data-snippet-clipboard-copy-content="wget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt"><pre><code>wget https://ml-site.cdn-apple.com/models/sharp/sharp_2572gikvuh.pt
</code></pre></div>
<p dir="auto">To use a manually downloaded checkpoint, specify it with the <code>-c</code> flag:</p>
<div data-snippet-clipboard-copy-content="sharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt"><pre><code>sharp predict -i /path/to/input/images -o /path/to/output/gaussians -c sharp_2572gikvuh.pt
</code></pre></div>
<p dir="auto">The results will be 3D gaussian splats (3DGS) in the output folder. The 3DGS <code>.ply</code> files are compatible to various public 3DGS renderers. We follow the OpenCV coordinate convention (x right, y down, z forward). The 3DGS scene center is roughly at (0, 0, +z). When dealing with 3rdparty renderers, please scale and rotate to re-center the scene accordingly.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rendering trajectories (CUDA GPU only)</h3><a id="user-content-rendering-trajectories-cuda-gpu-only" aria-label="Permalink: Rendering trajectories (CUDA GPU only)" href="#rendering-trajectories-cuda-gpu-only"></a></p>
<p dir="auto">Additionally you can render videos with a camera trajectory. While the gaussians prediction works for all CPU, CUDA, and MPS, rendering videos via the <code>--render</code> option currently requires a CUDA GPU. The gsplat renderer takes a while to initialize at the first launch.</p>
<div data-snippet-clipboard-copy-content="sharp predict -i /path/to/input/images -o /path/to/output/gaussians --render

# Or from the intermediate gaussians:
sharp render -i /path/to/output/gaussians -o /path/to/output/renderings"><pre><code>sharp predict -i /path/to/input/images -o /path/to/output/gaussians --render

# Or from the intermediate gaussians:
sharp render -i /path/to/output/gaussians -o /path/to/output/renderings
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Evaluation</h2><a id="user-content-evaluation" aria-label="Permalink: Evaluation" href="#evaluation"></a></p>
<p dir="auto">Please refer to the paper for both quantitative and qualitative evaluations.
Additionally, please check out this <a href="https://apple.github.io/ml-sharp/" rel="nofollow">qualitative examples page</a> containing several video comparisons against related work.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you find our work useful, please cite the following paper:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{Sharp2025:arxiv,
  title      = {Sharp Monocular View Synthesis in Less Than a Second},
  author     = {Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\&quot;{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun},
  journal    = {arXiv preprint arXiv:2512.10685},
  year       = {2025},
  url        = {https://arxiv.org/abs/2512.10685},
}"><pre><span>@inproceedings</span>{<span>Sharp2025:arxiv</span>,
  <span>title</span>      = <span><span>{</span>Sharp Monocular View Synthesis in Less Than a Second<span>}</span></span>,
  <span>author</span>     = <span><span>{</span>Lars Mescheder and Wei Dong and Shiwei Li and Xuyang Bai and Marcel Santos and Peiyun Hu and Bruno Lecouat and Mingmin Zhen and Ama\"{e}l Delaunoy and Tian Fang and Yanghai Tsin and Stephan R. Richter and Vladlen Koltun<span>}</span></span>,
  <span>journal</span>    = <span><span>{</span>arXiv preprint arXiv:2512.10685<span>}</span></span>,
  <span>year</span>       = <span><span>{</span>2025<span>}</span></span>,
  <span>url</span>        = <span><span>{</span>https://arxiv.org/abs/2512.10685<span>}</span></span>,
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<p dir="auto">Our codebase is built using multiple opensource contributions, please see <a href="https://github.com/apple/ml-sharp/blob/main/ACKNOWLEDGEMENTS">ACKNOWLEDGEMENTS</a> for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Please check out the repository <a href="https://github.com/apple/ml-sharp/blob/main/LICENSE">LICENSE</a> before using the provided code and
<a href="https://github.com/apple/ml-sharp/blob/main/LICENSE_MODEL">LICENSE_MODEL</a> for the released models.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OrangePi 6 Plus Review (108 pts)]]></title>
            <link>https://boilingsteam.com/orange-pi-6-plus-review/</link>
            <guid>46401499</guid>
            <pubDate>Sat, 27 Dec 2025 12:51:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boilingsteam.com/orange-pi-6-plus-review/">https://boilingsteam.com/orange-pi-6-plus-review/</a>, See on <a href="https://news.ycombinator.com/item?id=46401499">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <p>So after our previous reviews (that started <a href="https://boilingsteam.com/orange-pi-rv2-new-risc-v-board-review/">mainly around RISC-V</a> since we are really interested in this new architecture) of SBC, we continue to review what’s available these days in the world of small, versatile computers. Today this is going to be about the <em>OrangePi 6 Plus</em>, following our previous review of the <a href="https://boilingsteam.com/orange-pi-5-ultra-review/">OrangePi 5 ultra board</a>.</p>
<p>This is NOT a super small, credit card format SBC. We are talking about something that’s definitely larger, and that comes directly with an integrated heatsink.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/orangepi6plus_board.avif" alt="OrangePi 6 Plus seen in real"></p>
<p>At the bottom there’s a wealth of ports, and it’s where you will install the necessary wireless module if you want Wifi and Bluetooth.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/orangepi6plus_board_wifi.avif" alt="OrangePi 6 Plus bottom view with wifi"></p>
<p>Now let’s dive into what you can do with this.</p>
<h2 id="sec:ports">Ports</h2>
<p>Here is what you can get from the top of the device. Note that most of it will be hidden from view as the board comes pre-installed with the heatsink that covers the SOC and the memory chips.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/orangepi6plus_top_ports.avif" alt=""></p>
<p>And the bottom view.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/orangepi6plus_bottom_ports.avif" alt=""></p>
<p>You can tell just from the format that you should have higher expectations from the hardware.</p>
<h2 id="sec:specs-of-the-orangepi-6-plus">Specs of the OrangePi 6 Plus</h2>
<p>In a table format:</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Specification</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SoC</strong></td>
<td>CIX CD8180 / CD8160 (12-core 64-bit)</td>
</tr>
<tr>
<td><strong>CPU Architecture</strong></td>
<td>4× Cortex-A720 (High-perf) + 4× Cortex-A720 (Main) + 4× Cortex-A520 (Efficiency)</td>
</tr>
<tr>
<td><strong>GPU</strong></td>
<td>Arm Immortalis-G720 MC10 (Ray Tracing &amp; 8K Decoding support)</td>
</tr>
<tr>
<td><strong>NPU (AI)</strong></td>
<td>Up to 45 TOPS (System-wide); ~30 TOPS Dedicated NPU</td>
</tr>
<tr>
<td><strong>RAM</strong></td>
<td>16GB / 32GB / 64GB LPDDR5 (128-bit)</td>
</tr>
<tr>
<td><strong>Storage</strong></td>
<td>2× M.2 2280 slots (PCIe 4.0 x4 NVMe), 1× MicroSD (TF) slot</td>
</tr>
<tr>
<td><strong>Networking</strong></td>
<td>Dual 5GbE (5000Mbps) Ethernet ports</td>
</tr>
<tr>
<td><strong>Wireless</strong></td>
<td>M.2 Key-E (2230) slot for Wi-Fi 6E/7 &amp; Bluetooth 5.4</td>
</tr>
<tr>
<td><strong>Video Output</strong></td>
<td>1× HDMI 2.1 (8K@60Hz), 1× DP 1.4, 2× USB-C (DP Alt Mode), 1× eDP</td>
</tr>
<tr>
<td><strong>USB Ports</strong></td>
<td>2× USB 3.0 Type-A, 2× USB 2.0 Type-A, 2× Full-function USB Type-C</td>
</tr>
<tr>
<td><strong>Camera (MIPI)</strong></td>
<td>2× 4-lane MIPI CSI interfaces</td>
</tr>
<tr>
<td><strong>Expansion</strong></td>
<td>40-pin GPIO header (UART, I2C, SPI, PWM, etc.)</td>
</tr>
<tr>
<td><strong>Audio</strong></td>
<td>3.5mm Headphone/Mic jack, 2× Speaker headers, 1× Analog MIC header</td>
</tr>
<tr>
<td><strong>Power Supply</strong></td>
<td>100W Dual USB Type-C PD (20V/5A)</td>
</tr>
<tr>
<td><strong>Dimensions</strong></td>
<td>115mm × 100mm</td>
</tr>
</tbody>
</table>
<p>As you can see from the specs, this is no joke. 16GB RAM by default, 12-cores processor, with a powerful GPU (Immortalis G720), and a NPU that has a claimed performance up to 30 TOPS. Not just that, but there’s numerous ports on this SBC, with 2 full sized M2 2280 slots! You can tell that the IO is not going to be a joke here.</p>
<p>If you are wondering about the SOC itself, we have more info about what to expect from CIX.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Specification</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SoC Model</strong></td>
<td>CIX CD8180 / CD8160 (Codename: CIX P1)</td>
</tr>
<tr>
<td><strong>Architecture</strong></td>
<td>Armv9.2-A (64-bit)</td>
</tr>
<tr>
<td><strong>Total CPU Cores</strong></td>
<td>12 Cores (Tri-cluster configuration)</td>
</tr>
<tr>
<td><strong>Big Cores</strong></td>
<td>4× Cortex-A720 @ Up to 2.8 GHz (Performance)</td>
</tr>
<tr>
<td><strong>Medium Cores</strong></td>
<td>4× Cortex-A720 @ Up to 2.4 GHz (Mainstream)</td>
</tr>
<tr>
<td><strong>Little Cores</strong></td>
<td>4× Cortex-A520 @ 1.8 GHz (Efficiency)</td>
</tr>
<tr>
<td><strong>L3 Cache</strong></td>
<td>12MB Shared L3 Cache</td>
</tr>
<tr>
<td><strong>GPU</strong></td>
<td>Arm Immortalis-G720 MC10</td>
</tr>
<tr>
<td><strong>Graphics Features</strong></td>
<td>Hardware Ray Tracing, Vulkan 1.3, OpenGL ES 3.2, OpenCL 3.0</td>
</tr>
<tr>
<td><strong>NPU (AI Engine)</strong></td>
<td>Arm-China Zhouyi: 30 TOPS (Dedicated); ~45 TOPS (Total System AI)</td>
</tr>
<tr>
<td><strong>AI Precision</strong></td>
<td>INT4, INT8, INT16, FP16, TF32</td>
</tr>
<tr>
<td><strong>VPU (Video)</strong></td>
<td>Linlon V8: 8K@60fps Decode (AV1/H.265/VP9), 8K@30fps Encode (H.265)</td>
</tr>
<tr>
<td><strong>Memory Interface</strong></td>
<td>128-bit LPDDR5 / LPDDR5X (Up to 5500 MT/s)</td>
</tr>
<tr>
<td><strong>Memory Bandwidth</strong></td>
<td>Up to 96 GB/s (Theoretical peak)</td>
</tr>
<tr>
<td><strong>PCIe Support</strong></td>
<td>PCIe Gen4 (Supports x8, x4, and x2 configurations)</td>
</tr>
<tr>
<td><strong>System Security</strong></td>
<td>Integrated Security Engine (Standard Arm SystemReady / ACPI support)</td>
</tr>
</tbody>
</table>
<p>2.8Ghz ARM Big Cores processors! That’s no joke, this is way beyond the typical frequency we see for small boards where things are usually below 2 Ghz. The memory interface also has a huge bandwidth, and we get full PCI4 with 8 lanes! This means that this board could probably be attached to an external GPU (eGPU) and be able to drive it (provided adequate software support).</p>
<p>About the NPU, the usual problem on Linux is that there is poor software support, and it’s certainly not in the mainline either. To leverage the 30 TOPS of dedicated AI power, you cannot simply use standard versions of PyTorch or TensorFlow out of the box. You must use the NeuralONE AI SDK. This also means that the NPU cannot use regular weights, and need to compile them into a different format to make them work. On paper, the NPU is highly versatile and supports the following data types: INT4, INT8, INT16, FP16, BF16, and TF32. The board has extensive documentation on a bunch of embedded models for vision detection, automation, such as YOLOv8 (Vision), ResNet50, OpenPose, and DeepLabv3.</p>
<p>Now let’s jump into the actual user experience.</p>
<h2 id="sec:software-support">Software Support</h2>
<h3 id="sec:desktop-experience">Desktop Experience</h3>
<h4 id="sec:debian-bookworm">Debian Bookworm</h4>
<p>There is a <em>Debian Bookworm</em> (12) image available at the time of writing. I was actually waiting for a 24.04 Ubuntu image to be made available, and that was the plan at some point according to my <em>OrangePi</em> contact, but they had to shift priorities apparently and now there is no ETA for the Ubuntu image. So instead of waiting, I decided to go ahead and review what’s possible with this Debian image. As you know, <em>Debian Bookworm</em> is not that new anymore (the base is from Mid-2023), and is now superseded by <em>Debian Trixie</em> (released in 2025). This Debian image comes with two kernels available, 6.1 and 6.6. The 6.6 kernel is also from 2023, and it’s not surprising you don’t get a more recent kernel, since the Linux support for various parts of the boards, not being upstreamed and mainlined, is very likely to be stuck on an older version. This is usually what causes headaches down the road: maybe some general functionality can be upstreamed, but will the NPU have working drivers for a more recent kernel? Your guess is as good as mine.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/fastfetch.avif" alt="Fastfetch for the OrangePi 6 Plus Bookworm"></p>
<p>You can burn the image directly on the NVME drive - no need to boot on a MicroSD card this time around. Note that at the beginning, my OrangePi did not boot, but I could see from the BIOS (yes, this board comes with a BIOS-like interface!) that everything was supposed to be seen by the SBC. Turns out that the firmware required an update to be able to boot on this Linux kernel, and after imaging the latest firmware on a USB stick and booting on it, a few minutes later, things worked as expected.</p>
<p>In any case, we get a GNOME desktop after boot. And everything works pretty much as you’d expect. Once thing that is immediately apparent when you start with the desktop is how snappy everything is. SBC boards like the Raspberry Pi 4 provide a good desktop experience but have some general sluggishness to them. On this board, this is pretty much like having a X86_64 experience. It recognized immediately my Ultra Wide Display and supported the 3440 x 1440 resolution without a hitch. Everything from navigating the desktop and settings is very fast. You get Chromium by default (and Firefox ESR in the repos) and the browser experience is very clean and fast, too. This board has absolutely no problem to play Youtube streams, even at 4k. This thing is FAST!</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/browsing2_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/browsing2_optimized.mp4" type="video/mp4">
</video>
<p>A quick look at vulkaninfo shows that we have working Vulkan drivers on this board! This is something that was initially very exciting, but it turns out that there are some limitations. More on that later.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/glxgears.avif" alt="Glx gears running on the OrangePi 6 Plus"></p>
<p>Turns out the Vulkan driver is limited to some early 1.3 version. You have options to upgrade Mesa, by using Debian backports - it gets you to a 25.07 Mesa version, where you don’t rely anymore on the proprietary driver - no, this time you get Panfrost, which means a more robust driver potentially… except that you are stuck on the 6.6 Linux Kernel, and Panfrost requires a more recent kernel to work properly (6.10+ apparently). So the solution would be to move to a more recent kernel, right? Do the Debian backports have it? Yes. Problem is, moving away from 6.6 will break a lot of patches that are necessary for the hardware of this board to work. Such as HDMI out at resolutions higher than 1080p, and NPU support! So, it’s a major trade-off.</p>
<h4 id="sec:getting-bluetooth-to-work">Getting Bluetooth to work</h4>
<p>Even though bluetooth shows in the GNOME desktop controls, and that it could see some of my peripherals, it could not connect to any of my external audio devices. But I don’t give up so fast. Turns out there’s an issue with a missing pipewire dependency to allow for bluetooth audio to work. Here’s how you can get it done:</p>
<pre><code>sudo apt install libspa-0.2-bluetooth pipewire-audio-client-libraries
</code></pre>
<p>Afterwards you can launch <em>bluetoothctl</em> from the command line, and you can execute the following commands one by one</p>
<pre><code>power on
agent on
default-agent
scan on
</code></pre>
<p>After the scan is activated you should see bluetooth device popping up in the terminal. Note the ID of the bluetooth device you want to connect, and then:</p>
<pre><code>pair &lt;device id&gt;
trust &lt;device id&gt;
connect &lt;devic_id&gt;
</code></pre>
<p>Once this is fixed, things work as expected. And now I get audio!</p>
<h4 id="sec:compiling-obs">Compiling OBS</h4>
<p>OBS is not available as flatpak for arm64, and not in the repos either. This means, that you are in for a compilation from sources. This is not the thing that scares me. But in our situation, it’s a little more convoluted that I expected. First, turns out that OBS did not like the fact that the Cmake was relatively old. So I had to get one of the recent Cmake binaries. Thanksfully they have <a href="https://cmake.org/download/">arm64 binaries already available on their website</a>, so that was easy.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/cmake_aarch64_linux.avif" alt="Cmake binaries"></p>
<p>Next, OBS would complain of a too old FFmpeg version. Not too surprising for something that depends so heavily on it. So I had to go for a full FFmpeg compilation. Here’s what I did to save you time:</p>
<pre><code>git clone https://github.com/FFmpeg/FFmpeg.git
cd ffmpeg
git checkout release/7.1 # in order to have a stable release, not the master branch in itself

./configure --prefix=/usr/local --enable-shared --disable-static --enable-gpl --enable-libx264 --enable-libx265
make -j$(nproc)

sudo make install
</code></pre>
<p>Finally, compiling OBS is a game of cat and mouse, you need to basically give up one extension after the other as new errors arise. Most of the things you need to remove are not critical (NVENC, not relevant on non-nvidia hardware, browser support, not really our thing either) and at the end you get a fairly long series of command.</p>
<pre><code>git clone https://github.com/obsproject/obs-studio.git
cd obs-studio

# remove the obs-browser plugin from the obs folder root directlroy
mv plugins/obs-browser plugins/obs-browser.bak

# from the obs root folder
git submodule update --init --recursive

# a couple of exports to avoid the compilation to fail because of FFmpeg warnings
export CFLAGS="-Wno-error=deprecated-declarations"
export CXXFLAGS="-Wno-error=deprecated-declarations"

# since the flags can be ignored, we also edit the following file from the root directory
echo 'target_compile_options(obs-ffmpeg PRIVATE "-Wno-error=deprecated-declarations")' &gt;&gt; plugins/obs-ffmpeg/CMakeLists.txt

# use the path to the cmake version you just downloaded
&lt;path_to_newer_cmake_binary&gt;/cmake -DCMAKE_PREFIX_PATH=/usr/local -DFFMPEG_INCLUDE_DIR=/usr/local/include  -DPKG_CONFIG_PATH=/usr/local/lib/pkgconfig -DENABLE_AJA=OFF -DUNIX_STRUCTURE=1 -DENABLE_GIO=OFF -DENABLE_VPL=OFF  -DENABLE_QSV11=OFF -DENABLE_BROWSER=OFF  -DENABLE_WEBRTC=OFF -DENABLE_NATIVE_NVENC=OFF -DCMAKE_COMPILE_WARNING_AS_ERROR=OFF ..

make -j$(nproc)

sudo make install

</code></pre>
<p>It took a little while, but it worked!</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/obs-working_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/obs-working_optimized.mp4" type="video/mp4">
</video>
<p>I must admit, I did not expect that it would go so well in the end.
Now, thanks to this, you will get a lot of videos from the board in action that no other site reviewing that board has been able to offer.</p>
<h4 id="sec:noise-and-temperature">Noise and Temperature</h4>
<p>The board is very quiet by default. Even when the fan is activated, you barely hear it, at least under fairly typical conditions. Things change when you push the board to its full power, for example a long compilation time. In such conditions, the fan becomes louder with a <em>woosh</em> kind of sound. You will definitely hear it when doing benchmarks, and in my case, when running LLMs, for example.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/temperature_control.avif" alt="Pretty good temperature control for the OrangePi 6 Plus"></p>
<p>In terms of temperature, the control is very good. At 100% usage even for long durations, while the fan becomes clearly noticeable, it maintains the temperature under 60 C (this is winter right now, and the temperature remained at 58 C). Kudos for the good engineering there. It’s as good as the <a href="https://boilingsteam.com/upgrading-fans-with-a-custom-shroud-on-a-rtx3090-goodbye-fan-noise/">custom cooling solution for my RTX3090</a>.</p>
<h4 id="sec:power-draw">Power Draw</h4>
<p>I don’t have a way to measure the power draw currently, but based on reports from other publications it looks like we are looking at:</p>
<ul>
<li>15W at idle, which is fairly high</li>
<li>30+W during usage - this is why they recommend to use the provided PSU with USB-C. Note that any charger that can provide 45W should do the trick as well.</li>
</ul>
<p>So, if you are looking for something that has almost no footprint at idle, this is not it. At idle the <em>OrangePi 5 Ultra</em> consumes more than 3 times less, so that sound more like something you’d use for a server.</p>
<h4 id="sec:benchmarks">Benchmarks</h4>
<p>A quick run at Geekbench 6 shows a very strong single core score, and an exceptional multi-core score.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/overall-bench-orangepi6plus-vs-raspberrypi5.svg" alt="Geekbench 6 results for the orangepi 6 plus vs Raspbnerry Pi 5"></p>
<p>Now the details in single score:</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/single-core-bench-orangepi6-plus-vs-raspberrypi5.svg" alt="Geekbench 6 results for the orangepi 6 plus vs Raspbnerry Pi 5 single core"></p>
<p>And in multi-core where we can see that the OrangePi 6 absolutely crushes what you can find on a <em>Raspberry Pi 5</em>.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/multi-core-bench-orangepi6-plus-vs-raspberrypi5.svg" alt="Geekbench 6 results for the orangepi 6 plus vs Raspbnerry Pi 5 single core"></p>
<p>Of course, this is a relatively cheap system that is not going to win the benchmark charts. But look at the results. On single core, we get a score equivalent to a <em>i5-10500</em> running at a similar frequency (2.3 Ghz).</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/geekbench_context_single_core_1290.avif" alt="Geekbench 6 results context for the orangepi 6 plus single core"></p>
<p>On multicore this is much more impressive, thanks to its twelve cores. And it gets very close, according to the bench, to what an <em>AMD Ryzen 7 4800H</em> (8 cores) can deliver.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/geekbench_context_multi_core_6032.avif" alt="Geekbench 6 results context for the orangepi 6 plus multi core"></p>
<p>This is clearly a powerhouse, CPU-wise. For a starting price at 199 USD for the 16GB version, this is a fantastic value proposition. We reviewed the <em>OrangePi 5 Ultra</em> a few months back and the price point is very close (around 160 USD), and unless you need something very small and fanless, the <em>OrangePi 6 Plus</em> is (very clearly) a better deal.</p>
<h4 id="sec:gaming">Gaming</h4>
<p>Since we have both a fairly powerful SOC, and a working <em>Vulkan</em> driver, this means that we can expect <a href="https://boilingsteam.com/new-box86-and-box64-releases-for-may-2024/">Box64</a> to do some magic for us there. I compiled Box64 to make it possible to launch Steam, and for some reason there is some Vulkan related error that prevents <em>Steam</em> from launching. Too bad. I still have a GOG account with a few games that have Linux clients. I tried the following:</p>
<ul>
<li>Beholder 2</li>
<li>Shadow Warrior 2013</li>
<li>Oxenfree</li>
<li>Day of the Tentacle Remastered</li>
<li>Torchlight 2</li>
</ul>
<p>And with some degree of success! <em>Beholder 2</em> worked just fine, and are definitely playable on this board, while the framerate remains between the 20s and 30s FPS (here a little slower on this video because of software OBS capture).</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/beholder2_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/beholder2_optimized.mp4" type="video/mp4">
</video>
<p>Here’s <em>Oxenfree’s</em> Linux x86_64 client running in Full HD on the OrangePi 6 Plus, using Box64:</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/oxenfree_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/oxenfree_optimized.mp4" type="video/mp4">
</video>
<p><em>Torchlight 2</em> runs nicely too, at something like 20 to 30 FPS in Full HD.</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/torchlight2_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/torchlight2_optimized.mp4" type="video/mp4">
</video>
<p><em>Shadow Warrior</em> refused to launch, complaining about the graphics drivers not being recent enough (turns out that Zink provides OpenGL support, and there is some issue to detect that the OpenGL version is properly supported… my guess). <em>Day of the Tentacle</em> crashes at start too, not sure why. When games work, the performance is not staggering but convincing for a somewhat small SoC at the end of the day. AMD and Intel are certainly ahead in terms of graphics performance on integrated chips, but they have much larger processors and much more expensive ones, too.</p>
<p>I also tried FOSS games or engines:</p>
<ul>
<li>GZDoom (compiled from source, the flatpak version sucks in performance!)</li>
<li>Luanti (ex-Minetest)</li>
<li>IOQuake3 (compiled from source)</li>
<li>OAD (from Debian repos)</li>
</ul>
<p><em>GZDoom</em> works exceptionally well with the OpenGLES renderer at Full HD. It’s fast, responsive. Stable at 60 FPS on Full HD. And <em>Doom</em> is still as fun as it was in the days, or more so if you run <strong>Brutal Doom</strong>.</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/gzdoom_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/gzdoom_optimized.mp4" type="video/mp4">
</video>
<p>While I don’t have a sample video here, <em>Quake 3 Arena</em> with the <em>IOQuake3</em> engine works perfectly, and runs at 60 fps on Full HD without a sweat.</p>
<p><em>Luanti</em> (ex Minetest) works extremely well - I turned most of the details to the max at 1080p and it kept at a solid 60 FPS. Sure, that’s no AAA game, but it’s a good alternative to <em>Minecraft</em>.</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/luanti_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/luanti_optimized.mp4" type="video/mp4">
</video>
<p><em>0Ad</em> works extremely well, even in full screen at my Ultra Wide screen resolution. I took a video at FUll HD, and while OBS does slow the framerate a little, you can still see it’s very smooth.</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/0ad_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/0ad_optimized.mp4" type="video/mp4">
</video>
<p><em>0ad</em> has come a long way, I really need to revisit a recent version of the game to see how much it has changed!</p>
<h3 id="sec:server-use">Server use</h3>
<p>This is a very capable board that would make a very powerful server. The <em>Debian</em> image comes with <em>Docker</em>, and as we have seen before during the <a href="https://boilingsteam.com/orange-pi-5-ultra-review/">OrangePi 5 Ultra review</a>, the landscape of ready-to-use Docker Hub images is huge and can get you started with numerous server-side applications in no time. Since we have at least 16Gb of memory, very fast I/O (PCIe4!), and a very fast CPU, this SBC will be able to wonders with a wide variety of applications. The only problem is the power draw. It looks like we are stuck with 15W as an idle baseline, and that seems a bit too much for some light server use.</p>
<h3 id="sec:ai-applications">AI applications</h3>
<p>This board comes with a very large repository (60 GB!) that you can install and sync automatically, with tons of applications and demos you can try out. There is a large part of their documentation dedicated to that part <a href="https://orangepi.net/wp-content/uploads/2025/11/OrangePi_6_Plus_Cix_Linux-System_User-Manual_v0.4.pdf">in their manual</a>. And for what I could try, it seems to work as long as you follow their instructions.</p>
<p>In my case I like to work with LLMs for a range of applications, so I was interested to see how fast this board could run some small-ish models. One of the major limitations is that we don’t have working NPU support for <a href="https://github.com/ggml-org/llama.cpp">llama.cpp</a> (oh no!). I was thinking, <em>“no worries, we have a vulkan driver so let’s use Vulkan instead”</em>. I like to be optimistic sometimes. Turns out that the <em>Vulkan</em> driver is below the minimum Vulkan drivers specs required by <em>llama.cpp</em> recently, and I would need to go back to a end 2024 build to be able to compile for Vulkan support. Going back one year on <em>llama.cpp</em>… not an option, sorry (too many models would not be supported going back so far in time). So, we are left with using the raw power of the CPU. Which means it won’t be quiet - expect some good old noise fan in such use cases.</p>
<p>Anyway, I went for <a href="https://huggingface.co/Qwen/Qwen3-1.7B">Qwen3 1.7b</a> - downloaded the safetensors weights, then converted them to <a href="https://github.com/ggml-org/ggml/blob/master/docs/gguf.md">gguf</a>, followed by a quantization step to make them IQ4_S. Running the model at this kind of quantization gives us about 14 tokens per second when doing inference. Definitely usable, very usable even, while this is a small model.</p>
<video autoplay="" loop="" muted="" playsinline="">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/llm_inference_optimized.webm" type="video/webm">
  <source src="https://boilingsteam.com/orange-pi-6-plus-review/llm_inference_optimized.mp4" type="video/mp4">
</video>
<p>Ideally, you’d want to have a board that can run a 7b model with proper hardware acceleration and no fan usage. Not sure if having actual NPU support in the future would help for that or not. In other words, we are not there yet.</p>
<h2 id="sec:existing-alternatives">Existing Alternatives</h2>
<p>As usual there is not a single vendor who can provide you with a CIX experience. This time the main competitor is Radxa, and they have two boards called Orion 6 that feature the same chip - one that is much bigger in footprint (mini-ITX size), and another one that is more similar to the OrangePi 6 Plus size (the Orion 6N). While I don’t have access to the Radxa ones, I can’t comment on how good their support is, but they are very likely to suffer from the same limitations, software wise.</p>
<p><img src="https://boilingsteam.com/orange-pi-6-plus-review/orangepi6plus_alternatives.avif" alt="OrangePi 6 Plus versus the Radxa competition"></p>
<p>As expected, they also only provide a <a href="https://docs.radxa.com/en/orion/download#radxa-os">Debian Bookworm image under the Radxa OS nickname</a> so this is no different from what you can get on the <em>OrangePi 6 Plus</em>. Price-wise, they are both available at around similar price points, so you could make you decision based on the best deal you can get.</p>
<h2 id="sec:verdict">Verdict</h2>
<p>This is a <strong>very impressive SBC</strong>, with an exceptional value proposition. Its performance profile puts if far away from the <em>toy</em> category from what we have seen before in the SBC category, and puts it right into the desktop performance realm. It’s still relatively small so you could easily attach it behind a monitor and power a personal computer this way. As a Linux user, things are so fast that you’d be very surprised this is an ARM board running under the hood. For server use, you’d probably want to avoid this one, because of the fairly heavily baseline power consumption.</p>
<p>As usual, the pitfalls are always going to be the same. This time around you get an <em>older Debian image</em> (bookworm), with an <em>older kernel</em> (6.6), and some proprietary drivers that you have to live with. Ultimately if you want to upgrade the software running on your SBC that will mean breaking things, and there’s often a fairly long time (if ever) for some hardware components to be supported in newer distros. It’s not necessarily a deal breaker these days. This board is fast enough to compile software fairly quickly if needed. You have Flatpak as well providing some good coverage for a lot of application (even if performance may be sub-par). There are ongoing efforts to mainline the GPU found in the chip, so it could be that a year from now we are in a better place when it comes to proper hardware support beyond kernel 6.6.</p>
<p>In any case, this is a surprising new entry in terms of performance / price point. This puts the bar very high for future SBCs ARM64 SBCs. On a personal note, I’d like to see some serious hardware dedicated to running AI models (instead of large desktops chaining multiple GPUs), and maybe highly customized ARM64 SBCs are going to become one option at some point.</p>
<p>If you are interested in getting one, there are many resellers, but one of the most direct ones are on Aliexpress:</p>
<ul>
<li><a href="https://ja.aliexpress.com/item/1005010153505031.html">Link1</a> : 16GB at about 223 USD</li>
<li><a href="https://ja.aliexpress.com/item/1005010153177606.html">Link2</a> : 32GB at about 258 USD</li>
<li><a href="https://ja.aliexpress.com/item/1005010159135486.html">Link3</a> : 32GB at about 258 USD</li>
</ul>
<p>Since the difference of price between 16 GB and 32GB is so small currently, it would make total sense to go for the 32GB.</p>
<p>Note: we were provided with a review unit from <em>OrangePi</em> (more specifically the OrangePi 6 Plus 16GB version).</p>
    
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Ez FFmpeg – Video editing in plain English (306 pts)]]></title>
            <link>http://npmjs.com/package/ezff</link>
            <guid>46400251</guid>
            <pubDate>Sat, 27 Dec 2025 08:45:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://npmjs.com/package/ezff">http://npmjs.com/package/ezff</a>, See on <a href="https://news.ycombinator.com/item?id=46400251">Hacker News</a></p>
Couldn't get http://npmjs.com/package/ezff: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Pre-commit hooks are broken (119 pts)]]></title>
            <link>https://jyn.dev/pre-commit-hooks-are-fundamentally-broken/</link>
            <guid>46398906</guid>
            <pubDate>Sat, 27 Dec 2025 03:45:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jyn.dev/pre-commit-hooks-are-fundamentally-broken/">https://jyn.dev/pre-commit-hooks-are-fundamentally-broken/</a>, See on <a href="https://news.ycombinator.com/item?id=46398906">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  

	

  
    

  <div itemprop="articleBody">
    <p>Let's start a new Rust project.</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>mkdir</span></span><span> best-fizzbuzz-ever</span>
</span><span><span>$</span> <span><span>cd</span></span><span> best-fizzbuzz-ever</span>
</span><span><span>$</span> <span><span>cat</span></span><span> <span><span>&lt;&lt;</span> <span>EOF</span></span> <span><span>&gt;</span> main.rs</span><span>
</span></span></span><span><span><span>fn main() { for i in 0.. {
</span></span></span><span><span><span>    println ("fizzbuzz");
</span></span></span><span><span><span>}}
</span></span></span><span><span><span><span>EOF</span></span></span>
</span><span><span>$</span> <span><span>git</span></span><span> init</span>
</span><span>Initialized empty Git repository in /home/jyn/src/third-website/best-fizzbuzz-ever/.git/
</span><span><span>$</span> <span><span>git</span></span><span> add main.rs</span>
</span><span><span>$</span> <span><span>git</span></span><span> commit<span><span> --</span>message</span> fizzbuzz</span>
</span><span>[main (root-commit) 661dc28] fizzbuzz
</span><span> 1 file changed, 4 insertions(+)
</span><span> create mode 100644 main.rs
</span></code></pre>
<p>Neat. Now let's say I add this to some list of fizzbuzz projects in different languages.
Maybe .... <a href="https://github.com/joshkunz/fizzbuzz">this one</a>.
They tell me I need to have "proper formatting" and "use consistent style".
How rude.</p>
<p>Maybe I can write a pre-commit hook that checks that for me?</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>cat</span></span><span> <span><span>&lt;&lt;</span> <span>'</span><span>EOF</span><span>'</span></span> <span><span>&gt;</span> pre-commit</span><span>
</span></span></span><span><span><span>#!/bin/sh
</span></span></span><span><span><span>set -eu
</span></span></span><span><span><span>for f in *.rs; do
</span></span></span><span><span><span>  rustfmt --check "$f"
</span></span></span><span><span><span>done
</span></span></span><span><span><span><span>EOF</span></span></span>
</span><span><span>$</span> <span><span>chmod</span></span><span> +x pre-commit</span>
</span><span><span>$</span> <span><span>ln</span></span><span><span><span> -</span>s</span> ../../pre-commit .git/hooks/pre-commit</span>
</span><span><span>$</span> <span><span>git</span></span><span> add pre-commit</span>
</span><span><span>$</span> <span><span>git</span></span><span> commit<span><span> --</span>message</span> <span><span>"</span>add pre-commit hook<span>"</span></span></span>
</span><span>Diff in /home/jyn/src/third-website/best-fizzbuzz-ever/src/main.rs:1:
</span><span>-fn main() { for i in 0.. {
</span><span>-    println ("fizzbuzz");
</span><span>-}}
</span><span>+fn main() {
</span><span>+    for i in 0.. {
</span><span>+        println("fizzbuzz");
</span><span>+    }
</span><span>+}
</span></code></pre>
<p>Neat! Let's commit that change.</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>rustfmt</span></span><span> main.rs</span>
</span><span><span>$</span> <span><span>git</span></span><span> commit<span><span> --</span>message</span> <span><span>"</span>add pre-commit hook<span>"</span></span></span>
</span><span>[main 3be7b87] add pre-commit hook
</span><span> 1 file changed, 4 insertions(+)
</span><span> create mode 100755 pre-commit
</span><span><span>$</span> <span><span>git</span></span><span> status</span>
</span><span>On branch main
</span><span>Changes not staged for commit:
</span><span>  (use "git add &lt;file&gt;..." to update what will be committed)
</span><span>  (use "git restore &lt;file&gt;..." to discard changes in working directory)
</span><span>	modified:   main.rs
</span></code></pre>
<p>Oh ... We fixed the formatting, but we didn't actually stage the changes.
The pre-commit hook runs on the <em>working tree</em>, not on the <em>index</em>, so it didn't catch the issue.
We can see that the version tracked by git still has the wrong formatting:</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>git</span></span><span> show HEAD:main.rs</span>
</span><span>fn main() { for i in 0.. {
</span><span>    println ("fizzbuzz");
</span><span>}}
</span></code></pre>
<p>Maybe we can make the script smarter?
Let's checkout all the files in the index into a temporary directory and run our pre-commit hook there. <sup id="fr-4-1"><a href="#fn-4">1</a></sup></p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>cat</span></span><span> <span><span>&lt;&lt;</span> <span>'</span><span>EOF</span><span>'</span></span> <span><span>&gt;</span> pre-commit</span><span>
</span></span></span><span><span><span>#!/bin/sh
</span></span></span><span><span><span>set -eu
</span></span></span><span><span><span>
</span></span></span><span><span><span>tmpdir=$(mktemp -d --tmpdir "$(basename "$(realpath .)")-pre-commit.XXXX")
</span></span></span><span><span><span>trap 'rm -r "$tmpdir"' EXIT
</span></span></span><span><span><span>git checkout-index --all --prefix="$tmpdir/"
</span></span></span><span><span><span>for f in $tmpdir/*.rs; do
</span></span></span><span><span><span>  rustfmt --check "$f"
</span></span></span><span><span><span>done
</span></span></span><span><span><span><span>EOF</span></span></span>
</span><span><span>$</span> <span><span>git</span></span><span> add pre-commit</span>
</span><span><span>$</span> <span><span>git</span></span><span> commit<span><span> --</span>message</span> <span><span>"</span>make pre-commit hook smarter<span>"</span></span></span>
</span><span>Diff in /tmp/best-fizzbuzz-ever-pre-commit.ZNyw/main.rs:1:
</span><span>-fn main() { for i in 0.. {
</span><span>-    println ("fizzbuzz");
</span><span>-}}
</span><span>+fn main() {
</span><span>+    for i in 0.. {
</span><span>+        println("fizzbuzz");
</span><span>+    }
</span><span>+}
</span><span>
</span></code></pre>
<p>Yay! That caught the issue.
Now let's add our rust program to that collection of fizzbuzz programs.</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>git</span></span><span> add main.rs</span>
</span><span><span>$</span> <span><span>git</span></span><span> commit<span><span> --</span>message</span> <span><span>"</span>make pre-commit hook smarter<span>"</span></span></span>
</span><span>[main 3cb40f6] make pre-commit hook smarter
</span><span> 2 files changed, 11 insertions(+), 4 deletions(-)
</span><span><span>$</span> <span><span>git</span></span><span> remote add upstream https://github.com/joshkunz/fizzbuzz</span>
</span><span><span>$</span> <span><span>git</span></span><span> fetch upstream</span>
</span><span>remote: Enumerating objects: 222, done.
</span><span>remote: Total 222 (delta 0), reused 0 (delta 0), pack-reused 222 (from 1)
</span><span>Receiving objects: 100% (222/222), 29.08 KiB | 29.08 MiB/s, done.
</span><span>Resolving deltas: 100% (117/117), done.
</span><span>From https://github.com/joshkunz/fizzbuzz
</span><span> * [new branch]      master     -&gt; upstream/master
</span><span><span>$</span> <span><span>git</span></span><span> rebase upstream</span>
</span><span>Successfully rebased and updated refs/heads/main.
</span></code></pre>
<p>Maybe we'll make one last tweak...</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>sed</span></span><span><span><span> -</span>i</span> <span><span>'</span>1i // Written by jyn<span>'</span></span> main.rs</span>
</span><span><span>$</span> <span><span>git</span></span><span> commit main.rs<span><span> --</span>message</span> <span><span>"</span>mark who wrote fizzbuzz<span>"</span></span></span>
</span><span>Diff in /tmp/best-fizzbuzz-ever-pre-commit.n1Pj/fizzbuzz-traits.rs:4:
</span><span> use std::iter;
</span><span>
</span><span> struct FizzBuzz {
</span><span>-    from : i32
</span><span>-  , to : i32
</span><span>+    from: i32,
</span><span>+    to: i32,
</span><span> }
</span><span>
</span><span> impl FizzBuzz {
</span></code></pre>
<p>Uh. Huh. Right.
The code that was already here wasn't formatted according to rustfmt.
Our script is running on every file in the git repo, so it won't let us commit.</p>
<p>Maybe we can change it to only run on modified files?</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>cat</span></span><span> <span><span>&lt;&lt;</span> <span>'</span><span>EOF</span><span>'</span></span> <span><span>&gt;</span> pre-commit</span><span>
</span></span></span><span><span><span>#!/bin/sh
</span></span></span><span><span><span>set -eu
</span></span></span><span><span><span>
</span></span></span><span><span><span>files=$(git diff --name-only --cached --no-ext-diff --diff-filter=d)
</span></span></span><span><span><span>
</span></span></span><span><span><span>tmpdir=$(mktemp -d --tmpdir "$(basename "$(realpath .)")-pre-commit.XXXX")
</span></span></span><span><span><span>trap 'rm -r "$tmpdir"' EXIT
</span></span></span><span><span><span>
</span></span></span><span><span><span>printf %s "$files" | tr '\n' '\0' | xargs -0 git checkout-index --prefix="$tmpdir/"
</span></span></span><span><span><span>for f in $tmpdir/*.rs; do
</span></span></span><span><span><span>  rustfmt --check "$f"
</span></span></span><span><span><span>done
</span></span></span><span><span><span><span>EOF</span></span></span>
</span><span><span>$</span> <span><span>git</span></span><span> commit main.rs pre-commit <span>\
</span></span></span><span><span><span><span>  --</span>message</span> <span><span>"</span>update main.rs; make pre-commit even smarter<span>"</span></span></span>
</span><span>[main f2925bc] update main.rs; make pre-commit even smarter
</span><span> 2 files changed, 5 insertions(+), 1 deletion(-)
</span></code></pre>
<p>Alright. Cool.</p>
<p>Let's do one last thing.
Let's say we had an existing PR to this repo and we need to rebase it.
Maybe it had a merge conflict, or maybe there was a fix on main that we need in order to implement our solution.</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>git</span></span><span> checkout upstream/HEAD  <span><span>#</span></span><span> Simulate an old PR by checking out an old commit</span></span>
</span><span>HEAD is now at 56bf3ab Adds E to the README
</span><span><span>$</span> <span><span>echo</span></span><span> <span><span>'</span>fn main() { println!("this counts as fizzbuzz, right?"); }<span>'</span></span> <span>&gt;</span> print.rs</span>
</span><span><span>$</span> <span><span>git</span></span><span> add print.rs</span>
</span><span><span>$</span> <span><span>git</span></span><span> commit<span><span> --</span>message</span> <span><span>"</span>Add print.rs<span>"</span></span></span>
</span><span>[detached HEAD 3d1bbf7] Add print.rs
</span><span> 1 file changed, 1 insertion(+)
</span><span> create mode 100644 print.rs
</span></code></pre>
<p>And let's also say that we want to edit the commit message.</p>
<pre data-lang="console"><code data-lang="console"><span><span>$</span> <span><span>git</span></span><span> rebase<span><span> -</span>i</span> main  <span><span>#</span></span><span> Rebase this whole branch over our main branch</span></span>
</span><span>reword 3d1bbf7 Add print.rs
</span><span><span>#</span> <span><span>Rebase</span></span><span> f2925bc..3d1bbf7 onto f2925bc (1 command</span><span></span>)
</span></code></pre>
<h2 id="now-we-really-have-a-problem">Now, we <em>really</em> have a problem.<a href="#now-we-really-have-a-problem" aria-label="Anchor link for: now-we-really-have-a-problem"></a>
</h2>
<pre><code><span>Error: file `/tmp/best-fizzbuzz-ever-pre-commit.p3az/*.rs` does not exist
</span><span>Could not apply 3d1bbf7... Add print.rs
</span></code></pre>
<p>Two things went wrong here:</p>
<ol>
<li>Our pre-commit hook can't handle commits that don't have any Rust files.</li>
<li>Our pre-commit hook <em>ran on a branch we were rebasing</em>. <sup id="fr-2-1"><a href="#fn-2">2</a></sup></li>
</ol>
<p>Fixing the first thing doesn't really help us, because we don't control other people's branches.
They might have used <code>git commit --no-verify</code>.
They might not even have a pre-commit hook installed.
They might have had a branch that passed the hook when they originally wrote it, but not after a rebase (e.g. if your hook is <code>cargo check</code> or something like that).
They might have had a branch that used an old version of the hook that didn't have as many checks as a later version.</p>
<p>Our only real choice here is to pass <code>--no-verify</code> to <code>git rebase</code> every time we run it, and to <code>git commit</code> for every commit in the rebase we modify,
and possibly even to every <code>git merge</code> we run outside of a rebase.</p>
<p>This is because pre-commit hooks are a <em>fundamentally broken idea</em>.
Code does not exist in isolation.
Commits that are local to a developer machine do not ever go through CI.
Commits don't even necessarily mean that that the code is ready to publish—pre-commit hooks don't run on <code>git stash</code> for a reason!
I don't use <code>git stash</code>, I use <code>git commit</code> so that my stashes are tied to a branch, and hooks completely break this workflow.</p>
<p>There are a <a href="https://blog.plover.com/prog/git/hook-disaster.html">bunch</a>
of <a href="https://dev.to/afl_ext/are-pre-commit-git-hooks-a-good-idea-i-dont-think-so-38j6">other</a> footguns with pre-commit hooks.
This doesn't even count the fact that nearly all pre-commit hooks are implemented in a broken way and just blindly run on the worktree, and are slow or unreliable or both.
Don't get me started on pre-commit hooks that try to add things to the commit you're about to make.</p>

<p>Please just don't use <code>pre-commit</code> hooks. Use <code>pre-push</code> instead. <sup id="fr-1-1"><a href="#fn-1">4</a></sup>
<code>pre-push</code> hooks nearly avoid all of these issues.</p>
<p>The only use case where I think pre-commit hooks are a good idea is for things that must <em>never</em> committed, that are worth interrupting a complicated rebase to prevent; namely: credentials.
Once credentials are committed they're quite difficult to get out, and even harder to be sure you haven't missed them.</p>
<h2 id="tips-for-writing-a-pre-push-hook">Tips for writing a <code>pre-push</code> hook<a href="#tips-for-writing-a-pre-push-hook" aria-label="Anchor link for: tips-for-writing-a-pre-push-hook"></a>
</h2>
<ul>
<li>Run on the index, not the working tree, as described above. <sup id="fr-3-1"><a href="#fn-3">5</a></sup></li>
<li>Only add checks that are fast and reliable. Checks that touch the network should never go in a hook. Checks that are slow and require an up-to-date build cache should never go in a hook. Checks that require credentials or a running local service should never go in a hook.</li>
<li>Be as quiet as possible. This hook is running buried inside a bunch of other commands, often without the developer knowing that the hook is going to run. Don't hide other important output behind a wall of progress messages.</li>
<li>Don't set the hook up automatically. Whatever tool you use that promises to make this reliable is wrong. There is not a way to do this reliably, and the number of times it's broken on me is more than I can count. Please just add docs for how to set it up manually, prominantly featured in your CONTRIBUTING docs. (You do have contributing docs, right?)</li>
</ul>

<p>And don't write <code>pre-commit</code> hooks!</p>
<section>
<ol>
<li id="fn-4">
<p>This is really quite slow on large enough repos, but there's not any real alternative. <code>git stash --keep-index</code> messes with git index state and also with your stashes. The only VCS that exposes a FUSE filesystem of its commits is <a href="https://github.com/facebook/sapling/blob/main/eden/fs/docs/Overview.md">Sapling</a>, which is poorly supported outside Facebook. The best you can do is give up on looking at the whole working copy and only write hooks that read a single file at a time. <a href="#fr-4-1">↩</a></p>
</li>
<li id="fn-2">
<p>By default this doesn't happen when running bare <code>rebase</code>, but the second you add <code>--interactive</code>, nearly anything you do runs a hook. Hooks will also run when you attempt to resolve merge conflicts. <a href="#fr-2-1">↩</a></p>
</li>
<li id="fn-lint-staged">
<p><code>lint-staged</code> does actually have a <code>--fail-on-changes</code> flag which aborts the commit, but that still modifies the working tree, and it's not on by default. <a href="#fr-lint-staged-1">↩</a></p>
</li>
<li id="fn-1">
<p>For more info about the difference, and a full list of possible hooks, see <a href="https://git-scm.com/docs/githooks"><code>man 5 githooks</code></a>. <a href="#fr-1-1">↩</a></p>
</li>
<li id="fn-3">
<p>Notice that I don't say "only run on changed files". That's because it's <a href="https://lore.kernel.org/git/CAHnEOG2o784dk+OpkGt-1qjRJb34=sFMJvh-JRJ3v+GNBxFywQ@mail.gmail.com/">not actually possible to reliably determine which branch the current commit is based on</a>, the best you can do is pick a random branch that looks likely. <a href="#fr-3-1">↩</a></p>
</li>
</ol>
</section>

  </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CEO of Health Care Software Company Sentenced for $1B Fraud Conspiracy (122 pts)]]></title>
            <link>https://www.justice.gov/opa/pr/ceo-health-care-software-company-sentenced-1b-fraud-conspiracy</link>
            <guid>46398752</guid>
            <pubDate>Sat, 27 Dec 2025 03:14:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.justice.gov/opa/pr/ceo-health-care-software-company-sentenced-1b-fraud-conspiracy">https://www.justice.gov/opa/pr/ceo-health-care-software-company-sentenced-1b-fraud-conspiracy</a>, See on <a href="https://news.ycombinator.com/item?id=46398752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>An Arizona man was sentenced Friday to 15 years in prison and ordered to pay more than $452 million in restitution for conspiring to defraud Medicare and other federal health care benefit programs of more than $1 billion by operating a platform that generated false doctors’ orders used to support fraudulent claims for various medical items.</p><p>“This just sentence is the result of one of the largest telemarketing Medicare fraud cases ever tried to verdict,” said Acting Assistant Attorney General Matthew R. Galeotti of the Justice Department’s Criminal Division.&nbsp;“Telemedicine scammers who use junk mailers, spam calls and the internet to target senior citizens steal taxpayer money and harm vulnerable populations. The Criminal Division will continue dedicating substantial resources to the fight against telemedicine and medical equipment frauds that drain our health care benefit programs.”</p><p>“Together with our partners, the FBI will aggressively pursue those who defraud taxpayer funded health care programs,” said Acting Assistant Director Rebecca Day of the FBI’s Criminal Investigative Division.&nbsp;“Programs like Medicare are intended to help the most vulnerable among us, and fraud schemes like the one orchestrated by the defendant can jeopardize the delivery of critical care to those who need it the most.”</p><p>“This sentence sends a clear message: those who exploit telemedicine to prey on seniors and steal from taxpayer-funded health care programs will be held accountable,” said Deputy Inspector General for Investigations Christian J. Schrank of the U.S. Department of Health and Human Services, Office of Inspector General (HHS-OIG). “This scheme was a massive betrayal of trust, built on deception and greed. Our investigators, working with law enforcement partners, dismantled this billion-dollar fraud operation that targeted vulnerable patients and undermined the integrity of Medicare. We will not relent in our mission to protect the public and safeguard Medicare and other federal health care programs from fraud, waste, and abuse.”</p><p>“This sentencing underscores the Veterans Affairs Office of Inspector General’s (VA OIG) commitment to vigorously investigate those who would seek to defraud VA healthcare programs,” said Special Agent in Charge David Spilker with the VA OIG Southeast Field Office. “The VA OIG thanks the Department of Justice and our law enforcement partners for their efforts in this investigation.”</p><p>“This investigation underscores the Defense Criminal Investigative Service’s (DCIS) commitment to protecting the integrity of the TRICARE program and ensuring that taxpayer-funded military health benefits are not exploited for personal gain,” said Special Agent in Charge Jason Sargenski of DCIS’s Southeast Field Office. &nbsp;“Fraud schemes that siphon resources from TRICARE directly undermine the care promised to service members and their families.&nbsp;As the criminal investigative arm of DoD’s Office of Inspector General, DCIS remains focused on disrupting these schemes and holding responsible parties accountable.”</p><p>According to court documents and evidence presented at trial, Gary Cox, 79, of Maricopa County, was the CEO of Power Mobility Doctor Rx, LLC (DMERx). Cox and his co-conspirators targeted hundreds of thousands of Medicare beneficiaries who provided their personally identifiable information and agreed to accept medically unnecessary orthotic braces, pain creams and other items through misleading mailers, television advertisements and calls from offshore call centers. Cox and his co-conspirators owned, controlled and operated DMERx, an internet-based platform that generated false and fraudulent doctors’ orders for these items. As part of the scheme, Cox connected pharmacies, durable medical equipment (DME) suppliers and marketers with telemedicine companies that would accept illegal kickbacks and bribes in exchange for signed doctors’ orders transmitted using the DMERx platform. Cox and his co-conspirators received payments for coordinating these illegal kickback transactions and referring the completed doctors’ orders to the DME suppliers, pharmacies and telemarketers that paid kickbacks and bribes for the orders.</p><p>The fraudulent doctors’ orders generated by DMERx falsely represented that a doctor had examined and treated the Medicare beneficiaries when, in fact, purported telemedicine companies paid doctors to sign the orders without regard to medical necessity, based only on a brief telephone call with the beneficiary or no interaction with the beneficiary at all. The DME suppliers and pharmacies that paid illegal kickbacks in exchange for these doctors’ orders billed Medicare and other insurers more than $1&nbsp;billion, and Medicare and the insurers paid more than $360 million based on these claims. According to evidence presented at trial, Cox and his co-conspirators concealed the scheme through sham contracts and by eliminating from doctors’ orders what one co-conspirator described as “dangerous words” that might cause Medicare to audit the scheme’s DME suppliers.</p><p>In June 2025, Cox was&nbsp;<a href="https://www.justice.gov/usao-sdfl/pr/ceo-health-care-software-company-convicted-1b-fraud-conspiracy">convicted</a> of conspiracy to commit health care fraud and wire fraud, three counts of health care fraud, conspiracy to pay and receive health care kickbacks and conspiracy to defraud the United States and make false statements in connection with health care matters.</p><p>The FBI, HHS-OIG, VA-OIG and DCIS investigated the case.</p><p>Trial Attorneys Darren C. Halverson and Jennifer E. Burns of the Criminal Division’s Fraud Section prosecuted the case. Fraud Section Trial Attorney Shane Butland assisted in the prosecution. Trial Attorney Evan N. Schlom with the Fraud Section’s Special Matters Unit provided valuable assistance.</p><p>The Fraud Section leads the Criminal Division’s efforts to combat health care fraud through the Health Care Fraud Strike Force Program. Since March 2007, this program, currently comprised of nine strike forces operating in 27 federal districts, has charged more than 5,800 defendants who collectively have billed federal health care programs and private insurers more than $30 billion. In addition, the Centers for Medicare &amp; Medicaid Services, working in conjunction with HHS-OIG, are taking steps to hold providers accountable for their involvement in health care fraud schemes. More information can be found at&nbsp;<a href="http://www.justice.gov/criminal-fraud/health-care-fraud-unit">www.justice.gov/criminal-fraud/health-care-fraud-unit</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inside the proton, the ‘most complicated thing you could possibly imagine’ (2022) (111 pts)]]></title>
            <link>https://www.quantamagazine.org/inside-the-proton-the-most-complicated-thing-imaginable-20221019/</link>
            <guid>46398666</guid>
            <pubDate>Sat, 27 Dec 2025 03:00:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/inside-the-proton-the-most-complicated-thing-imaginable-20221019/">https://www.quantamagazine.org/inside-the-proton-the-most-complicated-thing-imaginable-20221019/</a>, See on <a href="https://news.ycombinator.com/item?id=46398666">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody">
                <div>
        
    <h2>Inside the Proton, the ‘Most Complicated Thing You Could Possibly Imagine’</h2>
    
    <div>
        <p>
            The positively charged particle at the heart of the atom is an object of unspeakable complexity, one that changes its appearance depending on how it is probed. We’ve attempted to connect the proton’s many faces to form the most complete picture yet.          </p>
        
    </div>
    </div>
    <figure>
        <div>
                            <p><img width="2560" height="1440" src="https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-scaled.webp" alt="" decoding="async" fetchpriority="high" srcset="https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-scaled.webp 2560w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-1720x968.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-520x293.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-768x432.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-1536x864.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_2880x1620_Lede-2048x1152.webp 2048w" sizes="(max-width: 2560px) 100vw, 2560px">                </p>
                        </div>
        <figcaption>
    <div>
                            <p>Researchers recently discovered that the proton sometimes includes a charm quark and charm antiquark, colossal particles that are each heavier than the proton itself.</p>
            <p data-pm-slice="1 1 []">Samuel Velasco/Quanta Magazine</p>
        </div>
</figcaption>
    </figure>
<div>
            <h2>Introduction</h2>
            <div data-role="selectable">
    <p>More than a century after Ernest Rutherford discovered the positively charged particle at the heart of every atom, physicists are still struggling to fully understand the proton.</p>
<p>High school physics teachers describe them as featureless balls with one unit each of positive electric charge — the perfect foils for the negatively charged electrons that buzz around them. College students learn that the ball is actually a bundle of three elementary particles called quarks. But decades of research have revealed a deeper truth, one that’s too bizarre to fully capture with words or images.</p>
<p>“This is the most complicated thing that you could possibly imagine,” said <a href="https://physics.mit.edu/faculty/michael-williams/">Mike Williams</a>, a physicist at the Massachusetts Institute of Technology. “In fact, you can’t even imagine how complicated it is.”</p>
<p>The proton is a quantum mechanical object that exists as a haze of probabilities until an experiment forces it to take a concrete form. And its forms differ drastically depending on how researchers set up their experiment. Connecting the particle’s many faces has been the work of generations. “We’re kind of just starting to understand this system in a complete way,” said <a href="https://physics.mit.edu/faculty/richard-milner/">Richard Milner</a>, a nuclear physicist at MIT.</p>
        
        
<p>As the pursuit continues, the proton’s secrets keep tumbling out. Most recently, a <a href="https://www.nature.com/articles/s41586-022-04998-2">monumental data analysis</a> published in August found that the proton contains traces of particles called charm quarks that are heavier than the proton itself.</p>
<p>The proton “has been humbling to humans,” Williams said. “Every time you think you kind of have a handle on it, it throws you some curveballs.”</p>
<p>Recently, Milner, together with Rolf Ent at Jefferson Lab, MIT filmmakers Chris Boebel and Joe McMaster, and animator James LaPlante, set out to transform a set of arcane plots that compile the results of hundreds of experiments into a series of animations of the shape-shifting proton. We’ve incorporated their animations into our own attempt to unveil its secrets.</p>
<h2><strong>Cracking Open the Proton</strong></h2>
<p>Proof that the proton contains multitudes came from the Stanford Linear Accelerator Center (SLAC) in 1967. In earlier experiments, researchers had pelted it with electrons and watched them ricochet off like billiard balls. But SLAC could hurl electrons more forcefully, and researchers saw that they bounced back differently. The electrons were hitting the proton hard enough to shatter it — a process called deep inelastic scattering — and were rebounding from point-like shards of the proton called quarks. “That was the first evidence that quarks actually exist,” said <a href="https://www.phys.virginia.edu/People/personal.asp?UID=xz5y">Xiaochao Zheng</a>, a physicist at the University of Virginia.</p>
<p>After SLAC’s discovery, which won the Nobel Prize in Physics in 1990, scrutiny of the proton intensified. Physicists have carried out hundreds of scattering experiments to date. They infer various aspects of the object’s interior by adjusting how forcefully they bombard it and by choosing which scattered particles they collect in the aftermath.</p>
</div>
    </div>
    <figure>
        <div>
                            <p><img src="https://www.quantamagazine.org/wp-content/uploads/2022/10/Proton_gun_fix_560-Mobile.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2022/10/Proton_gun_fix_920-Desktop.svg" alt="" decoding="async">                </p>
                        </div>
            </figure>
<figure>
    
</figure>
<div data-role="selectable">
    <p>By using higher-energy electrons, physicists can ferret out finer features of the target proton. In this way, the electron energy sets the maximum resolving power of a deep inelastic scattering experiment. More powerful particle colliders offer a sharper view of the proton.</p>
<p>Higher-energy colliders also produce a wider array of collision outcomes, letting researchers choose different subsets of the outgoing electrons to analyze. This flexibility has proved key to understanding quarks, which careen about inside the proton with different amounts of momentum.</p>
<p>By measuring the energy and trajectory of each scattered electron, researchers can tell if it has glanced off a quark carrying a large chunk of the proton’s total momentum or just a smidgen. Through repeated collisions, they can take something like a census — determining whether the proton’s momentum is mostly bound up in a few quarks, or distributed over many.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_STATES_FIX_3-Desktop.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2022/10/PROTON_STATES_FIX_3-Mobile.svg" alt="" decoding="async">    </p>
    </figure>

<p>Even SLAC’s proton-splitting collisions were gentle by today’s standards. In those scattering events, electrons often shot out in ways suggesting that they had crashed into quarks carrying a third of the proton’s total momentum. The finding matched a theory from Murray Gell-Mann and George Zweig, who in 1964 posited that a proton consists of three quarks.</p>
<p>Gell-Mann and Zweig’s “quark model” remains an elegant way to imagine the proton. It has two “up” quarks with electric charges of +2/3 each and one “down” quark with a charge of −1/3, for a total proton charge of +1.</p>
</div>
<figure>
    
</figure>
<div data-role="selectable">
    <p>But the quark model is an oversimplification that has serious shortcomings.</p>
<p>It fails, for instance, when it comes to a proton’s spin, a quantum property analogous to angular momentum. The proton has half a unit of spin, as do each of its up and down quarks. Physicists initially supposed that — in a calculation echoing the simple charge arithmetic — the half-units of the two up quarks minus that of the down quark must equal half a unit for the proton as a whole. But in 1988, the European Muon Collaboration <a href="https://www.sciencedirect.com/science/article/abs/pii/0370269388915237?via%3Dihub">reported</a> that the quark spins add up to far less than one-half. Similarly, the masses of two up quarks and one down quark only comprise about 1% of the proton’s total mass. These deficits drove home a point physicists were already coming to appreciate: The proton is much more than three quarks.</p>
<h2><strong>Much More Than Three Quarks</strong></h2>
<p>The Hadron-Electron&nbsp;Ring&nbsp;Accelerator (HERA), which operated in Hamburg, Germany, from 1992 to 2007, slammed electrons into protons roughly a thousand times more forcefully than SLAC had. In HERA experiments, physicists could select electrons that had bounced off of extremely low-momentum quarks, including ones carrying as little as 0.005% of the proton’s total momentum. And detect them they did: HERA’s electrons rebounded from a maelstrom of low-momentum quarks and their antimatter counterparts, antiquarks.</p>
</div>
<figure>
    
</figure>
<div data-role="selectable">
    <p>The results confirmed a sophisticated and outlandish theory that had by then replaced Gell-Mann and Zweig’s quark model. Developed in the 1970s, it was a quantum theory of the “strong force” that acts between quarks. The theory describes quarks as being roped together by force-carrying particles called gluons. Each quark and each gluon has one of three types of “color” charge, labeled red, green and blue; these color-charged particles naturally tug on each other and form a group — such as a proton — whose colors add up to a neutral white. The colorful theory became known as quantum chromodynamics, or QCD.</p>
<p>According to QCD, gluons can pick up momentary spikes of energy. With this energy, a gluon splits into a quark and an antiquark — each carrying just a tiny bit of momentum — before the pair annihilates and disappears. It’s this “sea” of transient gluons, quarks and antiquarks that HERA, with its greater sensitivity to lower-momentum particles, detected firsthand.</p>
<p>HERA also picked up hints of what the proton would look like in more powerful colliders. As physicists adjusted HERA to look for lower-momentum quarks, these quarks — which come from gluons — showed up in greater and greater numbers. The results suggested that in even higher-energy collisions, the proton would appear as a cloud made up almost entirely of gluons.</p>
</div>
<figure>
    
</figure>
<div data-role="selectable">
    <p>The gluon dandelion is exactly what QCD predicts. “The HERA data are direct experimental proof that QCD describes nature,” Milner said.</p>
<p>But the young theory’s victory came with a bitter pill: While QCD beautifully described the dance of short-lived quarks and gluons revealed by HERA’s extreme collisions, the theory is useless for understanding the three long-lasting quarks seen in SLAC’s gentle bombardment.</p>
<p>QCD’s predictions are easy to understand only when the strong force is relatively weak. And the strong force weakens only when quarks are extremely close together, as they are in short-lived quark-antiquark pairs. Frank Wilczek, David Gross and David Politzer identified this defining feature of QCD in 1973, winning the Nobel Prize for it 31 years later.</p>
<p>But for gentler collisions like SLAC’s, where the proton acts like three quarks that mutually keep their distance, these quarks pull on each other strongly enough that QCD calculations become impossible. Thus, the task of further demystifying the three-quark view of the proton has fallen largely to experimentalists. (Researchers who run “digital experiments,” in which QCD predictions are simulated on supercomputers, have also made <a href="https://www.quantamagazine.org/impossible-particle-discovery-adds-key-piece-to-the-strong-force-puzzle-20210927/">key contributions</a>.) And it’s in this low-resolution picture that physicists keep finding surprises.</p>
<h2><strong>A Charming New View</strong></h2>
<p>Recently, a team led by <a href="https://research.vu.nl/en/persons/juan-rojo">Juan Rojo</a> of the National Institute for Subatomic Physics in the Netherlands and VU University Amsterdam analyzed more than 5,000 proton snapshots taken over the last 50 years, using machine learning to infer the motions of quarks and gluons inside the proton in a way that sidesteps theoretical guesswork.</p>
<p><strong><em>&nbsp;</em></strong>The new scrutiny picked up a background blur in the images that had escaped past researchers. In relatively soft collisions just barely breaking the proton open, most of the momentum was locked up in the usual three quarks: two ups and a down. But a small amount of momentum appeared to come from a “charm” quark and charm antiquark — colossal elementary particles that each outweigh the entire proton by more than one-third.</p>
</div>
<figure>
    <div>
        
                    
            </div>
</figure>
<div data-role="selectable">
    <p>Short-lived charms frequently show up in the “quark sea” view of the proton (gluons can split into any of six different quark types if they have enough energy). But the results from Rojo and colleagues suggest that the charms have a more permanent presence, making them detectable in gentler collisions. In these collisions, the proton appears as a quantum mixture, or superposition, of multiple states: An electron usually encounters the three lightweight quarks. But it will occasionally encounter a rarer “molecule” of five quarks, such as an up, down and charm quark grouped on one side and an up quark and charm antiquark on the other.</p>
<p>Such subtle details about the proton’s makeup could prove consequential. At the Large Hadron Collider, physicists search for new elementary particles by bashing high-speed protons together and seeing what pops out; to understand the results, researchers need to know what’s in a proton to begin with. The occasional apparition of giant charm quarks would <a href="https://arxiv.org/abs/1512.06666">throw off the odds</a> of making more exotic particles.</p>
<p>And when protons called cosmic rays hurtle here from outer space and slam into protons in Earth’s atmosphere, charm quarks popping up at the right moments would shower Earth with <a href="https://arxiv.org/abs/2107.13852">extra-energetic neutrinos</a>, researchers calculated in 2021. These could confound observers <a href="https://www.quantamagazine.org/cosmic-map-of-ultrahigh-energy-particles-points-to-long-hidden-treasures-20210427/">searching</a> for high-energy neutrinos coming from across the cosmos.</p>
<p>Rojo’s collaboration plans to continue exploring the proton by searching for an imbalance between charm quarks and antiquarks. And heavier constituents, such as the top quark, could make even rarer and harder-to-detect appearances.</p>
<p>Next-generation experiments will seek still more unknown features. Physicists at Brookhaven National Laboratory hope to fire up the Electron-Ion Collider in the 2030s and pick up where HERA left off, taking higher-resolution snapshots that will enable the first 3D reconstructions of the proton. The EIC will also use spinning electrons to create detailed maps of the spins of the internal quarks and gluons, just as SLAC and HERA mapped out their momentums. This should help researchers to finally pin down the origin of the proton’s spin, and to address other fundamental questions about the baffling particle that makes up most of our everyday world.</p>

<p><em><strong>Correction:</strong> October 20, 2022</em><br>
<em>A previous version of the article erroneously implied that lower-momentum quarks live shorter lives than higher-momentum quarks in the quark sea. The text has been updated to clarify that all these quarks are lower-momentum and shorter-lived than those in the three quark-picture.</em></p>
</div>
                
                
            </div><div>
        <div data-name="next-post__image-wrapper">
    <p><img width="1720" height="729" src="https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-1720x729.webp" alt="Harry Halpin in a T-shirt and sports coat outdoors" decoding="async" srcset="https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-1720x729.webp 1720w, https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-520x220.webp 520w, https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-768x325.webp 768w, https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-1536x651.webp 1536w, https://www.quantamagazine.org/wp-content/uploads/2022/10/Harry-Halpin_2880x1220_HP-2048x868.webp 2048w" sizes="(max-width: 1720px) 100vw, 1720px">    </p>
</div>
        
        <div>
                <h2>Next article</h2>
                <p>The Computer Scientist Who’s Boosting Privacy on the Internet</p>
            </div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[QNX Self-Hosted Developer Desktop (256 pts)]]></title>
            <link>https://devblog.qnx.com/qnx-self-hosted-developer-desktop-initial-release/</link>
            <guid>46398201</guid>
            <pubDate>Sat, 27 Dec 2025 01:16:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblog.qnx.com/qnx-self-hosted-developer-desktop-initial-release/">https://devblog.qnx.com/qnx-self-hosted-developer-desktop-initial-release/</a>, See on <a href="https://news.ycombinator.com/item?id=46398201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://devblog.qnx.com/tag/news/">News</a>
            
                <p>Try out the initial release of the QNX Developer Desktop -- a self-hosted development environment for QNX. No more cross-compilation!</p>

            <div>
                <p><a href="https://devblog.qnx.com/author/johnatqnx/">
                                <img src="https://devblog.qnx.com/content/images/size/w160/2025/01/me_avatar_good_cropped_more.png" alt="JohnAtQNX">
                            </a>
                </p>
                
            </div>

                <figure>
        <img srcset="https://devblog.qnx.com/content/images/size/w320/2025/12/desktop-preview1-1.png 320w,
                    https://devblog.qnx.com/content/images/size/w600/2025/12/desktop-preview1-1.png 600w,
                    https://devblog.qnx.com/content/images/size/w960/2025/12/desktop-preview1-1.png 960w,
                    https://devblog.qnx.com/content/images/size/w1200/2025/12/desktop-preview1-1.png 1200w,
                    https://devblog.qnx.com/content/images/size/w2000/2025/12/desktop-preview1-1.png 2000w" sizes="(max-width: 1200px) 100vw, 1120px" src="https://devblog.qnx.com/content/images/size/w1200/2025/12/desktop-preview1-1.png" alt="QNX Self-Hosted Developer Desktop -- Initial Release">
    </figure>

        </header>

        <section>
            <p>The team and I are beyond excited to share what we've been cooking up over the last little while: <strong>a full desktop environment running on QNX 8.0, with support for self-hosted compilation</strong>! This environment both makes it easier for newly-minted QNX developers to get started with building for QNX, but it also vastly simplifies the process of porting Linux applications and libraries to QNX 8.0.</p><p>This self-hosted target environment is pre-loaded with many of the ports you'll find on <a href="https://oss.qnx.com/?ref=devblog.qnx.com" rel="noreferrer">the QNX Open-source Dashboard</a>. (The portal currently includes over 1,400 ports across various targets, QNX versions, and architectures, of which more than 600 are unique ports!)</p><p>In this initial release, you can grab a copy of the QEMU image and give it a try for yourself. There's still so much more to add, but it's in a great place today for this first release. The team is really passionate about this one, and we're eagerly looking forward to your feedback!</p><h2 id="whats-included">What's Included</h2><p>For the initial release of Desktop, we tried to cover all the basics: windowing, terminal, IDEs, browser, file management, and samples. To that end, here's what makes up the QNX Developer Desktop:</p><ul><li>A customizable XFCE desktop environment running on Wayland</li><li>The tools you need to compile and/or run your code (<code>clang</code>, gcc, <code>clang++</code>, Python, <code>make</code>, <code>cmake</code>, <code>git</code>, etc)</li><li>A web browser (can you join <a href="https://discord.gg/Jj4EkkrFTT?ref=devblog.qnx.com" rel="noreferrer">the QNX Discord</a> from the QNX Desktop? 🏅👀)</li><li>Ports of popular IDEs/editors, like Geany, Emacs, Neovim, and vim</li><li>Thunar, for file management</li><li>Preloaded samples, like Hello World in C, C++, and Python, and GTK demos OpenGL ES demos</li><li>... and of course, a terminal.</li></ul><h2 id="system-requirements">System Requirements</h2><p>This environment runs as a virtual machine, using QEMU on Ubuntu. To try the image, you'll need:</p><ul><li>Ubuntu 22.04 or 24.04</li></ul><h2 id="try-it-yourself">Try It Yourself</h2><p>(Keep in mind this is the first release, so it takes a minute to get started and it's a bit rough around the edges.)</p><p>With <a href="https://qnx.com/getqnx?ref=devblog.qnx.com" rel="noreferrer">a free QNX license</a>, you can find this release in QNX Software Center. On the <strong>Available</strong> tab of the <strong>Manage Installation</strong> pane, search for "quick start" and install the "QNX SDP 8.0 Quick Start Target Image for QEMU".</p><p>You'll find the image in your QNX installation directory, usually <code>~/qnx800/images</code> by default. Follow the <code>README.md</code> file in the <code>qemu</code> directory to extract &amp; combine the multiple QNX packages downloaded under the hood.</p><p>Next, follow the PDF instructions found in the new <code>./qemu_qsti/docs/</code> directory to install the required dependencies and boot up.</p><div><p>💡</p><p>If you experience any trouble starting the environment, check the PDF's <b><strong>Troubleshooting</strong></b> chapter, or come <a href="https://discord.gg/Jj4EkkrFTT?ref=devblog.qnx.com" rel="noreferrer">ask us on Discord</a>.</p></div><h2 id="whats-next">What's Next</h2><p>This is just the very first release! Over the next few months and beyond, we'll drop more updates of Desktop. You can look forward to:</p><ul><li>QEMU images for Windows &amp; macOS, and native images for x86</li><li>A native Desktop image on Raspberry Pi</li><li>Enhanced documentation</li><li>Features to help use this self-hosted environment in CI jobs</li><li>More samples &amp; stability</li><li>... and more! Have suggestions? Let us know.</li></ul><p>Lastly, if you want some help with your QNX journey, you can find the QNX team and community:</p><ul><li>in Discord here: <a href="https://discord.gg/Jj4EkkrFTT?ref=devblog.qnx.com" rel="noreferrer">discord.gg/Jj4EkkrFTT</a></li><li>on Reddit at: <a href="https://reddit.com/r/qnx?ref=devblog.qnx.com" rel="noreferrer">reddit.com/r/qnx</a></li></ul>
        </section>

    </article>

        

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Publishing your work increases your luck (266 pts)]]></title>
            <link>https://github.com/readme/guides/publishing-your-work</link>
            <guid>46397991</guid>
            <pubDate>Sat, 27 Dec 2025 00:43:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/readme/guides/publishing-your-work">https://github.com/readme/guides/publishing-your-work</a>, See on <a href="https://news.ycombinator.com/item?id=46397991">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="">



        <p>No matter how hard you work, it still takes a little bit of luck for something to hit. That can be discouraging, since luck feels like a force outside our control. But the good news is that we can increase our chances of encountering good luck. That may sound like magic, but it’s not supernatural. The trick is to increase the number of opportunities we have for good fortune to find us. The simple act of publishing your work is one of the best ways to invite a little more luck into your life.</p>
<p>Before we get into the “how,” it’s important to get on the same page about the “what.” What are we talking about when we say “luck?” There are a lot of definitions that could apply, but let’s stick with a simple one: Luck is when something unexpected and good happens to you. Unexpected and good. Who doesn’t want to increase the odds of something unexpected and good?</p>
<p>In our world, luck can include:</p>
<ul><li><p>Having your OSS library take off</p></li><li><p>Being invited to speak at a conference</p></li><li><p>Landing a new job</p></li><li><p>Getting a new consulting client</p></li><li><p>Being invited onto a podcast</p></li><li><p>Making new friends in your community</p></li></ul>
<p>None of these things are totally in your control, which can at times feel frustrating.&nbsp;</p>
<p>How can we increase the odds of finding luck? By being a person who works in public. By doing work and being public about it, you build a reputation for yourself. You build a track record. You build a public body of work that speaks on your behalf better than any resume ever could.</p>
<p>The goal is not to become famous, the goal is to increase the chances of luck finding us. For me, one of the most helpful ways to think about this has always been the concept of the “Luck Surface Area,” described in an <a href="https://www.codusoperandi.com/posts/increasing-your-luck-surface-area"><u>old post by Jason Roberts</u></a>. He wrote (and note, the emphasis is mine):&nbsp;</p>
<p><i>"The amount of serendipity that will occur in your life, your Luck Surface Area, is directly proportional to the degree to which you </i><i><b>do something</b></i><i> you’re passionate about combined with the total number of people to whom this is </i><i><b>effectively communicated</b></i><i>."</i></p>
<p>Going further, he codifies it into a formula where:</p>
<div tabindex="0">
        <pre><span><span>Luck</span> <span>=</span> <span>[</span><span>Doing</span> <span>Things</span><span>]</span> <span>*</span> <span>[</span><span>Telling</span> <span>People</span><span>]</span></span></pre>

  </div>


<p>The more things you do multiplied by the more people you tell, the larger your Luck Surface Area becomes. The larger your Luck Surface Area, the more likely you are to catch luck as it flows by.</p>
<picture>
  <source srcset="https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=468&amp;fm=avif 468w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=374&amp;fm=avif 374w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=280&amp;fm=avif 280w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=234&amp;fm=avif 234w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=187&amp;fm=avif 187w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=93&amp;fm=avif 93w" sizes="(max-width: 930px) 90vw, 840px" type="image/avif">
  <source srcset="https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=468&amp;fm=webp 468w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=374&amp;fm=webp 374w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=280&amp;fm=webp 280w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=234&amp;fm=webp 234w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=187&amp;fm=webp 187w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=93&amp;fm=webp 93w" sizes="(max-width: 930px) 90vw, 840px" type="image/webp">
  <source srcset="https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=468&amp;fm=jpg 468w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=374&amp;fm=jpg 374w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=280&amp;fm=jpg 280w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=234&amp;fm=jpg 234w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=187&amp;fm=jpg 187w,https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=93&amp;fm=jpg 93w" sizes="(max-width: 930px) 90vw, 840px" type="image/jpeg">
  <img width="468" height="342" loading="lazy" decoding="async" alt="A graph with &quot;Doing&quot; on the Y axis, and &quot;Telling&quot; on the X axis. The more you do and tell, the better." src="https://images.ctfassets.net/s5uo95nf6njh/1mf4WuCw33UGsrQTdkErFe/1b5c74f7b54deb664b6bb39aa6303d35/AF_inline.jpg?w=468&amp;fm=jpg">
</picture>
<h3>Source:<a href="https://www.codusoperandi.com/posts/increasing-your-luck-surface-area"><u> Jason Roberts</u></a></h3>
<hr>

<h2>Doing the work</h2>
<p>Before you can publish your work, you have to actually <i>do</i> the work. The good news for you is that by even reading this Guide on The ReadME Project, you’ve probably already self-selected into a group of people for whom “doing things” comes somewhat naturally. You’re a developer, a designer, a creator, an author, or something else entirely.&nbsp; Whatever moniker you want to give yourself, you’re built to <i>do</i> things, and that’s the important part.&nbsp;</p>
<p>If that doesn’t ring true for you, you may fall into one of two groups:</p>
<ol><li><p>You actually <i>are</i> doing things, you’ve just trained yourself to think that anything you do isn’t worth sharing.</p></li><li><p>You <i>want</i> to be doing things, but you can’t bring yourself to get started.</p></li></ol>
<p>If you’re in the first group, you may need to step back and reframe the work you’re already doing. This is a common blind spot for people who are executing at a high level! They’ve forgotten just how much they know. They think that they’re not doing anything interesting because they assume that everyone knows as much as they do. This effect is only exacerbated when everyone in your immediate vicinity is at a similar—or higher—skill level. As you become more of an expert, your quality bar gets higher and higher and you forget that everything you know is not known by everyone.</p>
<p>If you’re in this group I want to give you a challenge: Watch the communities where you hang out and see what people are sharing and what gets noticed. Is it something you could have done? Is it something you’ve <i>already</i> done? At its worst this could lead you in the direction of becoming bitter, critical, and thinking that you’re smarter than everyone. To that I say “resist!” There is no life there. My encouragement to you is to view that as objective evidence that people want to know all of the things that you already know! There is a huge opportunity for you, should you decide to start sharing your work.&nbsp;</p>
<p>If you’re in the second group, you just need to start. Start anywhere, start on anything, start something. You’ll never come up with the perfect idea for an OSS library, a business, a podcast, or an article by just thinking about it. Start on something, today. It won’t be the perfect version of the thing you have in your head, but you’ll be in motion. Motion begets motion, progress begets progress. Pick the smallest thing you can do and get started.</p>
<p>Doing the work is the most important part. It’s the nucleus around which everything else revolves. What that “work” looks like, though, is entirely up to you! That’s the fun part. It can take any form and be in any domain. Wherever your curiosity or expertise draw you, dive into that.&nbsp;</p>
<p>Projects outside of work are a good place to dive into your curiosity.&nbsp;</p>
<ul><li><p>If you want to make<a href="https://twitter.com/aschmelyun/status/1506960015063625733"><u> a thermal receipt printer that prints GitHub issues</u></a>, you should.&nbsp;</p></li><li><p>If you want to<a href="https://twitter.com/aarondfrancis/status/1333866090573811723"><u> turn a prefabricated shed into an office</u></a>, go for it.&nbsp;</p></li><li><p>If you want to go all in on an<a href="https://twitter.com/steveruizok/status/1419048412431933441"><u> SVG drawing tool</u></a>, do it.&nbsp;</p></li><li><p>If you want to write tens of thousands of words about <a href="https://bam.kalzumeus.com/archive/"><u>the infrastructure of modern money</u></a>, that’s a newsletter.&nbsp;</p></li></ul>
<p>Your curiosity will naturally pull you in certain directions, so don’t be afraid to go super deep into a topic that you’re interested in. When a person is truly interested in the thing they’re writing or talking about, their excitement is contagious. Whatever you’re excited about, be excited about it publicly. Whatever you’re curious about, be curious about it publicly. People will want to follow along and you’ll inspire people along the way.</p>
<p>Projects at work can be a good place to dive into your expertise.&nbsp;</p>
<p>It's likely you're constantly solving problems and learning interesting things at your job. This is a great opportunity to take what you’re already doing and repurpose it for the benefit of others. You can turn those learnings into blog posts, conference talks, meetups, podcasts, or open source projects.&nbsp;</p>
<p>Of course not everything you do at work is shareable. If the specifics aren’t shareable, the concepts, lessons, and takeaways likely are. While you’re working, keep a scratch pad open and jot down any problems you come across, interesting patterns you see, or things you found confusing. Do this for a month and you’ll have more things to share than you know what to do with!</p>
<p>You’ve done the work, now it's time to tell people.</p>


<h2>Hitting the publish button</h2>
<p>This part of the formula can be harder for most of us. Most of us really enjoy the building aspect but start to get a little shy when it comes to telling people about the stuff we’ve built. That could be for any number of reasons: fear, embarrassment, self-preservation, or an aversion to being perceived as hawking your wares.</p>
<p>It’s a valuable exercise to investigate whether or not you resonate with any of those reasons. Are you afraid people are going to make fun of what you built? Are you embarrassed that it isn’t up to your own (admittedly high) standards? Are you waiting for some elusive perfect moment? Do you have an aversion to “marketing” and don’t want to become the thing you hate? Whatever it is for you, I encourage you to really dig into it and see if that fear is worth keeping around.</p>
<p>Sharing things you’re learning or making is not prideful. People are drawn to other people in motion. People <i>want</i> to follow along, people <i>want</i> to learn things, people <i>want</i> to be a part of your journey. It’s not bragging to say, “I’ve made a thing and I think it’s cool!” Bringing people along is a good thing for everyone. By publishing your work you’re helping people learn. You’re inspiring others to create.</p>
<p>You can “publish” anywhere. For me that’s mostly Twitter because that’s where most of my peers hang out. It doesn’t have to be Twitter for you. It could be GitHub, a newsletter, a podcast, forums, your blog, YouTube, or something completely different that’s not even on my radar. Anywhere that’s not your hard drive counts!&nbsp;</p>
<p>Publishing is a skill, it’s something you can learn. You’ll need to build your publishing skill just like you built every other skill you have.&nbsp;</p>
<p>Don’t be afraid to publish along the way. You don’t have to wait until you’re done to drop a perfect, finished artifact from the sky (in fact, you may use that as an excuse to <i>never</i> publish). People like stories, so use that to your benefit. Share the wins, the losses, and the thought processes. Bring us along! If you haven’t been in the habit of sharing your work, it’s going to feel weird when you start. That’s normal! Keep going, you get used to it.&nbsp;</p>
<p>You’ve done the work. You’ve hit the publish button. You’ve done your part!&nbsp;</p>
<h2>Capturing the luck</h2>
<p>You’ve <i>increased the odds</i> that good, unexpected things will come your way. The exact form is hard to predict, but here are a few potential outcomes:&nbsp;</p>
<ul><li><p>People start to know you as the person that talks about X, Y, and Z.&nbsp;</p></li><li><p>You start to get emails from people saying that they read your stuff and liked it.&nbsp;</p></li><li><p>You get a DM about a job you might be interested in.</p></li><li><p>People ask you if you’re taking on new clients.</p></li><li><p>Someone you’ve never met or interacted with will mention you as being an expert in your area.&nbsp;</p></li><li><p>A meetup asks you to come talk about the things you’ve been sharing.</p></li><li><p>You become friends with other people in your industry.</p></li><li><p>Your OSS library starts gaining mindshare.</p></li></ul>
<p>This is not a random list of made-up examples, it’s a list of things that have literally happened to me once I got over my fears and started sharing my work. I had been doing the work all along, but was too afraid to publish. Once I overcame that fear, my Luck Surface Area expanded and good, unexpected things started happening.&nbsp;</p>
<p>The formula is simple.</p>
<p>Do the work. Don’t be afraid to dive deep into your curiosity and your expertise. We need more people that are intensely curious. We need more people with deep expertise.</p>
<p>Tell people. Press publish, bring us along, share the journey. Tell us what you’ve learned, what you’ve built, or what you’re excited about.</p>
<p>The formula may be simple, but I’ll admit it’s not always easy. It’s scary to put yourself out there. It’s hard to open yourself up to criticism. People online can be mean. But for every snarky comment, there are ten times as many people quietly following along and admiring not only your work, but your bravery to put it out publicly. And at some point, one of those people quietly following along will reach out with a life-changing opportunity and you’ll think, “Wow, that was lucky.”</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exe.dev (393 pts)]]></title>
            <link>https://exe.dev/</link>
            <guid>46397609</guid>
            <pubDate>Fri, 26 Dec 2025 23:42:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://exe.dev/">https://exe.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=46397609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span>ssh exe.dev <span>_</span></span>
        </p>
        <p>
            The disk persists. You have sudo.
        </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Always Bet on Text (325 pts)]]></title>
            <link>https://graydon2.dreamwidth.org/193447.html</link>
            <guid>46397379</guid>
            <pubDate>Fri, 26 Dec 2025 23:09:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://graydon2.dreamwidth.org/193447.html">https://graydon2.dreamwidth.org/193447.html</a>, See on <a href="https://news.ycombinator.com/item?id=46397379">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I figured I should just post this somewhere so I can make future reference to how I feel about the matter, anytime someone asks me about such-and-such video, 3D, game or "dynamic" multimedia system. Don't get me wrong, I like me some illustrations, photos, movies and music.</p><p>But text wins by a mile. Text is everything. My thoughts on this are quite absolute: <em>text is the most powerful, useful, effective communication technology ever</em>, period.</p><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/The_oldest_writing_in_the_world_-_The_Sumerian_Stone_Tablet.jpg/220px-The_oldest_writing_in_the_world_-_The_Sumerian_Stone_Tablet.jpg">Text is the <em>oldest and most stable</em> communication technology (assuming we treat speech/signing as natural phenomenon -- there are no human societies without it -- whereas textual capability has to be transmitted, taught, acquired) and it's incredibly <em>durable</em>. We can read texts from <em>five thousand years ago</em>, almost the moment they started being produced. It's (literally) "rock solid" -- you can readily inscribe it in granite that will likely outlast the human species.<br></p></div><br><div><p><img src="https://upload.wikimedia.org/math/1/3/e/13e30a48d35ec57948e8a577db7eb787.png">Text is the <em>most flexible</em> communication technology. Pictures may be worth a thousand words, when there's a picture to match what you're trying to say. But let's hit the random button on wikipedia and pick a sentence, see if you can draw a picture to convey it, mm? Here:</p><blockquote>"Human rights are moral principles or norms that describe certain standards of human behaviour, and are regularly protected as legal rights in national and international law."<br></blockquote><p>Not a <em>chance</em>. Text can convey <em>ideas</em> with a precisely controlled level of ambiguity and precision, implied context and elaborated content, unmatched by anything else. It is not a coincidence that all of literature and poetry, history and philosophy, mathematics, logic, programming and engineering rely on textual encodings for their ideas.<br></p></div><br><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Rees%27s_Cyclopaedia_Chappe_telegraph.png/220px-Rees%27s_Cyclopaedia_Chappe_telegraph.png"> Text is the <em>most efficient</em> communication technology. By <em>orders of magnitude</em>. This blog post is likely to take perhaps 5000 bytes of storage, and could compress down to maybe 2000; by comparison the following 20-pixel-square image of the silhouette of a tweeting bird takes 4000 bytes: <img src="https://abs.twimg.com/errors/logo23x19.png">. At every step of communication technology, textual encoding comes first, everything else after. Because it's <em>vastly cheaper</em> on a symbol-by-symbol basis. You have a working <a href="https://en.wikipedia.org/wiki/Optical_telegraph">optical telegraph</a> network running in <em>1790</em> in France. You the better part of a century of <a href="https://en.wikipedia.org/wiki/Electrical_telegraph">electrical telegraphy</a>, trans-oceanic cables and everything, before anyone bothers with trying to carry voice. You have decades of teleprinter and text-only computer networking, mail and news, chat and publishing, editing and diagnostics, before bandwidth gets cheap enough for images, voice and video. You have pagers, SMS, WAP, USSD and blackberries before iPhones. You have <a href="http://en.wikipedia.org/wiki/Teletext">Teletext</a> and BBSs, netnews and gopher before the web. And today many of the best, and certainly the most efficient parts of the web remain text-centric. I can download <em>all of wikipedia</em> and carry it around on the average smartphone.</p></div><br><div><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Gnus-reading-news.png/220px-Gnus-reading-news.png"><br>Text is the most <em>socially useful</em> communication technology. It works <em>well</em> in 1:1, 1:N, and M:N modes. It can be <em>indexed</em> and <em>searched</em> efficiently, even by hand. It can be <em>translated</em>. It can be produced and consumed at variable speeds. It is asynchronous. It can be compared, diffed, clustered, corrected, summarized and filtered algorithmically. It permits multiparty editing. It permits branching conversations, lurking, annotation, quoting, reviewing, summarizing, structured responses, exegesis, even fan fic. The breadth, scale and depth of ways people use text is unmatched by anything. There is no equivalent in <em>any other communication technology</em> for the social, communicative, cognitive and reflective complexity of a library full of books or an internet full of postings. Nothing else comes close.<br></p></div><p>So this is my stance on text: always pick text first. As my old boss might have said: always bet on text. If you can use text for something, use it. It will very seldom let you down.</p></div></div>]]></description>
        </item>
    </channel>
</rss>