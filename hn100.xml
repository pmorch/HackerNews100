<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 10 Jan 2025 19:30:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Meta's memo to employees rolling back DEI programs (198 pts)]]></title>
            <link>https://www.axios.com/2025/01/10/meta-dei-memo-employees-programs</link>
            <guid>42657901</guid>
            <pubDate>Fri, 10 Jan 2025 17:48:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2025/01/10/meta-dei-memo-employees-programs">https://www.axios.com/2025/01/10/meta-dei-memo-employees-programs</a>, See on <a href="https://news.ycombinator.com/item?id=42657901">Hacker News</a></p>
Couldn't get https://www.axios.com/2025/01/10/meta-dei-memo-employees-programs: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Starlink is now cheaper than leading internet provider in some African countries (115 pts)]]></title>
            <link>https://restofworld.org/2025/starlink-cheaper-internet-africa/</link>
            <guid>42657692</guid>
            <pubDate>Fri, 10 Jan 2025 17:24:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2025/starlink-cheaper-internet-africa/">https://restofworld.org/2025/starlink-cheaper-internet-africa/</a>, See on <a href="https://news.ycombinator.com/item?id=42657692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>Starlink, launched in 2019 by Elon Musk’s SpaceX, has become the leading satellite internet provider in the world. Now available in more than 100 countries, Starlink can also be a relatively affordable option for users trying to log on in countries with limited internet service providers. Across Africa, for example, Starlink is sometimes the cheapest way to get online.&nbsp;</p>



<p>A <em>Rest of World</em> analysis indicates that in at least five of the 16 African countries where the service is available, a monthly Starlink subscription is cheaper than the leading fixed internet service provider. That subscription cost does not include the upfront cost of Starlink hardware, which ranges in price and availability from $178 for a Starlink Mini in Kenya to $381 for a Standard Actuated kit in Nigeria.</p>



<p><em>Rest of World</em> determined the leading fixed internet service providers through reports published by each country’s communications authority, and obtained the cheapest prices from each company’s website. Prices have been converted to U.S. dollars to make comparisons.</p>



<p>Starlink does not have fixed rates; instead it intermittently raises and lowers its prices. On its website, it notes, “Starlink may adjust prices over time to reflect market conditions resulting in a decrease or increase of the monthly service plan cost.” SpaceX did not respond to <em>Rest of World</em>’s request for comment.&nbsp;</p>



<p>To conduct this analysis, <em>Rest of World</em> compared the price of Starlink’s residential service to the cheapest unlimited fixed internet plan offered by leading internet service providers on January 9, 2025.</p>



<figure></figure>



<p>Historically, internet connections around the globe have typically been enabled by ground-based internet service providers using fiber-optic cables and mobile base stations. But in many parts of the world, that infrastructure is sparse or nonexistent. “This is where satellite providers come in,” said Nitinder Mohan, a computer science professor at the Delft University of Technology in the Netherlands who has studied Starlink’s performance around the world.</p>



<p>“I can be in the middle of a forest and, if I have a direct view of the sky, I can get my internet connectivity,” he told <em>Rest of World</em>. “Regions which are previously underconnected — where there was no way of getting internet connectivity to them — now with these satellites, you can actually enable that.”&nbsp;</p>



<p>Satellite internet’s reach makes it an important tool for <a href="https://restofworld.org/2024/mobile-internet-users-growth-rate/">getting more people online</a> in areas that are internet-poor.&nbsp;</p>



<p><em>Rest of World </em>identified at least five countries in Africa where Starlink is cheaper than the average price of internet service: Kenya, Ghana, Zimbabwe, Mozambique, and Cape Verde. According to the latest <a href="https://www.itu.int/itu-d/reports/statistics/2024/11/10/ff24-internet-use/">figures</a> by the International Telecommunication Union, a U.N. agency focused on information and communication technologies, 38% of the population in Africa uses the internet, compared to 91% of Europe.&nbsp;</p>



<p>Starlink prices range widely, from $10 in Kenya to $50 in Eswatini. For most countries in Africa, the cheapest available Starlink plan costs between $28 and $34 per month. </p>



<p>Since launching in Kenya in July 2023, Starlink has disrupted the existing internet service provider industry. Starlink offers high connectivity speeds and wide availability in remote areas, along with dramatically lower prices. The company also introduced a rental option.&nbsp;</p>



<p>According to the <a href="https://www.ca.go.ke/sites/default/files/2024-10/Sector%20Statistics%20Report%20Q4%202023-2024.pdf">latest figures</a> published by the Communications Authority of Kenya, as of June 2024, just over 8,000 Kenyans subscribe to Starlink, making it the tenth most popular service provider in the country. While legacy telecom providers like Safaricom and Jamii maintain control of the market — with 546,000 and 360,000 subscribers, respectively — TechCabal, an Africa-focused tech publication, <a href="https://techcabal.com/2024/10/14/starlink-becomes-kenyas-tenth-largest-isp/">noted</a> that Kenya’s adoption of Starlink has been swift and continues to rise rapidly.</p>



<p>Safaricom and other legacy providers have responded by <a href="https://cioafrica.co/safaricom-woos-back-customers-with-cheaper-offer/">lowering prices</a> and <a href="https://techcabal.com/2024/09/23/safaricom-internet-speeds-upgraded/">increasing internet speeds</a>. Tim Hatt, head of research and consulting at GSMA Intelligence, the research wing of the Global Systems for Mobile Communications Association, told <em>Rest of World</em> internet service providers are also developing their own satellite networks. Safaricom’s parent company, Vodacom, for example, recently announced a <a href="https://www.investors.com/news/ast-spacemobile-asts-stock-vodafone-deal-space-satellite-cell-service/">partnership</a> with satellite mobile network AST SpaceMobile to provide satellite internet in Europe and Africa. AST SpaceMobile launched its first satellites with the help of SpaceX.</p>



<figure><blockquote><p>Starlink has become so popular in Kenya that the company&nbsp;paused new subscriptions.</p></blockquote></figure>



<p>In rural Kenya, where Safaricom services are either too expensive or unreliable, Starlink is becoming a choice internet provider for households, Abel Boreto, an investor based in Nairobi, told <em>Rest of World</em>. Boreto became tired of using unreliable internet from Safaricom when he visited his hometown, and switched to Starlink in August. He said installing Starlink has saved him money and time.</p>



<p>“Safaricom was quite on the high side and the internet wasn’t even reliable so I decided to try out Starlink, which is more affordable ($10 per month for 50GB) to subscribe and use in the long term,” Boreto said. “It’s very fast and allows me to also share the internet with my parents and relatives when I’m not there.”</p>



<p>Starlink has become so popular in Kenya that the company <a href="https://www.connectingafrica.com/connectivity/starlink-halts-new-sign-ups-in-kenya">paused new subscriptions</a> in major cities in early November due to network overload. The company plans to deploy more infrastructure in Nairobi and Johannesburg in order to bring more people online, said Mohan, the computer science professor at Delft University.</p>



<p>For Mohan, the global Starlink boom raises monopolization concerns. A single dominant player not only leaves customers vulnerable to price hikes and decreasing quality of service but also gives a single company the power to control internet access for an entire country. Kenyan telecoms have also raised <a href="https://african.business/2024/11/technology-information/starlinks-aggressive-push-in-africa-keeps-telcos-on-high-alert">concerns</a> about Starlink taking market share away from local companies that employ thousands of people on the African continent.</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Web apps built with Ruby on Rails (130 pts)]]></title>
            <link>https://weuserails.com/</link>
            <guid>42656559</guid>
            <pubDate>Fri, 10 Jan 2025 15:39:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://weuserails.com/">https://weuserails.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42656559">Hacker News</a></p>
Couldn't get https://weuserails.com/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Formal Methods: Just Good Engineering Practice? (2024) (105 pts)]]></title>
            <link>https://brooker.co.za/blog/2024/04/17/formal</link>
            <guid>42656433</guid>
            <pubDate>Fri, 10 Jan 2025 15:25:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brooker.co.za/blog/2024/04/17/formal">https://brooker.co.za/blog/2024/04/17/formal</a>, See on <a href="https://news.ycombinator.com/item?id=42656433">Hacker News</a></p>
Couldn't get https://brooker.co.za/blog/2024/04/17/formal: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[I've Acquired a New Superpower (441 pts)]]></title>
            <link>https://danielwirtz.com/blog/spot-the-difference-superpower</link>
            <guid>42655870</guid>
            <pubDate>Fri, 10 Jan 2025 14:34:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danielwirtz.com/blog/spot-the-difference-superpower">https://danielwirtz.com/blog/spot-the-difference-superpower</a>, See on <a href="https://news.ycombinator.com/item?id=42655870">Hacker News</a></p>
Couldn't get https://danielwirtz.com/blog/spot-the-difference-superpower: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Learning How to Think with Meta Chain-of-Thought (120 pts)]]></title>
            <link>https://arxiv.org/abs/2501.04682</link>
            <guid>42655098</guid>
            <pubDate>Fri, 10 Jan 2025 12:37:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2501.04682">https://arxiv.org/abs/2501.04682</a>, See on <a href="https://news.ycombinator.com/item?id=42655098">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiang,+V" rel="nofollow">Violet Xiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Snell,+C" rel="nofollow">Charlie Snell</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gandhi,+K" rel="nofollow">Kanishk Gandhi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Albalak,+A" rel="nofollow">Alon Albalak</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+A" rel="nofollow">Anikait Singh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Blagden,+C" rel="nofollow">Chase Blagden</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Phung,+D" rel="nofollow">Duy Phung</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rafailov,+R" rel="nofollow">Rafael Rafailov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lile,+N" rel="nofollow">Nathan Lile</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mahan,+D" rel="nofollow">Dakota Mahan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Castricato,+L" rel="nofollow">Louis Castricato</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Franken,+J" rel="nofollow">Jan-Philipp Franken</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Haber,+N" rel="nofollow">Nick Haber</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Finn,+C" rel="nofollow">Chelsea Finn</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2501.04682">View PDF</a></p><blockquote>
            <span>Abstract:</span>We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Rafael Rafailov [<a href="https://arxiv.org/show-email/39233bf9/2501.04682" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 8 Jan 2025 18:42:48 UTC (24,263 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Who Can Understand the Proof? A Window on Formalized Mathematics (145 pts)]]></title>
            <link>https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/</link>
            <guid>42654995</guid>
            <pubDate>Fri, 10 Jan 2025 12:21:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/">https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/</a>, See on <a href="https://news.ycombinator.com/item?id=42654995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
<h2 id="the-simplest-axiom-for-logic">The Simplest Axiom for Logic</h2>



<p><a href="https://www.wolframscience.com/nks/p808--implications-for-mathematics-and-its-foundations/">Theorem <span>(Wolfram with Mathematica, 2000)</span></a>: <br>The single axiom <span>((<em>a</em>•<em>b</em>)•<em>c</em>)•(<em>a</em>•((<em>a</em>•<em>c</em>)•<em>a</em>))<span></span><em>c</em></span> is a complete axiom system for Boolean algebra (and is the simplest possible)</p>
<p>For more than a century <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-history">people had wondered</a> how simple the axioms of logic (Boolean algebra) could be. <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#a-discovery-about-basic-logic">On January 29, 2000, I found the answer</a>—and made the surprising discovery that they could be about twice as simple as anyone knew. (I also showed that what I found was <a href="https://www.wolframscience.com/nks/notes-12-9--searching-for-logic-axioms/">the simplest possible</a>.) </p>
<p>It was an interesting result—that gave new intuition about just how simple the foundations of things can be, and for example helped inspire my efforts to find a <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">simple underlying theory of physics</a>. </p>
<p>But how did I get the result? Well, I used automated theorem proving (specifically, what’s now <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> in <a href="https://www.wolfram.com/language/">Wolfram Language</a>). Automated theorem proving is something that’s <a href="https://www.wolframscience.com/nks/notes-12-9--automated-theorem-proving/">been around since at least the 1950s</a>, and its core methods haven’t changed in a long time. But in the rare cases it’s been used in mathematics it’s typically been to confirm things that were already believed to be true. And in fact, to my knowledge, my Boolean algebra axiom is actually the only truly unexpected result that’s ever been found for the first time using automated theorem proving.<span id="more-65170"></span></p>
<p>But, OK, so we know it’s true. And that’s interesting. But what about the proof? Does the proof, for example, show us why the result is true? Well, actually, in a quarter of a century, nobody (including me) has ever made much headway at all in understanding the proof (which, at least in the form we currently know it, is long and complicated). So is that basically inevitable—say as a consequence of <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility">computational irreducibility</a>? Or is there some way—perhaps <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">using modern AI</a>—to “humanize” the proof to a point where one can understand it?</p>
<p>It is, I think, an interesting challenge—that gets at the heart of what one can (and can’t) expect to achieve with formalized mathematics. In what follows, I’ll discuss what I’ve been able to figure out—and how it relates to foundational questions about what mathematics is and how it can be done. And while I think I’ve been able to clarify some of the issues, the core problem is still out there—and I’d like to issue it here as a challenge:</p>
<p><span>Challenge:</span> Understand the proof of the Theorem</p>
<p>What do I mean by “understand”? Inevitably, “understand” has to be defined in human terms. Something like “so a human can follow and reproduce it”—and, with luck, feel like saying “aha!” at some point, the kind of way they might on hearing a proof of the Pythagorean theorem (or, in logic, something like de Morgan’s law <tt><a href="http://reference.wolfram.com/language/ref/Not.html">Not</a></tt>[<tt><a href="http://reference.wolfram.com/language/ref/And.html">And</a></tt>[<em>p</em>, <em>q</em>]]<span></span><tt><a href="http://reference.wolfram.com/language/ref/Or.html">Or</a></tt>[<tt><a href="http://reference.wolfram.com/language/ref/Not.html">Not</a></tt>[<em>p</em>], <tt><a href="http://reference.wolfram.com/language/ref/Not.html">Not</a></tt>[<em>q</em>]]). </p>
<p>It should be said that it’s certainly not clear that such an understanding would ever be possible. After all, as we’ll discuss, it’s a basic metamathematical fact that out of all possible theorems almost none have short proofs, at least in terms of any particular way of stating the proofs. But what about an “interesting theorem” like the one we’re considering here? Maybe that’s different. Or maybe, at least, there’s some way of building out a “higher-level mathematical narrative” for a theorem like this that will take one through the proof in human-accessible steps.</p>
<p>In principle one could always imagine a somewhat bizarre scenario in which people would just rote learn chunks of the proof, perhaps giving each chunk some name (a bit like how people learned bArbArA and cElArEnt syllogisms in the Middle Ages). And in terms of these chunks there’d presumably then be a “human way” to talk about the proof. But learning the chunks—other than as some kind of recreational or devotional activity—doesn’t seem to make much sense unless there’s metamathematical structure that somehow connects the chunks to “general concepts” that are widely useful elsewhere. </p>
<p>But of course it’s still conceivable that there might be a “big theory” that would lead us to the theorem in an “understandable way”. And that could be a traditional mathematical theory, built up with precise, if potentially very abstract, constructs. But what about something more like a theory in natural science? In which we might treat our automatically generated proof as an object for empirical study—exploring its characteristics, trying to get intuition about it, and ultimately trying to deduce the analog of “natural laws” that give us a “human-level” way of understanding it. </p>
<p>Of course, for many purposes it doesn’t really matter why the theorem is true. All that matters is that it is true, and that one can deduce things on the basis of it. But as one thinks about the future of mathematics, and the future of doing mathematics, it’s interesting to explore to what extent it might or might not ultimately be possible to understand in a human-accessible way the kind of seemingly alien result that the theorem represents. </p>
<h2 id="the-proof-as-we-know-it">The Proof as We Know It</h2>
<p>I first presented a version of the proof on <a href="https://www.wolframscience.com/nks/p810--implications-for-mathematics-and-its-foundations/">two pages</a> of my 2002 book <em><a href="https://www.wolframscience.com/nks/">A New Kind of Science</a></em>, printing it in 4-point type to make it fit: </p>
<p><a href="https://files.wolframcdn.com/pub/www.wolframscience.com/nks/nks-ch12-sec9.pdf"><img src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg1.png" alt="Axiom proof" title="Axiom proof" width="619" height="373"></a></p>
<p>Today, generating a very similar proof is a one-liner in Wolfram Language (as we’ll discuss below, the · dot here can be thought of as representing the <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> operation):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg2.png" alt="" title="" width="455" height="100"> </p>
</div>
<p>The proof involves 307 (mostly rather elaborate) steps. And here’s one page of it (out of about 30)—presented in the form of a computable Wolfram Language dataset:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg4.png" alt="Example proof steps page" title="Example proof steps page" width="619" height="385"></p>
</div>
<p>What’s the basic idea of this proof? Essentially it’s to perform a sequence of purely structural symbolic operations that go from our axiom to <a href="https://www.wolframscience.com/nks/p808--implications-for-mathematics-and-its-foundations/">known axioms of Boolean algebra</a>. And the proof does this by proving a series of lemmas which can be combined to eventually give what we want: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg5A.png" alt="" title="" width="689" height="1012"> </p>
</div>
<p>The highlighted “targets” here are the standard Sheffer axioms for Boolean algebra from 1913:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg6.png" alt="" title="" width="278" height="60"> </p>
</div>
<p>And, yes, even though these are quite short, the intermediate lemmas involved in the proof get quite long—the longest involving 60 symbols (i.e. having <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt> 60):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg7.png" alt="" title="" width="488" height="40"> </p>
</div>
<p>It’s as if to get to where it’s going, the proof ends up having to go through the wilds of metamathematical space. And indeed one gets a sense of this if one plots the sizes (i.e. <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt>) of successive lemmas:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg8.png" alt="" title="" width="674" height="142"> </p>
</div>
<p>Here’s the distribution of these sizes, showing that while they’re often small, there’s a long tail (note, by the way, that if dot · appears <em>n</em> times in a lemma, the <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt> will be 2<em>n</em> + 3):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg9.png" alt="" title="" width="259" height="122"> </p>
</div>
<p>So how are these lemmas related? Here’s a graph of their interdependence (with the size of each dot being proportional to the size of the lemma it represents):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg10.png" alt="" title="" width="565" height="710"> </p>
</div>
<p>Zooming in on the top we see more detail:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg11.png" alt="" title="" width="595" height="375"> </p>
</div>
<p>We start from our axiom, then derive a whole sequence of lemmas—as we’ll see later, always <a href="https://www.wolframscience.com/metamathematics/proofs-in-accumulative-systems/">combining two lemmas to create a new one</a>. (And, yes, we could equally well call these things theorems—but we generate so many of them it seems more natural to call them “lemmas”.) </p>
<p>So, OK, we’ve got a complicated proof. But how can we check that it’s correct? Well, from the symbolic representation of the proof in the Wolfram Language we can immediately generate a “proof function” that in effect contains executable versions of all the lemmas—implemented using simple structural operations:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofCLOUDAimg11A.png" alt="" title="" width="551" height="453"> </p>
</div>
<p>And when you run this function, it applies all these lemmas and checks that the result comes out right:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg14.png" alt="" title="" width="259" height="43"> </p>
</div>
<p>And, yes, this is basically what one would do in a proof assistant system (like <a href="https://lean-lang.org/" target="_blank" rel="noopener">Lean</a> or <a href="https://us.metamath.org/index.html" target="_blank" rel="noopener">Metamath</a>)—except that here the steps in the proof were generated purely automatically, without any human guidance (or effort). And, by the way, the fact that we can readily translate our symbolic proof representation into a function that we can run provides an operational manifestation of the equivalence between proofs and programs. </p>
<p>But let’s look back at our lemma-interdependence “proof graph”. One notable feature is that we see several nodes with high out-degree—corresponding to what we can think of as “pivotal lemmas” from which many other lemmas end up directly being proved. So here’s a list of the “most pivotal” lemmas in our proof:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg15.png" alt="" title="" width="329" height="178"> </p>
</div>
<p>Or, more graphically, here are the results for all lemmas that occur:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg16.png" alt="" title="" width="344" height="239"> </p>
</div>
<p>So what are the “pivotal lemmas”? <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em> we readily recognize as commutativity. But the others—despite their comparative simplicity—don’t seem to correspond to things that have specifically shown up before in the mathematical literature (or, as we’ll <a href="https://writings.stephenwolfram.com/2025/01/who-can-understand-the-proof-a-window-on-formalized-mathematics/#llms-to-the-rescue">discuss later</a>, that’s at least what the current generation of LLMs tell us).</p>
<p>But looking at our proof graph something we can conclude is that a large fraction of the “heavy lifting” needed for the whole proof has already happened by the time we can prove <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>. So, for the sake of avoiding at least some of hairy detail in the full proof, in most of what follows, we’ll concentrate on the proof of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>—which <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> tells us we can accomplish in 104 steps, with a proof graph of the form</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg17.png" alt="" title="" width="666" height="824"> </p>
</div>
<p>with the sizes of successive lemmas (in what is basically a breadth-first traversal of the proof graph) being:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025proofimg18A.png" alt="" title="" width="509" height="117"> </p>
</div>
<h2 id="the-machine-code-of-the-proof">The “Machine Code” of the Proof</h2>
<p>It’s already obvious from the previous section that the proof as we currently know it is long, complicated, and fiddly—and in many ways reminiscent of something at a “machine-code” level. But to get a grounded sense of what’s going on in the proof, it’s useful to dive into the details—even if, yes, they can be seriously hard to wrap one’s head around. </p>
<p>At a fundamental level, the way the proof—say of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>—works is by starting from our axiom, and then progressively deducing new lemmas from pairs of existing lemmas. In the simplest case, that deduction works by <a href="https://www.wolframscience.com/metamathematics/the-metamodeling-of-axiomatic-mathematics/">straightforward symbolic substitution</a>. So, for example, let’s say we have the lemmas</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg1.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>and </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg2.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>Then it turns out that from these lemmas we can deduce:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg3.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>Or, in other words, knowing that the first two lemmas hold for any <em>a</em> gives us enough information about · that the third lemma must inevitably also hold. So how do we derive this?</p>
<p>Our lemmas in effect <a href="https://www.wolframscience.com/metamathematics/rules-applied-to-rules/">define two-way equivalences</a>: their left-hand sides are defined as equal to their right-hand sides, which means that if we see an expression that (structurally) matches one side of a lemma, we can always replace it by the other side of the lemma. And to implement this, we can write our second lemma explicitly as a rule—where to avoid confusion we’re using <em>x</em> rather than <em>a</em>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg4.png" alt="" title="" width="141" height="14"> </p>
</div>
<p>But if we now look at our first lemma, we see that there’s part of it (indicated with a frame) that matches the left-hand side of our rule:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg5.png" alt="" title="" width="132" height="25"> </p>
</div>
<p>If we replace this part (which is at position {2,2}) using our rule we then get</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg6.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>which is precisely the lemma we wanted to deduce. </p>
<p>We can summarize what happened here as a fragment of our proof graph—in which a “substitution event” node takes our first two lemmas as input, and “outputs” our final lemma:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg1.png" alt="" title="" width="277" height="99"> </p>
</div>
<p>As always, the symbolic expressions we’re working with here can be represented as trees:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg8.png" alt="" title="" width="391" height="158"> </p>
</div>
<p>The substitution event then corresponds to a tree rewriting:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg2.png" alt="" title="" width="275" height="242"> </p>
</div>
<p>The <a href="https://www.wolframscience.com/metamathematics/relations-to-automated-theorem-proving/">essence of automated theorem proving</a> is to find a particular sequence of substitutions etc. that get us from whatever axioms or lemmas we’re starting with, to whatever lemmas or theorems we want to reach. Or in effect to find a suitable “path” through the multiway graph of all possible substitutions etc. that can be made. </p>
<p>So, for example, in the particular case we’re considering here, this is the graph that represents all possible transformations that can occur through a single substitution event:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg10.png" alt="" title="" width="582" height="285"> </p>
</div>
<p>The particular transformation (or “path”) we’ve used to prove <em>a</em> · <em>a</em> = <em>a</em> · ((<em>a</em> · <em>a</em>) · <em>a</em>) is highlighted. But as we can see, there are many other possible lemmas that can be generated, or in other words that can be proved from the two lemmas we’ve given as input. Put another way, we can think of our input lemmas as implying or entailing all the other lemmas shown here. And, by analogy to the concept of a light cone in physics, we can view the collection of everything entailed by given lemmas or given events as the (future) “<a href="https://www.wolframscience.com/metamathematics/metamathematical-space/#p-28">entailment cone</a>” of those lemmas or events. A proof that reaches a particular lemma is then effectively a path in this entailment cone—analogous in physics to a world line that reaches a particular spacetime point.</p>
<p>If we continue building out the entailment cone from our original lemmas, then after two (substitution) events we get:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg11.png" alt="" title="" width="701" height="454"> </p>
</div>
<p>There are 49 lemmas generated here. But it turns out that beyond the lemma we already discussed there are only three (highlighted here) that appear in the proof we are studying here:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg12.png" alt="" title="" width="281" height="60"> </p>
</div>
<p>And indeed the <a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-mechanics-of-proof">main algorithmic challenge of theorem proving</a> is to figure out which lemmas to generate in order to get a path to the theorem one’s trying to prove. And, yes, as we’ll discuss later, there are typically many paths that will work, and different algorithms will yield different paths and therefore different proofs.</p>
<p>But, OK, seeing how new lemmas can be derived from old by substitution is already quite complicated. But actually there’s something even more complicated we need to discuss: deriving lemmas not only by substitution but also by what we’ve called <a href="https://www.wolframscience.com/metamathematics/beyond-substitution-cosubstitution-and-bisubstitution/">bisubstitution</a>. </p>
<p>We can think of both substitution and bisubstitution as turning one lemma X == Y into a transformation rule (either X <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> Y or Y <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> X), and then applying this rule to another lemma, to derive a new lemma. In ordinary substitution, the left-hand side of the rule directly matches (in a Wolfram Language pattern-matching sense) a subexpression in the lemma we’re transforming. But the key point is that all the variables that appear in both our lemmas are really “pattern variables” (<tt>x_</tt> etc. in Wolfram Language). So that means there’s another way that one lemma can transform another, in which in effect replacements are made not only in the lemma being transformed, but also in the lemma that’s doing the transforming. </p>
<p>The net effect, though, is still to take two lemmas and derive another, as in:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg3.png" alt="" title="" width="414" height="130"> </p>
</div>
<p>But in tracing through the details of our proof, we need to distinguish “substitution events” (shown yellowish) from “bisubstitution” ones (shown reddish). (In <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> in Wolfram Language, lemmas produced by ordinary substitution are called “substitution lemmas”, while lemmas produced by bisubstitution are called “critical pair lemmas”.)</p>
<p>OK, so how does bisubstitution work? Let’s look at an example. We’re going to be transforming the lemma </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg14.png" alt="" title="" width="235" height="14"> </p>
</div>
<p>using the lemma (which in this case happens to be our original axiom)</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg15.png" alt="" title="" width="183" height="14"> </p>
</div>
<p>to derive the new lemma:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg16.png" alt="" title="" width="235" height="14"> </p>
</div>
<p>We start by creating a rule from the second lemma. In this case, the rule we need happens to be reversed relative to the way we wrote the lemma, and this means that (in the canonical form we’re using) it’s convenient to rename the variables that appear:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg17.png" alt="" title="" width="237" height="14"> </p>
</div>
<p>To do our bisubstitution we’re going to apply this rule to a subterm of our first lemma. We can write that first lemma with explicit pattern variables:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg18.png" alt="" title="" width="311" height="14"> </p>
</div>
<p>As always, the particular names of those variables don’t matter. And to avoid confusion, we’re going to rename them:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg19.png" alt="" title="" width="306" height="14"> </p>
</div>
<p>Now look at this subterm of this lemma (which is part {2,1,1,2} of the expression):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg20.png" alt="" title="" width="102" height="14"> </p>
</div>
<p>It turns out that with appropriate bindings for pattern variables this can be matched (or “unified”) with the left-hand side of our rule. This provides a way to find such bindings:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg21.png" alt="" title="" width="345" height="14"> </p>
</div>
<p>(Note that in these bindings things like c_ stand only for explicit expressions, like c_, not for expressions that the ordinary Wolfram Language pattern <tt>c_</tt> would match.)</p>
<p>Now if we apply the bindings we’ve found to the left-hand side of our rule</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg22.png" alt="" title="" width="277" height="14"> </p>
</div>
<p>and to the subterm we picked out from our lemma</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg23.png" alt="" title="" width="277" height="14"> </p>
</div>
<p>we see that we get the same expression. Which means that with these bindings the subterm matches the left-hand side of our rule, and we can therefore replace this subterm with the right-hand side of the rule. To see all this in operation, we first apply the bindings we’ve found to the lemma we’re going to transform (and, as it happens, the binding for y_ is the only one that matters here):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg24.png" alt="" title="" width="615" height="14"> </p>
</div>
<p>Now we take this form and apply the rule at the position of the subterm we identified:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg25.png" alt="" title="" width="342" height="14"> </p>
</div>
<p>Renaming variables</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg26.png" alt="" title="" width="345" height="14"> </p>
</div>
<p> we now finally get exactly the lemma that we were trying to derive:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg27.png" alt="" title="" width="235" height="14"> </p>
</div>
<p>And, yes, getting here was a pretty complicated process. But with the symbolic character of our lemmas, it’s one that is inevitably possible, and so can be used in our proof. And in the end, out of the 101 lemmas used in the proof, 47 were derived by ordinary substitution, while 54 were derived by bisubstitution.</p>
<p>And indeed the first few steps of the proof turn out to use only bisubstituion. An example is the first step—which effectively applies the original axiom to itself using bisubsitution:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg4.png" alt="" title="" width="243" height="149"> </p>
</div>
<p>And, yes, even this very first step is pretty difficult to follow. </p>
<p>If we start from the original axiom, there are 16 lemmas that can be derived purely by a single ordinary substitution (effectively of the axiom into itself)—resulting in the following entailment cone:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg29.png" alt="" title="" width="693" height="323"> </p>
</div>
<p>As it happens, though, none of the 16 new lemmas here actually get used in our proof. On the other hand, in the bisubstitution entailment cone</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg30.png" alt="" title="" width="693" height="251"> </p>
</div>
<p>there are 27 new lemmas, and 4 of them get used in the proof—as we can see from the first level of the proof graph (here rotated for easier rendering):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg31.png" alt="" title="" width="657" height="101"> </p>
</div>
<p>At the next level of the entailment cone from ordinary substitution, there are 5153 new lemmas—none of which get used in the proof. But of the 23215 new lemmas in the (pure) bisubstitution entailment cone, 5 do get used:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg32.png" alt="" title="" width="663" height="112"> </p>
</div>
<p>At the next level, lemmas generated by ordinary substitution also start to get used:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg33.png" alt="" title="" width="657" height="126"> </p>
</div>
<p>Here’s another rendering of these first few levels of the proof graph:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg34.png" alt="" title="" width="295" height="198"> </p>
</div>
<p>Going to another couple of levels we’re starting to see quite a few independent chains of lemmas developing</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg35.png" alt="" title="" width="454" height="289"> </p>
</div>
<p>which eventually join up when we assemble the whole proof graph:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg36.png" alt="" title="" width="605" height="631"> </p>
</div>
<p>A notable feature of this proof graph is that it has more bisubstitution events at the top, and more ordinary substitution events at the bottom. So why is that? Essentially it seems to be because bisubstitution events tend to produce larger lemmas, and ordinary substitution events tend to produce smaller ones—as we can see if we plot input and output lemma sizes for all events in the proof:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg37.png" alt="" title="" width="451" height="479"> </p>
</div>
<p>So in effect what seems to be happening is that the proof first has to “spread out in <a href="https://www.wolframscience.com/metamathematics/metamathematical-space/">metamathematical space</a>”, using bisubstitution to generate large lemmas “far out in metamathematical space”. Then later the proof has to “corral things back in”, using ordinary substitution to generate smaller lemmas. And for example, at the very end, it’s a substitution event that yields the final theorem we’re trying to prove:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg38.png" alt="" title="" width="277" height="100"> </p>
</div>
<p>And earlier in the graph, there’s a similar “collapse” to a small (and rather pivotal) lemma:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025machineCLOUDimg39.png" alt="" title="" width="406" height="120"> </p>
</div>
<p>As the plot above indicates, ordinary substitution can lead to large lemmas, and indeed bisubstitution can also lead to smaller ones, as in</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025machineCLOUDimg40.png" alt="" title="" width="367" height="87"> </p>
</div>
<p>or slightly more dramatically:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025machineCLOUDimg41.png" alt="" title="" width="630" height="140"> </p>
</div>
<p>But, OK, so this is some of what’s going on at a “machine-code” level inside the proof we have. Of course, given our axiom and the operations of substitution and bisubstitution there are inevitably a huge number of different possible proofs that could be given. The particular proof we’re considering is what the Wolfram Language <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> gives. (In the Appendix, we’ll also look at results from some other automated theorem proving systems. The results will be very comparable, if usually a little lengthier.) </p>
<p>We won’t discuss the detailed (and rather elaborate) algorithms inside <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt>. But fundamentally what they’re doing is to try constructing certain lemmas, then to find sequences of lemmas that eventually form a “path” to what we’re trying to prove. And as some indication of what’s involved in this, here’s a plot of the number of “candidate lemmas” that are being maintained as possible when different lemmas in the proof are generated:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025machineimg42.png" alt="" title="" width="354" height="145"> </p>
</div>
<p>And, yes, for a while there’s roughly exponential growth, leveling off at just over a million when we get to the “pulling everything together” stage of the proof.</p>
<h2 id="unrolling-the-proof">Unrolling the Proof</h2>
<p>In what we’ve done so far, we’ve viewed our proof as working by starting from an axiom, then <a href="https://www.wolframscience.com/nks/notes-12-9--proof-structures/">progressively building up lemmas</a>, until eventually we get to the theorem we want. But there’s an alternative view that’s in some ways useful in getting a more direct, “mechanical” intuition about what’s going on in the proof.</p>
<p>Let’s say we’re trying to prove that our axiom implies that <em>p</em> · <em>q</em> = <em>q</em> · <em>p</em>. Well, then there must be some way to start from the expression <em>p</em> · <em>q</em> and just keep on judiciously applying the axiom until eventually we get to the expression <em>q</em> · <em>p</em>. And, yes, the number of axiom application steps required might be very large. But ultimately, if it’s true that the axiom implies <em>p</em> · <em>q</em> = <em>q</em> · <em>p</em> there must be a path that gets from <em>p</em> · <em>q</em> to <em>q</em> · <em>p</em>.</p>
<p>But before considering the case of our full proof, let’s start with something simpler. Let’s assume that we’ve already established the lemmas:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg1.png" alt="" title="" width="121" height="37"> </p>
</div>
<p>Then we can treat them as axioms, and ask a question like whether they imply the lemma</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg2.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>or, in our current approach, whether they can be used to form a path from <em>a</em> · <em>a</em> to <em>a</em> · (<em>a</em> · (<em>a</em> · <em>a</em>)). </p>
<p>Well, it’s not too hard to see that in fact there is such a path. Apply our second lemma to <em>a</em> · <em>a</em> to get:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg3.png" alt="" title="" width="77" height="14"> </p>
</div>
<p>But now this subterm</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg4.png" alt="" title="" width="88" height="25"> </p>
</div>
<p>matches the left-hand of the first lemma, so that it can be replaced by the right-hand side of that lemma (i.e. by <em>a</em> · (<em>a</em> · <em>a</em>)), giving in the end the desired <em>a</em> · (<em>a</em> · (<em>a</em> · <em>a</em>)).</p>
<p>So now we can summarize this process as:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg5.png" alt="" title="" width="252" height="138"> </p>
</div>
<p>In what follows, it’ll be convenient to label lemmas. We’ll call our original axiom A1, we’ll call our successive lemmas generated by ordinary substitution S<em>n</em> and the ones generated by bisubsitution B<em>n:</em></p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg6.png" alt="" title="" width="399" height="621"> </p>
</div>
<p>In our proof we’ll also use <img src="https://content.wolfram.com/sites/43/2025/01/rightgreenarrow.png" width="18" height="25"> and <img src="https://content.wolfram.com/sites/43/2025/01/leftpinkarrow.png" width="18" height="25"> to indicate whether we’re going to use the lemma (say <nobr>X = Y)</nobr> in the “forward direction” X <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> Y or the “reverse direction” X <img src="https://content.wolfram.com/uploads/sites/32/2022/10/leftarrow.png" width="15" height="11"> Y. And with this labeling, the proof we just gave (which is for the lemma S23) becomes:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg7.png" alt="" title="" width="168" height="138"> </p>
</div>
<p>Each step here is a pure substitution, and requires no replacement in the rule (i.e. “axiom”) being used. But proofs like this can also be done with bisubstitution, where replacements are applied to the rule to get it in a form where it can directly be applied to transform an expression:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg8.png" alt="" title="" width="529" height="136"> </p>
</div>
<p>OK, so how about the first lemma in our full proof? Here’s a proof that its left-hand side can be transformed to its right-hand side just by judiciously applying the original axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg9.png" alt="" title="" width="504" height="140"> </p>
</div>
<p>Here’s a corresponding proof for the second lemma:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg10.png" alt="" title="" width="531" height="136"> </p>
</div>
<p>Both these involve bisubstitution. Here’s a proof of the first lemma derived purely by ordinary substitution:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg11.png" alt="" title="" width="611" height="136"> </p>
</div>
<p>This proof is using not only the original axiom but also the lemma B5. Meanwhile, B5 can be proved using the original axiom together with B2:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg12.png" alt="" title="" width="693" height="181"> </p>
</div>
<p>But now, inserting the proof we just gave above for B2, we can give a proof of B5 just in terms of the original axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg13.png" alt="" title="" width="693" height="278"> </p>
</div>
<p>And recursively continuing this unrolling process, we can then prove S1 purely using the original axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg14.png" alt="" title="" width="693" height="329"> </p>
</div>
<p>What about the whole proof? Well, at the very end we have:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg15.png" alt="" title="" width="222" height="137"> </p>
</div>
<p>If we “unroll” one step we have</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg16.png" alt="" title="" width="275" height="349"> </p>
</div>
<p>and after 2 steps:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg17.png" alt="" title="" width="435" height="444"> </p>
</div>
<p>In principle we could go on with this unrolling, in effect recursively replacing each rule by the sequence of transformations that represents its proof. Typically this process will, however, generate exponentially longer proof sequences. But say for lemma S5</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg18.png" alt="" title="" width="334" height="14"> </p>
</div>
<p>the result is still very easily manageable:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg19.png" alt="" title="" width="670" height="274"> </p>
</div>
<p>We can summarize this result by in effect plotting the sizes of the intermediate expressions involved—and indicating what part of each expression is replaced at each step (with <img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/01082025redbox.png" alt="" title="" width="15" height="15"> as above indicating “forward” use of the axiom A1 <img src="https://content.wolfram.com/uploads/sites/32/2022/10/rightarrow2.png" width="15" height="11"> and <img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/greenbox.png" '="" title="" width="15" height="15"> “backward” A1 <img src="https://content.wolfram.com/uploads/sites/32/2022/10/leftarrow.png" width="15" height="11">):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg20.png" alt="" title="" width="357" height="160"> </p>
</div>
<p>For lemma B33</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg21.png" alt="" title="" width="681" height="14"> </p>
</div>
<p>the unrolled proof is now 30 steps long</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg22.png" alt="" title="" width="357" height="160"> </p>
</div>
<p>while for lemma S11</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg23.png" alt="" title="" width="467" height="14"> </p>
</div>
<p>the unrolled proof is 88 steps long:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg24.png" alt="" title="" width="412" height="182"> </p>
</div>
<p>But here there is a new subtlety. Doing a direct substitution of the “proof paths” for the lemmas used to prove S11 in our original proof gives a proof of length 104:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg25.png" alt="" title="" width="466" height="177"> </p>
</div>
<p>But this proof turns out to be repetitive, with the whole gray section going from one copy to another of:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg26.png" alt="" title="" width="237" height="14"> </p>
</div>
<p>As an example of a larger proof, we can consider lemma B47:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg27.png" alt="" title="" width="157" height="14"> </p>
</div>
<p>And despite the simplicity of this lemma, our proof for it is 1008 steps long: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg28.png" alt="" title="" width="609" height="204"> </p>
</div>
<p>If we don’t remove repetitive sections, it’s 6805 steps:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg29.png" alt="" title="" width="460" height="157"> </p>
</div>
<p>Can we unroll the whole proof of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>? We can get closer by considering lemma S36:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg30.png" alt="" title="" width="121" height="14"> </p>
</div>
<p>Its proof is 27105 steps long:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg31A.png" alt="" title="" width="621" height="204"> </p>
</div>
<p>The distribution of expression sizes follows a roughly exponential distribution, with a maximum of 20107:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg32.png" alt="" title="" width="274" height="122"> </p>
</div>
<p>Plotting the expression sizes on a log scale one gets: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg33.png" alt="" title="" width="409" height="142"> </p>
</div>
<p>And what stands out most here is a kind of recursive structure—which is the result of long sequences that basically represent the analog of “subroutine calls” that go back and repeatedly prove lemmas that are needed.</p>
<p>OK, so what about the whole proof of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>? Yes, it can be unrolled—in terms of 83,314 applications of the original axiom. The sequence of expression sizes is:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg34.png" alt="" title="" width="571" height="187"> </p>
</div>
<p>Or on a log scale:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg35.png" alt="" title="" width="519" height="175"> </p>
</div>
<p>The distribution of expression sizes now shows clear deviation from being exponential:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025unrollCLOUDimg36A.png" alt="" title="" width="359" height="160"> </p>
</div>
<p>The maximum is 63245, which occurs just 81 steps after the exact midpoint of the proof. In other words, in the middle, the proof has wandered incredibly far out in metamathematical space (there are altogether <tt><a href="http://reference.wolfram.com/language/ref/CatalanNumber.html">CatalanNumber</a></tt>[63245] ≈ 10<sup>38178</sup> possible expressions of the size it reaches). </p>
<p>The proof returns to small expressions just a few times; here are all the cases in which the size is below 10:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092024JGCLOUDimg8.png" alt="" title="" width="399" height="274"> </p>
</div>
<p>So, yes, it is possible to completely unroll the proof into a sequence of applications of the original axiom. But if one does this, it inevitably involves repeating lots of work. Being able to use intermediate lemmas in effect lets one “share common subparts” in the proof. So that one ends up with just 104 “rule applications”, rather than 63245. Not that it’s easy to understand those 104 steps…</p>
<h2 id="is-there-a-better-notation">Is There a Better Notation?</h2>
<p> Looking at our proof—either in its original “lemma” form, or in its “unrolled” form—the most striking aspect of it is how complicated (and incomprehensible) it seems to be. But one might wonder whether much of that complexity is just the result of not “using the right notation”. In the end, we’ve got a huge number of expressions written in terms of · operations that we can interpret as <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> (or <tt><a href="http://reference.wolfram.com/language/ref/Nor.html">Nor</a></tt>). And maybe it’s a little like seeing the operation of a microprocessor down at the level of individual gates implementing <tt>Nand</tt>s or <tt>Nor</tt>s. And might there perhaps be an analog of a higher-level representation—with higher-level operations (even like arithmetic) that are more accessible to us humans?</p>
<p>It perhaps doesn’t help that <tt>Nand</tt> itself is a rather non-human construct. For example, not a single natural human language seems to have a word for <tt>Nand</tt>. But there are combinations of <tt>Nand</tt>s that have more <a href="https://www.wolframscience.com/nks/p807--implications-for-mathematics-and-its-foundations/">familiar interpretations</a>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025notationimg1.png" alt="" title="" width="228" height="127"> </p>
</div>
<p>But what combinations actually occur in our proof? Here are the most common subexpressions that appear in lemmas in the proof:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092925NLCLOUDimg12.png" alt="" title="" width="290" height="316"> </p>
</div>
<p>And, yes, we could give the most common of these special names. But it wouldn’t really help in “compressing” the proof—or making it easier to understand.</p>
<p>What about “upgrading” our “laws of inference”, i.e. the way that we can derive new lemmas from old? Perhaps instead of substitution and bisubstitution, which both take two lemmas and produce one more, we could set up more elaborate “tactics” that for example take in more input lemmas. We’ve seen that if we completely unroll the proof, it gets much longer. So perhaps there is a “higher-order” setup that for example dramatically shortens the proof. </p>
<p>One way one might identify this is by seeing commonly repeating structures in the subgraphs that lead to lemmas. But in fact these subgraphs are quite diverse:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025notationimg3.png" alt="" title="" width="589" height="130"> </p>
</div>
<h2 id="what-are-the-popular-lemmas">What Are the Popular Lemmas?</h2>
<p>A typical feature of human-written mathematical proofs is that they’re “anchored” by famous theorems or lemmas. They may have fiddly technical pieces. But usually there’s a backbone of “theorems people know”. </p>
<p>We have the impression that the proof we’re discussing here “spends most of its time wandering around the wilds of metamathematical space”. But perhaps it visits waypoints that are somehow recognizable, or at least should be. Or in other words, perhaps out in the metamathematical space of lemmas there are ones that are somehow sufficiently popular that they’re worth giving names to, and learning—and can then be used as “reference points” in terms of which our proof becomes simpler and more human accessible.</p>
<p>It’s a story very much like what happens with human language. There are things out there in the world, but when there’s a certain category of them that are somehow common or important enough, we make a word for them in our language, which we can then use to “compactly” refer to them. (It’s again the same story when it comes to computational language, and in particular the Wolfram Language, except that in that case it’s been my personal responsibility to come up with the appropriate definitions and names for functions to represent “common lumps of computation”.) </p>
<p>But, OK, so what are the “popular lemmas” of <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> proofs? One way to explore this is to enumerate statements that are “true about <tt>Nand</tt>”—then to look at proofs of these statements (say found with <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> from our axiom) and see what lemmas show up frequently in them. </p>
<p><a href="https://www.wolframscience.com/nks/p818--implications-for-mathematics-and-its-foundations/">Enumerating statements “true about </a><tt><a href="https://www.wolframscience.com/nks/p818--implications-for-mathematics-and-its-foundations/">Nand”</a></tt>, starting from the smallest, we get</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg1.png" alt="" title="" width="598" height="222"> </p>
</div>
<p>where we have highlighted statements from this list that appear as lemmas in our proof.</p>
<p>Proving each of these statements from our original axiom, here are the <a href="https://www.wolframscience.com/nks/notes-12-9--proof-lengths-in-logic/">lengths of proofs we find</a> (for all 1341 distinct theorems with up to <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt> 4 on each side):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg2.png" alt="" title="" width="471" height="204"> </p>
</div>
<p>A histogram shows that it’s basically a bimodal distribution</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg3.png" alt="" title="" width="359" height="134"> </p>
</div>
<p>with the smallest “long-proof” theorem being:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg4.png" alt="" title="" width="173" height="14"> </p>
</div>
<p>In aggregate, all these proofs use about 200,000 lemmas. But only about 1200 of these are distinct. And we can plot which lemmas are used in which proofs—and we see that there are indeed many lemmas that are used across wide ranges of proofs, while there are a few others that are “special” to each proof (the diagonal stripe is associated with lemmas close to the statement being proved):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg5.png" alt="" title="" width="428" height="401"> </p>
</div>
<p>If we rank all distinct lemmas from most frequently to least frequently used, we get the following distribution of lemma usage frequencies across all our proofs: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg6.png" alt="" title="" width="355" height="155"> </p>
</div>
<p>It turns out that there is a “common core” of 49 lemmas that are used in every single one of the proofs. So what are these lemmas? Here’s a plot of the usage frequency of lemmas against their size—with the “common ones” populating the top line: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg7.png" alt="" title="" width="438" height="192"> </p>
</div>
<p>And at first this might seem surprising. We might have expected that short lemmas would be the most frequent, but instead we’re seeing long lemmas that always appear, the very longest being:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg8.png" alt="" title="" width="516" height="40"> </p>
</div>
<p>So why is this? Basically it’s that these long lemmas are being used at the beginning of every proof. They’re the result of applying bisubstitution to the original axiom, and in some sense they seem to be laying down a kind of net in metamathematical space that then allows more diverse—and smaller—lemmas to be derived. </p>
<p>But how are these “common core” popular lemmas distributed within proofs? Here are a few examples:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025lemmasCLOUDXimg9.png" alt="" title="" width="566" height="579"> </p>
</div>
<p>And what we see is that while, yes, the common core lemmas are always at the beginning, they don’t seem to have a uniform way of “plugging into” the rest of the proof. And it doesn’t, for example, seem as if there’s just some small set of (perhaps simple) “waypoint” lemmas that one can introduce that will typically shorten these proofs.</p>
<p>If one effectively allows all the common core lemmas to be used as axioms, then inevitably proofs will be shortened; for example, the proof of <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>—which only ends up using 5 of the common core lemmas—is now shortened to 51 lemmas:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025lemmasCLOUDZimg10B.png" alt="" title="" width="282" height="479"> </p>
</div>
<p>It doesn’t seem to become easier to understand, though. And if it’s unrolled, it’s still 5013 steps. </p>
<p>Still, one can ask what happens if one just introduces particular “recognizable” lemmas as additional axioms. For example, if we include “commutativity” <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em> then we find that, yes, we do manage to <a href="https://www.wolframscience.com/nks/notes-12-9--proof-lengths-in-logic/">reduce the lengths of some proofs</a>, but certainly not all:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg11.png" alt="" title="" width="554" height="280"> </p>
</div>
<p>Are there any other “pivotal” lemmas we could add? In particular, what about lemmas that can help with the length-200 or more proofs? It turns out that all of these proofs involve the lemma: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg12.png" alt="" title="" width="130" height="14"> </p>
</div>
<p>So what happens if we add this? Well, it definitely reduces proof lengths:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg13.png" alt="" title="" width="607" height="316"> </p>
</div>
<p>And sometimes it even seems like it brings proofs into “human range”. For example, a proof of</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025lemmasimg14.png" alt="" title="" width="104" height="14"> </p>
</div>
<p>from our original axiom has length 56. Adding in commutativity reduces it to length 18. And adding our third lemma reduces it to just length 9—and makes it not even depend directly on the original axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025lemmasCLOUDZimg15.png" alt="" title="" width="272" height="249"> </p>
</div>
<p>But despite the apparent simplicity here, the steps involved—particularly when bisubstitution is used—are remarkably hard to follow. (Note the use of <em>a </em>= <em>a</em> as a kind of “implicit axiom”—something that has actually also appeared, without comment, in many of our other proofs.)</p>
<h2 id="can-we-get-a-shorter-proof">Can We Get a Shorter Proof?</h2>
<p>The proof that we’ve been studying can be seen in some ways as a rather arbitrary artifact. It’s the output of <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt>, with all its specific detailed internal algorithms and choices. In the Appendix, we’ll see that other automated theorem proving systems give very similar results. But we still might wonder whether actually the complexity of the proof as we’ve been studying it is just a consequence of the details of our automated theorem proving—and that in fact there’s a much shorter (and perhaps easier to understand) proof that exists.</p>
<p>One approach we could take—reminiscent of higher category theory—is to think about just simplifying the proof we have, effectively using proof-to-proof transformations. And, yes, this is technically difficult, though it doesn’t seem impossible. But what if there are <a href="https://www.wolframscience.com/metamathematics/the-topology-of-proof-space/">“holes” in proof space</a>? Then a “continuous deformation” of one proof into another will get stuck, and even if there is a much shorter proof, we’re liable to get “topologically stuck” before we find it.</p>
<p>One way to be sure we’re getting the shortest proof of a particular lemma is to explicitly find the first place that lemma appears in the (future) entailment cone of our original axiom. For example, as we saw above, a single substitution event leads to the entailment cone:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg1.png" alt="" title="" width="645" height="285"> </p>
</div>
<p>Every lemma produced here is, by construction, in principle derivable by a proof involving a single substitution event. But if we actually use <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> to prove these lemmas, the proofs we get most involve 2 events (and in one case 4):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025shorterAimg2.png" alt="" title="" width="645" height="75"> </p>
</div>
<p>If we take another step in the entailment cone, we get a total of 5151 lemmas. From the way we generated them, we know that all these lemmas can in principle be reached by proofs of length 2. But if we run <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> on them, we find a distribution of proof lengths:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg3.png" alt="" title="" width="314" height="141"> </p>
</div>
<p>And, yes, there is one lemma (with <tt><a href="http://reference.wolfram.com/language/ref/LeafCount.html">LeafCount</a></tt> 183) that is found only by a proof of length 14. But most often the proof length is 4—or about double what it could be. </p>
<p>If we generate the entailment cone for lemmas using bisubstitution rather than just ordinary substitution, there are slightly more cases where <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> does worse at getting minimal proofs. </p>
<p>For example, the lemma</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg4.png" alt="" title="" width="671" height="14"> </p>
</div>
<p>and 3 others can be generated by a single bisubstitution from the original axiom, but <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> gives only proofs of length 4 for all of these.</p>
<p>What about unrolled proofs, in which one can generate an entailment cone by starting from a particular expression, and then applying the original axiom in all possible ways? For example, let’s say we start with:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg5.png" alt="" title="" width="77" height="14"> </p>
</div>
<p>Then applying bisubstitution with the original axiom once in all possible ways gives:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg6.png" alt="" title="" width="601" height="186"> </p>
</div>
<p>Applying bisubstitution a second time gives a larger entailment cone: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025shorterCimg7.png" alt="" title="" width="451" height="406"> </p>
</div>
<p>But now it turns out that—as indicated—one of the expressions in this cone is: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg8.png" alt="" title="" width="262" height="14"> </p>
</div>
<p>So this shows that the lemma</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg9.png" alt="" title="" width="359" height="14"> </p>
</div>
<p>can in principle be reached with just two steps of “unrolled” proof:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg10.png" alt="" title="" width="441" height="41"> </p>
</div>
<p>And in this particular case, if we use <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> and then unroll the resulting proof we also get a proof of length 3—but it goes through a different intermediate expression:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025shorterimg11.png" alt="" title="" width="542" height="41"> </p>
</div>
<p>As it happens, this intermediate expression is also reached in the entailment cone that we get by starting from our “output” expression and then applying two bisubsitutions:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025shorterEimg12-min.png" alt="" title="" width="528" height="417"> </p>
</div>
<h2 id="what-actually-is-the--models-and-the-proof">What Actually Is the “·”? Models and the Proof</h2>
<p>We can think of logic (or Boolean algebra) as being associated with a certain collection of theorems. And what our axiom does is to provide something from which all theorems of logic (and nothing but theorems of logic) can be derived. At some level, we can think of it as just being about symbolic expressions. But in our effort to understand what’s going on—say with our proof—it’s sometimes useful to ask how we can “concretely” interpret these expressions.</p>
<p>For example, we might ask what the · operator actually is. And what kinds of things can our symbolic variables be? In effect we’re asking for what in model theory are called <a href="https://www.wolframscience.com/metamathematics/the-model-theoretic-perspective/">“models” of our axiom system</a>. And in aligning with logic the most obvious model to discuss is one in which variables can be <tt><a href="http://reference.wolfram.com/language/ref/True.html">True</a></tt> or <tt><a href="http://reference.wolfram.com/language/ref/False.html">False</a></tt>, and the · represents either the logical operator <tt><a href="http://reference.wolfram.com/language/ref/Nand.html">Nand</a></tt> or the logical operator <tt><a href="http://reference.wolfram.com/language/ref/Nor.html">Nor</a></tt>.</p>
<p>The truth table, say for <tt>Nand</tt>, is:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg1.png" alt="" title="" width="156" height="152"> </p>
</div>
<p>And as expected, with this model for ·, we can confirm that our original axiom holds:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg2.png" alt="" title="" width="379" height="274"> </p>
</div>
<p>In general, though, our original axiom allows two size-2 models (that we can interpret as <tt>Nand</tt> and <tt>Nor</tt>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg4.png" alt="" title="" width="98" height="52"> </p>
</div>
<p>It allows no size-3 models, and in fact in general <a href="https://www.wolframscience.com/nks/notes-12-9--operators-on-sets/">allows only models of size 2<sup><em>n</em></sup></a>; for example, for size 4 its models are:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg6.png" alt="" title="" width="551" height="210"> </p>
</div>
<p>So what about <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>? What models does it allow? For size 2, it’s all 8 possible models with symmetric “multiplication tables”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01082025actuallyimg7.png" alt="" title="" width="467" height="52"> </p>
</div>
<p>But the crucial point is that the 2 models for our original axiom system are part of these. In other words, at least for size-2 models, satisfying the original axiom system implies satisfying <nobr><em>a</em> · <em>b</em> = <em>b</em> · <em>a</em>.</nobr></p>
<p>And indeed any lemma derived from our axiom system must allow the models associated with our original axiom system. But it may also allow more—and sometimes many more. So here’s a map of our proof, showing how many models (out of 16 possible) each lemma allows:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg5.png" alt="" title="" width="329" height="674"> </p>
</div>
<p>Here are the results for size-3 models:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01092025KDCLOUDimg6.png" alt="" title="" width="478" height="972"> </p>
</div>
<p>And, once again, these look complicated. We can think of models as defining—in some sense—<a href="https://www.wolframscience.com/nks/p804--implications-for-mathematics-and-its-foundations/">what lemmas are “about”</a>. So, for example, our original axiom is “about” <tt>Nand</tt> and <tt>Nor</tt>. The lemma <em>a</em> · <em>b</em> = <em>b</em> · <em>a</em> is “about” symmetric functions. And so on. And we might have hoped that we could gain some understanding of our proof by looking at how different lemmas that occur in it “sculpt” what is being talked about. But in fact we just seem to end up with complicated descriptions of sets that don’t seem to have any obvious relationship with each other.</p>
<h2 id="what-about-a-higher-level-abstraction">What about a Higher-Level Abstraction?</h2>
<p>If there’s one thing that stands out about our proof—and the analysis we’ve given of it here—it’s how fiddly and “in the weeds” it seems to be. But is that because we’re missing some big picture? Is there actually a more abstract way of discussing things, that gets to our result without having to go through all the details? </p>
<p>In the history of mathematics many of the most important themes have been precisely about finding such higher-level abstractions. We could start from the <a href="https://www.wolframscience.com/nks/notes-12-9--groups-and-axioms/">explicit symbolic axioms</a></p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025abstractionimg1.png" alt="" title="" width="120" height="59"> </p>
</div>
<p>or even</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025abstractionimg2.png" alt="" title="" width="165" height="22"> </p>
</div>
<p>and start building up theorems much as we’ve done here. Or we could recognize that these are axioms for group theory, and then start using the abstract ideas of group theory to derive our theorems.</p>
<p>So is there some higher-level version of what we’re discussing here? Remember that the issue is not about the overall structure of Boolean algebra; rather it’s about the more metamathematical question of how one can prove that all of Boolean algebra can be generated from the axiom:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025abstractionimg3.png" alt="" title="" width="183" height="14"> </p>
</div>
<p>In the last few sections we’ve tried a few semi-empirical approaches to finding higher-level representations. But they haven’t gotten very far. And to get further we’re probably going to need a serious new idea.</p>
<p>And, if history is a guide, we’re going to need to come up with an abstraction that somehow “goes outside of the system” before “coming back”. It’s like trying to figure out the real roots of a cubic equation, and realizing that the best way to do this is to introduce complex numbers, even though the imaginary parts will cancel at the end. </p>
<p>In the direct exploration of our proof, it feels as if the intermediate lemmas we generate “wander off into the wilds of metamathematical space” before coming back to establish our final result. And if we were using a higher-level abstraction, we’d instead be “wandering off” into the space of that abstraction. But what we might hope is that—at least with the concepts we would use in discussing that abstraction—the path that would be involved would be “short enough to be accessible to human understanding”.</p>
<p>Will we be able to find such an abstraction? It’s a subtle question. Because in effect it asks whether we can reduce the computational effort needed for the proof—or, in other words, whether we can find a pocket of computational reducibility in what in general will be a computationally irreducible process. But it’s not a question that can really be answered just for our specific proof on it own. After all, our “abstraction” could in principle just involve introducing a primitive that represents our whole proof or a large part of it. But to make it what we can think of as a real abstraction we need something that spans many different specific examples—and, in our case, likely many axiomatic systems or symbolic proofs.</p>
<p>So is such an abstraction possible? In the history of mathematics the experience has been that after enough time (often measured in centuries) has passed, abstractions tend to be found. But at some level this has been self fulfilling. Because the areas that are considered to have remained “interesting for mathematics” tend to be just those where general abstractions have in fact been found. </p>
<p>In <a href="https://writings.stephenwolfram.com/2021/09/charting-a-course-for-complexity-metamodeling-ruliology-and-more/">ruliology</a>, though, the typical experience has been different. Because there it’s been routine to <a href="https://www.wolframscience.com/nks/">sample the computational universe of possible simple programs</a> and encounter computational irreducibility. In the end it’s still inevitable that among the computational irreducibility there must be pockets of computational reducibility. But the issue is that these pockets of computational reducibility may not involve features of our system that we care about. </p>
<p>So is a proof of the kind we’re discussing here more like ruliology, or more like “typical mathematics”? Insofar as it’s a mathematical-style proof of a mathematical statement it feels more like typical mathematics. But insofar as it’s something found by the computational process of automated theorem proving it perhaps seems more ruliology. </p>
<p>But what might a higher-level abstraction for it look like? Figuring that out is probably tantamount to finding the abstraction. But perhaps one can at least expect that in some ways it will be metamathematical, and more about the structure and character of proofs than about their content. Perhaps it will be something related to the framework of higher category theory, or some form of meta-algebra. But as of now, we really don’t know—and we can’t even say that such an abstraction with any degree of generality is possible.</p>
<h2 id="llms-to-the-rescue">LLMs to the Rescue?</h2>
<p>The unexpected success of LLMs in language generation and related tasks has led to the idea that perhaps eventually <a href="https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/">systems like LLMs will be able to “do everything”</a>—including for example math. We already know—not least thanks to Wolfram Language—that <a href="https://www.wolfram.com/mathematica/">lots of math can be done computationally</a>. But often the computations are hard—and, as in the example of the proof we’re discussing here, incomprehensible to humans. So the question really is: can LLMs “humanize” what has to be done in math, turning everything into a human-accessible narrative? And here our proof seems like an excellent—if challenging—test case. </p>
<p>But what happens if we just ask a current LLM to generate the proof from scratch? It’s not a good picture. Very often the LLM will eagerly generate a proof, but it’ll be completely wrong, often with the same kind of mistakes that a student somewhat out of their depth might make. Here’s a typical response where an LLM simply assumes that the · operator is associative (which it isn’t in Boolean algebra) then produces a proof that on first blush looks at least vaguely plausible, but is in fact completely wrong:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/01/sw01072025rescueimg1.png" alt="Inadequate LLM proof" title="Inadequate LLM proof" width="611" height="489"></p>
<p>Coming up with an explanation for what went wrong is basically an exercise in “LLM psychology”. But in a first approximation one might say the following. LLMs are trained to “fill in what’s typical”, where “typical” is defined by what appears in the training set. But (absent some recent Wolfram Language and <a href="https://www.wolframalpha.com/">Wolfram|Alpha</a> based technology of ours) what’s been available as a training set has been human-generated mathematical texts, where, yes, operators are often associative, and typical proofs are fairly short. And in the “psychology of LLMs” an LLM is much more likely to “do what’s typical” than to “rigorously follow the rules”. </p>
<p>If you press the LLM harder, then it might just “abdicate”, and suggest using the <a href="https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/">Wolfram Language as a tool</a> to generate the proof. So what happens if we do that, then feed the finished proof to the LLM and ask it to explain? Well, typically it just does what LLMs do so well, and writes an essay:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/01/sw01072025rescueimg2.png" alt="LLM proof essay" title="LLM proof essay" width="614" height="516"></p>
<p>So, yes, it does fine in “generally framing the problem”. But not on the details. And if you press it for details, it’ll typically eventually just start parroting what it was given as input. </p>
<p>How else might we try to get the LLM to help? One thing I’ve certainly wondered is how the lemmas in the proof relate to known theorems—perhaps in quite different areas of mathematics. It’s something one might imagine one would be able to answer by searching the literature of mathematics. But, for example, textual search won’t be sufficient: it has to be some form of <a href="https://writings.stephenwolfram.com/2024/07/yet-more-new-ideas-and-new-functions-launching-version-14-1-of-wolfram-language-mathematica/#vector-databases-and-semantic-search">semantic search</a> based on the meaning or symbolic structure of lemmas, not their (fairly arbitrary) textual presentation. A vector database might be all one needs, but one can certainly ask an LLM too:</p>
<p><img src="https://content.wolfram.com/sites/43/2025/01/sw01072025rescueimg3.png" alt="LLM semantic search results" title="LLM semantic search results" width="619" height="485"></p>
<p>It’s not extremely helpful, though, charmingly, it correctly identifies the source of our original axiom. I’ve tried similar queries for our whole set of lemmas across a variety of LLMs, with a variety of RAG systems. Often the LLM will talk about an interpretation for some lemma—but the lemma isn’t actual present in our proof. But occasionally the LLM will mention possible connections (“band theory”; “left self-distributive operations in quandles”; “Moufang loops”)—though so far none have seemed to quite hit the mark.</p>
<p>And perhaps this failure is itself actually a result—telling us that the lemmas that show up in our proof really are, in effect, out in the wilds of metamathematical space, probing places that haven’t ever been seriously visited before by human mathematics.</p>
<p>But beyond LLMs, what about more general machine learning and neural net approaches? Could we imagine <a href="https://writings.stephenwolfram.com/2024/03/can-ai-solve-science/#science-as-narrative">using a neural net as a probe to find “exploitable regularities”</a> in our proof? It’s certainly possible, but I suspect that the systematic algorithmic methods we’ve already discussed for finding optimal notations, popular lemmas, etc. will tend to do better. I suppose it would be one thing if our systematic methods had failed to even find a proof. Then we might have wanted something like neural nets to try to guess the right paths to follow, etc. But as it is, our systematic methods rather efficiently do manage to successfully find a proof. </p>
<p>Of course, there’s still the issue that we’re discussing here that the proof is very “non-human”. And perhaps we could imagine that neural nets, etc.—especially when trained on existing human knowledge—could be used to “form concepts” that would help us humans to understand the proof. </p>
<p>We can get at least a rough analogy for how this might work by looking at <a href="https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/">visual images produced by a generative AI system</a> trained from billions of human-selected images. There’s a concept (like “a cube”) that exists somewhere in the feature space of possible images. But “around” that concept are other things—<a href="https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/#the-notion-of-interconcept-space">“out in interconcept space”</a>—that we don’t (at least yet) explicitly have words for:</p>
<div>
<p><img src="https://content.wolfram.com/sites/43/2025/01/sw01072025rescueimg4.png" alt="Interconcept space" title="Interconcept space" width="494" height="494"></p>
</div>
<p>And it’ll presumably be similar for math, though harder to represent in something like a visual way. There’ll be existing math concepts. But these will be embedded in a vast domain of “mathematical interconcept space” that we humans haven’t yet “colonized”. And what we can imagine is that—perhaps with the help of neural nets, etc.—we can identify a limited number of “points in interconcept space” that we can introduce as new concepts that will, for example, provide useful “waypoints” in understanding our proof.</p>
<h2 id="but-why-is-the-theorem-true">But Why Is the Theorem True?</h2>
<p>It’s a common human urge to think that anything that’s true must be true for a reason. But what about our theorem? Why is it true? Well, we’ve seen a proof. But somehow that doesn’t seem satisfactory. We want “an explanation we can understand”. But we know that in general we can’t always expect to get one.</p>
<p>It’s a fundamental implication of computational irreducibility that things can happen where the only way to “see how they happen” is just to “watch them happen”; there’s no way to “compress the explanation”.</p>
<p>Consider the following patterns. They’re all generated by cellular automata. And all <a href="https://writings.stephenwolfram.com/2024/05/why-does-biological-evolution-work-a-minimal-model-for-biological-evolution-and-other-adaptive-processes/">live exactly 100 steps before dying out</a>. But why?</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025theoremimg1-1.png" alt="" title="" width="597" height="509"> </p>
</div>
<p>In a few cases it seems like we can perhaps at least begin to imagine “narratively describing” a mechanism. But most of the time all we can say is basically that they “live 100 steps because they do”. </p>
<p>It’s a quintessential consequence of computational irreducibility. It might not be what we’d expect, or hope for. But it’s reality in the computational universe. And it seems very likely that our theorem—and its proof—is like this too. The theorem in effect “just happens to be true”—and if you run the steps in the proof (or find the appropriate path in the entailment cone) you’ll find that it is. But there’s no “narrative explanation”. No “understanding of why it’s true”. </p>
<h2 id="intuition-and-automated-theorem-proving">Intuition and Automated Theorem Proving</h2>
<p>We’ve been talking a lot about the proof of our theorem. But where did the theorem to prove come from in the first place? Its immediate origin was an <a href="https://www.wolframscience.com/nks/notes-12-9--searching-for-logic-axioms/">exhaustive search I did of simple axiom systems</a>, filtering for ones that could conceivably generate Boolean algebra, followed by testing each of the candidates using automated theorem proving. </p>
<p>But how did I even get the idea of searching for a simple axiom system for Boolean algebra? Based on the axiom systems for Boolean algebra known before—and the historical difficulty of finding them—one might have concluded that it was quite hopeless to find an axiom system for Boolean algebra by exhaustive search. But by 2000 I had nearly two decades of experience in exploring the computational universe—and I was well used to the <a href="https://www.wolframscience.com/nks/chap-2--the-crucial-experiment/">remarkable phenomenon</a> that even very simple computational rules can lead to behavior of great complexity. So the result was that when I came to think about axiom systems and the foundations of mathematics my intuition led me to imagine that perhaps the simplest axiom system for something like Boolean algebra might be simple enough to exhaustively search for.</p>
<p>And indeed discovering the axiom system we’ve discussed here helped further expand and deepen my intuition about the consequences of simple rules. But what about the proof? What intuition might one get from the proof as we now know it, and as we’ve discussed here?</p>
<p>There’s much intuition to be got from observing the world as it is. But for nearly half a century I’ve had another crucial source of intuition: observing the computational universe—and doing computational experiments. I was recently reflecting on how I came to start developing intuition in this way. And what it might mean for intuition I could now develop from things like automated theorem proving and AI.</p>
<p>Back in the mid-1970s <a href="https://www.stephenwolfram.com/publications/academic/particle-physics">my efforts in particle physics</a> led me to start using computers to do not just numerical, but <a href="https://writings.stephenwolfram.com/2013/06/there-was-a-time-before-mathematica/">also algebraic computations</a>. In numerical computations it was usual to just get a few numbers out, that perhaps one could plot to make a curve. But in algebraic computations one instead got out formulas—and <a href="https://content.wolfram.com/sw-publications/2020/07/effective-coupling-qcd.pdf" target="_blank" rel="noopener">often very ornate ones full of structure and detail</a>. And for me it was routine to get not just one formula, but many. And looking at these formulas I started to develop intuition about them. What functions would they involve? What algebraic form would they take? What kind of numbers would they involve? </p>
<p>I don’t think I ever consciously realized that I was developing a new kind of computationally based intuition. But I soon began to take it for granted. And when—at the beginning of the 1980s—<a href="https://www.wolframscience.com/nks/chap-1--the-foundations-for-a-new-kind-of-science#sect-1-4--the-personal-story-of-the-science-in-this-book">I started to explore the consequences of simple abstract systems</a> like cellular automata it was natural to expect that I would get intuition from just “seeing” how they behaved. And here there was also another important element. Because part of the reason I concentrated on cellular automata was precisely because one could readily visualize their behavior on a computer. </p>
<p>I don’t think I would have learned much if I’d just been printing out “numerical summaries” of what cellular automata do. But as it was, I was seeing their behavior in full detail. And—surprising though what I saw was—I was soon able to start getting an intuition for what could happen. It wasn’t a matter of knowing what the value of every cell would be. But I started doing things like identifying four general classes of cellular automata, and then recognizing the phenomenon of computational irreducibility. </p>
<p>By the 1990s I was much more broadly exploring the computational universe—always trying to see what could happen there. And in almost all cases it was a story of defining simple rules, then running them, and making an explicit step-by-step visualization of what they do—and thereby in effect “seeing computation in action”.</p>
<p>In recent years—spurred by our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a>—I’ve increasingly explored not just computational processes, but also <a href="https://writings.stephenwolfram.com/2021/09/multicomputation-a-fourth-paradigm-for-theoretical-science/">multicomputational ones</a>. And although it’s more difficult I’ve made every effort to visualize the behavior of multiway systems—and to get intuition about what they do. </p>
<p>But what about automated theorem proving? In effect, automated theorem proving is about finding a particular path in a multiway system that leads to a theorem we want. We’re not getting to see “complete behavior”; we’re in effect just seeing one particular “solution” for how to prove a theorem. </p>
<p>And after one’s seen many examples, the challenge once again is to develop intuition. And that’s a large part of what I’ve been trying to do here. It’s crucial, I think, to have some way to visualize what’s happening—in effect because visual input is the most efficient way to get information into our brains. And while the visualizations we’ve developed here aren’t as direct and complete as, say, for cellular automaton evolution, I think they begin to give some overall sense of our proof—and other proofs like it.</p>
<p>In studying simple programs like cellular automata, the intuition I developed led me to things like my <a href="https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-2--four-classes-of-behavior">classification of cellular automaton behavior</a>, as well as to bigger ideas like the <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/">Principle of Computational Equivalence</a> and computational irreducibility. So having now exposed myself to automated theorem proving as I exposed myself to algebraic computation and the running of simple rules in the past, what general principles might I begin to see? And might they, for example, somehow make the fact that our proof works ultimately seem “obvious”?</p>
<p>In some ways yes, but in other ways no. Much as with simple programs, there are axiom systems so simple that, for example, the <a href="https://www.wolframscience.com/metamathematics/axiom-systems-in-the-wild/">multiway systems they generate are highly regular</a>. But beyond a low threshold, it’s common to get very complicated—and in many ways seemingly random—multiway system structures. Typically an infinite number of lemmas are generated, with little or no obvious regularity in their forms.</p>
<p>And one can expect that—following the ideas of universal computation—it’ll typically be possible to encode in any one such multiway system the behavior of any other multiway system. In terms of axioms what one’s saying is that if one sets up the right translation between theorems, one will be able to use any one such axiom system to generate the theorems of any other. But the issue is that the translation will often make major changes to the structure of the theorems, and in effect define not just a “mathematical translation” (like between geometry and algebra) but a <a href="https://www.wolframscience.com/metamathematics/uniformity-and-motion-in-metamathematical-space/#p-146">metamathematical one (as one would need to get from Peano arithmetic to set theory)</a>. </p>
<p>And what this means is that it isn’t surprising that even a very simple axiom system can generate a complicated set of possible lemmas. But knowing this doesn’t immediately tell one whether those lemmas will align with some particular existing theory—like Boolean algebra. And in a sense that’s a much more detailed question.</p>
<p>At some metamathematical level it might not be a natural question. But at a “mathematical level” it is. And it’s what we have to address in connection with the theorem—and proof—we’re discussing here. Many aspects of the overall form and properties of the proof will be quite generic, and won’t depend on the particulars of the axiom system we’re using. But some will. And quite what intuition we may be able to get about these isn’t clear. And perhaps it’ll necessarily be fragmented and specific—in effect responding to the presence of computational irreducibility.</p>
<p>It’s perhaps worth commenting that LLMs—and machine learning in general—represent another potential source of intuition. That intuition may well be more about the general features of us as observers and thinkers. But such intuition is potentially critical in framing just what we can experience, not only in the natural world, but also in the mathematical and metamathematical worlds. And perhaps the apparent impotence of LLMs when faced with the proof we’ve been discussing already tells us something significant about the nature of “mathematical observers” like us.</p>
<h2 id="so-what-does-it-mean-for-the-future-of-mathematics">So What Does It Mean for the Future of Mathematics?</h2>
<p>Let’s say we never manage to “humanize” the proof we’ve been discussing here. Then in effect we’ll end up with a “black-box theorem”—that we can be sure is true—but we’ll never know quite how or why. So what would that mean for mathematics?</p>
<p>Traditionally, mathematics has tended to operate in a “white box” kind of way, trying to build narrative and understanding along with “facts”. And in this respect it’s very different from natural science. Because in natural science much of our knowledge has traditionally been empirical—derived from observing the world or experimenting on it—and without any certainty that we can “understand its origins”. </p>
<p>Automated theorem proving of the kind we’re discussing here—or, for that matter, pretty much any exploratory computational experimentation—aligns mathematics much more with natural science, deriving what’s true without an expectation of having a narrative explanation of why. </p>
<p>Could one imagine practicing mathematics that way? One’s already to some extent following such a path as soon as one introduces axiom systems to base one’s mathematics on. Where do the axiom systems come from? In <a href="https://writings.stephenwolfram.com/2020/09/the-empirical-metamathematics-of-euclid-and-beyond/">the time of Euclid</a> perhaps they were thought of as an idealization of nature. But in more modern times they are realistically much more the result of human choice and human aesthetics.</p>
<p>So let’s say we determine (given a particular axiom system) that some black-box theorem is true. Well, then we can just add it, just as we could another axiom. Maybe one day it’ll be possible to prove <a href="https://www.wolframscience.com/nks/p765--undecidability-and-intractability/">P≠NP</a> or the <a href="https://writings.stephenwolfram.com/2021/03/after-100-years-can-we-finally-crack-posts-problem-of-tag-a-story-of-computational-irreducibility-and-more/#classic-unsolved">Riemann Hypothesis</a> from existing axioms of mathematics (if they don’t in fact turn out to be independent). And—black box or not—we can expect to add them to what we assume in subsequent mathematics we do, much as they’re routinely added right now, even though their status isn’t yet known. </p>
<p>But it’s one thing to add one or two “black-box theorems”. But what happens when black-box theorems—that we can think of as “experimentally determined”—start to dominate the landscape of mathematics? </p>
<p>Well, then mathematics will take on much more of the character of ruliology—or of an experimental science. When it comes to the applications of mathematics, this probably won’t make much difference, except that in effect mathematics will be able to become much more powerful. But the “inner experience” of mathematics will be quite different—and much less “human”.</p>
<p>If one indeed starts from axioms, it’s not at the outset obvious why everything in mathematics should not be mired in the kind of alien-seeming metamathematical complexity that we’ve encountered in the discussion of our proof here. But <a href="https://www.wolframscience.com/metamathematics/mathematics-and-physics-have-the-same-foundations/">what I’ve argued elsewhere</a> is that the fact that in our experience of doing mathematics it’s not is a reflection of how “mathematical observers like us” sample the raw metamathematical structure generated by axioms (or ultimately by the <a href="https://www.wolframscience.com/metamathematics/going-below-axiomatic-mathematics/">subaxiomatic structure of the ruliad</a>). </p>
<p>The physics analogy I’ve used is that we succeed in doing mathematics at a “fluid dynamics level”, far above the detailed “molecular dynamics level” of things like the proof we’ve discussed here. Yes, we can ask questions—like ones about the structure of our proof—that probe the axiomatic “molecular dynamics level”. But it’s an important fact that in doing what we normally think of as mathematics we almost never have to; there’s a coherent way to operate purely at the “fluid dynamics level”.</p>
<p>Is it useful to “dip down” to the molecular dynamics? Definitely yes, because that’s where we can readily do computations—like those in our proof, or in general those going on in the internals of the Wolfram Language. But a key idea in the design of the Wolfram Language is to provide a computational language that can express concepts at a humanized “fluid dynamics” level—in effect bridging between the way humans can think and understand things, and the way raw computation can be done with them.</p>
<p>And it’s notable that while we’ve had great success over the years in defining “human-accessible” high-level representations for what amount to the “inputs” and “outputs” of computations, that’s been much less true of the “ongoing processes” of computation—or, for example, of the innards of proofs. </p>
<p>Is there a good “human-level” way to represent proofs? If the proofs are short, it’s not too difficult (and the <a href="https://www.wolframalpha.com/pro/step-by-step-math-solver">step-by-step solutions technology of Wolfram|Alpha</a> provides a good large-scale example of what can be done). But—as we’ve discussed—computational irreducibility implies that some proofs will inevitably be long. </p>
<p>If they’re not too long, then at least some parts of them might be constructed by human effort, say in a system like a proof assistant. But as soon as there’s much automation (whether with automated theorem proving or with LLMs) it’s basically inevitable that one will end up with things that at least approach what we’ve seen with the proof we’re discussing here. </p>
<p>What can then be done? Well, that’s the challenge. Maybe there is some way to simplify, abstract or otherwise “humanize” the proof we’ve been discussing. But I rather doubt it. I think this is likely one of those cases where we inevitably find ourselves face to face with computational irreducibility. </p>
<p>And, yes, there’s important science (particularly ruliology) to do on the structures we see. But it’s not mathematics as it’s traditionally been practiced. But that’s not to say that the results that come out of things like our proof won’t be useful for mathematics. They will be. But they make mathematics more like an experimental science—where what matters most is in effect the input and output rather than a “publishable” or human-readable derivation in between. And where the key issue in making progress is less in the innards of derivations than in defining clear computational ways to express input and output. Or, in effect, in capturing “human-level mathematics” in the primitives and structure of <a href="https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/">computational language</a>. </p>
<h2 id="appendix-what-about-a-different-theorem-proving-system">Appendix: What about a Different Theorem Proving System?</h2>
<p>The proof we’ve been discussing here was created using <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> in the Wolfram Language. But what if we were to use a different automated theorem proving system? How different would the results be? In the spectrum of things that automated theorem proving systems do, our proof here is on the difficult end. And many existing automated theorem proving systems don’t manage to do it all. But some of the stronger ones do. And in the end—despite their different internal algorithms and heuristics—it’s remarkable how similar the results they give are to those from the Wolfram Language <tt>FindEquationalProof</tt> (differences in the way lemmas vs. inference steps, etc. are identified make detailed quantitative comparisons difficult):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2025/01/sw01072025appendiximg1.png" alt="" title="" width="650" height="433"> </p>
</div>
<h2 id="thanks">Thanks</h2>
<p>Thanks to Nik Murzin of the <a href="https://wolframinstitute.org/" target="_blank" rel="noopener">Wolfram Institute</a> for his extensive help as part of the Wolfram Institute Empirical Metamathematics Project. Also Roger Germundsson, Sergio Sandoval, Adam Strzebonski, Michael Trott, Liubov Tupikina, James Wiles and Carlos Zapata for input. Thanks to Arnim Buch and Thomas Hillenbrand for their work in the 1990s on Waldmeister which is now part of <tt><a href="http://reference.wolfram.com/language/ref/FindEquationalProof.html">FindEquationalProof</a></tt> (also to Jonathan Gorard for his 2017 work on the interface for <tt>FindEquationalProof)</tt>. I was first seriously introduced to automated theorem proving in the late 1980s by Dana Scott, and have interacted with many people about it over the years, including Richard Assar, Bruno Buchberger, David Hillman, Norm Megill, Todd Rowland and Matthew Szudzik. (I’ve also interacted with many people about proof assistant, proof presentation and proof verification systems, both recently and in the past.)</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia-Ingest: Multi-modal data extraction (105 pts)]]></title>
            <link>https://github.com/NVIDIA/nv-ingest</link>
            <guid>42654019</guid>
            <pubDate>Fri, 10 Jan 2025 09:17:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/NVIDIA/nv-ingest">https://github.com/NVIDIA/nv-ingest</a>, See on <a href="https://news.ycombinator.com/item?id=42654019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">NVIDIA-Ingest: Multi-modal data extraction</h2><a id="user-content-nvidia-ingest-multi-modal-data-extraction" aria-label="Permalink: NVIDIA-Ingest: Multi-modal data extraction" href="#nvidia-ingest-multi-modal-data-extraction"></a></p>
<p dir="auto">NVIDIA-Ingest is a scalable, performance-oriented document content and metadata extraction microservice. Including support for parsing PDFs, Word and PowerPoint documents, it uses specialized NVIDIA NIM microservices to find, contextualize, and extract text, tables, charts and images for use in downstream generative applications.</p>
<p dir="auto">NVIDIA Ingest enables parallelization of the process of splitting documents into pages where contents are classified (as tables, charts, images, text), extracted into discrete content, and further contextualized via optical character recognition (OCR) into a well defined JSON schema. From there, NVIDIA Ingest can optionally manage computation of embeddings for the extracted content, and also optionally manage storing into a vector database <a href="https://milvus.io/" rel="nofollow">Milvus</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Table of Contents</h3><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ol dir="auto">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#repo-structure">Repo Structure</a></li>
<li><a href="#notices">Notices</a></li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What NVIDIA-Ingest is ✔️</h3><a id="user-content-what-nvidia-ingest-is-️" aria-label="Permalink: What NVIDIA-Ingest is ✔️" href="#what-nvidia-ingest-is-️"></a></p>
<p dir="auto">A microservice that:</p>
<ul dir="auto">
<li>Accepts a JSON Job description, containing a document payload, and a set of ingestion tasks to perform on that payload.</li>
<li>Allows the results of a Job to be retrieved; the result is a JSON dictionary containing a list of Metadata describing objects extracted from the base document, as well as processing annotations and timing/trace data.</li>
<li>Supports PDF, Docx, pptx, and images.</li>
<li>Supports multiple methods of extraction for each document type in order to balance trade-offs between throughput and accuracy. For example, for PDF documents we support extraction via pdfium, Unstructured.io, and Adobe Content Extraction Services.</li>
<li>Supports various types of pre and post processing operations, including text splitting and chunking; transform, and filtering; embedding generation, and image offloading to storage.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">What NVIDIA-Ingest is not ✖️</h3><a id="user-content-what-nvidia-ingest-is-not-️" aria-label="Permalink: What NVIDIA-Ingest is not ✖️" href="#what-nvidia-ingest-is-not-️"></a></p>
<p dir="auto">A service that:</p>
<ul dir="auto">
<li>Runs a static pipeline or fixed set of operations on every submitted document.</li>
<li>Acts as a wrapper for any specific document parsing library.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware</h3><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>GPU</th>
<th>Family</th>
<th>Memory</th>
<th># of GPUs (min.)</th>
</tr>
</thead>
<tbody>
<tr>
<td>H100</td>
<td>SXM or PCIe</td>
<td>80GB</td>
<td>2</td>
</tr>
<tr>
<td>A100</td>
<td>SXM or PCIe</td>
<td>80GB</td>
<td>2</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Software</h3><a id="user-content-software" aria-label="Permalink: Software" href="#software"></a></p>
<ul dir="auto">
<li>Linux operating systems (Ubuntu 22.04 or later recommended)</li>
<li><a href="https://docs.docker.com/engine/install/" rel="nofollow">Docker</a></li>
<li><a href="https://docs.docker.com/compose/install/" rel="nofollow">Docker Compose</a></li>
<li><a href="https://developer.nvidia.com/cuda-downloads" rel="nofollow">CUDA Toolkit</a> (NVIDIA Driver &gt;= <code>535</code>, CUDA &gt;= <code>12.2</code>)</li>
<li><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html" rel="nofollow">NVIDIA Container Toolkit</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<p dir="auto">To get started using NVIDIA Ingest, you need to do a few things:</p>
<ol dir="auto">
<li><a href="#step-1-starting-containers">Start supporting NIM microservices</a> 🏗️</li>
<li><a href="#step-2-installing-python-dependencies">Install the NVIDIA Ingest client dependencies in a Python environment</a> 🐍</li>
<li><a href="#step-3-ingesting-documents">Submit ingestion job(s)</a> 📓</li>
<li><a href="#step-4-inspecting-and-consuming-results">Inspect and consume results</a> 🔍</li>
</ol>
<p dir="auto">Optional:</p>
<ol dir="auto">
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/docs/deployment.md">Direct Library Deployment</a> 📦</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 1: Starting containers</h3><a id="user-content-step-1-starting-containers" aria-label="Permalink: Step 1: Starting containers" href="#step-1-starting-containers"></a></p>
<p dir="auto">This example demonstrates how to use the provided <a href="https://github.com/NVIDIA/nv-ingest/blob/main/docker-compose.yaml">docker-compose.yaml</a> to start all needed services with a few commands.</p>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">NIM containers on their first startup can take 10-15 minutes to pull and fully load models.</p>
</div>
<p dir="auto">If preferred, you can also <a href="https://github.com/NVIDIA/nv-ingest/blob/main/docs/deployment.md">start services one by one</a>, or run on Kubernetes via <a href="https://github.com/NVIDIA/nv-ingest/blob/main/helm/README.md">our Helm chart</a>. Also of note are <a href="https://github.com/NVIDIA/nv-ingest/blob/main/docs/environment-config.md">additional environment variables</a> you may wish to configure.</p>
<ol dir="auto">
<li>
<p dir="auto">Git clone the repo:
<code>git clone https://github.com/nvidia/nv-ingest</code></p>
</li>
<li>
<p dir="auto">Change directory to the cloned repo
<code>cd nv-ingest</code>.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/NVIDIA/nv-ingest/blob/main/docs/ngc-api-key.md">Generate API keys</a> and authenticate with NGC with the <code>docker login</code> command:</p>
</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="# This is required to access pre-built containers and NIM microservices
$ docker login nvcr.io
Username: $oauthtoken
Password: <Your Key>"><pre><span><span>#</span> This is required to access pre-built containers and NIM microservices</span>
$ docker login nvcr.io
Username: <span>$oauthtoken</span>
Password: <span>&lt;</span>Your Key<span>&gt;</span></pre></div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">during the early access (EA) phase, your API key must be created as a member of <code>nemo-microservice / ea-participants</code> which you may join by applying for early access here: <a href="https://developer.nvidia.com/nemo-microservices-early-access/join" rel="nofollow">https://developer.nvidia.com/nemo-microservices-early-access/join</a>. When approved, switch your profile to this org / team, then the key you generate will have access to the resources outlined below.</p>
</div>
<ol start="4" dir="auto">
<li>Create a .env file containing your NGC API key, and the following paths:</li>
</ol>
<div data-snippet-clipboard-copy-content="# Container images must access resources from NGC.
NGC_API_KEY=... # Optional, set this if you are deploying NIMs locally from NGC
NVIDIA_BUILD_API_KEY=... # Optional, set this is you are using build.nvidia.com NIMs"><pre><code># Container images must access resources from NGC.
NGC_API_KEY=... # Optional, set this if you are deploying NIMs locally from NGC
NVIDIA_BUILD_API_KEY=... # Optional, set this is you are using build.nvidia.com NIMs
</code></pre></div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">As configured by default in <a href="https://github.com/NVIDIA/nv-ingest/blob/main/docker-compose.yaml#L52">docker-compose.yaml</a>, the DePlot NIM is on a dedicated GPU. All other NIMs and the nv-ingest container itself share a second. This is to avoid DePlot and other NIMs competing for VRAM on the same device.</p>
<p dir="auto">Change the <code>CUDA_VISIBLE_DEVICES</code> pinnings as desired for your system within docker-compose.yaml.</p>
</div>
<div dir="auto"><p dir="auto">Important</p><p dir="auto">Make sure NVIDIA is set as your default container runtime before running the docker compose command with the command:
<code>sudo nvidia-ctk runtime configure --runtime=docker --set-as-default</code></p>
</div>
<ol start="5" dir="auto">
<li>Start all services:
<code>docker compose up</code></li>
</ol>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">By default we have <a href="https://github.com/NVIDIA/nv-ingest/blob/main/docker-compose.yaml#L27">configured log levels to be verbose</a>.</p>
<p dir="auto">It's possible to observe service startup proceeding: you will notice <em>many</em> log messages. Disable verbose logging by configuring <code>NIM_TRITON_LOG_VERBOSE=0</code> for each NIM in <a href="https://github.com/NVIDIA/nv-ingest/blob/main/docker-compose.yaml">docker-compose.yaml</a>.</p>
<p dir="auto">If you want to build from source, use <code>docker compose up --build</code> instead. This will build from your repo's code rather than from an already published container.</p>
</div>
<ol start="6" dir="auto">
<li>When all services have fully started, <code>nvidia-smi</code> should show processes like the following:</li>
</ol>
<div data-snippet-clipboard-copy-content="# If it's taking > 1m for `nvidia-smi` to return, it's likely the bus is still busy setting up the models.
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   1352957      C   tritonserver                                762MiB |
|    1   N/A  N/A   1322081      C   /opt/nim/llm/.venv/bin/python3            63916MiB |
|    2   N/A  N/A   1355175      C   tritonserver                                478MiB |
|    2   N/A  N/A   1367569      C   ...s/python/triton_python_backend_stub       12MiB |
|    3   N/A  N/A   1321841      C   python                                      414MiB |
|    3   N/A  N/A   1352331      C   tritonserver                                478MiB |
|    3   N/A  N/A   1355929      C   ...s/python/triton_python_backend_stub      424MiB |
|    3   N/A  N/A   1373202      C   tritonserver                                414MiB |
+---------------------------------------------------------------------------------------+"><pre><code># If it's taking &gt; 1m for `nvidia-smi` to return, it's likely the bus is still busy setting up the models.
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   1352957      C   tritonserver                                762MiB |
|    1   N/A  N/A   1322081      C   /opt/nim/llm/.venv/bin/python3            63916MiB |
|    2   N/A  N/A   1355175      C   tritonserver                                478MiB |
|    2   N/A  N/A   1367569      C   ...s/python/triton_python_backend_stub       12MiB |
|    3   N/A  N/A   1321841      C   python                                      414MiB |
|    3   N/A  N/A   1352331      C   tritonserver                                478MiB |
|    3   N/A  N/A   1355929      C   ...s/python/triton_python_backend_stub      424MiB |
|    3   N/A  N/A   1373202      C   tritonserver                                414MiB |
+---------------------------------------------------------------------------------------+
</code></pre></div>
<p dir="auto">Observe the started containers with <code>docker ps</code>:</p>
<div data-snippet-clipboard-copy-content="CONTAINER ID   IMAGE                                                                      COMMAND                  CREATED          STATUS                    PORTS                                                                                                                                                                                                                                                                                NAMES
0f2f86615ea5   nvcr.io/ohlfw0olaadg/ea-participants/nv-ingest:24.10                       &quot;/opt/conda/bin/tini…&quot;   35 seconds ago   Up 33 seconds             0.0.0.0:7670->7670/tcp, :::7670->7670/tcp                                                                                                                                                                                                                                            nv-ingest-nv-ingest-ms-runtime-1
de44122c6ddc   otel/opentelemetry-collector-contrib:0.91.0                                &quot;/otelcol-contrib --…&quot;   14 hours ago     Up 24 seconds             0.0.0.0:4317-4318->4317-4318/tcp, :::4317-4318->4317-4318/tcp, 0.0.0.0:8888-8889->8888-8889/tcp, :::8888-8889->8888-8889/tcp, 0.0.0.0:13133->13133/tcp, :::13133->13133/tcp, 55678/tcp, 0.0.0.0:32849->9411/tcp, :::32848->9411/tcp, 0.0.0.0:55680->55679/tcp, :::55680->55679/tcp   nv-ingest-otel-collector-1
02c9ab8c6901   nvcr.io/ohlfw0olaadg/ea-participants/cached:0.2.0                          &quot;/opt/nvidia/nvidia_…&quot;   14 hours ago     Up 24 seconds             0.0.0.0:8006->8000/tcp, :::8006->8000/tcp, 0.0.0.0:8007->8001/tcp, :::8007->8001/tcp, 0.0.0.0:8008->8002/tcp, :::8008->8002/tcp                                                                                                                                                      nv-ingest-cached-1
d49369334398   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.1.0                                  &quot;/opt/nvidia/nvidia_…&quot;   14 hours ago     Up 33 seconds             0.0.0.0:8012->8000/tcp, :::8012->8000/tcp, 0.0.0.0:8013->8001/tcp, :::8013->8001/tcp, 0.0.0.0:8014->8002/tcp, :::8014->8002/tcp                                                                                                                                                      nv-ingest-embedding-1
508715a24998   nvcr.io/ohlfw0olaadg/ea-participants/nv-yolox-structured-images-v1:0.2.0   &quot;/opt/nvidia/nvidia_…&quot;   14 hours ago     Up 33 seconds             0.0.0.0:8000-8002->8000-8002/tcp, :::8000-8002->8000-8002/tcp                                                                                                                                                                                                                        nv-ingest-yolox-1
5b7a174a0a85   nvcr.io/ohlfw0olaadg/ea-participants/deplot:1.0.0                          &quot;/opt/nvidia/nvidia_…&quot;   14 hours ago     Up 33 seconds             0.0.0.0:8003->8000/tcp, :::8003->8000/tcp, 0.0.0.0:8004->8001/tcp, :::8004->8001/tcp, 0.0.0.0:8005->8002/tcp, :::8005->8002/tcp                                                                                                                                                      nv-ingest-deplot-1
430045f98c02   nvcr.io/ohlfw0olaadg/ea-participants/paddleocr:0.2.0                       &quot;/opt/nvidia/nvidia_…&quot;   14 hours ago     Up 24 seconds             0.0.0.0:8009->8000/tcp, :::8009->8000/tcp, 0.0.0.0:8010->8001/tcp, :::8010->8001/tcp, 0.0.0.0:8011->8002/tcp, :::8011->8002/tcp                                                                                                                                                      nv-ingest-paddle-1
8e587b45821b   grafana/grafana                                                            &quot;/run.sh&quot;                14 hours ago     Up 33 seconds             0.0.0.0:3000->3000/tcp, :::3000->3000/tcp                                                                                                                                                                                                                                            grafana-service
aa2c0ec387e2   redis/redis-stack                                                          &quot;/entrypoint.sh&quot;         14 hours ago     Up 33 seconds             0.0.0.0:6379->6379/tcp, :::6379->6379/tcp, 8001/tcp                                                                                                                                                                                                                                  nv-ingest-redis-1
bda9a2a9c8b5   openzipkin/zipkin                                                          &quot;start-zipkin&quot;           14 hours ago     Up 33 seconds (healthy)   9410/tcp, 0.0.0.0:9411->9411/tcp, :::9411->9411/tcp                                                                                                                                                                                                                                  nv-ingest-zipkin-1
ac27e5297d57   prom/prometheus:latest                                                     &quot;/bin/prometheus --w…&quot;   14 hours ago     Up 33 seconds             0.0.0.0:9090->9090/tcp, :::9090->9090/tcp                                                                                                                                                                                                                                            nv-ingest-prometheus-1"><pre><code>CONTAINER ID   IMAGE                                                                      COMMAND                  CREATED          STATUS                    PORTS                                                                                                                                                                                                                                                                                NAMES
0f2f86615ea5   nvcr.io/ohlfw0olaadg/ea-participants/nv-ingest:24.10                       "/opt/conda/bin/tini…"   35 seconds ago   Up 33 seconds             0.0.0.0:7670-&gt;7670/tcp, :::7670-&gt;7670/tcp                                                                                                                                                                                                                                            nv-ingest-nv-ingest-ms-runtime-1
de44122c6ddc   otel/opentelemetry-collector-contrib:0.91.0                                "/otelcol-contrib --…"   14 hours ago     Up 24 seconds             0.0.0.0:4317-4318-&gt;4317-4318/tcp, :::4317-4318-&gt;4317-4318/tcp, 0.0.0.0:8888-8889-&gt;8888-8889/tcp, :::8888-8889-&gt;8888-8889/tcp, 0.0.0.0:13133-&gt;13133/tcp, :::13133-&gt;13133/tcp, 55678/tcp, 0.0.0.0:32849-&gt;9411/tcp, :::32848-&gt;9411/tcp, 0.0.0.0:55680-&gt;55679/tcp, :::55680-&gt;55679/tcp   nv-ingest-otel-collector-1
02c9ab8c6901   nvcr.io/ohlfw0olaadg/ea-participants/cached:0.2.0                          "/opt/nvidia/nvidia_…"   14 hours ago     Up 24 seconds             0.0.0.0:8006-&gt;8000/tcp, :::8006-&gt;8000/tcp, 0.0.0.0:8007-&gt;8001/tcp, :::8007-&gt;8001/tcp, 0.0.0.0:8008-&gt;8002/tcp, :::8008-&gt;8002/tcp                                                                                                                                                      nv-ingest-cached-1
d49369334398   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.1.0                                  "/opt/nvidia/nvidia_…"   14 hours ago     Up 33 seconds             0.0.0.0:8012-&gt;8000/tcp, :::8012-&gt;8000/tcp, 0.0.0.0:8013-&gt;8001/tcp, :::8013-&gt;8001/tcp, 0.0.0.0:8014-&gt;8002/tcp, :::8014-&gt;8002/tcp                                                                                                                                                      nv-ingest-embedding-1
508715a24998   nvcr.io/ohlfw0olaadg/ea-participants/nv-yolox-structured-images-v1:0.2.0   "/opt/nvidia/nvidia_…"   14 hours ago     Up 33 seconds             0.0.0.0:8000-8002-&gt;8000-8002/tcp, :::8000-8002-&gt;8000-8002/tcp                                                                                                                                                                                                                        nv-ingest-yolox-1
5b7a174a0a85   nvcr.io/ohlfw0olaadg/ea-participants/deplot:1.0.0                          "/opt/nvidia/nvidia_…"   14 hours ago     Up 33 seconds             0.0.0.0:8003-&gt;8000/tcp, :::8003-&gt;8000/tcp, 0.0.0.0:8004-&gt;8001/tcp, :::8004-&gt;8001/tcp, 0.0.0.0:8005-&gt;8002/tcp, :::8005-&gt;8002/tcp                                                                                                                                                      nv-ingest-deplot-1
430045f98c02   nvcr.io/ohlfw0olaadg/ea-participants/paddleocr:0.2.0                       "/opt/nvidia/nvidia_…"   14 hours ago     Up 24 seconds             0.0.0.0:8009-&gt;8000/tcp, :::8009-&gt;8000/tcp, 0.0.0.0:8010-&gt;8001/tcp, :::8010-&gt;8001/tcp, 0.0.0.0:8011-&gt;8002/tcp, :::8011-&gt;8002/tcp                                                                                                                                                      nv-ingest-paddle-1
8e587b45821b   grafana/grafana                                                            "/run.sh"                14 hours ago     Up 33 seconds             0.0.0.0:3000-&gt;3000/tcp, :::3000-&gt;3000/tcp                                                                                                                                                                                                                                            grafana-service
aa2c0ec387e2   redis/redis-stack                                                          "/entrypoint.sh"         14 hours ago     Up 33 seconds             0.0.0.0:6379-&gt;6379/tcp, :::6379-&gt;6379/tcp, 8001/tcp                                                                                                                                                                                                                                  nv-ingest-redis-1
bda9a2a9c8b5   openzipkin/zipkin                                                          "start-zipkin"           14 hours ago     Up 33 seconds (healthy)   9410/tcp, 0.0.0.0:9411-&gt;9411/tcp, :::9411-&gt;9411/tcp                                                                                                                                                                                                                                  nv-ingest-zipkin-1
ac27e5297d57   prom/prometheus:latest                                                     "/bin/prometheus --w…"   14 hours ago     Up 33 seconds             0.0.0.0:9090-&gt;9090/tcp, :::9090-&gt;9090/tcp                                                                                                                                                                                                                                            nv-ingest-prometheus-1
</code></pre></div>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">nv-ingest is in Early Access mode, meaning the codebase gets frequent updates. To build an updated nv-ingest service container with the latest changes you can:</p>

<p dir="auto">After the image is built, run <code>docker compose up</code> per item 5 above.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 2: Installing Python dependencies</h3><a id="user-content-step-2-installing-python-dependencies" aria-label="Permalink: Step 2: Installing Python dependencies" href="#step-2-installing-python-dependencies"></a></p>
<p dir="auto">To interact with the nv-ingest service, you can do so from the host, or by <code>docker exec</code>-ing into the nv-ingest container.</p>
<p dir="auto">To interact from the host, you'll need a Python environment and install the client dependencies:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# conda not required, but makes it easy to create a fresh python environment
conda create --name nv-ingest-dev --file ./conda/environments/nv_ingest_environment.yml
conda activate nv-ingest-dev

cd client
pip install .

# When not using Conda, pip dependencies for the client can be installed directly via pip. Pip based installation of
# the ingest service is not supported.
cd client
pip install -r requirements.txt
pip install ."><pre><span><span>#</span> conda not required, but makes it easy to create a fresh python environment</span>
conda create --name nv-ingest-dev --file ./conda/environments/nv_ingest_environment.yml
conda activate nv-ingest-dev

<span>cd</span> client
pip install <span>.</span>

<span><span>#</span> When not using Conda, pip dependencies for the client can be installed directly via pip. Pip based installation of</span>
<span><span>#</span> the ingest service is not supported.</span>
<span>cd</span> client
pip install -r requirements.txt
pip install <span>.</span></pre></div>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Interacting from the host depends on the appropriate port being exposed from the nv-ingest container to the host as defined in <a href="https://github.com/NVIDIA/nv-ingest/blob/main/docker-compose.yaml#L141">docker-compose.yaml</a>.</p>
<p dir="auto">If you prefer, you can disable exposing that port, and interact with the nv-ingest service directly from within its container.</p>
<p dir="auto">To interact within the container:</p>
<div data-snippet-clipboard-copy-content="docker exec -it nv-ingest-nv-ingest-ms-runtime-1 bash"><pre><code>docker exec -it nv-ingest-nv-ingest-ms-runtime-1 bash
</code></pre></div>
<p dir="auto">You'll be in the <code>/workspace</code> directory, which has <code>DATASET_ROOT</code> from the .env file mounted at <code>./data</code>. The pre-activated <code>morpheus</code> conda environment has all the python client libraries pre-installed:</p>
<div data-snippet-clipboard-copy-content="(morpheus) root@aba77e2a4bde:/workspace#"><pre><code>(morpheus) root@aba77e2a4bde:/workspace#
</code></pre></div>
<p dir="auto">From the bash prompt above, you can run nv-ingest-cli and Python examples described below.</p>
</div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 3: Ingesting Documents</h3><a id="user-content-step-3-ingesting-documents" aria-label="Permalink: Step 3: Ingesting Documents" href="#step-3-ingesting-documents"></a></p>
<p dir="auto">You can submit jobs programmatically in Python or via the nv-ingest-cli tool.</p>
<p dir="auto">In the below examples, we are doing text, chart, table, and image extraction:</p>
<ul dir="auto">
<li><code>extract_text</code>, - uses <a href="https://github.com/pypdfium2-team/pypdfium2/">PDFium</a> to find and extract text from pages</li>
<li><code>extract_images</code> - uses <a href="https://github.com/pypdfium2-team/pypdfium2/">PDFium</a> to extract images</li>
<li><code>extract_tables</code> - uses <a href="https://github.com/Megvii-BaseDetection/YOLOX">YOLOX</a> to find tables and charts. Uses <a href="https://github.com/PaddlePaddle/PaddleOCR">PaddleOCR</a> for table extraction, and <a href="https://huggingface.co/google/deplot" rel="nofollow">Deplot</a> and CACHED for chart extraction</li>
<li><code>extract_charts</code> - (optional) enables or disables the use of Deplot and CACHED for chart extraction.</li>
</ul>
<div dir="auto"><p dir="auto">Important</p><p dir="auto"><code>extract_tables</code> controls extraction for both tables and charts. You can optionally disable chart extraction by setting <code>extract_charts</code> to false.</p>
</div>
<p dir="auto"><h4 tabindex="-1" dir="auto">In Python (you can find more documentation and examples <a href="https://github.com/NVIDIA/nv-ingest/blob/main/client/client_examples/examples/python_client_usage.ipynb">here</a>):</h4><a id="user-content-in-python-you-can-find-more-documentation-and-examples-here" aria-label="Permalink: In Python (you can find more documentation and examples here):" href="#in-python-you-can-find-more-documentation-and-examples-here"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import logging, time

from nv_ingest_client.client import NvIngestClient
from nv_ingest_client.primitives import JobSpec
from nv_ingest_client.primitives.tasks import ExtractTask
from nv_ingest_client.util.file_processing.extract import extract_file_content

logger = logging.getLogger(&quot;nv_ingest_client&quot;)

file_name = &quot;data/multimodal_test.pdf&quot;
file_content, file_type = extract_file_content(file_name)

# A JobSpec is an object that defines a document and how it should
# be processed by the nv-ingest service.
job_spec = JobSpec(
  document_type=file_type,
  payload=file_content,
  source_id=file_name,
  source_name=file_name,
  extended_options=
    {
      &quot;tracing_options&quot;:
      {
        &quot;trace&quot;: True,
        &quot;ts_send&quot;: time.time_ns()
      }
    }
)

# configure desired extraction modes here. Multiple extraction
# methods can be defined for a single JobSpec
extract_task = ExtractTask(
  document_type=file_type,
  extract_text=True,
  extract_images=True,
  extract_tables=True
)

job_spec.add_task(extract_task)

# Create the client and inform it about the JobSpec we want to process.
client = NvIngestClient(
  message_client_hostname=&quot;localhost&quot;, # Host where nv-ingest-ms-runtime is running
  message_client_port=7670 # REST port, defaults to 7670
)
job_id = client.add_job(job_spec)
client.submit_job(job_id, &quot;morpheus_task_queue&quot;)
result = client.fetch_job_result(job_id, timeout=60)
print(f&quot;Got {len(result)} results&quot;)"><pre><span>import</span> <span>logging</span>, <span>time</span>

<span>from</span> <span>nv_ingest_client</span>.<span>client</span> <span>import</span> <span>NvIngestClient</span>
<span>from</span> <span>nv_ingest_client</span>.<span>primitives</span> <span>import</span> <span>JobSpec</span>
<span>from</span> <span>nv_ingest_client</span>.<span>primitives</span>.<span>tasks</span> <span>import</span> <span>ExtractTask</span>
<span>from</span> <span>nv_ingest_client</span>.<span>util</span>.<span>file_processing</span>.<span>extract</span> <span>import</span> <span>extract_file_content</span>

<span>logger</span> <span>=</span> <span>logging</span>.<span>getLogger</span>(<span>"nv_ingest_client"</span>)

<span>file_name</span> <span>=</span> <span>"data/multimodal_test.pdf"</span>
<span>file_content</span>, <span>file_type</span> <span>=</span> <span>extract_file_content</span>(<span>file_name</span>)

<span># A JobSpec is an object that defines a document and how it should</span>
<span># be processed by the nv-ingest service.</span>
<span>job_spec</span> <span>=</span> <span>JobSpec</span>(
  <span>document_type</span><span>=</span><span>file_type</span>,
  <span>payload</span><span>=</span><span>file_content</span>,
  <span>source_id</span><span>=</span><span>file_name</span>,
  <span>source_name</span><span>=</span><span>file_name</span>,
  <span>extended_options</span><span>=</span>
    {
      <span>"tracing_options"</span>:
      {
        <span>"trace"</span>: <span>True</span>,
        <span>"ts_send"</span>: <span>time</span>.<span>time_ns</span>()
      }
    }
)

<span># configure desired extraction modes here. Multiple extraction</span>
<span># methods can be defined for a single JobSpec</span>
<span>extract_task</span> <span>=</span> <span>ExtractTask</span>(
  <span>document_type</span><span>=</span><span>file_type</span>,
  <span>extract_text</span><span>=</span><span>True</span>,
  <span>extract_images</span><span>=</span><span>True</span>,
  <span>extract_tables</span><span>=</span><span>True</span>
)

<span>job_spec</span>.<span>add_task</span>(<span>extract_task</span>)

<span># Create the client and inform it about the JobSpec we want to process.</span>
<span>client</span> <span>=</span> <span>NvIngestClient</span>(
  <span>message_client_hostname</span><span>=</span><span>"localhost"</span>, <span># Host where nv-ingest-ms-runtime is running</span>
  <span>message_client_port</span><span>=</span><span>7670</span> <span># REST port, defaults to 7670</span>
)
<span>job_id</span> <span>=</span> <span>client</span>.<span>add_job</span>(<span>job_spec</span>)
<span>client</span>.<span>submit_job</span>(<span>job_id</span>, <span>"morpheus_task_queue"</span>)
<span>result</span> <span>=</span> <span>client</span>.<span>fetch_job_result</span>(<span>job_id</span>, <span>timeout</span><span>=</span><span>60</span>)
<span>print</span>(<span>f"Got <span><span>{</span><span>len</span>(<span>result</span>)<span>}</span></span> results"</span>)</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Using the the <code>nv-ingest-cli</code> (you can find more nv-ingest-cli examples <a href="https://github.com/NVIDIA/nv-ingest/blob/main/client/client_examples/examples/cli_client_usage.ipynb">here</a>):</h4><a id="user-content-using-the-the-nv-ingest-cli-you-can-find-more-nv-ingest-cli-examples-here" aria-label="Permalink: Using the the nv-ingest-cli (you can find more nv-ingest-cli examples here):" href="#using-the-the-nv-ingest-cli-you-can-find-more-nv-ingest-cli-examples-here"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="nv-ingest-cli \
  --doc ./data/multimodal_test.pdf \
  --output_directory ./processed_docs \
  --task='extract:{&quot;document_type&quot;: &quot;pdf&quot;, &quot;extract_method&quot;: &quot;pdfium&quot;, &quot;extract_tables&quot;: &quot;true&quot;, &quot;extract_images&quot;: &quot;true&quot;}' \
  --client_host=localhost \
  --client_port=7670"><pre>nv-ingest-cli \
  --doc ./data/multimodal_test.pdf \
  --output_directory ./processed_docs \
  --task=<span><span>'</span>extract:{"document_type": "pdf", "extract_method": "pdfium", "extract_tables": "true", "extract_images": "true"}<span>'</span></span> \
  --client_host=localhost \
  --client_port=7670</pre></div>
<p dir="auto">You should notice output indicating document processing status, followed by a breakdown of time spent during job execution:</p>
<div data-snippet-clipboard-copy-content="INFO:nv_ingest_client.nv_ingest_cli:Processing 1 documents.
INFO:nv_ingest_client.nv_ingest_cli:Output will be written to: ./processed_docs
Processing files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.47s/file, pages_per_sec=0.29]
INFO:nv_ingest_client.cli.util.processing:dedup_images: Avg: 1.02 ms, Median: 1.02 ms, Total Time: 1.02 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:dedup_images_channel_in: Avg: 1.44 ms, Median: 1.44 ms, Total Time: 1.44 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:docx_content_extractor: Avg: 0.66 ms, Median: 0.66 ms, Total Time: 0.66 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:docx_content_extractor_channel_in: Avg: 1.09 ms, Median: 1.09 ms, Total Time: 1.09 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:filter_images: Avg: 0.84 ms, Median: 0.84 ms, Total Time: 0.84 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:filter_images_channel_in: Avg: 7.75 ms, Median: 7.75 ms, Total Time: 7.75 ms, Total % of Trace Computation: 0.07%
INFO:nv_ingest_client.cli.util.processing:job_counter: Avg: 2.13 ms, Median: 2.13 ms, Total Time: 2.13 ms, Total % of Trace Computation: 0.02%
INFO:nv_ingest_client.cli.util.processing:job_counter_channel_in: Avg: 2.05 ms, Median: 2.05 ms, Total Time: 2.05 ms, Total % of Trace Computation: 0.02%
INFO:nv_ingest_client.cli.util.processing:metadata_injection: Avg: 14.48 ms, Median: 14.48 ms, Total Time: 14.48 ms, Total % of Trace Computation: 0.14%
INFO:nv_ingest_client.cli.util.processing:metadata_injection_channel_in: Avg: 0.22 ms, Median: 0.22 ms, Total Time: 0.22 ms, Total % of Trace Computation: 0.00%
INFO:nv_ingest_client.cli.util.processing:pdf_content_extractor: Avg: 10332.97 ms, Median: 10332.97 ms, Total Time: 10332.97 ms, Total % of Trace Computation: 99.45%
INFO:nv_ingest_client.cli.util.processing:pdf_content_extractor_channel_in: Avg: 0.44 ms, Median: 0.44 ms, Total Time: 0.44 ms, Total % of Trace Computation: 0.00%
INFO:nv_ingest_client.cli.util.processing:pptx_content_extractor: Avg: 1.19 ms, Median: 1.19 ms, Total Time: 1.19 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:pptx_content_extractor_channel_in: Avg: 0.98 ms, Median: 0.98 ms, Total Time: 0.98 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:redis_source_network_in: Avg: 12.27 ms, Median: 12.27 ms, Total Time: 12.27 ms, Total % of Trace Computation: 0.12%
INFO:nv_ingest_client.cli.util.processing:redis_task_sink_channel_in: Avg: 2.16 ms, Median: 2.16 ms, Total Time: 2.16 ms, Total % of Trace Computation: 0.02%
INFO:nv_ingest_client.cli.util.processing:redis_task_source: Avg: 8.00 ms, Median: 8.00 ms, Total Time: 8.00 ms, Total % of Trace Computation: 0.08%
INFO:nv_ingest_client.cli.util.processing:Unresolved time: 82.82 ms, Percent of Total Elapsed: 0.79%
INFO:nv_ingest_client.cli.util.processing:Processed 1 files in 10.47 seconds.
INFO:nv_ingest_client.cli.util.processing:Total pages processed: 3
INFO:nv_ingest_client.cli.util.processing:Throughput (Pages/sec): 0.29
INFO:nv_ingest_client.cli.util.processing:Throughput (Files/sec): 0.10"><pre><code>INFO:nv_ingest_client.nv_ingest_cli:Processing 1 documents.
INFO:nv_ingest_client.nv_ingest_cli:Output will be written to: ./processed_docs
Processing files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10&lt;00:00, 10.47s/file, pages_per_sec=0.29]
INFO:nv_ingest_client.cli.util.processing:dedup_images: Avg: 1.02 ms, Median: 1.02 ms, Total Time: 1.02 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:dedup_images_channel_in: Avg: 1.44 ms, Median: 1.44 ms, Total Time: 1.44 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:docx_content_extractor: Avg: 0.66 ms, Median: 0.66 ms, Total Time: 0.66 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:docx_content_extractor_channel_in: Avg: 1.09 ms, Median: 1.09 ms, Total Time: 1.09 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:filter_images: Avg: 0.84 ms, Median: 0.84 ms, Total Time: 0.84 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:filter_images_channel_in: Avg: 7.75 ms, Median: 7.75 ms, Total Time: 7.75 ms, Total % of Trace Computation: 0.07%
INFO:nv_ingest_client.cli.util.processing:job_counter: Avg: 2.13 ms, Median: 2.13 ms, Total Time: 2.13 ms, Total % of Trace Computation: 0.02%
INFO:nv_ingest_client.cli.util.processing:job_counter_channel_in: Avg: 2.05 ms, Median: 2.05 ms, Total Time: 2.05 ms, Total % of Trace Computation: 0.02%
INFO:nv_ingest_client.cli.util.processing:metadata_injection: Avg: 14.48 ms, Median: 14.48 ms, Total Time: 14.48 ms, Total % of Trace Computation: 0.14%
INFO:nv_ingest_client.cli.util.processing:metadata_injection_channel_in: Avg: 0.22 ms, Median: 0.22 ms, Total Time: 0.22 ms, Total % of Trace Computation: 0.00%
INFO:nv_ingest_client.cli.util.processing:pdf_content_extractor: Avg: 10332.97 ms, Median: 10332.97 ms, Total Time: 10332.97 ms, Total % of Trace Computation: 99.45%
INFO:nv_ingest_client.cli.util.processing:pdf_content_extractor_channel_in: Avg: 0.44 ms, Median: 0.44 ms, Total Time: 0.44 ms, Total % of Trace Computation: 0.00%
INFO:nv_ingest_client.cli.util.processing:pptx_content_extractor: Avg: 1.19 ms, Median: 1.19 ms, Total Time: 1.19 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:pptx_content_extractor_channel_in: Avg: 0.98 ms, Median: 0.98 ms, Total Time: 0.98 ms, Total % of Trace Computation: 0.01%
INFO:nv_ingest_client.cli.util.processing:redis_source_network_in: Avg: 12.27 ms, Median: 12.27 ms, Total Time: 12.27 ms, Total % of Trace Computation: 0.12%
INFO:nv_ingest_client.cli.util.processing:redis_task_sink_channel_in: Avg: 2.16 ms, Median: 2.16 ms, Total Time: 2.16 ms, Total % of Trace Computation: 0.02%
INFO:nv_ingest_client.cli.util.processing:redis_task_source: Avg: 8.00 ms, Median: 8.00 ms, Total Time: 8.00 ms, Total % of Trace Computation: 0.08%
INFO:nv_ingest_client.cli.util.processing:Unresolved time: 82.82 ms, Percent of Total Elapsed: 0.79%
INFO:nv_ingest_client.cli.util.processing:Processed 1 files in 10.47 seconds.
INFO:nv_ingest_client.cli.util.processing:Total pages processed: 3
INFO:nv_ingest_client.cli.util.processing:Throughput (Pages/sec): 0.29
INFO:nv_ingest_client.cli.util.processing:Throughput (Files/sec): 0.10
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Step 4: Inspecting and Consuming Results</h3><a id="user-content-step-4-inspecting-and-consuming-results" aria-label="Permalink: Step 4: Inspecting and Consuming Results" href="#step-4-inspecting-and-consuming-results"></a></p>
<p dir="auto">After the ingestion steps above have completed, you should be able to find <code>text</code> and <code>image</code> subfolders inside your processed docs folder. Each will contain JSON formatted extracted content and metadata.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">When processing has completed, you'll have separate result files for text and image data:</h4><a id="user-content-when-processing-has-completed-youll-have-separate-result-files-for-text-and-image-data" aria-label="Permalink: When processing has completed, you'll have separate result files for text and image data:" href="#when-processing-has-completed-youll-have-separate-result-files-for-text-and-image-data"></a></p>

<div dir="auto" data-snippet-clipboard-copy-content="processed_docs/:
image  structured  text

processed_docs/image:
multimodal_test.pdf.metadata.json

processed_docs/structured:
multimodal_test.pdf.metadata.json

processed_docs/text:
multimodal_test.pdf.metadata.json"><pre>processed_docs/:
image  structured  text

processed_docs/image:
multimodal_test.pdf.metadata.json

processed_docs/structured:
multimodal_test.pdf.metadata.json

processed_docs/text:
multimodal_test.pdf.metadata.json</pre></div>
<p dir="auto">You can view the full JSON extracts and the metadata definitions <a href="https://github.com/NVIDIA/nv-ingest/blob/main/docs/content-metadata.md">here</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">We also provide a script for inspecting <a href="https://github.com/NVIDIA/nv-ingest/blob/main/src/util/image_viewer.py">extracted images</a></h4><a id="user-content-we-also-provide-a-script-for-inspecting-extracted-images" aria-label="Permalink: We also provide a script for inspecting extracted images" href="#we-also-provide-a-script-for-inspecting-extracted-images"></a></p>
<p dir="auto">First, install <code>tkinter</code> by running the following commands depending on your OS.</p>
<ul dir="auto">
<li>For Ubuntu/Debian Linux:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt-get update
sudo apt-get install python3-tk"><pre>sudo apt-get update
sudo apt-get install python3-tk</pre></div>
<ul dir="auto">
<li>For Fedora/RHEL Linux:</li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="sudo dnf install python3-tkinter"><pre>sudo dnf install python3-tkinter</pre></div>
<ul dir="auto">
<li>For macOS using Homebrew:</li>
</ul>

<p dir="auto">Then run the following command to execute the script for inspecting the extracted image:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python src/util/image_viewer.py --file_path ./processed_docs/image/multimodal_test.pdf.metadata.json"><pre>python src/util/image_viewer.py --file_path ./processed_docs/image/multimodal_test.pdf.metadata.json</pre></div>

<p dir="auto"><h2 tabindex="-1" dir="auto">Repo Structure</h2><a id="user-content-repo-structure" aria-label="Permalink: Repo Structure" href="#repo-structure"></a></p>
<p dir="auto">Beyond the relevant documentation, examples, and other links above, below is a description of contents in this repo's folders:</p>
<ol dir="auto">
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/.github">.github</a>: GitHub repo configuration files</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/ci">ci</a>: scripts used to build the nv-ingest container and other packages</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/client">client</a>: docs and source code for the nv-ingest-cli utility</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/config">config</a>: various yaml files defining configuration for OTEL, Prometheus</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/data">data</a>: Sample PDFs provided for testing convenience</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/docker">docker</a>: houses scripts used by the nv-ingest docker container</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/docs">docs</a>: Various READMEs describing deployment, metadata schemas, auth and telemetry setup</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/examples">examples</a>: Example notebooks, scripts, and longer form tutorial content</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/helm">helm</a>: Documentation for deploying nv-ingest to a Kubernetes cluster via Helm chart</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/skaffold">skaffold</a>: Skaffold configuration</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/src">src</a>: source code for the nv-ingest pipelines and service</li>
<li><a href="https://github.com/NVIDIA/nv-ingest/blob/main/tests">tests</a>: unit tests for nv-ingest</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notices</h2><a id="user-content-notices" aria-label="Permalink: Notices" href="#notices"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Third Party License Notice:</h3><a id="user-content-third-party-license-notice" aria-label="Permalink: Third Party License Notice:" href="#third-party-license-notice"></a></p>
<p dir="auto">If configured to do so, this project will download and install additional third-party open source software projects.
Review the license terms of these open source projects before use:</p>
<p dir="auto"><a href="https://pypi.org/project/pdfservices-sdk/" rel="nofollow">https://pypi.org/project/pdfservices-sdk/</a></p>
<ul dir="auto">
<li><strong><code>INSTALL_ADOBE_SDK</code></strong>:
<ul dir="auto">
<li><strong>Description</strong>: If set to <code>true</code>, the Adobe SDK will be installed in the container at launch time. This is
required if you want to use the Adobe extraction service for PDF decomposition. Please review the
<a href="https://github.com/adobe/pdfservices-python-sdk?tab=License-1-ov-file">license agreement</a> for the
pdfservices-sdk before enabling this option.</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We require that all contributors "sign-off" on their commits. This certifies that the contribution is your original
work, or you have rights to submit it under the same license, or a compatible license.</p>
<p dir="auto">Any contribution which contains commits that are not Signed-Off will not be accepted.</p>
<p dir="auto">To sign off on a commit you simply use the --signoff (or -s) option when committing your changes:</p>
<div data-snippet-clipboard-copy-content="$ git commit -s -m &quot;Add cool feature.&quot;"><pre><code>$ git commit -s -m "Add cool feature."
</code></pre></div>
<p dir="auto">This will append the following to your commit message:</p>
<div data-snippet-clipboard-copy-content="Signed-off-by: Your Name <your@email.com>"><pre><code>Signed-off-by: Your Name &lt;your@email.com&gt;
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Full text of the DCO:</h4><a id="user-content-full-text-of-the-dco" aria-label="Permalink: Full text of the DCO:" href="#full-text-of-the-dco"></a></p>
<div data-snippet-clipboard-copy-content="  Developer Certificate of Origin
  Version 1.1

  Copyright (C) 2004, 2006 The Linux Foundation and its contributors.
  1 Letterman Drive
  Suite D4700
  San Francisco, CA, 94129

  Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed."><pre><code>  Developer Certificate of Origin
  Version 1.1

  Copyright (C) 2004, 2006 The Linux Foundation and its contributors.
  1 Letterman Drive
  Suite D4700
  San Francisco, CA, 94129

  Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.
</code></pre></div>
<div data-snippet-clipboard-copy-content="  Developer's Certificate of Origin 1.1

  By making a contribution to this project, I certify that:

  (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or

  (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or

  (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.

  (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved."><pre><code>  Developer's Certificate of Origin 1.1

  By making a contribution to this project, I certify that:

  (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or

  (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or

  (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.

  (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Glimmer: DSL Framework for Ruby GUI and More (118 pts)]]></title>
            <link>https://github.com/AndyObtiva/glimmer</link>
            <guid>42653939</guid>
            <pubDate>Fri, 10 Jan 2025 09:02:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/AndyObtiva/glimmer">https://github.com/AndyObtiva/glimmer</a>, See on <a href="https://news.ycombinator.com/item?id=42653939">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://rubygems.org/gems/glimmer" rel="nofollow"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer/master/images/glimmer-logo-hi-res.png" height="85"></a> Glimmer 2.8.0</h2><a id="user-content--glimmer-280" aria-label="Permalink:  Glimmer 2.8.0" href="#-glimmer-280"></a></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">DSL Framework for Ruby GUI and More</h2><a id="user-content-dsl-framework-for-ruby-gui-and-more" aria-label="Permalink: DSL Framework for Ruby GUI and More" href="#dsl-framework-for-ruby-gui-and-more"></a></p>
<p dir="auto"><a href="http://badge.fury.io/rb/glimmer" rel="nofollow"><img src="https://camo.githubusercontent.com/8afaf98d23e6be9c065fccae134766e8bb174d3a0560e21005f5aa47a574461c/68747470733a2f2f62616467652e667572792e696f2f72622f676c696d6d65722e737667" alt="Gem Version" data-canonical-src="https://badge.fury.io/rb/glimmer.svg"></a>
<a href="https://github.com/AndyObtiva/glimmer/actions?query=workflow%3Arspec"><img src="https://github.com/AndyObtiva/glimmer/workflows/rspec/badge.svg" alt="rspec"></a>
<a href="https://coveralls.io/github/AndyObtiva/glimmer?branch=master" rel="nofollow"><img src="https://camo.githubusercontent.com/b1300722ed1e389291166bae0c1b6a0826727c2d3e22adccb487657cb984773e/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f416e64794f62746976612f676c696d6d65722f62616467652e7376673f6272616e63683d6d6173746572" alt="Coverage Status" data-canonical-src="https://coveralls.io/repos/github/AndyObtiva/glimmer/badge.svg?branch=master"></a>
<a href="https://codeclimate.com/github/AndyObtiva/glimmer/maintainability" rel="nofollow"><img src="https://camo.githubusercontent.com/d6c55f1ba658de80648edae952851f55b67e2b062d66b7beed5de34920de2f5a/68747470733a2f2f6170692e636f6465636c696d6174652e636f6d2f76312f6261646765732f33386662633237383032323836323739343431342f6d61696e7461696e6162696c697479" alt="Maintainability" data-canonical-src="https://api.codeclimate.com/v1/badges/38fbc278022862794414/maintainability"></a>
<a href="https://gitter.im/AndyObtiva/glimmer?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge" rel="nofollow"><img src="https://camo.githubusercontent.com/34da5e29c959d7500e7a9c7922cddb396fc2910236c11e6a6406bd5e6aaa779e/68747470733a2f2f6261646765732e6769747465722e696d2f416e64794f62746976612f676c696d6d65722e737667" alt="Join the chat at https://gitter.im/AndyObtiva/glimmer" data-canonical-src="https://badges.gitter.im/AndyObtiva/glimmer.svg"></a></p>
<p dir="auto"><strong><a href="#faq">If You Liked Shoes, You'll Love Glimmer!</a></strong></p>
<p dir="auto"><strong>(Original Glimmer Library Handling World’s Ruby GUI Needs Since 2007. Beware of Imitators!)</strong></p>
<p dir="auto">(<strong><a href="https://andymaleh.blogspot.com/2022/02/glimmer-dsl-for-libui-wins-fukuoka-ruby.html" rel="nofollow">Glimmer DSL for LibUI Won a Fukuoka Ruby 2022 Special Award</a></strong> <a href="http://www.digitalfukuoka.jp/topics/187?locale=ja" rel="nofollow">[Announcement]</a>)</p>
<p dir="auto">(<strong><a href="https://github.com/AndyObtiva/how-to-build-desktop-applications-in-ruby"><em><strong>RubyConf 2023 Workshop - How To Build Desktop Applications in Ruby</strong></em></a></strong>)</p>
<p dir="auto">(<strong><a href="https://andymaleh.blogspot.com/2023/02/rubyconf-2022-talk-video-for-building.html" rel="nofollow"><em><strong>RubyConf 2022 Talk - Building Native GUI Apps in Ruby</strong></em></a></strong>)</p>
<p dir="auto"><a href="https://andymaleh.blogspot.com/2022/05/ruby-rogues-podcast-interview-desktop.html" rel="nofollow"><strong>(Ruby Rogues Podcast Interview - Desktop Apps in Ruby ft. Andy)</strong></a></p>
<p dir="auto"><a href="https://www.youtube.com/channel/UC5hzDE23HZXsZLAxYk2UJEw" rel="nofollow">GLIMMER VIDEO TUTORIAL CHANNEL</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/AndyObtiva/glimmer-dsl-swt/raw/master/images/glimmer-dsl-swt-demo-hello-world.gif"><img src="https://github.com/AndyObtiva/glimmer-dsl-swt/raw/master/images/glimmer-dsl-swt-demo-hello-world.gif" alt="glimmer demo" data-animated-image=""></a></p>
<p dir="auto"><a href="https://rubygems.org/gems/glimmer" rel="nofollow"><strong>Glimmer</strong></a> started out as a <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">GUI Library</a> and grew into a full-fledged <a href="#dsl-engine">DSL Framework</a> with support for <a href="https://github.com/AndyObtiva/glimmer#glimmer-dsl-comparison-table">multiple GUI DSLs</a>. Glimmer's namesake is referring to the Glimmer of Ruby in Graphical User Interfaces (contrary to <a href="http://blog.headius.com/2007/11/tab-sweep.html" rel="nofollow">popular myth</a> perpetrated by <a href="http://blog.headius.com/2007/11/tab-sweep.html" rel="nofollow">Charles Nutter</a>, Glimmer has nothing to do with the ill-fated Whitney Houston movie, which does not in fact share the same name)</p>
<p dir="auto"><a href="http://shop.oreilly.com/product/9780596519650.do" rel="nofollow"><img src="https://camo.githubusercontent.com/f330481965cbdaceca79d6acdf87263298bf5ac1377a4eb0a1b6e34ed7bafa18/68747470733a2f2f636f766572732e6f7265696c6c797374617469632e636f6d2f696d616765732f393738303539363531393635302f6c72672e6a7067" width="105" data-canonical-src="https://covers.oreillystatic.com/images/9780596519650/lrg.jpg"><br>
Featured in JRuby Cookbook</a> and <a href="http://www.cse.chalmers.se/~bergert/slides/guest_lecture_DSLs.pdf" rel="nofollow">Chalmers/Gothenburg University Software Engineering Master's Lecture Material</a></p>
<p dir="auto"><a href="https://rubygems.org/gems/glimmer" rel="nofollow"><strong>Glimmer</strong></a> is a DSL (Domain-Specific Language) Framework that consists of two things:</p>
<ul dir="auto">
<li><a href="#dsl-engine">DSL Engine</a>: enables building internal DSLs embedded in Ruby (e.g. for GUI, XML, or CSS).</li>
<li><a href="#data-binding-library">Data-Binding Library</a>: enables synchronizing GUI with Model Attributes bidirectionally <strong>(now with <a href="#shine-data-binding-syntax">Shine</a> syntax support in v2, which was <a href="https://andymaleh.blogspot.com/2007/12/data-shining-in-glimmer.html" rel="nofollow">originally conceived back in 2007</a>)</strong>.</li>
</ul>
<p dir="auto"><a href="https://rubygems.org/gems/glimmer" rel="nofollow"><strong>Glimmer</strong></a> is <em><strong>the cream of the crop</strong></em> when it comes to building DSLs in Ruby:</p>
<ul dir="auto">
<li>Supports building the tersest most concise domain specific language syntax in Ruby.</li>
<li>Maximum readability and maintainability.</li>
<li>No extra unnecessary block variables when not needed.</li>
<li>DSL Blocks are true Ruby closures that can conveniently leverage variables from the outside and utilize standard Ruby code in and around. Just code in Ruby as usual without any hinderances! No surprising restrictions or strange uses of <code>instance_exec</code>/<code>eval</code>.</li>
<li>DSL syntax is limited to classes that mixin the <code>Glimmer</code> module, so the rest of the code is fully safe from namespace pollution.</li>
<li>Multiple DSLs may be <a href="#multi-dsl-support">mixed</a> together safely to achieve maximum expressability, composability, and productivity.</li>
<li>DSLs are fully configurable, so you may activate and deactivate DSLs as per your current needs only.</li>
</ul>
<p dir="auto">Start by checking out:</p>
<ul dir="auto">
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a>, Glimmer's original GUI DSL (for <a href="https://www.jruby.org/" rel="nofollow">JRuby</a>), which got extracted into its own <a href="https://rubygems.org/gems/glimmer-dsl-swt" rel="nofollow">Ruby gem</a>.</li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-libui">Glimmer DSL for LibUI</a>, Glimmer's GUI DSL for standard <a href="https://www.ruby-lang.org/" rel="nofollow">Ruby</a> (aka MRI Ruby or CRuby), which has no prerequisites beyond installing the <a href="https://rubygems.org/gems/glimmer-dsl-libui" rel="nofollow">Ruby gem</a>, and has won a <a href="https://andymaleh.blogspot.com/2022/02/glimmer-dsl-for-libui-wins-fukuoka-ruby.html" rel="nofollow">Fukuoka Ruby 2022 Special Award</a>.</li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-web">Glimmer DSL for Web</a> enables using all the advanced data-binding features of Glimmer in Web Frontends by providing a Ruby HTML DSL and a Ruby CSS DSL.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto"><a href="https://rubygems.org/gems/glimmer" rel="nofollow">Glimmer</a> DSL Comparison Table:</h2><a id="user-content-glimmer-dsl-comparison-table" aria-label="Permalink: Glimmer DSL Comparison Table:" href="#glimmer-dsl-comparison-table"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>DSL</th>
<th>Platforms</th>
<th>Native?</th>
<th>Vector Graphics?</th>
<th>Pros</th>
<th>Cons</th>
<th>Prereqs</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT (JRuby Desktop Development GUI Framework)</a></td>
<td>Mac / Windows / Linux</td>
<td>Yes</td>
<td>Yes (Canvas Shape DSL)</td>
<td>Very Mature / Scaffolding / Native Executable Packaging / Custom Widgets</td>
<td>Slow JRuby Startup Time / Heavy Memory Footprint</td>
<td>Java / JRuby</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-libui">Glimmer DSL for LibUI (Prerequisite-Free Ruby Desktop Development GUI Library)</a></td>
<td>Mac / Windows / Linux</td>
<td>Yes</td>
<td>Yes (Area API)</td>
<td>Very Simple Setup / Fast Startup Time / Light Memory Footprint</td>
<td>LibUI is an Incomplete Mid-Alpha Only</td>
<td>None Other Than MRI Ruby</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-tk">Glimmer DSL for Tk (Ruby Tk Desktop Development GUI Library)</a></td>
<td>Mac / Windows / Linux</td>
<td>Some Native-Themed Widgets (Not Truly Native)</td>
<td>Yes (Canvas)</td>
<td>Fast Startup Time / Light Memory Footprint</td>
<td>Complicated setup / Widgets Do Not Look Truly Native, Espcially on Linux</td>
<td>ActiveTcl / MRI Ruby</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-gtk">Glimmer DSL for GTK (Ruby-GNOME Desktop Development GUI Library)</a></td>
<td>Mac / Windows / Linux</td>
<td>Only on Linux</td>
<td>Yes (Cairo)</td>
<td>Complete Access to GNOME Features on Linux (Forte)</td>
<td>Not Native on Mac and Windows</td>
<td>None Other Than MRI Ruby on Linux / Brew Packages on Mac / MSYS &amp; MING Toolchains on Windows / MRI Ruby</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-fx">Glimmer DSL for FX (FOX Toolkit Ruby Desktop Development GUI Library)</a></td>
<td>Mac (requires XQuartz) / Windows / Linux</td>
<td>No</td>
<td>Yes (Canvas)</td>
<td>No Prerequisites on Windows (Forte Since Binaries Are Included Out of The Box)</td>
<td>Widgets Do Not Look Native / Mac Usage Obtrusively Starts XQuartz</td>
<td>None Other Than MRI Ruby on Windows / XQuarts on Mac / MRI Ruby</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-wx">Glimmer DSL for WX (wxWidgets Ruby Desktop Development GUI Library)</a></td>
<td>Mac / Windows / Linux</td>
<td>Yes</td>
<td>Yes</td>
<td>Fast Startup Time / Light Memory Footprint</td>
<td>wxruby3 is still beta and does not support Mac yet</td>
<td>wxWidgets, Doxygen, SWIG, GNU g++ 4.8 on Linux or RubyInstaller+DevKit on Windows</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-jfx">Glimmer DSL for JFX (JRuby JavaFX Desktop Development GUI Library)</a></td>
<td>Mac / Windows / Linux</td>
<td>No</td>
<td>Yes (javafx.scene.shape and javafx.scene.canvas)</td>
<td>Rich in Custom Widgets</td>
<td>Slow JRuby Startup Time / Heavy Memory Footprint / Widgets Do Not Look Native</td>
<td>Java / JRuby / JavaFX SDK</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-swing">Glimmer DSL for Swing (JRuby Swing Desktop Development GUI Library)</a></td>
<td>Mac / Windows / Linux</td>
<td>No</td>
<td>Yes (Java2D)</td>
<td>Very Mature</td>
<td>Slow JRuby Startup Time / Heavy Memory Footprint / Widgets Do Not Look Native</td>
<td>Java / JRuby</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-web">Glimmer DSL for Web (Ruby in the Browser Web GUI Frontend Library)</a></td>
<td>All Web Browsers</td>
<td>No</td>
<td>Yes (SVG)</td>
<td>Simpler than All JavaScript Technologies / Leverages Existing HTML/JS/CSS Skills</td>
<td>Setup Process / Early Alpha</td>
<td>Rails</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-opal">Glimmer DSL for Opal (Pure Ruby Web GUI and Auto-Webifier of Desktop Apps / Archived &amp; Superseded by Glimmer DSL for Web)</a></td>
<td>All Web Browsers</td>
<td>No</td>
<td>Yes (Canvas Shape DSL)</td>
<td>Simpler than All JavaScript Technologies / Auto-Webify Desktop Apps</td>
<td>Setup Process / Incomplete Alpha</td>
<td>Rails</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-xml">Glimmer DSL for XML (&amp; HTML)</a></td>
<td>All Web Browsers</td>
<td>No</td>
<td>Yes (SVG)</td>
<td>Programmable / Lighter-weight Than Actual XML</td>
<td>XML Elements Are Sometimes Not Well-Named (Many Types of Input)</td>
<td>None</td>
</tr>
<tr>
<td><a href="https://github.com/AndyObtiva/glimmer-dsl-css">Glimmer DSL for CSS</a></td>
<td>All Web Browsers</td>
<td>No</td>
<td>Yes</td>
<td>Programmable</td>
<td>CSS Is Over-Engineered / Too Many Features To Learn</td>
<td>None</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#-glimmer---dsl-framework-for-ruby-gui-and-more">Glimmer</a>
<ul dir="auto">
<li><a href="#dsl-engine">DSL Engine</a>
<ul dir="auto">
<li><a href="#setup">Setup</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#multi-dsl-support">Multi-DSL Support</a></li>
<li><a href="#official-dsls">Official DSLs</a>
<ul dir="auto">
<li><a href="#glimmer-dsl-for-swt-jruby-desktop-development-gui-framework">Glimmer DSL for SWT (JRuby Desktop Development GUI Framework)</a></li>
<li><a href="#glimmer-dsl-for-opal-pure-ruby-web-gui-and-auto-webifier-of-desktop-apps">Glimmer DSL for Opal (Pure Ruby Web GUI and Auto-Webifier of Desktop Apps)</a></li>
<li><a href="#glimmer-dsl-for-libui-prerequisite-free-ruby-desktop-development-gui-library">Glimmer DSL for LibUI (Prerequisite-Free Ruby Desktop Development GUI Library)</a></li>
<li><a href="#glimmer-dsl-for-tk-mri-ruby-desktop-development-gui-library">Glimmer DSL for Tk (MRI Ruby Desktop Development GUI Library)</a></li>
<li><a href="#glimmer-dsl-for-xml--html">Glimmer DSL for XML (&amp; HTML)</a></li>
<li><a href="#glimmer-dsl-for-css">Glimmer DSL for CSS</a></li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-gtk">Glimmer DSL for GTK (Ruby-GNOME Desktop Development GUI Library)</a></li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-fx">Glimmer DSL for FX (FOX Toolkit Ruby Desktop Development GUI Library)</a></li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-wx">Glimmer DSL for WX (wXwidgets Ruby Desktop Development GUI Library)</a></li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-jfx">Glimmer DSL for JFX (JRuby JavaFX Desktop Development GUI Library)</a></li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-swing">Glimmer DSL for Swing (JRuby Swing Desktop Development GUI Library)</a></li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-swing">Glimmer DSL for Web (Ruby in the Browser Web GUI Frontend Library)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#data-binding-library">Data-Binding Library</a></li>
<li><a href="#faq">FAQ</a></li>
<li><a href="#glimmer-process">Glimmer Process</a></li>
<li><a href="#resources">Resources</a></li>
<li><a href="#help">Help</a>
<ul dir="auto">
<li><a href="#issues">Issues</a></li>
<li><a href="#chat">Chat</a></li>
</ul>
</li>
<li><a href="#feature-suggestions">Feature Suggestions</a></li>
<li><a href="#change-log">Change Log</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#contributors">Contributors</a></li>
<li><a href="#hire-me">Hire Me</a></li>
<li><a href="#license">License</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">DSL Engine</h2><a id="user-content-dsl-engine" aria-label="Permalink: DSL Engine" href="#dsl-engine"></a></p>
<p dir="auto">Glimmer is fundamentally a DSL Engine that can support any number of DSLs like the official Glimmer DSLs (gems starting with the <code>glimmer-dsl-</code> prefix like <code>glimmer-dsl-swt</code>) or any DSLs for that matter.</p>
<p dir="auto">Glimmer DSL syntax consists mainly of:</p>
<ul dir="auto">
<li><strong>keywords</strong> (e.g. <code>table</code> for a table widget)</li>
<li><strong>style/args</strong> (e.g. :multi as in <code>table(:multi)</code> for a multi-line selection table widget)</li>
<li><strong>content (nested properties/keywords/listeners)</strong> (e.g. <code>{ table_column { text 'Name'} }</code> as in <code>table(:multi) { table_column { text 'Name'} }</code> for a multi-line selection table widget with a table column having header text property <code>'Name'</code> as content)</li>
<li><strong>methods</strong> (e.g. <code>shell.show</code> opens a window)</li>
</ul>
<p dir="auto">Here is a Hello, World! example from <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="include Glimmer

shell(:no_resize) { # keyword + style arg
  text &quot;Glimmer&quot; # attribute content
  
  label { # keyword content
    text &quot;Hello, World!&quot; # attribute content
  }
}.open"><pre><span>include</span> <span>Glimmer</span>

<span>shell</span><span>(</span><span>:no_resize</span><span>)</span> <span>{</span> <span># keyword + style arg</span>
  <span>text</span> <span>"Glimmer"</span> <span># attribute content</span>
  
  <span>label</span> <span>{</span> <span># keyword content</span>
    <span>text</span> <span>"Hello, World!"</span> <span># attribute content</span>
  <span>}</span>
<span>}</span><span>.</span><span>open</span></pre></div>
<p dir="auto">That code renders the following GUI (Graphical User Interface):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/AndyObtiva/glimmer/blob/master/images/glimmer-hello-world.png"><img src="https://github.com/AndyObtiva/glimmer/raw/master/images/glimmer-hello-world.png" alt="Hello World"></a></p>
<p dir="auto">The Glimmer DSL Engine's architecture is based on the following Design Patterns and Data Structures:</p>
<ul dir="auto">
<li><strong>Interpreter Design Pattern</strong>: to define interpretable expressions of DSL keywords</li>
<li><strong>Chain of Responsibility Design Pattern / Queue Data Structure</strong>: to chain expression handlers in order of importance for processing DSL keywords</li>
<li><strong>Adapter Design Pattern</strong>: to adapt expressions into handlers in a chain of responsibility</li>
<li><strong>Stack Data Structure</strong>: to handle processing parent/child nesting of DSL keyword expressions in the correct order</li>
<li><strong>Proxy Design Pattern</strong>: to shield consumers of GUI libraries built with Glimmer from low-level GUI widget details</li>
</ul>
<p dir="auto">Glimmer's use of the <strong>Interpreter Design Pattern</strong> in processing DSLs is also known as the <strong>Virtual Machine Architectural Style</strong>. After all, DSL expressions are virtual machine opcodes that process nested keywords stored in a stack. I built Glimmer's original DSL back in 2007 without knowing the <strong>Virtual Machine Architectural Style</strong> (except perhaps as an esoteric technology powering Java), but stumbled upon it anyways through following the Gang of Four Design Patterns mentioned above, chiefly the <strong>Interpreter Design Pattern</strong> and the <strong>Chain of Responsibility Design Pattern</strong>.</p>
<p dir="auto">Every keyword in a Glimmer DSL is represented by a DSL expression that is processed by an <code>Expression</code> subclass selected from a chain of expressions (interpreters) pre-configured in a DSL chain of responsibility via <code>Glimmer::DSL::Engine.add_dynamic_expressions(DSLNameModule, expression_names_array)</code>.</p>
<p dir="auto">Expressions are either:</p>
<ul dir="auto">
<li><strong>Static</strong> (subclass of <code>StaticExpression</code>, which is a subclass of <code>Expression</code>): if they represent a single pre-identified keyword (e.g. <code>color</code> or <code>display</code>)</li>
<li><strong>Dynamic</strong> (subclass of <code>Expression</code>): if they represent keywords calculated on the fly during processing (e.g. an SWT widget like <code>label</code> or a random XML element called <code>folder</code> representing <code>&lt;folder&gt;&lt;/folder&gt;</code>)</li>
</ul>
<p dir="auto">Optionally, expressions can be parent expressions that contain other expressions, and must include the <code>ParentExpression</code> mixin module as such.</p>
<p dir="auto">Additionally, every expression that serves as a top-level entry point into the DSL must mixin <code>TopLevelExpression</code></p>
<p dir="auto">Static expressions are optimized in performance since they pre-define methods on the <code>Glimmer</code> module matching the static keywords they represent (e.g. <code>color</code> causes creating a <code>Glimmer#color</code> method for processing <code>color</code> expressions) and completely bypass as a result the Glimmer DSL Engine Chain of Responsibility. That said, they must be avoided if the same keyword might occur multiple times, but with different requirements for arguments, block, and parenthood type.</p>
<p dir="auto">Every <code>Expression</code> sublcass must specify two methods at least:</p>
<ul dir="auto">
<li><code>can_interpret?(parent, keyword, *args, &amp;block)</code>: to quickly test if the keyword and arg/block/parent combination qualifies for interpretation by the current <code>Expression</code> or to otherwise delegate to the next expression in the chain of responsibility.</li>
<li><code>interpret(parent, keyword, *args, &amp;block)</code>: to go ahead and interpret a DSL expression that qualified for interpretation</li>
</ul>
<p dir="auto"><code>StaticExpression</code> sublcasses may skip the <code>can_interpret?</code> method since they include a default implementation for it that matches the name of the keyword from the class name by convention. For example, a <code>color</code> keyword would have a <code>ColorExpression</code> class, so <code>color</code> is inferred automatically from class name and used in deciding whether the class can handle a <code>color</code> keyword or not.
<code>StaticExpression</code> may declare the following class method options (if any other than <code>downcased</code> (default) is set, then <code>downcased</code> must be set explicitly if needed):</p>
<ul dir="auto">
<li><code>downcased true</code> (default): indicates that the StaticExpression expects downcased keywords (e.g. <code>COLOR {}</code>)</li>
<li><code>upcased true</code>: indicates that the StaticExpression expects upcased keywords (e.g. <code>COLOR {}</code>). Note that upcased static expressions always expect either argument parentheses or block curly braces to be invoked as a static expression method instead of a constant.</li>
<li><code>capitalized true</code>: indicates that the StaticExpression expects capitalized keywords (e.g. <code>Color {}</code>). Note that capitalized static expressions always expect either argument parentheses or block curly braces to be invoked as a static expression method instead of a constant.</li>
<li><code>case_insensitive true</code>: indicates that the StaticExpression supports downcased, upcased, and capitalized keywords (e.g. <code>color {}</code>, <code>COLOR {}</code>, and <code>Color {}</code>). Note that upcased/capitalized static expressions always expect either argument parentheses or block curly braces to be invoked as a static expression method instead of a constant.</li>
</ul>
<p dir="auto"><code>ParentExpression</code> subclasses can optionally override this extra method, which is included by default and simply invokes the parent's passed block to process its children:</p>
<ul dir="auto">
<li><code>add_content(parent, keyword, *args, &amp;block)</code></li>
</ul>
<p dir="auto">For example, some parent widgets use their block for other reasons or process their children at very specific times, so they may override that method and disable it, or otherwise call <code>super</code> and do additional work.</p>
<p dir="auto">Otherwise, all expressions support the <code>around</code> hook method:</p>
<ul dir="auto">
<li><code>around(parent, keyword, args, block, &amp;interpret_and_add_content)</code>: a hook for executing code around both <code>interpret</code> and <code>add_content</code>. Clients may invoke <code>interpret_and_add_content.call</code> or <code>yield</code> when ready for interpretation. <code>parent</code>, <code>keyword</code>, <code>args</code>, and <code>block</code> are supplied in case they are needed in the <code>around</code> logic.</li>
</ul>
<p dir="auto">Example of a dynamic expression:</p>
<div dir="auto" data-snippet-clipboard-copy-content="module Glimmer
  module DSL
    module SWT
      class WidgetExpression < Expression
        include ParentExpression

        EXCLUDED_KEYWORDS = %w[shell display tab_item]

        def can_interpret?(parent, keyword, *args, &amp;block)
          !EXCLUDED_KEYWORDS.include?(keyword) and
            parent.respond_to?(:swt_widget) and
            Glimmer::SWT::WidgetProxy.widget_exists?(keyword)
        end

        def interpret(parent, keyword, *args, &amp;block)
          Glimmer::SWT::WidgetProxy.create(keyword, parent, args)
        end

        def add_content(parent, keyword, *args, &amp;block)
          super
          parent.post_add_content
        end

      end
    end
  end
end"><pre><span>module</span> <span>Glimmer</span>
  <span>module</span> <span>DSL</span>
    <span>module</span> <span>SWT</span>
      <span>class</span> <span>WidgetExpression</span> &lt; <span>Expression</span>
        <span>include</span> <span>ParentExpression</span>

        <span>EXCLUDED_KEYWORDS</span> <span>=</span> <span>%w[</span><span>shell</span> <span>display</span> <span>tab_item</span><span>]</span>

        <span>def</span> <span>can_interpret?</span><span>(</span><span>parent</span><span>,</span> <span>keyword</span><span>,</span> *<span>args</span><span>,</span> &amp;<span>block</span><span>)</span>
          !<span>EXCLUDED_KEYWORDS</span><span>.</span><span>include?</span><span>(</span><span>keyword</span><span>)</span> <span>and</span>
            <span>parent</span><span>.</span><span>respond_to?</span><span>(</span><span>:swt_widget</span><span>)</span> <span>and</span>
            <span>Glimmer</span>::<span>SWT</span>::<span>WidgetProxy</span><span>.</span><span>widget_exists?</span><span>(</span><span>keyword</span><span>)</span>
        <span>end</span>

        <span>def</span> <span>interpret</span><span>(</span><span>parent</span><span>,</span> <span>keyword</span><span>,</span> *<span>args</span><span>,</span> &amp;<span>block</span><span>)</span>
          <span>Glimmer</span>::<span>SWT</span>::<span>WidgetProxy</span><span>.</span><span>create</span><span>(</span><span>keyword</span><span>,</span> <span>parent</span><span>,</span> <span>args</span><span>)</span>
        <span>end</span>

        <span>def</span> <span>add_content</span><span>(</span><span>parent</span><span>,</span> <span>keyword</span><span>,</span> *<span>args</span><span>,</span> &amp;<span>block</span><span>)</span>
          <span>super</span>
          <span>parent</span><span>.</span><span>post_add_content</span>
        <span>end</span>

      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span></pre></div>
<p dir="auto">Example of a static expression (does not need <code>can_interpret?</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="module Glimmer
  module DSL
    module Opal
      class ColorExpression < StaticExpression
        include TopLevelExpression
  
        def interpret(parent, keyword, *args, &amp;block)
          Glimmer::SWT::ColorProxy.new(*args)
        end
      end
    end
  end
end"><pre><span>module</span> <span>Glimmer</span>
  <span>module</span> <span>DSL</span>
    <span>module</span> <span>Opal</span>
      <span>class</span> <span>ColorExpression</span> &lt; <span>StaticExpression</span>
        <span>include</span> <span>TopLevelExpression</span>
  
        <span>def</span> <span>interpret</span><span>(</span><span>parent</span><span>,</span> <span>keyword</span><span>,</span> *<span>args</span><span>,</span> &amp;<span>block</span><span>)</span>
          <span>Glimmer</span>::<span>SWT</span>::<span>ColorProxy</span><span>.</span><span>new</span><span>(</span>*<span>args</span><span>)</span>
        <span>end</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span></pre></div>
<p dir="auto">Extra convenience expression mixins/superclasses for use via inclusion/subclassing in Glimmer GUI libraries:</p>
<ul dir="auto">
<li><code>Glimmer::DSL::BindExpression</code>: enables usage of <code>bind</code> data-binding keyword to build a <code>Glimmer::DataBinding::ModelBinding</code> object for <a href="#data-binding">data-binding</a> purposes.</li>
<li><code>Glimmer::DSL::ShineDataBindingExpression</code>: enables <a href="#shine-data-binding-syntax">Shine data-binding syntax</a> via <code>Glimmer::DataBinding::Shine</code>, a facade for the <code>bind</code> keyword, hiding it with the <code>&lt;=&gt;</code> operator for bidirectional (two-way) data-binding and the <code>&lt;=</code> operator for unidirectional (one-way) data-binding.</li>
<li><code>Glimmer::DSL::ObserveExpression</code>: enables a one-way <code>observe</code> operation.
You may learn more about them by looking at how <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> uses them.</li>
</ul>
<p dir="auto">DSL expressions go into the <code>glimmer/dsl/{dsl_name}</code> namespace directory.</p>
<p dir="auto">Also, every DSL requires a <code>glimmer/dsl/{dsl_name}/dsl.rb</code> file, which configures the DSL into Glimmer via a call to:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Glimmer::DSL::Engine.add_dynamic_expressions(DSLNameModule, expression_names_array)"><pre><span>Glimmer</span>::<span>DSL</span>::<span>Engine</span><span>.</span><span>add_dynamic_expressions</span><span>(</span><span>DSLNameModule</span><span>,</span> <span>expression_names_array</span><span>)</span></pre></div>
<p dir="auto">Expression names are underscored verions of <code>Expression</code> subclass names minus the <code>_expression</code> suffix.</p>
<p dir="auto">For example, here is an SWT DSL configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'glimmer/launcher'
require Glimmer::Launcher.swt_jar_file
require 'glimmer/dsl/engine'
Dir[File.expand_path('../*_expression.rb', __FILE__)].each {|f| require f}

module Glimmer
  module DSL
    module SWT
      Engine.add_dynamic_expressions(
        SWT,
        %w[
          layout
          widget_listener
          combo_selection_data_binding
          checkbox_group_selection_data_binding
          radio_group_selection_data_binding
          list_selection_data_binding
          tree_items_data_binding
          table_items_data_binding
          data_binding
          cursor
          font
          image
          property
          block_property
          widget
          custom_widget
        ]
      )
    end
  end
end"><pre><span>require</span> <span>'glimmer/launcher'</span>
<span>require</span> <span>Glimmer</span>::<span>Launcher</span><span>.</span><span>swt_jar_file</span>
<span>require</span> <span>'glimmer/dsl/engine'</span>
<span>Dir</span><span>[</span><span>File</span><span>.</span><span>expand_path</span><span>(</span><span>'../*_expression.rb'</span><span>,</span> <span>__FILE__</span><span>)</span><span>]</span><span>.</span><span>each</span> <span>{</span>|<span>f</span>| <span>require</span> <span>f</span><span>}</span>

<span>module</span> <span>Glimmer</span>
  <span>module</span> <span>DSL</span>
    <span>module</span> <span>SWT</span>
      <span>Engine</span><span>.</span><span>add_dynamic_expressions</span><span>(</span>
        <span>SWT</span><span>,</span>
        <span>%w[</span>
          <span>layout</span>
          <span>widget_listener</span>
          <span>combo_selection_data_binding</span>
          <span>checkbox_group_selection_data_binding</span>
          <span>radio_group_selection_data_binding</span>
          <span>list_selection_data_binding</span>
          <span>tree_items_data_binding</span>
          <span>table_items_data_binding</span>
          <span>data_binding</span>
          <span>cursor</span>
          <span>font</span>
          <span>image</span>
          <span>property</span>
          <span>block_property</span>
          <span>widget</span>
          <span>custom_widget</span>
        <span>]</span>
      <span>)</span>
    <span>end</span>
  <span>end</span>
<span>end</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setup</h3><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">Follow these steps to author a <a href="https://rubygems.org/gems/glimmer" rel="nofollow">Glimmer</a> DSL:</p>
<ul dir="auto">
<li>Add <code>gem 'glimmer', '~&gt; 2.8.0'</code> to <code>Gemfile</code> and run <code>bundle</code> or run <code>gem install glimmer -v2.8.0</code> and add <code>require 'glimmer'</code></li>
<li>Create <code>glimmer/dsl/[dsl_name]/dsl.rb</code>, which requires and adds all dynamic expressions for the [dsl_name] Glimmer DSL module as per the code shown in the previous section (or <a href="#official-dsls">Official DSLs</a> as examples)</li>
<li>Create <code>glimmer/dsl/[dsl_name]/[expresion_name]_expresion.rb</code> for every [expresion_name] expression needed, whether dynamic or static</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Glimmer configuration may be done via the <a href="https://github.com/AndyObtiva/glimmer/blob/master/lib/glimmer/config.rb"><code>Glimmer::Config</code></a> module.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">logger</h4><a id="user-content-logger" aria-label="Permalink: logger" href="#logger"></a></p>
<p dir="auto">The Glimmer DSL engine supports logging via a standard <code>STDOUT</code> Ruby <code>Logger</code> configured in the <code>Glimmer::Config.logger</code> config option.
It is set to level Logger::ERROR by default.
Log level may be adjusted via <code>Glimmer::Config.logger.level</code> just like any other Ruby Logger.</p>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Glimmer::Config.logger.level = :debug"><pre><span>Glimmer</span>::<span>Config</span><span>.</span><span>logger</span><span>.</span><span>level</span> <span>=</span> <span>:debug</span></pre></div>
<p dir="auto">This results in more verbose debug loggging to <code>STDOUT</code>, which is very helpful in troubleshooting Glimmer DSL syntax when needed.</p>
<p dir="auto">Example log:</p>
<div data-snippet-clipboard-copy-content="D, [2017-07-21T19:23:12.587870 #35707] DEBUG -- : method: shell and args: []
D, [2017-07-21T19:23:12.594405 #35707] DEBUG -- : ShellCommandHandler will handle command: shell with arguments []
D, [2017-07-21T19:23:12.844775 #35707] DEBUG -- : method: composite and args: []
D, [2017-07-21T19:23:12.845388 #35707] DEBUG -- : parent is a widget: true
D, [2017-07-21T19:23:12.845833 #35707] DEBUG -- : on listener?: false
D, [2017-07-21T19:23:12.864395 #35707] DEBUG -- : WidgetCommandHandler will handle command: composite with arguments []
D, [2017-07-21T19:23:12.864893 #35707] DEBUG -- : widget styles are: []
D, [2017-07-21T19:23:12.874296 #35707] DEBUG -- : method: list and args: [:multi]
D, [2017-07-21T19:23:12.874969 #35707] DEBUG -- : parent is a widget: true
D, [2017-07-21T19:23:12.875452 #35707] DEBUG -- : on listener?: false
D, [2017-07-21T19:23:12.878434 #35707] DEBUG -- : WidgetCommandHandler will handle command: list with arguments [:multi]
D, [2017-07-21T19:23:12.878798 #35707] DEBUG -- : widget styles are: [:multi]"><pre><code>D, [2017-07-21T19:23:12.587870 #35707] DEBUG -- : method: shell and args: []
D, [2017-07-21T19:23:12.594405 #35707] DEBUG -- : ShellCommandHandler will handle command: shell with arguments []
D, [2017-07-21T19:23:12.844775 #35707] DEBUG -- : method: composite and args: []
D, [2017-07-21T19:23:12.845388 #35707] DEBUG -- : parent is a widget: true
D, [2017-07-21T19:23:12.845833 #35707] DEBUG -- : on listener?: false
D, [2017-07-21T19:23:12.864395 #35707] DEBUG -- : WidgetCommandHandler will handle command: composite with arguments []
D, [2017-07-21T19:23:12.864893 #35707] DEBUG -- : widget styles are: []
D, [2017-07-21T19:23:12.874296 #35707] DEBUG -- : method: list and args: [:multi]
D, [2017-07-21T19:23:12.874969 #35707] DEBUG -- : parent is a widget: true
D, [2017-07-21T19:23:12.875452 #35707] DEBUG -- : on listener?: false
D, [2017-07-21T19:23:12.878434 #35707] DEBUG -- : WidgetCommandHandler will handle command: list with arguments [:multi]
D, [2017-07-21T19:23:12.878798 #35707] DEBUG -- : widget styles are: [:multi]
</code></pre></div>
<p dir="auto">The <code>logger</code> instance may be replaced with a custom logger via <code>Glimmer::Config.logger = custom_logger</code></p>
<p dir="auto">To reset <code>logger</code> to the default instance, you may call <code>Glimmer::Config.reset_logger!</code></p>
<p dir="auto">All logging is done lazily via blocks (e.g. <code>logger.debug {message}</code>) to avoid affecting app performance with logging when below the configured logging level threshold.</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> enhances Glimmer default logging support via the Ruby <a href="https://github.com/TwP/logging"><code>logging</code></a> gem, enabling buffered asynchronous logging in a separate thread, thus completely unhindering normal desktop app performance.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">loop_max_count</h4><a id="user-content-loop_max_count" aria-label="Permalink: loop_max_count" href="#loop_max_count"></a></p>
<p dir="auto">Glimmer has infinite loop detection support.
It can detect when an infinite loop is about to occur in method_missing and stops it.
It detects potential infinite loops when the same keyword and args repeat more than 100 times, which is unusual in a GUI app.</p>
<p dir="auto">The max limit can be changed via the <code>Glimmer::Config::loop_max_count=(count)</code> config option.</p>
<p dir="auto">Infinite loop detection may be disabled altogether if needed by setting <code>Glimmer::Config::loop_max_count</code> to <code>-1</code></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">excluded_keyword_checkers</h4><a id="user-content-excluded_keyword_checkers" aria-label="Permalink: excluded_keyword_checkers" href="#excluded_keyword_checkers"></a></p>
<p dir="auto">Glimmer permits consumers to exclude keywords from DSL processing by its engine via the <code>excluded_keyword_checkers</code> config option.</p>
<p dir="auto">To do so, add a proc to it that returns a boolean indicating if a keyword is excluded or not.</p>
<p dir="auto">Note that this proc runs within the context of the Glimmer object (as in the object mixing in the Glimmer module), so checker can can pretend to run there with its <code>self</code> object assumption.</p>
<p dir="auto">Example of keywords excluded by <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">glimmer-dsl-swt</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Glimmer::Config.excluded_keyword_checkers << lambda do |method_symbol, *args|
  method = method_symbol.to_s
  result = false
  result ||= method.start_with?('on_swt_') &amp;&amp; is_a?(Glimmer::UI::CustomWidget) &amp;&amp; respond_to?(method)
  result ||= method == 'dispose' &amp;&amp; is_a?(Glimmer::UI::CustomWidget) &amp;&amp; respond_to?(method)
  result ||= ['drag_source_proxy', 'drop_target_proxy'].include?(method) &amp;&amp; is_a?(Glimmer::UI::CustomWidget)
  result ||= method == 'post_initialize_child'
  result ||= method.end_with?('=')
  result ||= ['finish_edit!', 'search', 'all_tree_items', 'depth_first_search'].include?(method) &amp;&amp; is_a?(Glimmer::UI::CustomWidget) &amp;&amp; body_root.respond_to?(method)
end"><pre><span>Glimmer</span>::<span>Config</span><span>.</span><span>excluded_keyword_checkers</span> &lt;&lt; <span>lambda</span> <span>do</span> |<span>method_symbol</span><span>,</span> *<span>args</span>|
  <span>method</span> <span>=</span> <span>method_symbol</span><span>.</span><span>to_s</span>
  <span>result</span> <span>=</span> <span>false</span>
  <span>result</span> ||= <span>method</span><span>.</span><span>start_with?</span><span>(</span><span>'on_swt_'</span><span>)</span> &amp;&amp; <span>is_a?</span><span>(</span><span>Glimmer</span>::<span>UI</span>::<span>CustomWidget</span><span>)</span> &amp;&amp; <span>respond_to?</span><span>(</span><span>method</span><span>)</span>
  <span>result</span> ||= <span>method</span> == <span>'dispose'</span> &amp;&amp; <span>is_a?</span><span>(</span><span>Glimmer</span>::<span>UI</span>::<span>CustomWidget</span><span>)</span> &amp;&amp; <span>respond_to?</span><span>(</span><span>method</span><span>)</span>
  <span>result</span> ||= <span>[</span><span>'drag_source_proxy'</span><span>,</span> <span>'drop_target_proxy'</span><span>]</span><span>.</span><span>include?</span><span>(</span><span>method</span><span>)</span> &amp;&amp; <span>is_a?</span><span>(</span><span>Glimmer</span>::<span>UI</span>::<span>CustomWidget</span><span>)</span>
  <span>result</span> ||= <span>method</span> == <span>'post_initialize_child'</span>
  <span>result</span> ||= <span>method</span><span>.</span><span>end_with?</span><span>(</span><span>'='</span><span>)</span>
  <span>result</span> ||= <span>[</span><span>'finish_edit!'</span><span>,</span> <span>'search'</span><span>,</span> <span>'all_tree_items'</span><span>,</span> <span>'depth_first_search'</span><span>]</span><span>.</span><span>include?</span><span>(</span><span>method</span><span>)</span> &amp;&amp; <span>is_a?</span><span>(</span><span>Glimmer</span>::<span>UI</span>::<span>CustomWidget</span><span>)</span> &amp;&amp; <span>body_root</span><span>.</span><span>respond_to?</span><span>(</span><span>method</span><span>)</span>
<span>end</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">log_excluded_keywords</h4><a id="user-content-log_excluded_keywords" aria-label="Permalink: log_excluded_keywords" href="#log_excluded_keywords"></a></p>
<p dir="auto">(default = false)</p>
<p dir="auto">This just tells Glimmer whether to log excluded keywords or not (at the debug level). It is off by default.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Multi-DSL Support</h3><a id="user-content-multi-dsl-support" aria-label="Permalink: Multi-DSL Support" href="#multi-dsl-support"></a></p>
<p dir="auto">The Glimmer <a href="#dsl-engine">DSL Engine</a> allows mixing DSLs, which comes in handy when doing things like rendering a desktop GUI DSL <code>browser</code> widget additionally leveraging the HTML DSL and CSS DSL for its content.</p>
<p dir="auto">DSLs are activated by top-level keywords (expressions denoted as <code>TopLevelExpression</code>). For example, the <code>html</code> keyword activates the <a href="https://github.com/AndyObtiva/glimmer-dsl-xml">Glimmer DSL for XML</a> and the <code>css</code> keyword activates the <a href="https://github.com/AndyObtiva/glimmer-dsl-css">Glimmer DSL for CSS</a>. Glimmer automatically recognizes top-level keywords in each DSL and activates the DSL accordingly. Once done processing a nested DSL top-level keyword, Glimmer switches back to the prior DSL automatically.</p>
<p dir="auto">By default, all loaded DSLs (required glimmer DSL gems) are enabled.</p>
<p dir="auto">For example, this shows "Hello, World!" inside a <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> desktop app <code>browser</code> widget using <code>html</code> and <code>css</code> from <a href="https://github.com/AndyObtiva/glimmer-dsl-xml">Glimmer DSL for XML</a> and <a href="https://github.com/AndyObtiva/glimmer-dsl-css">Glimmer DSL for CSS</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'glimmer-dsl-swt'
require 'glimmer-dsl-xml'
require 'glimmer-dsl-css'

include Glimmer

shell {
  minimum_size 130, 130
  @browser = browser {
    text html {
      head {
        meta(name: &quot;viewport&quot;, content: &quot;width=device-width, initial-scale=2.0&quot;)
        style {
          css {
            h1 {
              background 'yellow'
            }
          }
        }
      }
      body {
        h1 { &quot;Hello, World!&quot; }
      }
    }
  }
}.open"><pre><span>require</span> <span>'glimmer-dsl-swt'</span>
<span>require</span> <span>'glimmer-dsl-xml'</span>
<span>require</span> <span>'glimmer-dsl-css'</span>

<span>include</span> <span>Glimmer</span>

<span>shell</span> <span>{</span>
  <span>minimum_size</span> <span>130</span><span>,</span> <span>130</span>
  <span>@browser</span> <span>=</span> <span>browser</span> <span>{</span>
    <span>text</span> <span>html</span> <span>{</span>
      <span>head</span> <span>{</span>
        <span>meta</span><span>(</span><span>name</span>: <span>"viewport"</span><span>,</span> <span>content</span>: <span>"width=device-width, initial-scale=2.0"</span><span>)</span>
        <span>style</span> <span>{</span>
          <span>css</span> <span>{</span>
            <span>h1</span> <span>{</span>
              <span>background</span> <span>'yellow'</span>
            <span>}</span>
          <span>}</span>
        <span>}</span>
      <span>}</span>
      <span>body</span> <span>{</span>
        <span>h1</span> <span>{</span> <span>"Hello, World!"</span> <span>}</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span><span>.</span><span>open</span></pre></div>
<p dir="auto"><strong>API methods to enable/disable DSLs:</strong></p>
<p dir="auto"><code>Glimmer::DSL::Engine.disable_dsl(dsl)</code>: disables a particular DSL</p>
<p dir="auto">Example: <code>Glimmer::DSL::Engine.disable_dsl(:swt)</code></p>
<p dir="auto"><code>Glimmer::DSL::Engine.enable_dsl(dsl)</code>: enables a particular DSL</p>
<p dir="auto">Example: <code>Glimmer::DSL::Engine.disable_dsl(:swt)</code></p>
<p dir="auto"><code>Glimmer::DSL::Engine.enabled_dsls=(dsls)</code>: enables only the specified DSLs, disabling all other loaded DSLs</p>
<p dir="auto">Example: <code>Glimmer::DSL::Engine.enabled_dsls = [:xml, :css]</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Official DSLs</h3><a id="user-content-official-dsls" aria-label="Permalink: Official DSLs" href="#official-dsls"></a></p>
<p dir="auto">Here, we showcase official Glimmer DSLs; that is <a href="https://rubygems.org/search?query=glimmer-dsl-" rel="nofollow">gems starting with the <code>glimmer-dsl-</code> prefix</a>.</p>
<p dir="auto">(you can skip ahead if you prefer to learn more about the Glimmer <a href="#dsl-engine">DSL Engine</a> or <a href="#data-binding-library">Data-Binding Library</a> first)</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Glimmer DSL for SWT (JRuby Desktop Development GUI Framework)</h4><a id="user-content-glimmer-dsl-for-swt-jruby-desktop-development-gui-framework" aria-label="Permalink: Glimmer DSL for SWT (JRuby Desktop Development GUI Framework)" href="#glimmer-dsl-for-swt-jruby-desktop-development-gui-framework"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> is a native-GUI cross-platform desktop development library written in <a href="https://www.jruby.org/" rel="nofollow">JRuby</a>, an OS-threaded faster version of <a href="https://www.ruby-lang.org/en/" rel="nofollow">Ruby</a>. <a href="https://rubygems.org/gems/glimmer" rel="nofollow">Glimmer</a>'s main innovation is a declarative <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#glimmer-dsl-syntax">Ruby DSL</a> that enables productive and efficient authoring of desktop application user-interfaces while relying on the robust <a href="https://www.eclipse.org/swt/" rel="nofollow">Eclipse SWT library</a>. <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> additionally innovates by having built-in <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#data-binding">data-binding</a> support, which greatly facilitates synchronizing the GUI with domain models, thus achieving true decoupling of object oriented components and enabling developers to solve business problems (test-first) without worrying about GUI concerns, or alternatively drive development GUI-first, and then write clean business models (test-first) afterwards. To get started quickly, <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> offers <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#scaffolding">scaffolding</a> options for <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#in-production">Apps</a>, <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#custom-shell-gem">Gems</a>, and <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#custom-widgets">Custom Widgets</a>. <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> also includes native-executable <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#packaging--distribution">packaging</a> support, sorely lacking in other libraries, thus enabling the delivery of desktop apps written in <a href="https://www.ruby-lang.org/en/" rel="nofollow">Ruby</a> as truly native DMG/PKG/APP files on the <a href="https://www.apple.com/ca/macos" rel="nofollow">Mac</a> + <a href="https://developer.apple.com/macos/distribution/" rel="nofollow">App Store</a> and MSI/EXE files on <a href="https://www.microsoft.com/en-ca/windows" rel="nofollow">Windows</a>.</p>
<p dir="auto">To get started, visit the <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#pre-requisites">Glimmer DSL for SWT project page</a> for instructions on installing the <a href="https://rubygems.org/gems/glimmer-dsl-swt" rel="nofollow">glimmer-dsl-swt gem</a>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Glimmer DSL for SWT Samples</h5><a id="user-content-glimmer-dsl-for-swt-samples" aria-label="Permalink: Glimmer DSL for SWT Samples" href="#glimmer-dsl-for-swt-samples"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Hello, World!</h6><a id="user-content-hello-world" aria-label="Permalink: Hello, World!" href="#hello-world"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/AndyObtiva/glimmer/blob/master/images/glimmer-hello-world.png"><img src="https://github.com/AndyObtiva/glimmer/raw/master/images/glimmer-hello-world.png" alt="Hello World"></a></p>
<p dir="auto">Glimmer GUI code (from <a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/master/samples/hello/hello_world.rb">samples/hello/hello_world.rb</a>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="include Glimmer

shell {
  text &quot;Glimmer&quot;
  label {
    text &quot;Hello, World!&quot;
  }
}.open"><pre><span>include</span> <span>Glimmer</span>

<span>shell</span> <span>{</span>
  <span>text</span> <span>"Glimmer"</span>
  <span>label</span> <span>{</span>
    <span>text</span> <span>"Hello, World!"</span>
  <span>}</span>
<span>}</span><span>.</span><span>open</span></pre></div>
<p dir="auto"><h6 tabindex="-1" dir="auto">Glimmer Tetris</h6><a id="user-content-glimmer-tetris" aria-label="Permalink: Glimmer Tetris" href="#glimmer-tetris"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-swt/v4.18.3.1/images/glimmer-tetris.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-swt/v4.18.3.1/images/glimmer-tetris.png" alt="Tetris"></a></p>
<p dir="auto">Glimmer GUI code (from <a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/v4.18.3.1/samples/elaborate/tetris.rb">samples/elaborate/tetris.rb</a>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# ...
    shell(:no_resize) {
      grid_layout {
        num_columns 2
        make_columns_equal_width false
        margin_width 0
        margin_height 0
        horizontal_spacing 0
      }
      
      text 'Glimmer Tetris'
      minimum_size 475, 500
      background :gray
      
      tetris_menu_bar(game: game)
            
      playfield(game_playfield: game.playfield, playfield_width: playfield_width, playfield_height: playfield_height, block_size: BLOCK_SIZE)
      
      score_lane(game: game, block_size: BLOCK_SIZE) {
        layout_data(:fill, :fill, true, true)
      }
    }
# ..."><pre><span># ...</span>
    <span>shell</span><span>(</span><span>:no_resize</span><span>)</span> <span>{</span>
      <span>grid_layout</span> <span>{</span>
        <span>num_columns</span> <span>2</span>
        <span>make_columns_equal_width</span> <span>false</span>
        <span>margin_width</span> <span>0</span>
        <span>margin_height</span> <span>0</span>
        <span>horizontal_spacing</span> <span>0</span>
      <span>}</span>
      
      <span>text</span> <span>'Glimmer Tetris'</span>
      <span>minimum_size</span> <span>475</span><span>,</span> <span>500</span>
      <span>background</span> <span>:gray</span>
      
      <span>tetris_menu_bar</span><span>(</span><span>game</span>: <span>game</span><span>)</span>
            
      <span>playfield</span><span>(</span><span>game_playfield</span>: <span>game</span><span>.</span><span>playfield</span><span>,</span> <span>playfield_width</span>: <span>playfield_width</span><span>,</span> <span>playfield_height</span>: <span>playfield_height</span><span>,</span> <span>block_size</span>: <span>BLOCK_SIZE</span><span>)</span>
      
      <span>score_lane</span><span>(</span><span>game</span>: <span>game</span><span>,</span> <span>block_size</span>: <span>BLOCK_SIZE</span><span>)</span> <span>{</span>
        <span>layout_data</span><span>(</span><span>:fill</span><span>,</span> <span>:fill</span><span>,</span> <span>true</span><span>,</span> <span>true</span><span>)</span>
      <span>}</span>
    <span>}</span>
<span># ...</span></pre></div>
<p dir="auto"><h6 tabindex="-1" dir="auto">Hello, Table!</h6><a id="user-content-hello-table" aria-label="Permalink: Hello, Table!" href="#hello-table"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-swt/master/images/glimmer-hello-table.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-swt/master/images/glimmer-hello-table.png" alt="Hello Table"></a></p>
<p dir="auto">Glimmer GUI code (from <a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/master/samples/hello/hello_table.rb">samples/hello/hello_table.rb</a>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# ...
    shell {
      grid_layout
      
      text 'Hello, Table!'
      
      label {
        layout_data :center, :center, true, false
        
        text 'Baseball Playoff Schedule'
        font height: 30, style: :bold
      }
      
      combo(:read_only) {
        layout_data :center, :center, true, false
        selection bind(BaseballGame, :playoff_type)
        font height: 16
      }
      
      table(:editable) { |table_proxy|
        layout_data :fill, :fill, true, true
      
        table_column {
          text 'Game Date'
          width 150
          sort_property :date # ensure sorting by real date value (not `game_date` string specified in items below)
          editor :date_drop_down, property: :date_time
        }
        table_column {
          text 'Game Time'
          width 150
          sort_property :time # ensure sorting by real time value (not `game_time` string specified in items below)
          editor :time, property: :date_time
        }
        table_column {
          text 'Ballpark'
          width 180
          editor :none
        }
        table_column {
          text 'Home Team'
          width 150
          editor :combo, :read_only # read_only is simply an SWT style passed to combo widget
        }
        table_column {
          text 'Away Team'
          width 150
          editor :combo, :read_only # read_only is simply an SWT style passed to combo widget
        }
        table_column {
          text 'Promotion'
          width 150
          # default text editor is used here
        }
        
        # Data-bind table items (rows) to a model collection property, specifying column properties ordering per nested model
        items bind(BaseballGame, :schedule), column_properties(:game_date, :game_time, :ballpark, :home_team, :away_team, :promotion)
        
        # Data-bind table selection
        selection bind(BaseballGame, :selected_game)
        
        # Default initial sort property
        sort_property :date
        
        # Sort by these additional properties after handling sort by the column the user clicked
        additional_sort_properties :date, :time, :home_team, :away_team, :ballpark, :promotion
        
        menu {
          menu_item {
            text 'Book'
            
            on_widget_selected {
              book_selected_game
            }
          }
        }
      }
      
      button {
        text 'Book Selected Game'
        layout_data :center, :center, true, false
        font height: 16
        enabled bind(BaseballGame, :selected_game)
        
        on_widget_selected {
          book_selected_game
        }
      }
    }.open
# ..."><pre><span># ...</span>
    <span>shell</span> <span>{</span>
      <span>grid_layout</span>
      
      <span>text</span> <span>'Hello, Table!'</span>
      
      <span>label</span> <span>{</span>
        <span>layout_data</span> <span>:center</span><span>,</span> <span>:center</span><span>,</span> <span>true</span><span>,</span> <span>false</span>
        
        <span>text</span> <span>'Baseball Playoff Schedule'</span>
        <span>font</span> <span>height</span>: <span>30</span><span>,</span> <span>style</span>: <span>:bold</span>
      <span>}</span>
      
      <span>combo</span><span>(</span><span>:read_only</span><span>)</span> <span>{</span>
        <span>layout_data</span> <span>:center</span><span>,</span> <span>:center</span><span>,</span> <span>true</span><span>,</span> <span>false</span>
        <span>selection</span> <span>bind</span><span>(</span><span>BaseballGame</span><span>,</span> <span>:playoff_type</span><span>)</span>
        <span>font</span> <span>height</span>: <span>16</span>
      <span>}</span>
      
      <span>table</span><span>(</span><span>:editable</span><span>)</span> <span>{</span> |<span>table_proxy</span>|
        <span>layout_data</span> <span>:fill</span><span>,</span> <span>:fill</span><span>,</span> <span>true</span><span>,</span> <span>true</span>
      
        <span>table_column</span> <span>{</span>
          <span>text</span> <span>'Game Date'</span>
          <span>width</span> <span>150</span>
          <span>sort_property</span> <span>:date</span> <span># ensure sorting by real date value (not `game_date` string specified in items below)</span>
          <span>editor</span> <span>:date_drop_down</span><span>,</span> <span>property</span>: <span>:date_time</span>
        <span>}</span>
        <span>table_column</span> <span>{</span>
          <span>text</span> <span>'Game Time'</span>
          <span>width</span> <span>150</span>
          <span>sort_property</span> <span>:time</span> <span># ensure sorting by real time value (not `game_time` string specified in items below)</span>
          <span>editor</span> <span>:time</span><span>,</span> <span>property</span>: <span>:date_time</span>
        <span>}</span>
        <span>table_column</span> <span>{</span>
          <span>text</span> <span>'Ballpark'</span>
          <span>width</span> <span>180</span>
          <span>editor</span> <span>:none</span>
        <span>}</span>
        <span>table_column</span> <span>{</span>
          <span>text</span> <span>'Home Team'</span>
          <span>width</span> <span>150</span>
          <span>editor</span> <span>:combo</span><span>,</span> <span>:read_only</span> <span># read_only is simply an SWT style passed to combo widget</span>
        <span>}</span>
        <span>table_column</span> <span>{</span>
          <span>text</span> <span>'Away Team'</span>
          <span>width</span> <span>150</span>
          <span>editor</span> <span>:combo</span><span>,</span> <span>:read_only</span> <span># read_only is simply an SWT style passed to combo widget</span>
        <span>}</span>
        <span>table_column</span> <span>{</span>
          <span>text</span> <span>'Promotion'</span>
          <span>width</span> <span>150</span>
          <span># default text editor is used here</span>
        <span>}</span>
        
        <span># Data-bind table items (rows) to a model collection property, specifying column properties ordering per nested model</span>
        <span>items</span> <span>bind</span><span>(</span><span>BaseballGame</span><span>,</span> <span>:schedule</span><span>)</span><span>,</span> <span>column_properties</span><span>(</span><span>:game_date</span><span>,</span> <span>:game_time</span><span>,</span> <span>:ballpark</span><span>,</span> <span>:home_team</span><span>,</span> <span>:away_team</span><span>,</span> <span>:promotion</span><span>)</span>
        
        <span># Data-bind table selection</span>
        <span>selection</span> <span>bind</span><span>(</span><span>BaseballGame</span><span>,</span> <span>:selected_game</span><span>)</span>
        
        <span># Default initial sort property</span>
        <span>sort_property</span> <span>:date</span>
        
        <span># Sort by these additional properties after handling sort by the column the user clicked</span>
        <span>additional_sort_properties</span> <span>:date</span><span>,</span> <span>:time</span><span>,</span> <span>:home_team</span><span>,</span> <span>:away_team</span><span>,</span> <span>:ballpark</span><span>,</span> <span>:promotion</span>
        
        <span>menu</span> <span>{</span>
          <span>menu_item</span> <span>{</span>
            <span>text</span> <span>'Book'</span>
            
            <span>on_widget_selected</span> <span>{</span>
              <span>book_selected_game</span>
            <span>}</span>
          <span>}</span>
        <span>}</span>
      <span>}</span>
      
      <span>button</span> <span>{</span>
        <span>text</span> <span>'Book Selected Game'</span>
        <span>layout_data</span> <span>:center</span><span>,</span> <span>:center</span><span>,</span> <span>true</span><span>,</span> <span>false</span>
        <span>font</span> <span>height</span>: <span>16</span>
        <span>enabled</span> <span>bind</span><span>(</span><span>BaseballGame</span><span>,</span> <span>:selected_game</span><span>)</span>
        
        <span>on_widget_selected</span> <span>{</span>
          <span>book_selected_game</span>
        <span>}</span>
      <span>}</span>
    <span>}</span><span>.</span><span>open</span>
<span># ...</span></pre></div>
<p dir="auto"><h5 tabindex="-1" dir="auto">Production Desktop Apps Built with Glimmer DSL for SWT</h5><a id="user-content-production-desktop-apps-built-with-glimmer-dsl-for-swt" aria-label="Permalink: Production Desktop Apps Built with Glimmer DSL for SWT" href="#production-desktop-apps-built-with-glimmer-dsl-for-swt"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/are-we-there-yet"><img alt="Are We There Yet Logo" src="https://raw.githubusercontent.com/AndyObtiva/are-we-there-yet/master/are-we-there-yet-logo.svg" width="40">Are We There Yet?</a> - Small Project Tracking App (leveraging ActiveRecord and SQLite)</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/are-we-there-yet"><img src="https://raw.githubusercontent.com/AndyObtiva/are-we-there-yet/master/are-we-there-yet-screenshot-windows.png" alt="Are We There Yet? App Screenshot"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/MathBowling"><img alt="Math Bowling Logo" src="https://raw.githubusercontent.com/AndyObtiva/MathBowling/master/images/math-bowling-logo.png" width="40">Math Bowling</a> - Elementary Level Math Game Featuring Bowling Rules</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/MathBowling"><img src="https://raw.githubusercontent.com/AndyObtiva/MathBowling/master/Math-Bowling-Screenshot.png" alt="Math Bowling App Screenshot"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/garderie_rainbow_daily_agenda"><img alt="Garderie Rainbow Daily Agenda Logo" src="https://raw.githubusercontent.com/AndyObtiva/garderie_rainbow_daily_agenda/master/images/garderie_rainbow_daily_agenda_logo.png" width="40">Garderie Rainbow Daily Agenda</a> -  A child nursery daily agenda reporting desktop app (communicates to a Rails Server and stores data using ActiveRecord/PostgreSQL [in its rails_server branch])</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/garderie_rainbow_daily_agenda"><img src="https://raw.githubusercontent.com/AndyObtiva/garderie_rainbow_daily_agenda/master/images/garderie_rainbow_daily_agenda_screenshot.png" alt="Garderie Rainbow Daily Agenda App Screenshot"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Glimmer DSL for Opal (Pure Ruby Web GUI and Auto-Webifier of Desktop Apps)</h4><a id="user-content-glimmer-dsl-for-opal-pure-ruby-web-gui-and-auto-webifier-of-desktop-apps" aria-label="Permalink: Glimmer DSL for Opal (Pure Ruby Web GUI and Auto-Webifier of Desktop Apps)" href="#glimmer-dsl-for-opal-pure-ruby-web-gui-and-auto-webifier-of-desktop-apps"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-opal">Glimmer DSL for Opal</a> is an experimental proof-of-concept web GUI adapter for <a href="https://github.com/AndyObtiva/glimmer">Glimmer</a> desktop apps (i.e. apps built with <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a>). It webifies them via <a href="https://rubyonrails.org/" rel="nofollow">Rails</a>, allowing Ruby desktop apps to run on the web via <a href="https://opalrb.com/" rel="nofollow">Opal Ruby</a> without changing a line of code. Apps may then be custom-styled for the web with standard CSS.</p>
<p dir="auto">Glimmer DSL for Opal webifier successfully reuses the entire <a href="https://github.com/AndyObtiva/glimmer">Glimmer</a> core DSL engine in <a href="https://opalrb.com/" rel="nofollow">Opal Ruby</a> inside a web browser, and as such inherits the full range of Glimmer desktop <a href="https://github.com/AndyObtiva/glimmer#data-binding">data-binding</a> capabilities for the web.</p>
<p dir="auto">To get started, visit the <a href="https://github.com/AndyObtiva/glimmer-dsl-opal">Glimmer DSL for Opal project page</a> for instructions on installing the <a href="https://rubygems.org/gems/glimmer-dsl-opal" rel="nofollow">glimmer-dsl-opal gem</a>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Glimmer DSL for Opal Samples</h5><a id="user-content-glimmer-dsl-for-opal-samples" aria-label="Permalink: Glimmer DSL for Opal Samples" href="#glimmer-dsl-for-opal-samples"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Hello, Computed!</h6><a id="user-content-hello-computed" aria-label="Permalink: Hello, Computed!" href="#hello-computed"></a></p>
<p dir="auto">Add the following require statement to <code>app/assets/javascripts/application.rb</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'samples/hello/hello_computed'"><pre><span>require</span> <span>'samples/hello/hello_computed'</span></pre></div>
<p dir="auto">Or add the Glimmer code directly if you prefer to play around with it:</p>
<div dir="auto" data-snippet-clipboard-copy-content="class HelloComputed
  class Contact
    attr_accessor :first_name, :last_name, :year_of_birth
  
    def initialize(attribute_map)
      @first_name = attribute_map[:first_name]
      @last_name = attribute_map[:last_name]
      @year_of_birth = attribute_map[:year_of_birth]
    end
  
    def name
      &quot;#{last_name}, #{first_name}&quot;
    end
  
    def age
      Time.now.year - year_of_birth.to_i
    rescue
      0
    end
  end
end

class HelloComputed
  include Glimmer

  def initialize
    @contact = Contact.new(
      first_name: 'Barry',
      last_name: 'McKibbin',
      year_of_birth: 1985
    )
  end

  def launch
    shell {
      text 'Hello, Computed!'
      composite {
        grid_layout {
          num_columns 2
          make_columns_equal_width true
          horizontal_spacing 20
          vertical_spacing 10
        }
        label {text 'First &amp;Name: '}
        text {
          text bind(@contact, :first_name)
          layout_data {
            horizontal_alignment :fill
            grab_excess_horizontal_space true
          }
        }
        label {text '&amp;Last Name: '}
        text {
          text bind(@contact, :last_name)
          layout_data {
            horizontal_alignment :fill
            grab_excess_horizontal_space true
          }
        }
        label {text '&amp;Year of Birth: '}
        text {
          text bind(@contact, :year_of_birth)
          layout_data {
            horizontal_alignment :fill
            grab_excess_horizontal_space true
          }
        }
        label {text 'Name: '}
        label {
          text bind(@contact, :name, computed_by: [:first_name, :last_name])
          layout_data {
            horizontal_alignment :fill
            grab_excess_horizontal_space true
          }
        }
        label {text 'Age: '}
        label {
          text bind(@contact, :age, on_write: :to_i, computed_by: [:year_of_birth])
          layout_data {
            horizontal_alignment :fill
            grab_excess_horizontal_space true
          }
        }
      }
    }.open
  end
end

HelloComputed.new.launch"><pre><span>class</span> <span>HelloComputed</span>
  <span>class</span> <span>Contact</span>
    <span>attr_accessor</span> <span>:first_name</span><span>,</span> <span>:last_name</span><span>,</span> <span>:year_of_birth</span>
  
    <span>def</span> <span>initialize</span><span>(</span><span>attribute_map</span><span>)</span>
      <span>@first_name</span> <span>=</span> <span>attribute_map</span><span>[</span><span>:first_name</span><span>]</span>
      <span>@last_name</span> <span>=</span> <span>attribute_map</span><span>[</span><span>:last_name</span><span>]</span>
      <span>@year_of_birth</span> <span>=</span> <span>attribute_map</span><span>[</span><span>:year_of_birth</span><span>]</span>
    <span>end</span>
  
    <span>def</span> <span>name</span>
      <span>"<span><span>#{</span><span>last_name</span><span>}</span></span>, <span><span>#{</span><span>first_name</span><span>}</span></span>"</span>
    <span>end</span>
  
    <span>def</span> <span>age</span>
      <span>Time</span><span>.</span><span>now</span><span>.</span><span>year</span> - <span>year_of_birth</span><span>.</span><span>to_i</span>
    <span>rescue</span>
      <span>0</span>
    <span>end</span>
  <span>end</span>
<span>end</span>

<span>class</span> <span>HelloComputed</span>
  <span>include</span> <span>Glimmer</span>

  <span>def</span> <span>initialize</span>
    <span>@contact</span> <span>=</span> <span>Contact</span><span>.</span><span>new</span><span>(</span>
      <span>first_name</span>: <span>'Barry'</span><span>,</span>
      <span>last_name</span>: <span>'McKibbin'</span><span>,</span>
      <span>year_of_birth</span>: <span>1985</span>
    <span>)</span>
  <span>end</span>

  <span>def</span> <span>launch</span>
    <span>shell</span> <span>{</span>
      <span>text</span> <span>'Hello, Computed!'</span>
      <span>composite</span> <span>{</span>
        <span>grid_layout</span> <span>{</span>
          <span>num_columns</span> <span>2</span>
          <span>make_columns_equal_width</span> <span>true</span>
          <span>horizontal_spacing</span> <span>20</span>
          <span>vertical_spacing</span> <span>10</span>
        <span>}</span>
        <span>label</span> <span>{</span><span>text</span> <span>'First &amp;Name: '</span><span>}</span>
        <span>text</span> <span>{</span>
          <span>text</span> <span>bind</span><span>(</span><span>@contact</span><span>,</span> <span>:first_name</span><span>)</span>
          <span>layout_data</span> <span>{</span>
            <span>horizontal_alignment</span> <span>:fill</span>
            <span>grab_excess_horizontal_space</span> <span>true</span>
          <span>}</span>
        <span>}</span>
        <span>label</span> <span>{</span><span>text</span> <span>'&amp;Last Name: '</span><span>}</span>
        <span>text</span> <span>{</span>
          <span>text</span> <span>bind</span><span>(</span><span>@contact</span><span>,</span> <span>:last_name</span><span>)</span>
          <span>layout_data</span> <span>{</span>
            <span>horizontal_alignment</span> <span>:fill</span>
            <span>grab_excess_horizontal_space</span> <span>true</span>
          <span>}</span>
        <span>}</span>
        <span>label</span> <span>{</span><span>text</span> <span>'&amp;Year of Birth: '</span><span>}</span>
        <span>text</span> <span>{</span>
          <span>text</span> <span>bind</span><span>(</span><span>@contact</span><span>,</span> <span>:year_of_birth</span><span>)</span>
          <span>layout_data</span> <span>{</span>
            <span>horizontal_alignment</span> <span>:fill</span>
            <span>grab_excess_horizontal_space</span> <span>true</span>
          <span>}</span>
        <span>}</span>
        <span>label</span> <span>{</span><span>text</span> <span>'Name: '</span><span>}</span>
        <span>label</span> <span>{</span>
          <span>text</span> <span>bind</span><span>(</span><span>@contact</span><span>,</span> <span>:name</span><span>,</span> <span>computed_by</span>: <span>[</span><span>:first_name</span><span>,</span> <span>:last_name</span><span>]</span><span>)</span>
          <span>layout_data</span> <span>{</span>
            <span>horizontal_alignment</span> <span>:fill</span>
            <span>grab_excess_horizontal_space</span> <span>true</span>
          <span>}</span>
        <span>}</span>
        <span>label</span> <span>{</span><span>text</span> <span>'Age: '</span><span>}</span>
        <span>label</span> <span>{</span>
          <span>text</span> <span>bind</span><span>(</span><span>@contact</span><span>,</span> <span>:age</span><span>,</span> <span>on_write</span>: <span>:to_i</span><span>,</span> <span>computed_by</span>: <span>[</span><span>:year_of_birth</span><span>]</span><span>)</span>
          <span>layout_data</span> <span>{</span>
            <span>horizontal_alignment</span> <span>:fill</span>
            <span>grab_excess_horizontal_space</span> <span>true</span>
          <span>}</span>
        <span>}</span>
      <span>}</span>
    <span>}</span><span>.</span><span>open</span>
  <span>end</span>
<span>end</span>

<span>HelloComputed</span><span>.</span><span>new</span><span>.</span><span>launch</span></pre></div>
<p dir="auto">Glimmer app on the desktop (using <a href="https://github.com/AndyObtiva/glimmer-dsl-swt"><code>glimmer-dsl-swt</code></a> gem):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/AndyObtiva/glimmer/blob/master/images/glimmer-hello-computed.png"><img src="https://github.com/AndyObtiva/glimmer/raw/master/images/glimmer-hello-computed.png" alt="Glimmer DSL for SWT Hello Computed"></a></p>
<p dir="auto">Glimmer app on the web (using <code>glimmer-dsl-opal</code> gem):</p>
<p dir="auto">Start the Rails server:</p>

<p dir="auto">Visit <code>http://localhost:3000</code></p>
<p dir="auto">You should see "Hello, Computed!"</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-opal/master/images/glimmer-dsl-opal-hello-computed.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-opal/master/images/glimmer-dsl-opal-hello-computed.png" alt="Glimmer DSL for Opal Hello Computed"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Glimmer Calculator</h6><a id="user-content-glimmer-calculator" aria-label="Permalink: Glimmer Calculator" href="#glimmer-calculator"></a></p>
<p dir="auto">Add the <a href="https://github.com/AndyObtiva/glimmer-cs-calculator">glimmer-cs-calculator</a> gem to <code>Gemfile</code> (without requiring):</p>
<div data-snippet-clipboard-copy-content="gem 'glimmer-cs-calculator', require: false"><pre><code>gem 'glimmer-cs-calculator', require: false
</code></pre></div>
<p dir="auto">Add the following require statement to <code>app/assets/javascripts/application.rb</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'glimmer-cs-calculator/launch'"><pre><span>require</span> <span>'glimmer-cs-calculator/launch'</span></pre></div>
<p dir="auto">Sample GUI code (relies on custom widgets <code>command_button</code>, <code>operation_button</code>, and <code>number_button</code>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# ...
shell {
  minimum_size (OS.mac? ? 320 : (OS.windows? ? 390 : 520)), 240
  image File.join(APP_ROOT, 'package', 'windows', &quot;Glimmer Calculator.ico&quot;) if OS.windows?
  text &quot;Glimmer - Calculator&quot;
  grid_layout 4, true
  # Setting styled_text to multi in order for alignment options to activate
  styled_text(:multi, :wrap, :border) {
    text bind(@presenter, :result)
    alignment swt(:right)
    right_margin 5
    font height: 40
    layout_data(:fill, :fill, true, true) {
      horizontal_span 4
    }
    editable false
    caret nil
  }
  command_button('AC')
  operation_button('÷')
  operation_button('×')
  operation_button('−')
  (7..9).each { |number|
    number_button(number)
  }
  operation_button('+', font: @button_font_big, vertical_span: 2)
  (4..6).each { |number|
    number_button(number)
  }
  (1..3).each { |number|
    number_button(number)
  }
  command_button('=', font: @button_font_big, vertical_span: 2)
  number_button(0, horizontal_span: 2)
  operation_button('.')
}
# ..."><pre><span># ...</span>
<span>shell</span> <span>{</span>
  <span>minimum_size</span> <span>(</span><span>OS</span><span>.</span><span>mac?</span> ? <span>320</span> : <span>(</span><span>OS</span><span>.</span><span>windows?</span> ? <span>390</span> : <span>520</span><span>)</span><span>)</span><span>,</span> <span>240</span>
  <span>image</span> <span>File</span><span>.</span><span>join</span><span>(</span><span>APP_ROOT</span><span>,</span> <span>'package'</span><span>,</span> <span>'windows'</span><span>,</span> <span>"Glimmer Calculator.ico"</span><span>)</span> <span>if</span> <span>OS</span><span>.</span><span>windows?</span>
  <span>text</span> <span>"Glimmer - Calculator"</span>
  <span>grid_layout</span> <span>4</span><span>,</span> <span>true</span>
  <span># Setting styled_text to multi in order for alignment options to activate</span>
  <span>styled_text</span><span>(</span><span>:multi</span><span>,</span> <span>:wrap</span><span>,</span> <span>:border</span><span>)</span> <span>{</span>
    <span>text</span> <span>bind</span><span>(</span><span>@presenter</span><span>,</span> <span>:result</span><span>)</span>
    <span>alignment</span> <span>swt</span><span>(</span><span>:right</span><span>)</span>
    <span>right_margin</span> <span>5</span>
    <span>font</span> <span>height</span>: <span>40</span>
    <span>layout_data</span><span>(</span><span>:fill</span><span>,</span> <span>:fill</span><span>,</span> <span>true</span><span>,</span> <span>true</span><span>)</span> <span>{</span>
      <span>horizontal_span</span> <span>4</span>
    <span>}</span>
    <span>editable</span> <span>false</span>
    <span>caret</span> <span>nil</span>
  <span>}</span>
  <span>command_button</span><span>(</span><span>'AC'</span><span>)</span>
  <span>operation_button</span><span>(</span><span>'÷'</span><span>)</span>
  <span>operation_button</span><span>(</span><span>'×'</span><span>)</span>
  <span>operation_button</span><span>(</span><span>'−'</span><span>)</span>
  <span>(</span><span>7</span>..<span>9</span><span>)</span><span>.</span><span>each</span> <span>{</span> |<span>number</span>|
    <span>number_button</span><span>(</span><span>number</span><span>)</span>
  <span>}</span>
  <span>operation_button</span><span>(</span><span>'+'</span><span>,</span> <span>font</span>: <span>@button_font_big</span><span>,</span> <span>vertical_span</span>: <span>2</span><span>)</span>
  <span>(</span><span>4</span>..<span>6</span><span>)</span><span>.</span><span>each</span> <span>{</span> |<span>number</span>|
    <span>number_button</span><span>(</span><span>number</span><span>)</span>
  <span>}</span>
  <span>(</span><span>1</span>..<span>3</span><span>)</span><span>.</span><span>each</span> <span>{</span> |<span>number</span>|
    <span>number_button</span><span>(</span><span>number</span><span>)</span>
  <span>}</span>
  <span>command_button</span><span>(</span><span>'='</span><span>,</span> <span>font</span>: <span>@button_font_big</span><span>,</span> <span>vertical_span</span>: <span>2</span><span>)</span>
  <span>number_button</span><span>(</span><span>0</span><span>,</span> <span>horizontal_span</span>: <span>2</span><span>)</span>
  <span>operation_button</span><span>(</span><span>'.'</span><span>)</span>
<span>}</span>
<span># ...</span></pre></div>
<p dir="auto">Glimmer app on the desktop (using the <a href="https://github.com/AndyObtiva/glimmer-dsl-swt"><code>glimmer-dsl-swt</code></a> gem):</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-cs-calculator/master/glimmer-cs-calculator-screenshot-linux.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-cs-calculator/master/glimmer-cs-calculator-screenshot-linux.png" alt="Glimmer Calculator Linux"></a></p>
<p dir="auto">Glimmer app on the web (using <code>glimmer-dsl-opal</code> gem):</p>
<p dir="auto">Start the Rails server:</p>

<p dir="auto">Visit <code>http://localhost:3000</code>
(or visit: <a href="http://glimmer-cs-calculator-server.herokuapp.com/" rel="nofollow">http://glimmer-cs-calculator-server.herokuapp.com</a>)</p>
<p dir="auto">You should see "Glimmer Calculator"</p>
<p dir="auto"><a href="http://glimmer-cs-calculator-server.herokuapp.com/" rel="nofollow"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-cs-calculator/master/glimmer-cs-calculator-screenshot-opal.png" alt="Glimmer Calculator Opal"></a></p>
<p dir="auto">Here is an Apple Calculator CSS themed version (with <a href="https://github.com/AndyObtiva/glimmer-cs-calculator/blob/master/server/glimmer-cs-calculator-server/app/assets/stylesheets/welcomes_apple.scss">CSS only</a>, no app code changes):</p>
<p dir="auto">Visit <a href="http://glimmer-cs-calculator-server.herokuapp.com/welcomes/apple" rel="nofollow">http://glimmer-cs-calculator-server.herokuapp.com/welcomes/apple</a></p>
<p dir="auto">You should see "Apple Calculator Theme"</p>
<p dir="auto"><a href="http://glimmer-cs-calculator-server.herokuapp.com/welcomes/apple" rel="nofollow"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-cs-calculator/master/glimmer-cs-calculator-screenshot-opal-apple.png" alt="Glimmer Calculator Opal Apple Calculator Theme"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Glimmer DSL for LibUI (Prerequisite-Free Ruby Desktop Development GUI Library)</h4><a id="user-content-glimmer-dsl-for-libui-prerequisite-free-ruby-desktop-development-gui-library" aria-label="Permalink: Glimmer DSL for LibUI (Prerequisite-Free Ruby Desktop Development GUI Library)" href="#glimmer-dsl-for-libui-prerequisite-free-ruby-desktop-development-gui-library"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-libui">Glimmer DSL for LibUI</a> is a prerequisite-free Ruby desktop development GUI library. No need to pre-install any prerequisites. Just install the gem and have platform-independent native GUI that just works!</p>
<p dir="auto"><a href="https://github.com/kojix2/LibUI">LibUI</a> is a thin <a href="https://www.ruby-lang.org/en/" rel="nofollow">Ruby</a> wrapper around <a href="https://github.com/andlabs/libui">libui</a>, a relatively new C GUI library that renders native controls on every platform (similar to <a href="https://www.eclipse.org/swt/" rel="nofollow">SWT</a>, but without the heavy weight of the <a href="https://www.java.com/en/" rel="nofollow">Java Virtual Machine</a>).</p>
<p dir="auto">The main trade-off in using <a href="https://rubygems.org/gems/glimmer-dsl-libui" rel="nofollow">Glimmer DSL for LibUI</a> as opposed to <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> or <a href="https://github.com/AndyObtiva/glimmer-dsl-tk">Glimmer DSL for Tk</a> is the fact that <a href="https://www.eclipse.org/swt/" rel="nofollow">SWT</a> and <a href="https://www.tcl.tk/" rel="nofollow">Tk</a> are more mature than mid-alpha <a href="https://github.com/andlabs/libui">libui</a> as GUI toolkits. Still, if there is only a need to build a small simple application, <a href="https://rubygems.org/gems/glimmer-dsl-libui" rel="nofollow">Glimmer DSL for LibUI</a> could be a good convenient choice due to having zero prerequisites beyond the dependencies included in the <a href="https://rubygems.org/gems/glimmer-dsl-libui" rel="nofollow">Ruby gem</a>. Also, just like <a href="https://github.com/AndyObtiva/glimmer-dsl-tk">Glimmer DSL for Tk</a>, its apps start instantly and have a small memory footprint. <a href="https://github.com/kojix2/LibUI">LibUI</a> is a promising new GUI toolkit that might prove quite worthy in the future.</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-libui">Glimmer DSL for LibUI</a> aims to provide a DSL similar to the <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> to enable more productive desktop development in Ruby with:</p>
<ul dir="auto">
<li>Declarative DSL syntax that visually maps to the GUI widget hierarchy</li>
<li>Convention over configuration via smart defaults and automation of low-level details</li>
<li>Requiring the least amount of syntax possible to build GUI</li>
<li>Bidirectional Data-Binding to declaratively wire and automatically synchronize GUI with Business Models</li>
<li>Custom Widget support</li>
<li>Scaffolding for new custom widgets, apps, and gems</li>
<li>Native-Executable packaging on Mac, Windows, and Linux</li>
</ul>
<p dir="auto"><h5 tabindex="-1" dir="auto">Glimmer DSL for LibUI Samples</h5><a id="user-content-glimmer-dsl-for-libui-samples" aria-label="Permalink: Glimmer DSL for LibUI Samples" href="#glimmer-dsl-for-libui-samples"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Hello, World!</h6><a id="user-content-hello-world-1" aria-label="Permalink: Hello, World!" href="#hello-world-1"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'glimmer-dsl-libui'

include Glimmer

window('hello world').show"><pre><span>require</span> <span>'glimmer-dsl-libui'</span>

<span>include</span> <span>Glimmer</span>

<span>window</span><span>(</span><span>'hello world'</span><span>)</span><span>.</span><span>show</span></pre></div>
<p dir="auto">Mac</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-mac-basic-window.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-mac-basic-window.png" alt="glimmer-dsl-libui-mac-basic-window.png"></a></p>
<p dir="auto">Windows</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-windows-basic-window.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-windows-basic-window.png" alt="glimmer-dsl-libui-windows-basic-window.png"></a></p>
<p dir="auto">Linux</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-linux-basic-window.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-linux-basic-window.png" alt="glimmer-dsl-libui-linux-basic-window.png"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Basic Table Progress Bar</h6><a id="user-content-basic-table-progress-bar" aria-label="Permalink: Basic Table Progress Bar" href="#basic-table-progress-bar"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'glimmer-dsl-libui'

include Glimmer

data = [
  ['task 1', 0],
  ['task 2', 15],
  ['task 3', 100],
  ['task 4', 75],
  ['task 5', -1],
]

window('Task Progress', 300, 200) {
  vertical_box {
    table {
      text_column('Task')
      progress_bar_column('Progress')

      cell_rows data # implicit data-binding
    }
    
    button('Mark All As Done') {
      stretchy false
      
      on_clicked do
        data.each_with_index do |row_data, row|
          data[row] = [row_data[0], 100] # automatically updates table due to implicit data-binding
        end
      end
    }
  }
}.show"><pre><span>require</span> <span>'glimmer-dsl-libui'</span>

<span>include</span> <span>Glimmer</span>

<span>data</span> <span>=</span> <span>[</span>
  <span>[</span><span>'task 1'</span><span>,</span> <span>0</span><span>]</span><span>,</span>
  <span>[</span><span>'task 2'</span><span>,</span> <span>15</span><span>]</span><span>,</span>
  <span>[</span><span>'task 3'</span><span>,</span> <span>100</span><span>]</span><span>,</span>
  <span>[</span><span>'task 4'</span><span>,</span> <span>75</span><span>]</span><span>,</span>
  <span>[</span><span>'task 5'</span><span>,</span> -<span>1</span><span>]</span><span>,</span>
<span>]</span>

<span>window</span><span>(</span><span>'Task Progress'</span><span>,</span> <span>300</span><span>,</span> <span>200</span><span>)</span> <span>{</span>
  <span>vertical_box</span> <span>{</span>
    <span>table</span> <span>{</span>
      <span>text_column</span><span>(</span><span>'Task'</span><span>)</span>
      <span>progress_bar_column</span><span>(</span><span>'Progress'</span><span>)</span>

      <span>cell_rows</span> <span>data</span> <span># implicit data-binding</span>
    <span>}</span>
    
    <span>button</span><span>(</span><span>'Mark All As Done'</span><span>)</span> <span>{</span>
      <span>stretchy</span> <span>false</span>
      
      <span>on_clicked</span> <span>do</span>
        <span>data</span><span>.</span><span>each_with_index</span> <span>do</span> |<span>row_data</span><span>,</span> <span>row</span>|
          <span>data</span><span>[</span><span>row</span><span>]</span> <span>=</span> <span>[</span><span>row_data</span><span>[</span><span>0</span><span>]</span><span>,</span> <span>100</span><span>]</span> <span># automatically updates table due to implicit data-binding</span>
        <span>end</span>
      <span>end</span>
    <span>}</span>
  <span>}</span>
<span>}</span><span>.</span><span>show</span></pre></div>
<p dir="auto">Mac</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-mac-basic-table-progress-bar.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-mac-basic-table-progress-bar.png" alt="glimmer-dsl-libui-mac-basic-table-progress-bar.png"></a></p>
<p dir="auto">Windows</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-windows-basic-table-progress-bar.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-windows-basic-table-progress-bar.png" alt="glimmer-dsl-libui-windows-basic-table-progress-bar.png"></a></p>
<p dir="auto">Linux</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-linux-basic-table-progress-bar.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-linux-basic-table-progress-bar.png" alt="glimmer-dsl-libui-linux-basic-table-progress-bar.png"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Area Gallery</h6><a id="user-content-area-gallery" aria-label="Permalink: Area Gallery" href="#area-gallery"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'glimmer-dsl-libui'

include Glimmer

window('Area Gallery', 400, 400) {
  area {
    path { # declarative stable path
      square(0, 0, 100)
      square(100, 100, 400)
      
      fill r: 102, g: 102, b: 204
    }
    path { # declarative stable path
      rectangle(0, 100, 100, 400)
      rectangle(100, 0, 400, 100)
      
      fill r: 204, g: 102, b: 204
    }
    path { # declarative stable path
      figure(100, 100) {
        line(100, 400)
        line(400, 100)
        line(400, 400)

        closed true
      }

      fill r: 202, g: 102, b: 104, a: 0.5
      stroke r: 0, g: 0, b: 0
    }
    path { # declarative stable path
      figure(0, 0) {
        bezier(200, 100, 100, 200, 400, 100)
        bezier(300, 100, 100, 300, 100, 400)
        bezier(100, 300, 300, 100, 400, 400)

        closed true
      }

      fill r: 202, g: 102, b: 204, a: 0.5
      stroke r: 0, g: 0, b: 0, thickness: 2, dashes: [50, 10, 10, 10], dash_phase: -50.0
    }
    path { # declarative stable path
      arc(200, 200, 90, 0, 360, false)

      fill r: 202, g: 102, b: 204, a: 0.5
      stroke r: 0, g: 0, b: 0, thickness: 2
    }
    
    on_mouse_event do |area_mouse_event|
      p area_mouse_event
    end
    
    on_mouse_moved do |area_mouse_event|
      puts 'moved'
    end
    
    on_mouse_down do |area_mouse_event|
      puts 'mouse down'
    end
    
    on_mouse_up do |area_mouse_event|
      puts 'mouse up'
    end
    
    on_mouse_drag_started do |area_mouse_event|
      puts 'drag started'
    end
    
    on_mouse_dragged do |area_mouse_event|
      puts 'dragged'
    end
    
    on_mouse_dropped do |area_mouse_event|
      puts 'dropped'
    end
    
    on_mouse_entered do
      puts 'entered'
    end
    
    on_mouse_exited do
      puts 'exited'
    end
    
    on_key_event do |area_key_event|
      p area_key_event
    end
    
    on_key_up do |area_key_event|
      puts 'key up'
    end
    
    on_key_down do |area_key_event|
      puts 'key down'
    end
  }
}.show"><pre><span>require</span> <span>'glimmer-dsl-libui'</span>

<span>include</span> <span>Glimmer</span>

<span>window</span><span>(</span><span>'Area Gallery'</span><span>,</span> <span>400</span><span>,</span> <span>400</span><span>)</span> <span>{</span>
  <span>area</span> <span>{</span>
    <span>path</span> <span>{</span> <span># declarative stable path</span>
      <span>square</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>100</span><span>)</span>
      <span>square</span><span>(</span><span>100</span><span>,</span> <span>100</span><span>,</span> <span>400</span><span>)</span>
      
      <span>fill</span> <span>r</span>: <span>102</span><span>,</span> <span>g</span>: <span>102</span><span>,</span> <span>b</span>: <span>204</span>
    <span>}</span>
    <span>path</span> <span>{</span> <span># declarative stable path</span>
      <span>rectangle</span><span>(</span><span>0</span><span>,</span> <span>100</span><span>,</span> <span>100</span><span>,</span> <span>400</span><span>)</span>
      <span>rectangle</span><span>(</span><span>100</span><span>,</span> <span>0</span><span>,</span> <span>400</span><span>,</span> <span>100</span><span>)</span>
      
      <span>fill</span> <span>r</span>: <span>204</span><span>,</span> <span>g</span>: <span>102</span><span>,</span> <span>b</span>: <span>204</span>
    <span>}</span>
    <span>path</span> <span>{</span> <span># declarative stable path</span>
      <span>figure</span><span>(</span><span>100</span><span>,</span> <span>100</span><span>)</span> <span>{</span>
        <span>line</span><span>(</span><span>100</span><span>,</span> <span>400</span><span>)</span>
        <span>line</span><span>(</span><span>400</span><span>,</span> <span>100</span><span>)</span>
        <span>line</span><span>(</span><span>400</span><span>,</span> <span>400</span><span>)</span>

        <span>closed</span> <span>true</span>
      <span>}</span>

      <span>fill</span> <span>r</span>: <span>202</span><span>,</span> <span>g</span>: <span>102</span><span>,</span> <span>b</span>: <span>104</span><span>,</span> <span>a</span>: <span>0.5</span>
      <span>stroke</span> <span>r</span>: <span>0</span><span>,</span> <span>g</span>: <span>0</span><span>,</span> <span>b</span>: <span>0</span>
    <span>}</span>
    <span>path</span> <span>{</span> <span># declarative stable path</span>
      <span>figure</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>)</span> <span>{</span>
        <span>bezier</span><span>(</span><span>200</span><span>,</span> <span>100</span><span>,</span> <span>100</span><span>,</span> <span>200</span><span>,</span> <span>400</span><span>,</span> <span>100</span><span>)</span>
        <span>bezier</span><span>(</span><span>300</span><span>,</span> <span>100</span><span>,</span> <span>100</span><span>,</span> <span>300</span><span>,</span> <span>100</span><span>,</span> <span>400</span><span>)</span>
        <span>bezier</span><span>(</span><span>100</span><span>,</span> <span>300</span><span>,</span> <span>300</span><span>,</span> <span>100</span><span>,</span> <span>400</span><span>,</span> <span>400</span><span>)</span>

        <span>closed</span> <span>true</span>
      <span>}</span>

      <span>fill</span> <span>r</span>: <span>202</span><span>,</span> <span>g</span>: <span>102</span><span>,</span> <span>b</span>: <span>204</span><span>,</span> <span>a</span>: <span>0.5</span>
      <span>stroke</span> <span>r</span>: <span>0</span><span>,</span> <span>g</span>: <span>0</span><span>,</span> <span>b</span>: <span>0</span><span>,</span> <span>thickness</span>: <span>2</span><span>,</span> <span>dashes</span>: <span>[</span><span>50</span><span>,</span> <span>10</span><span>,</span> <span>10</span><span>,</span> <span>10</span><span>]</span><span>,</span> <span>dash_phase</span>: -<span>50.0</span>
    <span>}</span>
    <span>path</span> <span>{</span> <span># declarative stable path</span>
      <span>arc</span><span>(</span><span>200</span><span>,</span> <span>200</span><span>,</span> <span>90</span><span>,</span> <span>0</span><span>,</span> <span>360</span><span>,</span> <span>false</span><span>)</span>

      <span>fill</span> <span>r</span>: <span>202</span><span>,</span> <span>g</span>: <span>102</span><span>,</span> <span>b</span>: <span>204</span><span>,</span> <span>a</span>: <span>0.5</span>
      <span>stroke</span> <span>r</span>: <span>0</span><span>,</span> <span>g</span>: <span>0</span><span>,</span> <span>b</span>: <span>0</span><span>,</span> <span>thickness</span>: <span>2</span>
    <span>}</span>
    
    <span>on_mouse_event</span> <span>do</span> |<span>area_mouse_event</span>|
      <span>p</span> <span>area_mouse_event</span>
    <span>end</span>
    
    <span>on_mouse_moved</span> <span>do</span> |<span>area_mouse_event</span>|
      <span>puts</span> <span>'moved'</span>
    <span>end</span>
    
    <span>on_mouse_down</span> <span>do</span> |<span>area_mouse_event</span>|
      <span>puts</span> <span>'mouse down'</span>
    <span>end</span>
    
    <span>on_mouse_up</span> <span>do</span> |<span>area_mouse_event</span>|
      <span>puts</span> <span>'mouse up'</span>
    <span>end</span>
    
    <span>on_mouse_drag_started</span> <span>do</span> |<span>area_mouse_event</span>|
      <span>puts</span> <span>'drag started'</span>
    <span>end</span>
    
    <span>on_mouse_dragged</span> <span>do</span> |<span>area_mouse_event</span>|
      <span>puts</span> <span>'dragged'</span>
    <span>end</span>
    
    <span>on_mouse_dropped</span> <span>do</span> |<span>area_mouse_event</span>|
      <span>puts</span> <span>'dropped'</span>
    <span>end</span>
    
    <span>on_mouse_entered</span> <span>do</span>
      <span>puts</span> <span>'entered'</span>
    <span>end</span>
    
    <span>on_mouse_exited</span> <span>do</span>
      <span>puts</span> <span>'exited'</span>
    <span>end</span>
    
    <span>on_key_event</span> <span>do</span> |<span>area_key_event</span>|
      <span>p</span> <span>area_key_event</span>
    <span>end</span>
    
    <span>on_key_up</span> <span>do</span> |<span>area_key_event</span>|
      <span>puts</span> <span>'key up'</span>
    <span>end</span>
    
    <span>on_key_down</span> <span>do</span> |<span>area_key_event</span>|
      <span>puts</span> <span>'key down'</span>
    <span>end</span>
  <span>}</span>
<span>}</span><span>.</span><span>show</span></pre></div>
<p dir="auto">Mac</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-mac-area-gallery.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-mac-area-gallery.png" alt="glimmer-dsl-libui-mac-area-gallery.png"></a></p>
<p dir="auto">Windows</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-windows-area-gallery.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-windows-area-gallery.png" alt="glimmer-dsl-libui-windows-area-gallery.png"></a></p>
<p dir="auto">Linux</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-linux-area-gallery.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-libui/master/images/glimmer-dsl-libui-linux-area-gallery.png" alt="glimmer-dsl-libui-linux-area-gallery.png"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Glimmer DSL for Tk (MRI Ruby Desktop Development GUI Library)</h4><a id="user-content-glimmer-dsl-for-tk-mri-ruby-desktop-development-gui-library" aria-label="Permalink: Glimmer DSL for Tk (MRI Ruby Desktop Development GUI Library)" href="#glimmer-dsl-for-tk-mri-ruby-desktop-development-gui-library"></a></p>
<p dir="auto"><a href="https://www.tcl.tk/" rel="nofollow">Tcl/Tk</a> has recently improved by gaining native looking themed widgets on Mac, Windows, and Linux in <a href="https://www.tcl.tk/software/tcltk/8.5.html#:~:text=Highlights%20of%20Tk%208.5&amp;text=Font%20rendering%3A%20Now%20uses%20anti,and%20window%20layout%2C%20and%20more." rel="nofollow">Tk version 8.5</a>. Additionally, <a href="https://www.ruby-lang.org/en/" rel="nofollow">Ruby</a> 3.0 Ractor (formerly known as <a href="https://olivierlacan.com/posts/concurrency-in-ruby-3-with-guilds/" rel="nofollow">Guilds</a>) supports truly parallel multi-threading, making both <a href="https://github.com/ruby/ruby">MRI</a> and <a href="https://www.tcl.tk/" rel="nofollow">Tk</a> finally viable for support in <a href="https://github.com/AndyObtiva/glimmer">Glimmer</a> (Ruby Desktop Development GUI Library) as an alternative to <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">JRuby on SWT</a>.</p>
<p dir="auto">The trade-off is that while <a href="https://www.eclipse.org/swt/" rel="nofollow">SWT</a> provides a plethora of high quality reusable widgets for the Enterprise (such as <a href="https://www.eclipse.org/nebula/" rel="nofollow">Nebula</a>), <a href="https://www.tcl.tk/" rel="nofollow">Tk</a> enables very fast app startup time and a small memory footprint via <a href="https://www.ruby-lang.org/en/" rel="nofollow">MRI Ruby</a>.</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-tk">Glimmer DSL for Tk</a> aims to provide a DSL similar to the <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a> to enable more productive desktop development in Ruby with:</p>
<ul dir="auto">
<li>Declarative DSL syntax that visually maps to the GUI widget hierarchy</li>
<li>Convention over configuration via smart defaults and automation of low-level details</li>
<li>Requiring the least amount of syntax possible to build GUI</li>
<li>Bidirectional Data-Binding to declaratively wire and automatically synchronize GUI with Business Models</li>
<li>Custom Widget support</li>
<li>Scaffolding for new custom widgets, apps, and gems</li>
<li>Native-Executable packaging on Mac, Windows, and Linux</li>
</ul>
<p dir="auto">To get started, visit the <a href="https://github.com/AndyObtiva/glimmer-dsl-tk#pre-requisites">Glimmer DSL for Tk project page</a> for instructions on installing the <a href="https://rubygems.org/gems/glimmer-dsl-tk" rel="nofollow">glimmer-dsl-tk gem</a>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Glimmer DSL for Tk Samples</h5><a id="user-content-glimmer-dsl-for-tk-samples" aria-label="Permalink: Glimmer DSL for Tk Samples" href="#glimmer-dsl-for-tk-samples"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Hello, World!</h6><a id="user-content-hello-world-2" aria-label="Permalink: Hello, World!" href="#hello-world-2"></a></p>
<p dir="auto">Glimmer code (from <a href="https://github.com/AndyObtiva/glimmer-dsl-tk/blob/master/samples/hello/hello_world.rb">samples/hello/hello_world.rb</a>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="include Glimmer

root {
  label {
    text 'Hello, World!'
  }
}.open"><pre><span>include</span> <span>Glimmer</span>

<span>root</span> <span>{</span>
  <span>label</span> <span>{</span>
    <span>text</span> <span>'Hello, World!'</span>
  <span>}</span>
<span>}</span><span>.</span><span>open</span></pre></div>
<p dir="auto">Run (with the <a href="https://rubygems.org/gems/glimmer-dsl-tk" rel="nofollow">glimmer-dsl-tk</a> gem installed):</p>
<div data-snippet-clipboard-copy-content="ruby -r glimmer-dsl-tk -e &quot;require '../samples/hello/hello_world.rb'&quot;"><pre><code>ruby -r glimmer-dsl-tk -e "require '../samples/hello/hello_world.rb'"
</code></pre></div>
<p dir="auto">Glimmer app:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-world.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-world.png" alt="glimmer dsl tk screenshot sample hello world"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Hello, Notebook!</h6><a id="user-content-hello-notebook" aria-label="Permalink: Hello, Notebook!" href="#hello-notebook"></a></p>
<p dir="auto">Glimmer code (from <a href="https://github.com/AndyObtiva/glimmer-dsl-tk/blob/master/samples/hello/hello_tab.rb">samples/hello/hello_tab.rb</a>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="include Glimmer

root {
  title 'Hello, Notebook!'
   
  notebook {
    frame(text: 'English') {
      label {
        text 'Hello, World!'
      }
    }
     
    frame(text: 'French') {
      label {
        text 'Bonjour, Univers!'
      }
    }
  }
}.open"><pre><span>include</span> <span>Glimmer</span>

<span>root</span> <span>{</span>
  <span>title</span> <span>'Hello, Notebook!'</span>
   
  <span>notebook</span> <span>{</span>
    <span>frame</span><span>(</span><span>text</span>: <span>'English'</span><span>)</span> <span>{</span>
      <span>label</span> <span>{</span>
        <span>text</span> <span>'Hello, World!'</span>
      <span>}</span>
    <span>}</span>
     
    <span>frame</span><span>(</span><span>text</span>: <span>'French'</span><span>)</span> <span>{</span>
      <span>label</span> <span>{</span>
        <span>text</span> <span>'Bonjour, Univers!'</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span><span>.</span><span>open</span></pre></div>
<p dir="auto">Run (with the <a href="https://rubygems.org/gems/glimmer-dsl-tk" rel="nofollow">glimmer-dsl-tk</a> gem installed):</p>
<div data-snippet-clipboard-copy-content="ruby -r glimmer-dsl-tk -e &quot;require '../samples/hello/hello_notebook.rb'&quot;"><pre><code>ruby -r glimmer-dsl-tk -e "require '../samples/hello/hello_notebook.rb'"
</code></pre></div>
<p dir="auto">Glimmer app:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-notebook-english.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-notebook-english.png" alt="glimmer dsl tk screenshot sample hello notebook English"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-notebook-french.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-notebook-french.png" alt="glimmer dsl tk screenshot sample hello notebook French"></a></p>
<p dir="auto"><h6 tabindex="-1" dir="auto">Hello, Combobox!</h6><a id="user-content-hello-combobox" aria-label="Permalink: Hello, Combobox!" href="#hello-combobox"></a></p>
<p dir="auto">Glimmer code (from <a href="https://github.com/AndyObtiva/glimmer-dsl-tk/blob/master/samples/hello/hello_combobox.rb">samples/hello/hello_combobox.rb</a>):</p>
<div dir="auto" data-snippet-clipboard-copy-content="require 'glimmer-dsl-tk'

class Person
  attr_accessor :country, :country_options

  def initialize
    self.country_options=[&quot;&quot;, &quot;Canada&quot;, &quot;US&quot;, &quot;Mexico&quot;]
    self.country = &quot;Canada&quot;
  end

  def reset_country
    self.country = &quot;Canada&quot;
  end
end

class HelloCombobox
  include Glimmer
  
  def launch
    person = Person.new
    
    root {
      title 'Hello, Combobox!'
      
      combobox {
        readonly true # this applies to text editing only (item selection still triggers a write to model)
        text <=> [person, :country]
      }
      
      button {
        text &quot;Reset Selection&quot;
        command {
          person.reset_country
        }
      }
    }.open
  end
end

HelloCombobox.new.launch"><pre><span>require</span> <span>'glimmer-dsl-tk'</span>

<span>class</span> <span>Person</span>
  <span>attr_accessor</span> <span>:country</span><span>,</span> <span>:country_options</span>

  <span>def</span> <span>initialize</span>
    <span>self</span><span>.</span><span>country_options</span><span>=</span><span>[</span><span>""</span><span>,</span> <span>"Canada"</span><span>,</span> <span>"US"</span><span>,</span> <span>"Mexico"</span><span>]</span>
    <span>self</span><span>.</span><span>country</span> <span>=</span> <span>"Canada"</span>
  <span>end</span>

  <span>def</span> <span>reset_country</span>
    <span>self</span><span>.</span><span>country</span> <span>=</span> <span>"Canada"</span>
  <span>end</span>
<span>end</span>

<span>class</span> <span>HelloCombobox</span>
  <span>include</span> <span>Glimmer</span>
  
  <span>def</span> <span>launch</span>
    <span>person</span> <span>=</span> <span>Person</span><span>.</span><span>new</span>
    
    <span>root</span> <span>{</span>
      <span>title</span> <span>'Hello, Combobox!'</span>
      
      <span>combobox</span> <span>{</span>
        <span>readonly</span> <span>true</span> <span># this applies to text editing only (item selection still triggers a write to model)</span>
        <span>text</span> &lt;=&gt; <span>[</span><span>person</span><span>,</span> <span>:country</span><span>]</span>
      <span>}</span>
      
      <span>button</span> <span>{</span>
        <span>text</span> <span>"Reset Selection"</span>
        <span>command</span> <span>{</span>
          <span>person</span><span>.</span><span>reset_country</span>
        <span>}</span>
      <span>}</span>
    <span>}</span><span>.</span><span>open</span>
  <span>end</span>
<span>end</span>

<span>HelloCombobox</span><span>.</span><span>new</span><span>.</span><span>launch</span></pre></div>
<p dir="auto">Run (with the <a href="https://rubygems.org/gems/glimmer-dsl-tk" rel="nofollow">glimmer-dsl-tk</a> gem installed):</p>
<div data-snippet-clipboard-copy-content="ruby -r glimmer-dsl-tk -e &quot;require '../samples/hello/hello_combobox.rb'&quot;"><pre><code>ruby -r glimmer-dsl-tk -e "require '../samples/hello/hello_combobox.rb'"
</code></pre></div>
<p dir="auto">Glimmer app:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-combobox.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-combobox.png" alt="glimmer dsl tk screenshot sample hello combobox"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-combobox-dropdown.png"><img src="https://raw.githubusercontent.com/AndyObtiva/glimmer-dsl-tk/master/images/glimmer-dsl-tk-screenshot-sample-hello-combobox-dropdown.png" alt="glimmer dsl tk screenshot sample hello combobox dropdown"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Glimmer DSL for XML (&amp; HTML)</h4><a id="user-content-glimmer-dsl-for-xml--html" aria-label="Permalink: Glimmer DSL for XML (&amp; HTML)" href="#glimmer-dsl-for-xml--html"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-xml">Glimmer DSL for XML</a> provides Ruby syntax for building XML (eXtensible Markup Language) documents.</p>
<p dir="auto">Within the context of desktop development, Glimmer DSL for XML is useful in providing XML data for the <a href="https://github.com/AndyObtiva/glimmer/tree/master#browser-widget">SWT Browser widget</a>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">XML DSL</h5><a id="user-content-xml-dsl" aria-label="Permalink: XML DSL" href="#xml-dsl"></a></p>
<p dir="auto">Simply start with <code>html</code> keyword and add HTML inside its block using Glimmer DSL syntax.
Once done, you may call <code>to_s</code>, <code>to_xml</code>, or <code>to_html</code> to get the formatted HTML output.</p>
<p dir="auto">Here are all the Glimmer XML DSL top-level keywords:</p>
<ul dir="auto">
<li><code>html</code></li>
<li><code>tag</code>: enables custom tag creation for exceptional cases by passing tag name as '_name' attribute</li>
<li><code>name_space</code>: enables namespacing html tags</li>
</ul>
<p dir="auto">Element properties are typically passed as a key/value hash (e.g. <code>section(id: 'main', class: 'accordion')</code>) . However, for properties like "selected" or "checked", you must leave value <code>nil</code> or otherwise pass in front of the hash (e.g. <code>input(:checked, type: 'checkbox')</code> )</p>
<p dir="auto">Example (basic HTML):</p>
<div dir="auto" data-snippet-clipboard-copy-content="@xml = html {
  head {
    meta(name: &quot;viewport&quot;, content: &quot;width=device-width, initial-scale=2.0&quot;)
  }
  body {
    h1 { &quot;Hello, World!&quot; }
  }
}
puts @xml"><pre><span>@xml</span> <span>=</span> <span>html</span> <span>{</span>
  <span>head</span> <span>{</span>
    <span>meta</span><span>(</span><span>name</span>: <span>"viewport"</span><span>,</span> <span>content</span>: <span>"width=device-width, initial-scale=2.0"</span><span>)</span>
  <span>}</span>
  <span>body</span> <span>{</span>
    <span>h1</span> <span>{</span> <span>"Hello, World!"</span> <span>}</span>
  <span>}</span>
<span>}</span>
<span>puts</span> <span>@xml</span></pre></div>
<p dir="auto">Output:</p>
<div data-snippet-clipboard-copy-content="<html><head><meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=2.0&quot; /></head><body><h1>Hello, World!</h1></body></html>"><pre><code>&lt;html&gt;&lt;head&gt;&lt;meta name="viewport" content="width=device-width, initial-scale=2.0" /&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello, World!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;
</code></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Glimmer DSL for CSS</h4><a id="user-content-glimmer-dsl-for-css" aria-label="Permalink: Glimmer DSL for CSS" href="#glimmer-dsl-for-css"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-css">Glimmer DSL for CSS</a> provides Ruby syntax for building CSS (Cascading Style Sheets).</p>
<p dir="auto">Within the context of <a href="https://github.com/AndyObtiva/glimmer">Glimmer</a> app development, Glimmer DSL for CSS is useful in providing CSS for the <a href="https://github.com/AndyObtiva/glimmer/tree/master#browser-widget">SWT Browser widget</a>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">CSS DSL</h5><a id="user-content-css-dsl" aria-label="Permalink: CSS DSL" href="#css-dsl"></a></p>
<p dir="auto">Simply start with <code>css</code> keyword and add stylesheet rule sets inside its block using Glimmer DSL syntax.
Once done, you may call <code>to_s</code> or <code>to_css</code> to get the formatted CSS output.</p>
<p dir="auto"><code>css</code> is the only top-level keyword in the Glimmer CSS DSL</p>
<p dir="auto">Selectors may be specified by <code>s</code> keyword or HTML element keyword directly (e.g. <code>body</code>)
Rule property values may be specified by <code>pv</code> keyword or underscored property name directly (e.g. <code>font_size</code>)</p>
<p dir="auto">Example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@css = css {
  body {
    font_size '1.1em'
    pv 'background', 'white'
  }
  
  s('body > h1') {
    background_color :red
    pv 'font-size', '2em'
  }
}
puts @css"><pre><span>@css</span> <span>=</span> <span>css</span> <span>{</span>
  <span>body</span> <span>{</span>
    <span>font_size</span> <span>'1.1em'</span>
    <span>pv</span> <span>'background'</span><span>,</span> <span>'white'</span>
  <span>}</span>
  
  <span>s</span><span>(</span><span>'body &gt; h1'</span><span>)</span> <span>{</span>
    <span>background_color</span> <span>:red</span>
    <span>pv</span> <span>'font-size'</span><span>,</span> <span>'2em'</span>
  <span>}</span>
<span>}</span>
<span>puts</span> <span>@css</span></pre></div>
<p dir="auto">Output:</p>
<div data-snippet-clipboard-copy-content="body{font-size:1.1em;background:white}body > h1{background-color:red;font-size:2em}"><pre><code>body{font-size:1.1em;background:white}body &gt; h1{background-color:red;font-size:2em}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Data-Binding Library</h2><a id="user-content-data-binding-library" aria-label="Permalink: Data-Binding Library" href="#data-binding-library"></a></p>
<p dir="auto">Data-Binding enables mapping GUI properties (like text and color) to Model attributes (like name and age) for bidirectional or unidirectional synchronization and conversion as needed.</p>
<p dir="auto">Data-binding supports utilizing the <a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93presenter" rel="nofollow">MVP (Model View Presenter)</a> flavor of <a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller" rel="nofollow">MVC</a> by observing both the View and a Presenter for changes and updating the opposite side upon encountering them. This enables writing more decoupled cleaner code that keeps View code and Model code disentangled and highly maintainable.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/66fac44b0cd245487d94ab24d9a30a2fb5a8ca0c9141a5cb6ad960f1f27069db/68747470733a2f2f7777772e7265736561726368676174652e6e65742f70726f66696c652f47696c6c65732d506572726f75696e2f7075626c69636174696f6e2f3332303234393538342f6669677572652f666967382f41533a36363832363039383730363834313840313533363333373234333338352f4d6f64656c2d766965772d70726573656e7465722d6172636869746563747572652e706e67"><img src="https://camo.githubusercontent.com/66fac44b0cd245487d94ab24d9a30a2fb5a8ca0c9141a5cb6ad960f1f27069db/68747470733a2f2f7777772e7265736561726368676174652e6e65742f70726f66696c652f47696c6c65732d506572726f75696e2f7075626c69636174696f6e2f3332303234393538342f6669677572652f666967382f41533a36363832363039383730363834313840313533363333373234333338352f4d6f64656c2d766965772d70726573656e7465722d6172636869746563747572652e706e67" alt="MVP" data-canonical-src="https://www.researchgate.net/profile/Gilles-Perrouin/publication/320249584/figure/fig8/AS:668260987068418@1536337243385/Model-view-presenter-architecture.png"></a></p>
<p dir="auto">Glimmer enhances observed models automatically (including array operations like <code>&lt;&lt;</code>, <code>delete</code>, and <code>reject!</code>) on first observation. As such, you get automatic observable support, including nested and computed observations. No need to change your model code to data-bind it to the view or add repetitive boilerplate modules. View data-binding is truly decoupled from model logic by being able to observe any model attribute (Ruby attribute reader/writer combo or Ruby attribute reader alone for read-only data-binding when needed)</p>
<p dir="auto">This relies mainly on the Observer Design Pattern and the MVP (Model-View-Presenter) Architectural Pattern (a variation on MVC)</p>
<p dir="auto">These are the main classes concerning data-binding:</p>
<ul dir="auto">
<li><code>Glimmer::DataBinding::Observer</code>: Provides general observer support including unique registration and deregistration for cleanup and prevention of memory leaks. Main methods concerned are: <code>call</code>, <code>register</code> (alias: <code>observe</code>), and <code>unregister</code> (alias: <code>unobserve</code> or <code>deregister</code>). Passing the option <code>ignore_frozen: true</code> at the end of the args of <code>register</code> (alias: <code>observe</code>) method results in silently ignoring any passed frozen observable without raising an error (it raises an error otherwise for frozen/immutable objects).</li>
<li><code>Glimmer::DataBinding::Observable</code>: General super-module for all observables. Main methods concerned are: <code>add_observer</code> and <code>remove_observer</code></li>
<li><code>Glimmer::DataBinding::ObservableModel</code>: Mixin module for any observable model (<code>Object</code>, <code>Struct</code> or <code>OpenStruct</code>) with observable attributes (observes attribute writers and <code>Struct</code>/<code>OpenStruct</code> <code>:[]=</code> method). In addition to <code>Observable</code> methods, it has a <code>notify_observers</code> method to be called when changes occur. It automatically enhances all attribute setters (ending with <code>=</code>) to notify observers on changes. Also, it automatically handles observing array attributes using <code>ObservableArray</code> appropriately so they would notify observers upon array mutation changes. <code>:attribute_writer_type</code> option can be specified (default: <code>:attribute=</code>) to observe different attribute styles (e.g. <code>attribute_writer_type: [:attribute=, :set_attribute]</code>).</li>
<li><code>Glimmer::DataBinding::ObservableArray</code>: Mixin module for any observable array collection that automatically handles notifying observers upon performing array mutation operations (e.g. <code>push</code>, <code>select!</code>, or <code>delete</code>) recursively (meaning if an array contained arrays and they changed, observers are notified). Accepts <code>recursive: true</code> option in <code>add_observer</code> method to recursively observe nested arrays all the way down. Alternatively, pass <code>recursive: [integer]</code> to limit recursion in <code>Array</code> observation to a specific number of levels beyond the first level (which is always included).</li>
<li><code>Glimmer::DataBinding::ObservableHash</code>: Mixin module for any observable hash that automatically handles notifying observers upon performing hash mutation operations (e.g. <code>hash[key]=value</code>, <code>select!</code>, <code>merge!</code>).  Also, it automatically handles observing array values using <code>ObservableArray</code> appropriately so they would notify observers upon array mutation changes.</li>
<li><code>Glimmer::DataBinding::ModelBinding</code>: a higher-level abstraction that relies on all the other observer/observable classes to support basic data-binding, nested data-binding, and computed data-binding</li>
<li><code>Glimmer::DataBinding::Shine</code>: enables <a href="#shine-data-binding-syntax">highly intuitive and visually expressive syntax</a> to perform bidirectional (two-way) data-binding with <code>&lt;=&gt;</code> and unidirectional (one-way) data-binding with <code>&lt;=</code></li>
</ul>
<p dir="auto">To do simple observation of models, arrays, or hashes, you can use the <code>Glimmer::DataBinding::Observer::proc</code> method, which builds an observer from a block. When invoking the <code>#observe</code> method on it, it automatically enhances the object argument being observed into an <code>Observable</code> (whether <code>ObservableModel</code>, <code>ObservableArray</code>, or <code>ObervableHash</code>).</p>
<p dir="auto">Example of observing a model attribute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Glimmer::DataBinding::Observer.proc do |new_value|
  # Do some work with new value for model attribute
end.observe(model, attribute)"><pre><span>Glimmer</span>::<span>DataBinding</span>::<span>Observer</span><span>.</span><span>proc</span> <span>do</span> |<span>new_value</span>|
  <span># Do some work with new value for model attribute</span>
<span>end</span><span>.</span><span>observe</span><span>(</span><span>model</span><span>,</span> <span>attribute</span><span>)</span></pre></div>
<p dir="auto">Example of observing an array recursively (avoid recursion unless really needed since it fires on all fine-grained nested array changes):</p>
<div dir="auto" data-snippet-clipboard-copy-content="Glimmer::DataBinding::Observer.proc do |new_value|
  # Do some work with new array value
end.observe(array, recursive: true)"><pre><span>Glimmer</span>::<span>DataBinding</span>::<span>Observer</span><span>.</span><span>proc</span> <span>do</span> |<span>new_value</span>|
  <span># Do some work with new array value</span>
<span>end</span><span>.</span><span>observe</span><span>(</span><span>array</span><span>,</span> <span>recursive</span>: <span>true</span><span>)</span></pre></div>
<p dir="auto">Example of observing a hash key:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Glimmer::DataBinding::Observer.proc do |new_value|
  # Do some work with new value for hash key
end.observe(hash, :price)"><pre><span>Glimmer</span>::<span>DataBinding</span>::<span>Observer</span><span>.</span><span>proc</span> <span>do</span> |<span>new_value</span>|
  <span># Do some work with new value for hash key</span>
<span>end</span><span>.</span><span>observe</span><span>(</span><span>hash</span><span>,</span> <span>:price</span><span>)</span></pre></div>
<p dir="auto">Example of observing a hash for all key changes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="Glimmer::DataBinding::Observer.proc do |new_value, changed_key|
  # Do some work with new value and changed key for hash
end.observe(hash)"><pre><span>Glimmer</span>::<span>DataBinding</span>::<span>Observer</span><span>.</span><span>proc</span> <span>do</span> |<span>new_value</span><span>,</span> <span>changed_key</span>|
  <span># Do some work with new value and changed key for hash</span>
<span>end</span><span>.</span><span>observe</span><span>(</span><span>hash</span><span>)</span></pre></div>
<p dir="auto">If you would like to observe nested model attribute changes, you can use the more advanced <code>Glimmer::DataBinding::ModelBinding</code> class instead.</p>
<p dir="auto">Example of observing nested model attributes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ModelBinding.new(model, &quot;address1.street&quot;).add_observer do |new_address1_street_value|
  # Do some work with new address 1 street value
end"><pre><span>ModelBinding</span><span>.</span><span>new</span><span>(</span><span>model</span><span>,</span> <span>"address1.street"</span><span>)</span><span>.</span><span>add_observer</span> <span>do</span> |<span>new_address1_street_value</span>|
  <span># Do some work with new address 1 street value</span>
<span>end</span></pre></div>
<p dir="auto">Example of observing indexed array changes (specifying an array index) (combined with a nested model attribute):</p>
<div dir="auto" data-snippet-clipboard-copy-content="ModelBinding.new(model, &quot;employees[5].name&quot;).add_observer do |new_employee_6_name|
  # Do some work with new employee 6 (index 5)'s name
end"><pre><span>ModelBinding</span><span>.</span><span>new</span><span>(</span><span>model</span><span>,</span> <span>"employees[5].name"</span><span>)</span><span>.</span><span>add_observer</span> <span>do</span> |<span>new_employee_6_name</span>|
  <span># Do some work with new employee 6 (index 5)'s name</span>
<span>end</span></pre></div>
<p dir="auto">Example of observing double-indexed nested array changes:</p>
<div dir="auto" data-snippet-clipboard-copy-content="ModelBinding.new(model, &quot;grid[5][7]&quot;).add_observer do |new_grid_cell_value|
  # Do some work with new grid cell value for row index 5 and column index 7
end"><pre><span>ModelBinding</span><span>.</span><span>new</span><span>(</span><span>model</span><span>,</span> <span>"grid[5][7]"</span><span>)</span><span>.</span><span>add_observer</span> <span>do</span> |<span>new_grid_cell_value</span>|
  <span># Do some work with new grid cell value for row index 5 and column index 7</span>
<span>end</span></pre></div>
<p dir="auto">Example of observing keyed hash changes (specifying a hash key as <code>Symbol</code> or single/double-quoted <code>String</code>) (combined with a nested model attribute):</p>
<div dir="auto" data-snippet-clipboard-copy-content="ModelBinding.new(model, &quot;employees[:manager].name&quot;).add_observer do |new_employee_6_name|
  # Do some work with new manager employee's name
end"><pre><span>ModelBinding</span><span>.</span><span>new</span><span>(</span><span>model</span><span>,</span> <span>"employees[:manager].name"</span><span>)</span><span>.</span><span>add_observer</span> <span>do</span> |<span>new_employee_6_name</span>|
  <span># Do some work with new manager employee's name</span>
<span>end</span></pre></div>
<p dir="auto">Data-bound <code>ModelBinding</code> attribute can be:</p>
<ul dir="auto">
<li><strong>Direct:</strong> <code>Symbol</code> representing attribute reader/writer (e.g. <code>[person, :name</code>])</li>
<li><strong>Nested:</strong> <code>String</code> representing nested attribute path (e.g. <code>[company, 'address.street']</code>). That results in "nested data-binding"</li>
<li><strong>Indexed:</strong> <code>String</code> containing array attribute index (e.g. <code>[customer, 'addresses[0].street']</code>). That results in "indexed data-binding"</li>
<li><strong>Keyed:</strong> <code>String</code> containing hash attribute key (e.g. <code>[customer, 'addresses[:main].street']</code>). That results in "keyed data-binding"</li>
</ul>
<p dir="auto">Data-binding options include:</p>
<ul dir="auto">
<li><code>before_read {|value| ...}</code>: performs an operation before reading data from Model to update View.</li>
<li><code>on_read {|value| ...}</code>: converts value read from Model to update the View.</li>
<li><code>after_read {|converted_value| ...}</code>: performs an operation after read from Model to update View.</li>
<li><code>before_write {|value| ...}</code>: performs an operation before writing data to Model from View.</li>
<li><code>on_write {|value| ...}</code>: converts value read from View to update the Model.</li>
<li><code>after_write {|converted_value| ...}</code>: performs an operation after writing to Model from View.</li>
<li><code>computed_by attribute</code> or <code>computed_by [attribute1, attribute2, ...]</code>: indicates model attribute is computed from specified attribute(s), thus updated when they are updated. That is known as "computed data-binding".</li>
</ul>
<p dir="auto">Note that if an observed model attribute or hash key is an <code>Array</code>, it is automatically observed for <code>Array</code> changes (e.g. via mutation methods <code>&lt;&lt;</code>, <code>delete</code>, <code>map!</code>), not just attribute/key-value changes.</p>
<p dir="auto">All of the features above make Glimmer's data-binding library one of the most sophisticated and advanced in the industry since it automates everything instead of requiring endless manual configuration, thus resulting in some of the tersest most declarative syntax for using observers and data-binding.</p>
<p dir="auto">You may learn more by looking into <a href="https://github.com/AndyObtiva/glimmer/blob/master/Users/andy/code/glimmer/spec/lib/glimmer/data_binding">data-binding specs</a> as well as <a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/master/docs/reference/GLIMMER_GUI_DSL_SYNTAX.md#data-binding">Data-Binding</a> and <a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/master/docs/reference/GLIMMER_GUI_DSL_SYNTAX.md#observer">Observer</a> usage in <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">Glimmer DSL for SWT</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Shine Data-Binding Syntax</h3><a id="user-content-shine-data-binding-syntax" aria-label="Permalink: Shine Data-Binding Syntax" href="#shine-data-binding-syntax"></a></p>
<p dir="auto">The Shine data-binding syntax is a highly intuitive and visually expressive way of data-binding that enables performing bidirectional (two-way) data-binding with the <code>&lt;=&gt;</code> operator and unidirectional (one-way) data-binding with the <code>&lt;=</code> operator.</p>
<p dir="auto">It is facilitated by the combination of the <code>Glimmer::DSL::ShineDataBindingExpression</code> and <code>Glimmer::DataBinding::Shine</code> classes, which depend on <code>Glimmer::DSL::BindExpression</code> and <code>Glimmer::DataBinding::ModelBinding</code>.</p>
<p dir="auto">Below are some examples of Shine usage in GUI DSLs:</p>
<p dir="auto"><code>text &lt;=&gt; [contact, :first_name]</code></p>
<p dir="auto">This example bidirectionally binds the text property of a widget like label to the first name of a contact model.</p>
<p dir="auto"><code>text &lt;=&gt; [contact, 'address.street']</code></p>
<p dir="auto">This example binds the text property of a widget like label to the nested street of the address of a contact. This is called nested property data binding.</p>
<p dir="auto"><code>text &lt;=&gt; [contact, 'address.street', on_read: :upcase, on_write: :downcase]</code></p>
<p dir="auto">This example adds on the one above it by specifying converters on read and write of the model property, like in the case of a text widget. The text widget will then displays the street upper case and the model will store it lower case. When specifying converters, read and write operations must be symmetric.</p>
<p dir="auto"><code>enabled &lt;= [user, :logged_in]</code></p>
<p dir="auto">This example unidirectionally binds the enabled property of a widget like button to the logged in status of a user.</p>
<p dir="auto"><code>enabled &lt;= [user, :logged_in, on_read: :!]</code></p>
<p dir="auto">This example unidirectionally binds the enabled property of a widget like entry to the negated logged in status of a user. Note that when using a single on read converter with unidirectional data-binding, there is no need for a symmetric on_write converter as well since writing is never done with unidirectional (one-way) data-binding.</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/master/docs/reference/GLIMMER_GUI_DSL_SYNTAX.md#shine">Learn more about Shine data-binding syntax from its usage in Glimmer DSL for SWT</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Glimmer Process</h2><a id="user-content-glimmer-process" aria-label="Permalink: Glimmer Process" href="#glimmer-process"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer/blob/master/PROCESS.md">Glimmer Process</a> is the lightweight software development process used for building Glimmer libraries and Glimmer apps, which goes beyond Agile, rendering all Agile processes obsolete. <a href="https://github.com/AndyObtiva/glimmer/blob/master/PROCESS.md">Glimmer Process</a> is simply made up of 7 guidelines to pick and choose as necessary until software development needs are satisfied.</p>
<p dir="auto">Learn more by reading the <a href="https://github.com/AndyObtiva/glimmer/blob/master/PROCESS.md">GPG</a> (Glimmer Process Guidelines)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">(Frequently Asked Questions)</p>
<p dir="auto"><strong>How do Glimmer GUI DSLs compare to Shoes?</strong></p>
<p dir="auto">If you liked Shoes, you'll love Glimmer!</p>
<p dir="auto">That is because Glimmer does everything that Shoes did, but with a lighter and better GUI DSL (Graphical User Interface Domain Specific Language) that addresses all the issues that Shoes suffered from, such as:</p>
<ul dir="auto">
<li>Shoes does not allow code in Shoes blocks to use variables defined outside of Shoes blocks in a straightforward manner as it changes <code>self</code> inside Shoes blocks, breaking Ruby expectations and producing confusing behavior. On the other hand, Glimmer DSL blocks are 100% standard Ruby blocks that represent real closures, so they enable usage of variables defined outside the blocks in a 100% standard Ruby way.</li>
<li>Shoes lacks support for high-quality business widget controls (View components) like table and tree. Glimmer GUI DSLs that are feature complete like Glimmer DSL for SWT do support table and tree widgets. Some non-final Glimmer GUI DSLs like Glimmer DSL for LibUI support the table control too.</li>
<li>Shoes does not encourage proper separation of concerns with a correct MVC architecture (Model-View-Controller), resulting in a lot of non-presentation logic mixed with View code. Glimmer GUI DSLs do support proper separation of concerns 100% following the MVC or MVP (Model-View-Presenter) architecture by default.</li>
<li>Shoes does not provide a simple way for connecting View components to Model data. Glimmer GUI DSLs provide full bidirectional/unidirectional data-binding support out of the box that provides the terest code syntax for connecting Views to Models and keeping them in sync.</li>
<li>Shoes does not support a native mechanism for building custom View components. Glimmer GUI DSLs do support the ability to build custom widgets (aka controls or View components), custom windows (aka shells), and custom shapes (canvas graphics), enabling software engineers to expand a Glimmer DSL’s vocabulary with new keywords representing brand new visual concepts. That results in much higher productivity by enabling the reuse of higher visual concepts as their own self-encapsulated components.</li>
<li>Shoes does not expose native features of its wrapped GUI toolkit. Glimmer GUI DSLs do expose all native features of their wrapped GUI toolkits, thus enabling developers to use a GUI toolkit like SWT directly when needed on top of using Glimmer DSL for SWT (a widget initialized via SWT directly could be passed to Glimmer DSL for SWT to wrap as a Glimmer <code>WidgetProxy</code> object and integrate with other Glimmer initialized <code>WidgetProxy</code> objects). That facilitates the 80/20 rule of having Glimmer GUI DSLs automate 80% of the work while still enabling software engineers to reach down to the low-level GUI toolkit API in 20% of the cases when needed (though in practice, it's probably more like 1% of the cases).</li>
</ul>
<p dir="auto">It is great that Shoes paved the way for creating desktop GUI DSLs in Ruby. Glimmer took that approach to its maximum and produced the ultimate evolution of Shoes.</p>
<p dir="auto"><strong>What is the difference between Glimmer and Glimmer DSL for SWT?</strong></p>
<p dir="auto">Glimmer DSL for SWT was the first GUI DSL created as part of the Glimmer project to enable building desktop applications, and it was originally just called Glimmer. It relied on the Eclipse SWT library to render native GUI (Graphical User Interface) widget controls (View components) on every platform (Mac, Windows, and Linux). Eventually, the idea of a Glimmer DSL proved itself so successful and viable for building desktop apps with a fraction of the effort needed in other programming languages/technologies that it was expanded to support other GUI toolkits. So, Glimmer got renamed to Glimmer DSL for SWT, and the core Glimmer DSL engine got extracted to Glimmer (becoming a DSL framework), which then got reused to build other Glimmer GUI DSLs such as Glimmer DSL for LibUI and Glimmer DSL for GTK, among many others.</p>
<p dir="auto"><strong>What is the difference between Glimmer DSL for SWT and Glimmer DSL for LibUI?</strong></p>
<p dir="auto">Both Glimmer DSL for SWT and Glimmer DSL for LibUI support rendering platform native widgets/controls, which enable building native desktop apps that look 100% native on every platform (Mac, Windows, and Linux).</p>
<p dir="auto">However, Glimmer DSL for SWT runs in JRuby (Ruby running in the JVM [Java Virtual Machine]) whereas Glimmer DSL for LibUI runs in standard Ruby (aka MRI Ruby or CRuby).</p>
<p dir="auto">Glimmer DSL for SWT is 100% feature-complete and has a final release. Glimmer DSL for LibUI is 100% complete as far as covering the LibUI features, but LibUI itself is still a mid-alpha library, so it is missing a few features that will get added eventually.</p>
<p dir="auto"><strong>What is the difference between Glimmer DSL for LibUI, Glimmer DSL for GTK, Glimmer DSL for Tk, Glimmer DSL for FX, and Glimmer DSL for WX?</strong></p>
<p dir="auto">All of Glimmer DSL for LibUI, Glimmer DSL for GTK, Glimmer DSL for Tk, Glimmer DSL for FX, and Glimmer DSL for WX run in standard Ruby (aka MRI Ruby or CRuby).</p>
<p dir="auto">However, only Glimmer DSL for LibUI and Glimmer DSL for WX render native controls on every platform. The other libraries do not render native controls on every platform, albeit Glimmer DSL for GTK renders native controls on Linux distributions utilizing Gnome.</p>
<p dir="auto">Also, Glimmer DSL for LibUI does not require any prerequisites beyond installing the Ruby gem, so you can install it and get instant GUI with very little effort, whereas Glimmer DSL for GTK, Glimmer DSL for Tk, Glimmer DSL for FX, and Glimmer DSL for WX do require extra dependencies in general, albeit Glimmer DSL for GTK has everything it needs in Linux Gnome flavors and both Glimmer DSL for FX and Glimmer DSL for WX have everything they need on Windows by including pre-built binaries.</p>
<p dir="auto">You may learn more about the differences between various Glimmer DSLs by checking out the <a href="#glimmer-dsl-comparison-table">Glimmer DSL Comparison Table</a>.</p>
<p dir="auto"><strong>What is the difference between Glimmer DSL for SWT, Glimmer DSL for Swing, and Glimmer DSL for JFX?</strong></p>
<p dir="auto">Glimmer DSL for SWT relies on the Eclipse SWT library, which renders native widgets on every platform (Mac, Windows, and Linux) to build desktop apps that look 100% native on every platform (Mac, Windows, and Linux).</p>
<p dir="auto">Glimmer DSL for Swing relies on Swing, which does not render native widgets on every platform. Glimmer DSL for JFX relies on JavaFX, which also does not render native widgets on every platform.</p>
<p dir="auto">Also, SWT initializes native widgets in memory using non-Java code (e.g. C/C++), thus ensuring native OS high performance for rendering native widgets without being prone to Java garbage collection pauses. On the other hand, Swing and JavaFX initialize non-native widgets in memory using Java code, thus depend on the performance of the Java Virtual Machine while being prone to Java garbage collection pauses. As a result, SWT provides a better user experience than Swing and JavaFX.</p>
<p dir="auto">You may learn more about the differences between various Glimmer DSLs by checking out the <a href="#glimmer-dsl-comparison-table">Glimmer DSL Comparison Table</a>.</p>
<p dir="auto"><strong>Why not just use SWT, LibUI, GTK, Tk, FOX Toolkit, wxWidgets, Swing, or JavaFX from Ruby directly?</strong></p>
<p dir="auto">GUI Toolkits implement low-level GUI rendering concerns. And, while some of them do offer object-oriented APIs, their APIs are very verbose and imperative by design due to being low-level APIs. As such, they require software engineers to write a lot more low-level code that does not map intuitively to the structure of the GUI visually, slowing down productivity and making maintainability more expensive.</p>
<p dir="auto">Glimmer GUI DSLs on the other hand are fully declarative and follow Rails' Convention Over Configuration maxim by including smart defaults and automation of low-level details, so they enable software engineers to write the simplest most minimalistic code that maps to the actual visual GUI concepts, maximizing productivity and resulting in code that is very maintainable and intuitive to reason about.</p>
<p dir="auto">Furthermore, Glimmer GUI DSLs offer advanced Bidirectional/Unidirectional Data-Binding Support, which enables syncing View data with Model attributes with the tersest code syntax possible to greatly simplify reasoning about the code while supporting proper separation of concerns through correct adherence to MVC (Model-View-Controller) and MVP (Model-View-Presenter).</p>
<p dir="auto">That's in addition to scaffolding and native executable packaging in some Glimmer GUI DSLs. As a result, productivity increases even further and maintainability becomes even less expensive, thus enabling software engineers to deliver pieces of software in a matter of minutes or hours for desktop application MVPs (Minimal Viable Products). As such, Glimmer GUI DSLs significantly shorten the feedback cycle and enable incrementally releasing features at a very fast pace, not possible with GUI toolkit low-level APIs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li><a href="https://andymaleh.blogspot.com/search/label/Tutorial+SWT" rel="nofollow">Glimmer DSL for SWT Video Tutorials</a></li>
<li><a href="http://andymaleh.blogspot.com/search/label/Glimmer" rel="nofollow">Code Master Blog</a></li>
<li><a href="http://shop.oreilly.com/product/9780596519650.do" rel="nofollow">JRuby Cookbook by Justin Edelson &amp; Henry Liu</a></li>
<li><a href="https://confreaks.tv/videos/mwrc2011-whatever-happened-to-desktop-development-in-ruby" rel="nofollow">MountainWest RubyConf 2011 Video</a></li>
<li><a href="https://confreaks.tv/videos/rubyconf2008-desktop-development-with-glimmer" rel="nofollow">RubyConf 2008 Video</a></li>
<li><a href="http://www.infoq.com/news/2008/02/glimmer-jruby-swt" rel="nofollow">InfoQ Article</a></li>
<li><a href="https://dzone.com/articles/an-introduction-glimmer" rel="nofollow">DZone Tutorial</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Help</h2><a id="user-content-help" aria-label="Permalink: Help" href="#help"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Issues</h3><a id="user-content-issues" aria-label="Permalink: Issues" href="#issues"></a></p>
<p dir="auto">You may submit <a href="https://github.com/AndyObtiva/glimmer/issues">issues</a> on <a href="https://github.com/AndyObtiva/glimmer/issues">GitHub</a>.</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer/issues">Click here to submit an issue.</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Chat</h3><a id="user-content-chat" aria-label="Permalink: Chat" href="#chat"></a></p>
<p dir="auto">If you need live help, try to <a href="https://gitter.im/AndyObtiva/glimmer?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge" rel="nofollow"><img src="https://camo.githubusercontent.com/34da5e29c959d7500e7a9c7922cddb396fc2910236c11e6a6406bd5e6aaa779e/68747470733a2f2f6261646765732e6769747465722e696d2f416e64794f62746976612f676c696d6d65722e737667" alt="Join the chat at https://gitter.im/AndyObtiva/glimmer" data-canonical-src="https://badges.gitter.im/AndyObtiva/glimmer.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feature Suggestions</h2><a id="user-content-feature-suggestions" aria-label="Permalink: Feature Suggestions" href="#feature-suggestions"></a></p>
<p dir="auto">These features have been suggested. You might see them in a future version of Glimmer. You are welcome to contribute more feature suggestions.</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/master/TODO.md">glimmer-dsl-swt/TODO.md</a></p>
<p dir="auto">Glimmer DSL Engine specific tasks are at:</p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer/blob/master/TODO.md">TODO.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Change Log</h2><a id="user-content-change-log" aria-label="Permalink: Change Log" href="#change-log"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/master/CHANGELOG.md">glimmer-dsl-swt/CHANGELOG.md</a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer/blob/master/CHANGELOG.md">CHANGELOG.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto"><strong>Contributors Wanted!</strong></p>
<p dir="auto">If you would like to contribute to Glimmer, please study up on Glimmer and <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#swt-reference">SWT</a>, run all Glimmer <a href="https://github.com/AndyObtiva/glimmer-dsl-swt#samples">samples</a>, and build a small sample app (perhaps from <a href="https://github.com/AndyObtiva/glimmer-dsl-swt/blob/master/TODO.md#samples">this TODO list</a>) to add to <a href="https://github.com/AndyObtiva/glimmer-dsl-swt">glimmer-dsl-swt</a> Hello or Elaborate samples via a Pull Request. Once done, contact me on <a href="#chat">Chat</a>.</p>
<p dir="auto">You may apply for contributing to any of these Glimmer DSL gems whether you prefer to focus on the desktop or web:</p>
<ul dir="auto">
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-swt">glimmer-dsl-swt</a>: Glimmer DSL for SWT (JRuby Desktop Development GUI Framework)</li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-opal">glimmer-dsl-opal</a>: Glimmer DSL for Opal (Pure Ruby Web GUI and Auto-Webifier of Desktop Apps)</li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-tk">glimmer-dsl-tk</a>: Glimmer DSL for Tk (MRI Ruby Desktop Development GUI Library)</li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-libui">glimmer-dsl-libui</a>: Glimmer DSL for LibUI (Prerequisite-Free Ruby Desktop Development GUI Library)</li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-xml">glimmer-dsl-xml</a>: Glimmer DSL for XML (&amp; HTML)</li>
<li><a href="https://github.com/AndyObtiva/glimmer-dsl-css">glimmer-dsl-css</a>: Glimmer DSL for CSS</li>
</ul>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer/blob/master/CONTRIBUTING.md">CONTRIBUTING.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributors</h2><a id="user-content-contributors" aria-label="Permalink: Contributors" href="#contributors"></a></p>
<ul dir="auto">
<li><a href="https://github.com/AndyObtiva">Andy Maleh</a> (Founder)</li>
<li><a href="https://github.com/Soleone">Dennis Theisen</a> (Contributor)</li>
</ul>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer/graphs/contributors">Click here to view contributor commits.</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Hire Me</h2><a id="user-content-hire-me" aria-label="Permalink: Hire Me" href="#hire-me"></a></p>
<p dir="auto">If your company would like to invest fulltime in further development of the Glimmer open-source project, <a href="https://www.linkedin.com/in/andymaleh/" rel="nofollow">hire me</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/AndyObtiva/glimmer/blob/master/LICENSE.txt">MIT</a></p>
<p dir="auto">Copyright (c) 2007-2024 - Andy Maleh.</p>
<p dir="auto">--</p>
<p dir="auto">Glimmer logo was made by <a href="https://www.flaticon.com/authors/freepik" title="Freepik" rel="nofollow">Freepik</a> from <a href="https://www.flaticon.com/" title="Flaticon" rel="nofollow"> </a><a href="http://www.flaticon.com/" rel="nofollow">www.flaticon.com</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tactility: OS for the ESP32 Microcontroller Family (125 pts)]]></title>
            <link>https://tactility.one/#/</link>
            <guid>42653811</guid>
            <pubDate>Fri, 10 Jan 2025 08:37:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tactility.one/#/">https://tactility.one/#/</a>, See on <a href="https://news.ycombinator.com/item?id=42653811">Hacker News</a></p>
Couldn't get https://tactility.one/#/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Before Squid Game, there was Battle Royale (123 pts)]]></title>
            <link>https://www.tokyoweekender.com/entertainment/movies-tv/before-squid-game-there-was-battle-royale/</link>
            <guid>42652967</guid>
            <pubDate>Fri, 10 Jan 2025 05:53:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tokyoweekender.com/entertainment/movies-tv/before-squid-game-there-was-battle-royale/">https://www.tokyoweekender.com/entertainment/movies-tv/before-squid-game-there-was-battle-royale/</a>, See on <a href="https://news.ycombinator.com/item?id=42652967">Hacker News</a></p>
Couldn't get https://www.tokyoweekender.com/entertainment/movies-tv/before-squid-game-there-was-battle-royale/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizing All ISBNs (296 pts)]]></title>
            <link>https://annas-archive.org/blog/all-isbns.html</link>
            <guid>42652577</guid>
            <pubDate>Fri, 10 Jan 2025 04:45:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://annas-archive.org/blog/all-isbns.html">https://annas-archive.org/blog/all-isbns.html</a>, See on <a href="https://news.ycombinator.com/item?id=42652577">Hacker News</a></p>
Couldn't get https://annas-archive.org/blog/all-isbns.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Gleam v1.7 (172 pts)]]></title>
            <link>https://gleam.run/news/improved-performance-and-publishing/</link>
            <guid>42652329</guid>
            <pubDate>Fri, 10 Jan 2025 04:04:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gleam.run/news/improved-performance-and-publishing/">https://gleam.run/news/improved-performance-and-publishing/</a>, See on <a href="https://news.ycombinator.com/item?id=42652329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
  <p>
    Published 05 Jan, 2025 by Louis Pilfold
  </p>
<p>Gleam is a type-safe and scalable language for the Erlang virtual machine and
JavaScript runtimes. Today Gleam <a href="https://github.com/gleam-lang/gleam/releases/tag/v1.7.0">v1.7.0</a> has been published, featuring
an array of wonderful improvements. Let’s take a look!</p>

<h2 id="faster-record-updates">Faster record updates</h2>

<p>Gleam is a language with immutable data, and it has a syntax for creating a new
record from an old one with some updated fields.</p>

<pre><code>/// Create a new version of the user with `admin` set to true.
pub fn make_admin(person: User) {
  User(..person, admin: True)
}
</code></pre>

<p>If you’re familiar with JavaScript this is similar to the object spread update
syntax, and similarly it is fast, only copying the references to the fields,
not the data itself.</p>

<p>The code that the Gleam compiler would generate would also be similar to how
JavaScript’s update works, using a small amount of dynamic code at runtime to
construct the new record with the new fields. This runtime conditional logic
had a small performance cost at runtime.</p>

<p>The compiler now instead <em>monomorphises</em> record updates, meaning it generates
exactly the most efficient code to construct the new record on a case-by-case
basis, removing the runtime conditional logic and its associated cost entirely.</p>

<p>The optimisation is for both the Erlang and the JavaScript targets, has no
additional compile speed cost or increase in code size, so it’s an improvement
in every way!</p>

<p>Another benefit of record update monomorphisation is that you can now change
the parameterised types of a generic record with the update syntax.</p>

<pre><code>pub type Named(element) {
  Named(name: String, value: element)
}

pub fn replace_value(data: Named(a), replacement: b) -&gt; Named(b) {
  Named(..data, value: replacement)
}
</code></pre>

<p>Previously this would not compile as the type parameter changed, and the
compiler wasn’t able to infer it was always done safely. Now it can tell, so
this compiles!</p>

<p>Thank you <a href="https://github.com/joshi-monster">yoshi</a> for this excellent change!</p>

<h2 id="generate-decoder-code-action">Generate decoder code action</h2>

<p>Gleam has a very robust type system, it won’t let you unsafely cast values
between different types. This results in a programming experience where the
compiler and language server can offer lots help to the programmer, especially
in unfamiliar or large codebases.</p>

<p>One drawback of this sound type system is that converting untyped input from
the outside world into data of known types requires some additional code which
would not be required in unsound systems. This decoder code can be unfamiliar
and confusing to those new to Gleam, and in simple cases it can seem a chore.</p>

<p>To aid with this the Gleam language server now includes code action to
generate a dynamic decoder for a custom type. For example, if you have this code:</p>

<pre><code>pub type Person {
  Person(name: String, age: Int)
}
</code></pre>

<p>If you place your cursor on the type header and select the code action in your
editor, then it’ll be updated to this:</p>

<pre><code>import gleam/dynamic/decode

pub type Person {
  Person(name: String, age: Int)
}

fn person_decoder() -&gt; decode.Decoder(Person) {
  use name &lt;- decode.field("name", decode.string)
  use age &lt;- decode.field("age", decode.int)
  decode.success(Person(name:, age:))
}
</code></pre>

<p>Thank you <a href="https://github.com/GearsDatapacks">Surya Rose</a>! I know this will be
a very popular addition.</p>

<h2 id="more-secure-package-manager-credential-handling">More secure package manager credential handling</h2>

<p>Gleam is part of the Erlang ecosystem, so it uses the <a href="https://hex.pm/">Hex package manager</a>.
To publish a package to Hex the build tool needs the credentials for your Hex
account, and you would type them into the command line to supply them.
We make this as secure as possible, but there’s always some risk when typing in
credentials. No amount of in-computer security can save you from someone
sitting behind you, watching your fingers on the keyboard.</p>

<p>Gleam now only asks for your Hex credentials once, and uses that to create a
long-lived API token, which will be stored on your filesystem and encrypted
using a local password of your choosing. For all future interactions with Hex
Gleam will ask for the local password, use that to decrypt the API key, and
then use it to authenticate with the Hex APIs.</p>

<p>With this if someone manages to learn the password you use to Hex they would
not be able to do anything with it unless they can also get access to your
computer and the encrypted file stored on it.</p>

<h2 id="package-namespace-checking">Package namespace checking</h2>

<p>The Erlang virtual machine has a single namespace for modules. It does not have
isolation of modules between different packages, so if two packages define
modules with the same name they can collide and cause a build failure or
other undesired behaviour.</p>

<p>To avoid this packages place their modules within their own namespace. For
example, if I am writing a package named <code>pumpkin</code> I would place my modules
within the <code>src/pumpkin/</code> directory.</p>

<p>Sometimes people from other ecosystems with per-package isolation may not
understand this convention and place all their code in the top-level namespace,
using generic names, which results in problems for any users of their package. To
avoid this the <code>gleam publish</code> command will now check for top-level namespace
pollution, explaining the problem and asking for confirmation if it is present.</p>

<p>Thank you <a href="https://github.com/guria">Aleksei Gurianov</a>!</p>

<h2 id="core-team-package-name-checking">Core team package name checking</h2>

<p>The Hex package manager system doesn’t have namespaces, so we can’t publish
packages maintained by the Gleam core team as <code>@gleam/*</code> or such. Instead Hex
users have to rely on adding a prefix to the names of their packages, and in
the Gleam core team we use the prefix <code>gleam_</code>.</p>

<p>These prefixes are unchecked, so one can use anyone else’s prefix without
issue. This is a problem for us as people occasionally publish packages using
the core team’s prefix, and then other people get confused as to why this
seemingly official package is of a low quality. To try and remedy this Gleam
will ask for confirmation when a package is published with the <code>gleam_</code> prefix.
Unfortunately this was not enough and another unofficial package was
accidentally published, so Gleam now asks for a much longer confirmation to be
typed in, to force the publisher to read the message.</p>

<h2 id="semantic-versioning-encouragement">Semantic versioning encouragement</h2>

<p>Sometimes people like to publish packages that are unfinished or unsuitable for
use by others, publishing them as version 0.*. Other people publish code that
is good to use, but shy away from semantic versioning and publish them as
v0.*. In both of these cases the users of these packages have an inferior
experience, unable to take advantage of the benefits that semantic versioning
is designed to bring, which can lead to irritating build errors.</p>

<p>Gleam will now ask for confirmation if a package is published with a v0.*
version, as it does not respect semantic versioning. The fewer zero-version
packages published the better experience users of the package ecosystem will
have.</p>

<h2 id="variant-deprecation">Variant deprecation</h2>

<p>In Gleam one can deprecate functions and types using the <code>@deprecated</code>
attribute, which causes the compiler to emit a warning if they are used. With
this release it is also possible to deprecate individual custom type variants
too!</p>

<pre><code>pub type HashAlgorithm {
  @deprecated("Please upgrade to another algorithm")
  Md5
  Sha224
  Sha512
}

pub fn hash_password(input: String) -&gt; String {
  hash(input:, algorithm: Md5) // Warning: Deprecated value used
}
</code></pre>

<p>Thank you <a href="https://github.com/wilbert-mad">Iesha</a> for this!</p>

<h2 id="canonical-documentation-links">Canonical documentation links</h2>

<p>When packages are published to Hex Gleam will also generate HTML documentation
and upload it to <a href="https://hexdocs.pm/">HexDocs</a>, the documentation hosting site
for the BEAM ecosystem.</p>

<p>Currently we have a problem where Google is returning documentation for very
old versions of Gleam libraries instead of the latest version, which can result
in confusion as people try to use functions that no longer exist, etc. To
prevent this from happening with future versions Gleam now adds a canonical
link when publishing, which should help search engines return the desired version.
In the near future we will write a tool that will update historical
documentation to add these links too.</p>

<p>Thank you <a href="https://github.com/rockerBOO">Dave Lage</a> for this improvement!</p>

<h2 id="custom-messages-for-pattern-assertions">Custom messages for pattern assertions</h2>

<p>Gleam’s <code>let assert</code> allows you to pattern match with a <em>partial pattern</em>, that
is: one that doesn’t match all possible values a type could be. When the value
does not match the program it crashes the program, which is most often used in
tests or in quick scripts or prototypes where one doesn’t care to implement
proper error handling.</p>

<p>With this version the <code>as</code> syntax can be used to add a custom error message for
the crash, which can be helpful for debugging when the unexpected does occur.</p>

<pre><code>let assert Ok(regex) = regex.compile("ab?c+") as "This regex is always valid"
</code></pre>

<p>Thank you <a href="https://github.com/GearsDatapacks">Surya Rose</a>!</p>

<h2 id="javascript-bit-array-compile-time-evaluation">JavaScript bit array compile time evaluation</h2>

<p>Gleam’s bit array literal syntax is a convenient way to build up and to pattern
match on binary data. When targeting JavaScript the compiler now generates
faster and smaller code for int values in these bit array expressions and
patterns by evaluating them at compile time where possible.</p>

<p>Thank you <a href="https://github.com/richard-viney">Richard Viney</a> for this!</p>

<h2 id="javascript-bit-array-slicing-optimisation">JavaScript bit array slicing optimisation</h2>

<p>Continuing on from his previous bit array optimisation, <a href="https://github.com/richard-viney">Richard Viney</a>
has made taking a sub-slice of a bit array a constant time operation on
JavaScript, to match the behaviour on the Erlang target. This is a significant
improvement to performance.</p>

<p>Thank you Richard! Our bit array magician!</p>

<h2 id="empty-blocks-are-valid">Empty blocks are valid!</h2>

<p>In Gleam one can write an empty function body, and it is considered a
not-yet-implemented function, emitting a warning when compiled. This is useful
for when writing new code, where you want to check some things about your
program but have not yet finished writing it entirely.</p>

<pre><code>pub fn wibble() {} // warning: unimplemented function
</code></pre>

<p>You can now also do the same for blocks, leaving them empty will result in a
compile warning but permit you to compile the rest of your code.</p>

<pre><code>pub fn wibble() {
  let x = {
     // warning: incomplete block
  }
  io.println(x)
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a>!</p>

<h2 id="external-modules-in-subdirectories">External modules in subdirectories</h2>

<p>Gleam has excellent interop with Erlang, Elixir, JavaScript, and other
languages running on its target platforms. Modules in these languages can be
added to your project and imported using Gleam’s <a href="https://tour.gleam.run/advanced-features/externals/">external functions</a>
feature.</p>

<p>Previously these external modules had to be at the top level of the <code>src/</code> or
<code>test/</code> directories, but now they can reside within subdirectories of them too.</p>

<p>Thank you <a href="https://github.com/PgBiel">PgBiel</a> for this long-awaited feature!</p>

<h2 id="installation-hints">Installation hints</h2>

<p>To run Gleam on the BEAM an Erlang installation is required, and to run it on
JavaScript a suitable runtime such as NodeJS is required. To initialise a
repository git is required. To compile Elixir code Elixir must be installed.
You get the idea- to use various external tools they need to be installed.</p>

<p>If there’s a particular recommended way to install a missing component for your
operating system the error message for its absence will now direct you to
install it that way.</p>

<pre><code>error: Shell command failed

The program `elixir` was not found. Is it installed?

You can install Elixir via homebrew: brew install elixir

Documentation for installing Elixir can be viewed here:
https://elixir-lang.org/install.html
</code></pre>

<p>Thank you <a href="https://github.com/enkerewpo">wheatfox</a> for this helpful improvement!</p>

<h2 id="faster-erlang-dependency-compilation">Faster Erlang dependency compilation</h2>

<p>You can add packages written in Erlang or Elixir to your Gleam projects, and
the Gleam build tool will compile them for you. To compile Erlang packages
rebar3, the Erlang build tool, is used.</p>

<p>Gleam now sets the <code>REBAR_SKIP_PROJECT_PLUGINS</code> environment variable
when using rebar3. With future versions of rebar3 this will cause it to skip
project plugins, significantly reducing the amount of code it’ll need to
download and compile, improving compile times.</p>

<p>Thank you to <a href="https://github.com/tsloughter">Tristan Sloughter</a> for this
contribution to both Gleam and rebar3! Elixir’s Mix build tool will also
benefit from this new rebar3 feature.</p>

<h2 id="sugar-and-desugar-use-expressions">Sugar and desugar <code>use</code> expressions</h2>

<p>Gleam’s <code>use</code> expression is a much loved and very useful bit of syntactic
sugar, good for making nested higher-order-functions easy to work with. It is
by-far Gleam’s most unusual feature, so it can take a little time to get a good
understanding of it.</p>

<p>To help with this, and to make refactoring easier, Jak has added two new code
actions to the language server, to convert to and from the <code>use</code> expression
syntax and the equivalent using the regular function call syntax.</p>

<p>Here’s the same code in each syntax, so you can get an idea of what the code
actions will convert to and from for you.</p>

<pre><code>pub fn main() {
  use profile &lt;- result.try(fetch_profile(user))
  render_welcome(user, profile)
}
</code></pre>

<pre><code>pub fn main() {
  result.try(fetch_profile(user), fn(profile) {
    render_welcome(user, profile)
  })
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for these!</p>

<h2 id="yet-more-language-server-hover-information">Yet more language server hover information</h2>

<p><a href="https://github.com/GearsDatapacks">Surya Rose</a> has been adding more hover
information to the language server. If you hover over patterns or function
labels in your editor then type and documentation information will be shown.
Thank you Surya!</p>

<h2 id="inexhaustive-let-to-case-code-action">Inexhaustive <code>let</code> to <code>case</code> code action</h2>

<p>Using a partial pattern that does not match all possible values with a <code>let</code>
binding is a compile error in Gleam.</p>

<pre><code>pub fn unwrap_result(result: Result(a, b)) -&gt; a {
  let Ok(inner) = result // error: inexhaustive
  inner
}
</code></pre>

<p>The language server now suggests a code action to convert this <code>let</code> into a
<code>case</code> expression with the missing patterns added, so you can complete the
code.</p>

<pre><code>pub fn unwrap_result(result: Result(a, b)) -&gt; a {
  let inner = case result {
    Ok(inner) -&gt; inner
    Error(_) -&gt; todo
  }
  inner
}
</code></pre>

<p>Thanks again <a href="https://github.com/GearsDatapacks">Surya Rose</a>!</p>



<p>The language server now provides an action to extract a value into a variable.
Given this code:</p>

<pre><code>pub fn main() {
  list.each(["Hello, Mike!", "Hello, Joe!"], io.println)
}
</code></pre>

<p>If you place your cursor on the list and trigger the code action in your editor
the code will be updated to this:</p>

<pre><code>pub fn main() {
  let value = ["Hello, Mike!", "Hello, Joe!"]
  list.each(value, io.println)
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for this!</p>

<h2 id="expand-function-capture-code-action">Expand function capture code action</h2>

<p>Gleam has a short-hand syntax for a function that takes a single argument and
passes it to another function, along with some other arguments. Here you can
see it being used with the <code>int.add</code> function to create a function that always
adds 11.</p>

<pre><code>pub fn main() {
  let add_eleven = int.add(_, 11)
  list.map([1, 2, 3], add_eleven)
}
</code></pre>

<p>Triggering the code action results in the function-capture being expanded to the
full anonymous function syntax:</p>

<pre><code>pub fn main() {
  list.map([1, 2, 3], fn(value) { int.add(value, 11) })
}
</code></pre>

<p>Thank you <a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a> for the
final code action of the release!</p>

<h3 id="and-the-rest">And the rest</h3>

<p>And thank you to the bug fixers and error message improvers
<a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a>,
<a href="https://github.com/ivanjermakov">Ivan Ermakov</a>,
<a href="https://github.com/Frank-III">Jiangda Wang</a>,
<a href="https://github.com/jrstrunk">John Strunk</a>,
<a href="https://github.com/PgBiel">PgBiel</a>,
<a href="https://github.com/richard-viney">Richard Viney</a>,
<a href="https://github.com/sbergen">Sakari Bergen</a>,
<a href="https://github.com/GearsDatapacks">Surya Rose</a>, and
<a href="https://github.com/joshi-monster">yoshi</a></p>

<p>For full details of the many fixes and improvements they’ve implemented see <a href="https://github.com/gleam-lang/gleam/blob/main/changelog/v1.7.md">the
changelog</a>.</p>

<h2 id="its-my-birthday-">It’s my birthday 🎁</h2>

<p>Today is my birthday! If you’d like to give me a gift please consider
<a href="https://github.com/sponsors/lpil">supporting Gleam on GitHub Sponsors</a>.</p>

<p>Gleam is not owned by a corporation; instead it is entirely supported by
sponsors, most of which contribute between $5 and $20 USD per month, and Gleam
is my sole source of income.</p>

<p><a href="https://github.com/sponsors/giacomocavalieri">Giacomo Cavalieri</a> is also
deserving of your support. He has been doing amazing work on Gleam without any
pay for nearly two years, but now he has GitHub sponsors, so show him some love!</p>

<p><a href="https://github.com/sponsors/lpil" rel="noopener" target="_blank">
  <img src="https://gleam.run/images/community/github.svg" alt="GitHub Sponsors">
</a></p>

<p>Thank you to all our sponsors, especially our top sponsor: Lambda.</p>



<ul>
  <li><a href="https://github.com/00bpa">00bpa</a></li>
  <li><a href="https://github.com/agundy">Aaron Gunderson</a></li>
  <li><a href="https://github.com/zeroows">Abdulrhman Alkhodiry</a></li>
  <li><a href="https://github.com/abeljim">Abel Jimenez</a></li>
  <li><a href="https://github.com/ad-ops">ad-ops</a></li>
  <li><a href="https://github.com/AdamBrodzinski">Adam Brodzinski</a></li>
  <li><a href="https://github.com/adjohnston">Adam Johnston</a></li>
  <li><a href="https://github.com/adam-wyluda">Adam Wyłuda</a></li>
  <li><a href="https://github.com/thebugcatcher">Adi Iyengar</a></li>
  <li><a href="https://github.com/amouat">Adrian Mouat</a></li>
  <li><a href="https://github.com/JitPackJoyride">Ajit Krishna</a></li>
  <li><a href="https://github.com/Guria">Aleksei Gurianov</a></li>
  <li><a href="https://alembic.com.au/">Alembic</a></li>
  <li><a href="https://github.com/eelmafia">Alex</a></li>
  <li><a href="https://github.com/ahouseago">Alex Houseago</a></li>
  <li><a href="https://github.com/rawhat">Alex Manning</a></li>
  <li><a href="https://github.com/aexvir">Alex Viscreanu</a></li>
  <li><a href="https://github.com/akoutmos">Alexander Koutmos</a></li>
  <li><a href="https://github.com/muonoum">Alexander Stensrud</a></li>
  <li><a href="https://github.com/defgenx">Alexandre Del Vecchio</a></li>
  <li><a href="https://github.com/Acepie">Ameen Radwan</a></li>
  <li><a href="https://github.com/abueide">Andrea Bueide</a></li>
  <li><a href="https://github.com/AndreHogberg">AndreHogberg</a></li>
  <li><a href="https://github.com/antharuu">Antharuu</a></li>
  <li><a href="https://github.com/anthony-khong">Anthony Khong</a></li>
  <li><a href="https://github.com/Illbjorn">Anthony Maxwell</a></li>
  <li><a href="https://github.com/amscotti">Anthony Scotti</a></li>
  <li><a href="https://github.com/aweagel">Arthur Weagel</a></li>
  <li><a href="https://github.com/aryairani">Arya Irani</a></li>
  <li><a href="https://github.com/azureflash">Azure Flash</a></li>
  <li><a href="https://github.com/chiroptical">Barry Moore</a></li>
  <li><a href="https://github.com/bartekgorny">Bartek Górny</a></li>
  <li><a href="https://github.com/requestben">Ben Martin</a></li>
  <li><a href="https://github.com/bgmarx">Ben Marx</a></li>
  <li><a href="https://github.com/benmyles">Ben Myles</a></li>
  <li><a href="https://github.com/bbkane">Benjamin Kane</a></li>
  <li><a href="https://github.com/bcpeinhardt">Benjamin Peinhardt</a></li>
  <li><a href="https://github.com/bentomas">Benjamin Thomas</a></li>
  <li><a href="https://github.com/bgwdotdev">bgw</a></li>
  <li><a href="https://github.com/bigtallbill">Bill Nunney</a></li>
  <li><a href="https://github.com/bjartelund">Bjarte Aarmo Lund</a></li>
  <li><a href="https://github.com/bmehder">Brad Mehder</a></li>
  <li><a href="https://github.com/brettkolodny">brettkolodny</a></li>
  <li><a href="https://github.com/brian-dawn">Brian Dawn</a></li>
  <li><a href="https://github.com/bglusman">Brian Glusman</a></li>
  <li><a href="https://github.com/bruce">Bruce Williams</a></li>
  <li><a href="https://github.com/nono">Bruno Michel</a></li>
  <li><a href="https://github.com/bucsi">bucsi</a></li>
  <li><a href="https://github.com/camray">Cam Ray</a></li>
  <li><a href="https://github.com/cameronpresley">Cameron Presley</a></li>
  <li><a href="https://github.com/carlomunguia">Carlo Munguia</a></li>
  <li><a href="https://github.com/csaltos">Carlos Saltos</a></li>
  <li><a href="https://github.com/chadselph">Chad Selph</a></li>
  <li><a href="https://github.com/ctdio">Charlie Duong</a></li>
  <li><a href="https://github.com/charlie-n01r">Charlie Govea</a></li>
  <li><a href="https://github.com/chazwatkins">Chaz Watkins</a></li>
  <li><a href="https://github.com/choonkeat">Chew Choon Keat</a></li>
  <li><a href="https://github.com/ceedon">Chris Donnelly</a></li>
  <li><a href="https://github.com/Morzaram">Chris King</a></li>
  <li><a href="https://github.com/chrislloyd">Chris Lloyd</a></li>
  <li><a href="https://github.com/utilForever">Chris Ohk</a></li>
  <li><a href="https://github.com/Chriscbr">Chris Rybicki</a></li>
  <li><a href="https://github.com/christophershirk">Christopher David Shirk</a></li>
  <li><a href="https://github.com/devries">Christopher De Vries</a></li>
  <li><a href="https://github.com/cdaringe">Christopher Dieringer</a></li>
  <li><a href="https://github.com/christopherhjung">Christopher Jung</a></li>
  <li><a href="https://github.com/christhekeele">Christopher Keele</a></li>
  <li><a href="https://github.com/specialblend">CJ Salem</a></li>
  <li><a href="https://github.com/clangley">clangley</a></li>
  <li><a href="https://github.com/CliffordAnderson">Clifford Anderson</a></li>
  <li><a href="https://github.com/codecrafters-io">CodeCrafters</a></li>
  <li><a href="https://github.com/coder">Coder</a></li>
  <li><a href="https://github.com/colelawrence">Cole Lawrence</a></li>
  <li><a href="https://github.com/insanitybit">Colin</a></li>
  <li><a href="https://github.com/Comamoca">Comamoca</a></li>
  <li><a href="https://github.com/Lucostus">Constantin (Cleo) Winkler</a></li>
  <li><a href="https://github.com/jcorentin">Corentin J.</a></li>
  <li><a href="https://github.com/ccarvalho-eng">Cristiano Carvalho</a></li>
  <li><a href="https://github.com/sdaigo">Daigo Shitara</a></li>
  <li><a href="https://github.com/dvic">Damir Vandic</a></li>
  <li><a href="https://github.com/ddresselhaus">Dan Dresselhaus</a></li>
  <li><a href="https://github.com/DanielleMaywood">Danielle Maywood</a></li>
  <li><a href="https://github.com/pinnet">Danny Arnold</a></li>
  <li><a href="https://github.com/despairblue">Danny Martini</a></li>
  <li><a href="https://github.com/davydog187">Dave Lucia</a></li>
  <li><a href="https://github.com/dbernheisel">David Bernheisel</a></li>
  <li><a href="https://github.com/davidcornu">David Cornu</a></li>
  <li><a href="https://github.com/davesnx">David Sancho</a></li>
  <li><a href="https://github.com/dangdennis">Dennis Dang</a></li>
  <li><a href="https://github.com/dennistruemper">dennistruemper</a></li>
  <li><a href="https://github.com/diemogebhardt">Diemo Gebhardt</a></li>
  <li><a href="https://github.com/dmmulroy">Dillon Mulroy</a></li>
  <li><a href="https://github.com/gothy">Dima Utkin</a></li>
  <li><a href="https://github.com/poroh">Dmitry Poroh</a></li>
  <li><a href="https://github.com/DoctorCobweb">DoctorCobweb</a></li>
  <li><a href="https://github.com/floodfx">Donnie Flood</a></li>
  <li><a href="https://github.com/ds2600">ds2600</a></li>
  <li><a href="https://github.com/ducdetronquito">ducdetronquito</a></li>
  <li><a href="https://github.com/gdcrisp">Dylan Carlson</a></li>
  <li><a href="https://github.com/edongashi">Edon Gashi</a></li>
  <li><a href="https://github.com/eeeli24">eeeli24</a></li>
  <li><a href="https://github.com/enoonan">Eileen Noonan</a></li>
  <li><a href="https://github.com/dropwhile">eli</a></li>
  <li><a href="https://github.com/Emma-Fuller">Emma</a></li>
  <li><a href="https://github.com/EMRTS">EMR Technical Solutions</a></li>
  <li><a href="https://github.com/yellowsman">Endo Shogo</a></li>
  <li><a href="https://github.com/ekosz">Eric Koslow</a></li>
  <li><a href="https://github.com/eterps">Erik Terpstra</a></li>
  <li><a href="https://liberapay.com/erikareads/">erikareads</a></li>
  <li><a href="https://github.com/ErikML">ErikML</a></li>
  <li><a href="https://github.com/erlend-axelsson">erlend-axelsson</a></li>
  <li><a href="https://github.com/oberernst">Ernesto Malave</a></li>
  <li><a href="https://github.com/EthanOlpin">Ethan Olpin</a></li>
  <li><a href="https://github.com/evaldobratti">Evaldo Bratti</a></li>
  <li><a href="https://github.com/evanj2357">Evan Johnson</a></li>
  <li><a href="https://github.com/evanasse">evanasse</a></li>
  <li><a href="https://github.com/fabridamicelli">Fabrizio Damicelli</a></li>
  <li><a href="https://github.com/fmesteban">Fede Esteban</a></li>
  <li><a href="https://github.com/yerTools">Felix Mayer</a></li>
  <li><a href="https://github.com/nandofarias">Fernando Farias</a></li>
  <li><a href="https://github.com/ffigiel">Filip Figiel</a></li>
  <li><a href="https://github.com/floriank">Florian Kraft</a></li>
  <li><a href="https://github.com/francishamel">Francis Hamel</a></li>
  <li><a href="https://github.com/Frank-III">frankwang</a></li>
  <li><a href="https://github.com/gvrooyen">G-J van Rooyen</a></li>
  <li><a href="https://github.com/gabrielvincent">Gabriel Vincent</a></li>
  <li><a href="https://github.com/janag">Ganesan Janarthanam (Jana)</a></li>
  <li><a href="https://github.com/gahjelle">Geir Arne Hjelle</a></li>
  <li><a href="https://github.com/hagenek">Georg H. Ekeberg</a></li>
  <li><a href="https://github.com/brasilikum">Georg Hartmann</a></li>
  <li><a href="https://github.com/george-grec">George</a></li>
  <li><a href="https://github.com/ggobbe">ggobbe</a></li>
  <li><a href="https://github.com/giacomocavalieri">Giacomo Cavalieri</a></li>
  <li><a href="https://github.com/giovannibonetti">Giovanni Kock Bonetti</a></li>
  <li><a href="https://github.com/obmarg">Graeme Coupar</a></li>
  <li><a href="https://github.com/grottohub">grotto</a></li>
  <li><a href="https://github.com/nirev">Guilherme de Maio</a></li>
  <li><a href="https://github.com/guillheu">Guillaume Heu</a></li>
  <li><a href="https://github.com/ghivert">Guillaume Hivert</a></li>
  <li><a href="https://github.com/hammad-r-javed">Hammad Javed</a></li>
  <li><a href="https://github.com/kwando">Hannes Nevalainen</a></li>
  <li><a href="https://github.com/ildorn">Hannes Schnaitter</a></li>
  <li><a href="https://github.com/oderwat">Hans Raaf</a></li>
  <li><a href="https://github.com/jhundman">Hayes Hundman</a></li>
  <li><a href="https://github.com/hayleigh-dot-dev">Hayleigh Thompson</a></li>
  <li><a href="https://github.com/hibachrach">Hazel Bachrach</a></li>
  <li><a href="https://github.com/hdahlheim">Henning Dahlheim</a></li>
  <li><a href="https://github.com/h14h">Henry Firth</a></li>
  <li><a href="https://github.com/henrysdev">Henry Warren</a></li>
  <li><a href="https://github.com/losfair">Heyang Zhou</a></li>
  <li><a href="https://github.com/human154">human154</a></li>
  <li><a href="https://github.com/hpiaia">Humberto Piaia</a></li>
  <li><a href="https://github.com/iainh">Iain H</a></li>
  <li><a href="https://github.com/Ian-GL">Ian González</a></li>
  <li><a href="https://github.com/ianmjones">Ian M. Jones</a></li>
  <li><a href="https://github.com/igordsm">Igor Montagner</a></li>
  <li><a href="https://github.com/irumiha">Igor Rumiha</a></li>
  <li><a href="https://github.com/nilliax">ILLIA NEGOVORA</a></li>
  <li><a href="https://github.com/intarga">Ingrid</a></li>
  <li><a href="https://github.com/inoas">inoas</a></li>
  <li><a href="https://github.com/graphiteisaac">Isaac</a></li>
  <li><a href="https://github.com/isaacharrisholt">Isaac Harris-Holt</a></li>
  <li><a href="https://github.com/imcquee">Isaac McQueen</a></li>
  <li><a href="https://github.com/ismaelga">Ismael Abreu</a></li>
  <li><a href="https://github.com/ivarvong">Ivar Vong</a></li>
  <li><a href="https://github.com/m-rinaldi">J. Rinaldi</a></li>
  <li><a href="https://github.com/jacobdalamb">Jacob Lamb</a></li>
  <li><a href="https://github.com/jakecleary">Jake Cleary</a></li>
  <li><a href="https://github.com/jamesbirtles">James Birtles</a></li>
  <li><a href="https://github.com/jamesmacaulay">James MacAulay</a></li>
  <li><a href="https://github.com/janpieper">Jan Pieper</a></li>
  <li><a href="https://github.com/monzool">Jan Skriver Sørensen</a></li>
  <li><a href="https://github.com/jlgeering">Jean-Luc Geering</a></li>
  <li><a href="https://github.com/okkdev">Jen Stehlik</a></li>
  <li><a href="https://github.com/jiangplus">jiangplus</a></li>
  <li><a href="https://github.com/hunkyjimpjorps">Jimpjorps™</a></li>
  <li><a href="https://github.com/joeykilpatrick">Joey Kilpatrick</a></li>
  <li><a href="https://github.com/joeytrapp">Joey Trapp</a></li>
  <li><a href="https://github.com/johan-st">Johan Strand</a></li>
  <li><a href="https://github.com/JohnBjrk">John Björk</a></li>
  <li><a href="https://github.com/johngallagher">John Gallagher</a></li>
  <li><a href="https://github.com/jmpavlick">John Pavlick</a></li>
  <li><a href="https://github.com/xjojorx">Jojor</a></li>
  <li><a href="https://github.com/jonlambert">Jon Lambert</a></li>
  <li><a href="https://github.com/igern">Jonas E. P</a></li>
  <li><a href="https://github.com/JonasHedEng">Jonas Hedman Engström</a></li>
  <li><a href="https://github.com/jooaf">jooaf</a></li>
  <li><a href="https://github.com/joseph-lozano">Joseph Lozano</a></li>
  <li><a href="https://github.com/joshocalico">Joshua Steele</a></li>
  <li><a href="https://liberapay.com/d2quadra/">Julian Lukwata</a></li>
  <li><a href="https://github.com/schurhammer">Julian Schurhammer</a></li>
  <li><a href="https://github.com/justinlubin">Justin Lubin</a></li>
  <li><a href="https://github.com/Neofox">Jérôme Schaeffer</a></li>
  <li><a href="https://github.com/jkbrinso">Kemp Brinson</a></li>
  <li><a href="https://github.com/keroami">Kero van Gelder</a></li>
  <li><a href="https://github.com/kevinschweikert">Kevin Schweikert</a></li>
  <li><a href="https://github.com/hamptokr">Kramer Hampton</a></li>
  <li><a href="https://github.com/Bearfinn">Kritsada Sunthornwutthikrai</a></li>
  <li><a href="https://github.com/krystofrezac">Kryštof Řezáč</a></li>
  <li><a href="https://github.com/krzysztofgb">Krzysztof G.</a></li>
  <li><a href="https://github.com/leostera">Leandro Ostera</a></li>
  <li><a href="https://github.com/leejarvis">Lee Jarvis</a></li>
  <li><a href="https://github.com/leonqadirie">Leon Qadirie</a></li>
  <li><a href="https://github.com/LeartS">Leonardo Donelli</a></li>
  <li><a href="https://github.com/defp">lidashuang</a></li>
  <li><a href="https://github.com/LighghtEeloo">LighghtEeloo</a></li>
  <li><a href="https://github.com/LilyRose2798">Lily Rose</a></li>
  <li><a href="https://github.com/wowi42">Loïc Tosser</a></li>
  <li><a href="https://github.com/lucaspellegrinelli">Lucas Pellegrinelli</a></li>
  <li><a href="https://github.com/lbjarre">Lukas Bjarre</a></li>
  <li><a href="https://github.com/lukasmeihsner">Lukas Meihsner</a></li>
  <li><a href="https://github.com/lamdor">Luke Amdor</a></li>
  <li><a href="https://github.com/2kool4idkwhat">Luna</a></li>
  <li><a href="https://github.com/manuel-rubio">Manuel Rubio</a></li>
  <li><a href="https://github.com/ideaMarcos">Marcos</a></li>
  <li><a href="https://github.com/marcusandre">marcusandre</a></li>
  <li><a href="https://github.com/AYM1607">Mariano Uvalle</a></li>
  <li><a href="https://github.com/mariuskalvo">Marius Kalvø</a></li>
  <li><a href="https://github.com/markholmes">Mark Holmes</a></li>
  <li><a href="https://github.com/markmark206">Mark Markaryan</a></li>
  <li><a href="https://github.com/datayja">Markéta Lisová</a></li>
  <li><a href="https://github.com/Janiczek">Martin Janiczek</a></li>
  <li><a href="https://github.com/rechsteiner">Martin Rechsteiner </a></li>
  <li><a href="https://github.com/martonkaufmann">martonkaufmann</a></li>
  <li><a href="https://github.com/han-tyumi">Matt Champagne</a></li>
  <li><a href="https://github.com/mhheise">Matt Heise</a></li>
  <li><a href="https://github.com/m">Matt Mullenweg</a></li>
  <li><a href="https://github.com/matthewrobinsondev">Matt Robinson</a></li>
  <li><a href="https://github.com/matt-savvy">Matt Savoia</a></li>
  <li><a href="https://github.com/mattvanhorn">Matt Van Horn</a></li>
  <li><a href="https://github.com/mwhitworth">Matthew Whitworth</a></li>
  <li><a href="https://github.com/maxmcd">Max McDonnell</a></li>
  <li><a href="https://github.com/max-tern">max-tern</a></li>
  <li><a href="https://github.com/metame">metame</a></li>
  <li><a href="https://github.com/metatexx">METATEXX GmbH</a></li>
  <li><a href="https://github.com/amiroff">Metin Emiroğlu</a></li>
  <li><a href="https://github.com/stunthamster">Michael Duffy</a></li>
  <li><a href="https://github.com/michaeljones">Michael Jones</a></li>
  <li><a href="https://github.com/monocursive">Michael Mazurczak</a></li>
  <li><a href="https://github.com/michallepicki">Michał Łępicki</a></li>
  <li><a href="https://github.com/karlsson">Mikael Karlsson</a></li>
  <li><a href="https://liberapay.com/Daybowbow/">Mike</a></li>
  <li><a href="https://github.com/mroach">Mike Roach</a></li>
  <li><a href="https://liberapay.com/mikej/">Mikey J</a></li>
  <li><a href="https://github.com/MoeDevelops">MoeDev</a></li>
  <li><a href="https://github.com/MoritzBoehme">Moritz Böhme</a></li>
  <li><a href="https://github.com/rykawamu">MzRyuKa</a></li>
  <li><a href="https://github.com/n8nio">n8n - Workflow Automation</a></li>
  <li><a href="https://github.com/natanaelsirqueira">Natanael Sirqueira</a></li>
  <li><a href="https://github.com/nathanielknight">Nathaniel Knight</a></li>
  <li><a href="https://github.com/Kuuuuuuuu">Nayuki</a></li>
  <li><a href="https://github.com/NFIBrokerage">NFIBrokerage</a></li>
  <li><a href="https://github.com/arcanemachine">Nicholas Moen</a></li>
  <li><a href="https://github.com/nchapman">Nick Chapman</a></li>
  <li><a href="https://github.com/ndreynolds">Nick Reynolds</a></li>
  <li><a href="https://github.com/NicklasXYZ">Nicklas Sindlev Andersen</a></li>
  <li><a href="https://github.com/NicoVIII">NicoVIII</a></li>
  <li><a href="https://github.com/mrniket">Niket Shah</a></li>
  <li><a href="https://github.com/ninanomenon">Ninaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa</a></li>
  <li><a href="http://www.ninefx.com/">NineFX</a></li>
  <li><a href="https://github.com/nomio">Nomio</a></li>
  <li><a href="https://github.com/oceanlewis">Ocean</a></li>
  <li><a href="https://github.com/osebelin">Olaf Sebelin</a></li>
  <li><a href="https://github.com/OldhamMade">OldhamMade</a></li>
  <li><a href="https://github.com/CanadaHonk">Oliver Medhurst</a></li>
  <li><a href="https://github.com/otosky">Oliver Tosky</a></li>
  <li><a href="https://github.com/optizio">optizio</a></li>
  <li><a href="https://github.com/daslaf">Osman Cea</a></li>
  <li><a href="https://github.com/PastMoments">PastMoments</a></li>
  <li><a href="https://github.com/Davorak">Patrick Wheeler</a></li>
  <li><a href="https://github.com/giddie">Paul Gideon Dann</a></li>
  <li><a href="https://github.com/pguse">Paul Guse</a></li>
  <li><a href="https://github.com/biernacki">Pawel Biernacki</a></li>
  <li><a href="https://github.com/Tulkdan">Pedro Correa</a></li>
  <li><a href="https://github.com/petejodo">Pete Jodo</a></li>
  <li><a href="https://github.com/pvsr">Peter Rice</a></li>
  <li><a href="https://github.com/philpax">Philpax</a></li>
  <li><a href="https://github.com/pierrot-lc">Pierrot</a></li>
  <li><a href="https://github.com/sz-piotr">Piotr Szlachciak</a></li>
  <li><a href="https://github.com/qdentity">Qdentity</a></li>
  <li><a href="https://github.com/raquentin">Race Williams</a></li>
  <li><a href="https://github.com/stoft">Rasmus</a></li>
  <li><a href="https://github.com/ray-delossantos">Ray</a></li>
  <li><a href="https://github.com/chouzar">Raúl Chouza </a></li>
  <li><a href="https://github.com/renatillas">re.natillas</a></li>
  <li><a href="https://github.com/redmar">Redmar Kerkhoff</a></li>
  <li><a href="https://github.com/reillysiemens">Reilly Tucker Siemens</a></li>
  <li><a href="https://github.com/renatomassaro">Renato Massaro</a></li>
  <li><a href="https://github.com/renovatorruler">Renovator</a></li>
  <li><a href="https://github.com/richard-viney">Richard Viney</a></li>
  <li><a href="https://github.com/rico">Rico Leuthold</a></li>
  <li><a href="https://github.com/ripta">Ripta Pasay</a></li>
  <li><a href="https://github.com/robertwayne">Rob</a></li>
  <li><a href="https://github.com/TanklesXL">Robert Attard</a></li>
  <li><a href="https://github.com/rellen">Robert Ellen</a></li>
  <li><a href="https://github.com/malkomalko">Robert Malko</a></li>
  <li><a href="https://github.com/Papipo">Rodrigo Álvarez</a></li>
  <li><a href="https://liberapay.com/Karakunai/">Ronan Harris</a></li>
  <li><a href="https://github.com/rotabull">Rotabull</a></li>
  <li><a href="https://github.com/reinefjord">Rupus Reinefjord</a></li>
  <li><a href="https://github.com/ustitc">Ruslan Ustitc</a></li>
  <li><a href="https://github.com/samaaron">Sam Aaron</a></li>
  <li><a href="https://github.com/metruzanca">Sam Zanca</a></li>
  <li><a href="https://github.com/soulsam480">sambit</a></li>
  <li><a href="https://github.com/samifouad">Sami Fouad</a></li>
  <li><a href="https://github.com/bkspace">Sammy Isseyegh</a></li>
  <li><a href="https://github.com/castletaste">Savva</a></li>
  <li><a href="https://github.com/sasa1977">Saša Jurić</a></li>
  <li><a href="https://github.com/scotttrinh">Scott Trinh</a></li>
  <li><a href="https://github.com/smweber">Scott Weber</a></li>
  <li><a href="https://github.com/scottwey">Scott Wey</a></li>
  <li><a href="https://github.com/seanjensengrey">Sean Jensen-Grey</a></li>
  <li><a href="https://github.com/SeanRoberts">Sean Roberts</a></li>
  <li><a href="https://github.com/sporto">Sebastian Porto</a></li>
  <li><a href="https://github.com/sekunho">sekun</a></li>
  <li><a href="https://github.com/tehprofessor">Seve Salazar</a></li>
  <li><a href="https://github.com/codemonkey76">Shane Poppleton</a></li>
  <li><a href="https://github.com/honsq90">Shuqian Hon</a></li>
  <li><a href="https://github.com/simonewebdesign">Simone Vittori</a></li>
  <li><a href="https://github.com/star-szr">star-szr</a></li>
  <li><a href="https://github.com/bytesource">Stefan</a></li>
  <li><a href="https://github.com/sthagen">Stefan Hagen</a></li>
  <li><a href="https://github.com/Qard">Stephen Belanger</a></li>
  <li><a href="https://github.com/stvpwrs">Steve Powers</a></li>
  <li><a href="https://github.com/Strandinator">Strandinator</a></li>
  <li><a href="https://github.com/threepointone">Sunil Pai</a></li>
  <li><a href="https://github.com/slafs">Sławomir Ehlert</a></li>
  <li><a href="https://github.com/Theosaurus-Rex">Theo Harris</a></li>
  <li><a href="https://github.com/thomaswhyyou">Thomas</a></li>
  <li><a href="https://github.com/tcoopman">Thomas Coopman</a></li>
  <li><a href="https://github.com/ernstla">Thomas Ernst</a></li>
  <li><a href="https://github.com/tmbrwn">Tim Brown</a></li>
  <li><a href="https://github.com/timgluz">Timo Sulg</a></li>
  <li><a href="https://github.com/modellurgist">Tom Calloway</a></li>
  <li><a href="https://github.com/tomjschuster">Tom Schuster</a></li>
  <li><a href="https://github.com/tomekowal">Tomasz Kowal</a></li>
  <li><a href="https://github.com/tommaisey">tommaisey</a></li>
  <li><a href="https://github.com/ThisGuyCodes">Travis Johnson</a></li>
  <li><a href="https://github.com/TristanCacqueray">Tristan de Cacqueray</a></li>
  <li><a href="https://github.com/tsloughter">Tristan Sloughter</a></li>
  <li><a href="https://github.com/tymak">tymak</a></li>
  <li><a href="https://github.com/upsidedownsweetfood">upsidedowncake</a></li>
  <li><a href="https://github.com/vvzen">Valerio Viperino</a></li>
  <li><a href="https://github.com/sandsower">Vic Valenzuela</a></li>
  <li><a href="https://github.com/rodrigues">Victor Rodrigues</a></li>
  <li><a href="https://github.com/PerpetualPossum">Viv Verner</a></li>
  <li><a href="https://github.com/yelps">Volker Rabe</a></li>
  <li><a href="https://github.com/weizhliu">Weizheng Liu</a></li>
  <li><a href="https://github.com/enkerewpo">wheatfox</a></li>
  <li><a href="https://github.com/Willyboar">Willyboar</a></li>
  <li><a href="https://github.com/wilsonsilva">Wilson Silva</a></li>
  <li><a href="https://github.com/HymanZHAN">Xucong Zhan</a></li>
  <li><a href="https://github.com/yamen">Yamen Sader</a></li>
  <li><a href="https://github.com/Yasuo-Higano">Yasuo Higano</a></li>
  <li><a href="https://github.com/joshi-monster">yoshi~ </a></li>
  <li><a href="https://github.com/Yuri2b">Yuriy Baranov</a></li>
  <li><a href="https://github.com/gasparinzsombor">Zsombor Gasparin</a></li>
  <li><a href="https://liberapay.com/~1814730/">~1814730</a></li>
  <li><a href="https://liberapay.com/~1847917/">~1847917</a></li>
  <li><a href="https://liberapay.com/~1867501/">~1867501</a></li>
  <li><a href="https://github.com/eberfreitas">Éber Freitas Dias</a></li>
</ul>

<p>Thanks for reading, I hope you have fun with Gleam! 💜</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A three month review of kagi search and the orion web browser (2024) (108 pts)]]></title>
            <link>https://flatfootfox.com/a-three-month-review-of-kagi-search-the-orion-web-browser/</link>
            <guid>42652125</guid>
            <pubDate>Fri, 10 Jan 2025 03:24:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flatfootfox.com/a-three-month-review-of-kagi-search-the-orion-web-browser/">https://flatfootfox.com/a-three-month-review-of-kagi-search-the-orion-web-browser/</a>, See on <a href="https://news.ycombinator.com/item?id=42652125">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p>There’s a new web search in town. No, it’s not a re-skin of Bing results. No, it’s not an AI powered tool chasing this particular moment of Large Language Model (LLM) hype. <a href="https://kagi.com/">Kagi</a> is an honest to goodness general purpose search engine with a simple proposal:</p><p><em>Pay us $10 USD a month and we’ll provide you with good search results.</em></p><p>A few of you just got very excited, and some others just closed this tab. Kagi is a very unusual product in 2024. The tool isn’t without its quirks, but it’s the sort of service you can easily write a few thousand words discussing. If you’re interested in technology and the web, it’s worth signing up and kicking their tires.</p><p>Kagi’s been building buzz for the last year, but I’d been dragging my heels on making the switch. It turns out the way to keep New Year’s tech resolutions is to replace your app defaults. Here’s my assorted thoughts after a January, February, and March without Google and Safari.</p><h2 id="paying-for-search">Paying For Search</h2><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiPricing.png" alt="" loading="lazy" width="1256" height="655" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiPricing.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiPricing.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiPricing.png 1256w" sizes="(min-width: 720px) 720px"><figcaption>Kagi's trial isn't time bound. You can start whenever you'd like without any extra pressure.</figcaption></figure><p>It’s hard not to talk about the state of the web as a whole when discussing search engines. The internet’s having a bit of a moment. Longstanding websites like Twitter and Reddit are making antagonistic moves against their user bases. Google and Amazon are dealing with a floor of affiliate links and search engine optimized sludge. Looming on the horizon, LLM outputs threatens to swallow the web whole.</p><p>Out of all of this comes Kagi. On the face of it, they’re <em>yet another</em> subscription looking to monetize some small silver of your daily life. If you take a step back however, some interesting things happen when you pay for search.</p><p>Kagi describes themselves as “user-centric search”. You’re paying them directly for the service they’re rendering. This has two significant and immediate effects: Kagi doesn’t need to show you ads, and their search has to be good.</p><p>Not having ads always makes for an enjoyable browsing experience. As cable TV has proven time and time again, paid services and advertising aren’t completely incompatible which one another. For the time being however Kagi is happy to be one of the increasingly few internet oases that isn’t actively trying to sell you something every time you interact with it.</p><p>This has substantial benefits on the privacy side of things. Since you’re the customer and not the product, Kagi doesn’t need to integrate extensive surveillance and tracking technologies into their tool. They don't log searches, associate searches with your accounts, and they even spell out, in plain English, the intent of the <a href="https://kagi.com/privacy">5 cookies</a> they use.</p><p>The privacy angle is nice, but <a href="https://duckduckgo.com/">Duck Duck Go</a> has also been around for a bit. They offer an anonymized ad-supported privacy-focused search. Building a successful web service at scale purely off of ad revenue has become increasingly tricky however, so Duck Duck Go’s search results are largely a proxy for Microsoft’s middling <a href="https://www.bing.com/">Bing</a> search. (Sorry Bing.)</p><p>The exciting thing about Kagi is that <em>you’re paying them for search</em>. If it’s not good, no one’s going to subscribe for a second month. The company actively pitches the aligned incentives that come from being an ad-free, paid search tool. They get their money from building a good search engine. They don’t benefit from driving traffic to sponsored links, affiliate vendors, or vertically integrated products. Kagi’s search is pretty good as a result.</p><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSearchResults.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiSearchResults.png" alt="" loading="lazy" width="1156" height="1112" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiSearchResults.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSearchResults.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiSearchResults.png 1156w" sizes="(min-width: 720px) 720px"><figcaption>Kagi's search results don't take much getting used to.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>It’s tricky to objectively judge something like the quality of search results. (Although there are some <a href="https://danluu.com/seo-spam/">good attempts to do so</a>.) When search results are good, you hardly notice them. When they’re off, it’s immediate and apparent. Not to harp on Bing again, but everyone’s had that experience of searching on a new work computer or public terminal and struggling to find what they’re looking for before realizing they’re not using Google.</p><p>Web search quality is vibes based. And the vibes are off. People have been whinging for a while now about whether or not <a href="https://www.theatlantic.com/technology/archive/2023/09/google-search-size-usefulness-decline/675409/">Google Search results are getting worse</a>. While drafting this blog post, Google themselves admitted they need to make some <a href="https://www.theverge.com/2024/3/5/24091099/google-search-high-quality-results-spam-ai-content">significant changes</a> to their search results to filter out AI generated spam and SEO reputation laundering.</p><p>All of this is to say that I’ve been satisfied with the search results Kagi provides. They “pass the vibe check” so to speak. While testing Kagi I rarely (if ever) experienced that mental whiplash of viewing “off” search results only to realize I’m “that other search engine”. Kagi’s search is good. It’s not a party trick. It’s not situational. It’s good search. You should <a href="https://kagi.com/pricing">give it a try</a>.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>When searching for a question online, Kagi does a good job of surfacing results from human-centric discussion sites like Reddit or Stack Overflow. When searching for cooking recipes, I notice a slightly more diverse set of food blogs then with Google Search. Comparison shopping or searching for information about a mainstream product does seem to fall down some of the same SEO pitfalls that Google has. This feels more like an artifact of the current state of the web then a particular search engine deficiency however.</p><p>The actual browsing experience with Kagi is pleasant. It sticks with the established Google-style search results page. It delivers links with blurbs, sub-links, and embedded rich content. If you search for something with significant video or image results, Kagi has a familiar strip of thumbnails inviting you to pivot over to one of those dedicated search views. It’s a very familiar search experience you can transition into easily. Kagi’s focus is on the search engine itself, and they’re not trying to be cute with unusual user interface innovations.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>That’s not to say Kagi doesn’t have some unique touches of its own however. Once you’ve gotten familiar with the initial search experience, there’s a lot of additional functionality you can start taking advantage of within Kagi.</p><p>On the search side, Kagi lets you adjust the ranking of pages within your personal search results. If your recipe searches are anything like mine, that might involve bumping up <a href="https://www.seriouseats.com/">SeriousEats.com</a> links. If you’ve been burned by one too many <a href="https://www.allrecipes.com/">AllRecipes.com</a> recipes, you can even go as far as hiding all pages in that domain from your search results.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>If you’re a web search power user, Kagi’s <a href="https://help.kagi.com/kagi/features/lenses.html">Lens</a> functionality may be of particular note to you. If you find yourself frequently searching for programming support for example, you can narrow down Kagi’s view of the web to just the handful of official documentation sites and support forums for the language you’re working with. It’s a bit of a brute force approach to cutting out blog spam, but you were probably scrolling down to the <a href="https://stackoverflow.com/">StackOverflow.com</a> or <a href="https://dev.to/">Dev.to</a> links anyways, right? This tool’s a bit more situational, but it’s nice seeing some thought put into how to push the search experience forward.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>There’s just something comforting about using opinionated software that doesn't feel like it has gone through eight code reviews and six departmental presentations. Kagi’s clearly built by a team who cares about search. When searching for an image, Kagi provides you with links to go to the page the image is on <em>and</em> a direct link to the image itself. No more having to right click and fiddling around for the “Open Image In New Tab” option! Fancy that. These nice touches range from quick shortcuts all the way to letting users upload a <a href="https://help.kagi.com/kagi/features/custom-css.html">custom CSS user theme</a> for the site.</p><p>If you’re hesitant to make the full time switch to Kagi, the “Bang” functionality they borrowed from Duck Duck Go is of particular note. If you prefix a search with <code>!g</code>, Kagi will instead forward that search query over to Google. <code>!r</code> sends you to Reddit’s internal search instead. <code>!yt</code> is a direct link to YouTube search, etc. Duck Duck Go has cataloged <a href="https://duckduckgo.com/bangs">thousands</a> of these integrations.</p><p>I didn’t find myself needing a Google escape hatch that often. When I did, it was mostly when searching for local businesses. Kagi has integration with Apple Maps and Yelp for finding nearby restaurants, but Google Maps remain notably strong in that one area. I use the <code>!g</code> shortcut maybe once a week on the unlimited plan, but they’re instrumental when working with Kagi’s lower priced tiers. Speaking of which;</p><h2 id="don%E2%80%99t-bother-with-kagi%E2%80%99s-5month-search-tier">Don’t Bother With Kagi’s $5/Month Search Tier</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSearches.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiSearches.png" alt="" loading="lazy" width="1060" height="677" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiSearches.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSearches.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiSearches.png 1060w" sizes="(min-width: 720px) 720px"><figcaption>If you're reading this, you use way more than 300 searches a month.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>Kagi’s updated their pricing a few times throughout the life of the product, so go <a href="https://kagi.com/pricing">check</a> and make sure this section is still relevant. Their initial offerings were focused on providing a few different metered tiers. Deciding between 300, 500, and 700 search queries a month harkened back to the days of paying for individual SMS messages. And not in a good way.</p><p>Luckily Kagi’s prices have been coming down. They now offer two main tiers: $5 USD per month for 300 searches, and $10 USD per month for unlimited searches. (There’s also a $25 USD per month tier with early access to upcoming features for super-fans.) Kagi will give you 100 free searches when you sign up that are time unconstrained. If you’re interested in seeing just a few sample search queries, feel free to open a new tab and run some test searches now. You don’t need to set aside a week to give the tool a proper test the moment you sign up.</p><p>Ten dollars per month is a <em>lot</em> to ask for search when an several ad-supported alternative exists. The five dollar tier sounds like a good compromise on paper, but I found it not worth the effort after having used it throughout January.</p><p>It’s hard to overstate just how often technology-minded folks search throughout the day. I apparently do about 45 Kagi searches a day on average. In reality this shakes out to about 30 searches on a weekday, and 60 searches on the weekend when I’m deep diving into side projects. This doesn’t include the Google searches I still perform during the day on my work computer. Just to drive this point home, I managed to perform over 600 searches on an unlimited plan during the month of March while out of the country and completely disconnected from the internet for a week.</p><p>Limiting myself to ten high quality Kagi searches a day in January actually wasn’t <em>too</em> difficult. The <code>!g</code> <a href="https://help.kagi.com/kagi/features/bangs.html">Google shortcut</a> doesn’t count against your Kagi search total. However, there’s a lot of mental overhead in deciding which search engine to use. Looking for programming help? That’s a job for Kagi. Just looking for <code>Local Pizza Brewpub Menu</code>? That’s a job for Google.</p><p>You can <em>technically </em>get through a month on the $5 plan. I ran out of searches a day and a half before next month’s auto-renewed batch of 300 searches. (Kagi doesn’t start billing you per-search. You just get an upsell notice and an option to go back to Google or Bing.) The 300 search plan unfortunately just isn’t a very pleasant experience. I’d find myself wincing any time I accidentally typed a query I already knew the result of &nbsp;like <code>Serious Eats Channa Masala</code> into Kagi’s search during my metered month.</p><p>If you want to experiment with Kagi after your 100 free searches, do yourself a favor and jump straight into the $10 USD per month plan. You’ll get a better feel for Kagi’s search results by the end of your experimentation month if you’re not frequently trying to remember to use the <code>!g</code> shortcut for menial searches throughout the day.</p><p>That’s my big takeaway for this blog post. Hopefully Kagi’s prices continue to come down to the point where they don’t need to offer a metered plan period. This critique may not be accurate anymore by the time you read it. If your spouse is also into tech, the cost may work out a bit better for you on their $14 USD per month two seat “Duo” plan.</p><h2 id="kagi-ai">Kagi &amp; AI</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/2024/04/KagiFastGPT.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiFastGPT.png" alt="" loading="lazy" width="838" height="1029" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiFastGPT.png 600w, https://flatfootfox.com/content/images/2024/04/KagiFastGPT.png 838w" sizes="(min-width: 720px) 720px"><figcaption>It's still surreal seeing myself paraphrased by an AI. For what it's worth, there's a sneaky hallucination in #5. Ergogen doesn't create finalized PCB files, and KiCAD's needed to generate the Gerber files.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>Kagi has been exploring the Large Language Model and AI tools space along with most of Silicon Valley. Their core offering is a model known as FastGPT. It’s a ChatGPT 3.5-style LLM with the ability to perform web searches. The answers it provides are citation-heavy, similar to <a href="https://copilot.microsoft.com/">Microsoft’s Bing Chat / Copilot</a>.</p><p>FastGPT is available as a standard interactive chat experience, but Kagi also exposes it a few other ways. They have a “Summarizer” web app which takes web page URLs and can provide short summaries or a bullet point list of key moments. The Summerizer tool also provides a conversational interface to further interrogate the context of a particular web page.</p><p>Finally, Kagi’s most prominent FastGPT integration is its “Quick Answers” functionality. At the top of every search result page is a “Quick Answers” button you can tap to get a brief 2-3 sentence answer to your query. The response appears inline in a new panel at the top of the search results page. As its name implies, FastGPT is designed to begin typing its response nearly instantaneously.</p><p>Quick Answers is a nice way to begin dabbling in large language models. You don’t need to remember, “Oh right, I can have a conversation with a chatbot about this question.” It’s just always right there at the top of the page if your query seems like one of those things that should be trivial for “the internet” to answer. Kagi will also save you the tap and automatically generate a Quick Answer response for some queries that it detects are in the form of a question.</p><p>Overall FastGPT responds about as well as other consumer LLMs. It does unfortunately produce occasional hallucinations depending on the query. Kagi as a company has a <a href="https://blog.kagi.com/kagi-ai-search">history</a> with AI technology, so these experiments most likely will not be flashes in the pan. Overall the company seems to be taking a balanced approach when using these tools to help users search and interrogate documents further, rather than just using them as search engine replacements. These LLM experiments aren’t as core to the product as, say, the <a href="https://arc.net/">Arc browser</a> or <a href="https://search.brave.com/">Brave Search</a>. All of the AI functionality can be disabled if you don’t want to use LLM-based tools. An unlimited number of interactions are included with Kagi’s $10 USD per month unlimited search plan, so you can think of it as a bonus functionality if you’re still in the experimental phase with AI tools and don’t want to spring for an OpenAI or Microsoft Copilot premium offering.</p><h2 id="the-orion-browser">The Orion Browser</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png" alt="" loading="lazy" width="1754" height="893" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png 1000w, https://flatfootfox.com/content/images/size/w1600/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png 1600w, https://flatfootfox.com/content/images/2024/04/Screenshot-2024-04-01-at-10.32.02-PM.png 1754w" sizes="(min-width: 720px) 720px"><figcaption>Kagi <em>may</em> have gotten a glance at Apple's homework.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>Speaking of browsers, Kagi has one! Wait wait, don’t close the tab yet! Orion’s not Chromium based!</p><p>All kidding aside, there’s been a trend as of late of companies announcing a “new” browser which is essentially a re-skin of Google Chrome leveraging the same underlying Chromium rendering engine. They’ll sometimes have new functionality which couldn’t fit within the framework of a traditional browser extension, or some novel UI layer justifying the separate product. The core experience is largely still dictated by Chromium’s foundation however.</p><p>The consolidation of rendering technologies and the risk browser monopolies propose to the open web is a bit outside of the scope of this blog post. Rest assured though, Orion’s not Chromium based.</p><p>It’s <a href="https://webkit.org/">Webkit</a> based!</p><p>Orion uses the same rendering engine Apple uses for its Safari web browser. (Incidentally, Orion is only available on macOS and iOS.) Webkit was the basis for Chromium, so this may be splitting hairs a bit. They’re important hairs though.</p><p>For those not familiar with the current state of the browser scene on macOS, there's generally two paths you can take. You can go with Apple's relatively basic but power efficient Safari web browser, or you can use a more feature rich but battery hungry web browser like <a href="https://www.google.com/chrome/">Google's Chrome</a> or <a href="https://www.mozilla.org/en-US/firefox/new/">Mozilla's Firefox</a>. (This isn’t a perfect characterization in all contexts, but it’s a good enough rubric.) I was personally a happy Safari user, but Chrome is useful for a few resource intensive web applications like Google Docs or browser-based video conferencing.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>One of Chrome’s largest selling points over Safari is its extensive 3rd party extension support. Safari technically has 3rd party extensions as well, but they’ve never quite caught on as much as the Chrome or Firefox ecosystems. Safari’s APIs aren’t as robust, and unlike its open source peers you need a paid Apple developer account to publish Safari extensions.</p><p>Put simply, Orion is Safari’s browser technology with Chrome and Firefox’s extension support grafted onto it. That’s not a metaphor. Orion’s developers have added support for approximately 70% of the WebExtensions API onto Orion. The end result is that you can use Chrome or Firefox's UBlock Origin, Bitwarden, and LastPass extensions on a Webkit-based browser. It's an impressive technical feat and it works surprisingly well.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>The rest of Orion is largely what you’d expect. There’s a few UI flourishes such as the option for vertical tabs, but they’ve largely stuck with what worked in Safari. It’s a fast snappy browser that’s not going to drain your battery life. It has all the usual password management, tab syncing, focus reading modes, and other modern touches you’d expect.</p><p>Speaking of syncing, Orion also has a mobile version for iOS. Its development started after the macOS version, and it can occasionally feel like it had less time in the oven so to speak. (It’s the only app in recent memory I’ve had actually crash the iOS Springboard.) It’s still probably the best Safari alternative I’ve tried on iOS... but Safari on iOS is the platonic ideal of mobile web browsing. Any rough edges are going to stick out like a sore thumb.</p><p>The biggest benefit of using both the macOS and iOS versions of Orion is tab syncing and Apple’s Continuity functionality. Using only the desktop app can feel a bit disjointed when suddenly your phone’s trying to open Hand Off links in Safari. Dealing with the occasional rough edge on your phone helps keep the overall experience a bit more seamless. That alone may be a hard sell, but there’s one other piece of functionality both the desktop and mobile apps have which may keep you around.</p><!--kg-card-begin: html--><!--kg-card-end: html--><p>Despite supporting the well regarded UBlock Origin, Orion also has ad blocking built in by default. This is mostly pitched from a privacy and performance perspective. Browser ads <em>do</em> have a measurable impact on battery life while on the go, and there are an absurd amount of advertisers looking to track you around the web.</p><p>Online advertising pays for the news sites you read, the social media you browse, and the videos you watch. Still, they’re obtrusive and dangerous enough that <a href="https://techcrunch.com/2022/12/22/fbi-ad-blocker/">even the US’s FBI is recommending ad blockers for your own safety</a>. If you’re part of the one third of all web citizens who use an ad blocker, Orion’s got you covered out of the box. It’s just a little odd to see a company openly brag about blocking YouTube ads and supporting Picture-In-Picture and background audio without a YouTube Red subscription.</p><p>Interestingly for a modern browser, <a href="https://orionfeedback.org/d/3882-open-source-the-browser">Orion isn’t open source</a>. This logistically makes sense given the size of the development team. There are still ways of checking some of their privacy-oriented claims (a packet sniffer can verify that it’s zero telemetry for example), but it’s worth pointing out given their otherwise privacy-focused posture. Safari isn’t fully open source either, so that may not be a sticking point for most macOS and iOS users.</p><h2 id="kagi-other-browsers">Kagi &amp; Other Browsers</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSafari.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiSafari.png" alt="" loading="lazy" width="1176" height="726" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiSafari.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiSafari.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiSafari.png 1176w" sizes="(min-width: 720px) 720px"><figcaption>Orion isn't strictly necessary to jump ship to Kagi.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>If you decide to skip Orion, you can still use <a href="https://help.kagi.com/kagi/getting-started/setting-default.html">Kagi as your default search engine</a> in other browsers. In most cases, you just need to dig into the search preferences and set <code>https://kagi.com/search?q=%s</code> as your default search provider. Chrome or Firefox will insert your query in the place of the <code>%s</code> when you enter search terms into your address bar and you’ll be all set from there.</p><p>Frustratingly, Apple didn’t get the memo on user-definable search providers. Safari is hardcoded to Google, Bing, Yahoo (Bing-powered), Duck Duck Go (powered by a variety of search sources including Bing), and Ecosia (a Bing-powered search that uses its ad revenue to plant trees). Hopefully Kagi will get added to Safari’s search engine list if it gets popular enough. Google's currently paying Apple <a href="https://www.theverge.com/2023/10/26/23933206/google-apple-search-deal-safari-18-billion">$18 billion dollars</a> to remain the default search engine in iOS, so we'll see what happens.</p><p>In the meantime, Kagi has a Safari browser extension for both the desktop and mobile clients which allow you to fake Kagi as your default search provider. The extension sees a search sent to one of the Safari default providers, parses the URL, grabs your search term, and then instantly redirects you to the Kagi search page for that term. In my limited testing it appears to perform the redirect imperceptibly fast. The extension doesn’t prevent you from visiting Google if you need to either. You can still use the <code>!g</code> override to get back to Google Search. If you do encounter flakey behavior, you can set the extension to only work on a specific search engine like Duck Duck Go or Ecosia if you run into trouble with accidental redirects. Apparently it was buggier in earlier versions, but it appears to have worked out the issues.</p><h2 id="doing-the-mental-calculus">Doing The Mental Calculus</h2><figure><img src="https://flatfootfox.com/content/images/2024/04/StarTrekHmpf.gif" alt="" loading="lazy" width="245" height="175"><figcaption>Hmpf.</figcaption></figure><p>I’m not quite sure what to make of Kagi after my first three months with it. It’s <em>really</em> good search. Is it $10 USD monthly search? That’s the tougher question.</p><p>There’s some amount of mental calculus involved when deciding to make the switch to a paid service like Kagi. I’m not personally looking for a zero tracking search service or seeking to completely de-Google my life. I care about privacy, but I still own a smart speaker or two. I’ve got an iPhone, but Google still manages my email.</p><p>I’m on the fence about spending a bit more time with Kagi as my default search engine. The gaps in local search aren’t that egregious, but in its current state it still feels like a $50 USD per year indulgence rather than a $100 USD per year splurge. Maybe try to find a friend to go halfsies on with a Duo plan?</p><p>One last factor to consider is Kagi’s recent partnership with Brave Search for use of their search index. Brave is a relatively new browser company helmed by Mozilla’s ousted CEO Brendan Eich. Since his departure from Mozilla, Eich has continued to defend his <a href="https://en.wikipedia.org/wiki/Brendan_Eich#Appointment_to_CEO_and_resignation">homophobia</a>, <a href="https://www.nytimes.com/2020/12/22/business/brave-brendan-eich-covid-19.html">spread Covid misinformation</a>, and generally introduced a slew of scammy crypto functionality within Brave.</p><p>After <a href="https://kagifeedback.org/d/2808-reconsider-your-partnership-with-brave">pushback from their userbase</a> about this partnership, Kagi’s CEO responded that the company is not yet in a position to be able to take moral stands.</p><blockquote>I understand that this has affected many of you in a negative way, creating a sense of betrayal that's against the very ethos of Kagi. I want to address this and be crystal-clear: any semblance of support for discrimination is completely against our principles. The rationale behind our choice was purely based on technological merits and business strategy, including the quality and cost-effectiveness of the service, as well as a critical need for redundancy and diversification in our data sources. The decision was treated the same as getting results from Google or Yandex (to which different groups of users in our userbase object to for various different reasons).</blockquote><blockquote>Kagi is currently not in the position to be fully independent. Searching the web is incredibly hard and Microsoft spent 20 years and billions of dollars building Bing, and it is still, let's say, suboptimal. Definitely not at the level people would pay for it. It is very hard for a small startup with many orders of magnitude less resources to crawl/index/rank the entire web and for it to be so good that people would pay for.</blockquote><blockquote>...</blockquote><blockquote>Choosing to focus on our mission to provide the best search results in the world is the only practical position we can have. This is not because we are ignorant to issues impacting societies across the globe, but because it is impractical for us to deal with them all. We cannot solve all of the world’s problems, human rights issues, conflicts and wars, but we have instead devoted our passion and energy to solving one problem that we believe is within our grasp, and that is the problem of web search, which is what Kagi is known and loved for. We intend to do that to the best of our ability.</blockquote><p>Man, this sucks.</p><p><a href="https://www.404media.co/friendship-ended-with-google-now-kagi-is-my-best-friend/">404 Media</a> also recently reported on Kagi's stance when it comes to manually manipulating search results in their service. The hypothetical discussed in the Kagi forums was whether or not the search engine should present suicide hotline information when someone searches for self harm-related topics. The company pushed back on this proposal saying they do not have any "moral, political, religious, social or any similar kind of bias" they want to explicitly code into their algorithms.</p><p>Arguing that Kagi "does not have any implicit bias built by us that is not search quality based" is a remarkable comment to hear from a CEO in 2023. Particularly when the company is eager to explore the burgeoning AI space. Everyone brings their own biases to the table when building a product. The best anyone can do is be aware of their blindspots and be open and honest about where they're coming from.</p><p>Kagi as a company likes to discuss their moral stances when there's only an upside. They frequently advertise their aligned incentives, and they bragged about going <a href="https://blog.kagi.com/celebrating-20k">"hard mode on"</a> when searching for an ethical t-shirt vendor. When it comes to more nuanced or messy topics however, Kagi's default approach appears to be hoping that not engaging with a topic is the same as not having a stance on the topic. I can't see this position being tenable forever.</p><h2 id="closing-thoughts">Closing Thoughts</h2><!--kg-card-begin: html--><a href="https://flatfootfox.com/content/images/size/w1000/2024/04/KagiHomepage.png"><!--kg-card-end: html--><figure><img src="https://flatfootfox.com/content/images/2024/04/KagiHomepage.png" alt="" loading="lazy" width="1293" height="878" srcset="https://flatfootfox.com/content/images/size/w600/2024/04/KagiHomepage.png 600w, https://flatfootfox.com/content/images/size/w1000/2024/04/KagiHomepage.png 1000w, https://flatfootfox.com/content/images/2024/04/KagiHomepage.png 1293w" sizes="(min-width: 720px) 720px"><figcaption>Everything else aside, Kagi's mascot is adorable.</figcaption></figure><!--kg-card-begin: html--></a><!--kg-card-end: html--><p>I’ve been dragging my heels wrapping up this blog post. Originally this was supposed to be a <em>two month</em> evaluation of Kagi. I needed a bit more time to get screenshots for this blog post though, so I decided to give myself an extra month to gather my thoughts on the tool. </p><p>I continue to be impressed by the search experience on Kagi. The results are solid, and the search page is pleasant to browse. Throughout the month I kept stumbling on nice touches. I recently discovered a website I was searching for was down, only to be delighted to find an <a href="https://archive.org/">Archive.org</a> link in the result’s kebab menu. These folks know how people interact with the web and want to make a great search experience.</p><p>The one thing I struggle with Kagi is to identify who its core user base is. For all the talk of Kagi’s aligned incentives with their customers, I can’t shake the feeling that I don’t know who they’re targeting. Duck Duck Go currently handles some amount of general privacy or Google-specific search concerns. So... Tech enthusiasts who would pay for a premium search experience, but aren’t turned off by LLM integrations? Orion is a privacy focused browser for people with strong opinions about Chrome’s Manifest v3 API transition who also don’t mind closed sourced tools?</p><p>There’s an unusual tension running throughout Kagi. They’ve had to hedge their bets throughout the development of the product. Maybe we’re just in a particularly compromising moment in tech. But that pragmatism can cut both ways.</p><p>After this three month experiment, I'm pausing my Kagi subscription. I might hang on to the Orion browser for a little bit longer. $10 USD is a lot to pay per month for search. That's more than the entry tier for most streaming services. I could see myself hopping back onboard once Kagi gets their prices down to $5 USD per month.</p><p>It'll be a pragmatic technology decision though. Tool X has a better experience than Tool Y justifying Z Cost.</p><p>Kagi is the type of service that would really benefit from a clearly defined user base who pays for the service not just because of the functionality of the tool, but because it's the type of search they'd like to see in the world. I'm a sucker for that type of narrative. I’m interested in any company that foregoes traditional venture capital funding and attempts to align their incentives with their customers. Kagi truly does appear to be trying their best, but I feel they've already had a few moments that peel back that early adoptor zeal.</p><p>Maybe it'll resonate more with you. Kagi's technology is good enough that it should be on your radar. So my final takeaway is: <a href="https://kagi.com/">Try it!</a> Kagi gives you a hundred free searches to kick its tires. See how things go. If you do want to give it a proper try after that, please just do yourself a favor and jump straight into the unlimited plan.</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[41% of Employers Worldwide Say They'll Reduce Staff by 2030 Due to AI (125 pts)]]></title>
            <link>https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131</link>
            <guid>42652076</guid>
            <pubDate>Fri, 10 Jan 2025 03:12:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131">https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131</a>, See on <a href="https://news.ycombinator.com/item?id=42652076">Hacker News</a></p>
Couldn't get https://gizmodo.com/41-of-employers-worldwide-say-theyll-reduce-staff-by-2030-due-to-ai-2000548131: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[TikTok tells staff impacted by wildfires to use sick hours if they can't work (174 pts)]]></title>
            <link>https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/</link>
            <guid>42652056</guid>
            <pubDate>Fri, 10 Jan 2025 03:08:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/">https://techcrunch.com/2025/01/09/tiktok-tells-la-staff-impacted-by-wildfires-to-use-personal-sick-hours-if-they-cant-work-from-home/</a>, See on <a href="https://news.ycombinator.com/item?id=42652056">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Wildfires are currently <a href="https://www.cbsnews.com/live-updates/california-windstorm-fuels-pacific-palisades-wildfire-as-residents-flee-live-updates/" target="_blank" rel="noreferrer noopener nofollow">devastating the greater Los Angeles area</a>, burning over 45 square miles, torching over 1,300 structures, and putting nearly 180,000 people under evacuation orders as of Thursday. And yet, TikTok’s LA-based employees are being told to either continue their work from home or use their personal/sick days if that’s not possible, while the company’s LA office remains closed due to power outages caused by high winds.</p>

<p>Already, the Palisades Fire is close enough to TikTok’s office that smoke can be seen out the windows. But although the Culver City-based office itself is <a href="https://www.culvercity.org/News/LA-Wildfire-Update" target="_blank" rel="noreferrer noopener nofollow">not under mandatory evacuation orders</a> at this time, both it and many employees’ homes are impacted by the fires, windstorms, and related problems.</p>







<p>TikTok’s employees in the region hail from all over the broader LA area — some even commuting hours into work — and many of their homes are currently without power, Wi-Fi, or both, we understand from employee sources at TikTok. Some could even be under evacuation orders of their own (though we have not directly confirmed this at this time).</p>

<p>Unfortunately for staff dealing with this large-scale natural disaster, TikTok is telling them to use their personal or sick time to account for the days they need to take off due to these conditions.</p>

<p>In messages from TikTok leadership to LA staff, the company informed employees that the LA office would be closed on January 8 and would remain closed through Sunday, January 12, as the fires continued to ravage the area and the office itself is without power. The days the office is closed are being made Work From Home days as opposed to days off, however — unless an individual team leader decides otherwise. </p>

<p>In one message, an HR representative shared links to other company resources for those impacted by the fires, including a Mental Wellbeing Portal, a way to sign up for free mental health sessions with <a href="https://www.lyrahealth.com/" target="_blank" rel="noreferrer noopener nofollow">Lyra</a>, and a link to TikTok’s “PSSL” policy. The latter refers to TikTok’s paid sick and safe leave program — essentially, sick time and personal days.</p>

<p>TikTok’s LA employees have 10 paid sick/personal (PSSL) days per year in addition to 15 PTO (paid time off/vacation) days, if they were hired before June. These sick/personal days are highly coveted, too, as TikTok’s strict return-to-office policy requires employees to work from the office a minimum of three days per week. (The days of the week are chosen by the team and can’t be swapped for other days if needed.)</p>


<p>That means if an employee is feeling unwell, like with a simple cold or flu, and they don’t want to spread their illness to coworkers, they do have the option to stay home. But because they’re required to be in the office for three days each week, they would still have to use their PSSL hours and take the day off on those working-from-home-while-sick days (rather than being allowed to work from home with no penalty).</p>

<p>This week, TikTok’s LA staff are being asked to use their personal/sick days if&nbsp;they cannot work from home due to power or Wi-Fi outages, or if they’re under evacuation orders (unless their entire team has been given time off, which is not the case for many impacted by the fires). This leaves them fewer days later in the year to use in case of an actual illness or other personal emergency, like staying home to care for a sick child. If they don’t have enough PSSL hours available, they can either borrow from next year or use their PTO time instead, we understand.</p>

<p>Employees who can work from home still must go into their “My RTO” portal, where they manage their sick time, and change their work-from-home status to “natural disaster” to not be penalized. This won’t subtract from their PSSL hours, though. </p>







<p>Meanwhile, TikTok’s PSSL policy documentation doesn’t specifically state that the time can be used for natural disasters, such as these massive wildfires.</p>

<p>Instead, the policy says employees can use the time for either a physical or mental health condition, to take care of a family member with a health condition, or if the office is closed by the “order of public officials” due to a public health emergency, including exposures to an infectious agent, biological toxin, or hazardous material. (While, arguably, smoke in the area could be “hazardous,” not every TikTok LA employee facing poor air quality is also under an evacuation order enacted by a public official.)</p>

<p>In several internal messages shared with us, employees are reporting their home has no power, or their city overall has no power. (News reports indicate that some <a href="https://ktla.com/news/california/wildfires/millions-without-power-in-southern-california-map-shows-latest-outages/" target="_blank" rel="noreferrer noopener nofollow">4 million people are without power</a> due to the wildfires as of yesterday.) Some employees are worried about how bad their air quality is getting. Others are worried about using up their precious battery power or generator fuel just to work at home, as it’s unclear how long these power outages will last.</p>

<p>Given the pressure TikTok is under due to the <a href="https://techcrunch.com/2024/12/28/trump-asks-supreme-court-to-pause-imminent-tiktok-ban/">upcoming ban in the U.S.</a>, which is probably already impacting U.S. employees’ mental health and stress levels, being told to keep working through a disaster of this scale comes across as a little tone-deaf. In fact, some internal messages reviewed by TechCrunch have very much a “business-as-usual” vibe to them despite the scale of the disaster at hand. One lead, for example, reached out to an employee without power for a status update on some of their work, messages show. </p>

<p>Employees have been told to contact the EAP (Employee Assistance Program) or their HR rep if they are told they need to evacuate. Though there are many messages from leaders stressing that employees should put their own safety and well-being first, asking staff to worry about using personal days if they can’t work from home seems to counter that narrative.</p>

<p>TikTok was asked for comment but didn’t offer a response ahead of publication. After publication, the company issued a statement, shared with TechCrunch. (See below). </p>

<p>TikTok claims that any communications to LA employees telling them to use personal time if they can’t work from home due to fires, power outages or internet issues must be a misunderstanding. (We should note that we’ve seen screenshots of TikTok HR’s communications to staff that contradict these claims. Additionally, after the story was published, TikTok enabled a feature that now alerts everyone in a company-wide Lark channel — <a rel="nofollow" href="https://en.wikipedia.org/wiki/Lark_(software)">a Slack competitor from TikTok parent ByteDance</a> — when a screenshot is taken.)</p>

<p>“The safety and well-being of our employees is our highest priority,” a TikTok spokesperson said. “In light of current circumstances, our offices have been closed since Tuesday and will remain so for as long as necessary. While employees who can work from home safely are encouraged to do so, we also recognize the unique challenges this situation may present and are committed to supporting our team with flexibility if they are unable to work remotely at this time.”</p>







<p><em>Sarah Perez can be reached via email at sarahp@techcrunch.com or @sarahperez.01 on Signal.</em> <em>Note that this article was updated after publication with TikTok’s statement.</em></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Predictions Scorecard, 2025 January 01 (226 pts)]]></title>
            <link>https://rodneybrooks.com/predictions-scorecard-2025-january-01/</link>
            <guid>42651275</guid>
            <pubDate>Fri, 10 Jan 2025 00:27:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rodneybrooks.com/predictions-scorecard-2025-january-01/">https://rodneybrooks.com/predictions-scorecard-2025-january-01/</a>, See on <a href="https://news.ycombinator.com/item?id=42651275">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-inner">
		<main id="main" role="main">
			<!-- .page-header -->
		

<article id="post-1698">
	
	<!-- .entry-header -->

	<div>
		<p>[You can follow me on social media: @rodneyabrooks.bsky.social and see my publications etc., at <a href="https://people.csail.mit.edu/brooks" target="_blank" rel="noopener">https://people.csail.mit.edu/brooks</a>]</p>
<p>This is my seventh annual update on how <a href="https://rodneybrooks.com/my-dated-predictions/" target="_blank" rel="noopener">my dated predictions</a> from January 1<sup>st</sup>, 2018&nbsp;concerning (1) <em>self driving cars</em>, (2) <em>robotics, AI , and machine learning</em>, and (3) <em>human space travel</em>, have held up. I promised then to review them at the start of the year every year until 2050 (right after my 95<sup>th</sup> birthday), thirty two years in total. The idea is to hold myself accountable for those predictions. How right or wrong was I?</p>
<p>I have decided to change my rules for myself a little bit after this year, in response to the many many people who have said how much they enjoy seeing my updates.</p>
<p>My predictions were mostly for the first few years, and by next year the density of due dates will be very low. &nbsp;So, on the eight anniversary of my first set of predictions, i.e., a year from today, I will be making a new set of predictions centered on the period January&nbsp;1<sup>st&nbsp;</sup>2026 to January&nbsp;1<sup>st</sup>&nbsp;2036, and that will give a new density of predictions where there will be real meat to see how accurately they turned out.</p>
<p><span><em>What I Want to Achieve and a Changing Hype-driven Landscape</em></span></p>
<p>The level of hype about AI, Machine Learning and Robotics completely distorts people’s understanding of reality. It distorts where VC money goes, always to something that promises impossibly large payoffs–it seems it is better to have an untested idea that would have an enormous payoff than a tested idea which can get to a sustainable business, but does not change the world for ever. It distorts what young researchers work on as they do not want to be seen as old fashioned even when the current hyped topic is sort of dumb–soon the dumbness is forgotten and the heat of the chase becomes all. It distorts what people think they need to get a degree in at college in order to have good career prospects.</p>
<p>I want people to use rational thought processes when they hear about hyped ideas and be able to assess what is really going on, and what is just plain (to use the technical term) bullshit.</p>
<p><span><em>My Color Scheme and Past Analysis</em></span></p>
<p>The acronyms I used for predictions in my original post were as follows.</p>
<p><strong>NET <em>year</em></strong> means it will not happen before that year (No Earlier Than)<br>
<strong>BY <em>year</em></strong> means I predict that it will happen by that year.<br>
<strong>NIML</strong>, Not In My Lifetime, i.e., not before 2050.</p>
<p>As time passes mentioned years I color then as <span>accurate</span>, <span>too pessimistic</span>, or&nbsp;<span>too optimistic</span>.</p>
<p>This year I have added <span>hemming and hawing</span>. This is for when something looks just like what I said would take a lot longer has happened, but the underlying achievement is not what everyone expected, and is not what was delivered. This is mostly for things that were talked about as being likely to happen with no human intervention and it now appears to happen that way, but in reality there are humans in the loop that the companies never disclose. So the technology that was promised to be delivered hasn’t actually been delivered but everyone thinks it has been.</p>
<p>I have not changed any of the text of the first three columns of the prediction tables since their publication on the first day of 2018. I only change the text in the fourth column to say what actually happened. &nbsp;This meant that by two years ago that fourth column was getting very long and skinny, so I removed them and started with fresh comments last year. I have kept last year’s comments and added new ones, with yellow backgrounds, for this year. If you want to see the previous five years of comments you can go back to&nbsp;&nbsp;<a href="https://rodneybrooks.com/predictions-scorecard-2023-january-01/" target="_blank" rel="noopener">the 2023 scorecard</a>.</p>
<h5>Overview of changes this year</h5>
<p>There has been a lot of activity in both self driving cars (the demise of Cruise a big push by Waymo to scale human assisted deployments, and lots of smoke and mirrors from an electric car company) and in AI, where robotics has been pulled into the ultra hyposphere while in generative AI the end of scaling and the introduction of inference mechanisms (!!) have been hotly announced and disputed. &nbsp;The human spaceflight endeavor, as it did last year, has crawled along and again has stretched out dates that were probably too optimistic in the first place.</p>
<h2><span>But First.</span></h2>
<p><span>&lt;rant&gt;</span></p>
<p>We all know about FOMO, Fear Of Missing Out. In late 2023, for a talk on generative AI that I gave at MIT,&nbsp;<a href="https://www.youtube.com/watch?v=pgrzEHJTPPM" target="_blank" rel="noopener">I coined another acronym</a>, &nbsp;FOBAWTPALSL, Fear Of Being A Wimpy Techno-Pessimist And Looking Stupid Later. Perhaps that one is a little bit too much of a mouthful to catch on. These two human insecurities lead people to herd-like behavior in establishing and propagating the zeitgeist on almost any topic.</p>
<p>They lead to people piling on the hype fiestas, rushing to invest (money, effort, or hope) in marginal ideas once they have become a little bit popular, or <a href="https://www.nytimes.com/2024/12/24/nyregion/new-jersey-new-york-drones.html" target="_blank" rel="noopener">believing our airspace is being invaded by foreign drones</a>.</p>
<p>“Mounting evidence, <a title="" href="https://www.nytimes.com/2024/12/19/video/new-jersey-drones-planes-videos.html" target="_blank" rel="noopener">and lack thereof</a>, suggests that perhaps the whole craze has been a sort of communal fever dream fueled by crowd mentality, confirmation bias and a general distrust in all things official.”</p>
<p>That quote is from the drone story linked to above, but it could well as been about the hype that we are moving towards AGI (Artificial General Intelligence).</p>
<p><span>I want to be clear, as there has been for almost seventy years now, there has been significant progress in Artificial Intelligence over the last decade. There are new tools and they are being applied widely in science and technology, and are changing the way we think about ourselves, and how to make further progress.</span></p>
<p>That being said, we are not on the verge of replacing and eliminating humans in either white collar jobs or blue collar jobs. Their tasks may shift in both styles of jobs, but the jobs are not going away. We are not on the verge of a revolution in medicine and the role of human doctors. We are not on the verge of the elimination of coding as a job. We are not on the verge of replacing humans with humanoid robots to do jobs that involve physical interactions in the world. We are not on the verge of replacing human automobile and truck drivers world wide. We are not on the verge of replacing scientists with AI programs.</p>
<p>Breathless predictions such as these have happened for seven decades in a row, and each time people have thought the end is in sight and that it is all over for humans, that we have figured out the secrets of intelligence and it will all just scale. &nbsp;The only difference this time is that these expectations have leaked out into the world at large. I’ll analyze why this continues to happen below in the section on AI and ML.</p>
<p>Here is a list of some of those hype cycles that I, personally, have perceived and lived through, as taken from my presentation at MIT in late 2023 that I referenced above re FOBAWTPALSL.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/AIhypecycles.jpg" alt="" width="2842" height="1590"></p>
<p>Really, was there really hype about all these things? &nbsp;Yes, there was, within the circles that cared. Those circles have gotten wider and wider and when reigning world chess champion Garry Kasparov was beaten by I.B.M.’s Deep Blue computer under tournament conditions in 1997 it was widely reported in the popular press, And it was declared that it was all over for humans.</p>
<p>Back in February 2011 a computer program named Watson <a href="https://www.pbs.org/wgbh/nova/article/watson-and-jeopardy/" target="_blank" rel="noopener">played on the television game show Jeopardy against all time human champions</a>. John Markoff, legendary technology reporter at the New York Times, wrote stories about this <a href="https://www.nytimes.com/2011/02/15/science/15essay.html" target="_blank" rel="noopener">the day before</a> the competition, and <a href="https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html" target="_blank" rel="noopener">the day after</a>, when Watson had indeed beaten the humans, with the same questions (fed as text to it as the same time as the humans heard the questions) all running on a cluster of machines not connected to an outside network. Here are three successive paragraphs from the second of those stories.</p>
<p><span><span>For I.B.M., the future will happen very quickly, company executives said. On Thursday it plans to announce that it will collaborate with Columbia University and the University of Maryland to create a physician’s assistant service that will allow doctors to query a cybernetic assistant. The company also plans to work with Nuance Communications Inc. to add voice recognition to the physician’s assistant, possibly making the service available in as little as 18 months.</span></span></p>
<p><span>“I have been in medical education for 40 years and we’re still a very memory-based curriculum,” said Dr. Herbert Chase, a professor of clinical medicine at Columbia University who is working with I.B.M. on the physician’s assistant. “The power of Watson- like tools will cause us to reconsider what it is we want students to do.”</span></p>
<p><span>I.B.M. executives also said they are in discussions with a major consumer electronics retailer to develop a version of Watson, named after I.B.M.’s founder, Thomas J. Watson, that would be able to interact with consumers on a variety of subjects like buying decisions and technical support.</span></p>
<p>My personal experience at that time was people I did not know, but who had heard about my role at MIT (as director of the MIT AI Lab, and then founding director of MIT CSAIL, the Computer Science and Artificial Intelligence Lab) would come up to me and ask about the future of medicine. The people were variously doctors or health industry executives. I reassured them that medicine as we knew it then would stay much the same and was not about to be rendered obsolete.</p>
<p>And then in 2016 Geoff Hinton, one of the key architects of Deep Learning (which has had undeniable impact on the world) said:</p>
<p>“People should stop training radiologists now. It is just completely obvious that within five years deep learning is going to be better than radiologists.”</p>
<p>More people asking me whether this was true. It wasn’t in five years and it isn’t now. We need more radiologists than ever. And yes they do use deep learning tools to help them see some things they wouldn’t otherwise see. But they also understand anomalies using causal reasoning and we would be in a sorry state if all radiology was done by programs today.</p>
<p>Now look at those plum colored paragraphs above again as you take yourself way back in time to a year or so ago when ChatGPT was just a baby AGI, You can find stories just like this one if you substitute “ChatGPT” for “Watson” and “Microsoft” for “I.B.M.”</p>
<p>The things confidently predicted in 2011 (and in 1979, and in 2016) about the end of doctors didn’t happen then and it is not happening now. Nor are all the other jobs ending.</p>
<p>Today I get asked about humanoid robots taking away people’s jobs. In March 2023 I was at a cocktail party and there was a humanoid robot behind the bar making jokes with people and shakily (in a bad way) mixing drinks. A waiter was standing about 20 feet away silently staring at the robot with mouth hanging open. I went over and told her it was tele-operated. “Thank God” she said. (And I didn’t need to explain what “tele-operated” meant). Humanoids are not going to be taking away jobs anytime soon (and by that I mean not for decades).</p>
<p>You, you people!, are all making fundamental errors in understanding the technologies and where their boundaries lie. Many of them will be useful technologies but their imagined capabilities are just not going to come about in the time frames the majority of the technology and prognosticator class, deeply driven by FOBAWTPALSL, think.</p>
<p>But this time it is different you say. This time it is really going to happen. You just don’t understand how powerful AI is now, you say. All the early predictions were clearly wrong and premature as the AI programs were clearly not as good as now and we had much less computation back then. This time it is all different and it is for sure now.</p>
<p>Yeah, well, I’ve got a <a href="https://en.wikipedia.org/wiki/Predictions_and_claims_for_the_Second_Coming" target="_blank" rel="noopener">Second Coming</a> to sell you…</p>
<p><span>&lt;/rant&gt;</span></p>
<h5>Self Driving Cars</h5>
<p>As with <em>flying cars</em> the definition, or common understanding, of what <em>self driving cars</em>&nbsp;really means has changed since my post on predictions seven years ago. &nbsp;At that time self driving cars meant that the cars would drive themselves to wherever they were told to go with no further human control inputs.</p>
<p>Now self driving cars means that there is no one in the drivers seat, but there may well be, and in all cases so far deployed, <a href="https://www.nytimes.com/2024/09/11/insider/when-self-driving-cars-dont-actually-drive-themselves.html" target="_blank" rel="noopener">humans monitoring those cars from a remote location</a>, and occasionally sending control inputs to the cars. The companies do not advertise this feature out loud too much, but they do acknowledge it, and the reports are that it happens somewhere between every one to two miles traveled. These inputs are not direct control of the normal human mechanism of control the steering wheel, the brakes, and the accelerator. &nbsp;Rather they are advice that overrides some of the algorithms. &nbsp;For instance, “steer out into the next lane and go around this truck” as the human realizes that the truck is just not going to move (see an anecdote below on the first night I took the new Waymo taxis in San Francisco (I had previously last ridden a Waymo in 2012 in Mountain View)).</p>
<p>Why is this difference important? &nbsp;One of the motivations for self driving cars was that the economics of taxis, cars that people hire at any time for a short ride of a few miles from where they are to somewhere else of their choosing, would be radically different as there would be no driver. Systems which do require remote operations assistance to get full reliability cut into that economic advantage and have a higher burden on their ROI calculations to make a business case for their adoption and therefore their time horizon to scaling across geographies.</p>
<p>But wait, you might say, isn’t that electric car company that used to be based in California and is now based in Texas going to roll this out imminently and have a fully digital taxi service. They demoed it on a Hollywood movie studio lot just this year, and the cars were painted gold. Hmm. The location of the demo and the fact that the cars, even down to the tires, were painted gold tells you everything you need to know. Both the cars and the humanoid robots at that event were presented as autonomous but in reality they were all tele-operated directly by people (see below in the humanoid section for more details). And <a href="https://gizmodo.com/tesla-is-looking-to-hire-a-team-to-remotely-control-its-self-driving-robotaxis-2000530600" target="_blank" rel="noopener">that same electric car company is actively hiring people into paying jobs as remote operators</a>.</p>
<p>There was a reasonably balanced appraisal from <a href="https://www.reuters.com/technology/teslas-musk-unveil-robotaxis-amid-fanfare-skepticism-2024-10-10/" target="_blank" rel="noopener">Reuters</a> just after the event, though it does not go into details of the demos. Here is a direct quote from the story:</p>
<p>“We do expect to start fully autonomous unsupervised FSD in Texas and California next year.” Musk said.</p>
<p>The astute reader will note that this is the 11<sup>th</sup> year in a row that the CEO of Tesla has made this prediction of the same milestone happening the next year. We can admire the consistency.</p>
<p><em>Actual</em> <a href="https://www.slashgear.com/1605007/autonomous-driving-plateau-engineer-expert-explains/" target="_blank" rel="noopener">self-driving is now generally accepted to be much harder than every one believed</a>.</p>
<p>The reason that this bait and switch is important to understand is that the promise of inevitable fully self driving technology upended a historical way that new transportation systems have been adopted.</p>
<p>In the past whenever we have introduced new transportation mechanisms there have been large investments in infrastructure and that infrastructure is shared and used by everyone. The Romans built roads so soldiers and traded goods could travel long distances–in Europe those road networks are still the basis of today’s road networks. When steam engine driven trains were the new transportation technology vast networks of rails were built allowing goods to move long distances in mere hours or days. When Ford started mass production of automobiles he built roads and the local governments followed and the the Federal government followed, and those roads are what we use today.</p>
<p>Actual fully self driving cars promised that no infrastructure changes would be needed to revolutionize how vehicles would be controlled. Each individual vehicle would do what was needed all by itself. As sensors and networks got better there was no need for expensive new infrastructure because of this promise.</p>
<p>The promise was false. If government and private partnerships in building smart roads, which was a hot topic in the 1990s. had continued, every one of us would now have smarter safer cars, but still with onboard human drivers taking over in many situations. But we would have had smart freeways where once you were on it your car would be self driving. The road would have had lots of sensors effectively shared across all cars, as that data would have been transmitted to all passing cars. It would have been a fraction of the cost per car compared to the sensing on today’s almost but not really self driving cars like those of Waymo. And we would have had much more accurate congestion data where the root causes of local congestion would have been sensed with semantic understanding rather than just inferring it from the aggregate collection of location data from phones, individual cars, and historical data from roadside sensors.</p>
<p>Instead we now have individual corporate actors using a mixture of partial self driving and remote human supervision. The big question is whether the economics of this works at scale, and whether the fake promises will drive out the human drivers in cheaper services and we’ll all end up paying more. Will the level of hype we saw push our decentralized transportation system into the hands of a few wealthy companies, and in effect make it a centralized system where everybody has to pay private companies to be part of it?</p>
<p>As a reminder of how strong the hype was and the certainty of promises that it was just around the corner here is a snapshot of a whole bunch of predictions by major executives from 2017.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Screenshot-2024-12-21-at-4.02.30%E2%80%AFPM.png" alt="" width="2470" height="1280"></p>
<p>I have shown this many times before but there is one new annotation here for 2024. The years in parentheses are when the predictions were made. The years in blue are the years are the predicted years of achievement. When a blue year is shaded pink it means that it did not come to pass by then. The predictions with orange arrows are those that I had noticed had later been retracted.</p>
<p>The prediction that Jaguar and Land-Rover made that they would have fully autonomous cars by 2024 did not come to pass, so I have shaded it pink,</p>
<p>Note that every single blue year up until now is shaded pink, and that every one that is shaded pink has still not come to pass. None of the predictions that were out there in 2017 for the next few years have happened. &nbsp;None. There are three more for 2025, and I am sure that a year from now they will all be shaded pink also.</p>
<p>One of the big selling points of self driving cars was that they would be safer than cars driven by humans. So far that is not holding up with real data. One electric car maker with self driving software had it disengage when it sensed there would be an accident, supposedly so that the human could take over in a split second. And then the company did not report the incident as the fault of the software as it was no longer controlling the car when the impact occurred. It was reported, and I had this experience myself in my last ride in a Cruise in 2023, that Cruise vehicles would freeze when an accident looked likely, and then not report it as their software’s fault as the car was stationary and was hit by another car. In many reported cases, and in my case, simply continuing to move forward would avert any likely accident (fortunately for me the human driver of the other car slammed on the brakes and did not hit my robot vehicle).</p>
<p>In&nbsp;<a href="https://www.washingtonpost.com/business/2024/05/24/all-major-robotaxi-firms-are-facing-federal-safety-investigations/" target="_blank" rel="noopener">this story from the Washington Post</a>&nbsp;about Federal investigations into the safety incidents with self driving cars, they report that the companies involved claim they have vast amounts of driving on our roads under their belt. Not so.</p>
<p>An industry association says autonomous vehicles have logged a total of 70 million miles, a figure that it compares to 293 trips to the moon and back. But it’s a tiny fraction of the almost 9 billion miles that Americans drive every day. The relatively small number of miles the vehicles have driven makes it difficult to draw broad conclusions about their safety.</p>
<p>To put that into perspective, the total number of miles driven by all autonomous (sort of) vehicles over the last decade is less than 1% of the miles driven by humans every day in the United States. It is a tiny, tiny portion.</p>
<p>Take a look at this embedded video from the Wall Street Journal about investigations of crashes (many of which have been fatal) involving autonomous driving systems.</p>
<p><iframe width="660" height="371" src="https://www.youtube.com/embed/mPUGh0qAqWA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="The Hidden Autopilot Data That Reveals Why Teslas Crash | WSJ"></iframe></p>
<p>From the audio:&nbsp;“The kinds of things that tend to go wrong with these systems are things like it was not trained on, pictures of an overturned double trailer. It just didn’t know what it was. There were some lights there, but the lights were in unusual positions. A person would have clearly said something big is in the middle of the road. But the way machine learning works is it trains it on a bunch of examples and if it encounters something it doesn’t have a bunch of examples for it may have no idea what’s going on.”</p>
<p>[[My own take is that the fetish of end to end learning leads people to leave out well known algorithms that might solve many of &nbsp;these problems (e.g,, the incredibly <a href="https://www.sciencedirect.com/science/article/pii/S0042698911003646" target="_blank" rel="noopener">simple time to collision algorithms based on looming</a>). Yes, end to end learning made speech understanding systems better, but that does not mean it is the appropriate fetish to apply everywhere.]]</p>
<p><strong><span>Pro tip:</span></strong> Think about this history of industry prognostications about fully autonomous driving being just around the corner when you read today’s prognostications about LLMs taking jobs, en masse, in the next couple of years, or humanoid robots being dirt cheap and being able to learn how to do any human manual task real real soon now. You know you have seen this movie before…</p>
<p><span><em>My own experiences with Waymo in 2024</em></span></p>
<p>I have two sorts of experiences with Waymo vehicles. First, as a driver of my own vehicle and sharing road space with them every single time that I drive. And second, as a user of their ride service.</p>
<p>The streets of San Francisco had been thick with Waymo vehicles with no driver in them especially in the second half of 2024. As I drive across the city every morning to head down to my robotics/AI startup half way down the peninsula I see them everywhere until I get on to 101. &nbsp;I see them in front of me and behind me and in adjacent lanes as I drive on multilane one way streets. Sometimes I see four of them in a single block. Twice I’ve seen four of them in a line, in my block and could see four of them in a line in the block ahead of me. &nbsp;When I am at four way intersections with no traffic lights I see them participating in the social ritual of taking your turn to drive through the intersection in the order you stopped, except when a pedestrian is crossing in front of you. They do that pretty well. They do less well when they accidentally get into a line of parents’ cars snaking around a corner for school drop off or pickup.</p>
<p>Over the last few months I have noticed that in general they are getting more aggressive about stretching the rules, just like people do. Otherwise human drivers (including me) take advantage of their politeness. That aggression is not always welcomed. One morning I saw a workman with a group doing some digging on a road, and holding a sign with SLOW on one side and STOP on the other side have to jump in front of a Waymo to get it to do what he was trying to tell it to do with the sign. STOP. It wasn’t stopping for no stinking sign!</p>
<p>The only time I have seen a Waymo go into reverse, ever, was when I was illegally driving the wrong way down a single lane street and we were heading straight at each other.</p>
<p>As a rider I feel they are not quite aggressive enough with human drivers some time, so a ride in a Waymo takes longer than with an Uber or Lyft.</p>
<p>It is hit and miss where they drop me off. Sometimes they take a place to pull over half a block from my house, even when it is raining. There is no way to adjust what they happen to decide that day, even though I know that they will always be able to pull in right in front of my house.</p>
<p>The first time I took a Waymo this year, on the way home it picked me up at a restaurant and then was about to make a right turn. But at that corner there was an 18 wheeler with its lights flashing and surrounded by green cones. It pulled right in behind that truck and waited a long time before it drove forward. I am guessing a remote operator intervened told it to go around because eventually it pulled around it in the lane just to the left. Based on seeing Waymos interact with orange cones I suspect it would have done better if the cones had been orange rather than green. &nbsp;This easily illustrates that the learning that this robot does, and indeed any robot does, is nothing like the learning that people do (see my rant about the seven deadly sins and mistaking performance for competence in the section below on advances in AI and ML).</p>
<p>I mostly feel safe when I am a passenger in a Waymo. &nbsp;Sometimes I don’t feel that my driver of an Uber that I am taking rides with Uber that are not as safe as I would prefer.</p>
<p><span><em>Self Driving Taxi Services</em></span></p>
<p>There have been three self driving taxi services in the US in various stages of play over the last handful of years, though it turns out, as pointed out above that all of them have remote operators. They are Waymo, Cruise, and Zoox.</p>
<p>Waymo and Cruise are similar in that they use conventional cars adorned with lots of sensors. Zoox has purpose built vehicles that have no steering wheel or pedals for brake or accelerator.</p>
<p>Waymo and Cruise went for deployments in large parts of two or more cities and have had ride services callable by apps, just as one can do with Uber or Lyft. Zoox is smaller scale, much more restricted in geography, and really not comparable.</p>
<p>At this time last year Cruise was in trouble has it had suspended all of its San Francisco operations under pressure from regulators after some bad accidents that happened in a way that never would happen for human driven cars. &nbsp;Briefly, their cars were getting hit at night by emergency vehicles with lights flashing as the Cruise cars crossed intersections. Human drivers see the reflections of lights from such vehicles flashing even if they don’t see the vehicles themselves. The Cruise vehicles were only reacting to flashing lights that they could perceive directly. But the accident that tipped the scales was when a pedestrian crossing in front of a human driven vehicle was hit and went flying in the air landing right in front of a Cruise. The Cruise hit the person (who now disappeared from sight) as a human driver would most likely have done. But then it proceeded to drive 20 feet with the human underneath the vehicle being dragged along as it went into a mode where it was supposed to get off the road. A human driver would not have reacted that way to having been in a collision, even if it was not their fault.</p>
<p>The hammer finally fell in December of 2024. General Motors shut down Cruise. The leading paragraphs from this <a href="https://www.wsj.com/business/autos/general-motors-scraps-cruise-robotaxi-program-ea3298a8" target="_blank" rel="noopener">linked story</a>&nbsp;from the Wall Street Journal&nbsp;are:</p>
<div>
<p>General Motors has scrapped its Cruise robotaxi program after nearly a decade and $10 billion in development, citing the time and costs needed to scale the business and rising competition.</p>
<p>GM on Tuesday said it plans to realign its autonomous driving strategy and give priority to development of advanced driver assistance systems, which take over steering and other functions in certain situations and are common on new vehicles today.</p>
<p>The automaker said it would continue to develop fully autonomous technology for personal vehicles, and build on the progress of its Super Cruise system, a hands-off, eyes-on driving feature that the company introduced several years ago.</p>
<p data-type="paragraph">GM said it owns about 90% of Cruise and intends to buy out the remaining investors. It plans to combine the technical teams from Cruise and GM into a single effort to advance autonomous and assisted driving.</p>
<p>“We want to leverage what already has been done as we go forward in this,” Chief Executive Mary Barra told analysts on a call Tuesday.</p>
<p>The Detroit automaker said it expects the restructuring to reduce spending by more than $1 billion annually after the proposed plan is completed, which is expected in the first half of next year.</p>
</div>
<p>While there are 40 companies that have permits to test autonomous driving in California, alone, the demise of Cruise leaves just one company, Waymo, trying to make an actual go of a digital taxi service in the United States. They have an enormous significant lead over anyone else who wants get into this business and have spent billions of dollars (probably very much north of $10 billion) on this endeavor over the last 15 years. In an email they sent me a couple of weeks ago as a user of their services they reported that they provided 4 million customer rides in 2024. That is approximately 4 million more than any other company in the United States.</p>
<p><span><span><i>Waymo</i></span></span></p>
<p>Despite being so far out in front it has not been all smooth sailing for Waymo.</p>
<p>Early in the year the operations center for Waymo somehow neglected to realize it was Chinese New Year in Chinatown in San Francisco. So Waymo vehicles were routed through that area on the biggest night of celebration. Any human driver would have realized that the streets, i.e., the street surfaces where cars usually drive, were completely packed with humans, no doubt some of whom were intoxicated as well as just being out having a good time. Not so the Waymo vehicles. They tried pushing through the very very dense crowds, no doubt annoying many people. And what do people have at Chinese New Year? &nbsp;Fireworks. So some revelers decided to push back on this robot car invading their space. Here are a couple of pictures of the results.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Waymo1.jpg" alt="" width="1924" height="1110"></p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Waymo2.jpg" alt="" width="1916" height="1440"></p>
<p>Not pretty. &nbsp;And an example of how taking away people’s agency is never a good idea for robots (<a href="https://rodneybrooks.com/rodney-brooks-three-laws-of-robotics/" target="_blank" rel="noopener">see my second law of robotics</a>).</p>
<p>Throughout 2024 Waymo has been investigates for various accidents such as those described in <a href="https://www.wsj.com/business/autos/regulators-probe-alphabets-waymo-after-22-self-driving-car-incidents-996fde65" target="_blank" rel="noopener">this Wall Street Journal article</a>. “Reports included collisions with stationary or semistationary objects, such as gates, chains or parked vehicles, according to the regulator.”</p>
<p>In the middle of the summer Waymo added a feature where they would honk their horns at cars in their way. But this backfired when hundreds of Waymos were coming back to their parking lot in the very early hours of the morning, and they started honking at each other and <a href="https://www.designnews.com/automotive-engineering/neighbors-hate-waymo-horn-hassles" target="_blank" rel="noopener">waking up human neighbors</a>. Eventually that got fixed.</p>
<p>In late September a motorcade for Kamala Harris in San Francisco was brought to a halt by a Waymo that <a href="https://sfstandard.com/2024/09/28/waymo-halts-kamala-harris-san-francisco-motorcade/" target="_blank" rel="noopener">stopped in the middle of California Street</a> doing a U-turn in front of it. I’m sure this incident was of great concern to the Secret Service. Eventually a San Francisco police officer got into the car and drove it out of the way–this is shown in a video included with the story above. I do not know how the officer got access to the vehicle and whether Waymo remote operations were cooperating.</p>
<p>More disturbingly humans outside the Waymos started harrassing humans inside them. The most concerning cases come from the realization that if a woman is in a Waymo at night she will be dropped off, outside, on a public road at the end of her journey with no option but to get out of the car where it has stopped. So groups of men have followed Waymos with women in them and then harassing the woman when she gets out. If she was driving her own car she might be heading to an off road parking space or she might choose not to stop if she knows she is being followed. There are no such options in a Waymo so taking a Waymo at night is less safe than other means of transportation–just follow it and eventually the preyed upon woman will have to get out. Here is a <a href="https://www.washingtonpost.com/technology/2024/12/22/waymo-robotaxi-passengers-harassment/" target="_blank" rel="noopener">very recent disturbing story</a>&nbsp;about this practice.</p>
<p>Meanwhile Waymo <a href="https://electrek.co/2024/10/25/waymo-secures-5-6-billion-funding-expanding-robotaxis-new-cities-2025/" target="_blank" rel="noopener">managed to raise $5.6B to expand to new cities in 2025</a>. It already operates in parts of San Francisco, Los Angeles, and Phoenix. The new money will let it expand to Austin and Atlanta in the United States and to start operating in parts of Tokyo in Japan. That is expensive expansion.</p>
<p>Here is the question for the future of watered down remote monitored “autonomous” driving systems (let’s call it “watered down autonomy”), and it is up to Waymo now. Can Waymo expand fast enough in these new markets in 2025 and take enough business from what is left of traditional taxi operators, along with those operating under the Uber and Lyft models, and do it in a way which is in sight of profitability, so that it has a case to raise the stupendous amounts of money needed to operate in all large cities in the US in the next 10 t0 20 years?</p>
<p>If Waymo can not succeed at this in the next two years I think the idea of large scale use of watered down autonomy will be dead for at least a decade or two. Right now full autonomy everywhere is already dead.</p>

<table id="tablepress-30">
<thead>
<tr>
	<th>Prediction<br>
[Self Driving Cars]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>A flying car can be purchased by any US resident if they have enough money.</td><td>NET 2036</td><td>There is a real possibility that this will not happen at all by 2050.<br>
</td><td></td>
</tr>
<tr>
	<td>Flying cars reach 0.01% of US total cars.</td><td>NET 2042</td><td>That would be about 26,000 flying cars given today's total.</td><td></td>
</tr>
<tr>
	<td>Flying cars reach 0.1% of US total cars.</td><td>NIML</td><td></td><td></td>
</tr>
<tr>
	<td>First dedicated lane where only cars in truly driverless mode are allowed on a public freeway.<br>
</td><td><p>NET 2021</p></td><td>This is a bit like current day HOV lanes. My bet is the left most lane on 101 between SF and Silicon Valley (currently largely the domain of speeding Teslas in any case). People will have to have their hands on the wheel until the car is in the dedicated lane.</td><td></td>
</tr>
<tr>
	<td>Such a dedicated lane where the cars communicate and drive with reduced spacing at higher speed than people are allowed to drive</td><td><p>NET 2024</p></td><td></td><td><span>20240101</span> <p>This didn't happen in 2023 so I can call it now. But there are no plans anywhere for infrastructure to communicate with cars, though some startups are finally starting to look at this idea--it was investigated and prototyped by academia 20 years ago.</p></td>
</tr>
<tr>
	<td>First driverless "taxi" service in a major US city, with dedicated pick up and drop off points, and restrictions on weather and time of day.</td><td><p>NET 2021</p></td><td>The pick up and drop off points will not be parking spots, but like bus stops they will be marked and restricted for that purpose only.</td><td><span>20240101</span> <p>People may think this happened in San Francisco in 2023, but it didn't. Cruise has now admitted that there were humans in the loop intervening a few percent of the time. THIS IS NOT DRIVERLESS. Without a clear statement from Waymo to the contrary, one must assume the same for them. Smoke and mirrors.</p></td>
</tr>
<tr>
	<td>Such "taxi" services where the cars are also used with drivers at other times and with extended geography, in 10 major US cities</td><td><p>NET 2025</p></td><td>A key predictor here is when the sensors get cheap enough that using the car with a driver and not using those sensors still makes economic sense.</td><td><span>20250101</span> <p>Imminent dual use of personal cars was the carrot that got lots of people to pay cash when buying a Tesla for the software subscription that would allow thei car to operate in this way. Shockingly the CEO of Tesla announced in smoke and mirrors roll out of Cyber Cab in 2024, that the service would use specially built vehicles to be produced at some indeterminate late date. I got suckered by his hype. This is unlikely to happen in the first half of this century.</p></td>
</tr>
<tr>
	<td>Such "taxi" service as above in 50 of the 100 biggest US cities.</td><td>NET 2028</td><td>It will be a very slow start and roll out. The designated pick up and drop off points may be used by multiple vendors, with communication between them in order to schedule cars in and out.<br>
</td><td><span>20250101</span> <p>Even the watered down version of this with remote operators is not gong to happen in 50 cities by 2028. Waymo has it in 3 cities and is currently planning on 2 more in the US in 2025.</p></td>
</tr>
<tr>
	<td>Dedicated driverless package delivery vehicles in very restricted geographies of a major US city.</td><td><p>NET 2023</p></td><td>The geographies will have to be where the roads are wide enough for other drivers to get around stopped vehicles.<br>
</td><td></td>
</tr>
<tr>
	<td>A (profitable) parking garage where certain brands of cars can be left and picked up at the entrance and they will go park themselves in a human free environment.</td><td><p>NET 2023</p></td><td>The economic incentive is much higher parking density, and it will require communication between the cars and the garage infrastructure.</td><td></td>
</tr>
<tr>
	<td>A driverless "taxi" service in a major US city with arbitrary pick and drop off locations, even in a restricted geographical area.<br>
</td><td>NET 2032
<p>NET 2032</p></td><td>This is what Uber, Lyft, and conventional taxi services can do today.</td><td><span>20240101</span> <p>Looked like it was getting close until the dirty laundry came out.</p><span>20250101</span> <p>Waymo now has a service that looks and feels like this in San Francisco, 8 years earlier than I predicted. But it is not what every one was expecting. There are humans in the loop. And for those of us who use it regularly we know it is not as general  case on drop off and pick up as it is with human drivers.</p></td>
</tr>
<tr>
	<td>Driverless taxi services operating on all streets in Cambridgeport, MA, and Greenwich Village, NY. </td><td>NET 2035</td><td>Unless parking and human drivers are banned from those areas before then.</td><td></td>
</tr>
<tr>
	<td>A major city bans parking and cars with drivers from a non-trivial portion of a city so that driverless cars have free reign in that area.</td><td>NET 2027<br>
BY 2031</td><td>This will be the starting point for a turning of the tide towards driverless cars.</td><td></td>
</tr>
<tr>
	<td>The majority of US cities have the majority of their downtown under such rules.</td><td>NET 2045</td><td></td><td></td>
</tr>
<tr>
	<td>Electric cars hit 30% of US car sales.</td><td>NET 2027</td><td></td><td><span>20240101</span> <p>This one looked pessimistic last year, but now looks at risk. There was a considerable slow down in the second derivative of adoption this year in the US.</p><span>20250101</span> <p>Q3 2024 had the rate 8.9% so there is no way it can reach 30% in 2027. I was way too optimistic at a time when EV enthusiasts thought I was horribly pessimistic.</p></td>
</tr>
<tr>
	<td>Electric car sales in the US make up essentially 100% of the sales.</td><td>NET 2038<br>
</td><td></td><td></td>
</tr>
<tr>
	<td>Individually owned cars can go underground onto a pallet and be whisked underground to another location in a city at more than 100mph.</td><td>NIML</td><td>There might be some small demonstration projects, but they will be just that, not real, viable mass market services.<br>
</td><td></td>
</tr>
<tr>
	<td>First time that a car equipped with some version of a solution for the trolley problem is involved in an accident where it is practically invoked.</td><td>NIML</td><td>Recall that a variation of this was a key plot aspect in the movie "I, Robot", where a robot had rescued the Will Smith character after a car accident at the expense of letting a young girl die.</td><td></td>
</tr>
</tbody>
</table>
<!-- #tablepress-30 from cache -->
<p><span><em>Electric Cars</em></span></p>
<p>Last year US manufacturers pulled back on their planned production of EVs. In data from <a href="https://caredge.com/guides/electric-vehicle-market-share-and-sales" target="_blank" rel="noopener">this report</a> we can see that sales dropped at the start of 2024 but have now picked up again.</p>

<table id="tablepress-34">
<tbody>
<tr>
	<td>2022</td><td>2022</td><td>2022</td><td>2022</td><td>2023</td><td>2023</td><td>2023</td><td>2023</td><td>2024</td><td>2024</td><td>2024</td>
</tr>
<tr>
	<td>Q1</td><td>Q2</td><td>Q3</td><td>Q4</td><td>Q1</td><td>Q2</td><td>Q3</td><td>Q4</td><td>Q1</td><td>Q2</td><td>Q3</td>
</tr>
<tr>
	<td>5.3%</td><td>5.6%</td><td>6.1%</td><td>6.5%</td><td>7.3%</td><td>7.2%</td><td>7.9%</td><td>8.1%</td><td>7.3%</td><td>8.0%</td><td>8.9%</td>
</tr>
</tbody>
</table>
<!-- #tablepress-34 from cache -->
<p>There is steady growth in sales but my prediction of 30% of US car sales being electric by 2027 now seems wildly optimistic. We need two doublings to get there in three years and the doubling rate seems more like one doubling in four to five years.</p>
<p>Note that some sources include hybrids and hydrogen powered cars in electric vehicles but I am using the battery electric vehicle (BEV) numbers.</p>
<p>To see how the trends are across brands you can see a breakout for Q2 of 2024 <a href="https://www.coxautoinc.com/wp-content/uploads/2024/07/Q2-2024-Kelley-Blue-Book-Electric-Vehicle-Sales-Report.pdf" target="_blank" rel="noopener">here</a>.</p>
<p>There appear to be two main headwinds for BEV adoption. Firstly, if one doesn’t have on property residential parking it is hard work in the US to find a place to recharge, and it takes hours for the charging to finish. This will stop many city dwellers from adopting. Secondly the increased tire wear adds up to real money. The maintenance requirements for BEVs are much less than for cars with an internal combustion engine. On the other hand tires do not last as long (I have had to buy four new tires in less than two years owning my first BEV), <a href="https://www.telegraph.co.uk//money/consumer-affairs/my-electric-car-heavy-had-change-tyres-after-7500-miles/" target="_blank" rel="noopener">apparently due to the increased weight of the car</a>.</p>
<p><span><em>Flying Cars</em></span></p>
<p>Flying cars are another category where the definitions have changed. Back when I made my predictions it meant a vehicle that could both drive on roads and fly through the air. &nbsp;Now it has come to mean an electric multi-rotor helicopter than can operate like a taxi between various fixed landing locations. Often touted are versions that have no human pilot. These are known as eVTOLs, for “electric vertical take off &amp; landing”.</p>
<p>Large valuations have been given to start ups who make nice videos of their electric air taxis flying about. But on inspection one sees that they don’t have people in them. Often, you might notice, even those flights are completely over water rather than land. I wrote about the <a href="https://rodneybrooks.com/where-are-the-crewed-evtol-videos/" target="_blank" rel="noopener">lack of videos of viable prototypes</a> back in November 2022.</p>
<p>Nevertheless there have been wild predictions. &nbsp;I ended a longer version of this component in <a href="https://rodneybrooks.com/predictions-scorecard-2024-january-01/" target="_blank" rel="noopener">last year’s annual review</a> with:</p>
<p>Also note the size of this vehicle. There are many fossil fuel powered helicopters that are much smaller. This is not going to be a personally owned vehicle for the masses.</p>
<p>Don’t hold your breath. They are not here. They are not coming soon.</p>
<p>Nothing has changed. Billions of dollars have been spent on this fantasy of personal flying cars. &nbsp;It is just that, a fantasy, largely fueled by spending by billionaires.</p>
<h6>Robotics, AI, and Machine Learning</h6>
<p>So what happened in Robotics, AI, and Machine Learning this year?</p>
<p>Many, many, many people got just a little bit over excited. That’s what happened.</p>
<p>There have been a lot of party tricks and it is the researchers who often play the tricks on themselves without realizing it. This is not new, none of it is new. But there are orders of magnitude more people watching it now, and more people are out to make a buck by being hypesters, promising riches to those who will invest in their irrationally overpriced companies.</p>
<p>How could this be?</p>
<p>We are seeing mass sinning, lots and lots of people committing some of the seven deadly sins of predicting the future of AI &nbsp;which I wrote about back in 2017 <a href="https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/" target="_blank" rel="noopener">here</a> (or <a href="https://www.technologyreview.com/2017/10/06/241837/the-seven-deadly-sins-of-ai-predictions/" target="_blank" rel="noopener">here</a> you can see a professionally edited version of that blog post of mine).</p>
<p>Four of those seven sins seem most relevant to today’s hyped up atmosphere around robotics, AI, and machine learning.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/PerformCompet-1.jpg" alt="" width="314" height="166">&nbsp;<img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Magic-1.jpg" alt="" width="287" height="165">&nbsp;<img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Exponentialism-1.jpg" alt="" width="277" height="173">&nbsp; <img src="http://rodneybrooks.com/wp-content/uploads/2024/12/Deployment-1.jpg" alt="" width="320" height="181"></p>
<p>Here now are short descriptions of these particular four sins, edited down from my earlier much more detailed descriptions. Then I will weave them together to explain how it is still pretty much business as usual, and I mean that in a good way, with steady progress on both the science and engineering of AI.</p>
<p><span><em>Performance versus Competence</em></span></p>
<p>One of the social skills that we all develop is an ability to estimate the capabilities of individual people with whom we interact. We use cues from how a person performs any particular task to estimate how well they might perform some different task. We are able to generalize from observing performance at one task to a guess at competence over a much bigger set of tasks.</p>
<p>These estimators that we have all inherited or learned do not generalize well to other creatures or machines. We are not good at guessing which smart things other species might be able to do, and we are not good at guessing what an AI system can do when we have seen it do a few tasks in a limited domain. We get it wrong all the time.</p>
<p><span><em>Indistinguishable from Magic</em></span></p>
<p>When people cannot explain how something works they cannot know its limits as they do not have any sort of model (nor have they seen enough examples of it before). Arthur C. Clarke said that any sufficiently advanced technology is indistinguishable from magic.</p>
<p>In our minds UFOs can do all sorts of amazing things as we have no way of knowing their limits–they may as well be magic, And that is what they become in speculation about them.</p>
<p>Isaac Newton spent half his working life on alchemy as he did not know that the nucleus of atoms were not subject to mere chemistry. He would have been just as ignorant of the limitations of an iPhone screen (different sort of apple…), despite his own ground breaking work in optics. Remember, he was a really really smart dude. But even he was not able to develop all the theories needed to understand the world around him, despite his successes with calculus and gravity and the makeup of white light. He attributed properties to chemistry that were way beyond its limits.</p>
<p><span><em>Exponentialism</em></span></p>
<p>We have just lived through sixty years of the most phenomenal growth of a technology in the history of humankind. It is the story of silicon-based computation. Everyone has some idea about Moore’s Law, at least as much to sort of know that computers get better and better on a clockwork like schedule.</p>
<p>This reality has trained people to think that probably a lot of other things in tech will change exponentially, especially when that thing has a strong computational component. The sin of exponentialism is to argue that some other process is going to follow a Moore’s-like law when it is unwarranted to so argue.</p>
<p>Moore’s law worked for so long because in the starting technology of the 1960s the currents used to represent digital information were many many orders of magnitude beyond the minimal physical limit needed to determine whether they &nbsp;were present or not, and hence distinguish a 1 from a 0. Those currents could be halved many times without breaking physics limits.</p>
<p><em><span>Speed of Deployment</span></em></p>
<p>New technologies get deployed much more slowly than people imagine. Even software technologies.</p>
<p>The old internet protocol, IPv4, can only address two billion, or&nbsp;2×10<sup>9</sup>, devices, which is way less than the number of people on our planet. A new protocol, IPv6, which can address more than&nbsp;3×10<sup>38</sup>&nbsp;devices was meant to replace it over a two year period of dual use by about 2003. But in 2024 IPv4 was still there and carrying over half the world’s internet traffic despite its inadequacies.</p>
<p>Must functioning businesses that operate in the physical world are very averse to taking up new technology as it dramatically increases existential risk to their business. They must foresee immediate and incredibly high return on investment (ROI) to be tempted to move to new technologies.</p>
<p>Even the military is slow to adopt new technologies. The US Air Force still flies the B-52H variant of the B-52 bomber. This version was introduced in 1961, making it 63 years old. The last one was built in 1963, a mere 61 years ago. Currently these planes are expected to keep flying until at least 2040, and perhaps longer–there is talk of extending their life out to 100 years.</p>
<p><em><span>What does this all mean?</span></em></p>
<p>Right now there is incredible hype for both Large Language Models (LLMs), and all their variations, and for humanoid robots, especially humanoid robots that are going to learn how to do things.</p>
<p>The hype is driven by the four sins above.</p>
<p><em><span>LLMs</span></em></p>
<p>LLMs have proved amazing facile with language. They have been trained on pretty much all the text that is available on the Web and all the digitized historical books that exist. Miraculously LLMs seem to be able to infer a representation of some sort, that is somewhat independent of the particular human language that they read. So they are able to translate between human languages, and when you ask them just about anything they produce text in the language that you asked in, and that text often seems entirely reasonable and informative.</p>
<p>I used the word “miraculously” as we do not really understand why they are able to do what they do. We, of course, know that the architecture for them is built around noticing correlations in vast amounts of text &nbsp;that connect some tens of thousands of tokens which are the components of words in each language that is digested. It is a surprise that they work as well as they they do, and produce coherent sounding language on just about any topic.</p>
<p>Here is the original architectural diagram from the 2017 <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" target="_blank" rel="noopener">Attention Is All You Need</a> paper:</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/attention.jpg" alt="" width="1618" height="1600"></p>
<p>Each column from bottom to top is a pure feed forward network, with no search, no iteration, no conventional algorithm at all. There are inputs at the bottom and then layer upon layer of linear neurons that have numbers or weights stored in them that multiply and add their inputs and threshold that sum to provide an output. The detail in the architectural diagram is how the connections between layers are organized.</p>
<p>On the left is an input or question, in a linear string of words, from a user. That gets injected half way up the network on the right and remains constant while a single iteration process runs. The stack on the right outputs a word (or token) and that gets fed back to the bottom of that stack, and a new token pops out the top. All the output tokens that have so far been produced remain in the right bottom input buffer as ordered input.</p>
<p>What the network has been trained to do, is given the user input on the left, and what the network has output so far, choose a very likely next word, given the billions of examples it has seen in training. Some randomness is used to choose among a small number of very likely next words at each stage.</p>
<p>There are hundreds of billions of weights that get learned and stored in the layers of network to act as multipliers for each individual input to each layer.</p>
<p>So now us humans are faced with looking at this system running and our human nature just makes us commit the first two sins from above. &nbsp;It is in our nature and we cannot help ourselves.</p>
<p>First, we see really impressive examples of responses to input questions, and if a human was giving those answers we would estimate that person to be quite clever and able to reason. Often though, because they have so many billions of examples on which they were trained LLMs are essentially looking up the question in the weights. The weight if gained from all of human knowledge that is out there on the network in language form. Invisibly the network is perhaps (but not in any intentional way) merging some similar questions, and then merging the answers which were already in the vast data that it has seen.</p>
<p>But us dumb humans just think the damn thing is really really smart.</p>
<p>Then, since we don’t have a real explanation in our heads for what it is doing we start thinking it is magic, and that there is no real limit to what it is extracting from all that data (that it used a significant portion of the energy budget for many different countries to compute) and how general its capabilities will be. It becomes magic. And then researchers try to show that it can reason, that it has inferred a spatial understanding of the world, that language can be used to do all sorts of things that Moravec’s paradox tells us it can’t. There is a lot of magical thinking that humans do about LLMs.</p>
<p>Of course it can diagnose diseases like a doctor talking about them. Of course it can teach a student as well as a human teacher. Of course it can program as well as a human computer programmer. It is magic after all.</p>
<p>But in reality the fact that it is just picking likely next words means that in fact we can’t trust its output. Some outputs are great. Some are pure confabulations (most people use the word “hallucinations” for this, but I prefer “confabulations”). And we do not know which we will get ahead of time, or more perniciously how much of each we will get, trustworthy pieces of output and confabulated pieces of output all jumbled together.</p>
<p>Not to worry say the proponents, More learning will fix it. Fire up a nuclear power plant (I am not making this up–the tech companies are getting more nuclear power built or activated so that their LLMs can learn what a human learns using just 20 watts powering their brain; I am not confabulating this!!), and we’ll feed it more data and it will become more trustworthy. &nbsp;It is magic after all. But the magic is not going as well as the proponents imagined and promised as this Wall Street Journal <a href="https://www.wsj.com/tech/ai/openai-gpt5-orion-delays-639e7693?mod=hp_lead_pos8" target="_blank" rel="noopener">story</a> explains. Their imaginations were definitely encourage by exponentialism, but in fact all they knew was that when the went from smallish to largish networks following the architectural diagram above, the performance got much better. So the inherent reasoning was that if <strong>more</strong> made things better then <em>more</em> <strong>more</strong> would make things <em>more</em> better. Alas for them it appears that this is probably not the case. But rabid exponentialists have not yet given up. Expect a bunch of VCs to adversely affect the growth of pension funds around the world as pension funds are a prime source of capital that VCs spend.</p>
<p>More serious academics are working on boxing in the LLMs with more external mechanism beyond just feeding the output tokens back in as a linear string of input. Many of these mechanisms look a lot like more conventional AI mechanisms, and we will see where these additions prove to be useful, how much of the wheel will be reinvented, and how long (months?, years?, decades?) to get there.</p>
<p>And the answers to those last questions will tell us how much sinning has been done by companies in predicting fast deployments. Back in rant at the beginning of this post I gave the example of I.B.M. and Watson and their completely optimistic predictions of how any problems of applying Watson (which seemed extremely competent based on its performance on live TV) to the real world would be solvable. The areas that it was predicted to be applicable came from magical thinking.</p>
<p>Surely no one today could be as dumb as that big company was back in 2011. Surely not. No, not us smart inhabitants of 2025. Its us. We are nowhere near as dumb as them!!</p>
<p><em><span>Humanoid Robots</span></em></p>
<p>The other thing that has gotten over hyped in 2024 is humanoids robots. &nbsp;The rationale for humanoid robots being a thing is a product of the four sins above and I think way less rooted in reality than the hype about LLMs. In fact I think it is pretty dumb. [[I suspect many people will reason that I cannot have a valid opinion about this precisely because I happen to have built more humanoid robots than anyone else on the planet. So read ahead with caution.]]</p>
<p><a href="https://rodneybrooks.com/rodney-brooks-three-laws-of-robotics/" target="_blank" rel="noopener">My first law of robotics</a> states:</p>
<p><em>The visual appearance of a robot makes a promise about what it can do and how smart it is. It needs to deliver or slightly over deliver on that promise or it will not be accepted.</em></p>
<p>The first sentence describes, I think, what is sucking people into believing that humanoid robots have a big future. It looks like a human, so its performance will be like a human, so it will be competent like a human. &nbsp;It’s the performance/competence sin without even waiting for the performance part!</p>
<p>The second sentence describes how the humanoid fever will break, and how the hundreds of millions of dollars put into many of these companies (billions of dollars overall) will disappear. The puppets will not perform at acceptable levels. It is easy to see this as you hear all the things investors and CEOs of humanoid robots say they will be able to do. They have hardly even got to the lab demonstration phase. &nbsp;My third law of robotics is:</p>
<p><em>Technologies for robots need 10+ years of steady improvement beyond lab demos of the target tasks to mature to low cost and to have their limitations characterized well enough that they can deliver 99.9% of the time. Every 10 more years gets another 9 in reliability.</em></p>
<p>For real work, robots need to operate with four, five, or six nines. We are a long way from that. The zeitgeist is that we will simply teach the robots to do stuff and then they will be able to do it.</p>
<p>BUT, we do not know yet whether that is going to work. In order for it to work you have to both collect the <strong>right sort of data</strong> and then <strong>learn the right things</strong> from that data. It is not at all clear to me that we know the answers to make either of those things true. I think it will be an active place for lots of good research for many years to come.</p>
<p>There is an excellent survey paper of current research state of the art called <a href="https://arxiv.org/abs/2408.03539">Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes</a>. Unfortunately I think the title of the paper is going to confuse many people. “Real-World Successes” to someone like me, who these days deploys robots that people pay for and that provide real ROI, sounds like it is about systems that have been deployed. But on reading the paper it turns out that they mean that it is learning and demonstrations done in a lab setting on physical hardware rather than just in simulations and simulators. &nbsp;And, to me the lab demonstrations are shakier (literally) than I imagined in my third law above.</p>
<p>I think we are a long way off from being able to for-real deploy humanoid robots which have even minimal performance to be useable and even further off from ones that have enough ROI for people want to use them for anything beyond marketing the forward thinking outlook of the buyer.</p>
<p>Despite this, many people have predicted that the cost of humanoid robots will drop exponentially as their numbers grow, and so they will get dirt cheap. I have seen people refer to the cost of integrated circuits having dropped so much over the last few decades as proof. Not so.</p>
<p>They are committing the sin of exponentialism in an obviously dumb way. As I explained above the first integrated circuits were far from working at the limits of physics of representing information. But today’s robots use mechanical components and motors that are not too far at all from physics based limits, about mass, force, and energy. You can’t just halve the size of a motor and have a robot lift the same sized payload. Perhaps you can halve it once to get rid of inefficiencies in current designs. Perhaps. But you certainly can’t do it twice. Physical robots are not ripe for exponential cost reduction by burning wastes in current designs. And it won’t happen just because we start (perhaps) mass producing humanoid robots (oh, but the way, I already did this a decade ago–see my parting shot below). We know that from a century of mass producing automobiles. They did not get exponentially cheaper, except in the computing systems. Engines still have mass and still need the same amount of energy to accelerate good old fashioned mass.</p>
<p><em><span>This Year’s Prediction Update</span></em></p>
<div class="page" title="Page 1">
<p>There is only one new comment in my robotics, AI and ML predictions table this year. There are a bunch of well funded new companies in the home robot space, and perhaps they will come up with new mobility solutions, which in my experience is the big blocker for home robots.</p>
</div>

<table id="tablepress-31">
<thead>
<tr>
	<th>Prediction<br>
[AI and ML]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>Academic rumblings about the limits of Deep Learning</td><td><p>BY 2017</p></td><td>Oh, this is already happening... the pace will pick up.</td><td></td>
</tr>
<tr>
	<td>The technical press starts reporting about limits of Deep Learning, and limits of reinforcement learning of game play.</td><td><p>BY 2018</p></td><td></td><td></td>
</tr>
<tr>
	<td>The popular press starts having stories that the era of Deep Learning is over.</td><td><p>BY 2020</p></td><td></td><td></td>
</tr>
<tr>
	<td>VCs figure out that for an investment to pay off there needs to be something more than "X + Deep Learning".</td><td><p>NET 2021</p></td><td>I am being a little cynical here, and of course there will be no way to know when things change exactly.</td><td></td>
</tr>
<tr>
	<td>Emergence of the generally agreed upon "next big thing" in AI beyond deep learning.</td><td><p>NET 2023</p><p>BY 2027</p></td><td>Whatever this turns out to be, it will be something that someone is already working on, and there are already published papers about it. There will be many claims on this title earlier than 2023, but none of them will pan out.</td><td><span>20240101</span> <p>It definitely showed up in 2023. It was in the public mind in December 2022, but was not yet the big thing that it became during 2023. A year ago I thought it would perhaps be neuro-symbolic AI, but clearly it is LLMs, and ChatGPT and its cousins. And, as I predicted in 2018 it was something already being worked on as the "attention is all you need" paper, the key set of ideas, was published in 2017.</p></td>
</tr>
<tr>
	<td>The press, and researchers, generally mature beyond the so-called "Turing Test" and Asimov's three laws as valid measures of progress in AI and ML.</td><td><p>NET 2022</p></td><td>I wish, I really wish.</td><td><span>20230101</span> <p>The Turing Test was missing from all the breathless press coverage of ChatGPT and friends in 2022. Their performance, though not consistent, pushes way past the old comparisons.</p> <span>20240101</span> <p>The Turing Test was largely missing from the press in 2024 also, and there was a <a href="https://www.nature.com/articles/d41586-023-02361-7" rel="noopener">story in Nature</a> commenting on that. So yes, this has now happened.</p> </td>
</tr>
<tr>
	<td>Dexterous robot hands generally available.</td><td>NET 2030<br>
BY 2040 (I hope!)</td><td>Despite some impressive lab demonstrations we have not actually seen any improvement in widely deployed robotic hands or end effectors in the last 40 years.</td><td></td>
</tr>
<tr>
	<td>A robot that can navigate around just about any US home, with its steps, its clutter, its narrow pathways between furniture, etc.</td><td>Lab demo: NET 2026<br>
Expensive product: NET 2030<br>
Affordable product: NET 2035</td><td>What is easy for humans is still very, very hard for robots.</td><td> <span>20250101</span> <p>A bunch of startups in the home robot space got significant funding in 2024. Two of them are run by ex-CEOs of large companies: iRobot and Cruise (and he was also an intern at iRobot after we were already a public company). So this one may be in play for a lab demo in the next few years if they have this as one of their goals..</p></td>
</tr>
<tr>
	<td>A robot that can provide physical assistance to the elderly over multiple tasks (e.g., getting into and out of bed, washing, using the toilet, etc.) rather than just  a point solution.</td><td>NET 2028</td><td>There may be point solution robots before that. But soon the houses of the elderly will be cluttered with too many robots.</td><td></td>
</tr>
<tr>
	<td>A robot that can carry out the last 10 yards of delivery, getting from a vehicle into a house and putting the package inside the front door.</td><td>Lab demo: NET 2025<br>
Deployed systems: NET 2028<br>
</td><td></td><td></td>
</tr>
<tr>
	<td>A conversational agent that both carries long term context, and does not easily fall into recognizable and repeated patterns.</td><td><p>Lab demo: NET 2023</p>Deployed systems: 2025</td><td>Deployment platforms already exist (e.g., Google Home and Amazon Echo) so it will be a fast track from lab demo to wide spread deployment.</td><td><span>20240101</span> <p>One half of this happened this year. ChatGPT has been connected to microphones and speakers so you can now talk to it. and It does not fall into recognizable patterns. BUT the other half is the half it does not have;  it has no updatable memory apart from its token buffer of what it has just said. Long term context may be long term in coming.</p></td>
</tr>
<tr>
	<td>An AI system with an ongoing existence (no day is the repeat of another day as it currently is for all AI systems) at the level of a mouse.</td><td>NET 2030</td><td>I will need a whole new blog post to explain this...</td><td></td>
</tr>
<tr>
	<td>A robot that seems as intelligent, as attentive, and as faithful, as a dog.</td><td>NET 2048</td><td>This is so much harder than most people imagine it to be--many think we are already there; I say we are not at all there.</td><td></td>
</tr>
<tr>
	<td>A robot that has any real idea about its own existence, or the existence of humans in the way that a six year old understands humans.</td><td>NIML</td><td></td><td></td>
</tr>
</tbody>
</table>
<!-- #tablepress-31 from cache -->
<p><span><em>A Parting Shot</em></span></p>
<p>I recently read a research paper on humanoid robots working in built for human environments. It was based on the argument that the best form for a robot that is to operate in human environments is something tallish and skinny-ish, and probably dynamically balancing, with arms that can reach down to table tops etc., and with a sensor system that can look down from above, as that is what our human environments are optimized for. Here is the first paragraph of the paper:</p>
<p>The past decade has seen an explosion of research in humanoid robotics. The stated motivations for this work have varied widely. Many teams have concentrated on bipedal locomotion, some have been interested in human level social interactions, understanding human intelligence, modeling human learning capabilities and others have been more interested in entertainment. Some humanoid robots have had manipulation capabilities on static humanoid platforms and some of that work is aimed at dexterity, plus there has been simple two armed grasping on mobile humanoid platforms. Overall there has been very little work combining dexterous manipulation with humanoid robots, static or mobile–much of that which has appeared, has been concerned with dynamic tasks like pole balancing and juggling rather than manipulation, or has used teleoperated manipulation.</p>
<p>Apart from the weird references to pole balancing and juggling this all sounds pretty reasonable and consistent with what is happening today, and with recent history. &nbsp;In fact this is the very first paragraph of the very first paper in the very first issue of the very first volume of&nbsp;the <a href="https://www.worldscientific.com/worldscinet/IJHR" target="_blank" rel="noopener">International Journal of Humanoid Robotics</a>.</p>
<p>And it was published in 2004, with me as first author. &nbsp;Let me spell that out in case you thought there was a typo in the year. This is from a paper that I and my students and post-docs wrote in the year <em>two thousand and four</em>. Here is the beginning of the contents page for that first issue.</p>
<div class="page" title="Page 1">
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/IJHR.jpg" alt="" width="1663" height="858"></p>
<p>You can download the text of that paper <a href="https://people.csail.mit.edu/brooks/papers/brooks04sensing.pdf" target="_blank" rel="noopener">here</a>. The journal&nbsp;is now in its 21<sup>st</sup> year of operation, an on its 21<sup>st</sup> volume of issues and papers.</p>
<p>By the time this paper was written my research group at MIT had been working on and building humanoid robots for twelve years. This paper, about a robot named Cardea, was probably our sixth or seventh humanoid robot. [[In 2008 I started a company that built and shipped thousands of humanoid robots. The picture at the top of this post was taken in China with a line up of humanoids that we had built in Massachusetts and New Hampshire and sold to people in China (before a US initiated trade war with China <a href="https://www.wired.com/story/a-long-goodbye-to-baxter-a-gentle-giant-among-robots/" target="_blank" rel="noopener">put an end to it in 2018</a>…irony can be personally hard to take at times…).]]</p>
<p>The robot&nbsp;Cardea (Cardea was an ancient Roman goddess of door hinges and handles; these are still a challenge for modern robots…) was a two wheeled dynamically balancing robot &nbsp;that lived in a built-for-humans office environment. Cardea was able to open doors using existing door handles and then make its way through doors it had opened.</p>
<p><strong><span>Pro tip:</span></strong>&nbsp;Just because you heard about a new idea this last year or two doesn’t mean that people haven’t been working on that very same idea for decades. So temper your expectations that it must be about to transform the world. Ideas that transform the world take decades, or centuries of development, and plenty of people long before you have been just as excited about the idea and had thought it was on the verge of taking off. And none of us, including you and me, are likely to be special enough or lucky enough to come along at just the right time to see it all happen.</p>
<p>Like all modern humanoid robots Cardea did not walk in a way that used passive dynamics to store energy, and basically modulate the behavior of a passive mechanism that had only low energy input, which is how all animals walk. So, like all modern mobile humanoid robots (and legged robots in general) when things were going awry its control algorithms tried to recover by pumping in large amounts of energy very quickly and sometimes that didn’t quite work and the energy needed to go somewhere.</p>
<p>Cardea could be a little dangerous in those circumstances, if it fell on you having just increased its kinetic energy. Even the spring based deployment system for its stick-like legs that were engaged when it realized it was going to fall could be dangerous.</p>
<p>This is still a problem with all modern humanoid robots. That is why the tele-operated humanoids that were in the Tesla movie lot theater show a couple of months ago operated in two modes. When they all walked out the human guests were kept away from them. Once they stopped walking and were operating in a very different mode people were allowed to approach them, and then get fooled into thinking they were talking to an AI powered robot when they were really talking to a remote human operator. But the robot was no longer moving its feet, and no longer a source of physical danger as a result.</p>
</div>
<p><strong><span>Another pro tip:</span></strong>&nbsp;Don’t stand anywhere near a walking or balancing wheeled humanoid when they are moving or doing any task. I have had some near misses for myself with my own humanoids twenty years ago and more recently with some of the humanoids from new start ups. And more generally never be below any sort of walking robot, no matter how many legs it has, when it is walking up stairs.</p>
<h5>HUMAN SpaceFLIGHT</h5>
<p>The numbers of flights in 2024 was not much different from those in 2023 (I neglected to include the flights by China last year). &nbsp;It does not feel like a golden age of human spaceflight, though there were other highlights from SpaceX.</p>
<p><span><em>Orbital Crewed Flights</em></span></p>
<p>Three countries put 28 people into orbit in 2024, the United States launched 16 people on five flights and Russia and China launched 6 people each with two launches. So there were nine crewed orbital flights total. Two were private and seven were government flights.</p>
<p><span><em>The United States:</em></span>&nbsp;There were four US flights to the International Space Station, starting with the private Axion-3 mission with a crew of four on January 18<sup>th</sup>. The launch vehicle for this was a SpaceX Falcon 9, and the crew vehicle was a SpaceX Dragon. The remaining US flights to the ISS were paid for by NASA. Two of them were SpaceX flights, with four people on March 4<sup>th</sup>, the Crew-8 mission, and two people on board Crew-9 on October 25<sup>th</sup>. The remaining US flight to the ISS was the inaugural crewed flight of Boeing’s Starliner, launched on June 5<sup>th</sup> atop an Atlas V rocket with two people aboard. They are still stuck in space and will be for a few more months–see the section on Boeing below.</p>
<p>The other US mission was also a SpaceX launch and vehicle flight, this time known as Polaris Dawn. It was the second mission paid for by billionaire Jared Isaacman, with him as commander. There was a former US Air Force fighter pilot as mission pilot and two SpaceX employees as mission specialists, giving a total crew size of four. They stayed aloft for five days, launching on September 10<sup>th</sup>, This mission flew higher above Earth than any mission since Apollo 17, the last lunar landing mission, in 1972. Two of the crew “spacewalked” with their feet inside the Dragon capsule but with their bodies outside. This was the first private spacewalk ever. Now Isaacman has been tapped by the incoming US President to be the administrator of NASA.</p>
<p><span><em>Russia:</em></span>&nbsp;There were two Soyuz launches, each with three people, up and down, but different people coming back. The launch dates were March 23<sup>rd</sup> and September 11<sup>the</sup>. The six people that launched on Soyuz in 2024 were 3 Russian Cosmonauts 2 NASA Astronauts and one Belarusian commercial airline flight attendant who won a national competition with 3,000 applications. She was the only one not set for a long duration mission and was off the ground for slightly less than 14 days. So there were no space tourists per so, but the Belarusian flyer was most likely included as part of Russia’s efforts to keep in good favor with Belarus which has aided it in its war in Ukraine, and was certainly not part of the regular scientific program of the ISS.</p>
<p><span><em>China:</em></span>&nbsp;There were two flights of &nbsp;Shenzhou (a larger more modern version of Soyuz) that were crewed in 2024. &nbsp;Both flights were to the Tiangong Space Station and both took along three Taikonauts, first on April 25<sup>th</sup> and then on October 9<sup>th</sup>. &nbsp;Both crews were assigned long duration missions and now the crews are overlapping previous crews at Tiangong so it is now being continuously occupied. The first handover this year took about five days and the second about three and a half weeks. &nbsp;Both times there were six Taikonauts onboard Tiangong at the same time.</p>
<p><span><em>Suborbital Crewed Flights</em></span></p>
<p>There have been two companies providing space tourism flights on suborbital flights. Blue Origin launches a capsule on top of a reusable rocket, New Shepard, and the capsule lands using a parachute and a brief rocket blast right before hitting the ground (similar to how Soyuz lands). Virgin Galactic has a winged craft which is carried aloft by a bigger a jet engined airplane, it separates at high altitude within the atmosphere and rockets into space. It flies back and lands on a runway.</p>
<p>Both companies are run by billionaires who made their money in other businesses. &nbsp;Both billionaires have flown to space on their own craft.</p>
<p>Both companies have aimed to have regular launches with lots of tourists, but neither has gotten to that scale and so far only a very small number of the many people who have paid a substantial deposit have been able to fly.</p>
<p>Blue Origin had a failure with an uncrewed version of the vehicle in 2022 and only flew one flight in 2023 which was also uncrewed. This year they flew three crewed flights on May 19<sup>th</sup>, August 29<sup>th</sup>, and November 22<sup>nd</sup>, each with six passengers (the system is automated and requires no pilots). In 2021 and 2022 they also had three flights, so there has now been nine crewed flights total. The first two took four passengers and the remaining seven have had six passengers, so altogether they have flown 50 people above the Karman line, 100 kilometers above Earth. &nbsp;This is not yet a regular cadence, nor a large scale tourist business.</p>
<p>In 2024 Virgin Galactic had two flights, each with two crew from the company and four passengers. These flights were on January 26<sup>th</sup> and June 8<sup>th</sup>. Virgin Galactic flights are now on hiatus, awaiting a new bigger and better vehicle in about two years. &nbsp;Virgin Galactic has had a total of twelve flights since December 13th in 2018. &nbsp;Three have had two people on board and nine have had six people on board, for a total of sixty filled seats that have crossed the Karman line. The total number of different people is smaller as the two pilot seats on each flight have been occupied by a small number of people who have flown multiple times.</p>
<p>So, in 2024 thirty people went on suborbital flights, and altogether there have been 110 people on these commercial suborbital flights. Space tourism on suborbital flights has yet to take off in a regular or scaled way.</p>

<table id="tablepress-32">
<thead>
<tr>
	<th>Prediction<br>
[Space]</th><th>Date</th><th>2018 Comments</th><th>Updates</th>
</tr>
</thead>
<tbody>
<tr>
	<td>Next launch of people (test pilots/engineers) on a sub-orbital flight by a private company.</td><td><p>BY 2018</p></td><td></td><td></td>
</tr>
<tr>
	<td>A few handfuls of customers, paying for those flights.</td><td><p>NET 2020</p></td><td></td><td></td>
</tr>
<tr>
	<td>A regular sub weekly cadence of such flights.</td><td><p>NET 2022</p>BY 2026</td><td></td><td><span>20240101</span> <p>There were four flights in 2021, three in 2022, and seven, five with customers on board, in 2023--all of them by Virgin Glactic. Blue Origin did not fly in 2023. At this point 2026 is looking doubtful for regular flights every week.</p><span>20250101</span> <p>Now 2026 is looking impossible given the data from 2023 and 2024, and one of the two companies being on hiatus for all of 2025, and well into 2026.</p></td>
</tr>
<tr>
	<td>Regular paying customer orbital flights.</td><td>NET 2027</td><td>Russia offered paid flights to the ISS, but there were only 8 such flights (7 different tourists). They are now suspended indefinitely. </td><td><span>20240101</span><p>There were three paid flights in 2021, and one each in 2022, and 2023, with the latter being the Axiom 2 mission using SpaceX hardware. So not regular yet, and certainly not common.</p><span>20250101</span> <p>There were two paid flights in 2024.</p></td>
</tr>
<tr>
	<td>Next launch of people into orbit on a US booster.</td><td><p>NET 2019</p><p>BY 2021</p><p>BY 2022 (2 different companies)</p><br>
</td><td>Current schedule says 2018.</td><td><span>20240101</span><p>Both SpaceX and Boeing were scheduled to have crewed flights in 2018. SpaceX pulled it off in 2020, Boeing's Starliner did not fly at all in 2023, but is scheduled to launch with people onboard for the first time in April 2024.</p><span>20250101</span> <p>The second company did finally launch humans into orbit in June 2024, so it has happened three years later than I predicted and six years later than what had been promised when my prediction was made. Of course, everyone implicitly assumed that along with getting humans into space the companies would also be able to bring them back. Not so for Boeing.</p></td>
</tr>
<tr>
	<td>Two paying customers go on a loop around the Moon, launch on Falcon Heavy.</td><td><p>NET 2020</p></td><td>The most recent prediction has been 4th quarter 2018. That is not going to happen.</td><td><span>20240101</span><p>Starship launched twice in 2023 but didn't get to orbit either time. This is going to be well over six years later than the original  prediction by the CEO of SpaceX.</p><span>20250101</span> <p>The billionaire who signed up for this and paid a hefty deposit in 2017 gave up waiting and cancelled the contract in 2024. This fantasy is over, for now at least.</p></td>
</tr>
<tr>
	<td>Land cargo on Mars for humans to use at a later date<br>
</td><td><p>NET 2026</p></td><td>SpaceX has said by 2022. I think 2026 is optimistic but it might be pushed to happen as a statement that it can be done, rather than for an pressing practical reason.</td><td><span>20240101</span><p>I was way too optimistic, and bought into the overoptimistic hype of the CEO of SpaceX even though I added four years, doubling his estimated time frame.</p><span>20250101</span> <p>I can now call this as orbital mechanics and Hohmann transfer windows dictate that the cargo would need to have been launched a few months ago for it to get to Mars in 2025. It has not been launched.</p></td>
</tr>
<tr>
	<td>Humans on Mars make use of cargo previously landed there.</td><td>NET 2032</td><td>Sorry, it is just going to take longer than every one expects.</td><td></td>
</tr>
<tr>
	<td>First "permanent" human colony on Mars.</td><td>NET 2036</td><td>It will be magical for the human race if this happens by then. It will truly inspire us all.<br>
</td><td></td>
</tr>
<tr>
	<td>Point to point transport on Earth in an hour or so (using a BF rocket).</td><td>NIML</td><td>This will not happen without some major new breakthrough of which we currently have no inkling.<br>
</td><td></td>
</tr>
<tr>
	<td>Regular service of Hyperloop between two cities.</td><td><p>NIML</p></td><td>I can't help but be reminded of when Chuck Yeager described the Mercury program as "Spam in a can".<br>
</td><td><span>20240101</span><p>Calling this one 26 years early. As of today no-one is still working on this in an operating company.</p></td>
</tr>
</tbody>
</table>
<!-- #tablepress-32 from cache -->
<p><span><em>Boeing’s Starliner</em></span></p>
<p>First announced in 2010 Boeing’s Starliner was originally scheduled to fly a human crew in 2018. It carried out its second uncrewed flight in May 2022, and finally did make its first crewed flight on June 5<sup>th</sup>. The crew of two docked with the ISS, but there were problems with multiple gas thrusters for fine motion during the docking. The original plan was that the crew would stay on the ISS for about a week and then return to Earth for a touchdown on to hard soil (as all Russian and Chinese crewed missions end along with all Blue Origin sub-orbital flights).</p>
<p>The option of that return was considered, but the thrusters were on a section of the vehicle which is discarded along the way before the landing so there was no possibility of getting a look at the hardware back on Earth. &nbsp;So a program of tests while docked to the ISS was started delaying the crew return.</p>
<p>Eventually it was decided that it was too risky for the crew to return on the craft and so it returned empty on &nbsp;September&nbsp;7<sup>th</sup>, landing in New Mexico. As it happened, although there were more anomalies with the thrusters the crew would have landed safely had they been on board.</p>
<p>Now the crew was stranded in space with no designated ride home. It was decided to remove two crew from the Crew-9 launch and have the Starliner astronauts, Barry Wilmore and Sunita Williams, fly back on that SpaceX Dragon with the other two, which after additional delays is now scheduled to happen some time in March 2025. Their one week visit to the ISS will have stretched out to nine months by then.</p>
<p>Boeing has committed to fixing the problems with Starliner. The boosters that it uses are no longer being built, but there are five existing ones reserved for the five additional contracted flights that Boeing has with NASA. They are supposed to happen once per year.</p>
<p>We do not know at this point, but I think it would not be a huge surprise if Starliner never flies again.</p>
<p><span><span><i>SpaceX Falcon 9&nbsp;</i></span></span></p>
<p>Once again the Falcon 9 launch system has broken all sorts of records for number of launches and reuse.</p>
<p>During 2024 there were 132 single booster launches. &nbsp;For two of those flights no attempt was made to recover the first stage (there is a performance penalty for the primary payload in order to recover the first stage). One attempted recovery failed when the booster (on its 23<sup>rd</sup> flight) caught fire as it landed on the recovery barge. Another booster has since flown a total of 24 times.</p>
<p>In terms of mission success all but one of these flights succeeded; one failed when the second stage failed during re-ignition for adjusting the orbit.</p>
<p>There were also two Falcon Heavy, the three booster version, launches, both of which succeeded. One of the had successful landings for the two side boosters, but there was no attempt to recoer the central booster on that flight and no attempt to recover any of the three boosters on the other Heavy flight.</p>
<p>This brings the total number of launches of the single booster version to 417 along with 11 launches of the three booster Heavy version. &nbsp;These numbers are way beyond the number of launches for any other orbital booster. &nbsp;Additionally it is the only flying orbital system that is reusable at the moment, though &nbsp;Blue Origin and Rocket Lab both plan on joining the club soon.</p>
<p>It is worth, once again, looking at how long it has taken to get to a total (across both single booster and Heavy triple booster versions) of 428 launches, with only three failures to deliver the payload to where it was intended to go.</p>
<p>The first launch occured in June 2010, and there were a total of 4 launches in the first three years. &nbsp;The first successful booster recover happened on the 20th flight, in December 2015, five and a half years in. The first reuse of a booster occured in 2017, in the 8<sup>th</sup> year of the program.</p>
<p>Since 2021 there has been a steady increase in the number of launches per year,</p>

<table id="tablepress-33">
<thead>
<tr>
	<th>Year</th><th># of launches</th>
</tr>
</thead>
<tbody>
<tr>
	<td>2010</td><td>2</td>
</tr>
<tr>
	<td>2011</td><td>0</td>
</tr>
<tr>
	<td>2012</td><td>2</td>
</tr>
<tr>
	<td>2013</td><td>3</td>
</tr>
<tr>
	<td>2014</td><td>6</td>
</tr>
<tr>
	<td>2015</td><td>7</td>
</tr>
<tr>
	<td>2016</td><td>8</td>
</tr>
<tr>
	<td>2017</td><td>18<br>
</td>
</tr>
<tr>
	<td>2018</td><td>21</td>
</tr>
<tr>
	<td>2019</td><td>13</td>
</tr>
<tr>
	<td>2020</td><td>26</td>
</tr>
<tr>
	<td>2021</td><td>31</td>
</tr>
<tr>
	<td>2022</td><td>61</td>
</tr>
<tr>
	<td>2023</td><td>96</td>
</tr>
<tr>
	<td>2024</td><td>134</td>
</tr>
</tbody>
</table>
<!-- #tablepress-33 from cache -->
<p>SpaceX had previously gotten satellites to orbit with its first rocket, the Falcon 1. &nbsp;Falcon 9 has been a spectacular success. &nbsp;But it was not instantaneous. &nbsp;It took time to build from the cadence of launches, about 10 years before the hockey stick curve showed up. &nbsp;Deployment is never sudden but comes after a long build.</p>
<p><em><span>SpaceX Starship</span></em></p>
<p>Starship is SpaceX’s superheavy two stage rocket, designed to put 150 tons of payload into orbit, but also be able to go to the Moon or Mars. There is the booster which is designed only to work in Earth atmosphere with 33 Raptor engines both to get the second stage high enough and fast enough and to let the first stage have a controlled return to the launch site. The second stage, called Starship, is both a booster and the payload. &nbsp;It has three Raptor engines and three Raptor vacuum engines. The Raptor engines are designed to get the Starship into orbit after the first stage drops away, and to guide the Starship as it returns to its Earth launch site. The Raptor vacuum engines are meant for breaking out of Earth orbit and going to the Moon or Mars, and to do soft landings on those two bodies where there is no or almost no atmosphere.</p>
<p>In 2024 SpaceX made steady progress with four launches of the two stages coupled together. &nbsp;The first two launches lead to both stages blowing up.</p>
<p>The third and fourth launches were a big improvement. &nbsp;As with earlier flights they launched from the coast of Texas. In both cases the second stage did a reentry burn on it first orbit and then did a soft landing in a target zone in the Indian Ocean. &nbsp;In the third flight the main booster returned to the launch site and hovered next to the launch tower betweeen two giant arms which then captured it and the engines shot down successfully. It was sifficiently damaged during flight however, that it was not reusable. In the fourth flight there were health anomalies to the first stage was ditched in the Gulf of Mexico.</p>
<p>On the fourth flight there was both less heat shielding and much less damage from heat during reentry. This is definite forward progress. But it is still quite a long way from both being operational and both stages being reusable. And it is even further away from being human rated.</p>
<p>This is the vehicle that the CEO of SpaceX recently said would be launched to Mars and attempt a soft landing there. &nbsp;He also said that if successful the humans would fly to Mars on it in 2030. These are enormously ambitious goals just from a maturity of technology standpoint. The real show stopper however may be human physiology as evidence accumulates that humans would not survive three years (the minimum duration of a Mars mission, due to orbital mechanics) in space with current shielding practices and current lack of gravity on board designs. Those two challenges may take decades, or even centuries to overcome (recall that Leonardo Da Vinci had designs for flying machines that took centuries to be developed…).</p>
<p>The President of SpaceX may be taking a leaf out of the CEO’s always overly optimistic predictions. In November she said <a href="https://www.dailystar.co.uk/news/us-news/spacex-president-says-could-easily-34143695" target="_blank" rel="noopener">“I would not be surprised if we fly 400 Starship launches in the next four years”</a>. Looking at the success of Falcon 9 it is certainly plausible that I may live to see 400 Starship launches in a four year period, but I am quite confident that it will not happen in the&nbsp;<strong>next</strong> four years (2025 through 2028).</p>
<p><em><strong>One more thing.</strong></em> Back when I first made the predictions there had been an announcement by the CEO of SpaceX that in 2018 the company was under contract to send a very rich paying customer in a trip around the moon in 2018, launched on a Falcon Heavy. I was completely skeptical. Over the years the date got pushed back and pushed back, and the proposed flight vehicle was changed to be Starship. As we all know the flight of the Japanese billionaire around the Moon still hasn’t happened. In 2024 Yusaku Maezawa finally gave up waiting and <a href="https://www.space.com/japanese-billionaire-cancels-spacex-starship-moon-dearmoon-flight" target="_blank" rel="noopener">cancelled the contract</a>.</p>
<p><span><em>NASA Artemis</em></span></p>
<p>NASA’s plan is that the second Artemis mission, using the Orion Capsule, Artemis II, will fly to the Moon with four people aboard, the first crewed Artemis flight. An uncrewed flight of Orion around the Moon flew in 2022. &nbsp;The crewed flight was scheduled to launch in May 2024, but it was first delayed by six months and then a little more and in the last year it has slipped another full year. It is now scheduled to fly in April 2026.</p>
<p>Artemis III was scheduled to launch in 2025 with a return to the surface of the Moon. However that relied on using a Starship (itself refueled in LEO by 14 (yes, <em><strong>fourteen</strong></em>!!) other Starship launches) to land there. &nbsp;No one any longer believes that schedule, and willlikely delay a few years, given where Starship is in its development and current capability. &nbsp;The officieal schedule says mid 2027, but that seems unlikely.</p>
<p>You can find the architecture of the Artemis III mission at this <a href="https://www.nasa.gov/missions/artemis/artemis-iii/" target="_blank" rel="noopener">website</a>.</p>
<p><span><em>Blue Origin Orbital BE-4 Engines and New Glenn</em></span></p>
<p>The suborbital tourist flights that Blue Origin operates are not its main business. It has ambitions to compete head to head with SpaceX. Another billionaire vs billionaire competition.</p>
<p>It has developed the BE-4 engine designed to fly 100 times, and to power the first stage of its massive New Glenn rocket (see below). &nbsp;But in the meantime it has started selling the BE-4 to ULA (United Launch Alliance) to power their Vulcan Centaur heavy launch vehicle. It’s first stage uses two BE-4 engines, along with a variable number of solid fuel strap ons.</p>
<p>Vulcan Centaur flew two times in 2024 and the BE-4 engines worked perfectly both times, on January 8<sup>th</sup> and again on October 4<sup>th</sup>. This is a solid validation of the engine’s capabilities.</p>
<p>Blue Origin’s own first orbital class rocket, New Glenn, is massive, and comparable to the Flacon Heavy (three boosters) rather than the Falcon 9 in capability. &nbsp; It has been in development for a long time, but saw its first visits to a launch pad, fully stacked in 2024. The first stage uses seven BE-4 engines, and is intended to land on a barge and be fully reusable. The second stage uses two BE-3U engines, a variant of the single engine used on their New Shepard sub-orbital space tourism vehicle. There is a project underway to make a fully reusable version of the second stage.</p>
<p>Launch seems imminent. &nbsp;Here it is at the launch pad in November 2024.</p>
<p><img src="http://rodneybrooks.com/wp-content/uploads/2024/12/image.jpeg" alt="" width="571" height="876"></p>
<p>On Friday December 27<sup>th</sup>, 2024, it was fully fueled in both stages and <a href="https://www.nytimes.com/2024/12/27/science/new-glenn-blue-origin.html" target="_blank" rel="noopener">went through a countdown and fired its seven BE-4 engines for 24 seconds</a>. Now it will leave the pad to have its payload installed. The launch could be as early January 6<sup>th</sup>. &nbsp;The very first launch will be an all up affair, attempting to get something to orbit and land the booster on its first flight. This is a very different development approach to that used by SpaceX.</p>
<p><span><em>Let’s Continue a Noble Tradition!</em></span></p>
<p>The billionaire founders of both Virgin Galactic and Blue Origin had faith in the systems they had created. They both personally flew on the first operational flights of their sub-orbital launch systems. They went way beyond simply talking about how great their technology was, they believed in it, and flew in it.</p>
<p>Let’s hope this tradition continues. Let’s hope the billionaire founder/CEO of SpaceX will be onboard the first crewed flight of Starship to Mars, and that it happens sooner than I expect. We can all cheer for that.</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article><!-- #post-## -->

<!-- .comments-area -->

	
		</main><!-- .site-main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Candy Crush, Tinder, MyFitnessPal: Apps hijacked to spy on location (177 pts)]]></title>
            <link>https://www.wired.com/story/gravy-location-data-app-leak-rtb/</link>
            <guid>42651115</guid>
            <pubDate>Fri, 10 Jan 2025 00:00:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/gravy-location-data-app-leak-rtb/">https://www.wired.com/story/gravy-location-data-app-leak-rtb/</a>, See on <a href="https://news.ycombinator.com/item?id=42651115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Some of the world’s most popular apps are likely being co-opted by rogue members of the advertising industry to harvest sensitive location data on a massive scale, with that data ending up with a location data company whose subsidiary has previously sold global location data to US law enforcement.</p><p>The thousands of apps, <a data-offer-url="https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/&quot;}" href="https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/" rel="nofollow noopener" target="_blank">included in hacked files</a> from location data company Gravy Analytics, include everything from games like <em>Candy Crush</em> and dating apps like Tinder to pregnancy tracking and religious prayer apps across both Android and iOS. Because much of the collection is occurring through the advertising ecosystem—not code developed by the app creators themselves—this data collection is likely happening without users’ or even app developers’ knowledge.</p><p>“For the first time publicly, we seem to have proof that one of the largest data brokers selling to both commercial and government clients appears to be acquiring their data from the online advertising ‘bid stream,’” rather than code embedded into the apps themselves, Zach Edwards, senior threat analyst at cybersecurity firm Silent Push and who has followed the location data industry closely, tells 404 Media after reviewing some of the data.</p><p>The data provides a rare glimpse inside the world of real-time bidding (RTB). Historically, location data firms <a data-offer-url="https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/&quot;}" href="https://www.vice.com/en/article/us-military-location-data-xmode-locate-x/" rel="nofollow noopener" target="_blank">paid app developers</a> to include bundles of code that collected the location data of their users. Many companies have turned instead to <a data-offer-url="https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co&quot;}" href="https://www.wsj.com/tech/cybersecurity/how-ads-on-your-phone-can-aid-government-surveillance-943bde04?ref=404media.co" rel="nofollow noopener" target="_blank">sourcing location information through the advertising ecosystem</a>, where companies bid to place ads inside apps. But a side effect is that data brokers can listen in on that process and harvest the location of peoples’ mobile phones.</p><p>“This is a nightmare scenario for privacy, because not only does this data breach contain data scraped from the RTB systems, but there's some company out there acting like a global honey badger, doing whatever it pleases with every piece of data that comes its way,” Edwards says.</p><p>Included in the hacked Gravy data are tens of millions of mobile phone coordinates of devices inside the US, Russia, and Europe. Some of those files also reference an app next to each piece of location data. 404 Media extracted the app names and built a list of mentioned apps.</p><p>The list includes dating sites Tinder and Grindr; massive games such as <em>Candy Crush</em>, <em>Temple Run</em>, <em>Subway Surfers</em>, and <em>Harry Potter: Puzzles &amp; Spells</em>; transit app Moovit; My Period Calendar &amp; Tracker, a period-tracking app with more than 10 million downloads; popular fitness app MyFitnessPal; social network Tumblr; Yahoo’s email client; Microsoft’s 365 office app; and flight tracker Flightradar24. The list also mentions multiple religious-focused apps such as Muslim prayer and Christian Bible apps, various pregnancy trackers, and many VPN apps, which some users may download, ironically, in an attempt to protect their privacy.</p><p>The full list can be found <a href="https://docs.google.com/spreadsheets/d/1Ukgd0gIWd9gpV6bOx2pcSHsVO6yIUqbjnlM4ewjO6Cs/edit?usp=sharing">here</a>. Multiple security researchers <a data-offer-url="https://pastejustit.com/atnbotturr" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://pastejustit.com/atnbotturr&quot;}" href="https://pastejustit.com/atnbotturr" rel="nofollow noopener" target="_blank">have published</a> <a data-offer-url="https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d&quot;}" href="https://gist.github.com/fs0c131y/f498b21cba9ee23956fc7d7629262e9d" rel="nofollow noopener" target="_blank">other lists</a> of apps included in the data, of varying sizes. Our version is relatively larger because it includes both Android and iOS apps, and we decided to keep duplicate instances of the same app that had slight name variations to make it easier for readers to search for apps they have installed.</p><p>Although this dataset came from an apparent hack of Gravy, it is not clear whether Gravy collected this location data itself or sourced it from another company, or which location company ultimately owns it or is licensed to use it.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Much of the location data attached to these app names does not have a time stamp. But there are indications it dates from 2024. One of the apps listed is <em>Call of Duty: Mobile</em>, and specifically its Season 5 iteration, <a data-offer-url="https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement&quot;}" href="https://www.callofduty.com/blog/2024/05/call-of-duty-mobile-season-5-digital-dusk-operators-map-challenge-announcement" rel="nofollow noopener" target="_blank">which launched in May 2024</a>.</p><p>Gravy is a company that powers much of the rest of the location data industry. It collates mobile phone location data from various sources, then sells that to commercial companies or, through its subsidiary Venntel, to US government agencies. Norwegian outlet NRK and I <a data-offer-url="https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/&quot;}" href="https://www.vice.com/en/article/ice-dhs-fbi-location-data-venntel-apps/" rel="nofollow noopener" target="_blank">previously revealed the flow of location data</a> from a handful of ordinary apps to Gravy and then to Venntel. Venntel’s clients have included <a data-offer-url="https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600&quot;}" href="https://www.wsj.com/articles/federal-agencies-use-cellphone-location-data-for-immigration-enforcement-11581078600" rel="nofollow noopener" target="_blank">Immigration and Customs Enforcement, Customs and Border Protection</a>, <a data-offer-url="https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/&quot;}" href="https://www.vice.com/en/article/irs-investigation-location-data-no-warrant-venntel/" rel="nofollow noopener" target="_blank">the IRS</a>, <a data-offer-url="https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/&quot;}" href="https://theintercept.com/2020/06/24/fbi-surveillance-social-media-cellphone-dataminr-venntel/" rel="nofollow noopener" target="_blank">the FBI</a>, <a data-offer-url="https://www.vice.com/en/article/dea-venntel-location-data/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/dea-venntel-location-data/&quot;}" href="https://www.vice.com/en/article/dea-venntel-location-data/" rel="nofollow noopener" target="_blank">and the Drug Enforcement Administration</a>.</p><p>Venntel has also provided the underlying data for another government-bought surveillance tool called Locate X. <a data-offer-url="https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/&quot;}" href="https://www.404media.co/inside-the-u-s-government-bought-tool-that-can-track-phones-at-abortion-clinics/" rel="nofollow noopener" target="_blank">404 Media and a group of other outlets</a> showed last year how that tool, made by a company called Babel Street, could be used to monitor visitors to out-of-state abortion clinics.</p><p>But the newly hacked data shows for the first time just how many apps could be part of a location data supply chain, even if their developers are not aware of it. Most app developers and companies included in the list did not respond to a request for comment. Flightradar24 said in an email that it had never heard of Gravy, but that it does display ads, which “help keep Flightradar24 free.”</p><p>Tinder said in an email that “Tinder takes safety and security very seriously. We have no relationship with Gravy Analytics and have no evidence that this data was obtained from the Tinder app” but did not answer questions about ads inside the app.</p><p>Muslim Pro, one of the Muslim prayer apps included in the list, said in an email that it was not aware of Gravy. “Yes, we display ads through several ad networks to support the free version of the app. However, as mentioned above, we do not authorize these networks to collect location data of our users,” the email said. That does not necessarily mean that a member of the advertising ecosystem can’t extract such data, though. (In 2020 <a data-offer-url="https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/&quot;}" href="https://www.vice.com/en/article/muslim-pro-location-data-military-xmode/" rel="nofollow noopener" target="_blank">I revealed Muslim Pro was selling its users’ location data</a> to a company called X-Mode, whose clients included US military contractors; Muslim Pro stopped the practice after my reporting.)</p><p>A Grindr spokesperson told 404 Media in an email that “Grindr has never worked with or provided data to Gravy Analytics. We do not share data with data aggregators or brokers and have not shared geolocation with ad partners for many years. Transparency is at the core of our privacy program, therefore the third parties and service providers we work with are listed here <a data-offer-url="https://www.grindr.com/privacy-policy/third-parties/en-us" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.grindr.com/privacy-policy/third-parties/en-us&quot;}" href="https://www.grindr.com/privacy-policy/third-parties/en-us" rel="nofollow noopener" target="_blank">on our website</a>.” Grindr <a data-offer-url="https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information&quot;}" href="https://lamag.com/news/grindr-sold-its-user-data-for-years-revealing-personal-information" rel="nofollow noopener" target="_blank">was previously found to have allowed data brokers</a> to obtain its users’ location data.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>It’s important that the data appears to be sourced through real-time bidding, because that dictates who is responsible (rogue members of the advertising industry and the tech giants that facilitate that industry), how users can protect themselves (attempting to block ads), and the fact that massive app publishers may not even be aware their users’ data is being harvested and therefore might not know how to stop it. An app developer will know if it implemented location-data-gathering code itself. It might not know that some company, somewhere, is silently listening in on the advertising process and siphoning data from their app.</p><p>Surveillance firms can obtain RTB data by <a data-offer-url="https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co&quot;}" href="https://www.bloomberg.com/news/articles/2023-05-11/surveillance-company-turns-ad-data-into-government-tracking-tool?ref=404media.co" rel="nofollow noopener" target="_blank">acquiring ad tech companies</a> and posing as prospective advertisers. The spy-run location data company doesn’t need to successfully place an ad; instead, it is able to gather data on devices by simply being plugged into that industry. Location data in this case can also include a users’ IP address, which is then geolocated to give their coarse location.</p><p>Last January, <a data-offer-url="https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/&quot;}" href="https://www.404media.co/inside-global-phone-spy-tool-patternz-nuviad-real-time-bidding/" rel="nofollow noopener" target="_blank">404 Media reported</a> on an Israeli surveillance company called Patternz, which was sourcing masses of location data through the RTB process.</p><p>In an exposed training video, Patternz showed some of the popular apps it got location data from: 9GAG, Kik, sports app FUTBIN, caller ID apps such as CallApp and Truecaller; and various word, sudoku, and solitaire puzzle games. Every one of these apps are also in the Gravy data. This suggests that Gravy, or wherever Gravy got that data from, also sourced it from interacting with the advertising system rather than location-tracking code baked into the apps.</p><p>404 Media shared some of the location data with another security researcher with knowledge of the advertising and location data industries. “It appears that at least some of this data would likely have been sourced from advertising related, real-time bidding,” Krzysztof Franaszek, founder of Adalytics, a digital forensics firm, told 404 Media after reviewing the data. He pointed out some of the user-agents in the file, which show how a user’s device connected to a service, referenced “afma-sdk.” That is a string <a href="https://ads-developers.googleblog.com/2022/07/use-new-google-mobile-ads-sdk.html">used by Google’s Mobile Ads SDK</a> (software development kit). In other words, in some cases, it is Google’s advertising platform that is delivering the ads that are eventually leading to this tracking by outside companies and potentially government contractors.</p><p>Google did not respond to multiple requests for comment for this article. Neither did Apple.</p><p>Franaszek also says that “a significant amount of this geolocation dataset appears to be inferred by IP address to geolocation lookups, meaning the vendor or their source is deriving the user's geolocation by checking their IP address rather than by using GNSS [Global Navigation Satellite System]/GPS data. That would suggest that the data is not being sourced entirely from a location data SDK.”</p><p>“What we’re seeing here in this data appears to me to be a huge diversity of apps,” Edwards says. “That’s not what you see from an SDK ingestion; that’s what you see from bulk RTB ingestions.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In December <a href="https://www.ftc.gov/news-events/news/press-releases/2024/12/ftc-takes-action-against-mobilewalla-collecting-selling-sensitive-location-data">the Federal Trade Commission banned another location data company</a> called Mobilewalla from collecting consumer data “from online advertising auctions for purposes other than participating in those auctions.” In other words, the agency banned Mobilewalla from participating in the RTB process for building datasets on peoples’ devices. The <a data-offer-url="https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/&quot;}" href="https://www.404media.co/ftc-bans-location-data-company-that-powers-the-surveillance-ecosystem/" rel="nofollow noopener" target="_blank">FTC also said Venntel and Gravy collected data</a> without obtaining user consent, ordered the company to delete historical location data, and banned it from selling data related to sensitive areas like health clinics and places of worship, except in “limited circumstances” involving national security or law enforcement.</p><p>404 Media has verified the hacked Gravy data in various ways. Some files include credentials for Gravy’s Snowflake instances, a data warehousing tool. 404 Media checked that the URLs in the hacked files do correspond to real Snowflake instances. One file called “users” contains a long list of companies. Some of these firms denied having any relationship with Gravy; Cuebiq, another location data firm mentioned in the file, told 404 Media it “routinely evaluates available data in the market to determine if they are an appropriate fit for our business. Most do not make it past the evaluation stage to production, as was the case here. Cuebiq tested a limited data sample, which was never made available to our customers, and the data was deleted at the end of the limited trial.”</p><p>404 Media also sent a section of the data to another data broker called Datonics. “We investigated the matter described in your email, and the segment IDs in those files are those of Gravy, not Datonics,” the company said in an email.</p><p>Unacast, <a data-offer-url="https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html&quot;}" href="https://www.prnewswire.com/news-releases/gravy-analytics-and-unacast-merge-to-become-leader-in-location-data-and-insights-302000184.html" rel="nofollow noopener" target="_blank">which merged with Gravy in 2023</a>, did not respond to multiple requests for comment, both on the hack and whether it or any of its suppliers have derived location data from the real-time bidding process.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Disappointed with the TVs at CES 2025 (187 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/</link>
            <guid>42650855</guid>
            <pubDate>Thu, 09 Jan 2025 23:20:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/">https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/</a>, See on <a href="https://news.ycombinator.com/item?id=42650855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
            <article data-id="2069712">
  
  <header>
  <div>
    <div>
      <div>
        <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    Won't someone please think of the viewer?
  </span>
</p>
      </div>

      

      <p>
        Op-ed: TVs miss opportunity for real improvement by prioritizing corporate needs.
      </p>

      
    </div>

    <div>
    
    <p>
      The TV industry is hitting users over the head with AI and other questionable gimmicks 

              <span>
          Credit:

          
          Getty

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>If you asked someone what they wanted from <a href="https://arstechnica.com/gadgets/2024/12/buying-a-tv-in-2025-expect-lower-prices-more-ads-and-an-os-war/">TVs released in 2025</a>, I doubt they'd say "more software and AI." Yet, if you look at what TV companies have planned for this year, which is being primarily promoted at the CES technology trade show in Las Vegas this week, software and AI are where much of the focus is.</p>
<p>The trend reveals the implications of TV brands increasingly viewing themselves as software rather than hardware companies, with their products being customer data rather than TV sets. This points to an alarming future for smart TVs, where even premium models sought after for top-end image quality and hardware capabilities are stuffed with unwanted gimmicks.</p>
<h2>LG’s remote regression</h2>
<p>LG has long made some of the best—and most expensive—TVs available. Its OLED lineup, in particular, has appealed to people who use their TVs to watch Blu-rays, enjoy HDR, and the like. However, some features that LG is introducing to high-end TVs this year seem to better serve LG’s business interests than those users' needs.</p>
<p>Take the new remote. Formerly known as the Magic Remote, LG is calling the 2025 edition the AI Remote. That is already likely to dissuade people who are skeptical about AI marketing in products (<a href="https://www.tandfonline.com/doi/full/10.1080/19368623.2024.2368040">research suggests</a> there are many such people). But the more immediately frustrating part is that the new remote doesn’t have a dedicated button for switching input modes, as previous remotes from LG and countless other remotes do.</p>
<figure>
    <p><img width="640" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-640x360.jpg" alt="LG AI remote" decoding="async" loading="lazy" srcset="https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-640x360.jpg 640w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1024x576.jpg 1024w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-768x432.jpg 768w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1536x864.jpg 1536w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-384x216.jpg 384w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1152x648.jpg 1152w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-980x551.jpg 980w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote-1440x810.jpg 1440w, https://cdn.arstechnica.net/wp-content/uploads/2025/01/LG-AI-remote.jpg 1920w" sizes="auto, (max-width: 640px) 100vw, 640px">
                  </p>
          <figcaption>
        <div><p>
      LG's AI Remote.

              <span>
          Credit:

                      <a href="https://www.youtube.com/watch?v=z_KexjTLETo" target="_blank">
          
          Tom's Guide/YouTube

                      </a>
                  </span>
          </p></div>
      </figcaption>
      </figure>

<p>To use the AI Remote to change the TV’s input—a common task for people using their sets to play video games, watch Blu-rays or DVDs, connect their PC, et cetera—you have to long-press the Home Hub button. Single-pressing that button brings up a dashboard of webOS (the operating system for LG TVs) apps. That functionality isn't immediately apparent to someone picking up the remote for the first time and detracts from the remote’s convenience.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>By overlooking other obviously helpful controls (play/pause, fast forward/rewind, and numbers) while including buttons dedicated to things like LG's free ad-supported streaming TV (FAST) channels and Amazon Alexa, LG missed an opportunity to update its remote in a way centered on how people frequently use TVs. That said, it feels like user convenience didn't drive this change. Instead, LG seems more focused on getting people to use webOS apps. LG can monetize app usage through, i.e., getting a cut of streaming subscription sign-ups, selling <a href="https://arstechnica.com/gadgets/2024/09/lg-tvs-continue-down-advertising-rabbit-hole-with-new-screensaver-ads/">ads on webOS</a>, and selling and<a href="https://arstechnica.com/gadgets/2024/10/streaming-industry-has-unprecedented-surveillance-manipulation-capabilities/">&nbsp;leveraging user data</a>.</p>
<h2>Moving from hardware provider to software platform</h2>
<p>LG, like many other TV OEMs, has been growing its ads and data business. Deals with data analytics firms like Nielsen give it more incentive to acquire customer data. Declining TV margins and rock-bottom prices from budget brands (like Vizio and Roku, which sometimes lose money on TV hardware sales and make up for the losses through ad sales and data collection) are also pushing LG's software focus. In the case of the AI Remote, software prioritization comes at the cost of an oft-used hardware capability.</p>
<p>Further demonstrating its motives, in September 2023, LG announced intentions to "become a media and entertainment platform company" by offering "services" and a "collection of curated content in products, including LG OLED and LG QNED TVs." At the time, the South Korean firm said it would invest 1 trillion KRW (about $737.7 million) into its webOS business through 2028.</p>
<p>Low TV margins, improved TV durability, market saturation, and broader economic challenges are all serious challenges for an electronics company like LG and have pushed LG to explore alternative ways to make money off of TVs. However, after paying four figures for TV sets, LG customers shouldn't be further burdened to help LG accrue revenue.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          

<h2>Google TVs gear up for subscription-based features</h2>
<p>There are numerous TV manufacturers, including Sony, TCL, and Philips, relying on Google software to power their TV sets. Numerous TVs announced at CES 2025 will come with what Google calls Gemini Enhanced Google Assistant. The idea that this is something that people using Google TVs have requested is somewhat contradicted by Google Assistant interactions with TVs thus far being “somewhat limited,” per a <a href="https://www.lowpass.cc/p/google-tv-far-field-microphones-ces-gemini">Lowpass</a> report.</p>
<p>Nevertheless, these TVs are adding far-field microphones so that they can hear commands directed at the voice assistant. For the first time, the voice assistant will include Google’s generative AI chatbot, Gemini, this year—another feature that TV users don’t typically ask for. Despite the lack of demand and the privacy concerns associated with microphones that can pick up audio from far away even when the TV is off, companies are still loading 2025 TVs with far-field mics to support Gemini. Notably, these TVs will likely allow the mics to be disabled, like you can with <a href="https://arstechnica.com/gadgets/2022/09/amazons-self-branded-tvs-get-fancier-with-quantum-dots-local-dimming/">other TVs using far-field mics.</a> But I still ponder about features/hardware that could have been implemented instead.</p>
<p>Google is also working toward having people pay a subscription fee to use Gemini on their TVs, <a href="https://www.pcworld.com/article/2568513/google-hopes-youll-pay-for-an-ai-tv-assistant-someday.html">PCWorld</a> reported.</p>
<p>“For us, our biggest goal is to create enough value that yes, you would be willing to pay for [Gemini],” Google TV VP and GM Shalini Govil-Pai told the publication.</p>
<p>The executive pointed to future capabilities for the Gemini-driven Google Assistant on TVs, including asking it to “suggest a movie like <em>Jurassic Park </em>but suitable for young children” or to show “Bollywood movies that are similar to <em>Mission: Impossible</em>.”</p>
<p>She also pointed to future features like showing weather, top news stories, and upcoming calendar events when someone is near the TV, showing AI-generated news briefings, and the ability to respond to questions like “explain the solar system to a third-grader” with text, audio, and YouTube videos.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>But when people have desktops, laptops, tablets, and phones in their homes already, how helpful are these features truly? Govil-Pai admitted to PCWorld that “people are not used to” using their TVs this way “so it will take some time for them to adapt to it.” With this in mind, it seems odd for TV companies to implement new, more powerful microphones to support features that Google acknowledges aren't in demand. I’m not saying that tech companies shouldn’t get ahead of the curve and offer groundbreaking features that users hadn’t considered might benefit them. But already planning to monetize those capabilities—with a subscription, no less—suggests a prioritization of corporate needs.</p>
<h2>Samsung is hungry for AI</h2>
<p>People who want to use their TV for cooking inspiration often turn to cooking shows or online cooking videos. However, Samsung wants people to use its TV software to identify dishes they want to try making.</p>
<p>During CES, Samsung announced Samsung Food for TVs. The feature leverages Samsung TVs’ AI processors to identify food displayed on the screen and recommend relevant recipes. Samsung introduced the capability in 2023 as an iOS and Android app after buying the app Whisk in 2019. As noted by <a href="https://techcrunch.com/2025/01/05/samsungs-new-tvs-can-find-recipes-for-dishes-in-shows/">TechCrunch</a>, though, <a href="https://www.tomsguide.com/ai/i-gave-claude-chatgpt-and-gemini-a-photo-of-some-ingredients-to-see-which-came-up-with-the-best-recipe-heres-the-results">other AI tools</a> for providing recipes based on food images <a href="https://www.cnet.com/tech/services-and-software/tired-of-eating-out-i-tried-this-recipe-generating-ai-tool-to-create-a-restaurant-meal-at-home/">are flawed</a>.</p>
<p>So why bother with such a feature? You can get a taste of Samsung’s motivation from its CES-announced deal with Instacart that lets people order off Instacart from Samsung smart fridges that support the capability. Samsung Food on TVs can show users the progress of food orders placed via the Samsung Food mobile app on their TVs. Samsung Food can also create a shopping list for recipe ingredients based on what it knows (using cameras and AI) is in your (supporting) Samsung fridge. The feature also requires a Samsung account, which allows the company to gather more information on users.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Other software-centric features loaded into Samsung TVs this year include a dedicated AI button on the new TVs’ remotes, the ability to use gestures to control the TV but only if you’re wearing a Samsung Galaxy Watch, and AI Karaoke, which lets people sing karaoke using their TVs by stripping vocals from music playing and using their phone as a mic.</p>
<p>Like LG, Samsung has shown growing interest in ads and data collection. In May, for example, it expanded its automatic content recognition tech to track ad exposure on streaming services viewed on its TVs. It also has an ads analytics partnership with Experian.</p>

<h2>Large language models on TVs</h2>
<p>TVs are mainstream technology in most US homes. Generative AI chatbots, on the other hand, are emerging technology that many people have yet to try. Despite these disparities, LG and Samsung are incorporating Microsoft’s Copilot chatbot into 2025 TVs.</p>
<p>LG claims that Copilot will help its TVs “understand conversational context and uncover subtle user intentions,” adding: “Access to Microsoft Copilot further streamlines the process, allowing users to efficiently find and organize complex information using contextual cues. For an even smoother and more engaging experience, the AI chatbot proactively identifies potential user challenges and offers timely, effective solutions.”</p>
<p>Similarly, Samsung, which is also adding Copilot to some of its smart monitors, said in its announcement that Copilot will help with “personalized content recommendations.” Samsung has also said that Copilot will help its TVs understand strings of commands, like increasing the volume and changing the channel, <a href="https://www.cnet.com/tech/home-entertainment/samsungs-2025-tvs-get-all-the-ai-extras-nobody-asked-for/">CNET</a> noted. Samsung said it intends to work with additional AI partners, namely Google, but it's unclear why it needs multiple AI partners, especially when it hasn’t yet seen how people use large language models on their TVs.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<h2>TV-as-a-platform</h2>
<p>To be clear, this isn't a condemnation against new, unexpected TV features. This also isn't a censure against new TV apps or the usage of AI in TVs.</p>
<p><a href="https://arstechnica.com/gadgets/2024/04/ai-marketing-hype-is-coming-for-your-favorite-gadgets/">AI marketing hype </a>is real and misleading regarding the demand, benefits, and possibilities of AI in consumer gadgets. However, there are some cases when innovative software, including AI, can improve things that TV users not only care about but actually want or need. For example, some TVs use AI for things like trying to optimize sound, color, and/or brightness, including based on current environmental conditions or upscaling. This week, Samsung announced AI Live Translate for TVs. The feature is supposed to be able to translate foreign language closed captions in real time, providing a way for people to watch more international content. It's a feature I didn't ask for but can see being useful and changing how I use my TV.</p>
<p>But a lot of this week's TV announcements underscore an alarming TV-as-a-platform trend where TV sets are sold as a way to infiltrate people's homes so that apps, <a href="https://arstechnica.com/gadgets/2024/12/tcl-tvs-will-use-films-made-with-generative-ai-to-push-targeted-ads/">AI</a>, and <a href="https://arstechnica.com/gadgets/2024/11/an-ad-giant-wants-to-control-your-next-tvs-operating-system/">ads</a> can be pushed onto viewers. Even high-end TVs are moving in this direction and amplifying features with questionable usefulness, effectiveness, and privacy considerations. Again, I can't help but wonder what better innovations could have come out this year if more R&amp;D was directed toward hardware and other improvements that are more immediately rewarding for users than karaoke with AI.</p>
<p>The TV industry is facing economic challenges, and, understandably, TV brands are seeking creative solutions for making money. But for consumers, that means paying for features that you’re likely to ignore. Ultimately, many people just want a TV with amazing image and sound quality. Finding that without having to sift through a bunch of fluff is getting harder.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/scharonharding/"><img src="https://arstechnica.com/wp-content/uploads/2021/09/scharon-harding-headshot.jpg" alt="Photo of Scharon Harding"></a></p>
  </div>

  <div>
    

    <p>
      Scharon is a Senior Technology Reporter at Ars Technica writing news, reviews, and analysis on consumer gadgets and services. She's been reporting on technology for over 10 years, with bylines at Tom’s Hardware, Channelnomics, and CRN UK.
    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/01/ces-2025-teases-alarming-smart-tv-future-loaded-with-unwanted-software-gimmicks/#comments" title="117 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    117 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/ai/2025/01/how-i-program-with-llms/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/01/ai-code-buddy-768x432.jpg" alt="Listing image for first story in Most Read: How I program with LLMs" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>


  

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Soldering the Tek way (184 pts)]]></title>
            <link>https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/</link>
            <guid>42650561</guid>
            <pubDate>Thu, 09 Jan 2025 22:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/">https://hackaday.com/2025/01/09/retrotechtacular-soldering-the-tek-way/</a>, See on <a href="https://news.ycombinator.com/item?id=42650561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
        <main id="main" role="main">

        
            
<article itemscope="" itemtype="http://schema.org/Article" id="post-752809">
    <!-- .entry-header -->

    <div itemprop="articleBody">
        <p>For a lot of us, soldering just seems to come naturally. But if we’re being honest, none of us was born with a soldering iron in our hand — ouch! — and if we’re good at soldering now, it’s only thanks to good habits and long practice. But what if you’re a company that lives and dies by the quality of the solder joints your employees produce? How do you get them to embrace the dark art of soldering?</p>
<p>If you’re Tektronix in the late 1970s and early 1980s, the answer is simple: make <a href="https://youtu.be/yZSveVpgmIM" target="_blank">in-depth training videos that teach people to solder the Tek way</a>. The first video below, from 1977, is aimed at workers on the assembly line and as such concentrates mainly on the practical aspects of making solid solder joints on PCBs and mainly with through-hole components. The video does have a bit of theory on soldering chemistry and the difference between eutectic alloys and other tin-lead mixes, as well as a little about the proper use of silver-bearing solders. But most of the time is spent discussing the primary tool of the trade: the iron. Even though the film is dated and looks like a multi-generation dupe from VHS, it still has a lot of valuable tips; we’ve been soldering for decades and somehow never realized that cleaning a tip on a wet sponge is so effective because the sudden temperature change helps release oxides and burned flux. The more you know.</p>
<p><a href="https://youtu.be/jMchFqu3Jx0" target="_blank">The second video below</a> is aimed more at the Tek repair and rework technicians. It reiterates a lot of the material from the first video, but then veers off into repair-specific topics, like effective desoldering. Pro tip: Don’t use the “Heat and Shake” method of desoldering, and wear those safety glasses. There’s also a lot of detail on how to avoid damaging the PCB during repairs, and how to fix them if you do manage to lift a trace. They put a fair amount of emphasis on the importance of making repairs look good, especially with bodge wires, which should be placed on the back of the board so they’re not so obvious. It makes sense; Tek boards from the era are works of art, and you don’t want to mess with that.</p>

<p><iframe title="Tektronix Solder And Its Application In Electrical Assembly 1977" width="800" height="450" src="https://www.youtube.com/embed/yZSveVpgmIM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p><iframe title="Tektronix Making Quality Circuit Board Repairs 1980" width="800" height="450" src="https://www.youtube.com/embed/jMchFqu3Jx0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
	            </div><!-- .entry-content -->
    
    <!-- .entry-footer -->
</article><!-- #post-## -->

            	<!-- .navigation -->
	
            

            
<!-- #comments -->

        
        

        
        

        
        </main><!-- #main -->
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: A friend has brain cancer: any bio hacks that worked? (130 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42649996</link>
            <guid>42649996</guid>
            <pubDate>Thu, 09 Jan 2025 21:33:06 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42649996">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=42649996: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[How to delete your Facebook account (112 pts)]]></title>
            <link>https://www.theverge.com/22231495/delete-facebook-page-account-how-to</link>
            <guid>42649887</guid>
            <pubDate>Thu, 09 Jan 2025 21:19:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/22231495/delete-facebook-page-account-how-to">https://www.theverge.com/22231495/delete-facebook-page-account-how-to</a>, See on <a href="https://news.ycombinator.com/item?id=42649887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><div><p>You may be wondering how to delete your Facebook account now that <a href="https://www.theverge.com/24339131/meta-content-moderation-fact-check-zuckerberg-texas">fact-checking is no longer considered important</a>, and Meta’s changing its <a href="https://www.theverge.com/2025/1/7/24338471/meta-hate-speech-hateful-conduct-policy-moderation">definition of what constitutes Hateful Conduct</a>. It’s easy to do, and we’ll show you how. But you should download all your stuff first.</p></div><p>The following instructions are for the web version of Facebook, but you can follow pretty much the same sequence on the mobile app.</p><p><h3>Download your archives</h3></p><p>Your Facebook archives contain just about <a href="https://www.facebook.com/help/405183566203254?helpref=faq_content">all of the pertinent information related to your account</a>, including your photos, active sessions, chat history, IP addresses, facial recognition data, and which ads you clicked. That’s personal information you should save.</p><div><ul><li>Click on your personal icon in the upper-right corner. </li><li>Go to <strong>Settings &amp; Privacy</strong> &gt; <strong>Settings</strong>. </li><li>Click on the <strong>Accounts Center</strong> box on the left.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Facebook “Settings and privacy” page showing Account Center on the left." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824202/Screenshot_2025_01_09_at_11.58.26_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>The Accounts Center is where you can both download your info and delete your account.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>Go to <strong>Your information and permissions</strong> on the left, and then <strong>Download Your Information &gt;  Download or transfer information.</strong></li><li>You can choose to transfer information from your Facebook or Instagram account (or both). </li><li>You now have another choice. You can select <strong>Available information</strong>, which includes everything but data logs. (Meta defines these as “additional details we collect and store that can be associated with you.”) Or you can select <strong>Specific types of information</strong>, which allows you to determine exactly what you want to download, including those data logs.</li><li>If you choose the latter, you can then select from the variety of data you’ve accumulated, including posts, friends, logged information, saved items and collections, etc. (You can also get the data logs, although Facebook warns it could take 15 days for them to show.) Click on <strong>Select all</strong> — but be aware you have to click it for each category.  When you’re ready, select <strong>Next</strong>.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A “Select information” pop-up box showing a list of types of Facebook data with checkboxes next to them." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824205/Screenshot_2025_01_08_at_5.10.09_PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>You can download various types of Facebook data, or all of it. The latter will take longer.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>Choose <strong>Download to device</strong> or <strong>Transfer to destination</strong>. According to Meta, the typical download is about 2.5GB.</li><li>You’ll now be able to select the date range of the info you want to download (or you can simply download all of it), the format (usually HTML or JSON), and the media quality (low, medium, or high). Enter an email address for a notification when the download is ready.</li><li>Finally, select <strong>Create files</strong>. You’ll receive an email when your file is ready, and it will be available for a few days. If you’ve been waiting a while and want to know the status of your download (or want to cancel it), go back to the <strong>Download your information</strong> tab.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up headed “Download your information” with a section labeled “In progress” halfway down." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824208/Screenshot_2025_01_09_at_9.58.34_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>You’ll be notified when your data is ready for download.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><p><h3>Delete your account</h3></p><p>You’re ready to delete your account once you’ve finished downloading your archive.</p><div><ul><li>When you are ready, go back to the <strong>Accounts Center</strong> and click on <strong>Personal Details &gt; Account ownership and control &gt; Deactivation or deletion</strong>. </li><li>Click&nbsp;<strong>Deactivation and Deletion</strong>.</li><li>If you have both Facebook and Instagram accounts, you will be asked to choose one.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up window headed Deactivating or deleting your Facebook account, with the choice of doing either underneath." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824209/Screenshot_2025_01_09_at_11.03.32_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Facebook gives you the option of temporarily deactivating your account — just in case you might change your mind.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>If you only want to deactivate your account temporarily (maybe you hope CEO Mark Zuckerberg <a href="https://www.theverge.com/24339131/meta-content-moderation-fact-check-zuckerberg-texas">will change his mind</a>?), you can choose to do so. Otherwise, select <strong>Delete account</strong> and click <strong>Continue</strong>.</li><li>You’ll be informed of any other accounts you have with Meta and given several options to explain why you’re leaving. Just keep hitting <strong>Continue</strong>.</li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up with check boxes so people can choose why they’re leaving Facebook." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824353/Screenshot_2025_01_09_at_1.14.09_PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Why do you want to leave? Choose your reason.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><div><ul><li>You’ll see an option to deactivate your account instead or save the posts in your archive, download your info, and review the apps you’re logged into. When you’re ready, hit <strong>Continue</strong>.</li><li>You’ll be asked for your password for confirmation. Enter it. </li><li>Finally ready? Hit <strong>Delete account</strong>.</li><li>Once you click <strong>Delete account</strong>, your account will be marked for termination and inaccessible to others using Facebook. </li></ul></div><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Pop-up headed Confirm permanent account deletion with explanatory text beneath and a blue Delete account button." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/376x203/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/384x207/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/415x224/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/480x259/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/540x292/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/640x346/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/750x405/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/828x447/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1080x583/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1200x648/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1440x778/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/1920x1037/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2048x1106/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:2362x1276/2400x1297/filters:focal(1181x638:1182x639):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25824210/Screenshot_2025_01_09_at_11.07.31_AM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>It takes a few pages, but you will finally get to the point where you can delete your account. And even after that, you have time to log in again.</em></figcaption> <p><cite>Screenshot: Meta</cite></p></div></div><p>Meta <a href="https://www.facebook.com/help/125338004213029?helpref=uf_permalink">notes</a> that it delays termination for a few days after the request has gone through.  The deletion will be canceled if you log back in during that period. So don’t sign on, or you’ll be forced to start the process over again. </p><p>Certain things, like comments you’ve made on a friend’s post, may still appear even after you delete your account. Facebook also says that copies of certain items like log records will remain in its database, but it notes that those are disassociated with personal identifiers. </p><p>If you’re really serious about quitting anything associated with Meta, remember that the company owns several other popular services as well, like Instagram, WhatsApp, and Threads, so you should delete your accounts there, too.</p><p><em><strong>Update January 9th, 2025:</strong> This article was originally published on September 28th, 2018, and has been updated several times to allow for changes in the Facebook interface.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Britain got its first internet connection (2015) (158 pts)]]></title>
            <link>https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404</link>
            <guid>42649340</guid>
            <pubDate>Thu, 09 Jan 2025 20:02:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404">https://theconversation.com/how-britain-got-its-first-internet-connection-by-the-late-pioneer-who-created-the-first-password-on-the-internet-45404</a>, See on <a href="https://news.ycombinator.com/item?id=42649340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><em>British computer scientist and <a href="https://www.internethalloffame.org/inductee/peter-kirstein">Internet Hall of Fame inductee</a> Peter Kirstein died in January 2020 at the age of 86, after a nearly 50-year career at UCL. A few years before he died, he was commissioned by then Conversation technology editor Michael Parker (now director of operations) to write an in-depth piece originally intended as part of a special series on the internet. It wasn’t published at the time, as the series was postponed, but now to mark Professor Kirsten’s contributions we are delighted to be able to publish his reflections on the challenges he faced connecting the UK in the early 1970s to the forerunner of what would become the modern internet. The article was edited by Michael with oversight kindly provided by <a href="https://theconversation.com/profiles/jon-crowcroft-143812">Professor Jon Crowcroft</a>, a colleague of Professor Kirstein’s.</em></p>

<p>The internet has become the most prevalent communications technology the world has ever seen. Though there are more fixed and mobile telephone connections, even they use internet technology in their core. For all the many uses the internet allows for today, its origins lie in the cold war and the need for a defence communications network that could survive a nuclear strike. But that defence communications network quickly became used for general communications and within only a few years of the first transmission, traffic on the predecessor to today’s internet was already 75% email.</p>

<h2>In the beginning</h2>

<p><a href="https://www.britannica.com/topic/ARPANET">Arpanet</a> was the vital precursor of today’s internet, commissioned by the US Defence Advanced Research Projects Agency (Darpa) in 1969. In his interesting account of <a href="https://www.computer.org/csdl/magazine/an/2011/03/man2011030004/13rRUxly9fL">why Arpanet came about</a>, Stephen Lukasic, Director of Darpa from 1970-75, wrote that if its true nature and impact had been realised it would never have been permitted under the US government structure of the time. The concept for a decentralised communications technology that would survive a nuclear attack would have placed it outside Darpa’s remit (as defence communications specifically were assigned to a different agency), so the focus changed to how to connect computers together so that major applications could be run on the most appropriate system available.</p>

<p>This was in the era of <a href="https://www.ibm.com/history/time-sharing">time-sharing computers</a>. Today’s familiar world of the ubiquitous “personal computer” on each desk was decades away. Computers of this time were generally very large, filling entire rooms, and comparatively rare. Users working at connected terminals would submit jobs to the computer which would allocate processing time for the job when available. The idea went that if these computers were networked together, an available remote computer could process a job even when the computers closer to the users were full. The resulting network was called Arpanet and the first packets of data traversed the network in September 1969. </p>

<figure>
            <a href="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=400&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=503&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639405/original/file-20241218-15-r3n8.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>A CDC 7600 mainframe computer fills an entire room at Lawrence Livermore National Laboratory, California, mid-1970s.</span>
              <span><a href="https://www.flickr.com/photos/llnl/3094299714/">Lawrence Livermore National Laboratory</a>, <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a></span>
            </figcaption>
          </figure>

<p>At this time the computing industry was dominated by a few large companies, which produced products that would work only with others from the same company. However the Arpanet concept included a vital decision on how the network would function: <a href="https://twobithistory.org/2021/03/08/arpanet-protocols.html">it sharply distinguished and separated</a> the technology and medium that would carry the communications (satellite link, copper cable, fibre optic), the network layer (the software that manages communications between different computers), and applications (the programs that users run over the network to do work) from one another. </p>

<p>This contrasted with the vertical “stove-pipe” philosophy that persisted among computer manufacturers at the time, where any networking that existed worked only in specific situations and for specific computer systems. For example, IBM computers could communicate using IBM’s <a href="https://www.ibm.com/docs/en/zos-basic-skills?topic=implementation-what-is-systems-network-architecture-sna">SNA protocol</a>, but not with non-IBM equipment. The direction Arpanet took was manufacturer-agnostic, where different types of computers could be networked together.</p>

<h2>First footprint in Europe</h2>

<p>In 1970, the leading network research outside the US was a group at the <a href="https://www.npl.co.uk/getattachment/about-us/History/11408-History-of-NPL-May-2023.pdf.aspx?lang=en-GB">National Physical Laboratory</a> (NPL) in London led by <a href="https://www.internethalloffame.org/inductee/donald-davies/">Donald Davies</a>. Davies had built a network with similar concepts to Arpanet, and as one of the inventors of <a href="https://www.npl.co.uk/getattachment/about-us/History/Famous-faces/Donald-Davies/UK-role-in-Packet-Switching-(1).pdf.aspx?lang=en-GB">packet-switching</a> his work had influenced the direction of Arpanet. But despite his plans for a national digital network, he was prevented from extending his project outside the lab by pressure from the British Post Office, which then held a monopoly on telecommunications. </p>

<p>Around this time the director of the Arpanet project, <a href="https://www.nytimes.com/2018/12/30/obituaries/lawrence-g-roberts-dies-at-81.html">Larry Roberts</a>, proposed connecting Arpanet to Davies’ NPL network in the UK. This would be possible because a few years previously a large seismic array in Norway run by Norwegian researchers for Darpa had been connected to Arpanet via a dedicated 2.4 Kbps connection to Washington. Due to the transatlantic technology of the time, this was by satellite link via the only earth station for satellite communications in Europe, in <a href="https://www.bbc.co.uk/news/uk-england-cornwall-62277946">Goonhilly, Cornwall</a>, and thence by cable to Oslo. Larry proposed to interrupt the connection in London, connect the NPL network, and then continue to Norway. </p>

<p>Since the international communications were the main cost, this seemed straightforward. Unfortunately Britain was at this point negotiating to join the Common Market, and the UK government was afraid that closer links with the US would jeopardise the talks. When the government refused NPL permission to participate, as I was doing relevant research at the University of London’s Institute of Computer Science and subsequently at <a href="https://www.ucl.ac.uk/computer-science/about-0">UCL</a>, I was the obvious alternative.  </p>

<h2>Vaulting many non-technical hurdles</h2>

<p>From the beginning I proposed a twin approach. I would connect the large computers at the University of London and the <a href="https://www.chilton-computing.org.uk/acl/pdfs/davies.pdf">Rutherford and Appleton laboratories</a> (RAL) in Oxfordshire, which were hubs for other UK computer networks, and I would provide services to allow UK researchers to use the networks to collaborate with colleagues in the US.</p>

<p>This novel approach would mean the IBM System 360/195 at RAL, then the most powerful computer in the UK, would be made available as a remote host – available to those in the US on the other side of the transatlantic link, without being directly connected to the interface message processor – the equipment which sent and received messages between Arapanet nodes, which would be installed in UCL.</p>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=1242&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=1561&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639315/original/file-20241218-15-wf1jfn.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p>
            <figcaption>
              <span>An interface message processor used to connect Arpanet nodes. About the size of a wardrobe, it is the type that would have been impounded by customs.</span>
              <span><a href="https://commons.wikimedia.org/wiki/File:ARPANET_first_router_2.jpg">Steve Jurvetson</a>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>Unfortunately there then came <a href="https://www.researchgate.net/publication/3330677_Early_experiences_with_the_Arpanet_and_Internet_in_the_UnitedKingdom">many non-technical hurdles</a>. I attempted to get other universities’ computer science departments to back the project, but this foundered because the Science Research Council did not consider the opportunity worth funding. The UK Department of Industry wanted a statement of interest from industry before funding, but even though I knew executives at ICL, the UK’s principal computer manufacturer, after months of agonising it declined stating that “one would gain more from a two-week visit to the US than from a physical link”. Consequently after a year of back and forth I had nothing.</p>

<p>However by 1973 the project was becoming a reality. By now the Norwegian siesmic array, <a href="https://www.norsar.no/about-us/history/arpanet">Norsar</a>, was connected to Arpanet via a newly opened satellite earth station at Tanum in Sweden, and so there was no longer a link via the UK at all. Now what was required was a link from UCL to Oslo. With a small grant of £5,000 from Donald Davies at the NPL, and the provision by the British Post Office of a 9.6 Kbps link to Oslo without charge for one year, we had the resources to proceed. </p>

<p>Darpa duly shipped its message processor with which to connect the new London node to Arpanet. It was promptly impounded at Heathrow Airport for import duty and the newly introduced Value Added Tax. I managed to avoid paying the duty by declaring it an “instrument on loan”, but it took all my available funds to provide a guarantee that would allow me to get hold of the equipment pending an appeal. With the equipment finally installed, in July 1973 I connected the first computers outside the US to the Arpanet, sending a transmission from London, via Norway, through the Arpanet to the Information Science Institute at the University of Southern California.</p>

<h2>First password on the internet</h2>

<p><a href="https://datatracker.ietf.org/doc/html/rfc588">Within three months</a> my group was able to implement the Arpanet network protocols and translate them to the IBM protocols necessary to communicate with computers at RAL. And so, once connected to the wider network through our gateway at UCL, the IBM computer at RAL became one of the most powerful on the Arpanet. </p>

<figure>
            <a href="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=430&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=540&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639177/original/file-20241217-17-8vfizz.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Arpanet map in 1977. The satellite connection from SDAC to NORSAR and then to London visible bottom right, with the large box bottom right representing the computers available at the Rutherford and Appleton Laboratories, Royal Signals and Radar Establishment and elsewhere.</span>
              <span><a href="https://en.wikipedia.org/wiki/ARPANET#/media/File:Arpanet_logical_map,_march_1977.png">The Computer History Museum</a></span>
            </figcaption>
          </figure>

<p>When I gave a talk stating this fact, RAL staff first did not believe me; they still saw only my small minicomputer, without understanding that it was the gateway to the rest of the Arpanet on the other side of the link. On realising they became very concerned that access to their computer services would be available not only to me, but with my complicity to the whole research community in the US. </p>

<p>However, I had been concerned that I would, in exactly this way, be criticised for improper use of both UK and US facilities. So from the beginning I put password protection on my gateway. This had been done in such a way that even if UK users telephoned directly into the communications computer provided by Darpa in UCL, they would require a password. </p>

<p>In fact this was the first password on Arpanet. It proved invaluable in satisfying authorities on both sides of the Atlantic for the 15 years I ran the service – during which no security breach occurred over my link. I also put in place a system of governance that any UK users had to be approved by a committee which I chaired but which also had UK government and British Post Office representation. </p>

<hr>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=112&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=140&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/313478/original/file-20200204-41481-1n8vco4.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p>
            <figcaption>
              <span></span>
              
            </figcaption>
          </figure>

<p><em>The <a href="https://theconversation.com/uk/insights">Insights section</a> is committed to high-quality <a href="https://theconversation.com/insights-the-conversations-long-reads-section-240155">longform journalism</a>. Our editors work with academics from many different backgrounds who are tackling a wide range of societal and scientific challenges.</em></p>

<hr>

<p>The transatlantic connection included terminal services (which connected users to remote computers to run jobs), file access and later email services. It was immediately very popular. Within a couple of years, I was supported by half a dozen government ministries, with leased line links (a dedicated line) to five remote sites – some of which allowed access through their own networks. Other users could telephone into my UCL site, or use the fledgling post office data network to which I also provided access. </p>

<p>Indeed, its profile had become so prominent that when the Queen opened a building at the Ministry of Defence’s Royal Radar Establishment at Malvern in Worcestershire in 1976 (which had taken over funding the leased line to Oslo), this was accompanied by her inaugurating the connection by <a href="https://www.internethalloffame.org/2012/12/31/how-queen-england-beat-everyone-internet/">sending an email</a> – the first to be sent by a head of state.</p>

<figure>
            <p><img alt="" data-src="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=396&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=498&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639184/original/file-20241217-15-ca6bh5.png?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p>
            <figcaption>
              <span>Her Majesty Queen Elizabeth II sends her first email, and the first email sent by a head of state, at the Royal Radar Establishment in 1976.</span>
              <span><span>Peter Kirstein</span></span>
            </figcaption>
          </figure>

<p>As the UK side of Arpanet continued growing, additional message processors had to be imported, each one racking up additional VAT and duty to be paid, pending the outcome of the appeal. Finally in 1976 the appeal was refused. But a meeting with senior treasury officials subsequently led to an agreement that my research group would be permitted to import equipment free of VAT and duty. The importance of this ruling cannot be overemphasised for ensuring the independence of our operation: over the following decade many government bodies considered trying take it over, and each time would be discouraged by the magnitude of the VAT and duty bill they would incur.</p>

<h2>Agreeing the language of Arpanet</h2>

<p>In their 1975 paper <a href="https://www.internethalloffame.org/inductee/robert-kahn/">Bob Kahn</a> at Darpa and <a href="https://www.internethalloffame.org/inductee/vint-cerf/">Vint Cerf</a> at Stanford University made the next vital contribution towards building the internet of today when they formulated the concept of connecting together different network technologies – such as those defined by different computer manufacturers, or designed for different communications media such as cable, satellite link or radio waves – with a <a href="https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf">common inter-network layer</a>, which would come to be known as TCP/IP. </p>

<p>Transport Control Protocol (TCP) managed the packaging and unpacking of data sent between computers, while Internet Protocol (IP) provided the pathfinding to ensure the data packets reached the intended destination. One of the important aspects of IP was that it allowed <a href="https://www.juniper.net/documentation/us/en/software/junos/interfaces-security-devices/topics/topic-map/security-interface-ipv4-ipv6-protocol.html">scalability</a>: the 8-bit number previously used to identify a computer on the network that allowed just 256 devices suddenly increased to a 32-bit number, which allowed 4 billion devices. </p>

<p>I misjudged how successful TCP/IP would be. In one of the <a href="https://archive.org/details/IssuesInPacketNetworkInterconnection/mode/1up?view=theater">first papers on network interconnection</a> Cerf argued that all computers should adopt TCP/IP, but I felt that this was unrealistic, and that gateways like the interface message processors were needed to “translate” communications between networks. While for the first 15 years my view prevailed, eventually in the long run Cerf’s view was the right one.</p>

<figure>
            <a href="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=388&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=488&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639313/original/file-20241218-17-ybihs5.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p></a>
            <figcaption>
              <span>Stanford Research Institute’s Packet Radio Van, used in the first TCP/IP internet experiments. The van drove across the Golden Gate Bridge while transmitting, and the steel girders interrupted the signal. But when it exited the bridge, the transmission picked up where it left off.</span>
              <span><a href="https://en.wikipedia.org/wiki/Packet_Radio_Van#/media/File:SRI_Packet_Radio_Van.jpg">SRI International</a>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>At UCL, my group participated in the first independent TCP/IP implementations, connecting in 1977 for the first time networks using a different technology to Arpanet. This saw three different types of network, Arpanet, the satellite network Satnet, and PRNET, a packet-radio network using <a href="https://computerhistory.org/blog/born-in-a-van-happy-40th-birthday-to-the-internet/">radio transmissions from mobile vans</a>, all connected using the same common “language”, TCP/IP. This was in essence the first demonstration of the internet – a network of networks.</p>

<p>Later, we connected the first multi-service heterogeneous network outside the US (<a href="https://www.jisc.ac.uk/janet">Janet</a>, the UK’s academic network connecting universities) to Arpanet, and then to the internet in the early 1980s. Indeed, UCL was the first organisation on Arpanet to adopt TCP/IP as standard.</p>

<figure>
            <a href="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=416&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=522&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/639293/original/file-20241217-15-okklbq.jpg?ixlib=rb-4.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Schematic of the first internet demonstration, connecting three different networks, PRNET, ARPANET and SATNET, with TCP/IP. This was the first connection that created a ‘network of networks’, as the internet would become.</span>
              <span><a href="https://en.wikipedia.org/wiki/History_of_the_Internet#/media/File:First_Internet_Demonstration,_1977.jpg">Computer History Museum</a></span>
            </figcaption>
          </figure>

<p>During the 1980s the internet approach took over, where computers used TCP/IP to manage their own connections to the network. Darpa provided funding to add TCP/IP into its <a href="https://klarasystems.com/articles/history-of-freebsd-part-4-bsd-and-tcp-ip/">chosen operating system of the time, BSD</a>, and this was later made available to the public. </p>

<p>After the release of the IBM PC microcomputer in 1981 there was a rapid growth of cheap (relatively speaking) personal computers in offices connected to each other by <a href="https://www.wired.com/story/what-is-ethernet/">ethernet</a> networks. And routers (small devices to connect networks) were developed that made the huge, outdated interface message processors used with the original Arpanet obsolete. </p>

<p>The universal adoption of common protocols that provided useful services like virtual terminal (telnet), file transfer (FTP), directory (LDAP) and email (SMTP) made the internet an invaluable tool for researchers. As fibre optic installations became more economical it allowed networks to scale up to very large numbers of interconnected computers. The internet’s most widespread and largest use by volume was still email, but a number of shared data repositories and resources developed. </p>

<p>Then in 1989, with the development of the <a href="https://home.cern/science/computing/birth-web/short-history-web">World Wide Web</a>, Tim Berners-Lee provided the killer application that would make the internet essential to all types of commercial and government use. The simplicity and ease of use of the web and web browsers, together with the internet as the distribution mechanism underpinning it, laid the basis for the universal use of the internet we have today. </p>

<h2>The little black book of the internet</h2>

<p>Back when there were even only a few hundred computers, discovering their addresses and maintaining a directory of them had become impractical. Bob Kahn, then director of the relevant office at Darpa, remedied this problem by commissioning the <a href="https://www.cs.cornell.edu/courses/cs6411/2018sp/papers/mockapetris.pdf">Domain Name Service</a>. This mapped IP addresses to names organised in hierarchical structure. The effect was a sort of directory of internet-connected computers, where top level domains (such as .com, .org, .uk, .fr) lay above second level domains (such as .ac.uk, .co.uk, or microsoft.com, wikipedia.org), which in turn lay above domains below them (such as www.microsoft.com or www.wikipedia.org, where the www. represents a subdomain below the domain). This domain model forms the basis of the URLs that we type into our browser address bars today.</p>

<p>Although four billion addresses seemed near infinite in 1974, by the early 1990s it was already evident that the internet would soon run out of IP (IPv4) addresses, necessary for computers to be connected to the internet. Work on the next generation of IP, IPv6, was to increase the number of routable network addresses from 32-bit (2<sup>32,</sup> or 4 billion) to 128-bit (or 2<sup>128</sup> or 3.4x10<sup>29</sup> billion) addresses. Technical fixes managed to extend the lifetime of IPv4, but over the last few years the need to move to IPv6 has become pressing, and adoption is now happening faster. </p>

<figure>
            <p><iframe data-src="https://www.youtube.com/embed/JLilyBJeYgQ?wmode=transparent&amp;start=0" frameborder="0" allowfullscreen="" width="100%" height="400"></iframe></p>
            
          </figure>

<h2>Growth and change</h2>

<p>Over the last two decades, the emergence of social networks, the increasing availability of internet streaming media and the integration of mobile telephone networks with the internet have hugely increased demand for internet capacity. Such demand will require large investments to meet, but probably without any radical rethink of the internet’s architecture. The number of internet-connected devices is growing significantly, but we can assume that it would increase only to a small multiple of the world’s population. So even if the protocols that govern how devices connect to the internet had to change to cope with demand, this could be achieved within only a few years.</p>

<p>The ability to monitor the activities of people – with or without their knowledge – is one important outcome of so many people so frequently connected to the network. The ability by unauthorised individuals to hack into private systems, to obtain private data or damage operations, are very worrying developments. The advances in computer and network security needed require massive research and development, and new legal and regulatory powers. And an even more disruptive development now looms, <a href="https://direct.mit.edu/daed/article/145/1/33/27105/Edge-Networks-amp-Devices-for-the-Internet-of">the Internet of Things</a>.</p>

<p>Increasingly devices and equipment found in all aspects of our life may incorporate sensors and actuators that can be operated remotely. The estimated numbers of devices to be network-connected is much larger: as many as hundreds of billions within ten years. Cars (for navigation or automated driving), home appliances (for automation, security), devices on the national power grid (monitoring and error correction), smart buildings (temperature or humidity control, security), smart cities (traffic control, services supply, waste management), wearable and implanted medical devices, and so on. </p>

<p>The characteristics of such devices are often quite different from today’s computers on the internet. The data rate may be very low, and often but not necessarily the data may be required only for local networks, rather than full internet availability. The devices or their controllers may have internet interfaces, but they may not obey other internet protocols, and would possibly need to be left in place for years, or decades.</p>

<p>They may not be able to carry out sophisticated security operations themselves, yet ensuring they are secure will be crucial if they are not to become a vast vulnerable network of potential points of entry for hostile actors. It is the Things on the internet of the future, rather than typical computing devices, that may prompt a radical re-think of the ways the internet works. </p>

<p>The impact of the internet on our way of life in its first 40 years has been immeasurable. It has expanded and developed in a way none of us envisaged in 1975. While we may have a better idea of what to expect over the next couple of decades, I am sure most of us will be mistaken.</p>
  </div></div>]]></description>
        </item>
    </channel>
</rss>