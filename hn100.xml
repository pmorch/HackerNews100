<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 05 Jun 2025 09:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: I made a 3D SVG Renderer that projects textures without rasterization (123 pts)]]></title>
            <link>https://seve.blog/p/i-made-a-3d-svg-renderer-that-projects</link>
            <guid>44187645</guid>
            <pubDate>Thu, 05 Jun 2025 02:05:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://seve.blog/p/i-made-a-3d-svg-renderer-that-projects">https://seve.blog/p/i-made-a-3d-svg-renderer-that-projects</a>, See on <a href="https://news.ycombinator.com/item?id=44187645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>I’ve been building a </span><a href="https://github.com/tscircuit/simple-3d-svg" rel="">vanilla 3D object to SVG renderer in Typescript</a><span> to help render </span><a href="https://tscircuit.com/seveibar/usb-c-flashlight#3dhttps://tscircuit.com/seveibar/usb-c-flashlight#3d" rel="">circuit boards that are made in React</a><span> and discovered an interesting trick to keep the SVGs small while getting approximately-correct looking perspective transformations with image textures.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png" width="1456" height="708" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:708,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:115620,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4678b33b-52b0-4d92-9fa2-584644b6de0e_1550x754.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>An example circuit board rendered with our vanilla Typescript 3D Renderer. Great for checking sizing! You can see we were able to project the “texture” containing the PCB traces!</figcaption></figure></div><p>SVGs don’t support perspective transforms like CSS (or at least they’re not guaranteed to work in image viewers), so we need a way to simulate this perspective transformation without creating a massive SVG. It’s easy to draw the box below, you can just project the face of each side of the cube into a polygon, but mapping the texture to that perspective transform isn’t natively possible!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png" width="1456" height="738" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:738,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0113af28-53ef-43b0-bb67-b7c8b8716ae9_1566x794.png 1456w" sizes="100vw"></picture></div></a><figcaption>Perspective transforms in CSS using the transform attribute (from MDN)</figcaption></figure></div><p><span>So SVGs don’t support perspective transforms, what do they support? SVGs support this nice little transform called an affine transform. This 6 number transform is what you get when you do </span><code>transform: matrix(a,b,c,d,e,f)</code><span> in CSS. They are super useful for 2D transformations, like panning/scaling/dragging, but can’t really project into 3d.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png" width="1456" height="621" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:621,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:284621,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5135f752-eda2-40f9-b6c8-2ff507ad7be6_4977x2122.png 1456w" sizes="100vw"></picture></div></a><figcaption>This projection isn’t possible with a single affine transform. You also can’t combine affine transformation matrices together to create this shape.</figcaption></figure></div><p>How can we approximate the transform? Here are some ideas I mulled over that could achieve a good result:</p><ul><li><p>Redraw the image with the distortion. This is potentially expensive and means that we can’t use SVGs as the images without converting them to bitmaps. It also means that things might look “fuzzy”</p></li><li><p>Ray trace everything! By projecting a ray to compute each pixel for the image, I could get a very conventional 3d renderer. This doesn’t achieve my goal of lightweight SVGs though</p></li><li><p>Subdivide the image and project each subdivision in the most locally-correct affine transformation. Use projected polygon clip paths to cut off the edges of regions.</p></li></ul><p>I was really curious how the last bullet point could work, and I could think of no other ideas that didn’t require rasterization. So with OpenAI O3’s help I implemented it into the vanilla Typescript 3D renderer. To test it, we’re going to project a checkerboard pattern onto a cube.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png" width="1456" height="1009" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1009,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:94975,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5689e55c-e737-477b-a9c5-243904eacafd_1636x1134.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Cube with no texture</figcaption></figure></div><p>Ok here’s our starting point, 2 subdivisions, the 2 checkboard images with affine transformations.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png" width="1456" height="1033" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1033,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:137143,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faa38af1d-5ffd-4d61-beda-0d6e9d5a2236_1468x1042.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Two subdivisions with a different affine transformation applied to each image</figcaption></figure></div><p>Not too bad, let’s see it with 4 subdivisions!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png" width="1244" height="900" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:900,&quot;width&quot;:1244,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120576,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffba9b7dd-a779-4b60-8790-5d27d2aca0f8_1244x900.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Four images with different affine transformations</figcaption></figure></div><p>Looks a bit rough (literally, it looks like it is not a flat surface). Let’s keep going!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png" width="1394" height="886" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:886,&quot;width&quot;:1394,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:137833,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff93393d8-fa7a-465f-a666-1fbaf159d98a_1394x886.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Eight images, eight clip paths, still not very smooth!!</figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png" width="1426" height="960" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:960,&quot;width&quot;:1426,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:147523,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F02695ead-34a8-4a0f-86c3-3740d02c8345_1426x960.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>16 images, 16 clip paths</figcaption></figure></div><p>Alright let’s go to the end, we want it to look flat!!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png" width="1456" height="891" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:891,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:187807,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F72cc35a4-2ac4-4c1a-b9e2-4fe44a058f7a_1478x904.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>512 Images &amp; Clip Paths</figcaption></figure></div><p>At around 512 images, it’s really hard to tell the difference. Awesome! We did image projection without any rasterization!</p><p>Now for the exciting part: The SVG isn’t that big! Because we can use the `defs` of the SVG to avoid repeating the image, we only need to define each clip path!</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png" width="1456" height="655" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:655,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:339186,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87b7410c-48c8-48f6-9f2b-b1e598d5f4a8_2062x928.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>This is the full file for the subdivision-2 cube. The polygons are the sides of the cube. The groups and clip paths are the only things that you need for each subregion.</figcaption></figure></div><p>Here’s a table of the file size as you increase subdivisions. I _think_ some differences in the matrix calculation account for the somewhat weird scaling. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png" width="1456" height="622" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:622,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75912,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3529abde-ef92-436b-af5e-2da469f4d17e_1844x788.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The math is fun but in the age of AI, below our pay-grade! </span><a href="https://github.com/tscircuit/simple-3d-svg" rel="">You can check out the full source code here</a><span>!</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png" width="546" height="490.5" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1308,&quot;width&quot;:1456,&quot;resizeWidth&quot;:546,&quot;bytes&quot;:322256,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18ad1d2a-fd0c-4e4d-aea1-524ee4165579_1578x1418.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Relevant snippet from the source where we subdivide and use bilinear interpolation to find the relevant quads</figcaption></figure></div><p><span>I’m excited to flesh out this 3D renderer because 3D SVGs can make great artifacts on GitHub, we want to make it so that people can easily review changes to circuit boards made with </span><a href="https://github.com/tscircuit/tscircuit" rel="">tscircuit</a><span> in pull requests.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png" width="662" height="471.49313186813185" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1037,&quot;width&quot;:1456,&quot;resizeWidth&quot;:662,&quot;bytes&quot;:97884,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://seve.blog/i/165235734?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef8ec6c9-71da-44e1-a42d-718d36310be1_1544x1100.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A visual snapshot test with a 3D SVG</figcaption></figure></div><p><span>I hope you enjoyed this neat little 3D trick! </span><em>Back to coding…</em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla seeks to guard crash data from public disclosure (293 pts)]]></title>
            <link>https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/</link>
            <guid>44186780</guid>
            <pubDate>Wed, 04 Jun 2025 23:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/">https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/</a>, See on <a href="https://news.ycombinator.com/item?id=44186780">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/government/musks-tesla-seeks-guard-crash-data-public-disclosure-2025-06-04/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[After court order, OpenAI is now preserving all ChatGPT user logs (762 pts)]]></title>
            <link>https://mastodon.laurenweinstein.org/@lauren/114627064774788581</link>
            <guid>44185913</guid>
            <pubDate>Wed, 04 Jun 2025 21:47:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.laurenweinstein.org/@lauren/114627064774788581">https://mastodon.laurenweinstein.org/@lauren/114627064774788581</a>, See on <a href="https://news.ycombinator.com/item?id=44185913">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Cursor 1.0 (365 pts)]]></title>
            <link>https://www.cursor.com/en/changelog/1-0</link>
            <guid>44185256</guid>
            <pubDate>Wed, 04 Jun 2025 20:39:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cursor.com/en/changelog/1-0">https://www.cursor.com/en/changelog/1-0</a>, See on <a href="https://news.ycombinator.com/item?id=44185256">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2>BugBot, Background Agent access to everyone and one-click MCP install</h2><p>Cursor 1.0 is here!</p>
<p>This release brings BugBot for code review, a first look at memories, one-click MCP setup, Jupyter support and general availability of Background Agent.</p>
<h3 id="automatic-code-review-with-bugbot"><span>Automatic code review with BugBot</span></h3>
<p>BugBot automatically reviews your PRs and catches potential bugs and issues.</p>
<p>When an issue is found, BugBot leaves a comment on your PRs in GitHub. You can click "<em><strong>Fix in Cursor</strong></em>" to move back to the editor with a pre-filled prompt to fix the issue.</p>
<p>To set it up, follow instructions in our <a target="_blank" href="https://docs.cursor.com/bugbot">BugBot docs</a></p>

<h3 id="background-agent-for-everyone"><span>Background Agent for everyone</span></h3>
<p>Since we released Background Agent, our remote coding agent, in early access a few weeks ago, the early signals we've been seeing have been positive.</p>
<p>We're now excited to expand Background Agent to all users! You can start using it right away by clicking the cloud icon in chat or hitting <code>Cmd/Ctrl+E</code> if you have privacy mode disabled. For users with privacy mode enabled - we'll soon have a way to enable it for you too!</p>

<!-- -->
<h3 id="agent-in-jupyter-notebooks"><span>Agent in Jupyter Notebooks</span></h3>
<p>Cursor can now implement changes in Jupyter Notebooks!</p>
<p>Agent will now create and edit multiple cells directly inside of Jupyter, a significant improvement for research and data science tasks. Only supported with Sonnet models to start.</p>

<h3 id="memories"><span>Memories</span></h3>
<p>With Memories, Cursor can remember facts from conversations and reference them in the future. Memories are stored per project on an individual level, and can be managed from Settings.</p>
<p>We're rolling out Memories as a beta feature. To get started, enable from Settings → Rules.</p>

<h3 id="mcp-one-click-install-and-oauth-support"><span>MCP one-click install and OAuth support</span></h3>
<p>You can now set up MCP servers in Cursor with one click and together with OAuth support, you can easily authenticate servers that support it.</p>
<p>We've curated a short list of official MCP servers you can add to Cursor at <a target="_blank" href="https://docs.cursor.com/tools">docs.cursor.com/tools</a>.</p>
<p>If you're an MCP developer, you can easily make your server available to developers by adding a <em>Add to Cursor</em> button in your documentation and READMEs. Generate one at <a target="_blank" href="https://docs.cursor.com/deeplinks">docs.cursor.com/deeplinks</a></p>

<h3 id="richer-chat-responses"><span>Richer Chat responses</span></h3>
<p>Cursor can now render visualizations inside of a conversation. In particular, Mermaid diagrams and Markdown tables can now be generated and viewed in the same place!</p>

<h3 id="new-settings-and-dashboard"><span>New Settings and Dashboard</span></h3>
<p>The setting and dashboard page have gotten some polish with this release.</p>
<p>With the new Dashboard, you can view your individual or team's usage analytics, update your display name, and view detailed statistics broken down by tool or model.</p>

<details><summary><span>Keyboard<!-- --> <span>(<!-- -->1<!-- -->)</span></span></summary><div><ul>
<li><span>Open Background Agent control panel with <kbd><code>Cmd/Ctrl+E</code></kbd></span></li>
</ul></div></details>
<details><summary><span>Improvements<!-- --> <span>(<!-- -->4<!-- -->)</span></span></summary><div><ul>
<li><span><code>@Link</code> and web search can now parse PDFs and include in context</span></li>
<li><span>Network diagnostics in settings to verify connectivity</span></li>
<li><span>Faster responses with parallel tool calls</span></li>
<li><span>Collapsable tool calls in Chat</span></li>
</ul></div></details>
<details><summary><span>Account<!-- --> <span>(<!-- -->3<!-- -->)</span></span></summary><div><ul>
<li><span>Enterprise users can only access stable release (no pre-release)</span></li>
<li><span>Team admins can now disable Privacy Mode</span></li>
<li><span><a target="_blank" href="https://docs.cursor.com/account/teams/admin-api">Admin API for teams</a> to access usage metrics and spend data</span></li>
</ul></div></details></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autonomous drone defeats human champions in racing first (183 pts)]]></title>
            <link>https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first</link>
            <guid>44184900</guid>
            <pubDate>Wed, 04 Jun 2025 20:03:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first">https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first</a>, See on <a href="https://news.ycombinator.com/item?id=44184900">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

                    
                            
                                    
    
    
    <p>
        News
        -
        15 April 2025
        
    </p>

                                
                        

                    <!--TYPO3SEARCH_begin-->
                    

    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580269">
        
            <p><strong>A team of scientists and students from TU Delft has taken first place at the A2RL Drone Championship in Abu Dhabi - an international race that pushes the limits of physical artificial intelligence, challenging teams to fly fully autonomous drones using only a single camera. The TU Delft drone competed against 13 autonomous drones and even human drone racing champions, using innovative methods to train deep neural networks for high-performance control. The gained knowledge on highly-efficient robust AI &nbsp;will contribute to many robotics applications, from self-driving cars to humanoid robots.</strong></p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580270">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/a/d/csm_WK%20Drone%20race%20groepsfoto_bde529bc18.jpg" data-imagezoom="" data-width="1248" data-height="1173">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/a/d/csm_WK%20Drone%20race%20groepsfoto_bde529bc18.jpg" width="1248" height="1173" alt="Group photo winners WK Drone race">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>The TU Delft team: Anton Lang, Quentin Missine, Aderik Verraest, Erin Lucassen, Till Blaha, Robin Ferede, Stavrow Bahnam, Christophe De Wagter and Guido de Croon.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580271">
        
            <h3>
                Beating human pilots
            </h3>
        



            
        



            



        
    

    




        
        

    <p>For the first time, a drone has beaten human pilots in an international drone racing competition, marking a new milestone in the development of artificial intelligence. On Saturday April 14, 2025, two drone racing events took place simultaneously: The Falcon Cup Finals for human pilots and the A2RL Drone Championship for AI-powered, autonomous drones. As a climax, the best AI drones also competed against the best human pilots. The AI drone developed by TU Delft first won the A2RL Grand Challenge. It then went on to win the knockout tournament against human pilots, beating three former DCL world champions and reaching flight speeds up to 95.8 km/h on the very winding track.</p>
<p>The team of scientists and students from TU Delft achieved this by developing an efficient and robust AI system, capable of split-second, high-performance control. Whereas earlier breakthroughs, like AI defeating world champions at chess or Go, have taken place in virtual settings, this achievement happened in the real world. Two years ago, the Robotics and Perception Group at the University of Zürich was the first to beat human drone racing champions with an autonomous drone. However, that impressive achievement occurred in a flight lab environment, where conditions, hardware, and the track were still controlled by the researchers – a very different situation from this world championship, where the hardware and track were fully designed and managed by the competition organisers.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580272">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/2/c/csm_TU_Delft_drone_on_track_3b9e85d62a.jpg" data-imagezoom="" data-width="4032" data-height="2395">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/2/c/csm_TU_Delft_drone_on_track_3b9e85d62a.jpg" width="4032" height="2395" alt="">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>The drone designed by the organizers, A2RL and DCL for use by the AI teams and human pilots.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580274">
        
            <h3>
                Pushing the frontiers of physical AI
            </h3>
        



            
        



            



        
    

    




        
        

    <p>The goal of the 2025 A2RL Drone Championship in Abu Dhabi was to push the frontier of physical AI, by stimulating research on robotic AI under extreme time pressure and with very limited computational and sensory resources. The drone had access to just one forward-looking camera, a major difference from previous autonomous drone races. This is more similar to how human FPV pilots fly, and leads to additional perception challenges for the AI.</p>
<p>The AI that won against the three former DCL world champions was developed by a team of scientists and students from the MAVLab at TU Delft’s Faculty of Aerospace Engineering. Team lead Christophe De Wagter is both exhausted and exhilarated.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580275">
        <blockquote>
            <p>I always wondered when AI would be able to compete with human drone racing pilots in real competitions. I’m extremely proud of the team that we were able to make it happen already this year. I hope that this achievement and this type of competition in general forms a springboard for real-world robot applications.</p>
            
            <cite>
                
                
                    Christophe De Wagter 
                
                
            </cite>
        </blockquote>
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580283">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/2/b/csm_timelapse_penultimate_gate_44b32560c9.jpg" data-imagezoom="" data-width="1920" data-height="974">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/2/b/csm_timelapse_penultimate_gate_44b32560c9.jpg" width="1919" height="974" alt="">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>Timelapse of the TU Delft drone flying through a gate of the racing track.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580284">
        
            <h3>
                AI that directly commands the motors
            </h3>
        



            
        



            



        
    

    




        
        

    <p>One of the core new elements of the drone’s AI is the use of a deep neural network that doesn’t send control commands to a traditional human controller, but directly to the motors. These networks were originally developed by the Advanced Concepts Team at the European Space Agency (ESA) under the name of “Guidance and Control Nets”. Traditional, human-engineered algorithms for optimal control were computationally so expensive that they would never be able to run onboard resource-constrained systems such as drones or satellites. ESA found that deep neural networks were able to mimick the outcomes of traditional algorithms, while requiring orders of magnitude less processing time. As it was hard to test whether the networks would perform well on real hardware in space, a collaboration was formed with the MAVLab at TU Delft.</p>
<p>“We now train the deep neural networks with reinforcement learning, a form of learning by trial and error. ”, says Christophe De Wagter. “This allows the drone to more closely approach the physical limits of the system. To get there, though, we had to redesign not only the training procedure for the control, but also how we can learn about the drone’s dynamics from its own onboard sensory data.”</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580286">
        
            <h3>
                Optimising robotic applications
            </h3>
        



            
        



            



        
    

    




        
        

    <p>The highly efficient AI developed for robust perception and optimal control are not only vital to autonomous racing drones but will extend to other robots. Christophe De Wagter: “Robot AI is limited by the required computational and energy resources. Autonomous drone racing is an ideal test case for developing and demonstrating highly-efficient, robust AI. Flying drones faster will be important for many economic and societal applications, ranging from delivering blood samples and defibrillators in time to finding people in natural disaster scenarios. Moreover, we can use the developed methods to strive not for optimal time but for other criteria such as optimal energy or safety. This will have an impact on many other applications, from vacuum robots to self-driving cars”.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    
        
    

    
        
        
    

    <div id="c1580458">
        
            <h3>
                Watch the video of the drone race
            </h3>
        



            
        



            



        
    

    




        
        

    


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <p id="c1580288">
        
        
            



        
        
            


        
    
        
            

    
            
                

    
            <h3>
                Contact
            </h3>
        



            
        



            



        
    

    




        
        

    


        
            



        
    </p>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    
    
        



    

    

    


                    <!--TYPO3SEARCH_end-->

                    



                </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Redesigned Swift.org is now live (147 pts)]]></title>
            <link>https://swift.org/</link>
            <guid>44184542</guid>
            <pubDate>Wed, 04 Jun 2025 19:26:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swift.org/">https://swift.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44184542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header>
    
  
    <!-- mobile-navigation -->
    
  </header>
   
<section id="what-is-swift">
    <div>
        <h2>Swift is the powerful, flexible,<br> multiplatform programming language.</h2>
        <p><h2>Fast. Expressive. Safe.</h2></p>
        <p><a href="https://swift.org/install/" data-text="Install">Install</a></p><p>Tools for Linux, macOS, and Windows</p>
        <h2>Create using Swift</h2>
    </div>
    <nav aria-label="Get started with Swift">
        <ul>
            
            <li>
                <a href="https://swift.org/get-started/cloud-services" data-text="Cloud Services">
                    <i></i>
                    <div>
                        <h3>Cloud Services</h3>
                        <p>Run performant services on Linux and deploy to the cloud.</p>
                    </div>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/get-started/command-line-tools" data-text="Command Line">
                    <i></i>
                    <div>
                        <h3>Command Line</h3>
                        <p>Create powerful CLI tools that are fast and memory safe.</p>
                    </div>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/get-started/embedded" data-text="Embedded">
                    <i></i>
                    <div>
                        <h3>Embedded</h3>
                        <p>Develop efficient, reliable firmware for devices like microcontrollers.</p>
                    </div>
                </a>
            </li>
            
        </ul>
        <ul>
            
            <li>
                <a href="https://swift.org/getting-started/swiftui/" data-text="iOS apps">
                    <h4>iOS apps</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/blog/swift-everywhere-windows-interop/" data-text="Windows apps">
                    <h4>Windows apps</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/blog/mlx-swift/" data-text="Machine learning and AI">
                    <h4>Machine Learning &amp; AI</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/getting-started/library-swiftpm/" data-text="Packages">
                    <h4>Packages</h4>
                </a>
            </li>
            
        </ul>
    </nav>
    
</section>

<section id="pillar-1">
    <div>
        <p>
            Swift is the only language that scales from embedded devices and kernels to apps and cloud infrastructure. It’s simple, and expressive, with incredible performance and safety. And it has unmatched interoperability with C and C++.
        </p>
        
        <p>
            It's the combination of approachability, speed, safety, and all of<br> Swift’s strengths that make it so unique.
        </p>
    </div>
    
 
<div>
  <div>
    
    <h3>Fast</h3>
     
    <p>Build with speed and performance.</p>
     
    <p>Swift meets the most performance-critical needs, while allowing your code to remain expressive and approachable. Swift compiles directly to native code and provides predictable memory management.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>// Vectorized check that a utf8 buffer is all ASCII</span>
<span>func</span> <span>isASCII</span><span>(</span><span>utf8</span><span>:</span> <span>Span</span><span>&lt;</span><span>SIMD16</span><span>&lt;</span><span>UInt8</span><span>&gt;&gt;</span><span>)</span> <span>-&gt;</span> <span>Bool</span> <span>{</span>
  <span>// combine all the code units into a single entry</span>
  <span>utf8</span><span>.</span><span>indices</span><span>.</span><span>reduce</span><span>(</span><span>into</span><span>:</span> <span>SIMD16</span><span>())</span> <span>{</span>
    <span>// fold each set of code units into the result</span>
    <span>$0</span> <span>|=</span> <span>utf8</span><span>[</span><span>$1</span><span>]</span>
  <span>}</span>
  <span>// check that every entry is in the ASCII range</span>
  <span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>0x80</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Expressive</h3>
     
    <p>Concise code. Powerful results.</p>
     
    <p>Swift empowers you to write advanced code in a concise, readable syntax that even a beginner can understand. Swift supports object-oriented, functional, and generic programming patterns that experienced developers are familiar with. Its progressive disclosure allows you to pick up the language quickly, taking advantage of power-user features as you need them.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>import</span> <span>ArgumentParser</span>

<span>// Complete implementation of a command line tool</span>
<span>@main</span> <span>struct</span> <span>Describe</span><span>:</span> <span>ParsableCommand</span> <span>{</span>
  <span>@Argument</span><span>(</span><span>help</span><span>:</span> <span>"The values to describe."</span><span>)</span>
  <span>var</span> <span>values</span><span>:</span> <span>[</span><span>Double</span><span>]</span> <span>=</span> <span>[]</span>

  <span>mutating</span> <span>func</span> <span>run</span><span>()</span> <span>{</span>
    <span>values</span><span>.</span><span>sort</span><span>()</span>
    <span>let</span> <span>total</span> <span>=</span> <span>values</span><span>.</span><span>reduce</span><span>(</span><span>0</span><span>,</span> <span>+</span><span>)</span>

    <span>print</span><span>(</span>
      <span>"""
      Smallest: </span><span>\(</span><span>values</span><span>.</span><span>first</span><span>,</span> <span>default</span><span>:</span> <span>"No value"</span><span>)</span><span>
      Total:    </span><span>\(</span><span>total</span><span>)</span><span>
      Mean:     </span><span>\(</span><span>total</span> <span>/</span> <span>Double</span><span>(</span><span>values</span><span>.</span><span>count</span><span>)</span><span>)</span><span>
      """</span><span>)</span>
  <span>}</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Safe</h3>
     
    <p>Protect memory safety.</p>
     
    <p>Swift prioritizes safety and eliminates entire classes of bugs and vulnerabilities by its design. Memory safety and data race safety are core features of the language, making them straightforward to integrate into your codebase. Safety is required at compile time, before your applications are ever run.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>let</span> <span>transform</span> <span>=</span> <span>Affine2DTransformBuilder</span><span>()</span>
    <span>.</span><span>translate</span><span>([</span><span>10.0</span><span>,</span> <span>20.0</span><span>]</span><span>.</span><span>span</span><span>)</span>
    <span>.</span><span>rotate</span><span>(</span><span>30.0</span><span>)</span>
    <span>.</span><span>build</span><span>()</span>

<span>let</span> <span>v</span> <span>=</span> <span>[</span><span>11.0</span><span>,</span> <span>22.0</span><span>,</span> <span>1.0</span><span>]</span>

<span>// Call C functions safely with Swift types</span>
<span>let</span> <span>u</span> <span>=</span> <span>mat_vec_mul</span><span>(</span>
  <span>transform</span><span>,</span> <span>rowCount</span><span>,</span> <span>colCount</span><span>,</span> <span>v</span><span>.</span><span>span</span><span>,</span> <span>allocator</span><span>)</span>
<span>let</span> <span>uMagnitude</span> <span>=</span> <span>vec_mag</span><span>(</span><span>u</span><span>.</span><span>span</span><span>)</span>
</code></pre></div>
    
</div>

    

    

</section>

<section id="pillar-2">
    
 
<div>
  <div>
    
    <h3>Interoperable</h3>
     
    <p>Adopt in existing code incrementally.</p>
     
    <p>Swift provides unmatched interoperability with its combination of natively understanding C and C++ types without the need for foreign function interfaces, and by providing bridging for bi-directional access. Swift’s interoperability features allow you to incrementally adopt the language into existing codebases without requiring a full code rewrite.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>import</span> <span>CxxStdlib</span>

<span>// Use types from C++, like std::string, directly</span>
<span>let</span> <span>beverages</span><span>:</span> <span>[</span><span>std</span><span>.</span><span>string</span><span>]</span> <span>=</span> <span>[</span>
  <span>"apple juice"</span><span>,</span> <span>"grape juice"</span><span>,</span> <span>"green tea"</span>
<span>]</span>

<span>let</span> <span>juices</span> <span>=</span> <span>beverages</span><span>.</span><span>filter</span> <span>{</span> <span>cppstring</span> <span>in</span>
  <span>// and call methods directly on C++ types</span>
  <span>cppstring</span><span>.</span><span>find</span><span>(</span><span>.</span><span>init</span><span>(</span><span>"juice"</span><span>))</span> <span>!=</span> <span>std</span><span>.</span><span>string</span><span>.</span><span>npos</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Adaptable</h3>
     
    <p>From microcontrollers to servers.</p>
     
    <p>The only language that can span from embedded and kernel, to server and apps. Swift excels no matter where it’s used: from constrained environments like firmware where every byte counts, to cloud services handling billions of requests a day.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>// Configure UART by direct register manipulation </span>
<span>// using Swift MMIO. Enables RX and TX, and sets</span>
<span>// baud rate to 115,200. Compiles down to an</span>
<span>// optimal assembly sequence with no overhead.</span>

<span>usart1</span><span>.</span><span>brr</span><span>.</span><span>modify</span> <span>{</span> <span>rw</span> <span>in</span>
  <span>rw</span><span>.</span><span>raw</span><span>.</span><span>brr_field</span> <span>=</span> <span>16_000_000</span> <span>/</span> <span>115_200</span>
<span>}</span>

<span>usart1</span><span>.</span><span>cr1</span><span>.</span><span>modify</span> <span>{</span> <span>rw</span> <span>in</span>
  <span>rw</span><span>.</span><span>ue</span> <span>=</span> <span>.</span><span>Enabled</span>
  <span>rw</span><span>.</span><span>re</span> <span>=</span> <span>.</span><span>Enabled</span>
  <span>rw</span><span>.</span><span>te</span> <span>=</span> <span>.</span><span>Enabled</span>
<span>}</span>
</code></pre></div>
    
</div>

    
    
</section>

<div id="pillar-3">
    
    <h3>Open Source</h3>
     
    <p>Contribute and get involved.</p>
     
  </div>
 

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Curtis Yarvin's Plot Against America (267 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile</link>
            <guid>44184305</guid>
            <pubDate>Wed, 04 Jun 2025 19:04:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile">https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile</a>, See on <a href="https://news.ycombinator.com/item?id=44184305">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure></figure><p>In the spring and summer of 2008, when <a href="https://www.newyorker.com/tag/donald-trump">Donald Trump</a> was still a registered Democrat, an anonymous blogger known as Mencius Moldbug posted a serial manifesto under the heading “An Open Letter to Open-Minded Progressives.” Written with the sneering disaffection of an ex-believer, the hundred-and-twenty-thousand-word letter argued that egalitarianism, far from improving the world, was actually responsible for most of its ills. That his bien-pensant readers thought otherwise, Moldbug contended, was due to the influence of the media and the academy, which worked together, however unwittingly, to perpetuate a left-liberal consensus. To this nefarious alliance he gave the name the Cathedral. Moldbug called for nothing less than its destruction and a total “reboot” of the social order. He proposed “the liquidation of democracy, the Constitution, and the rule of law,” and the eventual transfer of power to a C.E.O.-in-chief (someone like Steve Jobs or Marc Andreessen, he suggested), who would transform the government into “a heavily-armed, ultra-profitable corporation.” This new regime would sell off public schools, destroy universities, abolish the press, and imprison “decivilized populations.” It would also fire civil servants en masse (a policy Moldbug later called <em>RAGE</em>—Retire All Government Employees) and discontinue international relations, including “security guarantees, foreign aid, and mass immigration.”</p><p>Moldbug acknowledged that his vision depended on the sanity of his chief executive: “Clearly, if he or she turns out to be Hitler or Stalin, we have just recreated Nazism or Stalinism.” Yet he dismissed the failures of twentieth-century dictators, whom he saw as too reliant on popular support. For Moldbug, any system that sought legitimacy in the passions of the mob was doomed to instability. Though critics labelled him a techno-fascist, he preferred to call himself a royalist or a Jacobite—a nod to partisans of James II and his descendants, who, in the seventeenth and eighteenth centuries, opposed Britain’s parliamentary system and upheld the divine right of kings. Never mind the French Revolution, the bête noire of reactionary thinkers: Moldbug believed that the English and American Revolutions had gone too far.</p><p>If Moldbug’s “Open Letter” showed little affection for the masses, it intimated that they might still have a use. “Communism was not overthrown by <a href="https://www.newyorker.com/news/our-columnists/fifty-years-later-andrei-sakharovs-most-famous-essay-is-a-powerful-model-of-writing-for-social-change">Andrei Sakharov</a>, <a href="https://www.newyorker.com/magazine/1996/02/12/perfect-pitch-2">Joseph Brodsky</a>, and <a href="https://www.newyorker.com/books/page-turner/vaclav-havels-lessons-on-how-to-create-a-parallel-polis">Václav Havel</a>,” he wrote. “What was needed was the combination of philosopher and crowd.” The best place to recruit this crowd, he said, was on the internet—a shrewd intuition. Before long, links to Moldbug’s blog, “Unqualified Reservations,” were being passed around by libertarian techies, disgruntled bureaucrats, and self-styled rationalists—many of whom formed the shock troops of an online intellectual movement that came to be known as neo-reaction, or the Dark Enlightenment. While few turned into outright monarchists, their contempt for Obama-era uplift seemed to find voice in Moldbug’s heresies. In his most influential coinage, which quickly gained currency among the nascent alt-right, Moldbug urged his readers to rouse themselves from their ideological slumber by taking the “red pill,” like Keanu Reeves’s character in “The Matrix,” who chooses daunting truth over contented ignorance.</p><p>In 2013, an article on the news site <em>TechCrunch</em>, titled “Geeks for Monarchy,” revealed that Mencius Moldbug was the cyber alias of a forty-year-old programmer in San Francisco named Curtis Yarvin. At the same time that he was trying to redesign the U.S. government, Yarvin was also dreaming up a new computer operating system that he hoped would serve as a “digital republic.” He founded a company that he named Tlon, for the <a href="https://www.newyorker.com/magazine/1970/09/19/jorge-luis-borges-profile-autobiographical-notes">Borges</a> story “Tlön, Uqbar, Orbis Tertius,” in which a secret society describes an elaborate parallel world that begins to overtake reality. As he raised money for his startup, Yarvin became a kind of Machiavelli to his big-tech benefactors, who shared his view that the world would be better off if they were in charge. Tlon’s investors included the venture-capital firms Andreessen Horowitz and Founders Fund, the latter of which was started by the billionaire <a href="https://www.newyorker.com/news/letter-from-silicon-valley/what-is-it-about-peter-thiel">Peter Thiel</a>. Both Thiel and Balaji Srinivasan, then a general partner at Andreessen Horowitz, had become friends with Yarvin after reading his blog, though e-mails shared with me revealed that neither was thrilled to be publicly associated with him at the time. “How dangerous is it that we are being linked?” Thiel wrote to Yarvin in 2014. “One reassuring thought: one of our hidden advantages is that these people”—social-justice warriors—“wouldn’t believe in a conspiracy if it hit them over the head (this is perhaps the best measure of the decline of the Left). Linkages make them sound really crazy, and they kinda know it.”</p><p>A decade on, with the Trumpian right embracing strongman rule, Yarvin’s links to élites in Silicon Valley and Washington are no longer a secret. In a 2021 appearance on a far-right podcast, Vice-President J.&nbsp;D. Vance, a former employee of one of Thiel’s venture-capital firms, cited Yarvin when suggesting that a future Trump Administration “fire every single mid-level bureaucrat, every civil servant in the administrative state, replace them with our people,” and ignore the courts if they objected. Marc Andreessen, one of the heads of Andreessen Horowitz and an informal adviser to the so-called Department of Government Efficiency (<em>DOGE</em>), has started quoting his “good friend” Yarvin about the need for a founder-like figure to take charge of our “out of control” bureaucracy. Andrew Kloster, the new general counsel at the government’s Office of Personnel Management, has said that replacing civil servants with loyalists could help Trump defeat “the Cathedral.”</p><p>“There are figures who channel a Zeitgeist—Nietzsche calls them timely men—and Curtis is definitely a timely man,” a State Department official who has been reading Yarvin since the Moldbug era told me. Back in 2011, Yarvin said that Trump was one of two figures who seemed “biologically suited” to be an American monarch. (The other was Chris Christie.) In 2022, he recommended that Trump, if reëlected, appoint Elon Musk to run the executive branch. On a podcast with his friend Michael Anton, now the director of policy planning at the State Department, Yarvin argued that the institutions of civil society, such as Harvard, would need to be shut down. “The idea that you’re going to be a Caesar&nbsp;.&nbsp;.&nbsp;. with someone else’s Department of Reality in operation is just manifestly&nbsp;absurd,” he said.</p><p>In another timeline, Yarvin might have remained an obscure and ineffectual internet crank, a digital de Maistre. Instead, he has become one of America’s most influential illiberal thinkers, an engineer of the intellectual source code for the second Trump Administration. “Yarvin has pushed the Overton window,” Nikhil Pal Singh, a history professor at N.Y.U., told me. His work has revived ideas that once seemed outside the bounds of polite society, Singh said, and created a road map for the dismantling of “the administrative state and the global postwar order.”</p><p>As his ideas have been surrealized in <em>DOGE</em> and Trump has taken to self-identifying as a king, one might expect to find Yarvin in an exultant mood. In fact, he has spent the past few months fretting that the moment will go to waste. “If you have a Trump boner right now, enjoy it,” he wrote two days after the election. “It’s as hard as you’ll ever get.” What many see as the most dangerous assault on American democracy in the nation’s history Yarvin dismisses as woefully insufficient—a “vibes coup.” Without a full-blown autocratic takeover, he believes, a backlash is sure to follow. When I spoke to him recently, he quoted the words of Louis de Saint-Just, the French philosopher who championed the Reign of Terror: “He who makes half a revolution digs his own grave.”</p><p>Earlier this year, Yarvin and I had lunch in Washington, D.C., where he had come to celebrate the regime change. He was in his usual getup: bluejeans, Chelsea boots, a rumpled dress shirt under a motorcycle jacket. After taking a few bites of a cheeseburger topped with crispy onions, he pushed his plate away. Last year, he explained, he’d decided to start taking an Ozempic-like drug after a debate with the right-wing commentator Richard Hanania about the relative merits of monarchy and democracy. “I destroyed him in almost every way,” Yarvin said, nudging a tomato with his fork. “But he had one huge advantage, which was that I was fat and he was not.”</p><p>The injections seemed to be working. As I ate, Yarvin’s phone filled with messages, some of them complimenting his glow-up. That morning, the <em>Times Magazine</em> had published an interview with him, accompanied by a moody black-and-white portrait. Until recently, Yarvin, with his frazzled curtain of shoulder-length hair and ill-fitting wardrobe, had seemed indifferent to his appearance. Now, wearing his leather jacket, he glared out at the reader through stylishly tousled hair. His friend Steve Sailer, a writer for white-nationalist websites, said he looked like “the fifth Ramone.”</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a61101&quot;}" href="https://www.newyorker.com/cartoon/a61101" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“How can a hunter-gatherer and a rock designer afford such a nice cave?”</span></p><p><span>Cartoon by Enrico Pinto</span></p></div></span></p></figure><p>In person, as in print, Yarvin expresses himself with imperious self-assurance. He is nearly impossible to interrupt. “When the rabbi is speaking, you let the rabbi speak,” Razib Khan, a right-wing science blogger and a close friend of Yarvin’s, told me. Even his friends and family, however, acknowledge that he has room to grow as a communicator. He talks in a halting monotone, rarely answers questions directly, and is prone to disorienting asides. In the middle of saying one thing, he is always getting distracted by something else he could be saying, like a G.P.S. that keeps suggesting faster routes.</p><p>Yarvin, for his part, was relieved at how the interview with the <em>Times</em> had gone. “My main goal was, how do I not damage any of my relationships?” he said. For years, Yarvin was best known, to the extent that he was known at all, as the court philosopher of the Thiel-verse, the network of heterodox entrepreneurs, intellectuals, and hangers-on surrounding the tech mogul. He mentioned that a businessman he knew had once complained to a journalist that Thiel had not invested enough money in his company. “That’s one strike and you’re out, and he was out,” Yarvin said, sighing theatrically. His second goal, he said, was to reach the <em>Times</em> audience. This seemed surprising: he has called for the government to shut down the paper. “I tend to be more interested in outreach to people who share my own cultural background,” Yarvin explained.</p><p>He likes to tell the story of his paternal grandparents, Jewish Communists from Brooklyn who met at a leftist gathering in the thirties. (He has less to say about his maternal grandparents, Tarrytown Wasps with a cottage on Nantucket.) “The vibe of American communism was ‘We’ve got thirty I.Q. points on these people, and we’re going to win,’&nbsp;” he said. “It’s like, what if all the gifted kids formed a political party and tried to take over the world?” Yarvin’s parents met at Brown, where his father, Herbert, was pursuing a Ph.D. in philosophy. After finishing school and failing to get tenure (“too arrogant,” Yarvin said), Herbert tried his hand at writing the Great American Novel, then joined the Foreign Service as a diplomat. In the following years, the family lived in the Dominican Republic and Cyprus. Herbert was cynical about working for the government, and Yarvin seems to have inherited his disdain: he has repeatedly proposed closing America’s embassies, a prospect the State Department is now considering in parts of Europe and Africa.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin is reticent on the subject of his childhood, but friends and family suggested to me that his father could be harsh, domineering, and impossible to please. “He controlled their life with an iron fist,” someone with close knowledge of the family told me. “It was absolutely his domain.” (Yarvin vehemently rejected this view, saying that people who are controlling tend to be insecure, “and that is very much <em>not</em> the way of my father.” Better words to describe him, he said, would be “stubborn,” “intense,” and “formidable”—like “a good manager.”)</p><p>Growing up, Yarvin was sometimes homeschooled by his mother, and skipped three grades. (His older brother, Norman, skipped four.) The family eventually moved to Columbia, Maryland, where Yarvin entered high school as a twelve-year-old sophomore. “When you’re much younger than your classmates, you’re either an adorable mascot or a weird, threatening, disturbing alien,” Yarvin said, adding that he was the latter. Yarvin was selected to participate in a Johns Hopkins study of math prodigies. He attended the university’s Center for Talented Youth, a summer camp for gifted children, and was a Baltimore-area champion on “It’s Academic,” a television trivia show. Andrew Cone, a software engineer who currently lives in a spare room in Yarvin’s home, told me that Yarvin’s childhood seems to have left him with a lifelong feeling of inadequacy. “I think he has this sense of being not good enough, that he’s seen as ridiculous or small, and that the only way out is to perform,” Cone said.</p><p>Yarvin went to Brown, graduated at eighteen, and then entered a Ph.D. program in computer science at the University of California, Berkeley. Former peers told me that he wore a bicycle helmet in class and seemed eager to show off his knowledge to the professor. “Oh, you mean helmet-head?” one said when I asked about Yarvin. The joke among some of his classmates was that the helmet prevented new ideas from penetrating his mind. He found more of a community on Usenet, a precursor to today’s online forums. But even in groups like talk.bizarre, where intellectual peacocking was the norm, he stood out for his desire to dominate. Along with posting jokes, advice, light verse, and “flames” (blistering takedowns of other users), he maintained a “kill file,” a list of members he had blocked because he found their posts uninteresting. “He wanted to be viewed as the smart guy—that was really, really important to him,” his first girlfriend, Meredith Tanner, told me. She was drawn to Yarvin after reading one of his virtuosic flames, and the pair dated for a few years. “Don’t get involved with someone just because you’re impressed by how creatively they insult people,” she warned. “They will turn that skill on you.”</p><p>Friends from Yarvin’s twenties described him as a reflexive contrarian who revelled in provocation. “He wasn’t a sweet kid, and he could sometimes be nasty,&nbsp;but he wasn’t Moldbug,” one said. Politically and culturally, Yarvin was a liberal—“a big old hippie,” as Tanner put it. He had a ponytail, wore a silver hoop earring, dropped acid at raves, and wrote poetry. Tanner recalled that when she once questioned the value of affirmative action in college admissions, it was Yarvin who convinced her of its necessity.</p><p>After a year and a half of doctoral work, Yarvin left academia to seek his fortune in the tech industry. He helped design an early version of a mobile web browser for a company that came to be known as Phone.com. In 2001, he began dating Jennifer Kollmer, a playwright he met on Craigslist, whom he later married and had two children with. Phone.com had gone public, leaving him with a windfall of a million dollars. He used some of the money to buy a condo near the Haight-Ashbury neighborhood of San Francisco and the rest to fund a self-directed study of computer science and political theory. “I was used to getting pats on the head for being smart,” he said of his decision to leave the <em>cursus honorum</em> of the gifted child. “Diverging from the pat-on-the-head economy was a strange and scary choice.”</p><p>Out in the wilderness, Yarvin delved into recondite history and economics texts, many of them newly accessible through Google Books. He read Thomas Carlyle, James Burnham, and Albert Jay Nock, alongside an early-aughts profusion of political blogs. Yarvin traces his own red-pill moment to the Presidential election of 2004. As many of his peers were being driven to the left by lies about weapons of mass destruction in Iraq, Yarvin was pulled in the opposite direction by fabrications of a different sort: the Swift Boat conspiracy theory pushed by veterans allied with the George&nbsp;W. Bush campaign, who claimed that the Democratic candidate, John Kerry, had lied about his service in Vietnam. It seemed obvious to Yarvin, who believed the accusations, that once the truth emerged Kerry would be forced to drop out of the race. When that didn’t happen, he began to question what else he’d naïvely taken on trust. Facts no longer felt stable. How could he be confident in what he’d been told about Joseph McCarthy, the Civil War, or global warming? What about democracy itself? After years of energetic debates in the comments sections of other people’s blogs, he decided to start his own. It did not lack for ambition. The first post began, “The other day I was tinkering around in my garage and I decided to build a new ideology.”</p><p>The German academic Hans-Hermann Hoppe is sometimes described as an intellectual gateway to the far right. A retired economics professor at the University of Nevada, Las Vegas, Hoppe argues that universal suffrage has supplanted rule by a “natural élite”; advocates for breaking nations into smaller, homogenous communities; and calls for communists, homosexuals, and others who oppose this rigid social order to be “physically removed.” (Some white nationalists have made memes pairing Hoppe’s face with a helicopter—an allusion to the Chilean dictator Augusto Pinochet’s practice of executing opponents by throwing them from aircraft.) Though Hoppe favors a minimal state, he believes that freedom is better preserved by monarchy than by democracy.</p><p>Yarvin nearly ended up a libertarian. As a Bay Area coder and a devotee of Austrian-school economists in his late twenties, he exhibited all the risk factors. Then he discovered Hoppe’s book “<a data-offer-url="https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684&quot;}" href="https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684" rel="nofollow noopener" target="_blank">Democracy: The God That Failed</a>” (2001) and changed his mind. Yarvin soon adopted Hoppe’s imago of a benevolent strongman—someone who would govern efficiently, avoid senseless wars, and prioritize the well-being of his subjects. “It’s not copy-and-pasted, but it is such a direct influence that it’s kind of obscene,” Julian Waller, a scholar of authoritarianism at George Washington University, said. (Over e-mail, Hoppe recalled that he met Yarvin once at an exclusive gathering at Peter Thiel’s home, where Hoppe had been invited to speak. He acknowledged his influence on Yarvin, but added, “For my taste his writing has always been a bit too flowery and rambling.”) Hoppe argues that, unlike democratically elected officials, a monarch has a long-term incentive to safeguard his subjects and the state, because both belong to him. Anyone familiar with the history of dictatorships might find this idea disingenuous. Not Yarvin.</p><p>“You don’t ransack your own house,” he told me one afternoon, at an open-air café in Venice Beach. I’d asked him what would stop his C.E.O.-monarch from plundering the country—or enslaving his people—for personal gain. “For Louis XIV, when he says, ‘<em>L’état, c’est moi</em>,’ ransacking the state holds no meaning because it’s all his anyway.” Following Hoppe, Yarvin proposes that nations should eventually be broken up into a “patchwork” of statelets, like Singapore or Dubai, each with its own sovereign ruler. The eternal political problems of legitimacy, accountability, and succession would be solved by a secret board with the power to select and recall the otherwise all-powerful C.E.O. of each sovereign corporation, or SovCorp. (How the board itself would be selected is unclear, but Yarvin has suggested that airline pilots—“a fraternity of intelligent, practical, and careful people who are already trusted on a regular basis with the lives of others. What’s not to like?”—could manage the transition between regimes.) To prevent a C.E.O. from staging a military coup, the board members would have access to cryptographic keys that would allow them to disarm all government weapons, from nuclear missiles down to small arms, with the push of a button.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Mass political participation would cease, and the only way that people could vote would be with their feet, by moving from one SovCorp to another if they became dissatisfied with the terms of service, like switching from X to Bluesky. The irony that dissenters like Yarvin would probably be repressed in such a state appears not to concern him. In his imagined polity, he insists, there would still be freedom of speech. “You can think, say, or write whatever you want,” he has promised. “Because the state has no reason to care.”</p><p>Yarvin’s congenital cynicism about governance disappears as soon as he starts talking about dictatorial regimes. He has kind words for El Salvador’s strongman, <a href="https://www.newyorker.com/magazine/2022/09/12/the-rise-of-nayib-bukele-el-salvadors-authoritarian-president">Nayib Bukele</a>, and has encouraged Trump to let Putin end the liberal order “not just in Russian-speaking territories—but all the way to the English Channel.” Picking at a plate of fried calamari, Yarvin praised China and Rwanda (neither of which he has visited) for having strong governments that insured both public safety and personal liberty. In China, he told me, “you can think and pretty much say whatever you want.” He may have sensed my skepticism, given the country’s record of imprisoning critics and detaining ethnic minorities in concentration camps. “If you want to organize against the government, you’re gonna have problems,” he admitted. Then he returned to his airbrush: “Not Stalin problems. You’ll just, like, be cancelled.”</p><p>For certain people, like meth addicts or four-year-olds, Yarvin said, too much freedom could be deadly. Then, gesturing to the homeless population camped in the neighborhood, he suddenly began to cry. “The idea that this represents success, or this represents the ‘worst of all systems, except for all the others’&nbsp;”—he was referencing Churchill’s famous comment about democracy, which I’d paraphrased earlier—“is highly delusional,” he said, wiping away the tears. (A few weeks later, on a trip to London, I watched him break down while giving a similar speech to a member of the House of Lords. It was less affecting the second time around.)</p><p>Presumably, Yarvin’s monarch would act decisively to safeguard his wards. At the Venice café, Yarvin lauded the Delancey Street Foundation, a nonprofit rehab organization, whose strict program he has characterized as exerting “fascist-parent-level control.” Some of his own proposals go further. On his blog, he once joked about converting San Francisco’s underclasses into biodiesel to power the city’s buses. Then he suggested another idea: putting them in solitary confinement, hooked up to a virtual-reality interface. Whatever the exact solution, he has written, it is crucial to find “a humane alternative to genocide,” an outcome that “achieves the same result as mass murder (the removal of undesirable elements from society) but without any of the moral stigma.”</p><p>Yarvin’s call for an American strongman is often treated as an eccentric provocation. In fact, he considers it the only answer to a world in which most people are unfit for democracy. An “African country today,” he told me, has “enough smart people in the country to run it—you just don’t have enough smart people to have a democratic election in which everyone is smart.” Because of such remarks, Yarvin is sometimes identified as a white nationalist, a label he delicately resists. In a 2007 blog post titled “Why I Am Not a White Nationalist,” he explained that, though he is “not exactly allergic to the stuff,” he finds both whiteness and nationalism to be unhelpful political concepts. During lunch, he told me that he feels a rueful sympathy for the bigots of the past, who had some of the right intuitions but lacked the proper science. Neo-reactionaries tend to subscribe to what they call “human biodiversity,” a set of fringe beliefs which holds, among other things, that not all racial or population groups are equally intelligent. As Yarvin came to see it from his online research, these genetic differences contributed to (and, conveniently, helped explain away) demographic differences in poverty, crime, and educational attainment. “In this house, we believe in science—<em>race</em> science,” he wrote last year.</p><p>For several hours, Yarvin shuffled through his pitches for strongman rule, like an auctioneer desperate to clinch a sale. I listened patiently, though I was often puzzled by his factual distortions and peculiar asides. “What is the right policy in a completely new-from-scratch regime for African Americans?” he wondered aloud at one point. At first, this seemed like a non sequitur: I’d been pressing him on how he would define success in the second Trump Administration. Answering himself, he said that the “obvious solution” to problems of inner-city drug abuse and poverty would be to “put the church Blacks in charge of the ghetto Blacks.” Yarvin, who is an atheist, is not particularly interested in theocratic rule, but he advocates creating different legal codes to govern different populations. (He has cited the Ottoman <em>millet</em> system, which granted religious communities a measure of autonomy.) To keep the “ghetto Blacks” in line, he went on, they should be forced to live in a “traditional way,” like Orthodox Jews or the Amish. “The approach that the twentieth century took is, if we could just make the schools good enough, they would all turn into Unitarians,” he said. “If you’ve seen ‘The Wire’ and lived in Baltimore, both of which I have, that does not seem to work at all.” It wasn’t until he reached the end of his speech, ten minutes later, that I realized he was, in his own way, addressing my initial question. “Unless we can totally reëngineer DNA to change what a human being is, there are many people who should not live in a modern way but in a traditional way,” he concluded. “And <em>that</em> is a level of revolution that is so far beyond anything the Trump-Vance regime is doing.”</p><p>Yarvin is not known for his discretion. He has a habit of sharing private correspondence, as I discovered when he started sending me unsolicited screenshots of text messages and e-mails he’d exchanged with his wife, his friends, a fact checker at the <em>Times Magazine</em>, and someone nominated to the new Administration. He seemed troubled by the thought that the wit and wisdom they contained might be lost to posterity. He was more guarded about his friendship with Thiel, but he did mention a conversation they’d privately filmed together last year and boasted about a fortieth-birthday gift he’d received from the billionaire: Francis Neilson’s “The Tragedy of Europe,” a contemporaneous commentary on the Second World War, though not the first edition that Yarvin had been hoping for.</p><p>Thiel has always had a prophetic touch. He co-founded PayPal, became the first outside investor in Facebook, and created Palantir, a data-mining firm that has just received a new contract to help Immigration and Customs Enforcement officers carry out deportations. Thiel supported Trump back when doing so still made one a pariah in Silicon Valley. In 2022, he donated fifteen million dollars to J.&nbsp;D. Vance’s Senate campaign, the largest amount given to a single candidate in congressional history. A longtime libertarian, Thiel appears to have taken a Yarvinian turn around 2009, when, in a widely quoted essay published online by the Cato Institute, he wrote, “I no longer believe that freedom and democracy are compatible.” Yarvin linked to it approvingly in a blog post titled “Democraphobia Goes (Slightly) Viral.” They soon met for the first time, at Thiel’s house in San Francisco, and, according to private messages I reviewed, struck up a confiding correspondence. Yarvin’s e-mails were long and homiletic, full of precepts gleaned from pickup-artist blogs; Thiel’s were straightforward and concise. Both men seemed to take for granted that America was a communist country, that journalists acted like the Stasi, and that tech C.E.O.s were their prey.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In the fall of 2014, Thiel published “Zero to One,” a best-selling treatise on startups, with Blake Masters, his employee and a longtime Moldbug fan. Before the book tour, Thiel asked Yarvin for advice on fielding questions he might get on how to steer more women into tech. The premise appeared to strike them both as misguided, since women, in their view, were less likely to have men’s aptitude for computer science. As Yarvin put it in one e-mail, “There’s simply no way short of becoming a farce for Google, YC”—Y Combinator, the startup accelerator—“etc, etc, to ‘look like America.’&nbsp;” Yarvin suggested that Thiel deploy a pickup-artist tactic called “agree and amplify”—that is, ask a journalist, who probably had no solution in mind, what she would do to tackle the problem. “The purpose here is not to get the interlocutor to sleep with you, but to get her to fear this issue and run away from it—and ditto for future interviewers,” he wrote. Once, at a dinner, Thiel quizzed Yarvin on how one might go about taking down <em>Gawker</em>. (As it turned out, Thiel had already decided to secretly bankroll Hulk Hogan’s defamation lawsuit against the online publication, which eventually bankrupted it, in 2016.) In e-mails obtained by <em>BuzzFeed</em>, Yarvin bragged to Milo Yiannopoulos, the <em>Breitbart</em> editor, that he’d watched Trump’s first election at Thiel’s house and had been “coaching” him. “Peter needs guidance on politics for sure,” Yiannopoulos replied. Yarvin wrote back, “Less than you might think!&nbsp;.&nbsp;.&nbsp;. He’s fully enlightened, just plays it very carefully.”</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a60750&quot;}" href="https://www.newyorker.com/cartoon/a60750" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“Slowest to hit Skip has to watch the whole ad.”</span></p><p><span>Cartoon by Juan Astasio and Colin Mills</span></p></div></span></p></figure><p>When I recently visited Yarvin’s Craftsman home, in Berkeley, I noticed a painting that Thiel had given him: a portrait of Yarvin in the style of a role-playing-game character card, bearing the legend “Philosopher.” As I sipped tea from a novelty mug featuring an image of Yarvin with a cartoon crown, he told me that it would be “cringe” for him to broadcast his relationship with Thiel—or with Vance, for that matter, whom he met through Thiel around 2015. “Does a normal Ohio voter read&nbsp;.&nbsp;.&nbsp;. Mencius Moldbug? No,” Vance reportedly said one night at a bar during the 2021 National Conservatism Conference. “But do they agree with the broad thrust of where we think American public policy should go? Absolutely.” “He’s a really cool guy,” Yarvin said of the Vice-President, who followed him on X earlier this year. (The White House did not respond to requests for comment.)</p><p>Although Yarvin tried to be discreet, he mentioned that Thiel has a bit of a “weirdo edge” and described Andreessen, the venture capitalist, as someone who, “apart from the bizarre and possibly even nonhuman shape of his head, would seem much more normal than Peter.” After Andreessen invested in Yarvin’s startup, Tlon, the two got to know each other; they texted and went to brunch long before Andreessen came out as a Trump supporter, last year. Andreessen has been known to urge his associates to read Yarvin’s blog. “Tech people are not interested in appeals to virtue or beauty or tradition, like most conservatives,” the State Department official said. “They are more like right-wing progressives, and for a long time Moldbug was the only person speaking to them this way.” (Andreessen and Thiel declined to comment.) Apropos of his relationships with powerful men, Yarvin paraphrased to me “a wonderful piece of advice for courtiers” that he’d picked up from Lord Chesterfield’s “Letters to His Son,” an eighteenth-century etiquette manual addressed to the author’s illegitimate child: “Never bug them. And never let them forget you exist.”</p><p>Yarvin has had more success as a courtier to startup founders than as a founder himself. He launched Tlon in 2013, with a twentysomething former Thiel fellow. Yarvin approached computer science the same way he approached the U.S. government—with, as he put it, “utopian megalomania.” Yarvin’s visionary goal was to build a peer-to-peer computer network, named Urbit, that would allow users to control their own data, free from scolds, spies, and monopolies. Each user on the Urbit network is identified with an N.F.T. that acts like a digital passport. Even though Urbit promotes decentralization, the system is designed around a hierarchical model of virtual real estate, with users owning “planets,” “stars,” or “galaxies.”</p><p>In an early sketch of the system, Yarvin named himself its “prince,” but he struggled to attract subjects to his imaginary kingdom. Like Yarvin’s political theory, his programming language, which he wrote himself, was daring, abstruse, and sometimes mistaken for a hoax. Ever the contrarian, he reversed the meaning of zeros and ones. After decades of work and an estimated thirty million dollars of investment, Urbit seems to function less like a feudal society and more like the Usenet forums of Yarvin’s youth. (The trade publication <em>CoinDesk</em> has called it “a slower version of AOL Instant Messenger.”) “It doesn’t work the way it’s supposed to,” a former Urbit employee told me, describing Yarvin as “the world’s first computer-science crank.” Yarvin left the company in 2019.</p><p>No longer needing to worry about spooking investors, Yarvin threw himself into the life style of a self-described “rogue intellectual.” Under his own name, he launched a Substack newsletter, “Gray Mirror of the Nihilist Prince.” (Today, it is the platform’s third most popular “history” publication.) He became a fixture on the right-wing podcast circuit and seemed never to turn down an invitation to party. On his travels, he often hosted “office hours”—informal, freewheeling discussions with readers, many of them thoughtful young men, alienated by liberal guilt and groupthink. What wins Yarvin converts is less the soundness of his arguments than the transgressive energy they exude: he makes his listeners feel that he is granting them access to forbidden knowledge—about racial hierarchy, historical conspiracies, and the perfidy of democratic rule—that progressive culture is at pains to suppress. His approach seizes on the reality that most Americans have never learned how to defend democracy; they were simply brought up to believe in it.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin advises his followers to avoid culture-war battles over issues like D.E.I. and abortion. It is wiser, he argues, to let the democratic system collapse on its own. In the meantime, dissidents should focus on becoming “fashionable” by building a reactionary subculture—a counter-Cathedral. Sam Kriss, a left-wing writer who has debated Yarvin, said of his work, “It flatters people who believe they can change the world simply by having weird ideas on the Internet and decadent parties in Manhattan.”</p><p>Such people have come to be known as the “dissident right,” a loose constellation of artists and strivers clustered around the Bay Area, Miami, and the Lower East Side micro-neighborhood Dimes Square. The milieu was drawn together by a frustration with electoral politics, <em>Covid</em> lockdowns, and the strictures of “wokeness.” Vice signalling has been central to the scene’s countercultural allure: instead of sharing pronouns and employing the approved nomenclature (“unhoused,” “Latinx,” “justice-involved person”), its members have revived insults like “gay” and “retarded.” Dasha Nekrasova and Anna Khachiyan, the hosts of the “Red Scare” podcast, are among the most prominent avatars of the scene. In 2021, Thiel helped to fund an anti-woke film festival in New York, and Yarvin read his poetry at one of its packed events. Urbit now hosts a literary magazine designed to look like <em>The New York Review of Books</em>. “If you are an intelligent Jewish-American urbanite who wants to play around with certain Nietzschean and eugenic themes, you aren’t going to join tiki-torch-bearing marchers chanting that ‘the Jews will not replace us,’&nbsp;” the conservative commentator Sohrab Ahmari observed in an essay last year. “No, you turn to the dissident right.”</p><p>Yarvin has emerged as a veteran edgelord of this crowd, which he compared to San Francisco’s gay subculture in the seventies and to the Lost Generation of literary modernists—tight-knit communities whose members bonded over their sense of being outsiders. James Joyce, he said, sold few copies of “Ulysses,” but his friends, like Ezra Pound and T.&nbsp;S. Eliot, “knew that what he was doing was good.” So it was with the creatives of the dissident right, whose endeavors, he felt, had been overlooked by the intolerant Cathedral. This past April, Yarvin pitched Darren Beattie, the acting Under-Secretary of State for Public Diplomacy, on a plan for “dissident-right art hos” to take over the American pavilion at the Venice Biennale.</p><p>Lately, Yarvin has been trying to flip some of his newly acquired cultural capital into the real thing. Last year, he returned to Urbit as a “wartime C.E.O.,” after which several top employees resigned, and in February he raised more money from Andreessen Horowitz. According to a draft of an unpublished Substack post, his newest plan is to promote Urbit as an élite private club whose members, he believes, are destined to become “the stars of the new public sphere—a new Usenet, a new digital Athens built to last forever.”</p><p>The night before Trump’s Inauguration, I drove Yarvin to a black-tie “Coronation Ball” at the Watergate Hotel, in Washington, D.C. The event was organized by a neo-reactionary publishing house, Passage Press, which recently released Yarvin’s book “<a data-offer-url="https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P&quot;}" href="https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P" rel="nofollow noopener" target="_blank">Gray Mirror, Fascicle I: Disturbance</a>,” the first of a planned four-part cycle outlining his vision for a new political regime. Its endnotes predominantly consist of QR-code links to Wikipedia pages: “Denazification,” “L’État, c’est moi,” “Presentism (historical analysis).” As I negotiated the icy streets, Yarvin explained that during the Elizabethan era the finest minds in the arts and sciences were to be found at court. When I asked if he saw a parallel with Trump’s inner circle, he burst out laughing. “Oh, no,” he said. “My God.”</p><p>Like most journalists, I had been denied entry to the ball, so I ordered a drink at a bar in the lobby. Standing next to me was a man wearing a cowboy hat and a burgundy velour suit—a Yarvin enthusiast, it turned out, named Alex Maxa. He ran a party-bus company in San Francisco, and in his free time he made memes featuring Yarvin’s likeness. He said that he was drawn to Yarvin’s work because “it makes me feel like I’ve got something that people in Washington who think they’re really smart can’t actually make a compelling argument against.” He’d wanted to go to the ball but tickets, whose price had surged to twenty thousand dollars, were now sold out. Not long afterward, I met two of Yarvin’s friends, who encouraged me, and another journalist I was with, to confidently walk into the party with them. Maxa was already inside, having taken a similar approach. “Lol I just waltzed right in by asking where the coat check was,” he texted.</p><p>Passage Press had billed the event as “<em>MAGA</em> meets the Tech Right.” It was not false advertising. In a banquet hall awash in pink and purple light, Anton, from the State Department, Laura Loomer, a Trump whisperer known for her anti-Muslim bigotry, and Jack Posobiec, who popularized the Pizzagate conspiracy theory, mingled with venture capitalists, crypto accelerationists, and Substack all-stars. Earlier that evening, as guests dined on seared scallops and filet mignon, Steve Bannon, the ball’s keynote speaker, called for mass deportations, the “Götterdämmerung” of the administrative state, and Mark Zuckerberg’s imprisonment.</p><p>Eight years ago, Mike Cernovich, a first-gen alt-right influencer, had co-hosted an inaugural party known as the DeploraBall, a winking reference to Hillary Clinton’s unfortunate crack about half of Trump’s supporters belonging in a “basket of deplorables.” It was, by all accounts, a shambolic affair, plagued by journalists and protesters. One of Cernovich’s co-organizers, Tim Gionet, who goes by the online pseudonym Baked Alaska, was removed from his role after posting antisemitic content on Twitter. Now, at the Coronation Ball, Baked Alaska was served for dessert—a nod, it seemed, to Gionet, who was then on probation for participating in the January 6th insurrection. (He was pardoned by Trump the next day.) Cernovich pushed a baby around in a stroller and marvelled, like a proud father, at how&nbsp;far the movement had come. “I was one of the oldest guys in the place!” he tweeted the following afternoon. “Real right wing. High energy and high IQ.” In 2008, Yarvin, in his “Open Letter,” had called for a reactionary vanguard to form an underground political party. The Coronation Ball made it clear that this was no longer necessary. His web-addled counter-élite was now the establishment.</p><p>Yarvin was dressed in the same tuxedo, including a bright-red cummerbund, that he’d worn to a party at Thiel’s house in D.C. the night before, where, as <em>Politico</em> reported, Vance had amiably greeted him with “You reactionary fascist!” He’d also worn the tux to his wedding last year. Yarvin’s first wife died in 2021, from a hereditary heart disease, at the age of fifty. At the ball, he was accompanied by his second wife, Kristine Militello. A former Bernie Sanders supporter and an aspiring novelist, Kristine described herself as having been “red-pilled” during the pandemic, after losing her customer-service job at an online wine retailer. She first encountered Yarvin on YouTube, where she watched a video of him arguing against the legitimacy of the American Revolution, and proceeded to read everything he’d written. She sent him an admiring e-mail in 2022, seeking advice on how to break into New York’s dissident-right literary scene, and they met for drinks a few weeks later.</p><p>Recently, Yarvin has taken to describing himself as a “dark elf” whose role is to seduce “high elves”—blue-state élites—by planting “acorns of dark doubt in their high golden minds.” (In this Tolkien-inspired metaphor, red-state conservatives are “hobbits” who should submit to the “absolute power” of a new ruling class made up, unsurprisingly, of dark elves.) He didn’t always express himself so quaintly. In 2011, the day after the far-right terrorist Anders Behring Breivik killed sixty-nine people, many of them teen-agers, at a summer camp in Norway, Yarvin wrote, “If you’re going to change Norway into something new, you need the present ruling class of Norway to <em>join</em> and <em>follow</em> you. Or at least, you’ll need their children.” He praised Breivik for targeting the right group (“communists, not Muslims”), but condemned his methods: “Rape is beta. Seduction is alpha. Don’t slaughter the youth camp—<em>recruit</em> the youth camp.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin’s own recruitment efforts seemed to be working. Near the open bar, I spoke to Stevie Miller, a sprightly sophomore at Carnegie Mellon who has been reading Yarvin since the seventh grade. (Yarvin told me that he’d encountered several gifted Zoomers who’d read him as preteens because his “high-I.Q. style” served as a “high-I.Q. magnet.”) Two years ago, Miller hung out with Yarvin at Vibecamp, a gathering for nerds and techies in rural Maryland. Yarvin, who left early, asked Miller to help him throw his own party in D.C., which came to be known as Vibekampf. Afterward, Miller became Yarvin’s first personal intern. “My parents, New York Jewish liberals who I love, were totally mystified,” he said.</p><p>After half an hour, I was escorted out of the party, as were other reporters throughout the evening. Security mistook Maxa, my friend from the lobby, for one of our kind, and he was ejected, too, though not before pressing through the crowd to get his photo taken with the dark elf.</p><p>Even Trump’s most pessimistic critics have been startled by the speed with which the President, in his second term, has moved to impose autocracy on America, concentrating power in the executive branch—and often enough in the hands of the richest men on earth. Elon Musk, an unelected citizen, has led a squadron of twentysomethings on a spree through the federal government, laying off tens of thousands of civil servants, shuttering the U.S. Agency for International Development, and seizing control of the Treasury Department’s payment system. Meanwhile, the Administration has launched an assault on civil society, revoking funding at Harvard and other universities that it claims are bastions of ideological indoctrination and punishing law firms that have represented Trump’s opponents. It has expanded the machinery of immigration enforcement, deporting three U.S.-born children to Honduras, a group of Asian and Latin American immigrants to Africa, and more than two hundred Venezuelan migrants to a maximum-security prison in El Salvador, where they may remain until the end of their lives. U.S. citizens now find themselves with a government that claims the right to disappear them without due process: as Trump told Bukele, the President of El Salvador, during an Oval Office meeting, “Homegrowns are next.” Without a vigorous system of checks and balances, one man’s crank ideas—like starting an incoherent trade war that upends the global economy—don’t get filtered out. They become policies that enrich his family and his allies.</p><p>Since January, a cottage industry has arisen online to trace links between the government’s chaotic blitz of actions and Yarvin’s writings. Yarvin is hardly the Rasputin-like figure with Oval Office access that certain Bluesky users imagine him to be, but it isn’t difficult to see why some people may have come to this view. Last month, an anonymous <em>DOGE</em> adviser told the Washington <em>Post</em> that it was “an open secret that everyone in policymaking roles has read Yarvin.” Stephen Miller, the President’s deputy chief of staff, recently quote-tweeted him. Vance has called for the U.S. to retrench from Europe, a longtime Yarvin desideratum. Last spring, Yarvin proposed expelling all Palestinians from the Gaza Strip and turning it into a luxury resort. “Did I hear someone say ‘beachfront?’&nbsp;” he wrote on Substack. “The new Gaza—developed, of course, by Jared Kushner—is the LA of the Mediterranean, an entirely new charter city on humanity’s oldest ocean, sublime real estate with an absolutely perfect, Apple-quality government.” This February, during a joint press conference with Benjamin Netanyahu, the Israeli Prime Minister, Trump surprised his advisers when he made a nearly identical proposal, describing his redeveloped Gaza as “the Riviera of the Middle East.”</p><p>Whenever I asked Yarvin about resonances between his writing and real-world events, his response was nonchalant. He seemed to see himself as a conduit for pure reason—the only mystery, to him, was why it had taken others so long to catch up. “You can invent a lie, but you can only discover the truth,” he told me. We were in London, where he was attending the Alliance for Responsible Citizenship, a conservative conference co-founded by the psychologist Jordan Peterson. (Yarvin described Peterson to me as “a dandy” with “a weird narcissistic energy coming off of him.”) Accompanying Yarvin on his travels were Eduardo Giralt Brun and Alonso Esquinca Díaz, two millennial filmmakers who were shooting a documentary about his life. Their goal was to make a naturalistic character study in the style of “Grey Gardens,” in which, as Brun put it, “the camera just happens to be around.” It wasn’t going to plan. Yarvin kept repeating the same monologues, which meant that much of the footage was the same. The filmmakers worried that his racist remarks would turn viewers off. One afternoon in London, Díaz had filmed Yarvin getting his portrait painted with Lord Maurice Glasman, a post-liberal political theorist who has been called “Labour’s <em>MAGA</em> Lord,” for his support of Brexit and his ongoing dialogue with figures like Steve Bannon. At one point in their discussion, Yarvin had pulled out his iPhone to show Glasman that he’d hacked the chatbot Claude to get it to call him by the N-word.</p><p>Some thinkers would envy the attention Yarvin is receiving. But he dismissed his influence as a “fraudulent currency” since it has yet to cash out in the revolution he desires. He poured scorn on <em>DOGE</em> (“so much libertarian DNA”) and Trump’s tariff plan (not mercantilist enough). In a recent essay on Substack, he criticized the decision to dispatch plainclothes <em>ICE</em> officers to jail college students and professors for political speech—not on moral grounds, but because the thuggish optics were likely to provoke resistance. Yarvin’s oracular pronouncements and bottomless disdain for actually existing politics have inspired a viral post: his face under the words “Your anti-regime actions work well in practice. But do they work in theory?” The conservative activist Christopher Rufo has compared Yarvin to “a sullen teenager who insists that everything is pointless.” I came to think of him as a reactionary Goldilocks who would be satisfied with nothing less than the inch-perfect autocracy that he’d constructed in his mind.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a61050&quot;}" href="https://www.newyorker.com/cartoon/a61050" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Vi-An Nguyen</span></p></div></span></p></figure><p>This apparent desire for control also shows up in some of his relationships. Not long ago, I visited Lydia Laurenson, Yarvin’s ex-fiancée, in Berkeley. The two began dating in September, 2021, after Yarvin posted a personal ad on Substack, explaining that he’d recently lost his “widower virginity” and was looking to meet someone of “childbearing age.” Laurenson, a freelance writer and editor, replied the same day: “I have historically been a liberal but my IQ is really high, I want kids, and I’m incredibly curious to talk to you.” Yarvin went on Zoom dates with other women who answered the post—among them, Caroline Ellison, the ex-girlfriend of the now imprisoned crypto entrepreneur Sam Bankman-Fried—but he and Laurenson soon found themselves in an all-consuming romance. She told me that the ethos of her relationship with Yarvin was “&nbsp;‘We’re going to be geniuses together and have genius babies.’ I’m making fun of it a little bit, but that really was it.”</p><p>Like Yarvin, Laurenson had been a precocious child who went to college early. She’d also maintained a blog with a cult following, where, under the pseudonym Clarisse Thorn, she wrote about sex-positive feminism, B.D.S.M., and pickup artistry. She and Yarvin fought often, sometimes about politics. Laurenson had moved away from the left, but she hadn’t fully embraced neo-reaction. When I asked her if she’d ever changed Yarvin’s mind about anything, she said she’d gotten him to stop using the N-word, at least around her. (He later told this magazine that he was not using the word in the spirit of “a Southern plantation owner.”)</p><p>The bigger source of tension, according to Laurenson, was Yarvin’s autocratic attachment style. When they fought, Laurenson said, he insisted that she provide a rational justification for ending hostilities. She felt that Yarvin’s slippery personal attacks resembled his manner in public debates. “He makes up explanations that seem reasonable, but are actually false; he attacks the character of the person who is trying to point out what he’s doing; it’s like a DDOS attack of the soul,” she told me in an e-mail, referencing the cyberattack strategy of overwhelming a server with traffic from multiple sources. James Dama, a friend of Laurenson’s who had his own falling out with Yarvin, recalled, “He would make a coarse joke about Lydia’s weight or looks, not get a laugh, and then get angry at Lydia for being too stuck up.” (Tanner, Yarvin’s first girlfriend, described a similar pattern of insults and demands.)</p><p>Laurenson and Yarvin broke up in the summer of 2022, while Laurenson was pregnant. He told me that his desire for closeness might have struck Laurenson as “overbearing and stifling,” and that he had a bad habit of making “a joke that’s sort of a barb,” but he denied that he was ever purposefully cruel during the relationship. (He added that, after the relationship ended, “my natural instinct was, I’m going to cut her down to size every time I can”—something, he noted, he was “very good at.”) A few weeks after their son was born, that December, Yarvin sued for partial custody, which he received. An ongoing family-court case remains acrimonious. “The parents are in disagreement about nearly every issue,” their mediator observed last year.</p><p>Now that they share a toddler, Laurenson spends a lot of time thinking about Yarvin’s own childhood. “He has this class-clown thing going on, where he very much craves attention,” she said. To her, it seemed that his embrace of a provocative ideology was a kind of “repetition compulsion,” a psychological defense that allowed him to reframe the ostracization he experienced growing up. As America’s most famous living monarchist, he could tell himself that people were rejecting him for his outré ideas, not for his personality. She wondered if he’d first adopted “the monarchist thing” as a kind of intellectual sport, a bit from Usenet, and then, like the parallel world in the Borges story, it had slowly taken on a reality of its own. “Is it just like you found this place where people admire you and allow you to troll as much as you want, and then you just live in that world?” she asked.</p><p>In the past decade, liberalism has taken a beating from both sides of the political spectrum. Its critics to the left view its measured gradualism as incommensurate to the present’s multiple emergencies: climate change, inequality, the rise of an ethno-nationalist right. Conservatives, by contrast, paint liberalism as a cultural leviathan that has trampled traditional values underfoot. In “<a data-offer-url="https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447&quot;}" href="https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447" rel="nofollow noopener" target="_blank">Why Liberalism Failed</a>” (2018), the Notre Dame political scientist Patrick Deneen argues that the contemporary American emphasis on individual freedom has come at the expense of family, faith, and community, turning us into “increasingly separate, autonomous, non-relational selves replete with rights and defined by our liberty, but insecure, powerless, afraid, and alone.” Other post-liberal theorists, including Adrian Vermeule, have proposed that the state curtail certain rights in the service of an explicitly Catholic “common good.”</p><p>Yarvin is calling for something simpler and more libidinally satisfying: to burn it all down and start again from scratch. Since the advent of neoliberalism in the late seventies, political leaders have increasingly treated governance like corporate management, turning citizens into customers and privatizing services. The result has been greater inequality, a weakened social safety net, and the widespread perception that democracy itself is to blame for these ills, creating an appetite for exactly the kind of autocratic efficiency Yarvin now extolls. “A Yarvin program might seem seductive during a period of neoliberal rule, where efforts to change things, whether it is global warming or the war machine, feel futile,” the historian Suzanne Schneider told me. “You can sit back, not give a fuck, and let someone else run the show.” Yarvin has little to say on the question of human flourishing, or about humans in general, who appear in his work as sheep to be herded, idiots to be corrected, or marionettes controlled by leftist puppeteers.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Whatever gift Yarvin has for attracting attention, his work does not survive scrutiny. It is full of spurious syllogisms and arguments retconned to match his jaundiced intuitions. He has read widely, but he uses his knowledge merely as grist for the same reactionary fairy tale: once upon a time, people knew their place and lived in harmony; then along came the Enlightenment, with its “noble lie” of egalitarianism, plunging the world into disorder. Yarvin often criticizes academics for treating history like a Marvel movie, with oversimplified heroes and villains, but it’s unclear what he adds to the picture by calling Napoleon a “startup guy.” (He has favored the revisionist theories that Shakespeare’s plays were really written by the seventeenth Earl of Oxford and that the American Civil War, which he calls the War of Secession, worsened living conditions for Black Americans.) “The neat thing about primary sources is that often, it takes only one to prove your point,” he has proclaimed, which would come as news to historians.</p><p>Some of his most thoroughgoing critics are on the right. Rufo, the conservative activist, has written that Yarvin is a “sophist” whose debating style consists of “childish insults, bouts of paranoia, heavy italics, pointless digressions, competitive bibliography, and allusions to cartoons.” He added, “When one tries to locate what it is that you actually think, he cannot help but discover that there really isn’t much substance there.” The most generous engagement with Yarvin’s ideas has come from bloggers associated with the rationalist movement, which prides itself on weighing evidence for even seemingly far-fetched claims. Their formidable patience, however, has also worn thin. “He never addressed me as an equal, only as a brainwashed person,” Scott Aaronson, an eminent computer scientist, said of their conversations. “He seemed to think that if he just gave me one more reading assignment about happy slaves singing or one more monologue about F.D.R., I’d finally see the light.”</p><p>Intellectual seriousness may not be the point. Yarvin’s polemics have proved useful for those on the right in search of a rationale for nerd ressentiment and plutocratic will to power. “The guy does not have a coherent theory of the case,” the Democratic senator Chris Murphy, from Connecticut, told me. “He just happens to be saying something out loud that a lot of Republicans are eager to hear.”</p><p>It is not difficult to anticipate the totalitarian endgame of a world view that marries power worship with a contempt for human dignity—fascism, as some might call it. Like his ideological nemeses the Bolsheviks, Yarvin seems to believe that the only thing standing in the way of Utopia is an unwillingness to use every means possible to achieve it. He claims that the transition to his regime will be peaceful, even joyous, but fantasies of violence flicker throughout his work. “Unless the monarch is ready to actually <em>genocide</em> the nobility or the masses, he has to capture their loyalty,” he wrote in a Substack post in March. “You’re not going to <em>foam</em> these people, like turkeys with bird flu. Right?”</p><p>Yarvin’s strong opinions on how the world ought to work extended to this profile. Some of his suggestions were intriguing: he floated the idea of staging a debate with one of his ex-girlfriends, and invited me to follow him to Doha for a meeting with Omar bin Laden, one of Osama’s sons. Others were officious. At one point, he sent me nine texts objecting to my use of the word “extreme”—“a hostile pejorative,” he explained, which my article would be better off without. (He’d previously boasted several times in our taped conversations that he was more “extreme” than anyone in the current Administration.) A few days after the Coronation Ball at the Watergate Hotel, he wrote to <em>The New Yorker</em> to complain that I’d walked in without his publisher’s permission; he said that he hoped the incident would not turn into “Watergate 2,” and referred to himself as “certainly the most media-friendly person in the scene!” (Jonathan Keeperman, his publisher at Passage Press and the host of the ball, once suggested that the Republican Party should “lamppost”—that is, lynch—“the journos,” so this was not a particularly high bar to clear.)</p><p>One morning this winter, I woke up to twenty-eight texts from Yarvin expressing concerns about my reporting technique. “The problem is that your process is slack and I can feel it generating low-quality content—because it’s not adversarial enough,” he wrote. “When the process is not adversarial, I don’t know what I am contending against.” He briefly considered whether I was “too dumb to understand the ideas,” or whether I’d succumbed to the mental self-censorship that Orwell called “crimestop.” He urged me to watch “The Lives of Others,” an Oscar-winning film that depicts the relationship between an East German playwright and a Stasi agent who is tasked with surveilling him. The Stasi agent, he wrote, “can actually write up the ideas of the playwright, *without even thinking them* It is not even that he is ‘opposed’ to the dissident ideas. It is that he does not even let them touch his brain.” In the film, the Stasi agent eventually “cracks,” after he comes to sympathize with the playwright’s views. Yarvin, presumably, was the playwright.</p><p>He said that he was coming to see me, on the other hand, as an “NPC,” or non-player character. He proposed giving me a Voight-Kampff test, the fictional exam in “Blade Runner” used to distinguish androids from humans. His version would involve the two of us debating “the ‘blank slate theory’ versus ‘racism’&nbsp;” and recording the conversation. (“By ‘racism’ I mean of course human biodiversity,” he elaborated.) When I explained that my reporting process did not include submitting to on-demand tests, Yarvin sent me a screenshot of “August 1968,” W.&nbsp;H. Auden’s poem about the Soviet-led invasion of Czechoslovakia to suppress the Prague Spring:</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><blockquote data-testid="blockquote-wrapper"><p>The Ogre does what ogres can<br>Deeds quite impossible for Man,<br>But one prize is beyond his reach,<br>The Ogre cannot master Speech</p></blockquote><p>He went on to say that although he’d agreed to participate in this story because “no publicity is bad publicity,” he would now try to kill it if he could.</p><p>I was struck by the contrast between his messages and the coolheaded tone he’d recommended that Thiel and other friends deploy when handling the media. After the 2013 <em>TechCrunch</em> article identifying Yarvin came out, Balaji Srinivasan, the entrepreneur, proposed in an e-mail “to sic the Dark Enlightenment audience on a single vulnerable hostile reporter to dox them.” Yarvin dissuaded him. “What would Heartiste say?” Yarvin asked, referring to the white-nationalist pickup-artist blog “Chateau Heartiste.” “Almost always, the right alpha answer is ‘nothing.’ Say nothing. Do nothing.”</p><p>On a balmy afternoon in late February, Yarvin and his wife, Kristine, were driving down a country road in the South of France. They were accompanied by the documentarians, Brun and Díaz. “Where are we going, Kristine?” Brun asked from the passenger seat, turning the camera around to film her in the back beside me.</p><p>She said that she had only the vaguest notion. “Honestly, he just tells me everything last minute,” she explained. “It’s kind of like being a dog. You just know that you’re going in the car, and you don’t know if you’re gonna go to the dog park, or you’re gonna go to the vet, and you’ll find out when you get there.”</p><p>“Spontaneity,” Yarvin chimed in.</p><p>“That’s a word for it,” Kristine teased.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a22211&quot;}" href="https://www.newyorker.com/cartoon/a22211" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by John O’Brien</span></p></div></span></p></figure><p>We were on our way to meet Renaud Camus, a seventy-eight-year-old novelist and pamphleteer, who, in 2011, published “The Great Replacement,” an incendiary manifesto that argued that liberal élites were behind a conspiracy to replace white Europeans with migrants from Africa and the Middle East. The title phrase has since become a rallying cry for white nationalists around the world, from Charlottesville, Virginia, where, in 2017, marchers chanted, “You will not replace us,” to Christchurch, New Zealand, where, two years later, a man who’d published a manifesto with the same title as Camus’s killed fifty-one Muslims.</p><p>As we crested a hill, the walls of Camus’s castle, Château de Plieux, loomed into view. “Does anyone know if he’s related to Albert Camus?” Yarvin asked. “I think he’s not related to Albert, but he’s a lovely, old, gay, literary Frenchman.”</p><p>Brun, who is Venezuelan, wondered what he would do if Camus “has a sign that says ‘No Foreigners Allowed.’&nbsp;”</p><p>“Well, are you here to replace us?” Kristine joked. Nobody replied.</p><p>Yarvin rang an impressive metal bell beside the door, and we were soon ushered inside by Pierre Jolibert, Camus’s partner. Upstairs, Camus was waiting for us with a bottle of champagne. With his manicured white beard and brown corduroy jacket, complete with a bow tie and gold pocket-watch chain, he looked like a nineteenth-century man of letters. Speaking perfect English, with an English accent, he made it sound as though he’d had no choice but to buy the castle, which dated from the early thirteen-hundreds, after his library grew too large for his small Parisian flat. That was thirty-five years ago. Now, acknowledging the stacks of books that were overtaking his cavernous study, he said that he was running into the same problem here.</p><p>Over several glasses of champagne, Yarvin fired a series of questions at Camus, though he rarely waited long enough for his host to give a full answer. What did Camus think of Philippe Pétain? Charles de Gaulle? Napoleon III? Napoleon I? Ernst Jünger? Ernst von Salomon? Ezra Pound? Basil Bunting? More than an interaction, Yarvin, the former trivia champion, seemed to want a pat on the head for his display of learning.</p><p>After we headed downstairs for lunch—strips of sizzling duck, a quiche Lorraine, red wine—Yarvin resumed his cross-examination. Did Camus rate Thomas Carlyle? Michel Houellebecq? Louis XIV? What would he say to Charles Maurras if he were alive today? What would Dostoyevsky have thought about the <em>Covid</em> lab-leak theory?</p><p>Camus let out a high-pitched giggle whenever Yarvin asked a particularly odd question, but he was baffled by his guest’s repeated inquiries about Brigitte Macron, the French First Lady, who Yarvin suspected was actually a man. “We are dealing with the most important thing in the history of the Continent,” Camus exclaimed, referring to the rise of nonwhite immigration to Europe. “What does it matter if Mrs. Macron is a man or woman?”</p><p>Brun asked the men to move to a window so that he could shoot them from outside. As Yarvin gazed at the patchwork of neatly tended fields below, he spoke about the Great Replacement as “one of the greatest crimes” in history. “Is it greater than the Holocaust? I don’t know.&nbsp;.&nbsp;.&nbsp;. We haven’t seen it play out yet.” He’d been drinking since his arrival and seemed to be in an emotional state. “I have three children,” he told Camus. “Will they be basically lined up and marched into mass graves?” They had been discussing Jean Raspail’s apocalyptic novel, “The Camp of the Saints” (1973), which depicts an invasion of Indian migrants destroying European nations. Sobbing now, he continued, “I want my children to die in the twenty-second century. I don’t want them to experience some kind of insane post-colonial Holocaust.”</p><p>After dessert, coffee, and a rum from Guadeloupe, it was time for an evening stroll. Carrying a wooden cane, Camus led Yarvin through the small town of Plieux. Spring had arrived early: a cherry tree was blossoming with little flowers. As they passed the local church, Yarvin took out his phone to show Camus a photo of the toddler he shares with Laurenson. “The mother of that child was not my wife,” he said confidingly. A moment later, he was reading a poem by C.&nbsp;P. Cavafy, in tears once again.</p><p>When Yarvin and Camus went on ahead, the filmmakers paused to assess the day’s shoot. Brun said that Yarvin reminded him of the long-winded character in “Airplane!” who talks so incessantly that it drives his seatmates to kill themselves. We wondered what Camus was making of the afternoon. It wasn’t long before we found out. “If intellectual exchanges were commercial exchanges—which they are, to a certain extent—the amount of my exports would not reach one per cent of that of my imports,” Camus wrote in his diary, which he posted online the following day. “The visitor spoke without interruption from his arrival to his departure, for five hours, very quickly and very loudly, interrupting himself only for curious fits of tears, when he spoke of his deceased wife, but also, more strangely, certain political situations.”</p><p>It was dark by the time we all returned to the château. “Thank you so much for your hospitality and your duck and your castle,” Yarvin said, looking around. “How much money did you spend on it?”</p><p>Lovingly squeezing Yarvin’s arm, Kristine said, “You can’t just ask people that!”</p><p>Camus gave Yarvin some of his books as souvenirs, but Yarvin’s mind already seemed elsewhere. Tomorrow, he would fly to Paris to meet with a group of red-pilled Zoomers and Éric Zemmour, a far-right polemicist who once ran to be the President of France.</p><p>As we headed to the car, Yarvin was buzzing with boyish excitement about his performance. He turned to me and the filmmakers. “Was that good?” he asked. “Was that good?”&nbsp;♦</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Notes Expected to Gain Markdown Support in iOS 26 (237 pts)]]></title>
            <link>https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/</link>
            <guid>44183923</guid>
            <pubDate>Wed, 04 Jun 2025 18:29:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/">https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/</a>, See on <a href="https://news.ycombinator.com/item?id=44183923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/06/04/apple-notes-rumored-markdown-support-ios-26/"><p>Apple's Notes app is rumored to be getting limited Markdown support in iOS 26 and macOS 26, according to <em><a href="https://9to5mac.com/2025/06/03/exclusive-ios-26-messages-carplay-more/">9to5Mac</a></em>. The feature would allow users to export text in the markdown format.</p>
<p><img src="https://images.macrumors.com/t/KX5s3QPN8F9_4EB6jHu3huH7j-0=/400x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy" srcset="https://images.macrumors.com/t/KX5s3QPN8F9_4EB6jHu3huH7j-0=/400x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy 400w,https://images.macrumors.com/t/rNrX5sZI2CoLmeluvBSY9PV3oIQ=/800x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy 800w,https://images.macrumors.com/t/MFdPb4DjJL7dZ3MuK5-yqgdRmro=/1600x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg 1600w,https://images.macrumors.com/t/_Eb67huucTUo6d63cc3KL-IEMjQ=/2500x0/filters:no_upscale()/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS 26 Mock Rainbow Feature" width="2500" height="1406"><br>Markdown is a lightweight markup language that some writers prefer to use over rich text. Rather than using HTML for bold, italics, links, and headers, Markdown uses quick character shortcuts like **bold** or #header. It sounds like the feature will only add support for exporting text with markdown formatting and not writing in markdown directly. </p>
<p>If the rumor holds up, it's likely to be unveiled at next week's Worldwide Developers Conference alongside other iOS 26 improvements, including <a href="https://www.macrumors.com/2025/06/03/ios-26-messages-app-rumors/">automatic translation and polls in Messages</a>, not to mention a <a href="https://www.macrumors.com/2025/03/17/apple-believes-users-love-major-ios-19-redesign/">major visual redesign</a>.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/06/01/new-icloud-plus-perk-this-year/">iPhone Users Who Pay for iCloud Storage Received a New Perk This Year</a></h3><p>If you pay for iCloud storage on your iPhone, Apple introduced an additional perk for you this year, at no additional cost.
The perk is the ability to create invitations in the Apple Invites app for the iPhone, which was released in the App Store in February.
In the Apple Invites app, iCloud+ subscribers can create invitations for any occasion, such as birthday parties, graduations, baby...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/apple-sleek-peek/">Apple Shares New 'Sleek Peek' Teaser Ahead of WWDC 2025 Next Week</a></h3><p>WWDC 2025 is just one week away, with Apple's opening keynote scheduled to begin on Monday, June 9 at 10 a.m. Pacific Time. Ahead of the annual developer conference, Apple updated its WWDC page today with a new "Sleek peek" tagline, which replaces the original "On the horizon" tagline that it used over the past few weeks.
The graphic for WWDC 2025 has also been updated. It is now a...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/what-to-expect-from-ios-18-6/">What to Expect From iOS 18.6 as One of the Final Updates Before iOS 26</a></h3><p>It has been three weeks as of today since Apple released iOS 18.5, and we are still waiting for the first iOS 18.6 beta to follow.
Below, we outline everything we know about iOS 18.6 so far.
Timing
Apple's software engineers have been internally testing iOS 18.6 since late March, according to the MacRumors visitors logs.
The first betas of iOS 13.6 through iOS 16.6 were all released...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/04/ios-26-to-upgrade-carplay-in-two-ways/">iOS 26 to Upgrade CarPlay in Two Ways</a></h3><p>While the spotlight has been on CarPlay Ultra lately, the regular version of CarPlay is set to receive some enhancements alongside iOS 26.
Apple will announce iOS 26 at WWDC 2025 next week, and the software update is expected to upgrade the CarPlay experience in at least two ways.
The first iOS 26 beta should be seeded to developers shortly after Apple's keynote, and the update will...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/macos-26-tahoe-what-to-expect/">WWDC 2025: What to Expect From macOS 26 Tahoe</a></h3><p>Monday June 2, 2025 4:17 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>WWDC is less than a week away, and as we ramp up to the big announcement, we're going to share details on what we know about each operating system. We're starting with the next-generation version of macOS, which Apple is apparently going to call macOS Tahoe.
Name
Since the current version of macOS is macOS 15, it would normally be followed by macOS 16, but Apple is changing its naming...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/ios-26-rumored-features/">iOS 26 Will Add These New Features to Your iPhone</a></h3><p>WWDC 2025 is just one week away, with Apple's opening keynote scheduled to begin on Monday, June 9 at 10 a.m. Pacific Time.
Apple will announce its latest software updates, including iOS 26, iPadOS 26, macOS 26, tvOS 26, watchOS 26, and visionOS 26, which are all rumored to feature a sleek new glass-like design. There might not be any hardware announcements, however, as Bloomberg's Mark...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/04/ex-apple-designer-living-glass-ios-concepts/">Ex-Apple Designer Reveals 'Living Glass' iOS 26 Concepts</a></h3><p>Wednesday June 4, 2025 4:17 am PDT by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Designer Sebastiaan de With has published an impressive preview of what Apple's rumored iOS redesign might look like, complete with detailed mockups and a design philosophy that he believes could reshape how users interact with their devices.
With WWDC just days away, de With – co-founder of photography app maker Lux and former Apple designer – has created what he calls "Living Glass"...</p></div><div><h3><a href="https://www.macrumors.com/2025/05/27/iphone-17-pro-rumors-list/">iPhone 17 Pro Launching Later This Year With These 12 New Features</a></h3><p>While the iPhone 17 Pro and iPhone 17 Pro Max are not expected to launch until September, there are already plenty of rumors about the devices.
Below, we recap key changes rumored for the iPhone 17 Pro models as of May 2025:
Aluminum frame: iPhone 17 Pro models are rumored to have an aluminum frame, whereas the iPhone 15 Pro and iPhone 16 Pro models have a titanium frame, and the iPhone X ...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A proposal to restrict sites from accessing a users’ local network (384 pts)]]></title>
            <link>https://github.com/explainers-by-googlers/local-network-access</link>
            <guid>44183799</guid>
            <pubDate>Wed, 04 Jun 2025 18:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/explainers-by-googlers/local-network-access">https://github.com/explainers-by-googlers/local-network-access</a>, See on <a href="https://news.ycombinator.com/item?id=44183799">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Explainer for Local Network Access</h2><a id="user-content-explainer-for-local-network-access" aria-label="Permalink: Explainer for Local Network Access" href="#explainer-for-local-network-access"></a></p>
<p dir="auto">This proposal is an early design sketch by the Chrome Secure Web and Network team to describe the problem below and solicit feedback on the proposed solution. It has not been approved to ship in Chrome.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Proponents</h2><a id="user-content-proponents" aria-label="Permalink: Proponents" href="#proponents"></a></p>
<ul dir="auto">
<li>Chrome Secure Web and Network team</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Participate</h2><a id="user-content-participate" aria-label="Permalink: Participate" href="#participate"></a></p>
<ul dir="auto">
<li><a href="https://github.com/explainers-by-googlers/local-network-access/issues">https://github.com/explainers-by-googlers/local-network-access/issues</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Currently public websites can probe a user's local network, perform CSRF attacks against vulnerable local devices, and generally abuse the user's browser as a "confused deputy" that has access inside the user's local network or software on their local machine. For example, if you visit <code>evil.com</code> it can use your browser as a springboard to attack your printer (given an HTTP accessible printer exploit).</p>
<p dir="auto">Local Network Access aims to prevent these undesired requests to insecure devices on the local network. This is achieved by deprecating direct access to private IP addresses from public websites, and instead requiring that the user grants permission to the initiating website to make connections to their local network.</p>
<p dir="auto"><em>Note:</em> This proposal builds on top of Chrome's previously paused <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md">Private Network Access (PNA) work</a>, but differs by gating access on a permission rather than via preflight requests. This increases the level of user control (at the expense of new permissions that have to be explained to the user) but removes the explicit "device opt-in" that the preflight design achieved. We believe this simpler design will be easier to ship, in order to mitigate the real risks of local network access today. Unlike the previous Private Network Access proposal, which required changes to devices on local networks, this approach should only require changes to sites that need to access the local network. Sites are much easier to update than devices, and so this approach should be much more straightforward to roll out.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Goals</h2><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<ul dir="auto">
<li><strong>Stop exploitation of vulnerable devices and servers from the drive-by web.</strong></li>
<li>Allow public websites to communicate to private network devices when the user expects it and explicitly allows it.</li>
</ul>
<p dir="auto">An adjacent goal is that we want a path for browsers to be good stewards of OS-level local network access permissions. These OS-level permissions are increasingly common (<a href="https://developer.apple.com/documentation/technotes/tn3179-understanding-local-network-privacy" rel="nofollow">on iOS, and more recently on macOS</a>) -- simply because the <em>browser</em> has been granted the permission (for legitimate browser functionality the user may want to use, like mirroring the contents of a tab on a local device) should not expose users' local devices to the risks of the open web.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Non-goals</h2><a id="user-content-non-goals" aria-label="Permalink: Non-goals" href="#non-goals"></a></p>
<ul dir="auto">
<li>Break existing workflows and services that rely on a public web frontend that can control local network devices.
<ul dir="auto">
<li>As long as there is <em>some</em> path forward we should be okay with breaking some use cases (e.g., iframe and HTML subresources that aren't explicitly sourced from local hostnames), but overall we want to minimize breakage.</li>
</ul>
</li>
<li>Solve the local network HTTPS problem.
<ul dir="auto">
<li>As stated in the <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md#non-goals">original Private Network Access explainer</a>: <em>Provide a secure mechanism for initiating HTTPS connections to services running on the local network or the user's machine. This piece is missing to allow secure public websites to embed non-public resources without running into mixed content violations, with the exception of <a href="http://localhost/" rel="nofollow">http://localhost</a> which is embeddable. While a useful goal, and maybe even a necessary one in order to deploy Private Network Access more widely, it is out of scope of this specification.</em></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use cases</h2><a id="user-content-use-cases" aria-label="Permalink: Use cases" href="#use-cases"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use case 1</h3><a id="user-content-use-case-1" aria-label="Permalink: Use case 1" href="#use-case-1"></a></p>
<p dir="auto">The most common case is for users who don't have any services or devices on their local network that expect connections from websites. Today, browsers freely allow JavaScript and subresource requests to these devices without any indication to the end user. Unless this behavior is expected by the user, users should not be exposed to this risk by default.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use case 2</h3><a id="user-content-use-case-2" aria-label="Permalink: Use case 2" href="#use-case-2"></a></p>
<p dir="auto">Public web frontend for controlling or setting up local devices (such as an IoT device, home router, etc.).</p>
<p dir="auto">Device manufacturers want to be able to give users an easy process for setting up a new device, and one method that is used is to have a page hosted on the manufacturer's public website which then communicates with the device via the user's browser, explicitly relying on the browser's vantage point inside the user's network.</p>
<p dir="auto">This also reduces the complexity needed on the device itself -- for example, a smart toothbrush does not need to support a full webserver. Additionally, by being a public webpage under the control of the manufacturer, the setup page is always up to date.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Proposed Solution</h2><a id="user-content-proposed-solution" aria-label="Permalink: Proposed Solution" href="#proposed-solution"></a></p>
<p dir="auto">We propose gating the ability for a site to make requests to the users' local network behind a new "local network access" permission. Any origin that has not been granted this permission would be blocked from making such requests.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Address spaces</h3><a id="user-content-address-spaces" aria-label="Permalink: Address spaces" href="#address-spaces"></a></p>
<p dir="auto">As defined in the <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md#address-spaces">original Private Network Access proposal</a>, we organize an IP network into three layers from the point of view of a node, from most to least private:</p>
<ul dir="auto">
<li>Localhost: accessible only to the node itself, by default</li>
<li>Private IP addresses: accessible only to the members of the local network (e.g. RFC1918)</li>
<li>Public IP addresses: accessible to anyone</li>
</ul>
<p dir="auto">We call these layers <strong>address spaces</strong>: <code>loopback</code>, <code>local</code>, and <code>public</code>.</p>
<p dir="auto"><em>(Note: The original PNA proposal called these <code>local</code>, <code>private</code>, and <code>public</code>. Changing this was considered in <a data-error-text="Failed to load title" data-id="1459537487" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/91" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/91/hovercard" href="https://github.com/WICG/private-network-access/issues/91">WICG/private-network-access#91</a> but reverted due to already using the "private network access" name and values in headers implemented by sites and device manufacturers.)</em></p>
<p dir="auto">We note that <code>local</code> includes <a href="https://datatracker.ietf.org/doc/html/rfc1918" rel="nofollow">RFC 1918</a>/<a href="https://datatracker.ietf.org/doc/html/rfc4193" rel="nofollow">RFC 4193</a> private/local IP addresses and <a href="https://datatracker.ietf.org/doc/html/rfc6762" rel="nofollow">RFC 6762</a> link-local names (<code>.local</code> hostnames). (See <a href="https://github.com/WICG/private-network-access/issues/4" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/4/hovercard">discussion on the original PNA proposal repository</a>.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Local network requests</h3><a id="user-content-local-network-requests" aria-label="Permalink: Local network requests" href="#local-network-requests"></a></p>
<p dir="auto">We define a <strong>local network request</strong> as a request crossing an address space boundary to a more-private address space. That is, any of the following are considered to be local network requests:</p>
<ol dir="auto">
<li><code>public</code> -&gt; <code>local</code></li>
<li><code>public</code> -&gt; <code>loopback</code></li>
<li><code>local</code> -&gt; <code>loopback</code></li>
</ol>
<p dir="auto">Note that <code>local</code> -&gt; <code>local</code> is not a local network request, as well as <code>loopback</code> -&gt; anything. (See "cross-origin requests" below for a discussion on potentially expanding this definition in the future.)</p>
<p dir="auto">A request is considered to be going to a <code>local</code> space if:</p>
<ul dir="auto">
<li>The hostname is a private IP address literal (per RFC 1918 etc.), or</li>
<li>The hostname is a <code>.local</code> domain (per RFC 6762), or</li>
<li>The <code>fetch()</code> call is annotated with <code>targetAddressSpace="local"</code> (see "Integration with Fetch" below).</li>
</ul>
<p dir="auto">In these cases we know <em>a priori</em> that the request is <code>local</code>.</p>
<p dir="auto">Similarly, if a request is to a loopback IP literal (e.g., 127.0.0.1), <code>localhost</code>, or the <code>fetch()</code> call is annotated with <code>targetAddressSpace="loopback"</code>, then the request is <code>loopback</code>.</p>
<p dir="auto">Separately, a request may eventually end up being considered a local network request if the request's hostname resolves to a private or loopback IP address. (We do not know this a priori however, and so cannot exempt these requests from mixed content blocking -- see "Mixed Content" below.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Permission prompts</h3><a id="user-content-permission-prompts" aria-label="Permalink: Permission prompts" href="#permission-prompts"></a></p>
<p dir="auto">When a site makes a local network request, the UA should check if the origin has already been granted the "local network access" permission. If not, the request should be blocked while the UA displays a prompt to the user asking whether they want to allow the origin to make requests to their local network. If the user denies the permission prompt, the request fails. If the user accepts the permission prompt, the request continues.</p>
<p dir="auto">To reduce breakage (due to the lack of local network HTTPS), the permission also exempts requests that are known to be <code>local</code> or <code>loopback</code> from mixed content blocking (see "Mixed Content" below.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How this solution would solve the use cases</h3><a id="user-content-how-this-solution-would-solve-the-use-cases" aria-label="Permalink: How this solution would solve the use cases" href="#how-this-solution-would-solve-the-use-cases"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Use case 1: unexpected usage</h4><a id="user-content-use-case-1-unexpected-usage" aria-label="Permalink: Use case 1: unexpected usage" href="#use-case-1-unexpected-usage"></a></p>
<p dir="auto">For a user who is not expecting a site to connect to their local network, when <code>example.com</code> tries to call <code>fetch("http://192.168.0.1/routerstatus")</code>, the user's browser will ask whether or not to allow <code>example.com</code> to make connections to the local network. Since the user is not expecting this behavior, they can deny the permission request, and <code>example.com</code> is blocked from making these connections.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Use case 2: controlling local devices</h4><a id="user-content-use-case-2-controlling-local-devices" aria-label="Permalink: Use case 2: controlling local devices" href="#use-case-2-controlling-local-devices"></a></p>
<p dir="auto">An existing site run by a device manufacturer that talks to a local device by making <code>fetch()</code> requests would potentially make minor modifications (for example, ensuring that they either use a private IP address or <code>.local</code> name when referring to the device, or adding the <code>targetAddressSpace="local"</code> property to their <code>fetch()</code> calls). When the site first tries to make a request to the device, the user sees a permission prompt. If the user expects the site to be communicating with devices on their local network, they can choose to grant the permission and the site will continue to function. If the user does not expect this or does not want to grant the permission to the site, they choose to not grant the permission to the site, and no local network requests from the site will be allowed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Detailed design discussion</h2><a id="user-content-detailed-design-discussion" aria-label="Permalink: Detailed design discussion" href="#detailed-design-discussion"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Fetch</h3><a id="user-content-integration-with-fetch" aria-label="Permalink: Integration with Fetch" href="#integration-with-fetch"></a></p>
<p dir="auto">The Fetch spec does not integrate the details of DNS resolution, only defining an <strong>obtain a connection</strong> algorithm, thus Local Network Access checks are applied to the newly-obtained connection. Given complexities such as Happy Eyeballs (<a href="https://datatracker.ietf.org/doc/html/rfc6555" rel="nofollow">RFC6555</a>, <a href="https://datatracker.ietf.org/doc/html/rfc8305" rel="nofollow">RFC8305</a>), these checks might pass or fail non-deterministically for hosts with multiple IP addresses that straddle IP address space boundaries.</p>
<p dir="auto">After we have obtained a connection, if we detect a local network request:</p>
<ul dir="auto">
<li>If the client is not in a secure context, block the request.</li>
<li>Check if the origin has previously been granted the local network access permission; if not, prompt the user.</li>
<li>If the user grants permission, the request will proceed.</li>
</ul>
<p dir="auto">However, the requirement that local network requests be made from secure contexts means that any insecure request will be blocked as mixed content unless we can know ahead of time that the request should be considered a local network request.</p>
<p dir="auto">To make this easier for developers, we propose adding a new parameter to the <code>fetch()</code> options bag to explicitly tag the address space of the request. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fetch(&quot;http://router.com/ping&quot;, {
  targetAddressSpace: &quot;local&quot;,
});"><pre><span>fetch</span><span>(</span><span>"http://router.com/ping"</span><span>,</span> <span>{</span>
  <span>targetAddressSpace</span>: <span>"local"</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<p dir="auto">This would instruct the browser to allow the fetch even though the scheme is non-secure and obtain a connection to the target server. The <code>targetAddressSpace</code> should be either <code>local</code> or <code>loopback</code>.</p>
<p dir="auto">If the remote IP address does not belong to the IP address space specified as the targetAddressSpace option value, then the request is failed. This ensures the feature cannot be abused to bypass mixed content in general. See "Mixed content" below for more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mixed content</h3><a id="user-content-mixed-content" aria-label="Permalink: Mixed content" href="#mixed-content"></a></p>
<p dir="auto">There is a challenge in the combination of (1) requiring secure contexts in order to make PNA requests, (2) trying to load PNA subresources over HTTP (due to the lack of local network HTTPS), and (3) applying mixed content blocking. The Fetch specification applies mixed content upgrading and blocking steps well before we have obtained a connection.</p>
<p dir="auto">We can know ahead of the mixed content checks whether a request has a local target address if:</p>
<ul dir="auto">
<li>The hostname is a private IP address literal (per RFC1918 etc.), or</li>
<li>The hostname is a <code>.local</code> domain, or</li>
<li>The <code>fetch()</code> call is annotated with the <code>{ targetAddressSpace: "local" }</code> option.</li>
</ul>
<p dir="auto">If the request meets any of those requirements, we skip steps 6 and 7 of <strong>main fetch</strong> (upgrading mixed content and blocking mixed content) and mark the request as a local network request. Then, after obtaining the connection the local network access checks can fully run, and if the origin is not granted the local network access permission the request will be blocked. Additionally, if the request ends up not resolving to a local network endpoint, we need to block the request (as it should have been blocked as mixed content).</p>
<p dir="auto">A new parameter <code>targetAddressSpace</code> will be added as a <code>fetch()</code> API option, allowing a developer to specify that the request should be treated as going to a <code>local</code> or <code>loopback</code> address space. This allows for HTTP local network requests that are not private IP literals or .local domains as long as they are explicitly tagged with the target address space. This also allows the developer to deterministically request the permission.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with HTML</h3><a id="user-content-integration-with-html" aria-label="Permalink: Integration with HTML" href="#integration-with-html"></a></p>
<p dir="auto"><code>Document</code>s and <code>WorkerGlobalScope</code>s store an additional <strong>address space</strong> value. This is initialized from the IP address the document or worker was sourced from.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with WebRTC</h3><a id="user-content-integration-with-webrtc" aria-label="Permalink: Integration with WebRTC" href="#integration-with-webrtc"></a></p>
<p dir="auto">Local connection attempts that use WebRTC should also be gated behind the Local Network Access permission prompt.</p>
<p dir="auto">In the WebRTC spec, algorithms that add candidates to the ICE Agent already have steps that ensure <a href="https://www.w3.org/TR/webrtc/#dfn-administratively-prohibited" rel="nofollow">“administratively prohibited”</a> addresses are not used. We can modify these algorithms to perform the following steps if the candidate has a loopback or local address:</p>
<ul dir="auto">
<li>Check if the origin has previously been granted the local network access permission; if not, prompt the user.</li>
<li>If the user grants permission, the algorithm will continue.</li>
<li>If the user denies the permission, we won’t add the candidate to the ICE Agent and it won’t be used when establishing a connection.</li>
</ul>
<p dir="auto">Note that these checks are done asynchronously and don’t block resolving the methods where they are used i.e. setRemoteDescription() and addIceCandidate().</p>
<p dir="auto">The same checks should also be performed when connecting to STUN/TURN servers with loopback or local addresses.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Permissions Policy</h3><a id="user-content-integration-with-permissions-policy" aria-label="Permalink: Integration with Permissions Policy" href="#integration-with-permissions-policy"></a></p>
<p dir="auto">By default the ability to make local network requests will be limited to top-level documents that are secure contexts. There are use cases where a site needs to be able to delegate this permission into a subframe. To support these use cases, a new policy-controlled feature ("local-network-access") will be added that will allow top-level documents to delegate access to this feature to subframes.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Permissions API</h3><a id="user-content-integration-with-permissions-api" aria-label="Permalink: Integration with Permissions API" href="#integration-with-permissions-api"></a></p>
<p dir="auto">The permission will be integrated with the Permissions API, which will allow sites to query the status of the permission grant.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">HTML subresources</h3><a id="user-content-html-subresources" aria-label="Permalink: HTML subresources" href="#html-subresources"></a></p>
<p dir="auto">HTML subresource fetches go through the standard Fetch algorithm, but will not have the ability to specify an explicit <code>targetAddressSpace</code>. This includes subframe navigations.</p>
<p dir="auto">For HTTP subresources, only "local names" (i.e., private IP literals or .local hostnames) are allowed for local network requests. This is required for resolving the mixed content problem (see "Mixed content" above, and see "Considered alternatives" for more involved methods that have been discussed).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Websockets</h3><a id="user-content-websockets" aria-label="Permalink: Websockets" href="#websockets"></a></p>
<p dir="auto">The <strong>establish a WebSocket connection</strong> algorithm depends on the Fetch algorithm (<a href="https://websockets.spec.whatwg.org/#websocket-protocol" rel="nofollow">in the updated WHATWG spec</a>), so Websockets should behave like other Fetch requests and trigger local network access prompts without additional work.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Service Workers</h3><a id="user-content-service-workers" aria-label="Permalink: Service Workers" href="#service-workers"></a></p>
<p dir="auto">Requests from a service worker go through the Fetch algorithm, and will be included in local network access restrictions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Considered alternatives</h2><a id="user-content-considered-alternatives" aria-label="Permalink: Considered alternatives" href="#considered-alternatives"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Private Network Access (PNA)</h3><a id="user-content-private-network-access-pna" aria-label="Permalink: Private Network Access (PNA)" href="#private-network-access-pna"></a></p>
<p dir="auto">The previously proposed <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md">Private Network Access work</a> (PNA for short, also previously referred to as CORS-RFC1918) required a secure CORS preflight response from the private subresource server. If the preflight failed, the request would be blocked. If the request was for an insecure resource (e.g., due to the lack of trusted local HTTPS), they <a href="https://github.com/WICG/private-network-access/blob/main/permission_prompt/explainer.md">proposed a separate permission prompt</a> to allow the connection to a specific endpoint device <em>and</em> relax the mixed content restrictions on the connection.</p>
<p dir="auto">A lot of effort and developer outreach went into this effort, but it was never able to ship. Chrome currently has an opt-in mode behind an enterprise policy, and for a while Chrome shipped a restriction where private network access was restricted to secure contexts only (with a deprecation trial for developers who could not yet meet this requirement due to having HTTP-only private network endpoints). The previous plan was to build and ship the "mixed content permission prompt" to get these remaining developers out of the reverse origin trial.</p>
<p dir="auto">PNA met a lot of different developer and user needs, and in the "good case" (secure website talking to a local network device that had a publicly trusted TLS certificate and a "PNA-aware" server) could be quite seamless, since it required no user intervention. In the non-ideal cases, PNA accumulated a lot of workarounds to address use cases that would result in a permission prompt in many cases (a device chooser style prompt for insecure devices). For example:</p>
<ul dir="auto">
<li>Some routers filter out DNS responses mapping public domain names to private IP addresses -- this means that <a href="https://github.com/WICG/private-network-access/issues/23" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/23/hovercard">fallback to direct (insecure) private IP addresses is required</a> (and thus a permission prompt to bypass mixed content blocking), even for developers who provision publicly trusted certs to local network servers.</li>
<li>Some devices could not update to supply preflight, so the permission prompt was relaxed to allow this (<a href="https://github.com/WICG/private-network-access/blob/main/permission_prompt/explainer.md#ephemeral-permission">but only grant an ephemeral permission</a>).</li>
</ul>
<p dir="auto">Even if the local device has opted in to connections from a top level site, we believe there is value in user awareness and control over this exchange.</p>
<p dir="auto">The use of preflights (without any user consent speed bump) also exposed its own risks. For example, timing attacks could be used to determine valid IP addresses on the network (<a href="https://crbug.com/40051437" rel="nofollow">crbug.com/40051437</a>, <a data-error-text="Failed to load title" data-id="822980731" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/41" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/41/hovercard" href="https://github.com/WICG/private-network-access/issues/41">WICG/private-network-access#41</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Block all local network requests</h3><a id="user-content-block-all-local-network-requests" aria-label="Permalink: Block all local network requests" href="#block-all-local-network-requests"></a></p>
<p dir="auto">Given the risks of allowing sites to use the browser as an access point into the user's local network, we could simply block all local network requests (or just any requests that cross from a public address space to a more private one). This would be simplest, but would break many existing use cases, or require expensive workarounds.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Do nothing</h3><a id="user-content-do-nothing" aria-label="Permalink: Do nothing" href="#do-nothing"></a></p>
<p dir="auto">As more operating systems are implementing local network access permissions at the application level, we believe it is the duty of user agents to broker access to that privilege in order to be good stewards of it. A user may have legitimate reasons to grant access for their <em>browser</em> (e.g., Chrome's "cast this tab" functionality) but never want a <em>site</em> to be able to access their local network.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alternatives for the mixed content problem</h3><a id="user-content-alternatives-for-the-mixed-content-problem" aria-label="Permalink: Alternatives for the mixed content problem" href="#alternatives-for-the-mixed-content-problem"></a></p>
<p dir="auto">In our proposal above we recommend restricting local network HTML subresources to "assumed local" hostnames (such as <code>.local</code> domains) as a middle ground that meets developer needs, is relatively easy to deploy, and doesn't require complex technical or specification work to accomplish.</p>
<p dir="auto">Below are some alternatives that have been considered for addressing the mixed content problem.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Require secure local network subresources</h4><a id="user-content-require-secure-local-network-subresources" aria-label="Permalink: Require secure local network subresources" href="#require-secure-local-network-subresources"></a></p>
<p dir="auto">In order to restrict local network access to secure contexts (which is necessary in order for a permission prompt to make sense), we need some resolution of the mixed content problem. The Fetch specification orders the "upgrade or block mixed content" steps <em>before</em> we have obtained a connection, and thus before we can know the IP address.</p>
<p dir="auto">Currently, some developers can work around this by getting publicly trusted certificates for their servers running on local networks (e.g., Plex getting Let's Encrypt certs under a different subdomain for every install) but it is a substantial engineering and maintenance burden. Even for developers that go to the trouble of using publicly trusted certificates, <a href="https://github.com/WICG/private-network-access/issues/23" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/23/hovercard">fallback to HTTP is required in some network circumstances</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">List subresource address space details in a header or meta tag</h4><a id="user-content-list-subresource-address-space-details-in-a-header-or-meta-tag" aria-label="Permalink: List subresource address space details in a header or meta tag" href="#list-subresource-address-space-details-in-a-header-or-meta-tag"></a></p>
<p dir="auto">This could be a "treat hostname as public" / "treat hostname as local" property that could be specified in a response header or in a meta tag (or a response header meta equiv).</p>
<p dir="auto">An initial idea was to add this on to <a href="https://www.w3.org/TR/CSP3/" rel="nofollow">CSP</a>, however that was rejected due to CSP already being a bit overloaded and this not being a great conceptual fit for it. A separate "Sec-Treat-Origin-As-Private" header (or something along those lines) could be used to list origins that should be assumed to resolve to private IP addresses.</p>
<p dir="auto">Additionally, it might be useful if such a header could be specified via an http-equiv meta tag (like one can do with CSP).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Potential future changes</h3><a id="user-content-potential-future-changes" aria-label="Permalink: Potential future changes" href="#potential-future-changes"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Top-level navigations to local network</h4><a id="user-content-top-level-navigations-to-local-network" aria-label="Permalink: Top-level navigations to local network" href="#top-level-navigations-to-local-network"></a></p>
<p dir="auto">Top-level navigations remain a risk after restrictions on subresource local network requests are in place. For an attacker, main frame navigations are noisier (compared to subresource requests and iframe navigations), although popunder techniques could potentially be used to hide navigations from the user.</p>
<p dir="auto">To prevent these, we could block or show an interstitial warning when a public page navigates to a local one. To avoid too much breakage or over-warning, we could maybe scope protections to just these cases. We might consider that "complex" requests such as POST navigations and GET requests with URL parameters are particularly risky. We might also be able to "defang" navigations by stripping the URL parameters to reduce the risk of exploitation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Consider all cross-origin requests to private addresses as "local network requests"</h4><a id="user-content-consider-all-cross-origin-requests-to-private-addresses-as-local-network-requests" aria-label="Permalink: Consider all cross-origin requests to private addresses as &quot;local network requests&quot;" href="#consider-all-cross-origin-requests-to-private-addresses-as-local-network-requests"></a></p>
<p dir="auto">We could instead define a <strong>local network request</strong> as "any request targeting an IP address in the local or loopback address space, regardless of the requestor's address space", while maintaining the exception for not blocking same-origin requests. This is a stricter / broader definition, and would likely cause more widespread breakage than our proposal to only consider requests that cross from one address space class to a more private one.</p>
<p dir="auto">This would also allow the mixed content relaxation that is granted when an origin is given the local network access permission to apply to <code>local</code> -&gt; <code>local</code> or <code>loopback</code> -&gt; <code>local</code> requests. This has been raised as a concern by developers in <a data-error-text="Failed to load title" data-id="1795130611" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/109" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/109/hovercard" href="https://github.com/WICG/private-network-access/issues/109">WICG/private-network-access#109</a>. Note that the status quo could continue here for now -- i.e., the top level page can get around mixed content checks by remaining on HTTP.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">More granular permission grants</h4><a id="user-content-more-granular-permission-grants" aria-label="Permalink: More granular permission grants" href="#more-granular-permission-grants"></a></p>
<p dir="auto">A <a href="https://martina.lindorfer.in/files/papers/nwscanning_oakland25.pdf" rel="nofollow">recent study</a> (Schmidt et al., S&amp;P 2025) found that the <em>transitivity</em> of the local networks permission on iOS was the hardest for users to understand. It might be beneficial to scope the permission grant to an origin to the specific local network the user is currently connected to. Should the user grant permission to example.com on their home network, it may be reasonable for a UA to re-prompt the user if they bring their device to a different location and visit example.com again.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Add address space properties to HTML</h4><a id="user-content-add-address-space-properties-to-html" aria-label="Permalink: Add address space properties to HTML" href="#add-address-space-properties-to-html"></a></p>
<p dir="auto">PNA 1.0 worked to add a new <code>targetAddressSpace</code> parameter to <code>fetch()</code> to label the target address space of the request, so that mixed content checks could be relaxed (and then enforced if that connection ended up being public, to avoid it being a mixed content blocking bypass). The challenge is how to handle HTML subresources (e.g., img, iframe, etc.). We could add a new property to these HTML elements allowing developers to "label" them as public/local/loopback. This would function similarly to the parameter on <code>fetch()</code> and allow the user agent to initially bypass mixed content checks (and to know it should trigger the permission prompt). Currently we don't think this is necessary, as most use cases that require explicitly marking the <code>targetAddressSpace</code> should be able to switch to using <code>fetch()</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security &amp; Privacy Considerations</h2><a id="user-content-security--privacy-considerations" aria-label="Permalink: Security &amp; Privacy Considerations" href="#security--privacy-considerations"></a></p>
<p dir="auto"><strong>Security</strong></p>
<ul dir="auto">
<li>While the local network access permission exempts requests to a priori known local endpoints from mixed content blocking, the page should still be considered to have loaded mixed content if such a request is made. This means that, for example, browsers can choose to show a different security UI for pages that make insecure connections to the local network.</li>
<li>Compared to the original PNA proposal, a site granted the local network access permission has more power to probe and connect to devices on the local network, regardless of whether those devices expect it.</li>
<li>There is some risk of users accepting the permission without understanding it (which couldn't happen with preflights).</li>
</ul>
<p dir="auto"><strong>Privacy</strong></p>
<ul dir="auto">
<li>Compared to the original PNA proposal, no local network connections are allowed until the user has explicitly granted permission to a site.</li>
<li>Compared to the original PNA proposal, there are no preflights (and thus no risk of timing/probing attacks from them).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stakeholder Feedback / Opposition</h2><a id="user-content-stakeholder-feedback--opposition" aria-label="Permalink: Stakeholder Feedback / Opposition" href="#stakeholder-feedback--opposition"></a></p>
<p dir="auto">The previous PNA proposal (using preflights) was positively received by Mozilla (<a data-error-text="Failed to load title" data-id="419961868" data-permission-text="Title is private" data-url="https://github.com/mozilla/standards-positions/issues/143" data-hovercard-type="issue" data-hovercard-url="/mozilla/standards-positions/issues/143/hovercard" href="https://github.com/mozilla/standards-positions/issues/143">mozilla/standards-positions#143</a>) and WebKit (<a data-error-text="Failed to load title" data-id="1656934582" data-permission-text="Title is private" data-url="https://github.com/WebKit/standards-positions/issues/163" data-hovercard-type="issue" data-hovercard-url="/WebKit/standards-positions/issues/163/hovercard" href="https://github.com/WebKit/standards-positions/issues/163">WebKit/standards-positions#163</a>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">References &amp; acknowledgements</h2><a id="user-content-references--acknowledgements" aria-label="Permalink: References &amp; acknowledgements" href="#references--acknowledgements"></a></p>
<p dir="auto">Many thanks for valuable feedback and advice from:</p>
<ul dir="auto">
<li>Titouan Rigoudy, Jonathan Hao, and Yifan Luo who worked on the original PNA proposals and specification, and generously discussed their work with me and helped brainstorm paths forward.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The iPhone 15 Pro's Depth Maps (279 pts)]]></title>
            <link>https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html</link>
            <guid>44183591</guid>
            <pubDate>Wed, 04 Jun 2025 17:57:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html">https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html</a>, See on <a href="https://news.ycombinator.com/item?id=44183591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_text">
            <p>Since 2017, Apple have supported depth maps in the images its iPhones capture either via LiDAR scanners, 3D <a href="https://en.wikipedia.org/wiki/Time-of-flight_camera">time-of-flight</a> scanner-less LIDAR or <a href="https://en.wikipedia.org/wiki/Structured-light_3D_scanner">structured-light</a> 3D scanning.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/brave_j61zCFLlcU.png">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_j61zCFLlcU.png">
</a></p><p>These depth maps, along with other imagery, are stored in High Efficiency Image File Format (HEIF) container files. These files can contain multiple images and vast amounts of metadata. This format was originally designed between 2013 and 2015 and Apple adopted its HEIC variant in 2017.</p>
<p>Since then, HEIC files are the default storage container format for images captured on the iPhone. But with that said, it is possible to use the JPEG format instead if things like depth maps and HDR are not of interest.</p>
<p>Finn Jaeger, who is the head of VFX at Replayboys, a film production firm in Hamburg, Germany, <a href="https://www.linkedin.com/posts/finn-j%C3%A4ger-b44058176_did-you-know-your-iphone-heic-files-are-activity-7319460880750940162-Rn23/">posted</a> a screenshot a few weeks ago showing how multiple depth maps were being produced by his iPhone.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/brave_qqlvRZSjK9.png">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_qqlvRZSjK9.png">
</a></p><p>He announced he was working on a project called <a href="https://github.com/finnschi/heic-shenanigans">HEIC Shenanigans</a>. This project contains scripts to separate out images and their metadata from HEIC containers as well as convert them into EXR Files. As of this writing, the project contains 374 lines of Python.</p>
<p>In this post, I'll walk through Finn's codebase with an example image from an iPhone 15 Pro.</p>
<div id="my-workstation">
<h2>My Workstation</h2>
<p>I'm using a 5.7 GHz AMD Ryzen 9 9950X CPU. It has 16 cores and 32 threads and 1.2 MB of L1, 16 MB of L2 and 64 MB of L3 cache. It has a liquid cooler attached and is housed in a spacious, full-sized Cooler Master HAF 700 computer case.</p>
<p>The system has 96 GB of DDR5 RAM clocked at 4,800 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.</p>
<p>The system is powered by a 1,200-watt, fully modular Corsair Power Supply and is sat on an ASRock X870E Nova 90 Motherboard.</p>
<p>I'm running Ubuntu 24 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and ArcGIS Pro only supports Windows natively.</p>
</div>
<div id="installing-prerequisites">
<h2>Installing Prerequisites</h2>
<p>I'll use Python 3.12.3 and a few other tools in this post.</p>
<div><pre><span></span>$<span> </span>sudo<span> </span>add-apt-repository<span> </span>ppa:deadsnakes/ppa
$<span> </span>sudo<span> </span>apt<span> </span>update
$<span> </span>sudo<span> </span>apt<span> </span>install<span> </span><span>\</span>
<span>    </span>jq<span> </span><span>\</span>
<span>    </span>openexr<span> </span><span>\</span>
<span>    </span>libimage-exiftool-perl<span> </span><span>\</span>
<span>    </span>libopenexr-dev<span> </span><span>\</span>
<span>    </span>python3-pip<span> </span><span>\</span>
<span>    </span>python3.12-venv
</pre></div>
<p>Note, the <tt><span>libimage-exiftool-perl</span></tt> package installs <tt>exiftool</tt> version 12.76+dfsg-1 which was released at the end of January 2024. Since then, there have been at least ten releases that have addressed issues or enhanced exiftool's HEIC support.</p>
<p>The above version should work fine for the steps in this post but be mindful that any issues you encounter going forward might be resolved with a more up to date version of exiftool.</p>
<p>I'll be using <a href="https://github.com/kellyjonbrazil/jc?tab=readme-ov-file">JSON Convert</a> (jc) to convert the output of various CLI tools into JSON.</p>
<div><pre><span></span>$<span> </span>wget<span> </span>https://github.com/kellyjonbrazil/jc/releases/download/v1.25.2/jc_1.25.2-1_amd64.deb
$<span> </span>sudo<span> </span>dpkg<span> </span>-i<span> </span>jc_1.25.2-1_amd64.deb
</pre></div>
<p>I'll clone Finn Jaeger's HEIC Shenanigans repo.</p>
<div><pre><span></span>$<span> </span>git<span> </span>clone<span> </span>https://github.com/finnschi/heic-shenanigans<span> </span><span>\</span>
<span>    </span>~/heic-shenanigans
</pre></div>
<p>I'll set up a Python Virtual Environment and install a few dependencies.</p>
<div><pre><span></span>$<span> </span>python3<span> </span>-m<span> </span>venv<span> </span>~/.iphone_depth
$<span> </span><span>source</span><span> </span>~/.iphone_depth/bin/activate

$<span> </span>python3<span> </span>-m<span> </span>pip<span> </span>install<span> </span>-r<span> </span><span>\</span>
<span>    </span>~/heic-shenanigans/requirements.txt
$<span> </span>python3<span> </span>-m<span> </span>pip<span> </span>install<span> </span><span>\</span>
<span>    </span>OpenImageIO
</pre></div>
<p>I'll use <a href="https://github.com/darbyjohnston/DJV/releases">DJV</a> v2.0.8 to view EXR imagery produced in this post.</p>
</div>
<div id="an-iphone-15-pro-image">
<h2>An iPhone 15 Pro Image</h2>
<p><a href="https://www.linkedin.com/in/joel-joseph-b865981ab/">Joel Joseph</a>, a subject matter expert on the ArcGIS Desktop Products Suite in Mumbai, India, was kind enough to send me an HEIC-contained image he took on his iPhone 15 Pro. I'll use this image in this post. Below is a screenshot.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/Photos_5WbqRgJbWO.jpg">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/Photos_5WbqRgJbWO.jpg">
</a>
</p></div>

<div id="converting-an-heic-into-exr">
<h2>Converting an HEIC into EXR</h2>
<p>The <a href="https://github.com/AcademySoftwareFoundation">Academy Software Foundation</a> fosters various <a href="https://www.aswf.io/projects/">open source projects</a> and standards used in film, television and other creative industries. Its <a href="https://www.aswf.io/members/">members</a> include the Academy of Motion Picture Arts and Sciences, Disney, Nvidia and Netflix to name a few.</p>
<p>Below is a screenshot of their <a href="https://landscape.aswf.io/">landscape</a> of projects.</p>
<p><a href="https://landscape.aswf.io/">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_g5ZWZ7GWLE.png">
</a></p><p>One of their projects is <a href="https://github.com/AcademySoftwareFoundation/openexr">OpenEXR</a>, a high-dynamic range (HDR) image file format. It was originally developed by Industrial Light and Magic (ILM) in 1999 and was later made open source in 2003. It's used in producing visual effects and 3D renderings.</p>
<p>Below I'll convert iPhone 15 Pro image into an OpenEXR file.</p>
<div><pre><span></span>$<span> </span>python<span> </span>~/heic-shenanigans/heic_to_exr.py<span> </span><span>\</span>
<span>    </span>IMG_E2153.HEIC
</pre></div>
<p>The resulting file is 468 MB. Below is a screenshot of it in DJV.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/djv_EIMLqnjJmM.jpg">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/djv_EIMLqnjJmM.jpg">
</a></p><p>The above file was produced by making several calls to <tt>oiiotool</tt>, an image processing tool from the <a href="https://github.com/AcademySoftwareFoundation/OpenImageIO">OpenImageIO</a> project, which is also apart of the Academy Software Foundation.</p>
<p>Below I've annotated the calls to oiiotool made by the above script.</p>
<p>The script will first get the dimensions of the source image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>--info<span> </span>/tmp/tmpc3kmiaka/input_base.tiff
</pre></div>
<p>It will then produce a base image, converting the source image from an sRGB curve through Linear P3 and onto ACEScg.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_base.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>R,G,B<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>sdr.R,sdr.G,sdr.B<span> </span><span>\</span>
<span>    </span>--colorconfig<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio<span> </span><span>\</span>
<span>    </span>--colorconvert<span> </span><span>'sRGB - Texture'</span><span> </span><span>'Linear Rec.709 (sRGB)'</span><span> </span><span>\</span>
<span>    </span>--colorconvert<span> </span><span>'Linear P3-D65'</span><span>  </span><span>'ACES - ACEScg'</span><span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/base.exr
</pre></div>
<p>The above OCIO file is an <a href="https://opencolorio.org/">OpenColorIO</a> colour profile file. OpenColorIO, also under the Academy Software Foundation, is a colour management suite. The project provides several <a href="https://github.com/AcademySoftwareFoundation/OpenColorIO-Config-ACES/releases">reference configurations</a> including one from the <a href="https://en.wikipedia.org/wiki/Academy_Color_Encoding_System">Academy Color Encoding System (ACES)</a>. The OCIO file is text-based and contains 1,242 lines of content.</p>
<div><pre><span></span>$<span> </span>head<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio
</pre></div>
<div><pre><span></span>ocio_profile_version: 2.1

environment:
  {}
search_path: ""
strictparsing: true
luma: [0.2126, 0.7152, 0.0722]
name: studio-config-v1.0.0_aces-v1.3_ocio-v2.1
description: |
  Academy Color Encoding System - Studio Config [COLORSPACES v1.0.0] [ACES v1.3] [OCIO v2.1]
</pre></div>
<p>Then, an EXR-based gain map will be produced by converting the source HDR gain map from Rec709 curve to Linear.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_hdrgainmap_50.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>gainmap.Y<span> </span><span>\</span>
<span>    </span>--resize<span> </span>4032x3024<span> </span><span>\</span>
<span>    </span>--colorconfig<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio<span> </span><span>\</span>
<span>    </span>--ocionamedtransform<span> </span><span>'Rec.709 - Curve'</span><span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap.exr
</pre></div>
<p>That gain map will then be converted into RGB by duplicating the Y channel three times.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/gainmap.exr<span> </span><span>\</span>
<span>    </span>--ch<span> </span>gainmap.Y,gainmap.Y,gainmap.Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>gainmap.R,gainmap.G,gainmap.B<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr
</pre></div>
<p>Then, the HDR gain map's headroom value will be extracted.</p>
<div><pre><span></span>$<span> </span>exiftool<span> </span>-HDRGainMapHeadroom<span> </span>-b<span> </span>/tmp/tmpc3kmiaka/input_base.tiff
</pre></div>
<p>The gain map will then be scaled using the inverse of the HDR headroom value captured above.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr<span> </span><span>\</span>
<span>    </span>--mulc<span> </span>-0.12135654640000004<span> </span><span>\</span>
<span>    </span>--addc<span> </span><span>1</span>.0<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap_scaled.exr
</pre></div>
<p>Then, the base image will be multiplied by the scaled gain map to create an HDR base image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/base.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/gainmap_scaled.exr<span> </span><span>\</span>
<span>           </span>--mul<span> </span><span>\</span>
<span>           </span>--chnames<span> </span>R,G,B<span> </span><span>\</span>
</pre></div>
<p>Then, the Y channel from the depth map will be used to create an EXR-formatted depth map.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_depth_0.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>depth.Y<span> </span><span>\</span>
<span>    </span>--resize<span> </span>4032x3024<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/depth.exr
</pre></div>
<p>If the source image contained mattes they would be processed at this stage.</p>
<p>The first step in constructing the final EXR file starts by copying in the RGB channels from the EXR-formatted HDR base image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/hdr_base.exr<span> </span><span>\</span>
<span>    </span>--ch<span> </span>R,G,B<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the SDR channels are added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/base.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>sdr.R,sdr.G,sdr.B<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the gain map is added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>gainmap.R,gainmap.G,gainmap.B<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the depth map is added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/depth.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>depth.Y<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>If there were any matte layers, they would be added here.</p>
<p>The <tt>final.exr</tt> file is then moved to a <tt>&lt;prefix&gt;_acesCG.exr</tt> file alongside the source imagery.</p>
</div>

        </div><p>
            Thank you for taking the time to read this post. I offer both consulting and hands-on development services to clients in North America and Europe. If you'd like to discuss how my offerings can help your business please contact me via <a href="https://uk.linkedin.com/in/marklitwintschik/">LinkedIn</a>.
        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Code (188 pts)]]></title>
            <link>https://mistral.ai/products/mistral-code</link>
            <guid>44183515</guid>
            <pubDate>Wed, 04 Jun 2025 17:50:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/products/mistral-code">https://mistral.ai/products/mistral-code</a>, See on <a href="https://news.ycombinator.com/item?id=44183515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><nav aria-label="Main" data-orientation="horizontal" dir="ltr"><div><a rel="home" aria-label="Home" href="https://mistral.ai/"></a><div><ul data-orientation="horizontal" dir="ltr"><li></li><li></li><li></li><li></li><li><a target="_self" href="https://mistral.ai/pricing">Pricing</a></li><li></li></ul></div></div></nav></div><div><div><p>Lightning-fast completions, deep code understanding, and agentic software engineering—right where you work.</p></div><div><p><img alt="744a3c97-aba4-4cbd-9697-a4a4ead168c1" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=3840&amp;q=75"></p></div></div><div id=""><div><p>Code with intelligence, control with confidence.</p><p>Transform your development workflows with an AI coding assistant that deeply understands your codebase. Mistral Code delivers intelligent code completion, generation, and autonomous task execution right where you work.</p></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div><div><h3><p>Code with intelligence, control with confidence.</p></h3><p>Transform your development workflows with an AI coding assistant that deeply understands your codebase. Mistral Code delivers intelligent code completion, generation, and autonomous task execution right where you work.</p></div><div data-state="active" data-orientation="horizontal" role="tabpanel" aria-labelledby="radix-«Rm6fdbnb»-trigger-Intelligent coding assistance" id="radix-«Rm6fdbnb»-content-Intelligent coding assistance" tabindex="0" dir="ltr"><div><p>Elevate your coding with AI that understands, completes, and optimizes your code with frontier, purpose-built software engineering models.</p></div><div><p><img alt="ac37ae27-1d08-49aa-b191-26758baca735" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=3840&amp;q=75"></p></div></div></div></div><div id=""><div><h3><p>Why Mistral Code?</p></h3></div><div><div><div><p><img alt="icon-key-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=3840&amp;q=75"></p></div><div><p><span><p>State-of-the art AI-powered software development, in your control.</p></span></p><p>10X developer productivity and efficiency without sacrificing the privacy and safety of your codebase. Mistral Code can be deployed where your most important code resides.</p></div></div><div><div><p><img alt="icon-pencil-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=3840&amp;q=75"></p></div><div><p><span><p>Frontier coding models, fully customizable and tunable to your codebase.</p></span></p><p>Advanced models such as Codestral and Devstral under the hood, further customizable with fine-tuning, post-training, and distillation.</p></div></div><div><p><span><p>One platform across the full AI-powered software stack.</p></span></p><p>End‑to‑end tooling from models to IDE in one first-party platform. Unified SLA, faster support, and simpler compliance posture.</p></div><div><div><p><img alt="icon-lightning-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=3840&amp;q=75"></p></div><div><p><span><p>Deep code understanding, generation, and completion.</p></span></p><p>Advanced understanding of files and integrations with agent scaffolds for benchmark-setting performance on agentic tasks, along with lightning-fast code generation and completion.</p></div></div></div></div><div id=""><h4>
<center>Frontier intelligence, in your favorite IDE.</center>
</h4><div>
<center>Our unique mix of purpose-built models delivers an unmatched combination of lightning-fast completions and deep code understanding.</center>
</div></div><div id=""><p><img alt="c7ab7262-be13-45c1-8498-e37551db9527" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=3840&amp;q=75"></p></div><div id=""><p>Speed development time.</p><div><div><div><p>Tab to complete</p><p>Get intelligent code suggestions in real-time as you type, with multi-line completions tailored to your codebase.</p></div><p><img alt="350b6506-9400-45c2-8c1f-e79f7f56ba66" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75"></p></div><div><div><p>Context-aware chat</p><p>Ask questions about your code with the AI assistant that understands your entire project.</p></div><p><img alt="4778258c-77fe-4535-b018-d48546f1f0ba" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75"></p></div><div><div><p>Code edit</p><p>Select any code block and transform it using natural language instructions.</p></div><p><img alt="69a1aa81-e622-4844-a100-bd86b5705e98" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75"></p></div><div><div><p>Intelligent search and retrieval</p><p>Find and fetch relevant code through natural language queries.</p></div><p><img alt="80c19f46-01f7-45fa-a34f-42e2dc161ddc" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75"></p></div><div><div><p>Autonomous coding</p><p>Handle complex engineering tasks without leaving your IDE.</p></div><p><img alt="5e1da16b-081d-44d3-b6d4-b5859ebcb75e" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75"></p></div></div><div><div><div><p>Tab to complete</p><p>Get intelligent code suggestions in real-time as you type, with multi-line completions tailored to your codebase.</p></div><p><img alt="350b6506-9400-45c2-8c1f-e79f7f56ba66" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75"></p></div><div><div><p>Context-aware chat</p><p>Ask questions about your code with the AI assistant that understands your entire project.</p></div><p><img alt="4778258c-77fe-4535-b018-d48546f1f0ba" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75"></p></div><div><div><p>Code edit</p><p>Select any code block and transform it using natural language instructions.</p></div><p><img alt="69a1aa81-e622-4844-a100-bd86b5705e98" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75"></p></div><div><div><p>Intelligent search and retrieval</p><p>Find and fetch relevant code through natural language queries.</p></div><p><img alt="80c19f46-01f7-45fa-a34f-42e2dc161ddc" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75"></p></div><div><div><p>Autonomous coding</p><p>Handle complex engineering tasks without leaving your IDE.</p></div><p><img alt="5e1da16b-081d-44d3-b6d4-b5859ebcb75e" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75"></p></div></div></div><section><div><h3><p>How teams use Mistral Code today.</p></h3></div><div><div><div><p>Code completion</p><div><p><img alt="check-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=64&amp;q=75"></p><p>Suggests completions for code as you type, helping to speed up the coding process and reduce errors.</p></div></div><div><p>Code debugging and refactoring</p><div><p><img alt="glasses-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=64&amp;q=75"></p><p>Identifies syntax and logical errors, typos in real-time and provides suggestions for improving code structure.</p></div></div><div><p>Code documentation</p><div><p><img alt="icon-copy-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=64&amp;q=75"></p><p>Automatically generates documentation for code, including comments and API documentation, to improve code maintainability.</p></div></div></div><div><div><p>Code testing</p><div><p><img alt="icon-computer-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=64&amp;q=75"></p><p>Generates unit and system tests to ensure that the code generated is fully functional.</p></div></div><div><p>Code migration</p><div><p><img alt="icon-earth-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=64&amp;q=75"></p><p>Generate code snippets in the target language, helping to translate and adapt existing codebases to new languages or frameworks.</p></div></div><div><p>Code performance</p><div><p><img alt="icon-lightbulb" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=64&amp;q=75"></p><p>Analyzes code performance and identifies bottlenecks, helping developers optimize their code for better speed and efficiency.</p></div></div></div></div></section><div id=""><p>Powering the world's pioneers.</p></div><div id=""><h2><p>Your AI, from models to outputs.</p></h2><p>Experience the power of Mistral Code directly in your favorite IDE.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[When memory was measured in kilobytes: The art of efficient vision (104 pts)]]></title>
            <link>https://www.softwareheritage.org/2025/06/04/history_computer_vision/</link>
            <guid>44182698</guid>
            <pubDate>Wed, 04 Jun 2025 16:46:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.softwareheritage.org/2025/06/04/history_computer_vision/">https://www.softwareheritage.org/2025/06/04/history_computer_vision/</a>, See on <a href="https://news.ycombinator.com/item?id=44182698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<p><em>By Mathilde Fichen</em></p>



<p>In the early days of computer vision, when memory was scarce and every byte counted, innovation thrived under constraint. “An Efficient Chain-Linking Algorithm,” developed at Inria in the late 1980s, is a brilliant example of this spirit. Now preserved and shared by Software Heritage, this compact yet powerful piece of C code showcases how elegance and efficiency went hand in hand in outlining the future of image processing—one pixel chain at a time.</p>



<p>The code resulted from research work carried out between 1985 and 1991 at Inria, by Gérard Giraudon (research and principal investigator), Philippe Garnesson (a PhD student), and Patrick Cipière (software engineer). Down in sunny Sophia Antipolis, a <a href="https://en.wikipedia.org/wiki/Sophia_Antipolis">tech park</a> 20 minutes inland from Antibes, the team tackled computer vision with a distinctly local flavor. They called themselves PASTIS, a playful nod to the anise drink. Still, the acronym – Scene Analysis and Symbolic Image Processing Project (Projet d’Analyse de Scène et de Traitement d’Image Symbolique) – hinted at their serious mission.</p>





<h2>Preserving Inria legacy software</h2>



<p>The effort to preserve this source code is part of a broader initiative to preserve the legacy codes of <a href="https://www.inria.fr/en">Inria</a> (France’s National Institute for Research in Digital Science and Technology), launched in 2023 in a joint effort of Software Heritage, Inria Alumni, and Inria. The project to preserve Inria’s legacy software started by reaching out to the institute’s community, past and present, through a survey (as detailed in our <a href="https://ipres2024.pubpub.org/pub/hdap1420/release/1?readingCollection=21e62c05">iPres article</a>.)  This initial outreach informed a dedicated, <a href="https://www.softwareheritage.org/2024/12/10/preserving-inrias-legacy-software/">hands-on workshop in 2024</a>, which kicked off the practical work of exploring these historical codes. The current focus is on securely archiving the important legacy software we’ve identified within Software Heritage. Sharing the stories behind these codes with the broader community is just as vital.</p>



<p>The recovery of some code has been surprisingly straightforward. For instance, the code for “An Efficient Chain-Linking Algorithm” was readily accessible thanks to Gérard Giraudon’s personal preservation efforts on a local drive. That small success story is a reminder of how important individual initiative is for preserving digital work. Each piece of software recovered isn’t just code; it’s a piece of research history, carrying the stories of the people who created it.</p>


<div>
<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-scaled.jpg"><img decoding="async" width="1024" height="576" src="https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-1024x576.jpg" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-1024x576.jpg 1024w, https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-300x169.jpg 300w, https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-768x432.jpg 768w, https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-1536x864.jpg 1536w, https://www.softwareheritage.org/wp-content/uploads/2024/11/swhap-workshop-2048x1152.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption><em>Inria alumni workshop, October 2024, Inria Paris.</em></figcaption></figure></div>


<h2>An algorithm for outlining images&nbsp;</h2>



<p>The chain-linking algorithm processes a 2D pixel matrix, typically the output of a contour detection step.</p>


<div>
<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2025/05/image5.png"><img loading="lazy" decoding="async" width="325" height="307" src="https://www.softwareheritage.org/wp-content/uploads/2025/05/image5.png" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2025/05/image5.png 325w, https://www.softwareheritage.org/wp-content/uploads/2025/05/image5-300x283.png 300w" sizes="auto, (max-width: 325px) 100vw, 325px"></a></figure></div>


<p><em>Raw image used as input for the algorithm. Source: Chaînage efficace de contour, n° 605, Février 1987, Gérard Giraudon</em></p>



<p>The output is a list of contour chains. Each chain in this list is a sequence of pixel coordinates that define a continuous boundary, ready for further steps like polygonal approximation or shape analysis..</p>



<p>Processed image. Source : Chaînage efficace de contour, n° 605, Février 1987, Gérard Giraudon</p>



<p>Basically, the chain-linking connects the edge pixels to create smooth outlines, just like tracing the shape with a pencil.</p>



<h2>Computer vision in the 80s</h2>



<p>The mid-1980s aspiration of seeing robots hit a speed bump: computer vision algorithms weren’t fast enough. The core challenge? Real-time performance – the essential ingredient for giving robots camera “eyes” that could truly see and react. At the time, such a system was conceived as a pipeline of operations from image acquisition (one shot or video) to a decision-making system.&nbsp;</p>



<p>The goal was to extract meaningful information from raw image data, typically represented as an 8-bit matrix of pixel intensities, and convert it into a graph in list form (i.e., from matrix to list). This process begins with detecting and recognizing shapes within the image, either in 2D or 3D, depending on the available data. From there, the system identifies objects and analyzes their spatial relationships, ultimately constructing a graph of semantic connections between them. This graph captures not just what the objects are, but how they relate to one another within the observed scene. In some cases, the analysis extends over time (3D + t), allowing for the interpretation of motion and dynamic interactions in a sequence of frames.</p>



<h2>Solving key memory issues</h2>


<div>
<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2025/05/image3.png"><img loading="lazy" decoding="async" width="786" height="412" src="https://www.softwareheritage.org/wp-content/uploads/2025/05/image3.png" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2025/05/image3.png 786w, https://www.softwareheritage.org/wp-content/uploads/2025/05/image3-300x157.png 300w, https://www.softwareheritage.org/wp-content/uploads/2025/05/image3-768x403.png 768w" sizes="auto, (max-width: 786px) 100vw, 786px"></a></figure></div>


<p><em>The algorithm was first developed on a PerkinElmer Model 3250 computer, seen above in a brochure <a href="https://www.1000bit.it/ad/bro/perkin/PerkinElmer-3250.pdf">via 1000bit.&nbsp;</a></em></p>



<p>Another challenge in early image processing was the limited amount of short-term memory (RAM) available in computers. This made it essential to focus on reducing the amount of data stored and processed at any given time, while preserving important information.</p>



<p>Due to these constraints, the Efficient Chain-Linking Algorithm could only store three lines of the image at a time while reading it line by line. As each new line was read, the algorithm would build and extend chains of connected pixels on the fly, without knowing how those chains might continue in future lines. Once the entire image was scanned, a final processing stage merged pixel chains belonging to the same contour and resolved junctions or branching points. Importantly, this was done using just one full pass over the data, making it memory-efficient.</p>



<p>But the algorithm wasn’t the only clever bit. From a programming standpoint, the code’s true ingenuity lay in its dynamic memory allocation for storing the chain lists. Back then, predicting memory needs upfront was impossible, making Patrick Cipiere’s approach an elegant solution to an unpredictable challenge.</p>


<div>
<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2025/05/image1.png"><img loading="lazy" decoding="async" width="425" height="339" src="https://www.softwareheritage.org/wp-content/uploads/2025/05/image1.png" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2025/05/image1.png 425w, https://www.softwareheritage.org/wp-content/uploads/2025/05/image1-300x239.png 300w" sizes="auto, (max-width: 425px) 100vw, 425px"></a><figcaption><em>Source code for memory allocation</em> <em>(excerpt)</em></figcaption></figure></div>


<h2>Computer vision today</h2>



<p>With the dramatic advancements in computer vision, fueled by deep learning and the prevalence of large memory capacities, contemporary methods often involve storing the full image and constructing contour chains sequentially, possibly necessitating multiple passes over the data, one for each chain. Yet, even with today’s abundant memory, this algorithm retains its power: remarkable efficiency when every byte counts. By processing the image sequentially, storing only a few lines at a time, and building pixel chains incrementally without looking back, it offers a lean and effective alternative to the memory-hungry approaches now common.</p>



<h2>Links and references</h2>



<figure><a href="https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision.png"><img loading="lazy" decoding="async" width="1024" height="522" src="https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision.png" alt="" srcset="https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision.png 1024w, https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision-300x153.png 300w, https://www.softwareheritage.org/wp-content/uploads/2025/05/computer-vision-768x392.png 768w" sizes="auto, (max-width: 1024px) 100vw, 1024px"></a></figure>



<p>The preserved source code can be found on Software Heritage archive: <a href="https://archive.softwareheritage.org/browse/origin/directory/?origin_url=https://github.com/mathfichen/chainage_de_contour">https://archive.softwareheritage.org/browse/origin/directory/?origin_url=https://github.com/mathfichen/chainage_de_contour</a></p>



<p>The original 79-page paper in French, “Chaînage efficace de contour, n° 605, Février 1987, Gerard Giraudon” <a href="https://inria.hal.science/inria-00075949/document">https://inria.hal.science/inria-00075949/document</a></p>



<p>An Efficient Chain-Linking Algorithm, G. Giraudon, in The 5th Scandinavian Conference on Image Analysis, Stockholm, June 1987.</p>



<p>A Real Time Parallel Edge Following in Single Pass, GG, in Workshop on Computer Vision, Miami, 1987</p>



<p>Chaînage efficace de contour, G. Giraudon, 3ème Colloque Image-Cesta, Paris, Mai 1987</p>



<p>Un standard pour une représentation d’objets de type liste dans le domaine du traitement d’images, n° 82, décembre 1988 – 3ieme édition, P. Garnesson, G. Giraudon.</p>



<p>Un standard pour une représentation d’objets de type liste dans le domaine du traitement d’images, n° 82, décembre 1988 – 3ieme édition, P. Garnesson, G. Giraudon.<br><a href="https://inria.hal.science/inria-00070061v1">https://inria.hal.science/inria-00070061v1</a></p>



<p>« L’approximation polygonale, bilans et perspectives », Rapport de recherche n°1621 INRIA, juin 1991&nbsp; Garnesson P. and Giraudon G.&nbsp;<br><a href="https://inria.hal.science/inria-00074940v1/document">https://inria.hal.science/inria-00074940v1/document</a></p>



<p>This work has resulted in citations and a hardware implementation presented at the 16th GRETSI COLLOQUIUM — SEPTEMBER 15-19, 1997</p>




	
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We Are No Longer a Serious Country – Paul Krugman (150 pts)]]></title>
            <link>https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country</link>
            <guid>44182634</guid>
            <pubDate>Wed, 04 Jun 2025 16:39:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country">https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country</a>, See on <a href="https://news.ycombinator.com/item?id=44182634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>“If you’re explaining, you’re losing.” This line is usually attributed to Ronald Reagan. Whoever said it definitely had a point, and not just about politics. If you’re trying to explain to people, be they voters or bond investors, that you aren’t really as bad or untrustworthy as you seem, you’re already in deep trouble.</p><p><span>So when I saw Scott Bessent, the treasury secretary, </span><a href="https://www.ft.com/content/9c652f09-cfca-4078-9e2b-5fdacd772a75" rel="">declaring</a><span> Sunday that “The United States of America is never going to default, that is never going to happen,” my reaction was, “Uh-oh.”</span></p><p>And it’s not just me. For generations investors have treated U.S. government debt as the ultimate safe asset. Whenever disaster strikes — even if it’s disaster largely made in America, like the 2008 subprime crisis — bond buyers pile into U.S. Treasuries, because America is a serious country, and the idea that we would fail to honor our debts was unthinkable.</p><p>But are we still that country? Markets seem to have doubts.</p><p><span>Yesterday the Financial Times had a neat chart showing that there used to be a clear relationship between U.S. interest rates and the international value of the dollar. Actually, the chart was a bit </span><em>too</em><span> neat: When I set out to reproduce it, I found that the FT chose a time period during which the relationship looked especially clear. Still, it used to be true that when U.S. interest rates rose, so did the dollar, because higher yields pulled in foreign capital. But since Donald Trump returned to power, that relationship has broken down. Instead, we’ve seen a combination of rising interest rates and a </span><em>falling</em><span> dollar:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png" width="800" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75158,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://paulkrugman.substack.com/i/165025603?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>As many have noted, what we’ve been seeing in recent months, with interest rates and the dollar moving in opposite directions, doesn’t look like what we normally see in the United States, or for that matter advanced nations in general. Instead, it’s the kind of thing one sees in emerging markets, where big market moves often reflect crises of confidence: International investors lose faith, pulling their money out, and capital flight causes both a falling currency and rising interest rates.</p><p>Here, for example, is what Mexico looked like during the “tequila crisis” of 1994-5, which involved both soaring interest rates and a plunging peso:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png" width="800" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:61317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://paulkrugman.substack.com/i/165025603?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Why are markets beginning to treat America as unreliable? It’s not just the debt numbers. Yes, we have large debts, but we’re an immensely wealthy country that, among other things, has lower average taxes than most of our peers. So we certainly have the resources to honor our debts.</p><p><span>But do we have the political will? Maybe even more important, do we have the political </span><em>seriousness</em><span>?</span></p><p>Like many economists, I’ve spent a lot of time analyzing the substance of Trump’s tariffs — how much they are likely to raise prices and reduce trade volumes. I’ve also written about the impacts of policy uncertainty, about how hard it is for businesses to make plans when they have little idea what tariff rates will be even a few months from now, let alone over the next few years.</p><p><span>But I wonder whether we’ve spent enough time looking at the policy </span><em>process</em><span> — how decisions get made in Trump’s America. Consider: On April 2, “Liberation Day,” Trump announced extremely high tariffs on many countries, the biggest tariff hike in U.S. history. The tariff rates — which differed hugely from country to country — were determined by a formula universally panned as stupid and ridiculous. And this tariff announcement was made with so little planning and forethought that it included taxes on imports from remote islands inhabited only by </span><a href="https://www.bbc.com/news/articles/cly8xlj0485o" rel="">penguins</a><span>.</span></p><p><span>Then, a week later, these tariffs were replaced by a completely different set of tariffs. How did that happen? Two of Trump’s cabinet members were able to beard him in the Oval Office while Peter Navarro, responsible for the original tariffs, was </span><a href="https://www.wsj.com/politics/policy/trump-tariff-pause-navarro-bessent-lutnick-b9e864fb?gaa_at=eafs&amp;gaa_n=ASWzDAiBOSVk0wHQwGquJzeffoF7WWhmHKUg490AKNKAI7beYwEiwJ8au1jW-gBGH4A%3D&amp;gaa_ts=683dcdff&amp;gaa_sig=xIdEPFjIsG6tDwLXo4RJ5eBSP5faPCp3McPx6CkxYZ4dSRr3EJUr_ysQIYStqcwE5VL_0ZhadzjRqX58YVM2eA%3D%3D" rel="">in another meeting</a><span>.</span></p><p>Does this sound like policymaking in a serious country?</p><p>Then there’s the budget bill making its way through Congress. It’s a terrible thing, imposing savage cuts on social programs (and decimating U.S. science) while giving such big tax cuts to the wealthy that it will explode the deficit. But content aside, notice that this hugely important piece of legislation is being rushed through with essentially no hearings or analysis.</p><p>And when outsiders, including the Congressional Budget Office and a variety of think tanks — conservative and centrist as well as progressive — have put out the analyses the bill’s sponsors won’t, pointing out the likely effects on debt, health coverage, and so on, the G.O.P. response has basically been to accuse all of the independent analysts of being part of a globalist conspiracy.</p><p><span>Wait, it gets worse. The name of the legislation — not its nickname, its official title — is the </span><a href="https://www.congress.gov/bill/119th-congress/house-bill/1/text" rel="">One Big Beautiful Bill Act</a><span>, because that’s what Trump has been calling it. Are we a mature republic with a normal head of state, or are we being ruled by Kim Jong Un in orange makeup?</span></p><p><span>Why, next thing you’ll be telling me that key policy decisions, leading to layoffs of hundreds of thousands of federal workers and many deaths around the world, have been made by a presidential crony whose erratic behavior may have reflected massive consumption of ketamine, Ecstasy and psychedelic mushrooms. </span><a href="https://www.nytimes.com/2025/05/30/us/elon-musk-drugs-children-trump.html" rel="">Oh, wait</a><span>.</span></p><p><span>Imagine yourself as a foreigner considering investing in the United States. You may well know that the One Big Beautiful Bill Act contains a “</span><a href="https://www.bloomberg.com/news/articles/2025-05-30/trump-revenge-tax-would-lower-foreign-investment-in-us-scorekeeper-predicts?sref=qzusa8bC" rel="">revenge</a><span>” provision that would allow the U.S. government to impose extra taxes on foreign investors whose home countries have policies America doesn’t like. You probably know that one of Trump’s advisers has suggested the forced conversion of short-term debt into </span><a href="https://thedailyeconomy.org/article/turning-treasury-securities-into-century-bonds-is-a-dead-end/" rel="">century bonds</a><span>. Once upon a time everyone would have dismissed these things as stuff that couldn’t happen in America. Now? Who knows?</span></p><p>In a way, the amazing thing is that we haven’t seen even more capital flight. Presumably investors still can’t believe that America has changed so much from the responsible, reliable nation it seemed to be just a few months ago.</p><p>But I think they’re headed for a rude awakening.</p><p>MUSICAL CODA</p><div id="youtube2-pvlbrKKEPdA" data-attrs="{&quot;videoId&quot;:&quot;pvlbrKKEPdA&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/pvlbrKKEPdA?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VC money is fueling a global boom in worker surveillance tech (217 pts)]]></title>
            <link>https://restofworld.org/2025/employee-surveillance-software-vc-funding/</link>
            <guid>44182582</guid>
            <pubDate>Wed, 04 Jun 2025 16:35:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2025/employee-surveillance-software-vc-funding/">https://restofworld.org/2025/employee-surveillance-software-vc-funding/</a>, See on <a href="https://news.ycombinator.com/item?id=44182582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>Technologies that promise to track, manage, and supervise workers, increasingly using artificial intelligence, are getting entrenched in the developing world, according to a new <a href="https://home.coworker.org/little-tech-goes-global/">report</a> by Coworker.org, a labor rights nonprofit based in New York.&nbsp;</p>



<p>Audits of more than 150 startups and regional companies based in Kenya, Nigeria, Colombia, Brazil, Mexico, and India showed workplace surveillance is expanding in scale and sophistication, the researchers said. While large <a href="https://www.computerweekly.com/news/252521757/Microsoft-Office-365-has-ability-to-spy-on-workers">corporations</a> are <a href="https://www.cisco.com/site/us/en/learn/topics/general/what-is-employee-monitoring.html">known</a> to develop surveillance technologies, a so-called Little Tech ecosystem of mostly unregulated, venture capital-funded startups and small vendors making these products has grown since Covid-19, the report found. The term “Little Tech” was popularized by the VC firm <a href="https://a16z.com/the-little-tech-agenda/">Andreessen Horowitz</a>, which argued that excessive regulation was stifling innovation.</p>



<figure></figure>



<p><a href="https://restofworld.org/2022/workers-managed-algorithms/">Algorithmic management</a> and surveillance tools are getting even more intrusive in gig work, and are entering offices and the informal labor sector as well, Wilneida Negrón, director of research and policy at Coworker.org and a co-author of the report, told <em>Rest of World</em>.&nbsp;</p>



<p>“The pressure of the hyper-surveillance creates a lot of stress and creates a lot of uncertainty for workers. It brings a culture of suspiciousness,” she said.&nbsp;</p>



<p>Investments by Silicon Valley-based VC firms led to a boom in tech startups globally after Covid-19, Negrón said. This has carried over to companies building bossware products in the developing world, she said.&nbsp;&nbsp;</p>



<figure></figure>



<p>The technologies include biometric tracking, AI-powered productivity monitoring, and predictive analytics, the report found. Worker data is continuously collected and analyzed by algorithms with the stated aim to improve hiring, evaluate performance, and optimize processes.&nbsp;</p>



<p>Most managers in wealthier nations say algorithmic management tools improve their decision-making, according to a 2024 <a href="https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/algorithmic-management-in-the-workplace_3c84ed6d/287c13c4-en.pdf?utm_source=chatgpt.com">survey</a> of over 6,000 employers by the Organisation for Economic Co-operation and Development. More than 90% of American managers used such tools, especially to reward or sanction employees.</p>



<p>Many tools are first deployed in Latin America, where labor laws are less strictly enforced, according to Ayden Férdeline, a tech policy researcher in Berlin and a co-author of the report.</p>



<p>“There is a Latin America testing ground for products,” he told <em>Rest of World</em>. “If they are successful, they tend to be deployed in other jurisdictions, oftentimes with additional safeguards, sometimes not.”</p>



<p>Many workers are unaware of how their information is collected and used, Férdeline said.&nbsp;</p>



<p>Some gig workers in Kenya, Guatemala, and Brazil said bossware tools make them feel surveilled, and that they have less control over their work. In Porto Alegre, Brazil, Uber driver Carina Trindade told <em>Rest of World</em> she feels the app monitors her continuously, tracking her speed and braking patterns. The app has permissions <a href="https://dig.watch/updates/uber-starts-testing-tool-records-video-inside-car-during-rides">to access</a> her mic and camera, she said.&nbsp;</p>



<p>Uber spokesperson Gabriel Gabira said drivers have the option to record trips, and <a href="https://help.uber.com/en/driving-and-delivering/article/record-my-ride---frequently-asked-questions?nodeId=7f86cc51-2c3e-4888-8dc1-0ac2b6337b92">privacy terms</a> are followed to access the footage.</p>



<p>In Nairobi, Godfrey Sanya Wanga, a driver for ride-hailing firm SafeBoda, told <em>Rest of World </em>he felt the app undercharged a customer. “I really wanted to ask [the customer] to pay me more, but I remembered that I was being monitored and this would bring me trouble if the client reported me,” he said. SafeBoda did not respond to a request for comment.</p>



<p>Several nations have data protection and privacy laws, including Brazil, Nigeria, and Kenya. But enforcement is inconsistent, the report said.</p>



<p>Here are five current uses of algorithmic management tools. The companies mentioned below did not comment, unless otherwise stated.&nbsp;</p>



<p>1. <strong>Timekeeping and attendance systems&nbsp;</strong></p>



<p>What: Platforms that track the attendance of workers, often using geolocation and biometrics to verify presence.&nbsp;</p>



<p>Example: <a href="https://www.rankmi.com/es/productos/control-asistencia-y-turnos">Rankmi</a>, based in Chile, uses biometrics and geolocation to track workers. The platform also <a href="https://www.rankmi.com/es/ia-para-hr">gives</a> workers continuous performance feedback and evaluates job applicants using AI.&nbsp;</p>



<p>2. <strong>Biometric and identity verification tools</strong></p>



<p>What: Tools that use fingerprint and facial-recognition checks, special digital signatures stored on a secure network, and official records to confirm a worker’s identity before granting access.&nbsp;</p>



<p>Example: Cincel, based in Mexico, provides <a href="https://www.cincel.digital/productos/verificacion-identidad/">identity verification</a> tools that do various checks including biometrics, and also cross-check against government databases and <a href="https://www.cincel.digital/productos/background-check/">blacklists</a>.</p>



<p>3. <strong>Performance and productivity monitoring platforms</strong></p>



<p>What: Dashboards that score workers using tracked metrics such as keystrokes, transaction counts, customer interactions, and task completion times.</p>



<p>Example: <a href="https://www.ahgora.com/">Ahgora</a>, based in Brazil, offers HR software that allows managers to continually “oversee team attendance in real-time” and that tracks productivity. It uses the data to offer predictions about work, such as potential issues with attendance, which can inform decision-making.&nbsp;</p>



<p>4. <strong>Algorithmic management and predictive analytics</strong></p>



<p>What: Platforms that<strong> </strong>automate HR functions, such as hiring shortlists, performance reviews, attrition forecasting, and also unionization-risk scoring.</p>



<p>Example: Visier’s AI-powered analytics platform <a href="https://www.visier.com/solutions/internal-mobility/">analyzes</a> HR data and provides insights, including resignation risk. The platform is used by global firms including Deloitte, Accenture, and Tata Consultancy Services.&nbsp;</p>



<p>Andrea Derler, principal of research and customer value at Visier, told <em>Rest of World</em> the platform only “processes data that organizations load into the platform, and we are not responsible for the way the data and insights we help provide is being used.”&nbsp;</p>



<p>5.&nbsp; <strong>Gig economy and field workforce tracking</strong></p>



<p>What: Apps that use the workers’ smartphones to dispatch and route deliveries. They use location, trip history, and ratings to allocate jobs and <a href="https://restofworld.org/2021/only-gojek-knows-this-mystery/">evaluate performance</a>. Workers are managed mostly by platforms rather than humans.</p>



<p>Example: <span><mark><a href="https://restofworld.org/tag/rappi" aria-label="Click to learn more about Rappi.">Rappi<span>i</span></a></mark><span><span><a href="https://restofworld.org/tag/rappi">Rappi</a><svg aria-label="Close" role="button"><use xlink:href="#X"></use></svg></span>Rappi, a Colombian company, has been providing delivery services across most Latin American countries since 2015.<span><a href="https://restofworld.org/tag/rappi"><span>READ MORE</span><svg><use xlink:href="#chevron"></use></svg></a></span></span></span>, a Colombian delivery app, tracks workers in real time. It has <a href="https://americasmi.com/insights/rappi-evolving-business-model-impact-latam-logistics/">auto accept</a>, where a rider can’t decline orders — and it’s mandatory to qualify for bonuses. Delivery worker Carolina Ramírez told <em>Rest of World </em>she works 14-hour days to earn a bonus of 100,000 pesos ($25) every week, leaving her little time for anything else. “My boss is the app. It’s unfair because to earn a good salary, I have to dedicate myself almost exclusively to this,” she said.</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IRS Direct File on GitHub (558 pts)]]></title>
            <link>https://chrisgiven.com/2025/05/direct-file-on-github/</link>
            <guid>44182356</guid>
            <pubDate>Wed, 04 Jun 2025 16:16:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chrisgiven.com/2025/05/direct-file-on-github/">https://chrisgiven.com/2025/05/direct-file-on-github/</a>, See on <a href="https://news.ycombinator.com/item?id=44182356">Hacker News</a></p>
Couldn't get https://chrisgiven.com/2025/05/direct-file-on-github/: Error: getaddrinfo ENOTFOUND chrisgiven.com]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: GPT image editing, but for 3D models (143 pts)]]></title>
            <link>https://www.adamcad.com/</link>
            <guid>44182206</guid>
            <pubDate>Wed, 04 Jun 2025 16:00:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.adamcad.com/">https://www.adamcad.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44182206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-root="" id="main" data-framer-hydrate-v2="{&quot;routeId&quot;:&quot;TsrMrhCtf&quot;,&quot;localeId&quot;:&quot;default&quot;,&quot;breakpoints&quot;:[{&quot;hash&quot;:&quot;7rp3o1&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 1200px)&quot;},{&quot;hash&quot;:&quot;4ud97w&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 810px) and (max-width: 1199px)&quot;},{&quot;hash&quot;:&quot;43sqdp&quot;,&quot;mediaQuery&quot;:&quot;(max-width: 809px)&quot;}]}" data-framer-ssr-released-at="2025-05-14T07:55:41.954Z" data-framer-page-optimized-at="2025-05-16T23:26:25.868Z"><div data-border="true" data-framer-name="Nav Section"><!--$--><!--$--><a data-framer-name="Adam-Selected" aria-label="Adam CAD Logo" href="https://www.adamcad.com/" data-framer-page-link-current="true"><div data-framer-name="Btn"><p><img decoding="async" width="1276" height="410" sizes="(min-width: 1200px) max(68.4683px, 81px), (min-width: 810px) and (max-width: 1199px) max(68.4683px, 81px), (max-width: 809px) max(68.4683px, 81px)" srcset="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=512 512w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png 1276w" src="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024" alt="" data-framer-original-sizes="68.4683px"></p></div></a><!--/$--><!--/$--></div><div data-framer-name="Hero" id="hero"><div data-framer-name="Text + Signup" id="hero-textsignup"><div data-framer-name="Content" id="heading-content"><div><p data-framer-name="Heading" data-framer-component-type="RichTextContainer"><h2>Speak anything into <span>existence</span></h2></p></div><div><p data-framer-name="Heading" data-framer-component-type="RichTextContainer"><h2>Speak anything into <span>existence</span></h2></p></div><p>AdamCAD is an AI Powered CAD platform that generates 3D designs in seconds</p></div><div aria-label="Y Combinator Logo" data-framer-name="Back-by-yc"><p><img decoding="async" width="716" height="267" sizes="(min-width: 1200px) 169px, (min-width: 810px) and (max-width: 1199px) 169px, (max-width: 809px) 169px" srcset="https://framerusercontent.com/images/caOXl53E47cnt5G42C5Q4PfW48.png?scale-down-to=512 512w,https://framerusercontent.com/images/caOXl53E47cnt5G42C5Q4PfW48.png 716w" src="https://framerusercontent.com/images/caOXl53E47cnt5G42C5Q4PfW48.png?scale-down-to=512" alt="" data-framer-original-sizes="169px"></p></div></div><div aria-label="Adam CAD Preview" data-border="true" data-framer-name="Graphics" id="product-demo"><p><img decoding="async" width="3024" height="1964" sizes="(min-width: 1200px) max(min(100vw * 0.8, 1000px), 100vw), (min-width: 810px) and (max-width: 1199px) min(100vw * 0.8, 1000px), (max-width: 809px) min(100vw * 0.8, 1000px)" srcset="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=512 512w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png 3024w" src="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048" alt="" data-framer-original-sizes="min(100vw * 0.8, 1000px)"></p></div><div data-border="true" data-framer-name="Graphics-Mobile" id="product-demo-mobile"><p><img decoding="async" width="3024" height="1964" srcset="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=512 512w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png 3024w" src="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048" alt="" data-framer-original-sizes="" sizes="(min-width: 1200px) max(min(100vw * 0.8, 1000px), 100vw), (min-width: 810px) and (max-width: 1199px) min(100vw * 0.8, 1000px), (max-width: 809px) min(100vw * 0.8, 1000px)"></p></div><div data-border="true" data-framer-name="Graphics-Mobile" id="product-demo-mobile"><p><img decoding="async" width="3024" height="1964" sizes="(min-width: 1200px) max(min(100vw * 0.8, 1000px), 100vw), (min-width: 810px) and (max-width: 1199px) min(100vw * 0.8, 1000px), (max-width: 809px) min(100vw * 0.8, 1000px)" srcset="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=512 512w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png 3024w" src="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048" alt="" data-framer-original-sizes="min(100vw * 0.8, 1000px)"></p></div><div data-border="true" data-framer-name="Graphics-Mobile" id="product-demo-mobile"><p><img decoding="async" width="3024" height="1964" sizes="(min-width: 1200px) max(min(100vw * 0.8, 1000px), 100vw), (min-width: 810px) and (max-width: 1199px) min(100vw * 0.8, 1000px), (max-width: 809px) min(100vw * 0.8, 1000px)" srcset="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=512 512w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png 3024w" src="https://framerusercontent.com/images/30IylDZrpUR2De3UN1TAfneeMo.png?scale-down-to=2048" alt="" data-framer-original-sizes="min(100vw * 0.8, 1000px)"></p></div></div><div data-framer-name="How it works"><div data-framer-name="Row 1"><div data-framer-name="Card"><div data-framer-name="Image/Video" data-border="true"><p><img decoding="async" loading="lazy" width="1788" height="1104" sizes="(min-width: 1200px) 466.4348px, (min-width: 810px) and (max-width: 1199px) 466.4348px, (max-width: 809px) 466.4348px" srcset="https://framerusercontent.com/images/hRdDbUFK7C4p4EhrdCGYKZFVY.png?scale-down-to=512 512w,https://framerusercontent.com/images/hRdDbUFK7C4p4EhrdCGYKZFVY.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/hRdDbUFK7C4p4EhrdCGYKZFVY.png 1788w" src="https://framerusercontent.com/images/hRdDbUFK7C4p4EhrdCGYKZFVY.png?scale-down-to=1024" alt="" data-framer-original-sizes="466.4348px"></p></div><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h3>Text to CAD</h3></p><div data-framer-component-type="RichTextContainer"><p>Use prompts in AdamCAD to describe and edit the 3D</p><p>model you want to create.</p></div></div></div><div data-framer-name="Card"><div data-framer-name="Image/Video" data-border="true"><p><img decoding="async" loading="lazy" width="1788" height="1104" sizes="(min-width: 1200px) 466.4348px, (min-width: 810px) and (max-width: 1199px) 466.4348px, (max-width: 809px) 466.4348px" srcset="https://framerusercontent.com/images/fdAhDzYWOayxPpuW1HweGpoeh8.png?scale-down-to=512 512w,https://framerusercontent.com/images/fdAhDzYWOayxPpuW1HweGpoeh8.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/fdAhDzYWOayxPpuW1HweGpoeh8.png 1788w" src="https://framerusercontent.com/images/fdAhDzYWOayxPpuW1HweGpoeh8.png?scale-down-to=1024" alt="" data-framer-original-sizes="466.4348px"></p></div><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h3>Refine &amp; Export</h3></p><p>AdamCAD generates the 3D Model and a list of parameters based on your design to refine it further.</p></div></div></div><div data-framer-name="Row 2"><div data-framer-name="Card"><div data-framer-name="Image/Video" data-border="true"><p><img decoding="async" loading="lazy" width="1788" height="1104" sizes="(min-width: 1200px) 466.4348px, (min-width: 810px) and (max-width: 1199px) 466.4348px, (max-width: 809px) 466.4348px" srcset="https://framerusercontent.com/images/KzF3uCZLhMahHwkzPDO6hWfILA.png?scale-down-to=512 512w,https://framerusercontent.com/images/KzF3uCZLhMahHwkzPDO6hWfILA.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/KzF3uCZLhMahHwkzPDO6hWfILA.png 1788w" src="https://framerusercontent.com/images/KzF3uCZLhMahHwkzPDO6hWfILA.png?scale-down-to=1024" alt="" data-framer-original-sizes="466.4348px"></p></div><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h3>Image to 3D</h3></p><p>AdamCAD's creative mode turns any image into a 3D generation in seconds.</p></div></div><div data-framer-name="Card"><p data-framer-component-type="RichTextContainer"><h3>AdamCAD AI Co-Pilot</h3></p><p>AdamCAD is built to integrate with the CAD software professionals rely on.</p></div></div></div><div data-framer-name="Collapsed"><!--$--><a data-framer-name="Landing Nav Logo" href="https://www.adamcad.com/" data-framer-page-link-current="true"><p><img decoding="async" width="1276" height="410" sizes="(min-width: 1200px) max(68.4683px, 81px), (min-width: 810px) and (max-width: 1199px) max(68.4683px, 81px), (max-width: 809px) max(68.4683px, 81px)" srcset="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=512 512w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png 1276w" src="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024" alt="" data-framer-original-sizes="81px"></p></a><!--/$--></div><div data-framer-name="Collapsed"><!--$--><a data-framer-name="Landing Nav Logo" href="https://www.adamcad.com/" data-framer-page-link-current="true"><p><img decoding="async" width="1276" height="410" sizes="(min-width: 1200px) max(68.4683px, 81px), (min-width: 810px) and (max-width: 1199px) max(68.4683px, 81px), (max-width: 809px) max(68.4683px, 81px)" srcset="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=512 512w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png 1276w" src="https://framerusercontent.com/images/ByCaNnq6OUTqqnFe0wBmKXX9N7w.png?scale-down-to=1024" alt="" data-framer-original-sizes="81px"></p></a><!--/$--></div><div data-framer-name="Footer"><p>© 2025  All Rights Reserved AdamCAD</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Meta found 'covertly tracking' Android users through Instagram and Facebook (202 pts)]]></title>
            <link>https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083</link>
            <guid>44182204</guid>
            <pubDate>Wed, 04 Jun 2025 16:00:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083">https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083</a>, See on <a href="https://news.ycombinator.com/item?id=44182204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-name="ui-article-body" data-highlight-intro="true">
      
      <p>Meta and search engine company Yandex have been "covertly tracking" Android users in the background of their devices, according to experts.</p><p>Academics at the Radboud University in the Netherlands and IMDEA Networks said they discovered Meta and Yandex have been tracking Android users' browser activity without their consent and then using the data in their apps.</p>
<p>Meta said it was looking into the issue, while Yandex denied collecting any sensitive data.</p><p>Gunes Acar, assistant professor at Radboud University, said the "covert" data collection was spotted in January.</p><p>He said he discovered Meta's apps, including Facebook and Instagram, and Yandex's apps, such as Yandex Maps, were sitting in the background of Android devices and loading a script that sent data locally back to apps on users' phones.</p>
<p>The scripts bypassed Android's security measures and meant that Meta and Yandex could track what users were doing on web browsers, without the user consenting or even knowing, according to the expert.</p><p>"They are bridging these two worlds that we think are separate; web browsing and mobile app activities," Dr Acar told Sky News.</p><p>"That's very shocking."</p><p>The apps were able to track users' browser data on all major Android browsers, even if the user was in incognito mode, the academics said.</p>        
<p>"It's really concerning because it negates every privacy control that you have in modern browsers and also in modern mobile platforms like Android," said Narseo Vallina-Rodriguez, associate professor at IMDEA Networks, to Sky News.</p><p><strong><a href="https://news.sky.com/topic/google-5876/1" target="_blank">Google</a></strong>, which owns the Android operating system, confirmed the covert activity to Sky News.</p><p>It said Meta and Yandex used <strong><a href="https://news.sky.com/topic/android-8090/1" target="_blank">Android's</a></strong> capabilities "in unintended ways that blatantly violate our security and privacy principles".</p><p><strong>What have Meta and Yandex said?</strong></p><p>Meta told Sky News it was quickly looking into the issue.</p><p>"We are in discussions with Google to address a potential miscommunication regarding the application of their policies," said a Meta spokesperson.</p><p>"Upon becoming aware of the concerns, we decided to pause the feature while we work with Google to resolve the issue."</p><p>Yandex said it "strictly complies with data protection standards", adding: "The feature in question does not collect any sensitive information and is solely intended to improve personalisation within our apps."</p><p><strong>Read more science and tech news:<br><a href="https://news.sky.com/story/ai-foot-scanner-recognises-warning-signs-of-heart-failure-to-keep-people-out-of-hospital-researchers-say-13378605" target="_blank">AI foot scanner recognises heart warning signs</a></strong><br><strong><a href="https://news.sky.com/story/ai-foot-scanner-recognises-warning-signs-of-heart-failure-to-keep-people-out-of-hospital-researchers-say-13378605" target="_blank">Coffee 'helps women age more healthily'</a></strong></p>    
<p>Meta appeared to have been doing the data tracking for around eight months, while Yandex had since 2017, the academics said.</p><p>"We found that Facebook was doing it on roughly 16,000 websites when visited from the EU, [...] Yandex was doing this on 1,300 websites," said Tim Vlummens, a PHD student at KU Leuven who worked on the research.</p><p>Google told Sky News it had already "implemented changes to mitigate these invasive techniques and have opened our own investigation and are directly in touch with the parties".</p>     <a href="https://news.sky.com/download-app" target="blank" data-tracking-label="ui-app-promo-download-link" data-type="" data-component-name="ui-app-promo">
        
    </a>


<p>The tech giant did not respond when asked what repercussions Meta and Yandex were facing for their conduct.</p><p>Firefox, Microsoft Edge and DuckDuckGo browsers were also affected, with Firefox owner Mozilla and DuckDuckGo engineers taking action to stop any future covert tracking.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Prompt Engineering Playbook for Programmers (270 pts)]]></title>
            <link>https://addyo.substack.com/p/the-prompt-engineering-playbook-for</link>
            <guid>44182188</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://addyo.substack.com/p/the-prompt-engineering-playbook-for">https://addyo.substack.com/p/the-prompt-engineering-playbook-for</a>, See on <a href="https://news.ycombinator.com/item?id=44182188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Developers are increasingly relying on AI coding assistants to accelerate our daily workflows. These tools can autocomplete functions, suggest bug fixes, and even generate entire modules or MVPs. Yet, as many of us have learned, the </span><em>quality</em><span> of the AI’s output depends largely on the </span><em>quality of the prompt</em><span> you provide. In other words, </span><strong>prompt engineering</strong><span> has become an essential skill. A poorly phrased request can yield irrelevant or generic answers, while a well-crafted prompt can produce thoughtful, accurate, and even creative code solutions. This write-up takes a practical look at how to systematically craft effective prompts for common development tasks.</span></p><p><span>AI pair programmers are powerful but not magical – they have no prior knowledge of your specific project or intent beyond what you tell them or include as context. The more information you provide, the better the output. We’ll distill key prompt patterns, </span><strong>repeatable frameworks</strong><span>, and memorable examples that have resonated with developers. You’ll see side-by-side comparisons of </span><strong>good vs. bad prompts</strong><span> with actual AI responses, along with commentary to understand why one succeeds where the other falters. </span><strong>Here’s a cheat sheet to get started:</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png" width="1456" height="1915" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/be144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1915,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:520317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Prompting an AI coding tool is somewhat like communicating with a very literal, </span><em>sometimes</em><span> knowledgeable collaborator. To get useful results, you need to set the stage clearly and guide the AI on </span><em>what</em><span> you want and </span><em>how</em><span> you want it. </span></p><p>Below are foundational principles that underpin all examples in this playbook:</p><ul><li><p><strong>Provide rich context.</strong><span> Always assume the AI knows nothing about your project beyond what you provide. Include relevant details such as the programming language, framework, and libraries, as well as the specific function or snippet in question. If there’s an error, provide the exact error message and describe what the code is </span><em>supposed</em><span> to do. </span><strong>Specificity</strong><span> and </span><strong>context</strong><span> make the difference between vague suggestions and precise, actionable solutions . In practice, this means your prompt might include a brief setup like: “I have a Node.js function using Express and Mongoose that should fetch a user by ID, but it throws a TypeError. Here’s the code and error…”. The more setup you give, the less the AI has to guess.</span></p></li><li><p><strong>Be specific about your goal or question.</strong><span> Vague queries lead to vague answers. Instead of asking something like “Why isn’t my code working?”, pinpoint what insight you need. For example: “This JavaScript function is returning undefined instead of the expected result. Given the code below, can you help identify why and how to fix it?” is far more likely to yield a helpful answer. One prompt formula for debugging is: </span><em>“It’s expected to do [expected behavior] but instead it’s doing [current behavior] when given [example input]. Where is the bug?”</em><span> . Similarly, if you want an optimization, ask for a </span><em>specific kind</em><span> of optimization (e.g. </span><em>“How can I improve the runtime performance of this sorting function for 10k items?”</em><span>). Specificity guides the AI’s focus .</span></p></li><li><p><strong>Break down complex tasks.</strong><span> When implementing a new feature or tackling a multi-step problem, don’t feed the entire problem in one gigantic prompt. It’s often more effective to split the work into smaller chunks and iterate. For instance, </span><em>“First, generate a React component skeleton for a product list page. Next, we’ll add state management. Then, we’ll integrate the API call.”</em><span> Each prompt builds on the previous. It’s often not advised to ask for a whole large feature in one go; instead, start with a high-level goal and then iteratively ask for each piece . This approach not only keeps the AI’s responses focused and manageable, but also mirrors how a human would incrementally build a solution.</span></p></li><li><p><strong>Include examples of inputs/outputs or expected behavior.</strong><span> If you can illustrate what you want with an example, do it. For example, </span><em>“Given the array [3,1,4], this function should return [1,3,4].”</em><span> Providing a concrete example in the prompt helps the AI understand your intent and reduces ambiguity . It’s akin to giving a junior developer a quick test case – it clarifies the requirements. In prompt engineering terms, this is sometimes called “</span><strong>few-shot prompting</strong><span>,” where you show the AI a pattern to follow. Even one example of correct behavior can guide the model’s response significantly.</span></p></li><li><p><strong>Leverage roles or personas.</strong><span> A powerful technique popularized in many viral prompt examples is to ask the AI to “act as” a certain persona or role. This can influence the style and depth of the answer. For instance, </span><em>“Act as a senior React developer and review my code for potential bugs”</em><span> or </span><em>“You are a JavaScript performance expert. Optimize the following function.”</em><span> By setting a role, you prime the assistant to adopt the relevant tone – whether it’s being a strict code reviewer, a helpful teacher for a junior dev, or a security analyst looking for vulnerabilities. Community-shared prompts have shown success with this method, such as </span><em>“Act as a JavaScript error handler and debug this function for me. The data isn’t rendering properly from the API call.”</em><span> . In our own usage, we must still provide the code and problem details, but the </span><strong>role-play</strong><span> prompt can yield more structured and expert-level guidance.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3413086,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1456w" sizes="100vw"></picture></div></a></figure></div><ul><li><p><strong>Iterate and refine the conversation.</strong><span> Prompt engineering is an </span><em>interactive</em><span> process, not a one-shot deal. Developers often need to review the AI’s first answer and then ask follow-up questions or make corrections. If the solution isn’t quite right, you might say, </span><em>“That solution uses recursion, but I’d prefer an iterative approach – can you try again without recursion?”</em><span> Or, </span><em>“Great, now can you improve the variable names and add comments?”</em><span> The AI remembers the context in a chat session, so you can progressively steer it to the desired outcome. The key is to view the AI as a partner you can coach – </span><strong>progress over perfection</strong><span> on the first try .</span></p></li><li><p><strong>Maintain code clarity and consistency.</strong><span> This last principle is a bit indirect but very important for tools that work on your code context. Write clean, well-structured code and comments, even before the AI comes into play. Meaningful function and variable names, consistent formatting, and docstrings not only make your code easier to understand for humans, but also give the AI stronger clues about what you’re doing. If you show a consistent pattern or style, the AI will continue it . Treat these tools as extremely attentive junior developers – they take every cue from your code and comments.</span></p></li></ul><p><span>With these foundational principles in mind, let’s dive into specific scenarios. We’ll start with </span><strong>debugging</strong><span>, perhaps the most immediate use-case: you have code that’s misbehaving, and you want the AI to help figure out why.</span></p><p>Debugging is a natural fit for an AI assistant. It’s like having a rubber-duck that not only listens, but actually talks back with suggestions. However, success largely depends on how you present the problem to the AI. Here’s how to systematically prompt for help in finding and fixing bugs:</p><p><strong>1. Clearly describe the problem and symptoms.</strong><span> Begin your prompt by describing what is going wrong and what the code is supposed to do. Always include the exact error message or incorrect behavior. For example, instead of just saying “My code doesn’t work,” you might prompt: </span><em>“I have a function in JavaScript that should calculate the sum of an array of numbers, but it’s returning NaN (Not a Number) instead of the actual sum. Here is the code: [include code]. It should output a number (the sum) for an array of numbers like [1,2,3], but I’m getting NaN. What could be the cause of this bug?”</em><span> This prompt specifies the language, the intended behavior, the observed wrong output, and provides the code context – all crucial information. Providing a structured context (code + error + expected outcome + what you’ve tried) gives the AI a solid starting point . By contrast, a generic question like “Why isn’t my function working?” yields meager results – the model can only offer the most general guesses without context.</span></p><p><strong>2. Use a step-by-step or line-by-line approach for tricky bugs.</strong><span> For more complex logic bugs (where no obvious error message is thrown but the output is wrong), you can prompt the AI to walk through the code’s execution. For instance: </span><em>“Walk through this function line by line and track the value of total at each step. It’s not accumulating correctly – where does the logic go wrong?”</em><span> This is an example of a </span><strong>rubber duck debugging prompt</strong><span> – you’re essentially asking the AI to simulate the debugging process a human might do with prints or a debugger. Such prompts often reveal subtle issues like variables not resetting or incorrect conditional logic, because the AI will spell out the state at each step. If you suspect a certain part of the code, you can zoom in: </span><em>“Explain what the filter call is doing here, and if it might be excluding more items than it should.”</em><span> Engaging the AI in an explanatory role can surface the bug in the process of explanation.</span></p><p><strong>3. Provide minimal reproducible examples when possible.</strong><span> Sometimes your actual codebase is large, but the bug can be demonstrated in a small snippet. If you can extract or simplify the code that still reproduces the issue, do so and feed that to the AI. This not only makes it easier for the AI to focus, but also forces you to clarify the problem (often a useful exercise in itself). For example, if you’re getting a TypeError in a deeply nested function call, try to reproduce it with a few lines that you can share. Aim to isolate the bug with the minimum code, make an assumption about what’s wrong, test it, and iterate . You can involve the AI in this by saying: </span><em>“Here’s a pared-down example that still triggers the error [include snippet]. Why does this error occur?”</em><span> By simplifying, you remove noise and help the AI pinpoint the issue. (This technique mirrors the advice of many senior engineers: if you can’t immediately find a bug, simplify the problem space. The AI can assist in that analysis if you present a smaller case to it.)</span></p><p><strong>4. Ask focused questions and follow-ups.</strong><span> After providing context, it’s often effective to directly ask what you need, for example: </span><em>“What might be causing this issue, and how can I fix it?”</em><span> . This invites the AI to both diagnose and propose a solution. If the AI’s first answer is unclear or partially helpful, don’t hesitate to ask a follow-up. You could say, </span><em>“That explanation makes sense. Can you show me how to fix the code? Please provide the corrected code.”</em><span> In a chat setting, the AI has the conversation history, so it can directly output the modified code. If you’re using an inline tool like Copilot in VS Code or Cursor without a chat, you might instead write a comment above the code like // BUG: returns NaN, fix this function and see how it autocompletes – but in general, the interactive chat yields more thorough explanations. Another follow-up pattern: if the AI gives a fix but you don’t understand why, ask </span><em>“Can you explain why that change solves the problem?”</em><span> This way you learn for next time, and you double-check that the AI’s reasoning is sound.</span></p><p><span>Now, let’s illustrate these debugging prompt principles with a concrete example, showing a </span><strong>poor prompt vs. improved prompt</strong><span> and the difference in AI responses:</span></p><p>Imagine we have a simple Node.js function meant to convert a list of user objects to a lookup map by user ID. However, it’s throwing an error. Here’s the buggy code:</p><pre><code><code>// Buggy function: converts array of users to a map by ID
function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i &lt;= users.length; i++) {  
    // Note: using &lt;= will go out of bounds on the last iteration
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}

// Example usage:
const result = mapUsersById([{ id: 1, name: "Alice" }]);</code></code></pre><p>This code is supposed to iterate through the users array and build an object where keys are user IDs. However, there is a bug: the loop uses i &lt;= users.length instead of i &lt; users.length. This will cause users[i] to be undefined on the last iteration, likely leading to a runtime error (e.g., attempting to access property id of undefined). Let’s see how an AI assistant might respond to two different prompts about this issue.</p><p><strong>❌ Poor Prompt:</strong><span> “Why isn’t my mapUsersById function working?”</span></p><p><em>AI’s response to the poor prompt (simulated):</em></p><blockquote><p><em>It’s hard to say without more information. Are you sure the function is being called with a proper array of users? Perhaps the issue is that users is empty or not an array, causing the loop not to execute properly. It could also be that there’s an error when setting the property on userMap. Make sure that each user has an id property. If users is undefined or null, that would also cause it to not work.</em></p></blockquote><p><strong>Analysis:</strong><span> This answer is quite vague. The prompt gave no details except the function name, so the AI is guessing at generic causes (“maybe the array is empty or not an array”, “make sure each user has an id”). It did not identify the actual bug. This is the kind of response you get when the question is underspecified – the model can only regurgitate common debugging tips. As a developer, this isn’t very helpful because it doesn’t address the real mistake. The </span><strong>lack of context</strong><span> and specificity in the prompt yields a generic answer (essentially the AI equivalent of “have you tried turning it off and on again?”). This aligns with the community advice that vague prompts lead to irrelevant or generic answers .</span></p><p><strong>✅ Improved Prompt:</strong><span> “I have a JavaScript function mapUsersById that should convert an array of user objects into a map (object) keyed by user ID. However, it throws an error when I run it. For example, when I pass [ {id: 1, name: "Alice"} ], I get TypeError: Cannot read property 'id' of undefined. Here is the function code:</span></p><pre><code><code>function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i &lt;= users.length; i++) {  
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}</code></code></pre><p>It should return { "1": {id: 1, name: "Alice"} }. What is the bug and how can I fix it?”</p><p><em>AI’s response to the improved prompt (simulated):</em></p><blockquote><p>*The error Cannot read property 'id' of undefined suggests that at some point, user is undefined, and then you try to access user.id. Looking at your loop, the issue is with the loop bounds. You are using i &lt;= users.length. If users.length is 1 (for one user in the array), the loop runs with i = 0 and i = 1. When i is 1, users[1] is undefined (since the only index in the array is 0). Thus user becomes undefined on that iteration, and user.id throws a TypeError. The fix is to change the loop condition to i &lt; users.length so that i goes from 0 to users.length-1. For example:</p></blockquote><pre><code><code>for (let i = 0; i &lt; users.length; i++) {
  const user = users[i];
  userMap[user.id] = user;
}</code></code></pre><blockquote><p>This way, i will be 0 for the first (and only) element, and then the loop stops. With that change, your function will no longer hit an undefined user and should work as expected, returning {"1": {id: 1, name: "Alice"}} for the given input.*</p></blockquote><p><strong>Analysis:</strong><span> This response directly pinpoints the bug (using &lt;= in the loop) and explains why it causes user to be undefined. It then provides the correct code solution (i &lt; users.length) and confirms the expected outcome. The improved prompt succeeded because it followed our principles: it specified the language (JavaScript), described the function’s purpose, included the exact error message and a sample input, and even provided the code snippet in question. Notice how the AI used the error message as a clue to focus on the loop bounds – a targeted prompt enabled the AI to engage in true problem-solving, effectively simulating how a human debugger would think: “where could undefined come from? likely from the loop indexing”. This is a concrete demonstration of the benefit of detailed prompts. </span></p><p><strong>Additional Debugging Tactics:</strong><span> Beyond identifying obvious bugs, you can use prompt engineering for deeper debugging assistance:</span></p><ul><li><p><em>Ask for potential causes.</em><span> If you’re truly stumped, you can broaden the question slightly: </span><em>“What are some possible reasons for a TypeError: cannot read property 'foo' of undefined in this code?”</em><span> along with the code. The model might list a few scenarios (e.g. the object wasn’t initialized, a race condition, wrong variable scoping, etc.). This can give you angles to investigate that you hadn’t considered. It’s like brainstorming with a colleague.</span></p></li><li><p><em>“Ask the Rubber Duck”</em><span> – i.e., explain your code to the AI. This may sound counterintuitive (why explain to the assistant?), but the act of writing an explanation can clarify your own understanding, and you can then have the AI verify or critique it. For example: </span><em>“I will explain what this function is doing: [your explanation]. Given that, is my reasoning correct and does it reveal where the bug is?”</em><span> The AI might catch a flaw in your explanation that points to the actual bug. This technique leverages the AI as an active rubber duck that not only listens but responds.</span></p></li><li><p><em>Have the AI create test cases.</em><span> You can ask: </span><em>“Can you provide a couple of test cases (inputs) that might break this function?”</em><span> The assistant might come up with edge cases you didn’t think of (empty array, extremely large numbers, null values, etc.). This is useful both for debugging and for generating tests for future robustness.</span></p></li><li><p><em>Role-play a code reviewer.</em><span> As an alternative to a direct “debug this” prompt, you can say: </span><em>“Act as a code reviewer. Here’s a snippet that isn’t working as expected. Review it and point out any mistakes or bad practices that could be causing issues: [code]”.</em><span> This sets the AI into a critical mode. Many developers find that phrasing the request as a code review yields a very thorough analysis, because the model will comment on each part of the code (and often, in doing so, it spots the bug). In fact, one prompt engineering tip is to explicitly request the AI to behave like a meticulous reviewer . This can surface not only the bug at hand but also other issues (e.g. potential null checks missing) which might be useful.</span></p></li></ul><p><span>In summary, when debugging with an AI assistant, </span><strong>detail and direction are your friends</strong><span>. Provide the scenario, the symptoms, and then ask pointed questions. The difference between a flailing “it doesn’t work, help!” prompt and a surgical debugging prompt is night and day, as we saw above. Next, we’ll move on to another major use case: refactoring and improving existing code.</span></p><p><span>Refactoring code – making it cleaner, faster, or more idiomatic without changing its functionality – is an area where AI assistants can shine. They’ve been trained on vast amounts of code, which includes many examples of well-structured, optimized solutions. However, to tap into that knowledge effectively, </span><strong>your prompt must clarify what “better” means for your situation</strong><span>. Here’s how to prompt for refactoring tasks:</span></p><p><strong>1. State your refactoring goals explicitly.</strong><span> “Refactor this code” on its own is too open-ended. Do you want to improve readability? Reduce complexity? Optimize performance? Use a different paradigm or library? The AI needs a target. A good prompt frames the task, for example: </span><em>“Refactor the following function to improve its readability and maintainability (reduce repetition, use clearer variable names).”</em><span> Or </span><em>“Optimize this algorithm for speed – it’s too slow on large inputs.”</em><span> By stating </span><strong>specific goals</strong><span>, you help the model decide which transformations to apply . For instance, telling it you care about performance might lead it to use a more efficient sorting algorithm or caching, whereas focusing on readability might lead it to break a function into smaller ones or add comments. If you have multiple goals, list them out. A prompt template from the Strapi guide suggests even enumerating issues: </span><em>“Issues I’d like to address: 1) [performance issue], 2) [code duplication], 3) [outdated API usage].”</em><span> . This way, the AI knows exactly what to fix. Remember, it will not inherently know </span><em>what you consider a problem</em><span> in the code – you must tell it.</span></p><p><strong>2. Provide the necessary code context.</strong><span> When refactoring, you’ll typically include the code snippet that needs improvement in the prompt. It’s important to include the full function or section that you want to be refactored, and sometimes a bit of surrounding context if relevant (like the function’s usage or related code, which could affect how you refactor). Also mention the language and framework, because “idiomatic” code varies between, say, idiomatic Node.js vs. idiomatic Deno, or React class components vs. functional components. For example: </span><em>“I have a React component written as a class. Please refactor it to a functional component using Hooks.”</em><span> The AI will then apply the typical steps (using useState, useEffect, etc.). If you just said “refactor this React component” without clarifying the style, the AI might not know you specifically wanted Hooks.</span></p><ul><li><p><strong>Include version or environment details if relevant.</strong><span> For instance, </span><em>“This is a Node.js v14 codebase”</em><span> or </span><em>“We’re using ES6 modules”</em><span>. This can influence whether the AI uses certain syntax (like import/export vs. require), which is part of a correct refactoring. If you want to ensure it doesn’t introduce something incompatible, mention your constraints.</span></p></li></ul><p><strong>3. Encourage explanations along with the code.</strong><span> A great way to learn from an AI-led refactor (and to verify its correctness) is to ask for an explanation of the changes. For example: </span><em>“Please suggest a refactored version of the code, and explain the improvements you made.”</em><span> This was even built into the prompt template we referenced: </span><em>“…suggest refactored code with explanations for your changes.”</em><span> . When the AI provides an explanation, you can assess if it understood the code and met your objectives. The explanation might say: “I combined two similar loops into one to reduce duplication, and I used a dictionary for faster lookups,” etc. If something sounds off in the explanation, that’s a red flag to examine the code carefully. In short, </span><em>use the AI’s ability to explain as a safeguard</em><span> – it’s like having the AI perform a code review on its own refactor.</span></p><p><strong>4. Use role-play to set a high standard.</strong><span> As mentioned earlier, asking the AI to act as a code reviewer or senior engineer can be very effective. For refactoring, you might say: </span><em>“Act as a seasoned TypeScript expert and refactor this code to align with best practices and modern standards.”</em><span> This often yields not just superficial changes, but more insightful improvements because the AI tries to live up to the “expert” persona. A popular example from a prompt guide is having the AI role-play a mentor: </span><em>“Act like an experienced Python developer mentoring a junior. Provide explanations and write docstrings. Rewrite the code to optimize it.”</em><span> . The result in that case was that the AI used a more efficient data structure (set to remove duplicates) and provided a one-line solution for a function that originally used a loop . The role-play helped it not only refactor but also explain </span><em>why</em><span> the new approach is better (in that case, using a set is a well-known optimization for uniqueness).</span></p><p>Now, let’s walk through an example of refactoring to see how a prompt can influence the outcome. We will use a scenario in JavaScript (Node.js) where we have some less-than-ideal code and we want it improved.</p><p>Suppose we have a function that makes two database calls and does some processing. It works, but it’s not pretty – there’s duplicated code and it’s hard to read. We’d like to refactor it for clarity and efficiency. Here’s the original code:</p><pre><code><code>// Original function: Fetches two lists and processes them (needs refactoring)
async function getCombinedData(apiClient) {
  // Fetch list of users
  const usersResponse = await apiClient.fetch('/users');
  if (!usersResponse.ok) {
    throw new Error('Failed to fetch users');
  }
  const users = await usersResponse.json();
  // Fetch list of orders
  const ordersResponse = await apiClient.fetch('/orders');
  if (!ordersResponse.ok) {
    throw new Error('Failed to fetch orders');
  }
  const orders = await ordersResponse.json();
  // Combine data (match user to orders)
  const result = [];
  for (let user of users) {
    const userOrders = orders.filter(o =&gt; o.userId === user.id);
    result.push({ user, orders: userOrders });
  }
  return result;
}</code></code></pre><p>Issues in this code: We see duplicated fetch logic for users and orders (could be refactored into a helper or loop), lack of error detail (just a generic error message), and it could perhaps run the fetches in parallel to improve speed. Let’s see two prompting approaches:</p><p><strong>❌ Prompt 1 (Underspecified):</strong><span> “Refactor the above getCombinedData function.”</span></p><p><em>AI’s response to Prompt 1:</em></p><blockquote><p>*Sure. Here’s a refactored version of getCombinedData:</p></blockquote><pre><code><code>async function getCombinedData(apiClient) {
  // Fetch users and orders in parallel
  const [usersResponse, ordersResponse] = await Promise.all([
    apiClient.fetch('/users'),
    apiClient.fetch('/orders')
  ]);
  if (!usersResponse.ok || !ordersResponse.ok) {
    throw new Error('Failed to fetch data');
  }
  const [users, orders] = await Promise.all([
    usersResponse.json(),
    ordersResponse.json()
  ]);
  // Map orders by userId for efficient lookup
  const ordersByUser = {};
  for (const order of orders) {
    if (!ordersByUser[order.userId]) {
      ordersByUser[order.userId] = [];
    }
    ordersByUser[order.userId].push(order);
  }
  // Combine data
  return users.map(user =&gt; ({
    user,
    orders: ordersByUser[user.id] || []
  }));
}</code></code></pre><blockquote><p>I fetched users and orders in parallel to improve performance. I also consolidated error handling and used a map (ordersByUser) to group orders by user, which makes the combination step cleaner and potentially faster.*</p></blockquote><p><strong>Analysis:</strong><span> This is actually a pretty good refactor, hitting several improvements: parallel fetching, grouping orders by user for efficiency, cleaner combination using .map and defaulting to an empty array. The AI also explained what it did. So why do we label this prompt as underspecified or “poor”? In this case, the AI happened to make good choices, but it </span><em>assumed</em><span> a lot because the prompt was open-ended. It assumed that performance should be improved (parallel requests, which is a smart change but what if parallel wasn’t safe here?), and it changed error handling to a generic message “Failed to fetch data,” which in combining the errors lost the specificity of which call failed. </span></p><p>The risk with a generic “Refactor this” prompt is that the AI might make changes that alter behavior in unintended ways. Here, combining error handling might be acceptable or not, depending on requirements. The assistant also made a structural change (using a dictionary to group orders) – that’s fine and probably for the better, but if we had specific constraints (like wanting to stream results or keep memory usage low), we didn’t inform it. So while the result looks good, it was somewhat lucky that the AI’s own interpretation of “refactor” aligned with what we wanted. This illustrates that an underspecified prompt can sometimes get good results on simple cases, but for complex code, it might go off in an undesired direction or miss certain improvements you care about.</p><p><strong>✅ Prompt 2 (Goal-Oriented):</strong><span> “Refactor the above getCombinedData function to eliminate duplicate code and improve performance. Specifically: (1) Avoid repeating the fetch logic for users and orders – maybe use a helper or fetch them together. (2) Fetch both lists in parallel if possible. (3) Keep the error handling for each fetch (we want to know which call failed). (4) Improve the combination of data, possibly by using a more efficient structure for lookup instead of a nested loop. Provide the refactored code with comments explaining the changes.”</span></p><p><em>AI’s response to Prompt 2:</em></p><blockquote><p>*Sure! Here’s an improved version of getCombinedData with the requested refactoring:</p></blockquote><pre><code><code>async function getCombinedData(apiClient) {
  // 1. Fetch users and orders in parallel to improve performance
  const [usersResponse, ordersResponse] = await Promise.all([
    apiClient.fetch('/users'),
    apiClient.fetch('/orders')
  ]);

  // 2. Handle errors separately to know which call failed
  if (!usersResponse.ok) {
    throw new Error('Failed to fetch users');
  }
  if (!ordersResponse.ok) {
    throw new Error('Failed to fetch orders');
  }

  // Parse JSON responses in parallel
  const [users, orders] = await Promise.all([
    usersResponse.json(),
    ordersResponse.json()
  ]);

  // 3. Use a lookup map for orders by userId for efficient matching
  const ordersByUser = orders.reduce((map, order) =&gt; {
    if (!map[order.userId]) {
      map[order.userId] = [];
    }
    map[order.userId].push(order);
    return map;
  }, {});

  // 4. Combine data using the lookup map
  return users.map(user =&gt; ({
    user,
    orders: ordersByUser[user.id] || []  // empty array if no orders for user
  }));
}</code></code></pre><blockquote><p><strong>Explanation of changes:</strong></p></blockquote><ul><li><p><em>Parallel fetching:</em><span> I used Promise.all to fetch users and orders at the same time (point 1).</span></p></li><li><p><em>Preserved individual error handling:</em><span> I kept separate checks for usersResponse.ok and ordersResponse.ok so that any error clearly indicates which fetch failed (point 2).</span></p></li><li><p><em>Parallel JSON parsing:</em><span> Also used Promise.all for response.json() calls, which is a minor improvement.</span></p></li><li><p><em>Efficient combination:</em><span> Instead of a nested loop, I created an ordersByUser map (point 3). This groups orders by userId in a single pass. Then I map over users to build the result, pulling each user’s orders from the map in O(1) time (point 4). This is more efficient and concise.*</span></p></li></ul><p><strong>Analysis:</strong><span> This response directly addressed all the specified goals. The code is refactored to be cleaner and faster, and it maintained separate error messages as requested. The AI’s explanation confirms each point we listed, which shows it carefully followed the prompt instructions. This is a great outcome because we, as the prompter, defined what “refactor” meant in this context. By doing so, we guided the AI to produce a solution that matches our needs with minimal back-and-forth. If the AI had overlooked one of the points (say it still merged the error handling), we could easily prompt again: </span><em>“Looks good, but please ensure the error messages remain distinct for users vs orders.”</em><span> – however, in this case it wasn’t needed because our prompt was thorough.</span></p><p><span>This example demonstrates a key lesson: </span><strong>when you know what you want improved, spell it out.</strong><span> AI is good at following instructions, but it won’t read your mind. A broad “make this better” might work for simple things, but for non-trivial code, you’ll get the best results by enumerating what “better” means to you. This aligns with community insights that clear, structured prompts yield significantly improved results .</span></p><p><strong>Additional Refactoring Tips:</strong></p><ul><li><p><em>Refactor in steps:</em><span> If the code is very large or you have a long list of changes, you can tackle them one at a time. For example, first ask the AI to “refactor for readability” (focus on renaming, splitting functions), then later “optimize the algorithm in this function.” This prevents overwhelming the model with too many instructions at once and lets you verify each change stepwise.</span></p></li><li><p><em>Ask for alternative approaches:</em><span> Maybe the AI’s first refactor works but you’re curious about a different angle. You can ask, </span><em>“Can you refactor it in another way, perhaps using functional programming style (e.g. array methods instead of loops)?”</em><span> or </span><em>“How about using recursion here instead of iterative approach, just to compare?”</em><span> This way, you can evaluate different solutions. It’s like brainstorming multiple refactoring options with a colleague.</span></p></li><li><p><em>Combine refactoring with explanation to learn patterns:</em><span> We touched on this, but it’s worth emphasizing – use the AI as a learning tool. If it refactors code in a clever way, study the output and explanation. You might discover a new API or technique (like using reduce to build a map) that you hadn’t used before. This is one reason to ask for explanations: it turns an answer into a mini-tutorial, reinforcing your understanding of best practices.</span></p></li><li><p><em>Validation and testing:</em><span> After any AI-generated refactor, always run your tests or try the code with sample inputs. AI might inadvertently introduce subtle bugs, especially if the prompt didn’t specify an important constraint. For example, in our refactor, if the original code intentionally separated fetch errors for logging but we didn’t mention logging, the combined error might be less useful. It’s our job to catch that in review. The AI can help by writing tests too – you could ask </span><em>“Generate a few unit tests for the refactored function”</em><span> to ensure it behaves the same as before on expected inputs.</span></p></li></ul><p><span>At this point, we’ve covered debugging and refactoring – improving existing code. The next logical step is to use AI assistance for </span><strong>implementing new features</strong><span> or generating new code. We’ll explore how to prompt for that scenario effectively.</span></p><p><strong>❌ Poor Prompt:</strong><span> "My useEffect isn't working right"</span></p><p><strong>✅ Enhanced Prompt:</strong></p><pre><code><code>I have a React component that fetches user data, but it's causing infinite re-renders. Here's my code:


const UserProfile = ({ userId }) =&gt; {
  const [user, setUser] = useState(null);
  const [loading, setLoading] = useState(true);
  
  useEffect(() =&gt; {
    fetchUser(userId).then(setUser).finally(() =&gt; setLoading(false));
  }, [userId, setUser, setLoading]); // Problem is here
  
  return loading ? &lt;div&gt;Loading...&lt;/div&gt; : &lt;div&gt;{user?.name}&lt;/div&gt;;
};</code></code></pre><p><strong>Expected behavior:</strong><span> Should fetch user data once when userId changes Actual behavior: Component re-renders infinitely Error in console: "Warning: Maximum update depth exceeded"</span></p><p>What's causing this infinite loop and how do I fix the dependency array?</p><p><strong>Why this works:</strong><span> Provides exact code, error message, expected vs actual behavior, and focuses on a specific React pattern that's commonly misunderstood.</span></p><p><strong>❌ Poor Prompt:</strong><span> "Build the state management for my Next.js ecommerce app”</span></p><p>✅ Enhanced Prompt:</p><p>I'm building a Next.js 14 e-commerce app and need to design the state management architecture. Here are my requirements:</p><p>Components:</p><ul><li><p>Product listing page (needs: products[], filters, pagination)</p></li><li><p>Shopping cart (needs: cart items, totals, shipping info)</p></li><li><p>User auth (needs: user profile, auth status, preferences)</p></li><li><p>Real-time notifications (needs: toast messages, error states)</p></li></ul><p>Technical constraints:</p><ul><li><p>Next.js 14 with App Router and Server Components</p></li><li><p>TypeScript strict mode</p></li><li><p>Server-side data fetching for SEO</p></li><li><p>Client-side interactivity for cart/user actions</p></li><li><p>State should persist across navigation</p></li></ul><p>Should I use:</p><ol><li><p>Zustand stores for each domain (cart, auth, notifications)</p></li><li><p>React Query/TanStack Query for server state + Zustand for client state</p></li><li><p>A single Zustand store with slices</p></li></ol><p>Please provide a recommended architecture with code examples showing how to structure stores and integrate with Next.js App Router patterns.</p><p><strong>Why this works: </strong><span>Real-world scenario with specific tech stack, clear requirements, and asks for architectural guidance with implementation details.</span></p><p>One of the most exciting uses of AI code assistants is to help you write new code from scratch or integrate a new feature into an existing codebase. This could range from generating a boilerplate for a React component to writing a new API endpoint in an Express app. The challenge here is often that these tasks are open-ended – there are many ways to implement a feature. Prompt engineering for code generation is about guiding the AI to produce code that fits your needs and style. Here are strategies to do that:</p><p><strong>1. Start with high-level instructions, then drill down.</strong><span> Begin by outlining what you want to build in plain language, possibly breaking it into smaller tasks (similar to our advice on breaking down complex tasks earlier). For example, say you want to add a </span><strong>search bar feature</strong><span> to an existing web app. You might first prompt: </span><em>“Outline a plan to add a search feature that filters a list of products by name in my React app. The products are fetched from an API.”</em><span> </span></p><p>The AI might give you a step-by-step plan: “1. Add an input field for the search query. 2. Add state to hold the query. 3. Filter the products list based on the query. 4. Ensure it’s case-insensitive, etc.” Once you have this plan (which you can refine with the AI’s help), you can tackle each bullet with focused prompts. </p><p><span>For instance: </span><em>“Okay, implement step 1: create a SearchBar component with an input that updates a searchQuery state.”</em><span> After that, </span><em>“Implement step 3: given the searchQuery and an array of products, filter the products (case-insensitive match on name).”</em><span> By dividing the feature, you ensure each prompt is specific and the responses are manageable. This also mirrors iterative development – you can test each piece as it’s built.</span></p><p><strong>2. Provide relevant context or reference code.</strong><span> If you’re adding a feature to an existing project, it helps tremendously to show the AI how similar things are done in that project. For example, if you already have a component that is similar to what you want, you can say: </span><em>“Here is an existing UserList component (code…). Now create a ProductList component that is similar but includes a search bar.”</em><span> </span></p><p><span>The AI will see the patterns (maybe you use certain libraries or style conventions) and apply them. Having relevant files open or referencing them in your prompt provides context that leads to more project-specific and consistent code suggestions . Another trick: if your project uses a particular coding style or architecture (say Redux for state or a certain CSS framework), mention that. </span><em>“We use Redux for state management – integrate the search state into Redux store.”</em><span> </span></p><p><span>A well-trained model will then generate code consistent with Redux patterns, etc. Essentially, you are </span><strong>teaching the AI about your project’s environment</strong><span> so it can tailor the output. Some assistants can even use your entire repository as context to draw from; if using those, ensure you point it to similar modules or documentation in your repo.</span></p><ul><li><p><span>If starting something new but you have a preferred approach, you can also mention that: </span><em>“I’d like to implement this using functional programming style (no external state, using array methods).”</em><span> Or, </span><em>“Ensure to follow the MVC pattern and put logic in the controller, not the view.”</em><span> These are the kind of details a senior engineer might remind a junior about, and here </span><strong>you are the senior telling the AI</strong><span>.</span></p></li></ul><p><strong>3. Use comments and TODOs as inline prompts.</strong><span> When working directly in an IDE with Copilot, one effective workflow is writing a comment that describes the next chunk of code you need, then letting the AI autocomplete it. For example, in a Node.js backend, you might write: // TODO: Validate the request payload (ensure name and email are provided) and then start the next line. Copilot often picks up on the intent and generates a block of code performing that validation. This works because your comment is effectively a natural language prompt. However, be prepared to edit the generated code if the AI misinterprets – as always, verify its correctness.</span></p><p><strong>4. Provide examples of expected input/output or usage.</strong><span> Similar to what we discussed before, if you’re asking the AI to implement a new function, include a quick example of how it will be used or a simple test case. For instance: </span><em>“Implement a function formatPrice(amount) in JavaScript that takes a number (like 2.5) and returns a string formatted in USD (like $2.50). For example, formatPrice(2.5) should return '$2.50'.”</em><span> </span></p><p><span>By giving that example, you constrain the AI to produce a function consistent with it. Without the example, the AI might assume some other formatting or currency. The difference could be subtle but important. Another example in a web context: </span><em>“Implement an Express middleware that logs requests. For instance, a GET request to /users should log ‘GET /users’ to the console.”</em><span> This makes it clear what the output should look like. Including expected behavior in the prompt acts as a test the AI will try to satisfy.</span></p><p><strong>5. When the result isn’t what you want, rewrite the prompt with more detail or constraints.</strong><span> It’s common that the first attempt at generating a new feature doesn’t nail it. Maybe the code runs but is not idiomatic, or it missed a requirement. Instead of getting frustrated, treat the AI like a junior dev who gave a first draft – now you need to give feedback. For example, </span><em>“The solution works but I’d prefer if you used the built-in array filter method instead of a for loop.”</em><span> Or, </span><em>“Can you refactor the generated component to use React Hooks for state instead of a class component? Our codebase is all functional components.”</em><span> You can also add new constraints: </span><em>“Also, ensure the function runs in O(n) time or better, because n could be large.”</em><span> This iterative prompting is powerful. A real-world scenario: one developer asked an LLM to generate code to draw an ice cream cone using a JS canvas library, but it kept giving irrelevant output until they refined the prompt with more specifics and context . The lesson is, </span><em>don’t give up after one try</em><span>. Figure out what was lacking or misunderstood in the prompt and clarify it. This is the essence of prompt engineering – each tweak can guide the model closer to what you envision.</span></p><p>Let’s illustrate feature implementation prompting with an example scenario:</p><p>Suppose we have a React application and we want to add a new component: a product list with a search filter and a sort dropdown. This is a fairly typical feature in a web app. We’ll focus on prompting for the search functionality within this component for brevity.</p><p><strong>Initial Prompt (Feature request):</strong><span> “Create a React functional component called ProductList that displays a list of products and includes a text input to filter the products by name. The component should: fetch an array of products from /api/products (assume it returns JSON array of {id, name, ...}), store them in state, and allow the user to type in a search box to filter the displayed products by name (case-insensitive match). Display the filtered list in a simple &lt;ul&gt; with product names. Include basic error handling (e.g. show an error message if the API call fails) and a loading state while fetching.”</span></p><p>That’s a long prompt but it’s specific. Let’s assume we give this to an LLM in a chat.</p><p><em>AI’s response (summary of what it might produce):</em><span> The AI would likely output a ProductList component something like:</span></p><pre><code><code>import React, { useState, useEffect } from 'react';

function ProductList() {
  const [products, setProducts] = useState([]);
  const [search, setSearch] = useState('');
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);

  useEffect(() =&gt; {
    async function fetchProducts() {
      try {
        const res = await fetch('/api/products');
        if (!res.ok) throw new Error('Network response was not ok');
        const data = await res.json();
        setProducts(data);
      } catch (err) {
        console.error('Failed to fetch products:', err);
        setError('Failed to load products.');
      } finally {
        setLoading(false);
      }
    }
    fetchProducts();
  }, []);

  const handleSearchChange = (e) =&gt; {
    setSearch(e.target.value);
  };

  // Filter products by search query (case-insensitive)
  const filteredProducts = products.filter(product =&gt;
    product.name.toLowerCase().includes(search.toLowerCase())
  );

  if (loading) return &lt;div&gt;Loading products...&lt;/div&gt;;
  if (error) return &lt;div&gt;{error}&lt;/div&gt;;

  return (
    &lt;div className="product-list"&gt;
      &lt;input 
        type="text" 
        placeholder="Search products" 
        value={search} 
        onChange={handleSearchChange} 
      /&gt;
      &lt;ul&gt;
        {filteredProducts.map(prod =&gt; (
          &lt;li key={prod.id}&gt;{prod.name}&lt;/li&gt;
        ))}
      &lt;/ul&gt;
    &lt;/div&gt;
  );
}

export default ProductList;</code></code></pre><p><em>(Plus possibly some comments in the code explaining each part, if we asked for them.)</em></p><p>Now, let’s analyze this result and consider prompt refinements:</p><ul><li><p><span>Suppose the above code came back, but our project uses a context or a global store for data fetching instead of fetching inside the component. This is something we didn’t specify. The AI made a choice to fetch in useEffect. If we prefer using a custom hook or context, we should have hinted at that. So we could refine our prompt: </span><em>“Actually, in our app we use a custom hook useProducts() that already handles fetching. Please refactor the component to use useProducts hook instead of directly calling fetch.”</em><span> The AI would then adjust, perhaps assuming useProducts returns { products, loading, error } and simplify the component accordingly.</span></p></li><li><p><span>Another refinement: maybe we realize we also want a sort dropdown (which we didn’t mention initially). We can now extend the conversation: </span><em>“Great, now add a dropdown to sort the products by name (A-Z or Z-A). The dropdown should let the user choose ascending or descending, and the list should sort accordingly in addition to the filtering.”</em><span> Because the AI has the context of the existing code, it can insert a sort state and adjust the rendering. We provided a clear new requirement, and it will attempt to fulfill it, likely by adding something like:</span></p></li></ul><pre><code><code>const [sortOrder, setSortOrder] = useState('asc');
// ... a select input for sortOrder ...
// and sort the filteredProducts before rendering:
const sortedProducts = [...filteredProducts].sort((a, b) =&gt; {
  if (sortOrder === 'asc') return a.name.localeCompare(b.name);
  else return b.name.localeCompare(a.name);
});</code></code></pre><ul><li><p>(plus the dropdown UI).</p><p>By iterating like this, feature by feature, we simulate a development cycle with the AI. This is far more effective than trying to prompt for the entire, complex component with all features in one go initially. It reduces mistakes and allows mid-course corrections as requirements become clearer.</p></li><li><p><span>If the AI makes a subtle mistake (say it forgot to make the search filter case-insensitive), we just point that out: </span><em>“Make the search case-insensitive.”</em><span> It will adjust the filter to use lowercase comparison (which in our pseudo-output it already did, but if not it would fix it).</span></p></li></ul><p><span>This example shows that implementing features with AI is all about </span><strong>incremental development and prompt refinement</strong><span>. A Twitter thread might exclaim how someone built a small app by continually prompting an LLM for each part – that’s essentially the approach: build, review, refine, extend. Each prompt is like a commit in your development process.</span></p><p><strong>Additional tips for feature implementation:</strong></p><ul><li><p><em>Let the AI scaffold, then you fill in specifics:</em><span> Sometimes it’s useful to have the AI generate a rough structure, then you tweak it. For example, </span><em>“Generate the skeleton of a Node.js Express route for user registration with validation and error handling.”</em><span> It might produce a generic route with placeholders. You can then fill in the actual validation rules or database calls which are specific to your app. The AI saves you from writing boilerplate, and you handle the custom logic if it’s sensitive.</span></p></li><li><p><em>Ask for edge case handling:</em><span> When generating a feature, you might prompt the AI to think of edge cases: </span><em>“What edge cases should we consider for this feature (and can you handle them in the code)?”</em><span> For instance, in the search example, an edge case might be “what if the products haven’t loaded yet when the user types?” (though our code handles that via loading state) or “what if two products have the same name” (not a big issue but maybe mention it). The AI could mention things like empty result handling, very large lists (maybe needing debounce for search input), etc. This is a way to leverage the AI’s training on common pitfalls.</span></p></li><li><p><em>Documentation-driven development:</em><span> A nifty approach some have taken is writing a docstring or usage example first and having the AI implement the function to match. For example:</span></p></li></ul><pre><code><code>/**
 * Returns the nth Fibonacci number.
 * @param {number} n - The position in Fibonacci sequence (0-indexed).
 * @returns {number} The nth Fibonacci number.
 * 
 * Example: fibonacci(5) -&gt; 5  (sequence: 0,1,1,2,3,5,…)
 */
function fibonacci(n) {
  // ... implementation
}</code></code></pre><ul><li><p>If you write the above comment and function signature, an LLM might fill in the implementation correctly because the comment describes exactly what to do and even gives an example. This technique ensures you clarify the feature in words first (which is a good practice generally), and then the AI uses that as the spec to write the code.</p></li></ul><p><span>Having covered prompting strategies for debugging, refactoring, and new code generation, let’s turn our attention to some </span><strong>common pitfalls and anti-patterns</strong><span> in prompt engineering for coding. Understanding these will help you avoid wasting time on unproductive interactions and quickly adjust when the AI isn’t giving you what you need.</span></p><p><span>Not all prompts are created equal. By now, we’ve seen numerous examples of effective prompts, but it’s equally instructive to recognize </span><strong>anti-patterns</strong><span> – common mistakes that lead to poor AI responses. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png" width="1024" height="1077" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1077,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2173076,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here are some frequent prompt failures and how to fix them:</p><ul><li><p><strong>Anti-Pattern: The Vague Prompt.</strong><span> This is the classic </span><em>“It doesn’t work, please fix it”</em><span> or </span><em>“Write something that does X”</em><span> without enough detail. We saw an example of this when the question “Why isn’t my function working?” got a useless answer . Vague prompts force the AI to guess the context and often result in generic advice or irrelevant code. The fix is straightforward: </span><strong>add context and specifics</strong><span>. If you find yourself asking a question and the answer feels like a Magic 8-ball response (“Have you tried checking X?”), stop and reframe your query with more details (error messages, code excerpt, expected vs actual outcome, etc.). A good practice is to read your prompt and ask, </span><em>“Could this question apply to dozens of different scenarios?”</em><span> If yes, it’s too vague. Make it so specific that it could </span><em>only</em><span> apply to your scenario.</span></p></li><li><p><strong>Anti-Pattern: The Overloaded Prompt.</strong><span> This is the opposite issue: asking the AI to do too many things at once. For instance, </span><em>“Generate a complete Node.js app with authentication, a front-end in React, and deployment scripts.”</em><span> Or even on a smaller scale, </span><em>“Fix these 5 bugs and also add these 3 features in one go.”</em><span> The AI might attempt it, but you’ll likely get a jumbled or incomplete result, or it might ignore some parts of the request. Even if it addresses everything, the response will be long and harder to verify. The remedy is to </span><strong>split the tasks</strong><span>. Prioritize: do one thing at a time, as we emphasized earlier. This makes it easier to catch mistakes and ensures the model stays focused. If you catch yourself writing a paragraph with multiple “and” in the instructions, consider breaking it into separate prompts or sequential steps.</span></p></li><li><p><strong>Anti-Pattern: Missing the Question.</strong><span> Sometimes users will present a lot of information but never clearly ask a question or specify what they need. For example, dumping a large code snippet and just saying “Here’s my code.” This can confuse the AI – it doesn’t know what you want. Always include a clear ask, such as </span><em>“Identify any bugs in the above code”</em><span>, </span><em>“Explain what this code does”</em><span>, or </span><em>“Complete the TODOs in the code”</em><span>. A prompt should have a </span><em>purpose</em><span>. If you just provide text without a question or instruction, the AI might make incorrect assumptions (like summarizing the code instead of fixing it, etc.). Make sure the AI knows </span><em>why</em><span> you showed it some code. Even a simple addition like, </span><em>“What’s wrong with this code?”</em><span> or </span><em>“Please continue implementing this function.”</em><span> gives it direction.</span></p></li><li><p><strong>Anti-Pattern: Vague Success Criteria.</strong><span> This is a subtle one – sometimes you might ask for an optimization or improvement, but you don’t define what success looks like. For example, </span><em>“Make this function faster.”</em><span> Faster by what metric? If the AI doesn’t know your performance constraints, it might micro-optimize something that doesn’t matter or use an approach that’s theoretically faster but practically negligible. Or </span><em>“make this code cleaner”</em><span> – “cleaner” is subjective. We dealt with this by explicitly stating goals like “reduce duplication” or “improve variable names” etc. The fix: </span><strong>quantify or qualify the improvement</strong><span>. E.g., “optimize this function to run in linear time (current version is quadratic)” or “refactor this to remove global variables and use a class instead.” Basically, </span><em>be explicit about what problem you’re solving with the refactor or feature</em><span>. If you leave it too open, the AI might solve a different problem than the one you care about.</span></p></li><li><p><strong>Anti-Pattern: Ignoring AI’s Clarification or Output.</strong><span> Sometimes the AI might respond with a clarifying question or an assumption. For instance: </span><em>“Are you using React class components or functional components?”</em><span> or </span><em>“I assume the input is a string – please confirm.”</em><span> If you ignore these and just reiterate your request, you’re missing an opportunity to improve the prompt. The AI is signaling that it needs more info. Always answer its questions or refine your prompt to include those details. Additionally, if the AI’s output is clearly off (like it misunderstood the question), </span><em>don’t just retry the same prompt verbatim</em><span>. Take a moment to adjust your wording. Maybe your prompt had an ambiguous phrase or omitted something essential. Treat it like a conversation – if a human misunderstood, you’d explain differently; do the same for the AI.</span></p></li><li><p><strong>Anti-Pattern: Varying Style or Inconsistency.</strong><span> If you keep changing how you ask or mixing different formats in one go, the model can get confused. For example, switching between first-person and third-person in instructions, or mixing pseudocode with actual code in a confusing way. Try to maintain a consistent style within a single prompt. If you provide examples, ensure they are clearly delineated (use Markdown triple backticks for code, quotes for input/output examples, etc.). Consistency helps the model parse your intent correctly. Also, if you have a preferred style (say, ES6 vs ES5 syntax), consistently mention it, otherwise the model might suggest one way in one prompt and another way later.</span></p></li><li><p><strong>Anti-Pattern: Vague references like “above code”.</strong><span> When using chat, if you say “the above function” or “the previous output”, be sure the reference is clear. If the conversation is long and you say “refactor the above code”, the AI might lose track or pick the wrong code snippet to refactor. It’s safer to either quote the code again or specifically name the function you want refactored. Models have a limited attention window, and although many LLMs can refer to prior parts of the conversation, giving it explicit context again can help avoid confusion. This is especially true if some time (or several messages) passed since the code was shown.</span></p></li></ul><p><span>Finally, here’s a </span><strong>tactical approach to rewriting prompts</strong><span> when things go wrong:</span></p><ul><li><p><strong>Identify what was missing or incorrect in the AI’s response.</strong><span> Did it solve a different problem? Did it produce an error or a solution that doesn’t fit? For example, maybe you asked for a solution in TypeScript but it gave plain JavaScript. Or it wrote a recursive solution when you explicitly wanted iterative. Pinpoint the discrepancy.</span></p></li><li><p><strong>Add or emphasize that requirement in a new prompt.</strong><span> You might say, </span><em>“The solution should be in TypeScript, not JavaScript. Please include type annotations.”</em><span> Or, </span><em>“I mentioned I wanted an iterative solution – please avoid recursion and use a loop instead.”</em><span> Sometimes it helps to literally use phrases like </span><em>“Note:”</em><span> or </span><em>“Important:”</em><span> in your prompt to highlight key constraints (the model doesn’t have emotions, but it does weigh certain phrasing as indicating importance). For instance: </span><em><span>“</span><strong>Important:</strong><span> Do not use any external libraries for this.”</span></em><span> or </span><em><span>“</span><strong>Note:</strong><span> The code must run in the browser, so no Node-specific APIs.”</span></em><span>.</span></p></li><li><p><strong>Break down the request further if needed.</strong><span> If the AI repeatedly fails on a complex request, try asking for a smaller piece first. Or ask a question that might enlighten the situation: </span><em>“Do you understand what I mean by X?”</em><span> The model might then paraphrase what it thinks you mean, and you can correct it if it’s wrong. This is meta-prompting – discussing the prompt itself – and can sometimes resolve misunderstandings.</span></p></li><li><p><strong>Consider starting fresh if the thread is stuck.</strong><span> Sometimes after multiple tries, the conversation may reach a confused state. It can help to start a new session (or clear the chat history for a moment) and prompt from scratch with a more refined ask that you’ve formulated based on previous failures. The model doesn’t mind repetition, and a fresh context can eliminate any accumulated confusion from prior messages.</span></p></li></ul><p>By being aware of these anti-patterns and their solutions, you’ll become much faster at adjusting your prompts on the fly. Prompt engineering for developers is very much an iterative, feedback-driven process (as any programming task is!). The good news is, you now have a lot of patterns and examples in your toolkit to draw from.</p><p><span>Prompt engineering is a bit of an art and a bit of a science – and as we’ve seen, it’s quickly becoming a must-have skill for developers working with AI code assistants. By crafting clear, context-rich prompts, you essentially </span><em>teach</em><span> the AI what you need, just as you would onboard a human team member or explain a problem to a peer. Throughout this article, we explored how to systematically approach prompts for debugging, refactoring, and feature implementation:</span></p><ul><li><p>We learned to feed the AI the same information you’d give a colleague when asking for help: what the code is supposed to do, how it’s misbehaving, relevant code snippets, and so on – thereby getting much more targeted help .</p></li><li><p>We saw the power of iterating with the AI, whether it’s stepping through a function’s logic line by line, or refining a solution through multiple prompts (like turning a recursive solution into an iterative one, then improving variable names) . Patience and iteration turn the AI into a true pair programmer rather than a one-shot code generator.</p></li><li><p>We utilized role-playing and personas to up-level the responses – treating the AI as a code reviewer, a mentor, or an expert in a certain stack . This often produces more rigorous and explanation-rich outputs, which not only solve the problem but educate us in the process.</p></li><li><p>For refactoring and optimization, we emphasized defining what “good” looks like (be it faster, cleaner, more idiomatic, etc.) , and the AI showed that it can apply known best practices when guided (like parallelizing calls, removing duplication, handling errors properly). It’s like having access to the collective wisdom of countless code reviewers – but you have to ask the right questions to tap into it.</p></li><li><p>We also demonstrated building new features step by step with AI assistance, showing that even complex tasks can be decomposed and tackled one prompt at a time. The AI can scaffold boilerplate, suggest implementations, and even highlight edge cases if prompted – acting as a knowledgeable co-developer who’s always available.</p></li><li><p>Along the way, we identified pitfalls to avoid: keeping prompts neither too vague nor too overloaded, always specifying our intent and constraints, and being ready to adjust when the AI’s output isn’t on target. We cited concrete examples of bad prompts and saw how minor changes (like including an error message or expected output) can dramatically improve the outcome.</p></li></ul><p><span>As you incorporate these techniques into your workflow, you’ll likely find that working with AI becomes more intuitive. You’ll develop a feel for what phrasing gets the best results and how to guide the model when it goes off course. Remember that the AI is a product of its training data – it has seen many examples of code and problem-solving, but it’s </span><em>you</em><span> who provides direction on which of those examples are relevant now. In essence, </span><strong>you set the context, and the AI follows through</strong><span>.</span></p><p><strong>It’s also worth noting that prompt engineering is an evolving practice.</strong><span> The community of developers is constantly discovering new tricks – a clever one-liner prompt or a structured template can suddenly go viral on social media because it unlocks a capability people didn’t realize was there. Stay tuned to those discussions (on Hacker News, Twitter, etc.) because they can inspire your own techniques. But also, don’t be afraid to experiment yourself. Treat the AI as a flexible tool – if you have an idea (“what if I ask it to draw an ASCII diagram of my architecture?”), just try it. You might be surprised at the results, and if it fails, no harm done – you’ve learned something about the model’s limits or needs.</span></p><p><strong>In summary, prompt engineering empowers developers to get more out of AI assistants.</strong><span> It’s the difference between a frustrating experience (“this tool is useless, it gave me nonsense”) and a productive one (“this feels like pair programming with an expert who writes boilerplate for me”). By applying the playbook of strategies we’ve covered – from providing exhaustive context to nudging the AI’s style and thinking – you can turn these code-focused AI tools into true extensions of your development workflow. The end result is not only that you code faster, but often you pick up new insights and patterns along the way (as the AI explains things or suggests alternatives), leveling up your own skillset.</span></p><p><span>As a final takeaway, remember that </span><strong>prompting is an iterative dialogue</strong><span>. Approach it with the same clarity, patience, and thoroughness you’d use when communicating with another engineer. Do that, and you’ll find that AI assistants can significantly amplify your abilities – helping you debug quicker, refactor smarter, and implement features with greater ease. </span></p><p><strong>Happy prompting, and happy coding!</strong></p><p><strong>Further reading:</strong></p><ul><li><p><em><a href="https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/#:~:text=3%20best%20practices%20for%20prompt,crafting%20with%20GitHub%20Copilot" rel="">How to write better prompts for GitHub Copilot</a></em><a href="https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/#:~:text=3%20best%20practices%20for%20prompt,crafting%20with%20GitHub%20Copilot" rel="">. GitHub Blog</a></p></li><li><p><em><a href="https://strapi.io/blog/ChatGPT-Prompt-Engineering-for-Developers#:~:text=1" rel="">ChatGPT Prompt Engineering for Developers: 13 Best Examples</a></em></p></li><li><p><em><a href="https://medium.com/data-science/using-chatgpt-for-efficient-debugging-fc9e065b7856#:~:text=If%20nothing%20comes%20to%20mind%2C,don%E2%80%99t%20be%20afraid%20to%20experiment" rel="">Using ChatGPT for Efficient Debugging</a></em></p></li><li><p><em><a href="https://dev.to/jamesbright/prompt-engineering-for-lazy-programmers-getting-exactly-the-code-you-want-and-even-more-out-of-chatgpt-3plf#:~:text=The%20Trick%3A%20,rewrite%20the%20function%20if%20necessary" rel="">Prompt Engineering for Lazy Programmers: Getting Exactly the Code You Want</a></em></p></li><li><p><em><a href="https://www.linkedin.com/pulse/best-practices-prompting-github-copilot-vs-code-pamela-fox#:~:text=In%20this%20post%2C%20I%27m%20going,provide%20context%20and%20be%20predictable" rel="">Best practices for prompting GitHub Copilot in VS Code</a></em></p></li><li><p><em><a href="https://medium.com/@shamawali/chatgpt-a-new-age-debugger-10-prompts-20ee3e9c63aa#:~:text=2,Debug%20This%20Function%20for%20Me" rel="">ChatGPT: A new-age Debugger, 10 Prompts</a></em></p></li><li><p><em><a href="https://dev.to/techiesdiary/chatgpt-prompts-for-code-review-and-debugging-48j#:~:text=5%20Debug%20Debug%20the%20given,Enter%20your%20code%20here" rel="">ChatGPT Prompts for Code Review and Debugging</a></em></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:624626,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg Merges WebRTC Support (731 pts)]]></title>
            <link>https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00</link>
            <guid>44182186</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00">https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00</a>, See on <a href="https://news.ycombinator.com/item?id=44182186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
avformat/whip:&nbsp;Add&nbsp;WHIP&nbsp;muxer&nbsp;support&nbsp;for&nbsp;subsecond&nbsp;latency&nbsp;streaming</p><p>

0.&nbsp;WHIP&nbsp;Version&nbsp;3.<br>
1.&nbsp;The&nbsp;WHIP&nbsp;muxer&nbsp;has&nbsp;been&nbsp;renamed&nbsp;and&nbsp;refined,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;improved&nbsp;logging&nbsp;context&nbsp;and&nbsp;error&nbsp;messages&nbsp;for&nbsp;SSL,&nbsp;DTLS,&nbsp;and&nbsp;RTC.<br>
2.&nbsp;Magic&nbsp;numbers&nbsp;have&nbsp;been&nbsp;replaced&nbsp;with&nbsp;macros&nbsp;and&nbsp;extracted&nbsp;to&nbsp;functions,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;log&nbsp;levels&nbsp;have&nbsp;been&nbsp;altered&nbsp;for&nbsp;better&nbsp;clarity.<br>
3.&nbsp;DTLS&nbsp;curve&nbsp;list&nbsp;has&nbsp;been&nbsp;updated,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;SRTP&nbsp;profile&nbsp;names&nbsp;have&nbsp;been&nbsp;refined&nbsp;for&nbsp;FFmpeg&nbsp;and&nbsp;OpenSSL.<br>
4.&nbsp;ICE&nbsp;STUN&nbsp;magic&nbsp;number&nbsp;has&nbsp;been&nbsp;refined,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;RTP&nbsp;payload&nbsp;types&nbsp;have&nbsp;been&nbsp;updated&nbsp;based&nbsp;on&nbsp;Chrome's&nbsp;definition.<br>
5.&nbsp;Fixed&nbsp;frame&nbsp;size&nbsp;has&nbsp;been&nbsp;refined&nbsp;to&nbsp;rtc-&gt;audio_par-&gt;frame_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;h264_mp4toannexb&nbsp;is&nbsp;now&nbsp;used&nbsp;to&nbsp;convert&nbsp;MP4/ISOM&nbsp;to&nbsp;annexb.<br>
6.&nbsp;OPUS&nbsp;timestamp&nbsp;issue&nbsp;has&nbsp;been&nbsp;addressed,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;marker&nbsp;setting&nbsp;has&nbsp;been&nbsp;corrected&nbsp;after&nbsp;utilizing&nbsp;BSF.<br>
7.&nbsp;DTLS&nbsp;handshake&nbsp;and&nbsp;ICE&nbsp;handling&nbsp;have&nbsp;been&nbsp;optimized&nbsp;for&nbsp;improved&nbsp;performance,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;a&nbsp;single&nbsp;handshake&nbsp;timeout&nbsp;and&nbsp;server&nbsp;role&nbsp;to&nbsp;prevent&nbsp;ARQ.<br>
8.&nbsp;Consolidated&nbsp;ICE&nbsp;request/response&nbsp;handling&nbsp;and&nbsp;DTLS&nbsp;handshake&nbsp;into&nbsp;a&nbsp;single&nbsp;function,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;fixed&nbsp;OpenSSL&nbsp;build&nbsp;errors&nbsp;to&nbsp;work&nbsp;with&nbsp;Pion.<br>
9.&nbsp;Merge&nbsp;TLS&nbsp;&amp;&nbsp;DTLS&nbsp;implementation,&nbsp;shared&nbsp;BIO&nbsp;callbacks,&nbsp;read,&nbsp;write,<br>
&nbsp;&nbsp;&nbsp;&nbsp;print_ssl_error,&nbsp;openssl_init_ca_key_cert,<br>
&nbsp;&nbsp;&nbsp;&nbsp;init_bio_method&nbsp;function&nbsp;and&nbsp;shared&nbsp;same&nbsp;data&nbsp;structure<br>
10.&nbsp;Modify&nbsp;configure&nbsp;that&nbsp;whip&nbsp;is&nbsp;enabled&nbsp;only&nbsp;dtls&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;enabled(just&nbsp;support&nbsp;openssl&nbsp;for&nbsp;now)&nbsp;to&nbsp;fix&nbsp;build&nbsp;error</p><p>

<span>Co-authored-by: winlin &lt;winlinvip@gmail.com&gt;</span><br>
<span>Co-authored-by: yangrtc &lt;yangrtc@aliyun.com&gt;</span><br>
<span>Co-authored-by: cloudwebrtc &lt;duanweiwei1982@gmail.com&gt;</span><br>
<span>Co-authored-by: Haibo Chen &lt;495810242@qq.com&gt;</span><br>
<span>Co-authored-by: Steven Liu &lt;lq@chinaffmpeg.org&gt;</span><br>
<span>Co-authored-by: Jun Zhao &lt;barryjzhao@tencent.com&gt;</span><br>
<span>Signed-off-by: Jack Lau &lt;jacklau1222@qq.com&gt;</span><br>
<span>Signed-off-by: Steven Liu &lt;lq@chinaffmpeg.org&gt;</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we reduced the impact of zombie clients (110 pts)]]></title>
            <link>https://letsencrypt.org/2025/06/04/how-we-reduced-the-impact-of-zombie-clients/</link>
            <guid>44182184</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2025/06/04/how-we-reduced-the-impact-of-zombie-clients/">https://letsencrypt.org/2025/06/04/how-we-reduced-the-impact-of-zombie-clients/</a>, See on <a href="https://news.ycombinator.com/item?id=44182184">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>Every night, right around midnight (mainly <a href="https://en.wikipedia.org/wiki/Coordinated_Universal_Time">UTC</a>), a horde of zombies wakes up and clamors for … digital certificates!</p>
<p>The zombies in question are abandoned or misconfigured Internet servers and ACME clients that have been set to request certificates from Let’s Encrypt. As our certificates <a href="https://letsencrypt.org/2015/11/09/why-90-days/">last for at most 90 days</a>, these zombie clients’ software knows that their certificates are out-of-date and need to be replaced. What they don’t realize is that their quest for new certificates is doomed! These devices are cursed to seek certificates again and again, never receiving them.</p>
<p>But they do use up a lot of certificate authority resources in the process.</p>
<h3 id="the-zombie-client-problem">The Zombie Client Problem</h3>
<p>Unlike a human being, software doesn’t give up in frustration, or try to modify its approach, when it repeatedly fails at the same task. Our emphasis on automation means that the vast majority of Let’s Encrypt certificate renewals are performed by automated software. This is great when those renewals succeed, but it also means that forgotten clients and devices can continue requesting renewals unsuccessfully for months, or even years.</p>
<p>How might that happen? Most often, it happens when a device no longer has a domain name pointed to it. The device itself doesn’t know that this has changed, so it treats renewal failures as transient even though they are actually permanent. For instance:</p>
<ul>
<li>An organization may have allowed a domain name registration to lapse because it is no longer needed, but its servers are still configured to request certs for it.</li>
<li>Or, a home user stopped using a particular dynamic-DNS domain with a network-attached storage device, but is still using that device at home. The device doesn’t realize that the user no longer expects to use the name, so it keeps requesting certs for it.</li>
<li>Or, a web hosting or CDN customer migrated to a different service provider, but never informed the old service provider. The old service provider’s servers keep requesting certs unsuccessfully. If the customer was in a free service tier, there might not be invoices or charges reminding the customer to cancel the service.</li>
<li>Or any number of other, subtler changes in a subscriber’s infrastructure, such as changing a firewall rule or some webserver configuration.</li>
</ul>
<p>At the scale of Let’s Encrypt, which now covers <a href="https://letsencrypt.org/stats/">hundreds of millions of names</a>, scenarios like these have become common, and their impact has become substantial. In 2024, we noticed that about half of all certificate requests to the Let’s Encrypt ACME API came from about a million accounts that never successfully complete any validations. Many of these had completed validations and issued certificates sometime in the past, but nowadays every single one of their validation attempts fails, and they show no signs that this will change anytime soon.</p>
<p>Unfortunately, trying to validate those futile requests still uses resources. Our CA software has to generate challenges, reach out and attempt to validate them over the Internet, detect and report failures, and record all of the associated information in our databases and audit logs. And over time, we’ve seen more and more recurring failures: accounts that always fail their issuance requests have been growing at around 18% per year.</p>
<p>In January, we mentioned that we had been addressing the zombie client problem <a href="https://letsencrypt.org/2025/01/30/scaling-rate-limits/">through our rate limit system</a>. This post provides more detail on that progress.&nbsp;</p>
<h3 id="our-rate-limit-philosophy">Our Rate Limit Philosophy</h3>
<p>If you’ve used Let’s Encrypt as a subscriber, you may have run into one of our <a href="https://letsencrypt.org/docs/rate-limits/">rate limits</a> at some point, maybe during your initial setup process. We have eight different kinds of rate limits in place now; as our January post describes, they’ve become more algorithmically sophisticated and grown to address a wider range of problems. A key principle for Let’s Encrypt is that our rate limiting is not a punishment. We don’t think of rate limits as a way of retaliating against a client for misbehavior. Rate limits are simply a tool to maximize the efficient use of our limited resources and prevent people and programs from using up those resources for no constructive purpose.</p>
<p>We’ve consistently tried to design our rate limit mechanisms in line with that philosophy. So if a misconfiguration or misunderstanding has caused excessive requests in the past, we’re still happy to welcome the user in question back and start issuing them certificates again—once the problem has been addressed. We want the rate limits to put a brake on wasteful use of our systems, but not to frustrate users who are actively trying to make Let’s Encrypt work for them.</p>
<p>In addition, we’ve always implemented our rate limits to err on the side of permissiveness. For example, if the Redis instances where rate limits are tracked have an outage or lose data, the system is designed to permit more issuance rather than less issuance as a result.</p>
<p>We wanted to create additional limits that would target zombie clients, but in a correspondingly non-punitive way that would avoid any disruption to valid issuance, and welcome subscribers back quickly if they happened to notice and fix a long-time problem with their setups.</p>
<h3 id="our-zombie-related-rate-limits-and-their-impact">Our Zombie-Related Rate Limits and Their Impact</h3>
<p>In planning a new zombie-specific response, we decided on a “pausing” approach, which can temporarily limit an account’s ability to proceed with certificate requests. The core idea is that, if a particular account consistently fails to complete validation for a particular hostname, we’ll pause that account-hostname pair. The pause means that any new order requests from that account for that hostname will be rejected immediately, before we get to the resource-intensive validation phase.</p>
<p>This approach is more finely targeted than pausing an entire account. Pausing account-hostname pairs means that your ability to issue certs for a specific name could be paused due to repeated failures, but you can still get all of your other certs like normal. So a large hosting provider doesn’t have to fear that its certificate issuance on behalf of one customer will be affected by renewal failures related to a problem with a different customer’s domain name. The account-specificity of the pause, in turn, means that validation failures from one subscriber or device won’t prevent a different subscriber or device from attempting to validate the same name, as long as the devices in question don’t share a single Let’s Encrypt account.</p>
<p>In September 2024, we began applying our zombie rate limits manually by pausing about 21,000 of the most recurrently-failing account-hostname pairs, those which were consistently repeating the same failed requests many times per day, every day. After implementing that first round of pauses, we immediately saw a significant impact on our failed request rates. As we announced at that time, we also began <a href="https://community.letsencrypt.org/t/automatic-pausing-of-zombie-clients/229642">using a formula to automatically pause other zombie client account-hostname pairs from December 2024 onward</a>. The associated new rate limit is called “<a href="https://letsencrypt.org/docs/rate-limits/#consecutive-authorization-failures-per-hostname-per-account">Consecutive Authorization Failures per Hostname Per Account</a>” (and is independent of the existing “Authorization Failures per Hostname Per Account” limit, which resets every hour).</p>
<p>This formula relates to the frequency of successive failed issuance requests for the same domain name by the same Let’s Encrypt account. It applies only to failures that happen again and again, with no successful issuances at all in between: a single successful validation immediately resets the rate limit all the way to zero. Like all of our rate limits, this is not a punitive measure but is simply intended to reduce the waste of resources. So, we decided to set the thresholds rather high in the expectation that we would catch only the most disruptive zombie clients, and ultimately only those clients that were extremely unlikely to succeed in the future based on their substantial history of failed requests. We don’t hurry to block requesters as zombies: according to our current formula, client software following the default established by EFF’s <a href="https://certbot.eff.org/">Certbot</a> (two renewal attempts per day) would be paused as a zombie only after about ten years of constant failures. More aggressive failed issuance attempts will get a client paused sooner, but clients will generally have to fail hundreds or thousands of attempts in a row before they are paused.</p>
<p>Most subscribers using mainstream client applications with default configurations will never encounter this rate limit, even if they forget to deactivate renewal attempts for domains that are no longer pointed at their servers. As described below, our current limit is already providing noticeable benefits with minimal disruption, and we’re likely to tighten it a bit in the near future, so it will trigger after somewhat fewer consecutive failures.</p>
<h3 id="self-service-unpausing">Self-Service Unpausing</h3>
<p>A key feature in our zombie issuance pausing mechanism is self-service unpausing. Whenever an account-hostname pair is paused, any new certificate requests for that hostname submitted by that account are immediately rejected. But this means that the “one successful validation immediately resets the rate limit counter” feature can no longer come into effect: once they’re paused, they can’t even attempt validation anymore.</p>
<p>So every rejection comes with an error message explaining what has happened and a custom link that can be used to immediately unpause that account-hostname pair and remove any other pauses on the same account at the same time. The point of this is that subscribers who notice at some point that issuance is failing and want to intervene to get it working again have a straightforward option to let Let’s Encrypt know that they’re aware of the recurring failures and are still planning to use a particular account. As soon as subscribers notify us via the self-service link, they’ll be able to issue certificates again.</p>
<p>Currently, the user interface for an affected subscriber looks like this:</p>
<p><img src="https://letsencrypt.org/images/blog/blog-2025-06-04--image1.jpg" alt="Let's Encrypt unpause interface"></p><p>This link would be provided via an ACME error message in response to any request that was blocked due to a pause account-hostname pair.</p>
<p>As it’s turned out, the unpause option shown above has only been used by about 3% of affected accounts! This goes to show that most of the zombies we’ve paused were, in fact, well and truly forgotten about.</p>
<p>However, the unpause feature is there for whenever it’s needed, and there may be cases when it will become more important. A very large integration could trigger the zombie-related rate limits if a newly-introduced software bug causes what looks like a very high volume of zombie requests in a very short time. In that case, once that bug has been noticed and fixed, an integrator may need to unpause its issuance on behalf of lots of customers at once. Our unpause feature permits unpausing 50,000 domain names on a single account at a time, so even the largest integrators can get themselves unpaused expeditiously in this situation.</p>
<h3 id="conclusion">Conclusion</h3>
<p>We’ve been very happy with the results of our zombie mitigation measures, and, as far as we can tell, there’s been almost no impact for subscribers! Our statistics indicate that we’ve managed to reduce the load on our infrastructure while causing no detectable harm or inconvenience to subscribers’ valid issuance requests.</p>
<p>Since implementing the manual pauses in September and the automated pauses in December, we’ve seen:</p>
<ul>
<li>Over 100,000 account-hostname pairs have been paused for excessive failures.</li>
<li>We received zero (!) associated complaints or support requests.</li>
<li>About 3,200 people manually unpaused issuance.</li>
<li>Failed certificate orders fell by about 30% so far, and should continue to fall over time as we fine-tune the rate limit formula and catch more zombie clients.</li>
</ul>
<p>The new rate limit and the self-service unpause system are also ready to deal with circumstances that might produce more zombie clients in the future. For instance, we’ve announced that <a href="https://letsencrypt.org/2025/01/22/ending-expiration-emails/">we’re going to be discontinuing renewal reminder emails</a> soon. If some subscribers overlook failed renewals in the future, we might see more paused clients that result from unintentional renewal failures. We think taking advantage of the existing self-service unpause feature will be straightforward in that case. But it’s much better to notice problems and get them fixed up front, so please remember to <a href="https://letsencrypt.org/docs/monitoring-options/">set up your own monitoring</a> to avoid unnoticed renewal failures in the future.</p>
<p>If you’re a subscriber who’s had occasion to use the self-service unpause feature, we’d love your feedback on the <a href="https://community.letsencrypt.org/">Community Forum</a> about your experience using the feature and the circumstances that surrounded your account’s getting paused.</p>
<p>Also, if you’re a Let’s Encrypt client developer, please remember to make renewal requests at a random time (not precisely at midnight) so that the load on our infrastructure is smoothed out. You can also reduce the impact of zombie renewals by repeating failed requests somewhat less frequently over time (a “back-off” strategy), especially if the failure reason makes it look like a domain name may no longer be in use at all.</p>

      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A practical guide to building agents [pdf] (170 pts)]]></title>
            <link>https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf</link>
            <guid>44181700</guid>
            <pubDate>Wed, 04 Jun 2025 15:21:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf">https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=44181700">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[AGI is not multimodal (137 pts)]]></title>
            <link>https://thegradient.pub/agi-is-not-multimodal/</link>
            <guid>44181613</guid>
            <pubDate>Wed, 04 Jun 2025 15:15:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thegradient.pub/agi-is-not-multimodal/">https://thegradient.pub/agi-is-not-multimodal/</a>, See on <a href="https://news.ycombinator.com/item?id=44181613">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <blockquote>"In projecting language back as the model for thought, we lose sight of the tacit embodied understanding that undergirds our intelligence." –Terry Winograd</blockquote><p>The recent successes of generative AI models have convinced some that AGI is imminent. While these models appear to capture the essence of human intelligence, they defy even our most basic intuitions about it. They have emerged not because they are thoughtful solutions to the problem of intelligence, but because they <em>scaled</em> effectively on <a href="https://dl.acm.org/doi/abs/10.1145/3467017">hardware we already had</a>. Seduced by the fruits of scale, some have come to believe that it provides a clear pathway to AGI. The most emblematic case of this is the multimodal approach, in which massive modular networks are optimized for an array of modalities that, taken together, <em>appear</em> general. However, I argue that this strategy is sure to fail in the near term; it will not lead to human-level AGI that can, e.g., perform sensorimotor reasoning, motion planning, and social coordination. Instead of trying to glue modalities together into a patchwork AGI, <strong>we should pursue approaches to intelligence that treat embodiment and interaction with the environment as primary</strong>, and see modality-centered processing as emergent phenomena.</p><p>Preface: Disembodied definitions of Artificial General Intelligence — emphasis on <em>general</em> — exclude crucial problem spaces that we should expect AGI to be able to solve. <strong>A true AGI must be general across all domains.</strong> Any <em>complete</em> definition must at least include the ability to solve problems that originate in physical reality, e.g. repairing a car, untying a knot, preparing food, etc. As I will discuss in the next section, <strong>what is needed for these problems is a form of intelligence that is fundamentally situated in something like a physical world model</strong>. For more discussion on this, look out for <em>Designing an Intelligence</em>. Edited by George Konidaris, MIT Press, forthcoming.<br></p><h2 id="why-we-need-the-world-and-how-llms-pretend-to-understand-it">Why We Need the World, and How LLMs Pretend to Understand It</h2><p>TLDR: I first argue that <strong>true AGI needs a physical understanding of the world</strong>, as many problems <a href="https://hci.stanford.edu/winograd/papers/thinking-machines.html">cannot be converted into a problem of symbol manipulation</a>. It has been suggested by some that LLMs are learning a model of the world through next token prediction, but it is more likely that LLMs are learning bags of heuristics to predict tokens. This leaves them with a superficial understanding of reality and contributes to false impressions of their intelligence.</p><p><strong>The most shocking result of the predict-next-token objective is that it yields AI models that reflect a deeply human-like understanding of the world, despite having never observed it like we have.</strong> This result has <a href="https://aclanthology.org/2020.acl-main.463/">led to confusion</a> about what it means to <em>understand language</em> and even to <em>understand the world</em> — something we have long believed to be a prerequisite for language understanding. <strong>One explanation for the capabilities of LLMs comes from </strong><a href="https://www.youtube.com/watch?v=SjhIlw3Iffs&amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;index=63"><strong>an emerging theory</strong></a><strong> suggesting that they induce models of the world through next-token prediction</strong>. Proponents of this theory cite the prowess of SOTA LLMs on various benchmarks, the <a href="https://phillipi.github.io/prh/">convergence of large models to similar internal representations</a>, and their favorite rendition of the idea that “language mirrors the structure of reality,” a notion that has been espoused at least by Plato, Wittgenstein, Foucault, and Eco. While I’m generally in support of digging up esoteric texts for research inspiration, I’m worried that this metaphor has been taken too literally. Do LLMs really learn implicit models of the world? How could they otherwise be so proficient at language?</p><p>One source of evidence in favor of the LLM world modeling hypothesis is <a href="https://arxiv.org/abs/2210.13382">the Othello paper</a>, wherein researchers were able to predict the board of an Othello game from the hidden states of a transformer model trained on sequences of <em>legal</em> <em>moves</em>. However, there are <em>many</em> issues with generalizing these results to models of natural language. For one, whereas Othello moves can <em>provably</em> be used to deduce the full state of an Othello board,<strong> we have no reason to believe that a complete picture of the physical world can be inferred by a linguistic description. </strong>What sets the game of Othello apart from many tasks in the physical world is that <strong>Othello fundamentally resides in the land of symbols, and is merely implemented using physical tokens to make it easier for humans to play.</strong> A full game of Othello can be played with just pen and paper, but one can’t, e.g., sweep a floor, do dishes, or drive a car with just pen and paper. To solve such tasks, you need some physical conception of the world beyond what humans can merely <em>say</em> about it. Whether that conception of the world is encoded in a formal world model or, e.g., a value function is up for debate, but it is clear that <strong>there are many problems in the physical world that </strong><em><strong>cannot</strong></em><strong> be </strong><a href="https://dl.acm.org/doi/10.1145/360018.360022"><strong>fully represented by a system of symbols</strong></a><strong> and solved with mere symbol manipulation.</strong></p><p>Another issue stated in <a href="https://aiguide.substack.com/p/llms-and-world-models-part-2">Melanie Mitchell’s recent piece</a> and supported <a href="https://arxiv.org/abs/2406.03689">by this paper</a>, is that there is evidence that <strong>generative models can score remarkably well on sequence prediction tasks while failing to learn models of the worlds that created such sequence data</strong>, e.g. by learning comprehensive sets of idiosyncratic heuristics. E.g., it was pointed out in <a href="https://www.lesswrong.com/posts/gcpNuEZnxAPayaKBY/othellogpt-learned-a-bag-of-heuristics-1">this blog post</a> that OthelloGPT learned sequence prediction rules that don’t actually hold for all possible Othello games, like “if the token for B4 does not appear before A4 in the input string, then B4 is empty.” While one can argue that it doesn’t matter <em>how</em> a world model predicts the next state of the world, it should raise suspicion when that prediction reflects a better understanding of the training data than the underlying world that led to such data. This, unfortunately, is the central fault of the predict-next-token objective, which seeks only to retain information relevant to the prediction of the next token. <strong>If it can be done with something easier to learn than a world model, it likely will be.</strong></p><p>To claim without caveat that predicting the <em>effects of earlier symbols on later symbols</em> requires a model of the world like the ones humans generate from perception would be to abuse the <a href="https://lingo.csail.mit.edu/blog/world_models/">“world model” notion</a>. Unless we disagree on what the world is, it should be clear that a <em>true</em> world model can be used to predict the next state of the <em>physical</em> world given a history of states. Similar world models, which predict high fidelity observations of the physical world, are leveraged in many subfields of AI including <a href="http://www.incompleteideas.net/book/the-book-2nd.html">model-based reinforcement learning</a>, <a href="https://arxiv.org/abs/2010.01083">task and motion planning in robotics</a>, <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">causal world modeling</a>, and <a href="https://dynamic3dgaussians.github.io/">areas of computer vision</a> to solve problems instantiated in physical reality. LLMs are simply not running physics simulations in their latent next-token calculus when they ask you if your person, place, or thing is bigger than a breadbox. In fact, I conjecture that <strong>the behavior of LLMs is not thanks to a learned world model, but to brute force memorization of incomprehensibly abstract rules governing the behavior of symbols, i.e. a model of </strong><em><strong>syntax</strong></em><strong>.</strong></p><p>Quick primer:</p><ul><li><strong>Syntax</strong> is a subfield of linguistics that studies how words of various grammatical categories (e.g. parts of speech) are arranged together into sentences, which can be parsed into syntax trees. <em>Syntax studies the structure of sentences and the atomic parts of speech that compose them.</em></li><li><em><em><strong>Semantics</strong> is another subfield concerned with the literal meaning of sentences, e.g., compiling “I am feeling chilly” into the <em>idea</em> that you are experiencing cold. <em>Semantics boils language down to literal meaning, which is information about the world or human experience.</em></em></em></li><li><strong>Pragmatics</strong> studies the interplay of physical and conversational context on speech interactions, like when someone knows to close an ajar window when you tell them “I am feeling chilly.” <em>Pragmatics involves interpreting speech while reasoning about the environment and the intentions and hidden knowledge of other agents.</em></li></ul><!--kg-card-begin: markdown--><p>Without getting too technical, there is intuitive evidence that somewhat separate systems of cognition are responsible for each of these linguistic faculties. Look no further than the capability for humans to generate syntactically well-formed sentences that have no semantic meaning, e.g. Chomsky’s famous sentence “Colorless green ideas sleep furiously,” or sentences with well-formed semantics that make no pragmatic sense, e.g. responding merely with “Yes, I can” when asked, “Can you pass the salt?” Crucially, <strong>it is the fusion of the disparate cognitive abilities underpinning them that coalesce into human language understanding.</strong> For example, there isn’t anything syntactically wrong with the sentence, “The fridge is in the apple,” as a syntactic account of “the fridge” and “the apple” would categorize them as noun phrases that can be used to produce a sentence with the production rule, S → (NP “is in” NP). However, <strong>humans recognize an obvious semantic failure in the sentence that becomes apparent after attempting to reconcile its meaning with our understanding of reality</strong>: we know that fridges are larger than apples, and could not be fit into them.</p>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><p>But what if you have never perceived the real world, yet still were trying to figure out whether the sentence was ill-formed? <strong>One solution could be to embed semantic information at the level of syntax</strong>, e.g., by inventing new syntactic categories, NP<sub>the fridge</sub> and NP<sub>the apple </sub>, and a single new production rule that prevents semantic misuse: S → (NP<sub>the apple</sub> “is in” NP<sub>the fridge </sub>). While this strategy would no longer require grounded world knowledge about fridges and apples, e.g., <strong>it would require special grammar rules for every semantically well-formed construction… which is actually possible to learn given a massive corpus of natural language.</strong> Crucially, this would not be the same thing as grasping semantics, which in my view is fundamentally about understanding the nature of the world.</p>
<!--kg-card-end: markdown--><p>Finding that LLMs have reduced problems of semantics and pragmatics into syntax would have profound implications on how we should view their intelligence. People often treat language proficiency as a proxy for general intelligence by, e.g., strongly associating pragmatic and semantic understanding with the cognitive abilities that undergird them in humans. For example, someone who appears well-read and graceful in navigating social interactions is likely to score high in traits like sustained attention and theory of mind, which lie closer to measures of raw cognitive ability. In general, these proxies are reasonable for assessing a <em>person’s</em> general intelligence, but not an LLM’s, as the apparent linguistic skills of LLMs could come from entirely separate mechanisms of cognition.</p><h2 id="the-bitter-lesson-revisited">The Bitter Lesson Revisited</h2><p>TLDR: Sutton’s Bitter Lesson has sometimes been interpreted as meaning that making <em>any</em> assumptions about the structure of AI is a mistake. This is both unproductive and a misinterpretation; it is precisely when humans think deeply about the structure of intelligence that major advancements occur. Despite this, scale maximalists have implicitly suggested that multimodal models can be a structure-agnostic framework for AGI. Ironically, today’s multimodal models contradict Sutton’s Bitter Lesson by making implicit assumptions about the structure of individual modalities and how they should be sewn together. <strong>In order to build AGI, we must either think deeply about how to unite existing modalities, or dispense with them altogether in favor of an interactive and embodied cognitive process.</strong></p><figure><img src="https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png" alt="" loading="lazy" width="1024" height="733" srcset="https://thegradient.pub/content/images/size/w600/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png 600w, https://thegradient.pub/content/images/size/w1000/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png 1000w, https://thegradient.pub/content/images/2025/06/ChatGPT-Image-May-21--2025--11_31_12-AM-copy.png 1024w" sizes="(min-width: 720px) 720px"></figure><p>The paradigm that led to the success of LLMs is marked primarily by <em>scale</em>, not efficiency. We have effectively trained a pile of one trillion ants for one billion years to mimic the form and function of a Formula 1 race car; eventually it gets there, but wow was the process inefficient. This analogy nicely captures a debate between structuralists, who want to build things like "wheels" and "axles" into AI systems, and scale maximalists, who want more ants, years, and F1 races to train on. Despite many decades of structuralist study in linguistics, the unstructured approaches of scale maximalism have yielded far better ant-racecars in recent years. This was most notably articulated by Rich Sutton — a recent recipient of the Turing Award along with Andy Barto for their work in Reinforcement Learning — in his piece “The Bitter Lesson.”</p><p>[W]e should build in only the meta-methods that can find and capture this arbitrary complexity… Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. - Rich Sutton</p><p>Sutton’s argument is that methods that leverage computational resources will outpace methods that do not, and that any structure for problem-solving built as an inductive bias into AI will hinder it from learning better solutions. <strong>This is a compelling argument</strong> <strong>that I believe has been seriously misinterpreted by some as implying that making </strong><em><strong>any</strong></em><strong> assumptions about structure is a false step.</strong> It is, in fact, human intuition that was responsible for many significant advancements in the development of SOTA neural network architectures. For example, <a href="https://en.wikipedia.org/wiki/AlexNet#:~:text=AlexNet%20is%20a%20convolutional%20neural,Visual%20Recognition%20Challenge%20(ILSVRC).">Convolutional Neural Networks</a> made an assumption about translation invariance for pattern recognition in images and kickstarted the modern field of deep learning for computer vision; the <a href="https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf">attention mechanism of Transformers</a> made an assumption about the long-distance relationships between symbols in a sentence that made ChatGPT possible and had nearly everyone drop their RNNs; and <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">3D Gaussian Splatting</a> made an assumption about the solidity of physical objects that made it more performant than NeRFs. Potentially none of these methodological assumptions apply to the entire domain of <em>possible</em> scenes, images, or token streams, but they do for the specific ones that humans have curated and formed structural intuitions about. Let’s not forget that humans have co-evolved with the environments that these datasets are drawn from.</p><p>The real question is how we might heed Sutton’s Bitter Lesson in our development of AGI. The scale maximalist approach worked for LLMs and LVMs (large vision models) because we had natural deposits of text and image data, but <strong>an analogous application of scale maximalism to AGI would require forms of embodiment data that we simply don’t have. </strong>One solution to this data scarcity issue extends the generative modeling paradigm to multimodal modeling — encompassing language, vision, and action — <strong>with the hope that a general intelligence can be built by summing together general models of narrow modalities.</strong></p><p>There are multiple issues with this approach. First, <strong>there are deep connections between modalities that are unnaturally severed in the multimodal setting</strong>, making the problem of concept synthesis ever more difficult. In practice, uniting modalities often involves pre-training dedicated neural modules for each modality and joining them together into a joint embedding space. In the early days, this was achieved by nudging the embeddings of, e.g. (language, vision, action) tuples to converge to similar latent vectors of meaning, a vast oversimplification of the kinds of relationships that may exist between modalities. One can imagine, e.g., captioning an image at various levels of abstraction, or implementing the same linguistic instruction with different sets of physical actions. Such one-to-many relationships suggest that a contrastive embedding objective is not suitable.</p><p>While modern approaches do not make such stringent assumptions about how modalities should be united, they still universally encode percepts from all modalities (e.g. text, images) into the same latent space. Intuitively, it would seem that such latent spaces could serve as common conceptual ground across modalities, analogous to a space of human concepts. However, these latent spaces do not cogently capture all information relevant to a concept, and instead rely on modality-specific decoders to flesh out important details. <strong>The “meaning” of a percept is not </strong><em><strong>in</strong></em><strong> the vector it is encoded as, but in the way relevant decoders process this vector into meaningful outputs. </strong>As long as various encoders and decoders are subject to modality-specific training objectives, “meaning” will be decentralized and potentially inconsistent across modalities, especially as a result of pre-training. This is not a recipe for the formation of coherent concepts.</p><p>Furthermore, it is not clear that today’s modalities are an appropriate partitioning of the observation and action spaces for an embodied agent. It is not obvious that, e.g., images and text should be represented as separate observation streams, nor text production and motion planning as separate action capabilities. <strong>The human capacities for reading, seeing, speaking, and moving are ultimately mediated by overlapping cognitive structures.</strong> <strong>Making structural assumptions about how modalities ought to be processed is likely to hinder the discovery of more fundamental cognition </strong>that is responsible for processing data in all modalities. One solution would be to consolidate unnaturally partitioned modalities into a unified data representation. This would encourage networks to learn intelligent processes that generalize across modalities. Intuitively, <strong>a model that can understand the visual world as well as humans can</strong> — including everything from human writing to traffic signs to visual art — <strong>should not make a serious architectural distinction between images and text. </strong>Part of the reason why VLMs can’t, e.g., count the number of letters in a word is because they can’t <em>see</em> what they are writing.</p><p>Finally, the <strong>learn-from-scale approach trains models to copy the conceptual structure of humans instead of learning the general capability to form novel concepts on their own.</strong> Humans have spent hundreds of thousands of years refining concepts and passing them memetically through culture and language. Today’s models are trained only on the end result of this process: the present-day conceptual structures that make it into the corpus. By <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">optimizing for the ultimate products</a> of our intelligence, we have ignored the question of how those products were invented and discovered. Humans have a unique ability to form durable concepts from few examples and ascribe names to them, reason about them analogically, etc. While the in-context capabilities of today’s models can be impressive, <a href="https://arxiv.org/abs/2311.08993">they grow increasingly limited</a> as tasks become more complex and stray further from the training data. <strong>The flexibility to form new concepts from experience is a foundational attribute of general intelligence</strong>, we should think carefully about how it arises.</p><p>While structure-agnostic scale maximalism has succeeded in producing LLMs and LVMs that pass Turing tests, a multimodal scale maximalist approach to AGI will not bear similar fruit. <strong>Instead of pre-supposing structure in individual modalities, we should design a setting in which modality-specific processing emerges naturally.</strong> For example, <a href="https://arxiv.org/abs/2502.01568">my recent paper</a> on visual theory of mind saw abstract symbols naturally emerge from communication between image-classifying agents, blurring the lines between text and image processing. Eventually, we should hope to reintegrate as many features of intelligence as possible <a href="https://www.youtube.com/watch?v=-hztAN4MdrA">under the same umbrella</a>. However, it is not clear whether there is genuine commercial viability in such an approach as long as scaling and fine-tuning narrow intelligence models solves commercial use-cases.</p><h2 id="conclusion">Conclusion</h2><p>The overall promise of scale maximalism is that a Frankenstein AGI can be sewed together using general models of narrow domains. I argue that this is extremely unlikely to yield an AGI that feels complete in its intelligence. If we intend to continue reaping the streamlined efficiency of modality-specific processing, we must be intentional in how modalities are united — ideally drawing from human intuition and classical fields of study, e.g. <a href="http://nscl.csail.mit.edu/">this work from MIT</a>. Alternatively, we can re-formulate learning as an embodied and interactive process where disparate modalities naturally fuse together. We could do this by, e.g., processing images, text, and video using the same perception system and producing actions for generating text, manipulating objects, and navigating environments using the same action system. What we will lose in efficiency we will gain in flexible cognitive ability.</p><p>In a sense, the most challenging mathematical piece of the AGI puzzle has already been solved: the discovery of universal function approximators. What’s left is to inventory the functions we need and determine how they ought to be arranged into a coherent whole. This is a conceptual problem, not a mathematical one.</p><hr><h2 id="acknowledgements">Acknowledgements</h2><p>I would like to thank <a href="https://lucasgelfond.online/">Lucas Gelfond</a>, <a href="https://db7894.github.io/">Daniel Bashir</a>, <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>, and my father, Joseph Spiegel, for their thoughtful and thorough feedback on this work. Thanks to <a href="https://alinapringle.format.com/">Alina Pringle</a> for the wonderful illustration made for this piece.</p><p>Benjamin is a PhD candidate in Computer Science at Brown University. He is interested in models of language understanding that ground meaning to elements of structured decision-making. For more info see his <a href="https://benjaminaspiegel.com/">personal website</a>.</p><h2 id="citation">Citation</h2><p>For attribution in academic contexts or books, please cite this work as</p><!--kg-card-begin: markdown--><pre><code>Benjamin A. Spiegel, "AGI Is Not Multimodal", The Gradient, 2025.
</code></pre>
<!--kg-card-end: markdown--><!--kg-card-begin: markdown--><pre><code>@article{spiegel2025agi,
    author = {Benjamin A. Spiegel},
    title = {AGI Is Not Multimodal},
    journal = {The Gradient},
    year = {2025},
    howpublished = {\url{https://thegradient.pub/agi-is-not-multimodal},
}
</code></pre>
<!--kg-card-end: markdown--><h2 id="references">References</h2><p>Andreas, Jacob. “Language Models, World Models, and Human Model-Building.” <em>Mit.edu</em>, 2024, lingo.csail.mit.edu/blog/world_models/.</p><p>Belkin, Mikhail, et al. "Reconciling modern machine-learning practice and the classical bias–variance trade-off." <em>Proceedings of the National Academy of Sciences</em> 116.32 (2019): 15849-15854.</p><p>Bernhard Kerbl, et al. “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” <em>ACM Transactions on Graphics</em>, vol. 42, no. 4, 26 July 2023, pp. 1–14, https://doi.org/10.1145/3592433.</p><p>Chomsky, Noam. 1965. Aspects of the theory of syntax. Cambridge, Massachusetts: MIT Press.</p><p><em>Designing an Intelligence</em>. Edited by George Konidaris, MIT Press, 2026.</p><p>Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 5185–5198, Online. Association for Computational Linguistics.</p><p>Eye on AI. “The Mastermind behind GPT-4 and the Future of AI | Ilya Sutskever.” <em>YouTube</em>, 15 Mar. 2023, www.youtube.com/watch?v=SjhIlw3Iffs&amp;list=PLpdlTIkm0-jJ4gJyeLvH1PJCEHp3NAYf4&amp;index=64. Accessed 18 May 2025.</p><p>Frank, Michael C. “Bridging the data gap between children and large language models.” <em>Trends in cognitive sciences</em> vol. 27,11 (2023): 990-992. doi:10.1016/j.tics.2023.08.007</p><p>Garrett, Caelan Reed, et al. "Integrated task and motion planning." <em>Annual review of control, robotics, and autonomous systems</em> 4.1 (2021): 265-293.APA</p><p>Goodhart, C.A.E. (1984). Problems of Monetary Management: The UK Experience. In: Monetary Theory and Practice. Palgrave, London. https://doi.org/10.1007/978-1-349-17295-5_4</p><p>Hooker, Sara. The hardware lottery. Commun. ACM 64, 12 (December 2021), 58–65. https://doi.org/10.1145/3467017</p><p>Huh, Minyoung, et al. "The Platonic Representation Hypothesis." <em>Forty-first International Conference on Machine Learning</em>. 2024.</p><p>Kaplan, Jared, et al. "Scaling laws for neural language models." <em>arXiv preprint arXiv:2001.08361</em> (2020).</p><p>Lake, Brenden M. et al. “Building Machines That Learn and Think like People.” <em>Behavioral and Brain Sciences</em> 40 (2017): e253. Web.</p><p>Li, Kenneth, et al. "Emergent world representations: Exploring a sequence model trained on a synthetic task." <em>ICLR</em> (2023).</p><p>Luiten, Jonathon, Georgios, Kopanas, Bastian, Leibe, Deva, Ramanan. "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis." <em>3DV</em>. 2024.</p><p>Mao, Jiayuan, Chuang, Gan, Pushmeet, Kohli, Joshua B., Tenenbaum, Jiajun, Wu. "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision." <em>International Conference on Learning Representations</em>. 2019.</p><p>Mitchell, Melanie. “LLMs and World Models, Part 1.” <em>Substack.com</em>, AI: A Guide for Thinking Humans, 13 Feb. 2025, aiguide.substack.com/p/llms-and-world-models-part-1. Accessed 18 May 2025.</p><p>Mu, Norman. “Norman Mu | the Myth of Data Inefficiency in Large Language Models.” <em>Normanmu.com</em>, 14 Feb. 2025, www.normanmu.com/2025/02/14/data-inefficiency-llms.html. Accessed 18 May 2025.</p><p>Newell, Allen, and Herbert A. Simon. “Computer Science as Empirical Inquiry: Symbols and Search.” <em>Communications of the ACM</em>, vol. 19, no. 3, 1 Mar. 1976, pp. 113–126, https://doi.org/10.1145/360018.360022.</p><p>Peng, Hao, et al. “When Does In-Context Learning Fall Short and Why? A Study on Specification-Heavy Tasks.” <em>ArXiv.org</em>, 2023, arxiv.org/abs/2311.08993.</p><p>Spiegel, Benjamin, et al. “Visual Theory of Mind Enables the Invention of Early Writing Systems.” <em>CogSci</em>, 2025, arxiv.org/abs/2502.01568.</p><p>Sutton, Richard S. <em>Introduction to Reinforcement Learning</em>. Cambridge, Mass, Mit Press, 04-98, 1998.</p><p>Vafa, Keyon, et al. "Evaluating the world model implicit in a generative model." <em>Advances in Neural Information Processing Systems</em> 37 (2024): 26941-26975.</p><p>Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). "Attention is All you Need". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). <em>31st Conference on Neural Information Processing Systems (NIPS)</em>. Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc. arXiv:1706.03762.</p><p>Winograd, Terry. “Thinking Machines: Can There Be? Are We?” <em>The Boundaries of Humanity: Humans, Animals, Machines</em>, edited by James Sheehan and Morton Sosna, Berkeley: University of California Press, 1991, pp. 198–223.</p><p>Wu, Shangda, et al. "Beyond language models: Byte models are digital world simulators." <em>arXiv preprint arXiv:2402.19155</em> (2024). </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Right to Repair Is Law in Washington State (374 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state</link>
            <guid>44181421</guid>
            <pubDate>Wed, 04 Jun 2025 15:00:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state">https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state</a>, See on <a href="https://news.ycombinator.com/item?id=44181421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p>Thanks in part <a href="https://www.eff.org/deeplinks/2025/05/washingtons-right-repair-bill-heads-governor">to your support</a>, the right to repair is now law in Washington.</p>
<p>Gov. Bob Ferguson signed two bills guaranteeing Washingtonians' right to access tools, parts, and information so they can fix personal electronics, appliances, and wheelchairs. This is the epitome of common-sense legislation. When you own something, you should have the final say about who fixes, adapts, or modifies it—and how.</p>
<p>When you own something, you should have the final say about who fixes, adapts, or modifies it—and how.</p>
<p>Advocates in Washington have worked for years to pass a strong right-to-repair law in the state. In addition to Washington’s <a href="https://pirg.org/washington/updates/washington-becomes-eighth-state-with-right-to-repair-law-on-the-books/">Public Interest Research Group</a>, the consumer electronics bill moved forward with a growing group of supporting organizations, including environmental advocates, consumer advocates, and manufacturers such as Google and Microsoft. Meanwhile, advocacy from groups including&nbsp; <a href="https://disabilityrightswa.org/">Disability Rights Washington</a>&nbsp;and the <a href="https://www.hereandnowproject.org/">Here and Now Project</a> made the case for the wheelchair's inclusion in the right-to-repair bill, bringing their personal stories to Olympia to show why this bill was so important.</p>
<p>And it’s not just states that recognize the need for people to be able to fix their own stuff.&nbsp; Earlier this month, U.S. Army Secretary Dan Driscoll <a href="https://media.defense.gov/2025/May/01/2003702281/-1/-1/1/ARMY-TRANSFORMATION-AND-ACQUISITION-REFORM.PDF">issued a memo</a> stating that the Army should “[identify] and propose contract modifications for right to repair provisions where intellectual property constraints limit the Army's ability to conduct maintenance and access the appropriate maintenance tools, software, and technical data – while preserving the intellectual capital of American industry.” The memo said that the Army should seek this in future procurement contracts and also to amend existing contracts to include the right to repair.</p>
<p>This is a bedrock of sound procurement with a long history in America. President Lincoln only bought rifles with standardized tooling to outfit the Union Army, for the obvious reason that it would be a little embarrassing for the Commander in Chief to have to pull his troops off the field because the Army’s sole supplier had decided not to ship this week’s delivery of ammo and parts. Somehow, the Department of Defense forgot this lesson over the ensuing centuries, so that today, billions of dollars in public money are spent on material and systems that the US military can only maintain by buying service from a “beltway bandit.”</p>
<p>This recognizes what millions of people have said repeatedly: limiting people’s ability to fix their own stuff stands in the way of needed repairs and maintenance. That’s true whether you’re a farmer with a broken tractor during harvest, a homeowner with a misbehaving washing machine or a cracked smartphone screen, a hospital med-tech trying to fix a ventilator, or a soldier struggling with a broken generator.</p>
<p>The right to repair is gaining serious momentum. All 50 states have now considered some form of right-to-repair legislation. Washington is the eighth state to pass one of these bills into law—let’s keep it up.</p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["AI Will Replace All the Jobs " Is Just Tech Execs Doing Marketing (176 pts)]]></title>
            <link>https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/</link>
            <guid>44181172</guid>
            <pubDate>Wed, 04 Jun 2025 14:38:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/">https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/</a>, See on <a href="https://news.ycombinator.com/item?id=44181172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
<p>Over the weekend, I went digging for evidence that AI can, will, or has replaced a large percent of jobs. It doesn’t exist. Worse than that, actually, there’s hundreds of years of evidence and sophisticated analyses from hundreds of sources showing the opposite is true: AI will almost certainly create more jobs than it displaces, just like thousands of remarkable technologies before it.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;68405d5f71052&quot;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="1024" height="576" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-1024x576.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-1024x576.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-300x169.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-768x432.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<p>I don’t want anyone to think I’m raining on this parade without first attempting to convince myself that the opposite was true, and that AI really would be the first technology in 120 years to displace a massive portion of the workforce. So, dry though it may be, let’s walk through the logic together.</p>



<p>The majority of statements that have received press (and there have been dozens in the last 5 years) center on the claim that AI will destroy 20-50% of the current need for human labor. I’ll attempt to address each of the most robust points inherent in those arguments, rather than trying to argue that any one innovation or upgrade to a model’s capability hasn’t done it yet (or won’t):</p>



<ul>
<li>If AI is going to make such a huge percentage of jobs redundant, there must be historical analogies–i.e. other technologies that massively upended labor markets. What are these and how have they affected jobs in the past?
<ul>
<li><a href="https://www.technologyreview.com/2024/01/27/1087041/technological-unemployment-elon-musk-jobs-ai/">MIT’s Technology Review</a> noted that this “fear of new tech taking jobs,” is far from new. The automation of farm work is the most notable and most labor-impacting example we have from history, rapidly unemploying a huge portion of human beings in the developing economies of the late 19th and 20th centuries. And yet, at the conclusion of this era (~1940s/50s), the conclusion was that “technological unemployment is a myth,” because “technology has created so many new industries” and has expanded the market by “lowering the cost of production to make a price within reach of large masses of purchasers.” In short, technological advances had created more jobs overall.</li>



<li>Last year, Quarterly Journal of Economics <a href="https://academic.oup.com/qje/advance-article/doi/10.1093/qje/qjae008/7630187">published</a> a groundbreaking study on how technological innovations have impacted labor forces across industries since 1980. MIT did a nice <a href="https://news.mit.edu/2024/does-technology-help-or-hurt-employment-0401">summarization</a>: “the number of studies that support the labour replacement effect is more than offset by the number of studies that support the labour-creating/reinstating and real income effects.”</li>



<li><a href="https://www.sciencedirect.com/science/article/pii/S0040162523004353">This 2023 paper</a> looked at 127 previous studies of technology supposedly replacing labor forces from the 18th century to the present, concluding that “the labor displacing effect of technology appears to be more than offset by compensating mechanisms that create or reinstate labor.”</li>



<li>The Economic Policy Institute <a href="https://www.epi.org/publication/ai-unbalanced-labor-markets/">did a deep dive</a> into what drives labor market demand and unemployment, concluding: “Productivity growth (which technology sometimes enables and other times drives) has not historically been associated with higher unemployment or higher inequality,” and that “Anxieties over widespread technology-driven unemployment lack an empirical base.”</li>



<li>Perhaps the closest analogy to AI is the personal computer revolution of the 1980s. Millions of jobs in communication, documentation, research, analysis, and engineering became obsolete within a decade, and yet, the <a href="https://www.mckinsey.com/featured-insights/future-of-work/what-can-history-teach-us-about-technology-and-jobs">McKinsey Global Institute concluded</a> in 2018 that “We tallied up all the jobs destroyed in the US since 1980 as a result of the rise of personal computing and the Internet, and it’s about 3.5 million,” but “When we add up all the jobs created, we find that over 19 million jobs have been created as a result of the personal computer and Internet. We see a net gain of 15.8 million jobs in the US over the last few decades. That’s about 10 percent of the civilian labor force today.”</li>
</ul>
</li>



<li>If AI is going to have these massive impacts but hasn’t yet, why not?
<ul>
<li>Folks who claim AI will destroy the labor market have claimed this radical change is “only a few years away,” “on the immediate horizon,” or “imminent,” for the last 5 years, yet we’re at historically low unemployment (yes, even accounting for <a href="https://www.fastcompany.com/91341084/functional-unemployment-what-it-means">underemployment</a> and the <a href="https://www.marketplace.org/story/2025/05/23/is-the-unemployment-rate-truly-capturing-whats-happening-in-the-labor-market">way the BLS counts employment</a>). The US labor market is within a single percentage point of its post-war unemployment low, measured in <a href="https://econofact.org/factbrief/did-us-unemployment-fall-to-the-lowest-rate-in-50-years-under-biden">1953 at 3.4%</a>.</li>
</ul>
</li>
</ul>


<div>
<figure><img decoding="async" width="1024" height="744" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1024x744.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1024x744.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-300x218.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-768x558.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1536x1116.png 1536w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-2048x1488.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<ul>
<li>If AI is killing jobs, it’s doing so at an imperceptibly slow rate; why could that be? Is it still too early? Did other technologies take a long time to show their impacts on labor markets?
<ul>
<li>The broad consensus from rational industry observers, analysts, economists, and even AI-hyped technologists is that the end of cheap money (i.e. higher US interest rates) <a href="https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025">has driven</a> <a href="https://www.theatlantic.com/economy/archive/2025/04/job-market-youth/682641/?gift=o6MjJQpusU9ebnFuymVdsJ1qwI70CnAkjDXBfrYqvHw&amp;utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share">most of</a> <a href="https://news.ycombinator.com/item?id=43612448">the</a> <a href="https://www.zdnet.com/article/is-ai-making-it-harder-for-new-college-grads-to-get-hired-in-tech/">lower-than-pre-pandemic-demand</a> for <a href="https://www.techtarget.com/whatis/feature/Tech-sector-layoffs-explained-What-you-need-to-know">entry-level talent</a> (just as it has in times of inflation-fighting interest hikes of the past).</li>



<li>Machine-learning, the technology underpinning AI, <a href="https://courses.cs.washington.edu/courses/cse490h1/19wi/exhibit/machine-learning-1.html">has been around for decades</a>, with <a href="https://www.dataversity.net/a-brief-history-of-machine-learning/">widespread adoption</a> in tech companies between 2006-2013. The current generative-AI era, based on the transformer architecture model, kicked off in 2017, with significant public examples and tech adoption from 2018-2020. Most of the current, press-driven AI hype cycle, however, skyrocketed in late 2021 with OpenAI’s release of GPT-3 (longtime readers here will recall that <a href="https://britneymuller.com/">Britney Muller</a> showed off techniques <a href="https://www.linkedin.com/posts/britneymuller_bert-101-state-of-the-art-nlp-model-explained-activity-6907013731067523072-OXsd/">extremely similar</a> to what’s now associated with modern LLMs <a href="https://www.youtube.com/watch?v=47KHP3k2RPo">back in July 2018</a>).</li>



<li>We’ve had <a href="https://courses.cs.washington.edu/courses/cse490h1/19wi/exhibit/machine-learning-1.html">15-20 years</a> of robust machine learning development and adoption, and another <a href="https://toloka.ai/blog/history-of-generative-ai/">5-10 years of broad LLM/generative AI adoption</a>, improvement, and usage, yet labor market fluctuation has been far more dependent on other factors: the Covid pandemic itself, the post-pandemic surge and decline in tech hiring, inflation-fighting tactics by government banks, and (most recently), a renewal of early-20th-century-style tariffs and trade wars. When controlling for these events.</li>



<li>The effects of previous technological advancements also took time, but the most salient examples (of farm equipment in the 1910-1920 era and the personal computer in the 1980s) showed millions of displaced workers <a href="https://faculty.econ.ucdavis.edu/faculty/alolmstead/Recent_Publications/Reshaping_the_Landscape.pdf">within 5 years</a>. AI’s slower changes bode poorly for the argument that it will have a larger impact than those events.</li>



<li>Even if one assumes that AI was the only contributor to labor market changes between 2021-2025, the change has been incredibly slight, *even* in the software engineering market where it supposedly has the greatest impact. There <a href="https://www.reddit.com/r/programming/comments/1di8pe9/us_employment_of_software_developers_is_in/">was a greater loss</a> (nearly 150%) in percentage of software engineering jobs between 2019-2021 than from 2021-2025.</li>



<li>I found it particularly revealing that one of the most commonly cited examples of AI killing labor needs in the software field is the death of StackOverflow, and yet, a <a href="https://www.infoworld.com/article/3993482/ai-didnt-kill-stack-overflow.html">robust analysis of that site’s usage from 2008-2020</a> shows that “What really happened is a parable of human community and experiments in self-governance gone bizarrely wrong.”</li>
</ul>
</li>



<li>However, it seems likely that the perception of AI and its adoption are slowing hiring in the software engineering market in the post-bubble-popping era (2024-25). This thoughtful analysis by <a href="https://substack.com/@pragmaticengineer">Gergely Orosz</a> concludes a <a href="https://newsletter.pragmaticengineer.com/p/software-engineering-job-openings">well-visualized, data-driven walkthrough</a> with: “LLMs are a leading cause of the fall in software developer job postings: there’s uncertainty at large companies about whether to hire as fast as previously, given the productivity hype around AI tooling, and businesses are opting to “wait and see” by slowing down recruitment, as a result.”</li>



<li>It strains credibility to look at the data, history, and analyses and conclude that AI will eventually kill 20-50% of all jobs, when its largest impact in the prior 5-20 years of adoption (depending on one’s starting point) is <a href="https://archive.ph/t8f9X">~10% variation in a job sector that employs ~1% of US workers</a>.</li>



<li>Assuming AI will have an effect similar to 20th Century farm equipment’s on agriculture, why will that labor force behave differently to their 20th Century counterparts (and either refuse to or be prevented from finding new jobs)?
<ul>
<li>This point is hard to find citations for, given that it’s a future-looking, theoretical assertion. We can, however, compare the impact of the tractor (and farm machinery more broadly) on the economy from 1910-1960.</li>



<li>Tractors and farm equipment resulted in the shutdown of a huge number of farms, and a decline in the number of people employed in farming, from ~33% to ~2% of the labor force (notably, even that massive upheaval was less significant than the <a href="https://www.google.com/search?q=ai+will+take+half+of+all+jobs&amp;tbm=nws">prognostications</a> by tech company leaders that AI will displace half of all jobs). Nothing like it has happened in the American economy since, and only the industrial revolution of the 18th/19th centuries can compete in scale of transformation.</li>



<li>A superb breakdown of farm machinery’s impact on a sector that employed more than a quarter of all Americans comes from <a href="https://faculty.econ.ucdavis.edu/faculty/alolmstead/Recent_Publications/Reshaping_the_Landscape.pdf">Olmstead and Rhode at UC Davis</a>:</li>
</ul>
</li>
</ul>


<div>
<figure><img decoding="async" width="657" height="161" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image.png" alt="The upshot is that by 1960 the tractor reduced annual labor use by at least 3.44 billion man-hours of field and chore labor from the level required using the horse power technology. (The 3.44 billion figure combines the USDA estimate of annual labor savings before 1944 and our lower-bound estimate of saving between 1944 and 1959.) This was the equivalent of approximately 1,720 thousand workers, which represented 24.3 percent of farm employment in 1960 and 27.3 percent of the decline since 1910." srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image.png 657w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-300x74.png 300w" sizes="(max-width: 657px) 100vw, 657px"></figure></div>


<ul>
<li>Is it possible that AI will do to broad sectors of the economy what mechanized farm equipment did to agriculture?
<ul>
<li>Rationally, it’s difficult to fathom generative AI having a greater economic and labor-force impact than the PC revolution of the 1980s. AI makes many tasks more efficient, but evidence that it can wholesale replace entire human functions in a tractor-like way is pure speculation that exists in imagination, not reality.</li>



<li>The core assertion by the “AI will replace 20-50% of all jobs” crowd seems to be that the past 20 years of machine learning and generative AI improvements are not indicative of what will happen in the future: a leap in capability that will enable company management to instruct an AI on a job function (“get us press,” or “optimize our marketing campaign,” or “record and audit our financials” ) and rely on machines to correctly determine what needs to be done, how to do it, and then complete all associated tasks with little to no human supervision, intervention, or additional labor.</li>



<li>It’s impossible to argue against the assertion that AI will do what’s described above, because it’s based not on objective data, but rather on subjective belief about a possible future. Fighting about what someone believes may come about in the future is generally non-productive, so I’ll avoid that to spare us all a lot of wasted time 😉</li>
</ul>
</li>
</ul>



<p>I’ll move on from the dry argument analysis and citation process and attempt to summarize (and opine on) what’s really going on here.</p>



<p>Leaders of AI companies, and some AI proponents, marketers, journalists, and even critics have found that when they make scary predictions about their field destroying the job market, press and media eat it up. This media coverage, because it’s scary and the AI hype cycle is in full swing, draws clicks. Those clicks lead to employees, managers, and leaders at other businesses being scared into learning and adopting AI in their businesses.</p>



<p>Incentive also exists for those who criticize AI, AI companies, or their ethics/models/practices: these folks also benefit directly from the attention they earn when they amplify the message of AI as a job destroying technology.</p>



<p>If you’re feeling like the “AI will take all our jobs” discussion is familiar, you’re in good company. Many others have pointed out the similarities to stories like:</p>


<div>
<figure><a href="https://www.jalopnik.com/elon-musk-tesla-self-driving-cars-anniversary-autopilot-1850432357/"><img loading="lazy" decoding="async" width="816" height="557" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1.png 816w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1-300x205.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1-768x524.png 768w" sizes="auto, (max-width: 816px) 100vw, 816px"></a></figure></div>


<p>Source: <a href="https://www.jalopnik.com/elon-musk-tesla-self-driving-cars-anniversary-autopilot-1850432357/">Jalopnik</a></p>


<div>
<figure><img loading="lazy" decoding="async" width="786" height="631" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2.png 786w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2-300x241.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2-768x617.png 768w" sizes="auto, (max-width: 786px) 100vw, 786px"></figure></div>


<p>Source: <a href="https://www.insidehook.com/culture/older-generations-kids-too-soft">InsideHook</a></p>


<div>
<figure><img loading="lazy" decoding="async" width="767" height="816" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3.png 767w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3-282x300.png 282w" sizes="auto, (max-width: 767px) 100vw, 767px"></figure></div>


<p>Source: <a href="https://www.honestjobs.com/post/nobody-wants-to-work-anymore-is-not-new-and-it-s-not-true">Honest Jobs</a></p>



<p>Mechanization really did take jobs from farm workers. Automation took jobs from manual laborers. The PC took jobs from clerical and communication workers. But, all of these resulted in greater productivity, employment, and more optionality for workers. It’s both anti-historic and anti-evidence that AI will somehow prove to be the exception.</p>



<p>Could AI, along with thousands of other impactful technological, political, social, demographic, and black-swan-event changes permanently alter the employment landscape in our lifetimes? Absolutely. In fact, one of my favorite stats from this overly-ambitious weekend of research was MIT’s estimation that <a href="https://www.nber.org/papers/w30389">60% of employment in 2018 was in types of jobs that didn’t exist before 1940</a>.</p>



<p>By the time I’m in my 80s, y’all better have destroyed more than half of all the existing jobs, and that’s just to keep up with the 20th Century’s pace of change. But, don’t expect AI to do it for you in the next decade; that’s just marketing.</p>



<p>p.s. If you’re looking for the TL;DR, <a href="https://bsky.app/profile/edzitron.com/post/3lqaxozwxfc2o">Ed Zitron on Bluesky</a> has got you:</p>


<div>
<figure><a href="https://bsky.app/profile/edzitron.com/post/3lqaxozwxfc2o" target="_blank" rel=" noreferrer noopener"><img loading="lazy" decoding="async" width="710" height="736" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4.png 710w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4-289x300.png 289w" sizes="auto, (max-width: 710px) 100vw, 710px"></a></figure></div>


<p>p.p.s. I agree there’s evidence that this fear-based marketing campaign has been successful enough to disrupt some hiring, especially for <a href="https://www.reddit.com/r/Futurology/comments/1l100p3/ai_is_breaking_entrylevel_jobs_that_gen_z_workers/">early-stage jobs in a few tech-heavy fields</a>. But squinting at the evidence, it’s &lt;0.1% of jobs (&lt;200,000 total) being affected, and even here, the unbalanced capital vs. labor market is <a href="https://www.epi.org/publication/ai-unbalanced-labor-markets/">a far more compelling explanation</a>.</p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Sky's the limit: AI automation on Mac (113 pts)]]></title>
            <link>https://taoofmac.com/space/blog/2025/06/03/2155</link>
            <guid>44179691</guid>
            <pubDate>Wed, 04 Jun 2025 11:50:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taoofmac.com/space/blog/2025/06/03/2155">https://taoofmac.com/space/blog/2025/06/03/2155</a>, See on <a href="https://news.ycombinator.com/item?id=44179691">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://taoofmac.com/space/blog/2025/06/03/2155">Jun 3<sup>rd</sup> 2025</a> · 4 min read
 · <small>
#ai 
#apple 
#automation 
#desktop 
#intelligence 
#macos 
#shortcuts 
#sky 
#wwdc 
</small>
</p><section id="main">
    <p>I’ve been sitting on this draft for a few days now, partly because I thought it would turn down the bitterness, and partly because I kept asking myself whether I should even write it. But I think it is worth getting out of my system, so here goes.</p>
<p>In case you’re not in the Mac community, <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is an app that brings <a href="https://taoofmac.com/space/ai" rel="next">AI</a> automation to the Mac that <a href="https://www.macstories.net/stories/sky-for-mac-preview/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Federico Viticci wrote about at length last week</a>, and that not only looks and feels exactly like what I would expect <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">Apple Intelligence</a> to be like, it also completely blows out of the water all the desktop automation tools that have sprung out of the <a href="https://taoofmac.com/space/ai/mcp" rel="next">MCP</a> hype.</p>
<p>I have several questions, some of which I have already sort of asked <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">back in March</a>, but which I think are worth reformulating.</p>
<p>The people who created <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> are the same people who created Workflow and worked on Shortcuts, so here’s my first question:</p>
<p><em>Why wasn’t Apple able to harness their expertise in the first place?</em></p>
<p>I mean, people have free will and all, and can choose to work wherever they want, but this makes my <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">earlier rant about their having neglected automation</a> feel like the first clue to a corporate culture murder scene.</p>
<p>Not having made it possible for them to thrive feels like vanilla corporate politics, but having brilliant people <em>leave</em> Apple and ship something that is, even in preview, <em>much better than anything that Apple Intelligence promised</em> (including the made up bits they paraded as marketing material) is just gross mismanagement (now you know why I held back on this draft).</p>
<p>Which leads me to my second question:</p>
<p><em>Why has Apple failed this badly?</em></p>
<p>Was it just a consequence of their innately siloed nature? The internal decline of John Giannandrea’s team (and the <a href="https://daringfireball.net/linked/2025/03/20/gurman-rockwell-siri?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">rumored hand-over of Siri to Craig Federighi’s team</a>) might have played a role, but <a href="https://taoofmac.com/space/com/apple/macos" rel="next">macOS</a> has been largely stagnant from a UX perspective for ages (and as far as I know it isn’t even being addressed in the upcoming Solarium redesign), so I have to assume the <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> team saw this <em>huge</em> blind spot in terms of improving the desktop experience and just jumped on it.</p>
<p><em>Was it about control? Privacy?</em></p>
<p>I can see Apple balking at doing something like <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> (if they ever even considered it) because it not only has to share bits of your screen with an LLM, but also because it would have to open up the Mac to third-party automation in a way that it has never done before, and that would be a huge departure from their current approach.</p>
<p>Which, <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">again</a>, is pretty much non-existent, so… No, that doesn’t make sense.</p>
<p>But the privacy angle is interesting, because Apple was in a <em>perfect</em> position to do something exactly like <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> and ensure that it was done in a way that respected user privacy. Even though local models are still not quite there yet (remember that RAM requirements are still very high as far as running truly useful models are concerned), they do have the confidential computing tech to run inference in a privacy-preserving way–which might be the only bit of Apple Intelligence that actually works at this point.</p>
<p>But <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a>, despite having cloud inference, is designed to enhance your <em>local</em> Mac experience, and it does so in a way that looks extremely polished, and, above all, <em>feels like the way people always wanted to use computers</em>. Star Trek echoes aside, it has the ability to understand what you want to do, and <em>automates your Mac</em> to achieve that.</p>
<p>Federico’s post also goes into part of the <em>how</em> it does this, and I get the impression that even though <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> can leverage the remnants of Mac automation, for gathering context it is completely bypassing the standard automation APIs and inferring UI structure and content.</p>
<p>Everything I read about it makes me think that Apple has dropped the ball so badly that <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is like a perfect storm of what they could have done, but didn’t.</p>
<p>And now, not only is it a third-party app that is doing what Apple should have done, but it is also doing it in <em>a better way that anything they ever shipped</em>.</p>
<p>And if <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> takes off (let’s face it, the Mac desktop market isn’t really mainstream these days and there is too much AI hype, but just entertain the notion for a bit), that will have the added bonus of highlighting that Apple are completely out of touch with what people want from their computers.</p>
<p>Which leads me to my final question:</p>
<p><em>What will it take for Apple to get its act together?</em></p>
<p>I honestly don’t know. I mean, I have been asking this question for years now, and I have no idea what it will take for Apple to take any sort of integration or automation seriously. I currently have <em>zero</em> expectations towards next week’s WWDC, and not only because of the Mac. We were fooled once, and I don’t think we will be fooled again.</p>
<p>Unless they actually <em>ship</em> something, which seems highly unlikely unless it slots into their yearly release cycle (which they are rumored to be rebranding as “OS 26” because, well, why not ship the org chart <em>and</em> their corporate calendar?).</p>
<p>A case in point (and stop me if you’ve heard this before): Spotlight has been a complete mess for years, and Apple has done <em>nothing</em> to effectively fix it on any of its platforms–and it would be a <em>perfect</em> place to start integrating <a href="https://taoofmac.com/space/ai" rel="next">AI</a> in a way that would actually make sense and be useful to users.</p>
<p>Not to mention that it would be a key component of any sort of retrieval-augmented generation approach, etc.</p>
<p>So yes, <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is the limit. Or, at least, one very concrete yardstick by which we can measure how much Apple has failed to deliver on the promise of AI.</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just how bad are we at treating age-related diseases? (101 pts)]]></title>
            <link>https://www.ladanuzhna.xyz/writing/just-how-bad-are-we-at-treating-age-related-diseases</link>
            <guid>44179329</guid>
            <pubDate>Wed, 04 Jun 2025 10:51:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ladanuzhna.xyz/writing/just-how-bad-are-we-at-treating-age-related-diseases">https://www.ladanuzhna.xyz/writing/just-how-bad-are-we-at-treating-age-related-diseases</a>, See on <a href="https://news.ycombinator.com/item?id=44179329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-871d4e7f6dd1de2654fa">
  <p><em>This post was last updated in September 2024. I can’t promise this will be up-to-date when you come across it.</em></p><p>Whether you believe in preventing damage (the core aim of the aging field - though that's not the focus of this post) or treating damage as it arises, our current best efforts in both treatment and mechanism selection haven’t yielded promising results. None of the approved drugs for age-related diseases reverse any damage. They don't even halt disease progression. The primary endpoints mainly assess whether the treatment causes a slightly slower rate of decline than would occur otherwise. In some cases, approved drugs show no objective functional benefit at all.  So how did we get here?</p><h3>Geographic atrophy (GA)</h3><p>GA is a gradual loss of central vision due to the breakdown of the retinal cells responsible for detailed sight – retinal pigment epithelium and photoreceptors.&nbsp;</p><p>TLDR:</p><ul data-rte-list="default"><li><p>2 approved drugs, Syfovre&nbsp; (pegcetacoplan) and Izervay (ACP) - both approved in 2023</p></li><li><p>Neither drug halts disease progression. Atrophy progresses at a slightly slower rate (~19% less lesions after 2 years of monthly Syfovre eye injections)</p></li><li><p>Importantly, recent GA trials stopped relying on eyesight (measured through visual acuity score BCVA) as the primary endpoint. After multiple drugs failed to improve vision, the field switched to “lesion growth” as a trial readout</p></li><li><p>As a result, both Syfovre&nbsp; (pegcetacoplan) and Izervay (ACP) treated patients had a decline in vision similar to untreated patients - that is, while results of the trial were statistically significant, you or your family wouldn’t be able to tell as to whether you took the drug</p></li></ul><h3>Idiopathic Pulmonary Fibrosis (IPF)</h3><p>IPF is a disease where the lungs become scarred over time for no clear reason. This scarring makes it harder for your lungs to take in oxygen, so people with IPF often feel short of breath or have a dry cough.&nbsp;</p><p>TLDR:</p><ul data-rte-list="default"><li><p>2 approved drugs, nintedanib, pirfenidone - both approved in 2014</p></li><li><p>The primary endpoint for the trial is forced vital capacity (FVC) – the total amount of air a person can forcibly exhale from their lungs</p></li><li><p>Neither drug improves prognosis or FVC, but rather slightly decreases its decline</p></li><li><p>It's a usual practice to run 2 large Phase 3 trials if the drug is intended for approval. In case of pirfenidone, one of Phase 3 trials (STUDY 006 of CAPACITY Trial) showed no improvement in the primary endpoint relative to placebo. The drug was approved nevertheless.</p></li></ul><h3>MASH</h3><p>MASH is a liver disease that happens when too much fat builds up in the liver, causing hepatocyte damage and acute inflammation. It usually starts with just extra fat in the liver, but over time, leads to fibrosis (scarring), and eventually liver cancer.&nbsp;</p><p>TLDR:</p><ul data-rte-list="default"><li><p>Resmetirom was approved in 2024 and it is the first and only drug approved for MASH</p></li><li><p>MASH drugs are tested through 2 metrics: MASH resolution with no worsening of fibrosis (a scoring metric that focuses on metabolic and inflammatory aspects of the disease) and Fibrosis improvement with no worsening of metabolic aspects of the disease (4 stages of fibrosis: F1-F4)</p></li><li><p>In the high-dose group of resmetirom, MASH resolution was achieved in 29.9% – vs 9.7% resolution in placebo</p></li><li><p>Worth noting that resmetirom and other MASH drugs are primarily focused on earlier stages of liver disease - F1-F3 - and very few drugs are being tested for late-stage fibrotic MASH (F4)</p></li></ul><h3>Alzheimer's</h3><p>Alzheimer's is a neurodegenerative disease, with causes being the source of <a href="https://www.science.org/content/article/potential-fabrication-research-images-threatens-key-theory-alzheimers-disease" target="_blank">many disputes and investigations in the scientific community</a>. It is characterized by cognitive decline, memory loss, and eventually, the inability to perform basic functions.</p><p>TLDR:</p><ul data-rte-list="default"><li><p>The primary endpoint for AD trials is CDR (Clinical Dementia Rating) which measures orientation, memory, problem-solving, interactions with the community, and home life</p></li><li><p>Back in 2021, the FDA approved Aducanumab, targeting beta-amyloid, <a href="https://www.nature.com/articles/d41586-021-01763-9"><span>which was later taken from the market because many believed it not to be efficacious</span></a></p></li><li><p>In 2023, the FDA approved lecanemab (Leqembi), with a mechanism of action identical to Aducanumab. The European Medicines Agency ruled against approving the drug.</p></li><ul data-rte-list="default"><li><p>In the lecanemab trial, both the treatment group and the placebo group declined&nbsp; in Clinical Dementia Rating -&nbsp; all showing progressive cognitive impairment, amyloid deposition by PET scans or in their cerebrospinal fluid. The treatment group declined less, with the result being “statistically significant” - and <a href="https://www.nytimes.com/2022/11/29/health/lecanemab-alzheimers-drug.html"><span>many clinicians pointed out that statistically significant is not the same as clinically meaningful</span></a></p></li></ul><li><p>Donanemab (Kisunla) is yet another anti-amyloid antibody approved recently, which had a slightly different trial design by stratifying patients based on tau levels (another protein accumulated in AD) – with low-tau patients&nbsp;showing slightly better response to treatment</p></li><ul data-rte-list="default"><li><p>Unfortunately, treated patients also showed cerebral edema (ARIA-E), 2.1% of the placebo group as opposed to 24% of the treatment group; and brain microhemorrhages (ARIA-H), 13.6% in placebo and 31.4% in the treatment</p></li></ul><li><p>Many of the AD have a pretty bad side-effect profile, including <a href="https://www.neurology.org/doi/10.1212/WNL.0000000000207156"><span>cerebral shrinkage</span></a> (a shared feature of beta-amyloid drugs)</p></li></ul><h3>Data</h3><h4>Geographic Atrophy</h4>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I Wrote the BEAM Book (396 pts)]]></title>
            <link>https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/</link>
            <guid>44179257</guid>
            <pubDate>Wed, 04 Jun 2025 10:36:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/">https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/</a>, See on <a href="https://news.ycombinator.com/item?id=44179257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<h3> Post-mortems, coffee, and a decade of stubborn curiosity </h3><p>

	Posted:  2025-06-03</p>

	<h2>Why I wrote the Beam Book</h2>
<p>After ten years of keeping Klarna’s core system upright I know this: a 15
millisecond pause in the BEAM can stall millions of peak-shopping payments, trigger a 3 a.m. Christmas-Eve post-mortem, and earn you a very awake call from the CEO. I wrote <em>The BEAM Book</em> so the next engineer fixes that pause before the coffee cools.</p>
<p><img src="https://happihacking.com/images/thebeambooks.jpg" alt="A picture of two printed BEAM Books."></p><h3>Origins</h3>
<p>I opened the project on 12 October 2012 with a lone DocBook file with four lines of text and an oversized sense of optimism.
After two weeks, the commit log is mostly me adding structure, moving
headings, and updating metadata. Most of it is scaffolding. The actual
content is still just a few hopeful lines.</p>
<p>By November I had abandoned DocBook for AsciiDoc, written a custom build
script, and convinced myself the book could be wrapped up in six months.
Those early commits glow with energy: adds, rewrites, then more
rewrites to fix the rewrites.
Delusion is underrated.</p>
<p>In 2013 I managed to convince O’Reilly to publish. Moving the repo to their
Atlas system sounded simple until Atlas began hiding my main file and
overwriting half-finished chapters.</p>
<p>The Git history reads like a diary of frustration:
“Moving files to top level to cope with Atlas,” “Atlas seems to be
overwriting book.asciidoc”. Word count shot past 120 000 while actual
progress crawled. On 10 March 2015 I was literally “Smashing chapters into sections” just to keep the build green.</p>
<p>The quiet cancellation came two months later. No drama, just a polite call and a line through the contract. Relief mingled with embarrassment, I had spent two years rearranging files rather than finishing sentences.</p>
<p>Pragmatic Bookshelf took over that same year. I kept working in CVS for
their production system, but progress was slow. Eventually, they cancelled
too. On 20 January 2017, I imported everything into a new repo in one
massive commit: 6,622 files, over a million lines.
The rewrite stalled, and so did the project.</p>
<p>On 23 March 2017 I started fresh with Asciidoctor in a private GitHub repo, copy-pasting
only the parts that still made sense. Two weeks later, on April 7, minutes before
a lecture at Chalmers, I flipped the repository public. Within twenty-four
hours strangers fixed typos, added diagrams, and merged a Creative Commons
BY-4.0 license.</p>
<h3>What Kept Me Going</h3>
<p><img src="https://happihacking.com/images/star-history.svg" alt="A picture of the stars on GitHub passing 3000."></p><p>I kept going because I wanted to understand the BEAM properly. There’s
value in following the real logic, not just the surface explanations.</p>
<p>Community feedback made a difference. As soon as the repo was public,
people began sending corrections, examples, and improvements.</p>
<p>Seeing the numbers of people starring the repo on GitHub kept me going.
One highlight: <strong>Issue #113 – “Please continue being awesome.”</strong>
That emoji-laced drive-by encouragement (August 2018) still pops into my
head whenever motivation dips.</p>
<p><img src="https://happihacking.com/images/issue113.png" alt="Issue 113: This book
is ridiculously good. I have only read a few bits of it so far and have
learned a lot already. Please continue being awesome!"></p>
<p>The book started showing up as a reference in Erlang and BEAM conference
talks, sometimes several times in the same event. That was a clear signal
that others needed this as much as I did.</p>
<p>Even Twitter (in the good old days of Twitter) played a role. Whenever
someone mentioned the book or shared a
link, it was an extra nudge to keep at it.</p>
<p>Mostly, I just wanted a manual I could trust myself, a reference for the
parts of the VM that matter when things go wrong. That’s reason enough to
keep writing, even after the third rewrite.</p>
<h3>What’s Inside the Book &amp; Who It Helps</h3>
<p>The book covers what I wish I’d had when building and operating large
Erlang systems:</p>
<ul>
<li>Schedulers and process management: How the BEAM schedules,
prioritizes, and balances processes under real load.</li>
<li>Processes and their memory: How process heaps,
stack, messages, and binaries are managed and
why these details matter in production.</li>
<li>Garbage collection and memory: What actually happens
with per-process and global garbage collectors, binary references,
and memory leaks.</li>
<li>Tagging schemes and terms: How the BEAM represents data—integers,
floats, tuples, binaries, references—down to the tagging bits.</li>
<li>The compiler and the VM: How code is turned into instructions,
what the compiler does (and doesn’t do), and how the emulator executes it.</li>
<li>Tracing and debugging: Practical use of dbg, erlang:trace,
and other tools to follow messages, events, and identify bottlenecks.</li>
<li>Performance tuning: What matters when profiling real code,
understanding reductions, and tracking down real-world latency problems.</li>
<li>System architecture: How ERTS, the BEAM VM, and their subsystems
actually work together in a running node.</li>
</ul>
<p>If you build or operate Erlang or Elixir systems, especially under any kind
of scale—this book is for you. It saves you from hunting through mailing
lists, scattered docs, and code comments just to answer, “Why is the VM
behaving like this?”</p>
<h3>Lessons Learned</h3>
<p>Persistence beats perfection. Two cancelled publishing deals look bad on a
résumé, but an unfinished idea looks worse.</p>
<p>Boundaries matter. I made progress by blocking time for writing, turning
off notifications, and treating focus like a real deadline. Fika at 14:30
is non-negotiable.</p>
<p>The crowd helps. Making the repo public brought in corrections,
encouragement, and the occasional nudge when motivation was low.</p>
<p>Scope is everything. I cut the details on dirty schedulers, the new JIT,
and the debugger. Maybe those will end up in an appendix, but not in the
core.</p>
<p>Ship, then iterate. The BEAM changes every year. A living Git repo keeps
up.</p>
<p>A real deadline helps. This January, during my yearly review, I
decided to print the book in time for Code Beam Stockholm. I thought I had
until autumn, turns out the conference was June 2. That’s how you find out
what’s truly essential.</p>
<h3>Definition of Done</h3>
<p>Holding the print in my hands, it finally feels finished, at least for now. Years of scattered commits are bound into something real, so I’m calling it done.</p>
<h3>Get Involved</h3>
<p>You can now get the paperback—The BEAM Book 1.0 is live on Amazon. Buy it
here.&nbsp;<a href="https://www.amazon.com/dp/9153142535">Amazon</a></p>
<p>If you spot an error, want to improve something, or just want to see how it
works under the hood, star or fork the repo. File an issue or, even better,
submit a pull request. Contributors are credited in the acknowledgments.
<a href="https://github.com/happi/theBeamBook">GitHub: theBeamBook</a></p>
<p>If you read the book, please leave an honest review.
Algorithms notice real feedback more than marketing copy.</p>
<p>If your team wants a deep dive, I run hands-on BEAM internals
workshops, tailored for real systems, not just hello world.
Email me if that’s what you need.
<a href="mailto:happi@happihacking.com">happi@happihacking.com</a></p>


	<p>
	  - Happi
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cockatoos have learned to operate drinking fountains in Australia (282 pts)]]></title>
            <link>https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia</link>
            <guid>44178902</guid>
            <pubDate>Wed, 04 Jun 2025 09:42:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia">https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia</a>, See on <a href="https://news.ycombinator.com/item?id=44178902">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>