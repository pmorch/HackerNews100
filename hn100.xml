(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 08 Dec 2025 18:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Microsoft has a problem: nobody wants to buy or use its shoddy AI products (194 pts)]]></title>
            <link>https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai</link>
            <guid>46194615</guid>
            <pubDate>Mon, 08 Dec 2025 16:54:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai">https://www.windowscentral.com/artificial-intelligence/microsoft-has-a-problem-nobody-wants-to-buy-or-use-its-shoddy-ai</a>, See on <a href="https://news.ycombinator.com/item?id=46194615">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8.jpg" alt="Microsoft Chief Executicve (CEO) Satya Nadella takes part in the Partnership for Global Infrastructure and Investment Event during the G7 Summit at the Borgo Egnazia resort in Savelletri, Italy, on June 13, 2024." srcset="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/omkaTWDEPcQfgNNjFhyon8.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>Satya Nadella is burning decades of customer good will chasing the latest tech fad.</span>
<span>(Image credit: Getty Images | MANDEL NGAN)</span>
</figcaption>
</div>
<div id="article-body">
<p id="1e8f441c-4bde-4884-b8c2-b13351aaf279">If there's one thing that typifies Microsoft under CEO <a data-analytics-id="inline-link" href="https://www.windowscentral.com/tag/satya-nadella" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/tag/satya-nadella">Satya Nadella</a>'s tenure: it's a general inability to connect with customers.</p><p id="1e8f441c-4bde-4884-b8c2-b13351aaf279-2">A recent report from The Information <a data-analytics-id="inline-link" href="https://futurism.com/artificial-intelligence/microsoft-sell-ai-agents-disaster" data-url="https://futurism.com/artificial-intelligence/microsoft-sell-ai-agents-disaster" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">detailed</a> how Microsoft's internal AI efforts are going awry, with cut forecasts and sales goals for its Azure AI products across the board. The Information said that Microsoft's sales people are "struggling" to meet goals, owing to a complete lack of demand. Microsoft denied the reports, but it can't deny market share growth trends — all of which point to Google Gemini surging ahead.</p><p>Last week we wrote about how <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/microsofts-advantages-in-artificial-intelligence-evaporate-google-gemini-surges-ahead-and-openai-declares-code-red-situation" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/microsofts-advantages-in-artificial-intelligence-evaporate-google-gemini-surges-ahead-and-openai-declares-code-red-situation">Microsoft Copilot's backend partner OpenAI issued a "code red" situation</a>. <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/chatgpt" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt">ChatGPT</a> has fallen behind Google Gemini in problem solving, and Nano Banana image generation has outpaced <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/openai" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt">OpenAI</a>'s own DALLE by leaps and bounds.</p><p>With OpenAI's business model under constant scrutiny and racking up genuinely dangerous levels of debt, it's become a cascading problem for Microsoft to have tied up layer upon layer of its business in what might end up being something of a lame duck.</p><div id="slice-container-table-tyDfwpUzK9bsP5cDNpttgK-JCD6FwCDIVvwgcT8AXZb69Aits1NWxnM"><div><p>Swipe to scroll horizontally</p><svg viewBox="0 0 23 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M21.554 15.726a2.878 2.878 0 0 0-1.705-.374 2.881 2.881 0 0 0-1.388-3.068 2.877 2.877 0 0 0-1.992-.333 2.884 2.884 0 0 0-.1-.766 2.865 2.865 0 0 0-1.346-1.75c-.47-.27-.996-.4-1.527-.385l2.742-4.73a2.87 2.87 0 0 0 .323-.83h2.612V2.084h-2.661A2.861 2.861 0 0 0 15.18.385a2.903 2.903 0 0 0-3.952 1.055l-.373.644H2.983l1.003-1L2.99.09 1.28 1.793l-.999.995L2.99 5.484l.998-.994-1.003-.999h7.054L6.505 9.586c-.34.066-.905.186-1.523.366-1.405.41-2.321.895-2.8 1.483-.742.911-1.159 2.513-1.277 4.898l-.001.01c-.067 1.816.946 6.943.99 7.16a.688.688 0 0 0 1.35-.266c-.01-.051-1.023-5.177-.963-6.84.127-2.556.598-3.64.97-4.098.133-.163.602-.587 2.104-1.027l.206-.058-1.425 2.458a.685.685 0 0 0 .252.937c.33.19.75.077.94-.251L12.42 2.126a1.52 1.52 0 0 1 2.07-.552c.35.2.6.527.705.916.105.39.051.797-.15 1.145l-4.767 8.222a.685.685 0 0 0 .252.937c.33.19.75.077.94-.25l.794-1.368c.201-.348.529-.597.92-.702a1.508 1.508 0 0 1 1.854 1.066c.105.39.052.796-.15 1.144l-.377.652-.002.002-.898 1.55a.685.685 0 0 0 .252.938c.329.189.75.077.94-.251l.9-1.551c.201-.348.528-.597.92-.702a1.512 1.512 0 0 1 1.703 2.21l-1.223 2.11a.685.685 0 0 0 .252.938c.33.189.75.076.941-.252l.5-.862c.202-.348.529-.597.92-.702.392-.104.8-.051 1.15.15.723.416.972 1.34.554 2.06l-3.525 6.08c-.517.892-1.57 1.795-3.044 2.611-1.156.64-2.163.998-2.173 1.002a.685.685 0 0 0 .23 1.333.688.688 0 0 0 .229-.04c.18-.062 4.419-1.575 5.952-4.22l3.524-6.08a2.878 2.878 0 0 0-1.059-3.934Z" fill="#333"></path></svg></div><div><table tabindex="0"><caption>FirstPageSage AI Chatbot Usage Chart (December 3, 2025)</caption><thead><tr><th colspan="1"><p>#</p></th><th colspan="1"><p>Generative AI Chatbot</p></th><th colspan="1"><p>AI Search Market Share</p></th><th colspan="1"><p>Estimated Quarterly User Growth</p></th></tr></thead><tbody><tr><td colspan="1"><p>1</p></td><td colspan="1"><p>ChatGPT (excluding Copilot)</p></td><td colspan="1"><p>61.30%</p></td><td colspan="1"><p>7% ▲</p></td></tr><tr><td colspan="1"><p>2</p></td><td colspan="1"><p>Microsoft Copilot</p></td><td colspan="1"><p>14.10%</p></td><td colspan="1"><p>2% ▲</p></td></tr><tr><td colspan="1"><p>3</p></td><td colspan="1"><p>Google Gemini</p></td><td colspan="1"><p>13.40%</p></td><td colspan="1"><p>12% ▲</p></td></tr><tr><td colspan="1"><p>4</p></td><td colspan="1"><p>Perplexity</p></td><td colspan="1"><p>6.40%</p></td><td colspan="1"><p>4% ▲</p></td></tr><tr><td colspan="1"><p>5</p></td><td colspan="1"><p>Claude AI</p></td><td colspan="1"><p>3.80%</p></td><td colspan="1"><p>14% ▲</p></td></tr><tr><td colspan="1"><p>6</p></td><td colspan="1"><p>Grok</p></td><td colspan="1"><p>0.60%</p></td><td colspan="1"><p>6% ▲</p></td></tr><tr><td colspan="1"><p>7</p></td><td colspan="1"><p>Deepseek</p></td><td colspan="1"><p>0.20%</p></td><td colspan="1"><p>10% ▲</p></td></tr></tbody></table></div></div><p id="56a99c4f-3541-4d6e-9abd-84496f630185">There are reams of <a data-analytics-id="inline-link" href="https://futurism.com/professors-company-ai-agents" data-url="https://futurism.com/professors-company-ai-agents" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">research</a> that suggest agentic AI tools require human intervention at a frequency ratio that makes them cost ineffective, but Microsoft seems unbothered that its tools are poorly conceived.</p><p>In any case, <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt/openai-is-racing-to-give-chatgpt-a-flashy-upgrade" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/openai-chatgpt/openai-is-racing-to-give-chatgpt-a-flashy-upgrade">OpenAI is supposedly going to launch future models of ChatGPT early</a> in attempts to combat the rise of Google Gemini. I suspect the issues are deeper for Microsoft, who have worked tirelessly under Satya Nadella to create doubt around its products.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-tyDfwpUzK9bsP5cDNpttgK"><section><p>All the latest news, reviews, and guides for Windows and Xbox diehards.</p></section></div><p>SEO and analytics firm FirstPageSage has <a data-analytics-id="inline-link" href="https://firstpagesage.com/reports/top-generative-ai-chatbots/" data-url="https://firstpagesage.com/reports/top-generative-ai-chatbots/" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link">released</a> its AI market share report for the start of December, and it shows Google Gemini actively poised to supplant <a data-analytics-id="inline-link" href="https://www.windowscentral.com/artificial-intelligence/microsoft-copilot" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/microsoft-copilot" data-before-rewrite-localise="https://www.windowscentral.com/artificial-intelligence/microsoft-copilot">Microsoft Copilot</a>. Based on reports that Google Gemini is now actively beating ChatGPT's best models, FirstPageSage has Google Gemini sprinting past Microsoft Copilot quarter over quarter, although ChatGPT itself will remain the front runner.</p><h2 id="google-s-ai-advantages-are-accumulating-as-microsoft-s-disadvantages-snowball-3">Google's AI advantages are accumulating, as Microsoft's disadvantages snowball</h2><figure data-bordeaux-image-check="" id="0b22eb90-2613-4868-bb5c-37f46ab7d8ed"><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD.jpg" alt="Cloud servers" srcset="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD-320-80.jpg 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/k77PNE27iUvJCfzCWXrsMD.jpg">
</picture></p></div><figcaption itemprop="caption description"><span>Microsoft's destiny under Satya Nadella seems to increasingly point towards being a server broker for NVIDIA, rather than tech leader and innovator. </span><span itemprop="copyrightHolder">(Image credit: Microsoft)</span></figcaption></figure><p id="1a5d5863-697f-4059-b8e8-07196f40b762">Whether it's Google's Tensor server tech or dominating position with Google Play-bound Android, Microsoft's lack of forethought and attention paid to their actual customers is starting to catch up with the firm. <a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/satya-nadella-calls-microsofts-size-a-massive-disadvantage-in-ai" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/satya-nadella-calls-microsofts-size-a-massive-disadvantage-in-ai">Nadella has sought to blame the company's unwieldy size</a> for the lack of innovation, but it reads like an excuse to me. It's all about priorities — and Nadella has chased shareholder sentiment over delivering for its customers or employees, and that short-termism is going to put Microsoft on the backfoot if AI actually does deliver another computing paradigm shift.</p><p>Microsoft depends almost entirely on pricy NVIDIA technology for its data centers, whereas Google is actively investing to own the entire stack. Microsoft has also worked incredibly hard to cram half-baked AI features into its products, whereas Google has arguably been a lot more thoughtful in its approach. Microsoft sprinted out of the gate like a bull in a China shop, and investors rewarded them for it — but fast forward to 2025, and Google's AI products simply work better, and are more in-tune with how people might actually use them.</p><p>I am someone who is actively using the AI features across Google Android and Microsoft Windows on a day to day basis, and the delta between the two companies is growing ever wider. Basic stuff like the photo editing features on Google Pixel phones are <em>lightyears </em>beyond the abysmal tools found in the Microsoft Photos app on Windows. Google Gemini in Google Apps is also far smarter and far more intuitive than Copilot on <a data-analytics-id="inline-link" href="https://www.windowscentral.com/tag/microsoft-365" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/tag/microsoft-365">Microsoft 365</a>, as someone actively using both across the two businesses I work in.</p><figure id="4c687d83-84a4-4a8c-8b93-53ce94f3717c"><blockquote><p>Microsoft's "ship it now fix it later" attitude risks giving its AI products an Internet Explorer-like reputation for poor quality.</p></blockquote></figure><p id="48567dfa-f0e2-4a60-8b61-235ee6f45a7d">Dare I say it, Gemini is actually helpful, and can usually execute tasks you might actually need in a day to day job. "Find me a meeting slot on this date to accommodate these timezones" — Gemini will actually do it. Copilot 365 doesn't even have the capability to schedule a calendar event with natural language in the Outlook mobile app, or even provide something as basic as clickable links in some cases. At least Xbox's Gaming Copilot has a beta tag to explain why it fails half of the time. It's truly absurd how half-baked a lot of these features are, and it's odd that Microsoft sought to ship them in this state. And <a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/windows-11/microsoft-ai-ceo-pushes-back-against-critics-after-recent-windows-ai-backlash-the-fact-that-people-are-unimpressed-is-mindblowing-to-me" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/windows-11/microsoft-ai-ceo-pushes-back-against-critics-after-recent-windows-ai-backlash-the-fact-that-people-are-unimpressed-is-mindblowing-to-me">Microsoft wants to make Windows 12 AI first</a>? <em>Please</em>.</p><p>Microsoft's "ship it now fix it later" attitude risks giving its AI products an Internet Explorer-like reputation for poor quality, sacrificing the future to more patient, thoughtful companies who spend a little more time polishing first. Microsoft's strategy for AI seems to revolve around offering cheaper, lower quality products at lower costs (<em><a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/microsoft-teams" data-auto-tag-linker="true" data-mrf-recirculation="inline-link" data-before-rewrite-redirect="https://www.windowscentral.com/tag/microsoft-teams" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/microsoft-teams">Microsoft Teams</a>, hi</em>), over more expensive higher-quality options its competitors are offering. Whether or not that strategy will work for artificial intelligence, which is exorbitantly expensive to run, remains to be seen.</p><p>Microsoft's savvy early investment in OpenAI gave it an incredibly strong position early on, but as we get deeper into the cycle, some cracks are starting to show. Many of Microsoft's AI products to date simply scream of a total lack of direction and utter chaos, but it's not all hopeless. Some of Microsoft's enterprise solutions for AI are seeing strong growth. <a data-analytics-id="inline-link" href="https://www.windowscentral.com/software-apps/over-15-million-developers-now-use-this-ai-coding-tool-from-microsoft" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/software-apps/over-15-million-developers-now-use-this-ai-coding-tool-from-microsoft">Github Copilot</a> has been something of a success story for Redmond, and Microsoft is exploring its own <a data-analytics-id="inline-link" href="https://www.windowscentral.com/microsoft/microsoft-enters-the-chip-game-with-its-own-arm-processors-for-ai-and-computing-workloads" data-mrf-recirculation="inline-link" data-before-rewrite-localise="https://www.windowscentral.com/microsoft/microsoft-enters-the-chip-game-with-its-own-arm-processors-for-ai-and-computing-workloads">Maia and Cobalt chips</a> and even language models, in attempts to decouple itself from NVIDIA and OpenAI respectively. But Satya Nadella's Microsoft has an uncanny knack for failing to deliver on promising initiatives like those.</p><p>Without a stronger emphasis on quality, Microsoft's future in AI could simply end up revolving around re-selling NVIDIA server tech and jacking up local electricity prices, rather than providing any real home-grown innovation in the space. Shareholders will be more than happy for Microsoft to simply be a server reseller, but it would be a ignoble legacy for what was previously one of tech's most innovative companies.</p><hr id="5a919395-5c1e-4929-941e-3a2f3f0b04b3"><a href="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" id="f5f9035e-e569-452e-9804-21314e4a6271" data-url="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none"><figure data-bordeaux-image-check=""><div><p> <picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 1200w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 1024w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png.webp 970w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-650-80.png.webp 650w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-480-80.png.webp 480w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-320-80.png.webp 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png" alt="Click to follow Windows Central on Google News" srcset="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 1200w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 1024w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-661-80.png 970w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-650-80.png 650w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-480-80.png 480w, https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX-320-80.png 320w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-new-v2-image="true" data-original-mos="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png" data-pin-media="https://cdn.mos.cms.futurecdn.net/L3AsfTCaaiH29SBi5ptnDX.png">
</picture></p></div></figure></a><p id="c04c3c18-271c-4279-8d9e-972f8d84ee71"><em>Follow </em><a data-analytics-id="inline-link" href="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" target="_blank" data-url="https://news.google.com/publications/CAAqLggKIihDQklTR0FnTWFoUUtFbmRwYm1SdmQzTmpaVzUwY21Gc0xtTnZiU2dBUAE" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none" data-mrf-recirculation="inline-link"><em>Windows Central on Google News</em></a><em> to keep our latest news, insights, and features at the top of your feeds!</em></p><hr id="876d2032-2e29-42b2-b13e-5aa4a2af9e89">
</div>



<div id="slice-container-authorBio-tyDfwpUzK9bsP5cDNpttgK"><p>Jez Corden is the Executive Editor at Windows Central, focusing primarily on all things Xbox and gaming. Jez is known for breaking exclusive news and analysis as relates to the Microsoft ecosystem while being powered by tea. Follow on <a href="http://www.twitter.com/jezcorden">Twitter (X)</a> and tune in to the <a href="https://anchor.fm/thexboxtwo">XB2 Podcast</a>, all about, you guessed it, Xbox!</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AMD GPU Debugger (117 pts)]]></title>
            <link>https://thegeeko.me/blog/amd-gpu-debugging/</link>
            <guid>46193931</guid>
            <pubDate>Mon, 08 Dec 2025 16:06:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thegeeko.me/blog/amd-gpu-debugging/">https://thegeeko.me/blog/amd-gpu-debugging/</a>, See on <a href="https://news.ycombinator.com/item?id=46193931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-gjtny2mx="">    <a href="https://thegeeko.me/" data-astro-cid-cjjlykpo=""> <svg width="16" height="16" viewBox="0 0 16 16" fill="none" xmlns="http://www.w3.org/2000/svg" data-astro-cid-cjjlykpo=""> <path d="M2.5 6.5H9.5C11.1569 6.5 12.5 7.84315 12.5 9.5V9.5C12.5 11.1569 11.1569 12.5 9.5 12.5H7.5M2.5 6.5L5.5 9.5M2.5 6.5L5.5 3.5" stroke="currentColor" stroke-width="1.25" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-cjjlykpo=""></path> </svg>
index
</a>   <div id="toc"> <nav> <ul id="toc-list"> <!-- Back to top link --> <li> <a href="#" title="Back to top" data-text="Back to top">
Back to top
</a> </li> <!-- TOC items --> <li> <a href="#tbatma" title="TBA/TMA" data-text="TBA/TMA"> TBA/TMA </a> </li><li> <a href="#amdgpu-debugfs" title="AMDGPU Debugfs" data-text="AMDGPU Debugfs"> AMDGPU Debugfs </a> </li><li> <a href="#the-trap-handler" title="The Trap Handler" data-text="The Trap Handler"> The Trap Handler </a> </li><li> <a href="#spir-v" title="SPIR-V" data-text="SPIR-V"> SPIR-V </a> </li><li> <a href="#an-actual-debugger" title="An Actual Debugger" data-text="An Actual Debugger"> An Actual Debugger </a> </li><li> <a href="#breakpoints-and-stepping" title="Breakpoints and Stepping" data-text="Breakpoints and Stepping"> Breakpoints and Stepping </a> </li><li> <a href="#source-code-line-mapping" title="Source Code Line Mapping" data-text="Source Code Line Mapping"> Source Code Line Mapping </a> </li><li> <a href="#address-watching-aka-watchpoints" title="Address Watching aka Watchpoints" data-text="Address Watching aka Watchpoints"> Address Watching aka Watchpoints </a> </li><li> <a href="#variables-types-and-names" title="Variables Types and Names" data-text="Variables Types and Names"> Variables Types and Names </a> </li><li> <a href="#vulkan-integration" title="Vulkan Integration" data-text="Vulkan Integration"> Vulkan Integration </a> </li><li> <a href="#bonus-round" title="Bonus Round" data-text="Bonus Round"> Bonus Round </a> </li> </ul> </nav> </div>     <p>I’ve always wondered why we don’t have a GPU debugger similar to the one used for CPUs. A tool that allows pausing execution and examining the current state. This capability feels essential, especially since the GPU’s concurrent execution model is much harder to reason about. After searching for solutions, I came across rocgdb, a debugger for AMD’s ROCm environment. Unfortunately, its scope is limited to that environment. Still, this shows it’s technically possible. I then found a helpful <a href="https://martty.github.io/posts/radbg_part_1/">series of blog posts</a> by <a href="https://martty.github.io/about/">Marcell Kiss</a>, detailing how he achieved this, which inspired me to try to recreate the process myself.</p>
<h2 id="lets-try-to-talk-to-the-gpu-directly">Let’s Try To Talk To The GPU Directly</h2>
<p>The best place to start learning about this is <a href="https://docs.mesa3d.org/drivers/radv.html">RADV</a>. By tracing what it does, we can find how to do it. Our goal here is to run the most basic shader <code>nop 0</code> without using Vulkan, aka RADV in our case.</p>
<p>First of all, we need to open the DRM file to establish a connection with the KMD, using a simple open(“/dev/dri/cardX”), then we find that it’s calling <code>amdgpu_device_initialize</code>, which is a function defined in <code>libdrm</code>, which is a library that acts as middleware between user mode drivers(UMD) like <code>RADV</code> and and kernel mode drivers(KMD) like amdgpu driver, and then when we try to do some actual work we have to create a context which can be achieved by calling <code>amdgpu_cs_ctx_create</code> from <code>libdrm</code> again, next up we need to allocate 2 buffers one of them for our code and the other for writing our commands into, we do this by calling a couple of functions, here’s how I do it:</p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> bo_alloc</span><span>(</span><span>amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> size_t</span><span> size</span><span>,</span><span> u32 domain</span><span>,</span><span> bool</span><span> uncached</span><span>,</span><span> amdgpubo_t</span><span>*</span><span> bo) {</span></span>
<span><span> s32    ret         </span><span>=</span><span> -</span><span>1</span><span>;</span></span>
<span><span> u32    alignment   </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> u32    flags       </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> size_t</span><span> actual_size </span><span>=</span><span> 0</span><span>;</span></span>
<span></span>
<span><span> amdgpu_bo_handle bo_handle </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span> amdgpu_va_handle va_handle </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span> u64              va_addr   </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> void*</span><span>            host_addr </span><span>=</span><span> NULL</span><span>;</span></span></code></pre>
<p>Here we’re choosing the domain and assigning flags based on the params, some buffers we will need uncached, as we will see:</p>
<pre tabindex="0" data-language="c"><code><span><span> if</span><span> (</span></span>
<span><span>   domain </span><span>!=</span><span> AMDGPU_GEM_DOMAIN_GWS </span><span>&amp;&amp;</span><span> domain </span><span>!=</span><span> AMDGPU_GEM_DOMAIN_GDS </span><span>&amp;&amp;</span></span>
<span><span>   domain </span><span>!=</span><span> AMDGPU_GEM_DOMAIN_OA) {</span></span>
<span><span>  actual_size </span><span>=</span><span> (size </span><span>+</span><span> 4096</span><span> -</span><span> 1</span><span>) </span><span>&amp;</span><span> 0x</span><span>FFFFFFFFFFFFF000</span><span>ULL</span><span>;</span></span>
<span><span>  alignment   </span><span>=</span><span> 4096</span><span>;</span></span>
<span><span>  flags       </span><span>=</span><span> AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED </span><span>|</span><span> AMDGPU_GEM_CREATE_VRAM_CLEARED </span><span>|</span></span>
<span><span>          AMDGPU_GEM_CREATE_VM_ALWAYS_VALID;</span></span>
<span><span>  flags </span><span>|=</span></span>
<span><span>    uncached </span><span>?</span><span> (domain </span><span>==</span><span> AMDGPU_GEM_DOMAIN_GTT) </span><span>*</span><span> AMDGPU_GEM_CREATE_CPU_GTT_USWC </span><span>:</span><span> 0</span><span>;</span></span>
<span><span> } </span><span>else</span><span> {</span></span>
<span><span>  actual_size </span><span>=</span><span> size;</span></span>
<span><span>  alignment   </span><span>=</span><span> 1</span><span>;</span></span>
<span><span>  flags       </span><span>=</span><span> AMDGPU_GEM_CREATE_NO_CPU_ACCESS;</span></span>
<span><span> }</span></span>
<span></span>
<span><span> struct</span><span> amdgpu_bo_alloc_request req </span><span>=</span><span> {</span></span>
<span><span>  .alloc_size     </span><span>=</span><span> actual_size</span><span>,</span></span>
<span><span>  .phys_alignment </span><span>=</span><span> alignment</span><span>,</span></span>
<span><span>  .preferred_heap </span><span>=</span><span> domain</span><span>,</span></span>
<span><span>  .flags          </span><span>=</span><span> flags</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> // memory aquired!!</span></span>
<span><span> ret </span><span>=</span><span> amdgpu_bo_alloc</span><span>(dev</span><span>-&gt;</span><span>dev_handle</span><span>,</span><span> &amp;</span><span>req</span><span>,</span><span> &amp;</span><span>bo_handle);</span></span>
<span><span> HDB_ASSERT</span><span>(</span><span>!</span><span>ret</span><span>,</span><span> "can't allocate bo"</span><span>);</span></span></code></pre>
<p>Now we have the memory, we need to map it. I opt to map anything that can be CPU-mapped for ease of use. We have to map the memory to both the GPU and the CPU virtual space. The KMD creates the page table when we open the DRM file, as shown <a href="https://elixir.bootlin.com/linux/v6.18/source/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c#L1425">here</a>.</p>
<p>So map it to the GPU VM and, if possible, to the CPU VM as well. Here, at this point, there’s a libdrm function that does all of this setup for us and maps the memory, but I found that even when specifying <code>AMDGPU_VM_MTYPE_UC</code>, it doesn’t always tag the page as uncached, not quite sure if it’s a
bug in my code or something in <code>libdrm</code> anyways, the function is <code>amdgpu_bo_va_op</code>, I opted to do it manually here and issue the IOCTL call myself:</p>
<pre tabindex="0" data-language="c"><code><span><span> u32 kms_handle </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> amdgpu_bo_export</span><span>(bo_handle</span><span>,</span><span> amdgpu_bo_handle_type_kms</span><span>,</span><span> &amp;</span><span>kms_handle);</span></span>
<span></span>
<span><span> ret </span><span>=</span><span> amdgpu_va_range_alloc</span><span>(</span></span>
<span><span>   dev</span><span>-&gt;</span><span>dev_handle</span><span>,</span></span>
<span><span>   amdgpu_gpu_va_range_general</span><span>,</span></span>
<span><span>   actual_size</span><span>,</span></span>
<span><span>   4096</span><span>,</span></span>
<span><span>   0</span><span>,</span></span>
<span><span>   &amp;</span><span>va_addr</span><span>,</span></span>
<span><span>   &amp;</span><span>va_handle</span><span>,</span></span>
<span><span>   0</span><span>);</span></span>
<span><span> HDB_ASSERT</span><span>(</span><span>!</span><span>ret</span><span>,</span><span> "can't allocate VA"</span><span>);</span></span>
<span></span>
<span><span> u64 map_flags </span><span>=</span></span>
<span><span>   AMDGPU_VM_PAGE_EXECUTABLE </span><span>|</span><span> AMDGPU_VM_PAGE_READABLE </span><span>|</span><span> AMDGPU_VM_PAGE_WRITEABLE;</span></span>
<span><span> map_flags </span><span>|=</span><span> uncached </span><span>?</span><span> AMDGPU_VM_MTYPE_UC </span><span>|</span><span> AMDGPU_VM_PAGE_NOALLOC </span><span>:</span><span> 0</span><span>;</span></span>
<span></span>
<span><span> struct</span><span> drm_amdgpu_gem_va va </span><span>=</span><span> {</span></span>
<span><span>  .handle       </span><span>=</span><span> kms_handle</span><span>,</span></span>
<span><span>  .operation    </span><span>=</span><span> AMDGPU_VA_OP_MAP</span><span>,</span></span>
<span><span>  .flags        </span><span>=</span><span> map_flags</span><span>,</span></span>
<span><span>  .va_address   </span><span>=</span><span> va_addr</span><span>,</span></span>
<span><span>  .offset_in_bo </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .map_size     </span><span>=</span><span> actual_size</span><span>,</span></span>
<span></span>
<span><span> };</span></span>
<span></span>
<span><span> ret </span><span>=</span><span> drm_ioctl_write_read</span><span>(dev</span><span>-&gt;</span><span>drm_fd</span><span>,</span><span> DRM_AMDGPU_GEM_VA</span><span>,</span><span> &amp;</span><span>va</span><span>,</span><span> sizeof</span><span>(va));</span></span>
<span><span> HDB_ASSERT</span><span>(</span><span>!</span><span>ret</span><span>,</span><span> "can't map bo in GPU space"</span><span>);</span></span>
<span><span> // ret = amdgpu_bo_va_op(bo_handle, 0, actual_size, va_addr, map_flags,</span></span>
<span><span> // AMDGPU_VA_OP_MAP);</span></span>
<span></span>
<span><span> if</span><span> (flags </span><span>&amp;</span><span> AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {</span></span>
<span><span>  ret </span><span>=</span><span> amdgpu_bo_cpu_map(bo_handle</span><span>,</span><span> &amp;</span><span>host_addr)</span><span>;</span></span>
<span><span>  HDB_ASSERT(</span><span>!</span><span>ret</span><span>,</span><span> "can't map bo in CPU space"</span><span>)</span><span>;</span></span>
<span></span>
<span><span>  // AMDGPU_GEM_CREATE_VRAM_CLEARED doesn't really memset the memory to 0 anyways for</span></span>
<span><span>  // debug I'll just do it manually for now</span></span>
<span><span>  memset(host_addr</span><span>,</span><span> 0x</span><span>0</span><span>,</span><span> actual_size)</span><span>;</span></span>
<span><span> }</span></span>
<span></span>
<span><span> *</span><span>bo </span><span>=</span><span> (</span><span>amdgpubo_t</span><span>){</span></span>
<span><span>  .bo_handle </span><span>=</span><span> bo_handle</span><span>,</span></span>
<span><span>  .va_handle </span><span>=</span><span> va_handle</span><span>,</span></span>
<span><span>  .va_addr   </span><span>=</span><span> va_addr</span><span>,</span></span>
<span><span>  .size      </span><span>=</span><span> actual_size</span><span>,</span></span>
<span><span>  .host_addr </span><span>=</span><span> host_addr</span><span>,</span></span>
<span><span> };</span></span>
<span><span>}</span></span></code></pre>
<p>Now we have the context and 2 buffers. Next, fill those buffers and send our commands to the KMD, which will then forward them to the Command Processor (CP) in the GPU for processing.</p>
<p>Let’s compile our code. We can use clang assembler for that, like this:</p>
<pre tabindex="0" data-language="c"><code><span><span># https:</span><span>//gitlab.freedesktop.org/martty/radbg-poc/-/blob/master/ll-as.sh</span></span>
<span><span>clang </span><span>-</span><span>c </span><span>-</span><span>x assembler </span><span>-</span><span>target amdgcn</span><span>-</span><span>amd</span><span>-</span><span>amdhsa </span><span>-</span><span>mcpu</span><span>=</span><span>gfx1100 </span><span>-</span><span>o </span><span>asm</span><span>.o </span><span>"$1"</span></span>
<span><span>objdump </span><span>-</span><span>h </span><span>asm</span><span>.o </span><span>|</span><span> grep .text </span><span>|</span><span> awk </span><span>'{print "dd if='</span><span>asm</span><span>.o</span><span>' of='</span><span>asmc.bin</span><span>' bs=1 count=$[0x" $3 "] skip=$[0x" $6 "] status=none"}'</span><span> |</span><span> bash</span></span>
<span><span>#rm </span><span>asm</span><span>.o</span></span></code></pre>
<p>The bash script compiles the code, and then we’re only interested in the actual machine code, so we use objdump to figure out the offset and the size of the section and copy it to a new file called asmc.bin, then we can just load the file and write its bytes to the CPU-mapped address of the code buffer.</p>
<p>Next up, filling in the commands. This was extremely confusing for me because it’s not well documented.
It was mostly learning how <code>RADV</code> does things and trying to do similar things. Also, shout-out to the folks on the Graphics Programming Discord server for helping me, especially Picoduck. The commands are encoded in a special format called <code>PM4 Packets</code>, which has multiple types. We only care about <code>Type 3</code>: each packet has an opcode and the number of bytes it contains.</p>
<p>The first thing we need to do is program the GPU registers, then dispatch the shader. Some of those registers are <code>rsrc[1-3]</code>; those registers are responsible for a number of configurations, pgm_[lo/hi], which hold the pointer to the code buffer and <code>num_thread_[x/y/z]</code>; those are responsible for the number of threads inside a work group. All of those are set using the <code>set shader register</code> packets, and here is how to encode them:</p>
<p><span>It’s worth mentioning that we can set multiple registers in 1 packet if they’re consecutive.</span></p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> pkt3_set_sh_reg</span><span>(</span><span>pkt3_packets_t</span><span>*</span><span> packets</span><span>,</span><span> u32 reg</span><span>,</span><span> u32 value) {</span></span>
<span><span> HDB_ASSERT(</span></span>
<span><span>   reg </span><span>&gt;=</span><span> SI_SH_REG_OFFSET </span><span>&amp;&amp;</span><span> reg </span><span>&lt;</span><span> SI_SH_REG_END</span><span>,</span></span>
<span><span>   "can't set register outside sh registers span"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> // packet header</span></span>
<span><span> da_append(packets</span><span>,</span><span> PKT3(PKT3_SET_SH_REG</span><span>,</span><span> 1</span><span>,</span><span> 0</span><span>))</span><span>;</span></span>
<span><span> // offset of the register</span></span>
<span><span> da_append(packets</span><span>,</span><span> (reg </span><span>-</span><span> SI_SH_REG_OFFSET) </span><span>/</span><span> 4</span><span>)</span><span>;</span></span>
<span><span> da_append(packets</span><span>,</span><span> value)</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>Then we append the dispatch command:</p>
<pre tabindex="0" data-language="c"><code><span><span>// we're going for 1 thread since we want the simplest case here.</span></span>
<span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> PKT3</span><span>(PKT3_DISPATCH_DIRECT</span><span>,</span><span> 3</span><span>,</span><span> 0</span><span>) </span><span>|</span><span> PKT3_SHADER_TYPE_S</span><span>(</span><span>1</span><span>));</span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> 1</span><span>u</span><span>);</span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> 1</span><span>u</span><span>);</span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> 1</span><span>u</span><span>);</span></span>
<span><span>da_append</span><span>(</span><span>&amp;</span><span>pkt3_packets</span><span>,</span><span> dispatch_initiator);</span></span></code></pre>
<p>Now we want to write those commands into our buffer and send them to the KMD:</p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> dev_submit</span><span>(</span></span>
<span><span>  amdgpu_t</span><span>*</span><span>         dev</span><span>,</span></span>
<span><span>  pkt3_packets_t</span><span>*</span><span>   packets</span><span>,</span></span>
<span><span>  amdgpu_bo_handle</span><span>*</span><span> buffers</span><span>,</span></span>
<span><span>  u32               buffers_count</span><span>,</span></span>
<span><span>  amdgpu_submit_t</span><span>*</span><span>  submit</span></span>
<span><span>) {</span></span>
<span><span> s32        ret </span><span>=</span><span> -</span><span>1</span><span>;</span></span>
<span><span> amdgpubo_t</span><span> ib  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span></span>
<span><span> bo_alloc(dev</span><span>,</span><span> pkt3_size(packets)</span><span>,</span><span> AMDGPU_GEM_DOMAIN_GTT</span><span>,</span><span> false</span><span>,</span><span> &amp;</span><span>ib)</span><span>;</span></span>
<span><span> bo_upload(</span><span>&amp;</span><span>ib</span><span>,</span><span> packets</span><span>-&gt;</span><span>data</span><span>,</span><span> pkt3_size(packets))</span><span>;</span></span>
<span></span>
<span><span> amdgpu_bo_handle</span><span>*</span><span> bo_handles </span><span>=</span><span> // +1 for the indirect buffer</span></span>
<span><span>   (amdgpu_bo_handle</span><span>*</span><span>)</span><span>malloc(</span><span>sizeof</span><span>(amdgpu_bo_handle) </span><span>*</span><span> (buffers_count </span><span>+</span><span> 1</span><span>))</span><span>;</span></span>
<span></span>
<span><span> bo_handles[</span><span>0</span><span>] </span><span>=</span><span> ib</span><span>.</span><span>bo_handle;</span></span>
<span><span> for_range(i</span><span>,</span><span> 0</span><span>,</span><span> buffers_count)</span><span> {</span></span>
<span><span>  bo_handles[i </span><span>+</span><span> 1</span><span>] </span><span>=</span><span> buffers[i];</span></span>
<span><span> }</span></span>
<span></span>
<span><span> amdgpu_bo_list_handle bo_list </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span> ret </span><span>=</span></span>
<span><span>   amdgpu_bo_list_create(</span><span>dev</span><span>-&gt;</span><span>dev_handle</span><span>,</span><span> buffers_count </span><span>+</span><span> 1</span><span>,</span><span> bo_handles</span><span>,</span><span> NULL</span><span>,</span><span> &amp;</span><span>bo_list)</span><span>;</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>ret</span><span>,</span><span> "can't create a bo list"</span><span>)</span><span>;</span></span>
<span><span> free(bo_handles)</span><span>;</span></span>
<span></span>
<span><span> struct</span><span> amdgpu_cs_ib_info ib_info </span><span>=</span><span> {</span></span>
<span><span>  .flags         </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .ib_mc_address </span><span>=</span><span> ib</span><span>.</span><span>va_addr</span><span>,</span></span>
<span><span>  .size          </span><span>=</span><span> packets</span><span>-&gt;</span><span>count</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> struct</span><span> amdgpu_cs_request req </span><span>=</span><span> {</span></span>
<span><span>  .flags                  </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .ip_type                </span><span>=</span><span> AMDGPU_HW_IP_COMPUTE</span><span>,</span></span>
<span><span>  .ip_instance            </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .ring                   </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .resources              </span><span>=</span><span> bo_list</span><span>,</span></span>
<span><span>  .number_of_dependencies </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .dependencies           </span><span>=</span><span> NULL</span><span>,</span></span>
<span><span>  .number_of_ibs          </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>  .ibs                    </span><span>=</span><span> &amp;</span><span>ib_info</span><span>,</span></span>
<span><span>  .seq_no                 </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>  .fence_info             </span><span>=</span><span> { </span><span>0</span><span> }</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> ret </span><span>=</span><span> amdgpu_cs_submit(</span><span>dev</span><span>-&gt;</span><span>ctx_handle</span><span>,</span><span> 0</span><span>,</span><span> &amp;</span><span>req</span><span>,</span><span> 1</span><span>)</span><span>;</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>ret</span><span>,</span><span> "can't submit indirect buffer request"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> *</span><span>submit </span><span>=</span><span> (</span><span>amdgpu_submit_t</span><span>){</span></span>
<span><span>    .ib </span><span>=</span><span> ib</span><span>,</span></span>
<span><span>    .bo_list </span><span>=</span><span> bo_list</span><span>,</span></span>
<span><span>    .fence </span><span>=</span><span> {</span></span>
<span><span>      .context </span><span>=</span><span> dev</span><span>-&gt;</span><span>ctx_handle</span><span>,</span></span>
<span><span>      .ip_type </span><span>=</span><span> AMDGPU_HW_IP_COMPUTE</span><span>,</span></span>
<span><span>      .ip_instance </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>      .ring </span><span>=</span><span> 0</span><span>,</span></span>
<span><span>      .fence </span><span>=</span><span> req</span><span>.</span><span>seq_no</span><span>,</span></span>
<span><span>    }</span><span>,</span></span>
<span><span>  };</span></span>
<span><span>}</span></span></code></pre>
<p><span>Here is a good point to make a more complex shader that outputs something. For example, writing 1 to a buffer.</span></p>
<p>No GPU hangs ?! nothing happened ?! cool, cool, now we have a shader that runs on the GPU, what’s next? Let’s try to hang the GPU by pausing the execution, aka make the GPU trap.</p>
<h2 id="tbatma">TBA/TMA</h2>
<p>The RDNA3’s ISA manual does mention 2 registers, <code>TBA, TMA</code>; here’s how they describe them respectively:</p>
<blockquote>
<p>Holds the pointer to the current trap handler program address. Per-VMID register. Bit [63] indicates if the trap
handler is present (1) or not (0) and is not considered part of the address
(bit[62] is replicated into address bit[63]).  Accessed via S_SENDMSG_RTN.</p>
</blockquote>
<blockquote>
<p>Temporary register for shader operations. For example, it can hold a pointer to memory used by the trap handler.</p>
</blockquote>
<p><span>You can configure the GPU to enter the trap handler when encountering certain exceptions listed in the RDNA3 ISA manual.</span></p>
<p>We know from <a href="https://martty.github.io/about/">Marcell Kiss’s</a> blog posts that we need to compile a trap handler, which is a normal shader the GPU switches to when encountering a <code>s_trap</code>. The TBA register has a special bit that indicates whether the trap handler is enabled.</p>
<p>Since these are privileged registers, we cannot write to them from user space. To bridge this gap for debugging, we can utilize the debugfs interface. Luckily, we have <a href="https://umr.readthedocs.io/en/main/intro.html">UMR</a>, which uses that debugfs interface, and it’s open source; we copy AMD’s homework here which is great.</p>
<h2 id="amdgpu-debugfs">AMDGPU Debugfs</h2>
<p>The amdgpu KMD has a couple of files in debugfs under <code>/sys/kernel/debug/dri/{PCI address}</code>; one of them is <code>regs2</code>, which is an interface to a <a href="https://elixir.bootlin.com/linux/v6.18/source/drivers/gpu/drm/amd/amdgpu/amdgpu_debugfs.c#L369"><code>amdgpu_debugfs_regs2_write</code></a> in the kernel that writes to the registers. It works by simply opening the file, seeking the register’s offset, and then writing; it also performs some synchronisation and writes the value correctly. We need to provide more parameters about the register before writing to the file, tho and do that by using an ioctl call. Here are the ioctl arguments:</p>
<pre tabindex="0" data-language="c"><code><span><span>typedef</span><span> struct</span><span> amdgpu_debugfs_regs2_iocdata_v2 {</span></span>
<span><span> __u32 use_srbm</span><span>,</span><span> use_grbm</span><span>,</span><span> pg_lock;</span></span>
<span><span> struct</span><span> {</span></span>
<span><span>  __u32 se</span><span>,</span><span> sh</span><span>,</span><span> instance;</span></span>
<span><span> } grbm;</span></span>
<span><span> struct</span><span> {</span></span>
<span><span>  __u32 me</span><span>,</span><span> pipe</span><span>,</span><span> queue</span><span>,</span><span> vmid;</span></span>
<span><span> } srbm;</span></span>
<span><span> __u32 xcc_id;</span></span>
<span><span>} </span><span>regs2_ioc_data_t</span><span>;</span></span></code></pre>
<p>The 2 structs are because there are 2 types of registers, GRBM and SRBM, each of which is banked by different constructs; you can learn more about some of them here in <a href="https://docs.kernel.org/gpu/amdgpu/driver-core.html#gfx-compute-and-sdma-overall-behaviour">the Linux kernel documentation</a>.</p>
<p>Turns out our registers here are SBRM registers and banked by VMIDs, meaning each VMID has its own TBA and TMA registers. Cool, now we need to figure out the VMID of our process. As far as I understand, VMIDs are a way for the GPU to identify a specific process context, including the page table base address, so the address translation unit can translate a virtual memory address. The context is created when we open the DRM file. They get assigned dynamically at dispatch time, which is a problem for us; we want to write to those registers before dispatch.</p>
<p>We can obtain the VMID of the dispatched process by querying the <code>HW_ID2</code> register with s_getreg_b32. I do a hack here, by enabling the trap handler in every VMID, and there are 16 of them, the first being special, and used by the KMD and the last 8 allocated to the amdkfd driver. We loop over the remaining VMIDs and write to those registers. This can cause issues to other processes using other VMIDs by enabling trap handlers in them and writing the virtual address of our trap handler, which is only valid within our virtual memory address space. It’s relatively safe tho since most other processes won’t cause a trap<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup>.</p>
<p>Now we can write to TMA and TBA, here’s the code:</p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> dev_op_reg32</span><span>(</span></span>
<span><span>  amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> gc_11_reg_t</span><span> reg</span><span>,</span><span> regs2_ioc_data_t</span><span> ioc_data</span><span>,</span><span> reg_32_op_t</span><span> op</span><span>,</span><span> u32</span><span>*</span><span> value) {</span></span>
<span><span> s32 ret </span><span>=</span><span> 0</span><span>;</span></span>
<span></span>
<span><span> reg_info_t</span><span> reg_info     </span><span>=</span><span> gc_11_regs_infos[reg];</span></span>
<span><span> uint64_t</span><span>   reg_offset   </span><span>=</span><span> gc_11_regs_offsets[reg];</span></span>
<span><span> uint64_t</span><span>   base_offset  </span><span>=</span><span> dev</span><span>-&gt;</span><span>gc_regs_base_addr[</span><span>reg_info</span><span>.</span><span>soc_index];</span></span>
<span><span> uint64_t</span><span>   total_offset </span><span>=</span><span> (reg_offset </span><span>+</span><span> base_offset);</span></span>
<span></span>
<span><span> // seems like we're multiplying by 4 here because the registers database in UMRs</span></span>
<span><span> // source has them in indexes rather than bytes.</span></span>
<span><span> total_offset </span><span>*=</span><span> (</span><span>reg_info</span><span>.</span><span>type </span><span>==</span><span> REG_MMIO) </span><span>?</span><span> 4</span><span> :</span><span> 1</span><span>;</span></span>
<span></span>
<span><span> ret </span><span>=</span><span> hdb_ioctl(</span><span>dev</span><span>-&gt;</span><span>regs2_fd</span><span>,</span><span> AMDGPU_DEBUGFS_REGS2_IOC_SET_STATE_V2</span><span>,</span><span> &amp;</span><span>ioc_data)</span><span>;</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>ret</span><span>,</span><span> "Failed to set registers state"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> size_t</span><span> size </span><span>=</span><span> lseek(</span><span>dev</span><span>-&gt;</span><span>regs2_fd</span><span>,</span><span> total_offset</span><span>,</span><span> SEEK_SET)</span><span>;</span></span>
<span><span> HDB_ASSERT(size </span><span>==</span><span> total_offset</span><span>,</span><span> "Failed to seek register address"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> switch</span><span> (op) {</span></span>
<span><span> case</span><span> REG_OP_READ </span><span>:</span><span> size </span><span>=</span><span> read</span><span>(dev</span><span>-&gt;</span><span>regs2_fd</span><span>,</span><span> value</span><span>,</span><span> 4</span><span>); </span><span>break</span><span>;</span></span>
<span><span> case</span><span> REG_OP_WRITE</span><span>:</span><span> size </span><span>=</span><span> write</span><span>(dev</span><span>-&gt;</span><span>regs2_fd</span><span>,</span><span> value</span><span>,</span><span> 4</span><span>); </span><span>break</span><span>;</span></span>
<span><span> default</span><span>          :</span><span> HDB_ASSERT</span><span>(</span><span>false</span><span>,</span><span> "unsupported op"</span><span>);</span></span>
<span><span> }</span></span>
<span></span>
<span><span> HDB_ASSERT(size </span><span>==</span><span> 4</span><span>,</span><span> "Failed to write/read the values to/from the register"</span><span>)</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>And here’s how we write to <code>TMA</code> and <code>TBA</code>:
<span>If you noticed, I’m using bitfields. I use them because working with them is much easier than macros, and while the byte order is not guaranteed by the C spec, it’s guaranteed by System V ABI, which Linux adheres to.</span></p>
<pre tabindex="0" data-language="c"><code><span><span>void</span><span> dev_setup_trap_handler</span><span>(</span><span>amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> u64 tba</span><span>,</span><span> u64 tma) {</span></span>
<span><span> reg_sq_shader_tma_lo_t</span><span> tma_lo </span><span>=</span><span> { .raw </span><span>=</span><span> (u32)(tma) };</span></span>
<span><span> reg_sq_shader_tma_hi_t</span><span> tma_hi </span><span>=</span><span> { .raw </span><span>=</span><span> (u32)(tma </span><span>&gt;&gt;</span><span> 32</span><span>) };</span></span>
<span></span>
<span><span> reg_sq_shader_tba_lo_t</span><span> tba_lo </span><span>=</span><span> { .raw </span><span>=</span><span> (u32)(tba </span><span>&gt;&gt;</span><span> 8</span><span>) };</span></span>
<span><span> reg_sq_shader_tba_hi_t</span><span> tba_hi </span><span>=</span><span> { .raw </span><span>=</span><span> (u32)(tba </span><span>&gt;&gt;</span><span> 40</span><span>) };</span></span>
<span></span>
<span><span> tba_hi</span><span>.</span><span>trap_en </span><span>=</span><span> 1</span><span>;</span></span>
<span></span>
<span><span> regs2_ioc_data_t</span><span> ioc_data </span><span>=</span><span> {</span></span>
<span><span>  .use_srbm </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>  .xcc_id   </span><span>=</span><span> -</span><span>1</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> // NOTE(hadi):</span></span>
<span><span> // vmid's get assigned when code starts executing before hand we don't know which vmid</span></span>
<span><span> // will get assigned to our process so we just set all of them</span></span>
<span><span> for_range(i</span><span>,</span><span> 1</span><span>,</span><span> 9</span><span>)</span><span> {</span></span>
<span><span>  ioc_data</span><span>.</span><span>srbm</span><span>.</span><span>vmid </span><span>=</span><span> i;</span></span>
<span><span>  dev_op_reg32(dev</span><span>,</span><span> REG_SQ_SHADER_TBA_LO</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>tba_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span>  dev_op_reg32(dev</span><span>,</span><span> REG_SQ_SHADER_TBA_HI</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>tba_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span></span>
<span><span>  dev_op_reg32(dev</span><span>,</span><span> REG_SQ_SHADER_TMA_LO</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>tma_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span>  dev_op_reg32(dev</span><span>,</span><span> REG_SQ_SHADER_TMA_HI</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>tma_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> }</span></span>
<span><span>}</span></span></code></pre>
<p>Anyway, now that we can write to those registers, if we enable the trap handler correctly, the GPU should hang when we launch our shader if we added <code>s_trap</code> instruction to it, or we enabled the <code>TRAP_ON_START</code> bit in rsrc3<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup> register.</p>
<p>Now, let’s try to write a trap handler.</p>
<h2 id="the-trap-handler">The Trap Handler</h2>
<p><span>If you wrote a different shader that outputs to a buffer, u can try writing to that shader from the trap handler, which is nice to make sure it’s actually being run.</span></p>
<p>We need 2 things: our trap handler and some scratch memory to use when needed, which we will store the address of in the TMA register.</p>
<p>The trap handler is just a normal program running in privileged state, meaning we have access to special registers like TTMP[0-15]. When we enter a trap handler, we need to first ensure that the state of the GPU registers is saved, just as the kernel does for CPU processes when context-switching, by saving a copy of the stable registers and the program counter, etc. The problem, tho, is that we don’t have a stable ABI for GPUs, or at least not one I’m aware of, and compilers use all the registers they can, so we need to save everything.</p>
<p>AMD GPUs’ Command Processors (CPs) have context-switching functionality, and the amdkfd driver does implement some <a href="https://elixir.bootlin.com/linux/v6.18/source/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_gfx10.asm">context-switching shaders</a>. The problem is they’re not documented, and we have to figure them out from the amdkfd driver source and from other parts of the driver stack that interact with it, which is a pain in the ass. I kinda did a workaround here since I didn’t find luck understanding how it works, and some other reasons I’ll discuss later in the post.</p>
<p>The workaround here is to use only TTMP registers and a combination of specific instructions to copy the values of some registers, allowing us to use more instructions to copy the remaining registers. The main idea is to make use of the <code>global_store_addtid_b32</code> instruction, which adds the index of the current thread within the wave to the writing address, aka</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><msub><mi>D</mi><mrow><mi>t</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>d</mi></mrow></msub><mo>∗</mo><mn>4</mn><mo>+</mo><mi>a</mi><mi>d</mi><mi>d</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">ID_{thread} * 4 + address</annotation></semantics></math></span></span></span></p><p>This allows us to write a unique value per thread using only TTMP registers, which are unique per wave, not per thread<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup>, so we can save the context of a single wave.</p>
<p>The problem is that if we have more than 1 wave, they will overlap, and we will have a race condition.</p>
<p>Here is the code:</p>
<pre tabindex="0" data-language="asm"><code><span><span>start</span><span>:</span></span>
<span><span> ;; save the STATUS word into ttmp8</span></span>
<span><span> s_getreg_b32 ttmp8, hwreg(HW_REG_STATUS)</span></span>
<span></span>
<span><span> ;; save exec into ttmp[2:3]</span></span>
<span><span> s_mov_b64 ttmp[</span><span>2</span><span>:</span><span>3</span><span>], exec</span></span>
<span></span>
<span><span> ;; getting the address of our tma buffer</span></span>
<span><span> s_sendmsg_rtn_b64 ttmp[</span><span>4</span><span>:</span><span>5</span><span>], sendmsg(MSG_RTN_GET_TMA)</span></span>
<span><span> s_waitcnt lgkmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; save vcc</span></span>
<span><span> s_mov_b64 ttmp[</span><span>6</span><span>:</span><span>7</span><span>], vcc</span></span>
<span></span>
<span><span> ;; enable all threads so they can write their vgpr registers</span></span>
<span><span> s_mov_b64 exec, -</span><span>1</span></span>
<span></span>
<span><span> ;; FIXME(hadi): this assumes only 1 wave is running</span></span>
<span><span> global_store_addtid_b32 v0, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET        glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v1, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>256</span><span>  glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>512</span><span>  glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v3, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>768</span><span>  glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v4, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1024</span><span> glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v5, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1280</span><span> glc slc dlc</span></span>
<span><span> global_store_addtid_b32 v6, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1536</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; only first thread is supposed to write sgprs of the wave</span></span>
<span><span> s_mov_b64 exec, </span><span>1</span></span>
<span><span> v_mov_b32 v1, s0</span></span>
<span><span> v_mov_b32 v2, s1</span></span>
<span><span> v_mov_b32 v3, s2</span></span>
<span><span> v_mov_b32 v4, s3</span></span>
<span><span> v_mov_b32 v5, s4</span></span>
<span><span> v_mov_b32 v0, </span><span>0</span></span>
<span><span> global_store_b32 v0, v1, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET glc slc dlc</span></span>
<span><span> global_store_b32 v0, v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET + </span><span>4</span><span> glc slc dlc</span></span>
<span><span> global_store_b32 v0, v3, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET + </span><span>8</span><span> glc slc dlc</span></span>
<span><span> global_store_b32 v0, v4, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET + </span><span>12</span><span> glc slc dlc</span></span>
<span><span> global_store_b32 v0, v5, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_SREG_OFFSET + </span><span>16</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; enable all threads</span></span>
<span><span> s_mov_b64 exec, -</span><span>1</span></span></code></pre>
<p>Now that we have those values in memory, we need to tell the CPU: Hey, we got the data, and pause the GPU’s execution until the CPU issues a command. Also, notice we can just modify those from the CPU.</p>
<p>Before we tell the CPU, we need to write some values that might help the CPU. Here are they:</p>
<pre tabindex="0" data-language="asm"><code><span><span> ;; IDs to identify which parts of the hardware we are running on exactly</span></span>
<span><span> s_getreg_b32 ttmp10, hwreg(HW_REG_HW_ID1)</span></span>
<span><span> s_getreg_b32 ttmp11, hwreg(HW_REG_HW_ID2)</span></span>
<span><span> v_mov_b32 v3, ttmp10</span></span>
<span><span> v_mov_b32 v4, ttmp11</span></span>
<span><span> global_store_dwordx2 v1, v[</span><span>3</span><span>:</span><span>4</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_DATA_OFFSET glc slc dlc</span></span>
<span></span>
<span><span> ;; the original vcc mask</span></span>
<span><span> v_mov_b32 v3, ttmp6</span></span>
<span><span> v_mov_b32 v4, ttmp7</span></span>
<span><span> global_store_dwordx2 v1, v[</span><span>3</span><span>:</span><span>4</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>2048</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; the original exec mask</span></span>
<span><span> v_mov_b32 v3, ttmp2</span></span>
<span><span> v_mov_b32 v4, ttmp3</span></span>
<span><span> global_store_dwordx2 v1, v[</span><span>3</span><span>:</span><span>4</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>2056</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; the program counter</span></span>
<span><span> v_mov_b32 v3, ttmp0</span></span>
<span><span> v_mov_b32 v4, ttmp1</span></span>
<span><span> v_and_b32 v4, v4, </span><span>0xffff</span></span>
<span><span> global_store_dwordx2 v1, v[</span><span>3</span><span>:</span><span>4</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>16</span><span> glc slc dlc</span></span>
<span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span></code></pre>
<p>Now the GPU should just wait for the CPU, and here’s the spin code it’s implemented as described by Marcell Kiss <a href="https://martty.github.io/posts/radbg_part_4/#busier-waiting">here</a>:</p>
<pre tabindex="0" data-language="asm"><code><span><span>SPIN</span><span>:</span></span>
<span><span> global_load_dword v1, v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>] glc slc dlc</span></span>
<span></span>
<span><span>SPIN1</span><span>:</span></span>
<span><span> // I found the bit range of </span><span>10</span><span> to </span><span>15</span><span> using trial </span><span>and</span><span> error </span><span>in</span><span> the</span></span>
<span><span> // isa manual specifies that it's a </span><span>6</span><span>-bit number but the offset </span><span>10</span></span>
<span><span> // is just trial </span><span>and</span><span> error</span></span>
<span><span>  s_getreg_b32 ttmp13, hwreg(HW_REG_IB_STS, </span><span>10</span><span>, </span><span>15</span><span>)</span></span>
<span><span> s_and_b32 ttmp13, ttmp13, ttmp13</span></span>
<span><span> s_cbranch_scc1 SPIN1</span></span>
<span></span>
<span><span> v_readfirstlane_b32 ttmp13, v1</span></span>
<span><span> s_and_b32 ttmp13, ttmp13, ttmp13</span></span>
<span><span> s_cbranch_scc0 SPIN</span></span>
<span></span>
<span><span>CLEAR</span><span>:</span></span>
<span><span> v_mov_b32 v2, </span><span>0</span></span>
<span><span> v_mov_b32 v1, </span><span>0</span></span>
<span><span> global_store_dword v1, v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>] glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span></code></pre>
<p>The main loop in the CPU is like enable trap handler, then dispatch shader, then wait for the GPU to write some specific value in a specific address to signal all data is there, then examine and display, and tell the GPU all clear, go ahead.</p>
<p>Now that our uncached buffers are in play, we just keep looping and checking whether the GPU has written the register values. When it does, the first thing we do is halt the wave by writing into the <code>SQ_CMD</code> register to allow us to do whatever with the wave without causing any issues, tho if we halt for too long, the GPU CP will reset the command queue and kill the process, but we can change that behaviour by adjusting <a href="https://www.kernel.org/doc/html/v4.20/gpu/amdgpu.html#module-parameters">lockup_timeout</a> parameter of the amdgpu kernel module:</p>
<pre tabindex="0" data-language="c"><code><span><span>reg_sq_wave_hw_id1_t</span><span> hw1 </span><span>=</span><span> { .raw </span><span>=</span><span> tma[</span><span>2</span><span>] };</span></span>
<span><span>reg_sq_wave_hw_id2_t</span><span> hw2 </span><span>=</span><span> { .raw </span><span>=</span><span> tma[</span><span>3</span><span>] };</span></span>
<span></span>
<span><span>reg_sq_cmd_t</span><span> halt_cmd </span><span>=</span><span> {</span></span>
<span><span> .cmd  </span><span>=</span><span> 1</span><span>,</span></span>
<span><span> .mode </span><span>=</span><span> 1</span><span>,</span></span>
<span><span> .data </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>};</span></span>
<span></span>
<span><span>regs2_ioc_data_t</span><span> ioc_data </span><span>=</span><span> {</span></span>
<span><span> .use_srbm </span><span>=</span><span> false</span><span>,</span></span>
<span><span> .use_grbm </span><span>=</span><span> true</span><span>,</span></span>
<span><span>};</span></span>
<span></span>
<span><span>dev_op_reg32</span><span>(</span><span>&amp;</span><span>amdgpu</span><span>,</span><span> REG_SQ_CMD</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>halt_cmd.raw);</span></span>
<span><span>gpu_is_halted </span><span>=</span><span> true</span><span>;</span></span></code></pre>
<p>From here on, we can do whatever with the data we have. All the data we need to build a proper debugger. We will come back to what to do with the data in a bit; let’s assume we did what was needed for now.</p>
<p>Now that we’re done with the CPU, we need to write to the first byte in our TMA buffer, since the trap handler checks for that, then resume the wave, and the trap handler should pick it up. We can resume by writing to the <code>SQ_CMD</code> register again:</p>
<pre tabindex="0" data-language="c"><code><span><span>halt_cmd.mode </span><span>=</span><span> 0</span><span>;</span></span>
<span><span>dev_op_reg32</span><span>(</span><span>&amp;</span><span>amdgpu</span><span>,</span><span> REG_SQ_CMD</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_WRITE</span><span>,</span><span> &amp;</span><span>halt_cmd.raw);</span></span>
<span><span>gpu_is_halted </span><span>=</span><span> false</span><span>;</span></span></code></pre>
<p>Then the GPU should continue. We need to restore everything and return the program counter to the original address. Based on whether it’s a hardware trap or not, the program counter may point to the instruction before or the instruction itself. The ISA manual and Marcell Kiss’s posts explain that well, so refer to them.</p>
<pre tabindex="0" data-language="asm"><code><span><span>RETURN</span><span>:</span></span>
<span><span> ;; extract the trap ID from ttmp1</span></span>
<span><span> s_and_b32 ttmp9, ttmp1, PC_HI_TRAP_ID_MASK</span></span>
<span><span> s_lshr_b32 ttmp9, ttmp9, PC_HI_TRAP_ID_SHIFT</span></span>
<span></span>
<span><span> ;; if the trapID == 0, then this is a hardware trap,</span></span>
<span><span> ;; we don't need to fix up the return address</span></span>
<span><span> s_cmpk_eq_u32 ttmp9, </span><span>0</span></span>
<span><span> s_cbranch_scc1 RETURN_FROM_NON_S_TRAP</span></span>
<span></span>
<span><span> ;; restore PC</span></span>
<span><span> ;; add 4 to the faulting address, with carry</span></span>
<span><span> s_add_u32 ttmp0, ttmp0, </span><span>4</span></span>
<span><span> s_addc_u32 ttmp1, ttmp1, </span><span>0</span></span>
<span></span>
<span><span>RETURN_FROM_NON_S_TRAP</span><span>:</span></span>
<span><span> s_load_dwordx4 s[</span><span>0</span><span>:</span><span>3</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], TMA_SREG_OFFSET glc dlc</span></span>
<span><span> s_load_dword s4, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], TMA_SREG_OFFSET + </span><span>16</span><span> glc dlc</span></span>
<span><span> s_waitcnt lgkmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> s_mov_b64 exec, -</span><span>1</span></span>
<span><span> global_load_addtid_b32 v0, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET        glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v1, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>256</span><span>  glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v2, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>512</span><span>  glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v3, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>768</span><span>  glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v4, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1024</span><span> glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v5, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1280</span><span> glc slc dlc</span></span>
<span><span> global_load_addtid_b32 v6, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>offset</span><span>:</span><span>TMA_VREG_OFFSET + </span><span>1536</span><span> glc slc dlc</span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>)</span></span>
<span></span>
<span><span> ;; mask off non-address high bits from ttmp1</span></span>
<span><span> s_and_b32 ttmp1, ttmp1, </span><span>0xffff</span></span>
<span></span>
<span><span> ;; restore exec</span></span>
<span><span> s_load_b64 vcc, ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>2048</span><span> glc dlc</span></span>
<span><span> s_load_b64 ttmp[</span><span>2</span><span>:</span><span>3</span><span>], ttmp[</span><span>4</span><span>:</span><span>5</span><span>], </span><span>2056</span><span> glc dlc</span></span>
<span><span> s_waitcnt lgkmcnt(</span><span>0</span><span>)</span></span>
<span><span> s_mov_b64 exec, ttmp[</span><span>2</span><span>:</span><span>3</span><span>]</span></span>
<span></span>
<span><span> ;; restore STATUS.EXECZ, not writable by s_setreg_b32</span></span>
<span><span> s_and_b64 exec, exec, exec</span></span>
<span></span>
<span><span> ;; restore STATUS.VCCZ, not writable by s_setreg_b32</span></span>
<span><span> s_and_b64 vcc, vcc, vcc</span></span>
<span></span>
<span><span> ;; restore STATUS.SCC</span></span>
<span><span> s_setreg_b32 hwreg(HW_REG_STATUS, </span><span>0</span><span>, </span><span>1</span><span>), ttmp8</span></span>
<span></span>
<span><span> s_waitcnt vmcnt(</span><span>0</span><span>) lgkmcnt(</span><span>0</span><span>) expcnt(</span><span>0</span><span>)  </span><span>; Full pipeline flush</span></span>
<span><span> ;; return from trap handler and restore STATUS.PRIV</span></span>
<span><span> s_rfe_b64 [ttmp0, ttmp1]</span></span></code></pre>
<h2 id="spir-v">SPIR-V</h2>
<p>Now we can run compiled code directly, but we don’t want people to compile their code manually, then extract the text section, and give it to us. The plan is to take SPIR-V code, compile it correctly, then run it, or, even better, integrate with RADV and let RADV give us more information to work with.</p>
<p>My main plan was making like fork RADV and then add then make report for us the vulkan calls and then we can have a better view on the GPU work know the buffers/textures it’s using etc, This seems like a lot more work tho so I’ll keep it in mind but not doing that for now unless someone is willing to pay me for that ;).</p>
<p>For now, let’s just use RADV’s compiler <code>ACO</code>. Luckily, RADV has a <code>null_winsys</code> mode, aka it will not do actual work or open DRM files, just a fake Vulkan device, which is perfect for our case here, since we care about nothing other than just compiling code. We can enable it by setting the env var <code>RADV_FORCE_FAMILY</code>, then we just call what we need like this:</p>
<pre tabindex="0" data-language="c"><code><span><span>int32_t</span><span> hdb_compile_spirv_to_bin</span><span>(</span></span>
<span><span>  const</span><span> void*</span><span> spirv_binary</span><span>,</span></span>
<span><span>  size_t</span><span> size</span><span>,</span></span>
<span><span>  hdb_shader_stage_t</span><span> stage</span><span>,</span></span>
<span><span>  hdb_shader_t</span><span>*</span><span> shader</span></span>
<span><span>) {</span></span>
<span><span> setenv(</span><span>"RADV_FORCE_FAMILY"</span><span>,</span><span> "navi31"</span><span>,</span><span> 1</span><span>)</span><span>;</span></span>
<span><span> //  setenv("RADV_DEBUG", "nocache,noopt", 1);</span></span>
<span><span> setenv(</span><span>"ACO_DEBUG"</span><span>,</span><span> "nocache,noopt"</span><span>,</span><span> 1</span><span>)</span><span>;</span></span>
<span></span>
<span><span> VkInstanceCreateInfo i_cinfo </span><span>=</span><span> {</span></span>
<span><span>  .sType </span><span>=</span><span> VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO</span><span>,</span></span>
<span><span>  .pApplicationInfo </span><span>=</span></span>
<span><span>    &amp;</span><span>(VkApplicationInfo){</span></span>
<span><span>      .sType              </span><span>=</span><span> VK_STRUCTURE_TYPE_APPLICATION_INFO</span><span>,</span></span>
<span><span>      .pApplicationName   </span><span>=</span><span> "HDB Shader Compiler"</span><span>,</span></span>
<span><span>      .applicationVersion </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>      .pEngineName        </span><span>=</span><span> "HDB"</span><span>,</span></span>
<span><span>      .engineVersion      </span><span>=</span><span> 1</span><span>,</span></span>
<span><span>      .apiVersion         </span><span>=</span><span> VK_API_VERSION_1_4</span><span>,</span></span>
<span><span>    }</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> VkInstance vk_instance </span><span>=</span><span> {};</span></span>
<span><span> radv_CreateInstance(</span><span>&amp;</span><span>i_cinfo</span><span>,</span><span> NULL</span><span>,</span><span> &amp;</span><span>vk_instance)</span><span>;</span></span>
<span></span>
<span><span> struct</span><span> radv_instance</span><span>*</span><span> instance </span><span>=</span><span> radv_instance_from_handle(vk_instance)</span><span>;</span></span>
<span><span> instance</span><span>-&gt;</span><span>debug_flags </span><span>|=</span></span>
<span><span>   RADV_DEBUG_NIR_DEBUG_INFO </span><span>|</span><span> RADV_DEBUG_NO_CACHE </span><span>|</span><span> RADV_DEBUG_INFO;</span></span>
<span></span>
<span><span> uint32_t</span><span>         n       </span><span>=</span><span> 1</span><span>;</span></span>
<span><span> VkPhysicalDevice vk_pdev </span><span>=</span><span> {};</span></span>
<span><span> instance</span><span>-&gt;</span><span>vk</span><span>.</span><span>dispatch_table.</span><span>EnumeratePhysicalDevices</span><span>(</span><span>vk_instance</span><span>,</span><span> &amp;</span><span>n</span><span>,</span><span> &amp;</span><span>vk_pdev</span><span>);</span></span>
<span></span>
<span><span> struct</span><span> radv_physical_device</span><span>*</span><span> pdev </span><span>=</span><span> radv_physical_device_from_handle(vk_pdev)</span><span>;</span></span>
<span><span> pdev</span><span>-&gt;</span><span>use_llvm                    </span><span>=</span><span> false</span><span>;</span></span>
<span></span>
<span><span> VkDeviceCreateInfo d_cinfo </span><span>=</span><span> { VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };</span></span>
<span><span> VkDevice vk_dev </span><span>=</span><span> {};</span></span>
<span><span> pdev</span><span>-&gt;</span><span>vk</span><span>.</span><span>dispatch_table.</span><span>CreateDevice</span><span>(</span><span>vk_pdev</span><span>,</span><span> &amp;</span><span>d_cinfo</span><span>,</span><span> NULL</span><span>,</span><span> &amp;</span><span>vk_dev</span><span>);</span></span>
<span></span>
<span><span> struct</span><span> radv_device</span><span>*</span><span> dev </span><span>=</span><span> radv_device_from_handle(vk_dev)</span><span>;</span></span>
<span></span>
<span><span> struct</span><span> radv_shader_stage radv_stage </span><span>=</span><span> {</span></span>
<span><span>  .</span><span>spirv</span><span>.</span><span>data </span><span>=</span><span> spirv_binary</span><span>,</span></span>
<span><span>  .</span><span>spirv</span><span>.</span><span>size </span><span>=</span><span> size</span><span>,</span></span>
<span><span>  .entrypoint </span><span>=</span><span> "main"</span><span>,</span></span>
<span><span>  .stage      </span><span>=</span><span> MESA_SHADER_COMPUTE</span><span>,</span></span>
<span><span>  .layout </span><span>=</span><span> {</span></span>
<span><span>   .push_constant_size </span><span>=</span><span> 16</span><span>,</span></span>
<span><span>  }</span><span>,</span></span>
<span><span>  .key </span><span>=</span><span> {</span></span>
<span><span>   .optimisations_disabled </span><span>=</span><span> true</span><span>,</span></span>
<span><span>  }</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> struct</span><span> radv_shader_binary</span><span>*</span><span> cs_bin </span><span>=</span><span> NULL</span><span>;</span></span>
<span><span> struct</span><span> radv_shader</span><span>*</span><span>        cs_shader </span><span>=</span></span>
<span><span>   radv_compile_cs(dev</span><span>,</span><span> NULL</span><span>,</span><span> &amp;</span><span>radv_stage</span><span>,</span><span> true</span><span>,</span><span> true</span><span>,</span><span> false</span><span>,</span><span> true</span><span>,</span><span> &amp;</span><span>cs_bin)</span><span>;</span></span>
<span></span>
<span><span> *</span><span>shader </span><span>=</span><span> (</span><span>hdb_shader_t</span><span>){</span></span>
<span><span>  .bin              </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>code</span><span>,</span></span>
<span><span>  .bin_size         </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>code_size</span><span>,</span></span>
<span><span>  .rsrc1            </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>config</span><span>.</span><span>rsrc1</span><span>,</span></span>
<span><span>  .rsrc2            </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>config</span><span>.</span><span>rsrc2</span><span>,</span></span>
<span><span>  .rsrc3            </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>config</span><span>.</span><span>rsrc3</span><span>,</span></span>
<span><span>  .debug_info       </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>debug_info</span><span>,</span></span>
<span><span>  .debug_info_count </span><span>=</span><span> cs_shader</span><span>-&gt;</span><span>debug_info_count</span><span>,</span></span>
<span><span> };</span></span>
<span></span>
<span><span> return</span><span> 0</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>Now that we have a well-structured loop and communication between the GPU and the CPU, we can run SPIR-V binaries to some extent. Let’s see how we can make it an actual debugger.</p>
<h2 id="an-actual-debugger">An Actual Debugger</h2>
<p>We talked earlier about CPs natively supporting context-switching, this appears to be compute spcific feature,
which prevents from implementing it for other types of shaders, tho, it appears that mesh shaders and raytracing
shaders are just compute shaders under the hood, which will allow us to use that functionality. For now debugging
one wave feels enough, also we can moify the wave parameters to debug some specific indices.</p>
<p>Here’s some of the features</p>
<h2 id="breakpoints-and-stepping">Breakpoints and Stepping</h2>
<p>For stepping, we can use 2 bits: one in <code>RSRC1</code> and the other in <code>RSRC3</code>. They’re <code>DEBUG_MODE</code> and <code>TRAP_ON_START</code>, respectively. The former enters the trap handler after each instruction, and the latter enters before the first instruction. This means we can automatically enable instruction-level stepping.</p>
<p>Regarding breakpoints, I haven’t implemented them, but they’re rather simple to implement here by us having the base address of the code buffer and knowing the size of each instruction; we can calculate the program counter location ahead and have a list of them available to the GPU, and we can do a binary search on the trap handler.</p>
<h2 id="source-code-line-mapping">Source Code Line Mapping</h2>
<p>The ACO shader compiler does generate instruction-level source code mapping, which is good enough for our purposes here. By taking the offset<sup><a href="#user-content-fn-4" id="user-content-fnref-4" data-footnote-ref="" aria-describedby="footnote-label">4</a></sup> of the current program counter and indexing into the code buffer, we can retrieve the current instruction and disassemble it, as well as find the source code mapping from the debug info.</p>
<h2 id="address-watching-aka-watchpoints">Address Watching aka Watchpoints</h2>
<p>We can implement this by marking the GPU page as protected. On a GPU fault, we enter the trap handler, check whether it’s within the range of our buffers and textures, and then act accordingly. Also, looking at the registers, we can find these:</p>
<pre tabindex="0" data-language="c"><code><span><span>typedef</span><span> union</span><span> {</span></span>
<span><span> struct</span><span> {</span></span>
<span><span>  uint32_t</span><span> addr: </span><span>16</span><span>;</span></span>
<span><span> };</span></span>
<span><span> uint32_t</span><span> raw;</span></span>
<span><span>} </span><span>reg_sq_watch0_addr_h_t</span><span>;</span></span>
<span></span>
<span><span>typedef</span><span> union</span><span> {</span></span>
<span><span> struct</span><span> {</span></span>
<span><span>  uint32_t</span><span> __reserved_0 : </span><span>6</span><span>;</span></span>
<span><span>  uint32_t</span><span> addr: </span><span>26</span><span>;</span></span>
<span><span> };</span></span>
<span><span> uint32_t</span><span> raw;</span></span>
<span><span>} </span><span>reg_sq_watch0_addr_l_t</span><span>;</span></span></code></pre>
<p>which suggests that the hardware already supports this natively, so we don’t even need to do that dance. It needs more investigation on my part, tho, since I didn’t implement this.</p>
<h2 id="variables-types-and-names">Variables Types and Names</h2>
<p>This needs some serious plumbing, since we need to make NIR(Mesa’s intermediate representation) optimisation passes propagate debug info correctly. I already started on this <a href="https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/37705">here</a>. Then we need to make ACO track variables and store the information.</p>
<h2 id="vulkan-integration">Vulkan Integration</h2>
<p>This requires ditching our simple UMD we made earlier and using RADV, which is what should happen eventually, then we have our custom driver maybe pause on before a specific frame, or get triggered by a key, and then ask before each dispatch if to attach to it or not, or something similar, since we have a full proper Vulkan implementation we already have all the information we would need like buffers, textures, push constants, types, variable names, .. etc, that would be a much better and more pleasant debugger to use.</p>
<hr>
<p>Finally, here’s some live footage:</p>

    <figure>
      <iframe src="https://www.youtube.com/embed/HDMC9GhaLyc" title="YouTube video player" loading="lazy" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </figure>
    
<h2 id="bonus-round">Bonus Round</h2>
<p>Here is an incomplete user-mode page walking code for gfx11, aka rx7900xtx</p>
<pre tabindex="0" data-language="c"><code><span><span>typedef</span><span> struct</span><span> {</span></span>
<span><span> u64 valid         : </span><span>1</span><span>;</span><span>  // 0</span></span>
<span><span> u64 system        : </span><span>1</span><span>;</span><span>  // 1</span></span>
<span><span> u64 coherent      : </span><span>1</span><span>;</span><span>  // 2</span></span>
<span><span> u64 __reserved_0  : </span><span>3</span><span>;</span><span>  // 5</span></span>
<span><span> u64 pte_base_addr : </span><span>42</span><span>;</span><span> // 47</span></span>
<span><span> u64 pa_rsvd       : </span><span>4</span><span>;</span><span>  // 51</span></span>
<span><span> u64 __reserved_1  : </span><span>2</span><span>;</span><span>  // 53</span></span>
<span><span> u64 mall_reuse    : </span><span>2</span><span>;</span><span>  // 55</span></span>
<span><span> u64 tfs_addr      : </span><span>1</span><span>;</span><span>  // 56</span></span>
<span><span> u64 __reserved_2  : </span><span>1</span><span>;</span><span>  // 57</span></span>
<span><span> u64 frag_size     : </span><span>5</span><span>;</span><span>  // 62</span></span>
<span><span> u64 pte           : </span><span>1</span><span>;</span><span>  // 63</span></span>
<span><span>} </span><span>pde_t</span><span>;</span></span>
<span></span>
<span><span>typedef</span><span> struct</span><span> {</span></span>
<span><span> u64 valid          : </span><span>1</span><span>;</span><span> // = pte_entry &amp; 1;</span></span>
<span><span> u64 system         : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 1) &amp; 1;</span></span>
<span><span> u64 coherent       : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 2) &amp; 1;</span></span>
<span><span> u64 tmz            : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 3) &amp; 1;</span></span>
<span><span> u64 execute        : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 4) &amp; 1;</span></span>
<span><span> u64 read           : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 5) &amp; 1;</span></span>
<span><span> u64 write          : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 6) &amp; 1;</span></span>
<span><span> u64 fragment       : </span><span>5</span><span>;</span><span> // = (pte_entry &gt;&gt; 7) &amp; 0x1F;</span></span>
<span><span> u64 page_base_addr : </span><span>36</span><span>;</span></span>
<span><span> u64 mtype          : </span><span>2</span><span>;</span><span> // = (pte_entry &gt;&gt; 48) &amp; 3;</span></span>
<span><span> u64 prt            : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 51) &amp; 1;</span></span>
<span><span> u64 software       : </span><span>2</span><span>;</span><span> // = (pte_entry &gt;&gt; 52) &amp; 3;</span></span>
<span><span> u64 pde            : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 54) &amp; 1;</span></span>
<span><span> u64 __reserved_0   : </span><span>1</span><span>;</span></span>
<span><span> u64 further        : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 56) &amp; 1;</span></span>
<span><span> u64 gcr            : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 57) &amp; 1;</span></span>
<span><span> u64 llc_noalloc    : </span><span>1</span><span>;</span><span> // = (pte_entry &gt;&gt; 58) &amp; 1;</span></span>
<span><span>} </span><span>pte_t</span><span>;</span></span>
<span></span>
<span><span>static</span><span> inline</span><span> pde_t</span><span> decode_pde</span><span>(u64 pde_raw) {</span></span>
<span><span> pde_t</span><span> pde         </span><span>=</span><span> *</span><span>((</span><span>pde_t</span><span>*</span><span>)(</span><span>&amp;</span><span>pde_raw));</span></span>
<span><span> pde</span><span>.</span><span>pte_base_addr </span><span>=</span><span> (u64)</span><span>pde</span><span>.</span><span>pte_base_addr </span><span>&lt;&lt;</span><span> 6</span><span>;</span></span>
<span><span> return</span><span> pde;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>static</span><span> inline</span><span> pte_t</span><span> decode_pte</span><span>(u64 pde_raw) {</span></span>
<span><span> pte_t</span><span> pte          </span><span>=</span><span> *</span><span>((</span><span>pte_t</span><span>*</span><span>)(</span><span>&amp;</span><span>pde_raw));</span></span>
<span><span> pte</span><span>.</span><span>page_base_addr </span><span>=</span><span> (u64)</span><span>pte</span><span>.</span><span>page_base_addr </span><span>&lt;&lt;</span><span> 12</span><span>;</span></span>
<span><span> return</span><span> pte;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>static</span><span> inline</span><span> u64 </span><span>log2_range_round_up</span><span>(u64 s</span><span>,</span><span> u64 e) {</span></span>
<span><span> u64 x </span><span>=</span><span> e </span><span>-</span><span> s </span><span>-</span><span> 1</span><span>;</span></span>
<span><span> return</span><span> (x </span><span>==</span><span> 0</span><span> ||</span><span> x </span><span>==</span><span> 1</span><span>) </span><span>?</span><span> 1</span><span> :</span><span> 64</span><span> -</span><span> __builtin_clzll(x)</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> dev_linear_vram</span><span>(</span><span>amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> u64 phy_addr</span><span>,</span><span> size_t</span><span> size</span><span>,</span><span> void*</span><span> buf) {</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>((phy_addr </span><span>&amp;</span><span> 3</span><span>) </span><span>||</span><span> (size </span><span>&amp;</span><span> 3</span><span>))</span><span>,</span><span> "Must be page aligned address and size"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> size_t</span><span> offset </span><span>=</span><span> lseek(</span><span>dev</span><span>-&gt;</span><span>vram_fd</span><span>,</span><span> phy_addr</span><span>,</span><span> SEEK_SET)</span><span>;</span></span>
<span><span> HDB_ASSERT(offset </span><span>==</span><span> phy_addr</span><span>,</span><span> "Couldn't seek to the requested addr"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> offset </span><span>=</span><span> read(</span><span>dev</span><span>-&gt;</span><span>vram_fd</span><span>,</span><span> buf</span><span>,</span><span> size)</span><span>;</span></span>
<span><span> HDB_ASSERT(offset </span><span>==</span><span> size</span><span>,</span><span> "Couldn't read the full requested size"</span><span>)</span><span>;</span></span>
<span><span>}</span></span>
<span></span>
<span><span>void</span><span> dev_decode</span><span>(</span><span>amdgpu_t</span><span>*</span><span> dev</span><span>,</span><span> u32 vmid</span><span>,</span><span> u64 va_addr) {</span></span>
<span><span> reg_gcmc_vm_fb_location_base_t</span><span> fb_base_reg   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcmc_vm_fb_location_top_t</span><span>  fb_top_reg    </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcmc_vm_fb_offset_t</span><span>        fb_offset_reg </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span></span>
<span><span> regs2_ioc_data_t</span><span> ioc_data </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> dev_op_reg32(</span></span>
<span><span>   dev</span><span>,</span><span> REG_GCMC_VM_FB_LOCATION_BASE</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>fb_base_reg</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> REG_GCMC_VM_FB_LOCATION_TOP</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>fb_top_reg</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> REG_GCMC_VM_FB_OFFSET</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>fb_offset_reg</span><span>.</span><span>raw)</span><span>;</span></span>
<span></span>
<span><span> u64 fb_offset </span><span>=</span><span> (u64)</span><span>fb_offset_reg</span><span>.</span><span>fb_offset;</span></span>
<span></span>
<span><span> // TODO(hadi): add zfb mode support</span></span>
<span><span> bool</span><span> zfb </span><span>=</span><span> fb_top_reg</span><span>.</span><span>fb_top </span><span>+</span><span> 1</span><span> &lt;</span><span> fb_base_reg</span><span>.</span><span>fb_base;</span></span>
<span><span> HDB_ASSERT(</span><span>!</span><span>zfb</span><span>,</span><span> "ZFB mode is not implemented yet!"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> // printf(</span></span>
<span><span> //   "fb base: 0x%x\nfb_top: 0x%x\nfb_offset: 0x%x\n",</span></span>
<span><span> //   fb_base_reg.raw,</span></span>
<span><span> //   fb_top_reg.raw,</span></span>
<span><span> //   fb_offset_reg.raw);</span></span>
<span></span>
<span><span> gc_11_reg_t</span><span> pt_start_lo_id </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_start_hi_id </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_end_lo_id   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_end_hi_id   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_base_hi_id  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> pt_base_lo_id  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> gc_11_reg_t</span><span> ctx_cntl_id    </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span></span>
<span><span> switch</span><span> (vmid) {</span></span>
<span><span> case</span><span> 0</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT0_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 1</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT1_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 2</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT2_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 3</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT3_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 4</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT4_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 5</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT5_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 6</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT6_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 7</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT7_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 8</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT7_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 9</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT7_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 10</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT10_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 11</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT11_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 12</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT12_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 13</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT13_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 14</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT14_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> case</span><span> 15</span><span>:</span></span>
<span><span>  pt_start_lo_id </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_LO32;</span></span>
<span><span>  pt_start_hi_id </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_HI32;</span></span>
<span><span>  pt_end_lo_id   </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_LO32;</span></span>
<span><span>  pt_end_hi_id   </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_HI32;</span></span>
<span><span>  pt_base_lo_id  </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_LO32;</span></span>
<span><span>  pt_base_hi_id  </span><span>=</span><span> REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_HI32;</span></span>
<span><span>  ctx_cntl_id    </span><span>=</span><span> REG_GCVM_CONTEXT15_CNTL;</span></span>
<span><span>  break</span><span>;</span></span>
<span><span> default</span><span>:</span><span> HDB_ASSERT</span><span>(</span><span>false</span><span>,</span><span> "Out of range VMID 0-15 trying to access </span><span>%u</span><span>"</span><span>,</span><span> vmid);</span></span>
<span><span> }</span></span>
<span></span>
<span><span> // all the types of the contexts are the same so will just use 0 but pass the correct</span></span>
<span><span> // register enum to the read function</span></span>
<span><span> reg_gcvm_context0_page_table_start_addr_lo32_t</span><span> pt_start_lo </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_start_addr_hi32_t</span><span> pt_start_hi </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_end_addr_lo32_t</span><span>   pt_end_lo   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_end_addr_hi32_t</span><span>   pt_end_hi   </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_base_addr_lo32_t</span><span>  pt_base_lo  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_page_table_base_addr_hi32_t</span><span>  pt_base_hi  </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> reg_gcvm_context0_cntl_t</span><span>                       ctx_cntl    </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_start_lo_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_start_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_start_hi_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_start_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_end_lo_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_end_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_end_hi_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_end_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_base_lo_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_base_lo</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> pt_base_hi_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>pt_base_hi</span><span>.</span><span>raw)</span><span>;</span></span>
<span><span> dev_op_reg32(dev</span><span>,</span><span> ctx_cntl_id</span><span>,</span><span> ioc_data</span><span>,</span><span> REG_OP_READ</span><span>,</span><span> &amp;</span><span>ctx_cntl</span><span>.</span><span>raw)</span><span>;</span></span>
<span></span>
<span><span> u64 pt_start_addr </span><span>=</span><span> ((u64)</span><span>pt_start_lo</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 12</span><span>) </span><span>|</span><span> ((u64)</span><span>pt_start_hi</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 44</span><span>);</span></span>
<span><span> u64 pt_end_addr   </span><span>=</span><span> ((u64)</span><span>pt_end_lo</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 12</span><span>) </span><span>|</span><span> ((u64)</span><span>pt_end_hi</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 44</span><span>);</span></span>
<span><span> u64 pt_base_addr  </span><span>=</span><span> ((u64)</span><span>pt_base_lo</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 0</span><span>) </span><span>|</span><span> ((u64)</span><span>pt_base_hi</span><span>.</span><span>raw </span><span>&lt;&lt;</span><span> 32</span><span>);</span></span>
<span><span> u32 pt_depth      </span><span>=</span><span> ctx_cntl</span><span>.</span><span>page_table_depth;</span></span>
<span><span> u32 ptb_size      </span><span>=</span><span> ctx_cntl</span><span>.</span><span>page_table_block_size;</span></span>
<span></span>
<span><span> HDB_ASSERT(pt_base_addr </span><span>!=</span><span> 0x</span><span>ffffffffffffffff</span><span>ull</span><span>,</span><span> "Invalid page table base addr"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> printf(</span></span>
<span><span>   "\tPage Table Start: 0x</span><span>%lx</span><span>\n\tPage Table End: 0x</span><span>%lx</span><span>\n\tPage Table Base: "</span></span>
<span><span>   "0x</span><span>%lx</span><span>\n\tPage Table Depth: </span><span>%u</span><span>\n\tBlock Size: </span><span>%u</span><span>\n"</span><span>,</span></span>
<span><span>   pt_start_addr</span><span>,</span></span>
<span><span>   pt_end_addr</span><span>,</span></span>
<span><span>   pt_base_addr</span><span>,</span></span>
<span><span>   pt_depth</span><span>,</span></span>
<span><span>   ptb_size)</span><span>;</span></span>
<span></span>
<span><span> // decode base PDB</span></span>
<span><span> pde_t</span><span> pde </span><span>=</span><span> decode_pde(pt_base_addr)</span><span>;</span></span>
<span><span> pt_base_addr </span><span>-=</span><span> fb_offset </span><span>*</span><span> !</span><span>pde</span><span>.</span><span>system;</span><span> // substract only on vram</span></span>
<span></span>
<span><span> u64 pt_last_byte_addr </span><span>=</span><span> pt_end_addr </span><span>+</span><span> 0x</span><span>fff</span><span>;</span><span> // 0xfff is 1 page</span></span>
<span><span> HDB_ASSERT(</span></span>
<span><span>   pt_start_addr </span><span>&lt;=</span><span> va_addr </span><span>||</span><span> va_addr </span><span>&lt;</span><span> pt_last_byte_addr</span><span>,</span></span>
<span><span>   "Invalid virtual address outside the range of the root page table of this vm"</span><span>)</span><span>;</span></span>
<span></span>
<span><span> va_addr </span><span>-=</span><span> pt_start_addr;</span></span>
<span><span> //</span></span>
<span><span> // Size of the first PDB depends on the total coverage of the</span></span>
<span><span> // page table and the PAGE_TABLE_BLOCK_SIZE.</span></span>
<span><span> // Entire table takes ceil(log2(total_vm_size)) bits</span></span>
<span><span> // All PDBs except the first one take 9 bits each</span></span>
<span><span> // The PTB covers at least 2 MiB (21 bits)</span></span>
<span><span> // And PAGE_TABLE_BLOCK_SIZE is log2(num 2MiB ranges PTB covers)</span></span>
<span><span> // As such, the formula for the size of the first PDB is:</span></span>
<span><span> //                       PDB1, PDB0, etc.      PTB covers at least 2 MiB</span></span>
<span><span> //                                        Block size can make it cover more</span></span>
<span><span> //   total_vm_bits - (9 * num_middle_pdbs) - (page_table_block_size + 21)</span></span>
<span><span> //</span></span>
<span><span> // we need the total range range here not the last byte addr like above</span></span>
<span><span> u32 total_vaddr_bits </span><span>=</span><span> log2_range_round_up(pt_start_addr</span><span>,</span><span> pt_end_addr </span><span>+</span><span> 0x</span><span>1000</span><span>)</span><span>;</span></span>
<span></span>
<span><span> u32 total_pdb_bits </span><span>=</span><span> total_vaddr_bits;</span></span>
<span><span> // substract everything from the va_addr to leave just the pdb bits</span></span>
<span><span> total_pdb_bits </span><span>-=</span><span> 9</span><span> *</span><span> (pt_depth </span><span>-</span><span> 1</span><span>);</span><span> // middle PDBs each is 9 bits</span></span>
<span><span> total_pdb_bits </span><span>-=</span><span> (ptb_size </span><span>+</span><span> 21</span><span>);</span><span>    // at least 2mb(21) bits + ptb_size</span></span>
<span></span>
<span><span> // u64 va_mask = (1ull &lt;&lt; total_pdb_bits) - 1;</span></span>
<span><span> // va_mask &lt;&lt;= (total_vaddr_bits - total_pdb_bits);</span></span>
<span></span>
<span><span> // pde_t pdes[8]  = { 0 };</span></span>
<span><span> // u32   curr_pde = 0;</span></span>
<span><span> // u64   pde_addr = 0;</span></span>
<span><span> // u64  loop_pde = pt_base_addr;</span></span>
<span></span>
<span><span> if</span><span> (pt_depth </span><span>==</span><span> 0</span><span>) { </span><span>HDB_ASSERT(</span><span>false</span><span>,</span><span> "DEPTH = 0 is not implemented yet"</span><span>)</span><span>; }</span></span>
<span></span>
<span><span> pde_t</span><span> curr_pde    </span><span>=</span><span> pde;</span></span>
<span><span> u64   entry_bits  </span><span>=</span><span> 0</span><span>;</span></span>
<span><span> s32   curr_depth  </span><span>=</span><span> pt_depth;</span></span>
<span><span> bool</span><span>  pde0_is_pte </span><span>=</span><span> false</span><span>;</span></span>
<span><span> // walk all middle PDEs</span></span>
<span><span> while</span><span> (curr_depth </span><span>&gt;</span><span> 0</span><span>) {</span></span>
<span><span>  // printf("pde(%u):0x%lx \n", curr_depth, curr_pde.pte_base_addr);</span></span>
<span><span>  u64 next_entry_addr </span><span>=</span><span> 0</span><span>;</span></span>
<span></span>
<span><span>  u32 shift_amount </span><span>=</span><span> total_vaddr_bits;</span></span>
<span><span>  shift_amount </span><span>-=</span><span> total_pdb_bits;</span></span>
<span><span>  // for each pdb shift 9 more</span></span>
<span><span>  shift_amount </span><span>-=</span><span> ((pt_depth </span><span>-</span><span> curr_depth) </span><span>*</span><span> 9</span><span>);</span></span>
<span></span>
<span><span>  // shift address and mask out unused bits</span></span>
<span><span>  u64 next_pde_idx </span><span>=</span><span> va_addr </span><span>&gt;&gt;</span><span> shift_amount;</span></span>
<span><span>  next_pde_idx </span><span>&amp;=</span><span> 0x</span><span>1ff</span><span>;</span></span>
<span></span>
<span><span>  // if on vram we need to apply this offset</span></span>
<span><span>  if</span><span> (</span><span>!</span><span>curr_pde</span><span>.</span><span>system) </span><span>curr_pde</span><span>.</span><span>pte_base_addr </span><span>-=</span><span> fb_offset;</span></span>
<span></span>
<span><span>  next_entry_addr </span><span>=</span><span> curr_pde</span><span>.</span><span>pte_base_addr </span><span>+</span><span> next_pde_idx </span><span>*</span><span> 8</span><span>;</span></span>
<span><span>  curr_depth</span><span>--</span><span>;</span></span>
<span></span>
<span><span>  if</span><span> (</span><span>!</span><span>curr_pde</span><span>.</span><span>system) {</span></span>
<span><span>   dev_linear_vram(dev</span><span>,</span><span> next_entry_addr</span><span>,</span><span> 8</span><span>,</span><span> &amp;</span><span>entry_bits)</span><span>;</span></span>
<span><span>   curr_pde </span><span>=</span><span> decode_pde(entry_bits)</span><span>;</span></span>
<span><span>   printf(</span></span>
<span><span>     "\tPage Dir Entry(</span><span>%u</span><span>):\n\t  Addr:0x</span><span>%lx</span><span>\n\t  Base: 0x</span><span>%lx</span><span>\n\n\t        ↓\n\n"</span><span>,</span></span>
<span><span>     curr_depth</span><span>,</span></span>
<span><span>     next_entry_addr</span><span>,</span></span>
<span><span>     curr_pde</span><span>.</span><span>pte_base_addr)</span><span>;</span></span>
<span><span>  } </span><span>else</span><span> {</span></span>
<span><span>   HDB_ASSERT(</span><span>false</span><span>,</span><span> "GTT physical memory access is not implemented yet"</span><span>)</span><span>;</span></span>
<span><span>  }</span></span>
<span></span>
<span><span>  if</span><span> (</span><span>!</span><span>curr_pde</span><span>.</span><span>valid) { </span><span>break</span><span>; }</span></span>
<span></span>
<span><span>  if</span><span> (</span><span>curr_pde</span><span>.</span><span>pte) {</span></span>
<span><span>   // PDB0 can act as a pte</span></span>
<span><span>   // also I'm making an assumption here that UMRs code doesn't make</span></span>
<span><span>   // that the the PDB0 as PTE path can't have the further bit set</span></span>
<span><span>   pde0_is_pte </span><span>=</span><span> true</span><span>;</span></span>
<span><span>   break</span><span>;</span></span>
<span><span>  }</span></span>
<span><span> }</span></span>
<span></span>
<span><span> if</span><span> (pde0_is_pte) { </span><span>HDB_ASSERT(</span><span>false</span><span>,</span><span> "PDE0 as PTE is not implemented yet"</span><span>)</span><span>; }</span></span>
<span></span>
<span><span> // page_table_block_size is the number of 2MiB regions covered by a PTB</span></span>
<span><span> // If we set it to 0, then PTB cover 2 MiB</span></span>
<span><span> // If it's 9 PTB cover 1024 MiB</span></span>
<span><span> // pde0_block_fragment_size tells us how many 4 KiB regions each PTE covers</span></span>
<span><span> // If it's 0 PTEs cover 4 KiB</span></span>
<span><span> // If it's 9 PTEs cover 2 MiB</span></span>
<span><span> // So the number of PTEs in a PTB is 2^(9+ptbs-pbfs)</span></span>
<span><span> //</span></span>
<span><span> // size here is actually the log_2 of the size</span></span>
<span><span> u32 pte_page_size  </span><span>=</span><span> curr_pde</span><span>.</span><span>frag_size;</span></span>
<span><span> u32 ptes_per_ptb   </span><span>=</span><span> 9</span><span> +</span><span> ptb_size </span><span>-</span><span> pte_page_size;</span></span>
<span><span> u64 pte_index_mask </span><span>=</span><span> (</span><span>1</span><span>ul</span><span> &lt;&lt;</span><span> ptes_per_ptb) </span><span>-</span><span> 1</span><span>;</span></span>
<span></span>
<span><span> u32 pte_bits_count   </span><span>=</span><span> pte_page_size </span><span>+</span><span> 12</span><span>;</span></span>
<span><span> u64 page_offset_mask </span><span>=</span><span> (</span><span>1</span><span>ul</span><span> &lt;&lt;</span><span> pte_bits_count) </span><span>-</span><span> 1</span><span>;</span><span> // minimum of 12</span></span>
<span></span>
<span><span> u64 pte_index </span><span>=</span><span> (va_addr </span><span>&gt;&gt;</span><span> pte_bits_count) </span><span>&amp;</span><span> pte_index_mask;</span></span>
<span><span> u64 pte_addr  </span><span>=</span><span> curr_pde</span><span>.</span><span>pte_base_addr </span><span>+</span><span> pte_index </span><span>*</span><span> 8</span><span>;</span></span>
<span></span>
<span><span> pte_t</span><span> pte </span><span>=</span><span> { </span><span>0</span><span> };</span></span>
<span><span> if</span><span> (</span><span>!</span><span>curr_pde</span><span>.</span><span>system) {</span></span>
<span><span>  dev_linear_vram(dev</span><span>,</span><span> pte_addr</span><span>,</span><span> 8</span><span>,</span><span> &amp;</span><span>entry_bits)</span><span>;</span></span>
<span><span>  pte </span><span>=</span><span> decode_pte(entry_bits)</span><span>;</span></span>
<span></span>
<span><span>  printf(</span><span>"\tPage Table Entry: 0x</span><span>%lx</span><span>\n"</span><span>,</span><span> pte</span><span>.</span><span>page_base_addr)</span><span>;</span></span>
<span><span> } </span><span>else</span><span> {</span></span>
<span><span>  HDB_ASSERT(</span><span>false</span><span>,</span><span> "GTT physical memory access is not implemented yet"</span><span>)</span><span>;</span></span>
<span><span> }</span></span>
<span></span>
<span><span> if</span><span> (</span><span>pte</span><span>.</span><span>further) { </span><span>HDB_ASSERT(</span><span>false</span><span>,</span><span> "PTE as PDE walking is not implemented yet"</span><span>)</span><span>; }</span></span>
<span><span> if</span><span> (</span><span>!</span><span>pte</span><span>.</span><span>system) </span><span>pte</span><span>.</span><span>page_base_addr </span><span>-=</span><span> fb_offset;</span></span>
<span></span>
<span><span> u64 offset_in_page </span><span>=</span><span> va_addr </span><span>&amp;</span><span> page_offset_mask;</span></span>
<span><span> u64 physical_addr  </span><span>=</span><span> pte</span><span>.</span><span>page_base_addr </span><span>+</span><span> offset_in_page;</span></span>
<span><span> printf(</span><span>"\tFinal Physical Address: 0x</span><span>%lx</span><span>\n"</span><span>,</span><span> physical_addr)</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>Other processes need to have a s_trap instruction or have trap on exception flags set, which is not true for most normal GPU processes. <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Available since RDNA3, if I’m not mistaken. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>VGPRs are unique per thread, and SGPRs are unique per wave <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
<li id="user-content-fn-4">
<p>We can get that by subtracting the current program counter from the address of the code buffer. <a href="#user-content-fnref-4" data-footnote-backref="" aria-label="Back to reference 4">↩</a></p>
</li>
</ol>
</section>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tsunami warning issued after 7.6-magnitude earthquake strikes Japan (133 pts)]]></title>
            <link>https://earthquake.usgs.gov/earthquakes/map/?currentFeatureId=us6000rtdt&amp;extent=-5.61599,111.26953&amp;extent=70.40735,173.14453</link>
            <guid>46193413</guid>
            <pubDate>Mon, 08 Dec 2025 15:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://earthquake.usgs.gov/earthquakes/map/?currentFeatureId=us6000rtdt&#x26;extent=-5.61599,111.26953&#x26;extent=70.40735,173.14453">https://earthquake.usgs.gov/earthquakes/map/?currentFeatureId=us6000rtdt&#x26;extent=-5.61599,111.26953&#x26;extent=70.40735,173.14453</a>, See on <a href="https://news.ycombinator.com/item?id=46193413">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
          The Latest Earthquakes application supports most recent browsers,
          <a href="https://angular.io/guide/browser-support" target="_blank">view supported browsers</a>.
        </p>
        <p>
          If the application does not load, try our
          <a href="https://earthquake.usgs.gov/legacy/map/" target="_blank"> legacy Latest Earthquakes application</a>.
        </p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[7.6 earthquake off the coast of Japan (135 pts)]]></title>
            <link>https://www.data.jma.go.jp/multi/quake/quake_detail.html?eventID=20251208232600&amp;lang=en</link>
            <guid>46193035</guid>
            <pubDate>Mon, 08 Dec 2025 15:05:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.data.jma.go.jp/multi/quake/quake_detail.html?eventID=20251208232600&#x26;lang=en">https://www.data.jma.go.jp/multi/quake/quake_detail.html?eventID=20251208232600&#x26;lang=en</a>, See on <a href="https://news.ycombinator.com/item?id=46193035">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div>

<header>
<div>
<h2><img id="icon_jma" src="https://www.data.jma.go.jp/multi/images/common/logo.gif"></h2>
<div>

<form>
  
</form>
</div>
</div>
</header>

<div id="globalMenuS">

<nav id="global-nav">
<ul>
<li><a id="tab_home" href="https://www.data.jma.go.jp/multi/index.html"></a></li>
<li><a id="tab_warn" href="https://www.data.jma.go.jp/multi/warn/index.html"></a></li>
<li><a id="tab_yoho" href="https://www.data.jma.go.jp/multi/yoho/index.html"></a></li>
<li><a id="tab_cyclon" href="https://www.data.jma.go.jp/multi/cyclone/index.html"></a></li>
<li><a id="tab_cloud" target="_blank" href="#"></a></li>
<li><a id="tab_kotan" target="_blank" href="#"></a></li>
<li><a id="tab_sub_landslide" target="_blank" href="#"></a>
</li><li><a id="tab_sub_inundation" target="_blank" href="#"></a>
</li><li><a id="tab_sub_flood" target="_blank" href="#"></a>
</li><li><a id="tab_tsunami" href="https://www.data.jma.go.jp/multi/tsunami/index.html"></a></li>
<li><a id="tab_quake" href="https://www.data.jma.go.jp/multi/quake/index.html"></a></li>
<li><a id="tab_volcano" href="https://www.data.jma.go.jp/multi/volcano/index.html"></a></li>
</ul>
</nav>
</div><!-- globalMenuS end-->

<main>
<div>

<div>
<ul>
<li><a id="breadcrumb_home" href="https://www.data.jma.go.jp/multi/index.html"></a></li>
<li><a id="breadcrumb_list" href="https://www.data.jma.go.jp/multi/quake/index.html"></a></li>
<li id="breadcrumb_detail"></li>
</ul>
</div>

<section>



<div>
<ul id="list_legend">
<li id="list_si"></li>
<li id="list_si7"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level07.png" alt="震度7"></li>
<li id="list_si6p"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level06_p.png" alt="震度6強"></li>
<li id="list_si6m"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level06_m.png" alt="震度6弱"></li>
<li id="list_si5p"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level05_p.png" alt="震度5強"></li>
<li id="list_si5m"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level05_m.png" alt="震度5弱"></li>
<li id="list_si4"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level04.png" alt="震度4"></li>
<li id="list_si3"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level03.png" alt="震度3"></li>
<li id="list_si2"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level02.png" alt="震度2"></li>
<li id="list_si1"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_level01.png" alt="震度1"></li>
<li id="list_epicenter"><img src="https://www.data.jma.go.jp/multi/images/quake/quake_center.png" alt="震央"></li>
</ul>
</div>

<article>

<table id="quakeindex_table">
	<tbody>
		<tr>
			<th id="quakeinfotable_col01" scope="col"></th>
			<th id="quakeinfotable_col02" scope="col"></th>
			<th id="quakeinfotable_col03" scope="col"></th>
			<th id="quakeinfotable_col04" scope="col"></th>
			<th id="quakeinfotable_col05" scope="col"></th>
			<th id="quakeinfotable_col06" scope="col"></th>
		</tr>
		</tbody>
</table>



<table id="quakedetail_index">
	<tbody>
		<tr>
			<th id="quakedetailtable_col01" scope="col">都道府県</th>
			<th id="quakedetailtable_col02" scope="col">震度</th>
			<th id="quakedetailtable_col03" scope="col">市町村名</th>
		</tr>
	</tbody>
</table>

</article>




</section>

</div>
</main>

<div id="pagetop"><p><a href="#top">topへ</a></p></div>
<center>この地図は、国土地理院長の承認を得て、同院発行の電子地図（タイル）を複製したものである。（承認番号　令元情複、第462号）</center>


<!-- /footer -->








</div>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uber starts selling ride/eats data to marketers (179 pts)]]></title>
            <link>https://www.businessinsider.com/uber-ads-launches-intelligence-insights-trips-takeout-data-marketers-2025-12</link>
            <guid>46192962</guid>
            <pubDate>Mon, 08 Dec 2025 15:00:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/uber-ads-launches-intelligence-insights-trips-takeout-data-marketers-2025-12">https://www.businessinsider.com/uber-ads-launches-intelligence-insights-trips-takeout-data-marketers-2025-12</a>, See on <a href="https://news.ycombinator.com/item?id=46192962">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-content-container="">

  
    
  
    

  <section>
    
    
    
    
      <section id="post-body" data-component-type="post-body" data-load-strategy="exclude" data-lock-content="">
            
            
            
            <div data-component-type="post-hero" data-load-strategy="exclude">
                
                <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                    <div>
                      <meta itemprop="contentUrl" content="https://i.insider.com/6936854671107c9f34577f3f?width=700">
                      <p><img src="https://i.insider.com/6936854671107c9f34577f3f?width=700" srcset="https://i.insider.com/6936854671107c9f34577f3f?width=400&amp;format=jpeg&amp;auto=webp 400w, https://i.insider.com/6936854671107c9f34577f3f?width=500&amp;format=jpeg&amp;auto=webp 500w, https://i.insider.com/6936854671107c9f34577f3f?width=700&amp;format=jpeg&amp;auto=webp 700w, https://i.insider.com/6936854671107c9f34577f3f?width=1000&amp;format=jpeg&amp;auto=webp 1000w, https://i.insider.com/6936854671107c9f34577f3f?width=1300&amp;format=jpeg&amp;auto=webp 1300w, https://i.insider.com/6936854671107c9f34577f3f?width=2000&amp;format=jpeg&amp;auto=webp 2000w" sizes="(min-width: 1280px) 900px" alt="uber" decoding="sync">
                    </p></div>
                
                  <span>
                        <span>
                          
                          <label for="caption-drawer-btn">
                            <svg role="img" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 24 24">
                              <path fill="currentColor" fill-rule="evenodd" d="m4.56 18.5 7.486-7.72 7.394 7.626 2.56-2.64L12.046 5.5 2 15.86l2.56 2.64Z"></path>
                            </svg>        </label>
                  
                          <figcaption data-e2e-name="image-caption">
                            <span>Uber offers ads in its app, on in-car screens, and on vehicles.</span>
                            <span>
                              <span data-e2e-name="image-source" itemprop="creditText">Artur Widak/NurPhoto via Getty Images</span>          </span>
                          </figcaption>
                        </span>
                  </span></figure>
            </div>
    
    
    
              
      
            
      
              
              
              
              <div data-component-type="post-summary-bullets" data-load-strategy="exclude" data-track-marfeel="post-summary-bullets">
                <ul>
                    <li>Uber Advertising is launching an insights platform for marketers called Uber Intelligence.</li>
                    <li>It has partnered with LiveRamp to aggregate users' data without revealing their identities.</li>
                    <li>Uber has said its ad business is on track to generate $1.5 billion in revenue this year.</li>
                </ul>
              </div>
      
            
            
            
            
            <section data-component-type="post-body-content" data-load-strategy="exclude" data-track-content="" data-post-type="story" data-track-marfeel="post-body-content">
            
                <p>Uber wants advertisers to level up their marketing by tapping into data on the millions of rides and deliveries its users order every day.</p><p>The ride-hailing giant's ad division is announcing the launch of a new insights platform called Uber Intelligence on Monday, the company exclusively told Business Insider.</p><p>Launched in partnership with the data-connectivity platform LiveRamp, Uber Intelligence will let advertisers securely combine their customer data with Uber's to help surface insights about their audiences, based on what they eat and where they travel.</p><p>It uses LiveRamp's <a target="_self" href="https://www.businessinsider.com/the-adtech-firms-leading-the-charge-in-how-advertising-data-is-used-2021-7#liveramp-6" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">clean room technology</a>, which lets companies aggregate their data in a privacy-safe environment, without sharing or seeing each other's raw or personally identifiable customer information.</p><p>A hotel brand could use Uber Intelligence to help identify which restaurants or entertainment venues it might want to partner with for its loyalty program, for example.</p><p>Uber also hopes the platform can act as a flywheel for its broader ad business. Marketers can use the data clean room for segmentation, such as identifying customers who are heavy business travelers, then targeting them with ads on their next trip to the airport in the Uber app or on screens inside Uber cars.</p><p>"That seamlessness is why we're so excited," Edwin Wong, global head of measurement at Uber Advertising, told Business Insider in an interview. He added that the aim is for marketers to begin saying, "'Oh, I'm not just understanding Uber, I'm understanding Uber in my marketing context.'"</p><h2 id="42ba9d9a-79c8-4251-aa36-0bf02422f1ea" data-toc-id="42ba9d9a-79c8-4251-aa36-0bf02422f1ea">Uber's other route to revenue</h2><p>Uber Intelligence is the latest step in the evolution of <a target="_self" href="https://www.businessinsider.com/uber-ads-plans-1-billion-revenue-2024-investor-presentation-2022-2" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Uber's ad business</a>. Uber officially launched its dedicated advertising division in 2022. It offers an array of <a target="_self" href="https://www.businessinsider.com/uber-ads-plans-1-billion-revenue-2024-investor-presentation-2022-2" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">ad formats</a> in the Uber and Uber Eats apps, on in-car tablets, in emails to its users, and on car tops.</p><p>The company said in May that its ad business had reached a $1.5 billion revenue run rate — the figure it has projected to hit by the end of 2025 — which would represent a 60% increase on last year. The company doesn't break out a more specific ad-revenue figure and hasn't provided an update on the run-rate number since May.</p><p>Uber Intelligence forms part of a bespoke set of services it offers its top advertisers. Earlier this year, it launched a <a target="_self" href="https://www.businessinsider.com/uber-offers-ads-let-brands-pay-for-users-rides-2025-6" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">creative studio </a>where brands can partner with Uber to deliver more bespoke campaigns, such as offering rides to <a target="_self" rel="" href="https://www.businessinsider.com/miami-grand-prix-distinct-features-f1-race-marina-2024-5" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}"><u>Miami F1 Grand Prix attendees</u></a> in a luxury vehicle sponsored by La Mer, packed with freebie skincare products.</p><p>Andrew Frank, analyst at the research firm Gartner, said the launch of Uber Intelligence is another signal that Uber's ad business is maturing.</p><p>"Early-stage ad businesses tend to focus exclusively on selling inventory while more mature ones focus more on delivering differentiated value through targeting and measurement solutions that help brands understand and optimize the impact of their spend," Frank told Business Insider.</p><p>Uber's unique source of "terrestrial data" put it in good standing against the likes of Amazon, Google, and other retail media networks that emphasize the value of their data-driven insights, Frank added. However, he said Uber may need to address privacy concerns related to aggregating highly sensitive data in order to maintain consumer trust and to comply with evolving global regulators as a collector of first-party data.</p><p>Vihan Sharma, chief revenue officer of LiveRamp, said its platform provides technical guarantees to ensure "zero movement of data."</p><p>"The whole objective of a clean room technology is to build trust between data owners and consumers and the advertising ecosystem," Sharma said.</p>
            
            
            </section>
            
            
            
            
            
            
    
    
    
    
      </section>

    
    <!-- Included desktop "post-aside" -->  

    
      
      <section data-component-type="post-bottom" data-load-strategy="exclude" data-track-marfeel="post-bottom">
        <section>
    
    
    
          
          
          
          <div data-component-type="post-category-tags" data-load-strategy="lazy" data-track-marfeel="post-category-tags">
            <ul data-track-click-shared="{&quot;product_field&quot;:&quot;bi_value_unassigned&quot;,&quot;event&quot;:&quot;navigation&quot;,&quot;element_name&quot;:&quot;category_link&quot;}">
                
                <li>
                  <a data-track-click="" href="https://www.businessinsider.com/category/uber" title="Uber">Uber</a>
                </li>      
                <li>
                  <a data-track-click="" href="https://www.businessinsider.com/category/exclusive" title="Exclusive">Exclusive</a>
                </li>
          
            </ul>
          </div>
    
            
              
              
              <section data-component-type="dad-related-posts" data-delay-third-party-scripts="true" data-size="4" data-min-size="3" data-container-index="" data-included-verticals="advertising" data-placement="post-bottom" data-track-marfeel="dad-related-posts-post-bottom" data-excluded-verticals="bi-video" data-root-margin="250px 0px" data-track-view="{&quot;element_name&quot;:&quot;end_of_article_recirc&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;,&quot;subscription_experience&quot;:&quot;bi_value_unassigned&quot;}">
                  <p>
                    <h2>
                      Read next
                    </h2>
                  </p>
            
                
              </section>
        </section>
    
        
    
          <section data-track-page-area="Post Bottom">
          <!-- Included desktop "taboola" -->    <vendor-taboola data-component-type="vendor-taboola" data-root-margin="0px 0px 100% 0px" data-consent="MARKETING" config="{&quot;providerName&quot;:&quot;taboola&quot;,&quot;providerPageType&quot;:{&quot;article&quot;:&quot;auto&quot;},&quot;providerUrl&quot;:&quot;//cdn.taboola.com/libtrc/businessinsider/loader.js&quot;,&quot;providerFlushValue&quot;:{&quot;flush&quot;:true},&quot;providerData&quot;:{&quot;mode&quot;:&quot;thumbs-1r&quot;,&quot;container&quot;:&quot;taboola-below-main-column&quot;,&quot;placement&quot;:&quot;below-main-column&quot;,&quot;onlyOn&quot;:&quot;desktop&quot;,&quot;target_type&quot;:&quot;mix&quot;}}" data-load-strategy="defer">
                
              </vendor-taboola>
          
          <!-- Excluded mobile "taboola" --></section>
            
            
      </section>
  </section>

  
  


  <back-to-home data-component-type="back-to-home" data-load-strategy="defer" data-only-on="mobile">
  
    
  
    
  </back-to-home></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Strong earthquake hits northern Japan, tsunami warning issued (125 pts)]]></title>
            <link>https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/</link>
            <guid>46192846</guid>
            <pubDate>Mon, 08 Dec 2025 14:50:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/">https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/</a>, See on <a href="https://news.ycombinator.com/item?id=46192846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      
                    
                  <p>A strong earthquake has struck northern Japan. The quake struck off the eastern coast of Aomori Prefecture at 11:15 p.m. on Monday. Its focus was 54 kilometers deep, and the magnitude is estimated at 7.5.</p>
      
                    
                <h3>Strong tremors felt across the region</h3>
      
                    
                  <p>The Japan Meteorological Agency has downgraded the magnitude of the quake centered off the Pacific coast in Aomori Prefecture to 7.5 from 7.6.</p>
        
                    
                  <p>The tremor struck at 11:15 p.m. on Monday. The depth has also been adjusted to 54 kilometers, from an initial estimate of 50 kilometers.</p>
        
                    
                  <p>Tremors with an intensity of upper 6 on the Japanese intensity scale of 0 to 7 were observed in the city of Hachinohe in Aomori Prefecture.</p>
                                              
                                          
                  <p><em>A hotel employee in Hachinohe City said: </em>It seems there are multiple injured people. Everyone appears to be conscious.</p>
        
                    
                  <p>If you are in these areas, try to remain in a safe place and protect yourself. Use extreme caution if you must move.  Damage around you could be heavy. Stay alert. More tremors are possible</p>
      
                    
                <h3>Tsunami warnings downgraded to advisories</h3>
      
                    
                  <p>The Meteorological Agency issued a tsunami warning for the Pacific coastline in northern Japan, including Iwate Prefecture, and parts of Hokkaido and Aomori prefectures late Monday night. But the agency has downgraded tsunami warnings to advisories for coastal areas in the Hokkaido and Tohoku regions. Officials say people should still stay away from bodies of water.</p>
                                              
                                          
                  <p><em>The agency says: </em>it is the first time the agency has issued a tsunami warning since July, when a powerful quake off Kamchatka, Russia, prompted it to issue one for Japan's Pacific coastal areas.</p>
        
                    
                  <p>Tsunami advisories are in place for parts of Hokkaido and Aomori Prefecture, Iwate, Miyagi and Fukushima prefectures.</p>
        
                    
                  <p>The agency is calling on people to stay away from the coastline, as well as the mouths of rivers.</p>
        
                    
                  <p>According to authorities, long-period ground motions were recorded during the Monday earthquake.</p>
        
                    
                  <p>Long-period ground motions are slow, large-amplitude seismic waves with frequencies of 2 seconds or longer that occur during a large earthquake. Such motions are known to have a significant impact on high-rise buildings.</p>
        
                    
                  <p>Strong long-period motions, classified class-3, the second highest in the 4-level scale were observed in the village of Rokkasho in Aomori Prefecture. Such class-3 waves are strong enough to make it difficult for people in a high-rise building to stand up.</p>
      
                    
                <h3>Residents ordered to evacuate</h3>
      
                    
                  <p>After tsunami warnings were issued, some municipalities in Hokkaido, and the Tohoku region issued evacuation orders to residents.</p>
      
                    
                <h3>Traffic disrupted</h3>
      
                    
                  <p>East Japan Railway Company says that as of Tuesday, outbound trains on the Tohoku Shinkansen have been suspended between Fukushima and Shin-Aomori stations due to the earthquake. The company says three trains stopped in this section.</p>
        
                    
                  <p>The company says that it is checking for any damage to railway tracks and that it remains unclear when services will resume.</p>
        
                    
                  <p>The Morioka branch of East Japan Railway says that as of midnight on Tuesday, services on the Tohoku Main Line were suspended in Iwate Prefecture.</p>
        
                    
                  <p>It says two trains made emergency stops. It remains unclear when services will resume. There are no reports of injuries.</p>
        
                    
                  <p>As for Hokkaido, the operator of its busiest airport, New Chitose Airport near Sapporo, says that as of 11:40 p.m. on Monday, it was checking whether there are any abnormalities on two runways.</p>
        
                    
                  <p>Highways have been affected. East Nippon Expressway Company says that as of 11:45 p.m. on Monday, traffic was completely stopped between the Shiraoi and Shinchitose Airport Interchanges and between the Tomakomai Higashi and Numanohata Nishi Interchanges.</p>
      
                    
                <h3>Power Companies: No abnormalities at nuclear plants</h3>
      
                    
                  <p>Tokyo Electric Power Company says it has confirmed that there are no abnormalities at the Fukushima Daiichi and Daini nuclear plants.</p>
        
                    
                  <p>The company says it halted the release of treated and diluted water from the Fukushima Daiichi nuclear power plant at 11:42 pm on Monday, as per predetermined procedures.</p>
        
                    
                  <p>The facility suffered a triple meltdown during the March 2011 earthquake and tsunami.  The water used to cool molten fuel has been mixing with rain and groundwater.</p>
        
                    
                  <p>That has been treated to remove most radioactive substances, except tritium. It's then diluted, reducing levels of tritium to well below the World Health Organization's guidance for drinking water, before it is released into the ocean.</p>
        
                    
                  <p>TEPCO also ordered some employees at the facility to evacuate. There have been no reports so far of injuries at the nuclear power plant.</p>
        
                    
                  <p>Tohoku Electric Power Company says no abnormalities have been detected at the Higashidori nuclear power plant in Aomori Prefecture and the Onagawa plant in Miyagi Prefecture.</p>
        
                    
                  <p>Hokkaido Electric Power Company says no problems have been found at the Tomari nuclear power plant in the prefecture.</p>
      
                    
                <h3>Government bracing for damages</h3>
      
                    
                  <p>The Japanese government set up a task force at the crisis management center in the prime minister's office at 11:16 p.m. on Monday in response to the earthquake.</p>
        
                    
                  <p>Prime Minister Takaichi Sanae entered the prime minister's office shortly after 11:50 p.m.</p>
        
                    
                  <p>She instructed the government to immediately provide information on any tsunami and evacuation orders to the people in an appropriate manner, take thorough measures to prevent harm, such as evacuating residents, and get a grasp of the extent of damage as soon as possible.</p>
                                              
                                          
                  <p><em>Takaichi: </em>The central government will work closely with local governments and make the utmost effort to carry out measures, such as emergency response, including rescue for the affected people.</p>
        
                    
                  <p>Chief Cabinet Secretary Kihara Minoru held a news conference on Tuesday. Kihara said the government continues to assess the extent of the damage.</p>
        
                    
                  <p>He added that the government is devoting all its efforts to disaster prevention measures, with rescue and relief efforts as its top priority, led by the police, fire departments, Self-Defense Forces, and Japan Coast Guard.</p>
        
                    
                  <p>The Japan Meteorological Agency will soon hold a news conference on the earthquake. It is expected to explain what precautions should be taken in quake-hit areas.</p>
      
                    
                <h3>Expert view on the quake</h3>
                                            
                                          
                  <p><em>Sakai Shinichi, professor at the Earthquake Research Institute of the University of Tokyo, says: </em>If this was a shallow earthquake centered in the sea, there is a high possibility that a tsunami has already occurred. People should stay away from the coast. It is important to evacuate and to take measures to stay warm.</p>
                                              
                                          
                  <p><em>Sakai says: </em>The epicenter may be north of the epicenter area of the 2011 Great East Japan Earthquake. This time, the earthquake is believed to have occurred at the plate boundary, so I think it was a slightly larger earthquake. The magnitude could be revised in the future.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Increases Office 365 and Microsoft 365 License Prices (118 pts)]]></title>
            <link>https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/</link>
            <guid>46192186</guid>
            <pubDate>Mon, 08 Dec 2025 13:49:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/">https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/</a>, See on <a href="https://news.ycombinator.com/item?id=46192186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

			<main id="main" role="main">
				
<article id="post-71757">

	<!-- .entry-header -->
	
		
	<div>
		
<div id="ez-toc-container">
<p>Table of Contents</p>
<nav><ul><li><a href="#New_Microsoft_365_Pricing_Goes_into_Effect_on_July_1_2026">New Microsoft 365 Pricing Goes into Effect on July 1, 2026</a></li><li><a href="#Last_Microsoft_365_License_Increase_in_2022">Last Microsoft 365 License Increase in 2022</a></li><li><a href="#Justifying_the_Additional_Cost">Justifying the Additional Cost</a></li><li><a href="#So_Many_New_Features">So Many New Features</a></li><li><a href="#A_Question_of_Value">A Question of Value</a></li></ul></nav></div>
<h2><span id="New_Microsoft_365_Pricing_Goes_into_Effect_on_July_1_2026"></span>New Microsoft 365 Pricing Goes into Effect on July 1, 2026<span></span></h2>



<p>On December 4, 2025, Microsoft <a href="https://www.microsoft.com/en-us/microsoft-365/blog/2025/12/04/advancing-microsoft-365-new-capabilities-and-pricing-update/?WT.mc_id=M365-MVP-9501" target="_blank" rel="noreferrer noopener">announced a range of price increases</a> for Microsoft 365 monthly licenses. The new pricing (Figure 1) goes into effect from July 1, 2026, the start of Microsoft’s FY27 fiscal year.</p>






<div>
<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="732" height="831" data-attachment-id="71758" data-permalink="https://office365itpros.com/2025/12/08/microsoft-365-pricing-increase/microsoft-365-prices-july-1-2026/" data-orig-file="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?fit=732%2C831&amp;ssl=1" data-orig-size="732,831" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Microsoft 365 Prices July 1 2026" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?fit=264%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?fit=732%2C831&amp;ssl=1" src="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?resize=732%2C831&amp;ssl=1" alt="Microsoft 365 License pricing from July 1, 2026 (source: Microsoft)." srcset="https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?w=732&amp;ssl=1 732w, https://i0.wp.com/office365itpros.com/wp-content/uploads/2025/12/Microsoft-365-Prices-July-1-2026.jpg?resize=264%2C300&amp;ssl=1 264w" sizes="(max-width: 732px) 100vw, 732px"><figcaption>Figure 1: Microsoft 365 License pricing from July 1, 2026 (source: Microsoft)</figcaption></figure>
</div>


<p>According to Microsoft, they want to “<em>give customers ample time to plan</em>.” However, there’s not much choice for tenants if their operations are embedded in the Microsoft 365 ecosystem, so this is a case of “<em>getting used to new pricing</em>” rather than “<em>having time to consider migrating away from Microsoft 365.</em>” Once you’re embedded in the Microsoft 365 ecosystem, it’s hard to leave.</p>



<p>Some organizations do consider going back to on-premises servers. It’s certainly an option, even to the now available and oddly named <a href="https://learn.microsoft.com/en-us/azure/azure-local/concepts/microsoft-365-local-overview?view=azloc-2511&amp;WT.mc_id=M365-MVP-9501" target="_blank" rel="noreferrer noopener">Microsoft 365 Local</a>, a product that shares precisely nothing but its name with the rest of the Microsoft 365 ecosystem.</p>



<h2><span id="Last_Microsoft_365_License_Increase_in_2022"></span>Last Microsoft 365 License Increase in 2022<span></span></h2>



<p>Microsoft last increased <a href="https://practical365.com/microsoft-increase-prices-for-office-365-microsoft-365-march-2022/" target="_blank" rel="noopener">Microsoft 365 license prices in March 2022</a>. At the time, Microsoft added $3/monthly to Office 365 E3m and E5, and $4/monthly to Microsoft 365 E3. The Microsoft 365 E5 price was left unchanged.</p>



<p>This time round, the monthly increases range from zero (Office 365 E1) to $3 (the big plans used by large enterprises like Office 365 E3 and Microsoft 365 E5). At $2/average across the Microsoft 365 base (around 446 million paid seats based on data provided at <a href="https://office365itpros.com/2025/11/03/office-365-for-it-pros-125/" target="_blank" rel="noreferrer noopener">Microsoft’s FY26 Q1 earnings</a>), the increase could bring in an extra $10.7 billion. The price changes shown in Figure 1 apply to the commercial cloud. Equivalent increases apply to other sectors, such as education and government.</p>



<p>In FY26 Q1, the Microsoft Cloud operated at a <a href="https://view.officeapps.live.com/op/view.aspx?src=https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/TranscriptFY26Q1" target="_blank" rel="noreferrer noopener">healthy 68% operating margin</a>, so it’s not as if Microsoft does not achieve an adequate return from Microsoft 365. However, as noted in the earnings transcript, the operating margin for the Microsoft Cloud is down year-over-year due to “investments in AI.” One interpretation is that the extra $10 billion from the price increases will offset some of the red ink Microsoft is bleeding because of the investments they’re making in datacenter capacity, hardware, and software needed to make Copilot useful,</p>



<h2><span id="Justifying_the_Additional_Cost"></span>Justifying the Additional Cost<span></span></h2>



<p>Just like last time around, Microsoft justifies the increase by pointing to an array of new features and functionality that they’ve delivered. Microsoft 365 E5 customers recently received news that they will soon get Security Copilot, and another announcement revealed that the Microsoft 365 E3 and E5 plans will both <a href="https://techcommunity.microsoft.com/blog/microsoftintuneblog/microsoft-365-adds-advanced-microsoft-intune-solutions-at-scale/4474272?WT.mc_id=M365-MVP-9501" target="_blank" rel="noreferrer noopener">gain functionality from the Microsoft Intune Suite in the coming months</a>.</p>



<p>Plans that don’t get Security Copilot or the Intune Suite must do with new apps like Microsoft Loop, Clipchamp, and Places, all introduced since the 2022 price increase. Good as these apps are, a tenant has to use them to extract value to justify the additional cost,. A welcome change is the addition of Microsoft 365 Defender for Office 365 P1 to Office 365 E3 and Microsoft 365 E3, even if this might provoke further worry about i<a href="https://office365itpros.com/2025/11/25/microsoft-defender-for-office-365-3/" target="_blank" rel="noreferrer noopener">ncurring cost to license shared mailboxes that benefit from Defender functionality</a>.</p>



<h2><span id="So_Many_New_Features"></span>So Many New Features<span></span></h2>



<p>Curiously, the blog highlights the release of 1,100 new features in the last year across “<em>Microsoft 365, Copilot, and SharePoint</em>.” I thought SharePoint was a core part of Microsoft 365, but apparently, it’s so important that SharePoint deserves its own mention. Teams just doesn’t get a mention these days. I also wonder how many of the new features are related to Copilot and are therefore useless to tenants that don’t use Copilot.</p>



<p>By comparison, in 2022, Microsoft claimed <a href="https://www.microsoft.com/en-us/microsoft-365/blog/2021/08/19/new-pricing-for-microsoft-365/" target="_blank" rel="noreferrer noopener">the release of 1,400 new features</a> in communication and collaboration (aka Teams), security and compliance, and AI and automation (not Copilot!). At the time, I asked how many of the updates were useful. The same could be asked now. Quantity of updates pushed out in a never-ending stream is no substitute for usefulness or quality.</p>



<h2><span id="A_Question_of_Value"></span>A Question of Value<span></span></h2>



<p>I’m unsure if any organization can use all the functionality bundled into Microsoft 365. It’s a feature-rich environment with lots to recommend it. I worry about quality of software, the pace of change, the way that Microsoft relentlessly pushes AI at every opportunity, and poor communication about the value of changes at times.</p>



<p>Overall, Microsoft 365 remains very a competitive offering, even if the basic enterprise license is now $312/user/year and the headline E5 license a whopping $720/user/year. Then again, it wasn’t too long ago since a shrink-wrapped copy of Office cost over $300, so perhaps the cost isn’t so bad after all. Either way, I’m sure the increases will cause tenants to devote some time to study their current license mix and allocation to see if any savings are possible (the <a href="https://office365itpros.com/2024/09/12/microsoft-365-licensing-report-194/" target="_blank" rel="noreferrer noopener">Microsoft 365 licensing report script</a> might be useful here).</p>







<hr>







<p>Support the work of the Office 365 for IT Pros team by subscribing to the <a href="https://o365itpros.gumroad.com/l/O365IT/" target="_blank" rel="noreferrer noopener">Office 365 for IT Pros</a> eBook. Your support pays for the time we need to track, analyze, and document the changing world of Microsoft 365 and Office 365. Only humans contribute to our work!</p>
			
	</div><!-- .entry-content -->
	
		
	<!-- .entry-footer -->
</article><!-- #post-## -->
				
				
				
				
<!-- #comments -->

			</main><!-- #main -->
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM to Acquire Confluent (215 pts)]]></title>
            <link>https://www.confluent.io/blog/ibm-to-acquire-confluent/</link>
            <guid>46192130</guid>
            <pubDate>Mon, 08 Dec 2025 13:43:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.confluent.io/blog/ibm-to-acquire-confluent/">https://www.confluent.io/blog/ibm-to-acquire-confluent/</a>, See on <a href="https://news.ycombinator.com/item?id=46192130">Hacker News</a></p>
Couldn't get https://www.confluent.io/blog/ibm-to-acquire-confluent/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Flow: Actor-based language for C++, used by FoundationDB (118 pts)]]></title>
            <link>https://github.com/apple/foundationdb/tree/main/flow</link>
            <guid>46191763</guid>
            <pubDate>Mon, 08 Dec 2025 13:08:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/apple/foundationdb/tree/main/flow">https://github.com/apple/foundationdb/tree/main/flow</a>, See on <a href="https://news.ycombinator.com/item?id=46191763">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <nav aria-label="Global">
              <ul>
                  <li>
      

      <div>
        <div>
            <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>
          GitHub Copilot

        </p><p>

        Write better code with AI
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
        <p>
          GitHub Spark

            <span>
              New
            </span>
        </p><p>

        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
        <p>
          GitHub Models

            <span>
              New
            </span>
        </p><p>

        Manage and compare prompts
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
        <p>
          GitHub Advanced Security

        </p><p>

        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>
          Actions

        </p><p>

        Automate any workflow
      </p></div>

    
</a></li>

                  </ul>
                </div>
            <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>
          Codespaces

        </p><p>

        Instant dev environments
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>
          Issues

        </p><p>

        Plan and track work
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>
          Code Review

        </p><p>

        Manage code changes
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_platform_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>
          Discussions

        </p><p>

        Collaborate outside of code
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_platform_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>
          Code Search

        </p><p>

        Find more, search less
      </p></div>

    
</a></li>

                  </ul>
                </div>
            
        </div>

          <p>
            <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_features&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}" href="https://github.com/features">
              View all features
              
</a>          </p>
      </div>
</li>


                  <li>
      

      
</li>


                  <li>
      

      <div>

                      <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                      <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://github.com/resources/events">
      Events &amp; Webinars

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://github.com/partners">
      Partners

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                  </ul>
                </div>
</li>


                  <li>
      

      <div>
                <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>
          GitHub Sponsors

        </p><p>

        Fund open source developers
      </p></div>

    
</a></li>

                  </ul>
                </div>
                <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>
          The ReadME Project

        </p><p>

        GitHub community articles
      </p></div>

    
</a></li>

                  </ul>
                </div>
                
            </div>
</li>


                  <li>
      

      <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>
          Enterprise platform

        </p><p>

        AI-powered developer platform
      </p></div>

    
</a></li>

                  </ul>
                </div>
</li>


                  <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;platform&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;platform_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

              </ul>
            </nav>

        <div>
                


<qbsearch-input data-scope="repo:apple/foundationdb" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="VzGtbcoHR5NVfM3UEW4NeMWaGPCfTmFALHa-mZNBibPV9WIuNfySNvIrjewVzaPvAENZYtp5GPvO0CyO8Iqddg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="apple/foundationdb" data-current-org="apple" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Ffiles%2Fdisambiguate&amp;source=header-repo&amp;source_repo=apple%2Ffoundationdb" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/apple/foundationdb/tree/main/flow&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="adc6c520096aba4b9c312d1bbdb738f44da328c673bf7d9b59e937ae8e6f4a20" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/files/disambiguate;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-b53e156c-6526-4250-a31d-3ddcc5fe46e3" for="icon-button-bc02474f-9988-48de-b047-100fc93266f0" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.bda088c061b17a984ea2.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.753d458774a2f782559b.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bad Dye Job (176 pts)]]></title>
            <link>https://daringfireball.net/2025/12/bad_dye_job</link>
            <guid>46191194</guid>
            <pubDate>Mon, 08 Dec 2025 11:47:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daringfireball.net/2025/12/bad_dye_job">https://daringfireball.net/2025/12/bad_dye_job</a>, See on <a href="https://news.ycombinator.com/item?id=46191194">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="Box">



<p>In <a href="https://daringfireball.net/linked/2025/12/03/alan-dye-leaves-apple-for-meta">my post earlier today</a> on the then-breaking news that Alan Dye has left Apple to join Meta as chief design officer (<a href="https://www.meta.com/media-gallery/executives/">a new title at the company</a><sup id="fnr1-2025-12-03"><a href="#fn1-2025-12-03">1</a></sup>), I wrote:</p>

<blockquote>
  <p>It sounds like Dye chose to jump ship, and wasn’t squeezed out (as
it seems <a href="https://daringfireball.net/linked/2025/12/01/giannandrea-out">with former AI chief John Giannandrea</a> earlier this
week). Gurman/Bloomberg are spinning this like a coup for Meta
(headline: “<a href="https://www.bloomberg.com/news/articles/2025-12-03/apple-design-executive-alan-dye-poached-by-meta-in-major-coup">Apple Design Executive Alan Dye Poached by Meta in
Major Coup</a>”), but I think this is the best personnel news at Apple
in decades. Dye’s decade-long stint running Apple’s software
design team has been, on the whole, terrible — and rather than
getting better, the problems have been getting worse.</p>
</blockquote>

<p>Dye’s replacement at Apple is longtime Apple designer Stephen Lemay. I’ve never met Lemay (or at least can’t recall meeting him), and prior to today never heard much about him. But that’s typical for Apple employees. Part of the job working for Apple is remaining under the radar and out of the public eye. What I’ve learned today is that Lemay, very much unlike Dye, is a career interface/interaction designer. Sources I’ve spoken to who’ve worked with Lemay at Apple speak highly of him, particularly his attention to detail and craftsmanship. Those things have been sorely lacking in the Dye era. Not everyone loves everything Lemay has worked on, but nobody bats 1.000 and designers love to critique each other’s work. I’ve chatted with people with criticisms of specific things Lemay has worked on or led at Apple (e.g. aspects of iPadOS multitasking that struck many of us as deliberately limiting, rather than empowering), but <em>everyone</em> I’ve spoken to is happy — if not downright giddy — at the news that Lemay is replacing Dye. Lemay is well-liked personally and deeply respected talent-wise. Said one source, in a position to know the choices, “I don’t think there was a better choice than Lemay.”</p>

<p>The sentiment within the ranks at Apple is that today’s news is almost too good to be true. People had given up hope that Dye would ever get squeezed out, and no one expected that he’d just up and leave on his own. (If you care about design, there’s nowhere to go but down after leaving Apple. What people overlooked is the obvious: Alan Dye doesn’t actually care about design.)</p>

<p>What I struggled with in the wake of today’s news is how to square the following contradiction:</p>

<ul>
<li><p>Dye apparently left for Meta on his own; he wasn’t squeezed out.</p></li>
<li><p>Apple replacing Dye with Lemay seemingly signals a significant shift in direction, replacing a guy whose approach was almost entirely superficial/visual with a guy who’s spent his entire career sweating actual interaction details.</p></li>
</ul>

<p>If Apple’s senior leadership would have been happy to have Dye remain as leader of Apple’s software design teams, why didn’t they replace him with a Dye acolyte? Conversely, if the decision makers at Apple saw the need for a directional change, why wasn’t Dye pushed out?<sup id="fnr2-2025-12-03"><a href="#fn2-2025-12-03">2</a></sup></p>

<p>The answer, I think, is that the decision to elevate Lemay wasn’t about direction, but loyalty. Why risk putting in a Dye-aligned replacement when that person might immediately get poached too? We know, from this year’s AI recruitment battles, that Zuckerberg is willing to throw <a href="https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/">almost unfathomable sums of money</a> to poach talent he wants to hire from competitors. Gurman reported that Billy Sorrentino, a Dye deputy who has served as a senior director of design at Apple since 2016, is leaving for Meta with Dye.<sup id="fnr3-2025-12-03"><a href="#fn3-2025-12-03">3</a></sup> I don’t have any other names, but word on the street is that other members of Dye’s inner circle are leaving Apple for Meta with him. But those who remain — or who might remain, if they’d have been offered the promotion to replace Dye — simply can’t be trusted from the perspective of senior leadership, who were apparently blindsided by Dye’s departure for Meta. They wouldn’t have given Dye a prime spot in the WWDC keynote if they thought he might be leaving within months.</p>

<p>So the change in direction we may see — that many of us desperately <em>hope</em> to see — under Lemay’s leadership might be happenstance. More a factor of Lemay being politically safe, as someone predating Dye and outside Dye’s inner circle at Apple, than from Tim Cook or anyone else in senior leadership seeing a <em>need</em> for a directional change in UI design. But happenstance or not, it could be the best thing to happen to Apple’s HI design in the entire stretch since Steve Jobs’s passing and Scott Forstall’s ouster.</p>

<p>Putting Alan Dye in charge of user interface design was the one big mistake Jony Ive made as Apple’s Chief Design Officer.<sup id="fnr4-2025-12-03"><a href="#fn4-2025-12-03">4</a></sup> Dye had no background in user interface design — he came from a brand and print advertising background. Before joining Apple, <a href="https://thenextweb.com/news/how-alan-dye-went-from-iphone-box-designer-to-apples-head-of-ui">he was design director for the fashion brand Kate Spade</a>, and before that worked on branding for the ad agency Ogilvy. His promotion to lead Apple’s software interface design team under Ive happened in 2015, when Apple was launching Apple Watch, their closest foray into the world of fashion. It might have made some sense to bring someone from the fashion/brand world to lead software design for Apple Watch, but it sure didn’t seem to make sense for the rest of Apple’s platforms. And the decade of Dye’s HI leadership has proven it.</p>

<p>The most galling moment in Dye’s entire tenure was <a href="https://www.youtube.com/watch?v=H3KnMyojEQU">the opening of this year’s iPhone event keynote in September</a>, which began with a title card showing the <a href="https://daringfireball.net/linked/2007/01/23/how-it-works">oft-cited Jobs quote</a> “Design is not just what it looks like and feels like. Design is how it works.” The whole problem with the Dye era of HI design at Apple is that it has so largely — not entirely, but largely — been driven purely by how things look. There are a lot of things in Apple’s software — <a href="https://daringfireball.net/linked/2025/11/07/tahoes-terrible-icons">like app icons</a> — that don’t even look good any more. But it’s the “how it works” part that has gone so horribly off the rails. Alan Dye seems like <em>exactly</em> the sort of person Jobs was describing in the first part of that quote: “People think it’s this veneer — that the designers are handed this box and told, ‘Make it look good!’”</p>

<p>I am not a Liquid Glass hater. I actually think, on the whole, iOS 26 is a better and more usable UI than iOS 18. But MacOS 26 Tahoe is a mess, visually, and I’m not sure there’s a single thing about its UI that is better than MacOS 15 Sequoia. There are <a href="https://sixcolors.com/post/2025/09/macos-26-tahoe-review-power-under-glass/">new software features in Tahoe</a> that are excellent and serve as legitimate enticements to upgrade. But I’m talking about the user interface — the work from Alan Dye’s HI team, not Craig Federighi’s teams. I think the fact that Liquid Glass is worse on MacOS than it is on iOS is not just a factor of iOS being Apple’s most popular, most profitable, most important platform — and thus garnering more of Apple’s internal attention. I think it’s also about the fact that the Mac interface, with multiple windows, bigger displays, and more complexity, demands more nuanced, more expert, interaction design skills. Things like depth, layering, and unambiguous indications of input focus are important aspects of any platform. But they’re more important on the platform which, by design, shoulders more complexity. Back in 2010, predicting a bright future for the Mac at a time when many pundits were thinking Apple would soon put the entire platform out to pasture, I wrote, “<a href="https://daringfireball.net/2010/12/future_of_the_mac_in_an_ios_world">It’s the heaviness of the Mac that allows iOS to remain light</a>.” That remains as true today as it was 15 years ago. But Liquid Glass, especially as expressed on MacOS, is a lightweight poorly considered design system as a whole, and its conceptual thinness is not sufficient to properly allow the Mac to carry the weight it needs to bear.</p>

<p>Perhaps more tellingly, there should have been no need for the “<a href="https://daringfireball.net/linked/2025/10/21/ios-26-1-beta-4-liquid-glass-tinted-option">clear/tinted</a>” Liquid Glass preference setting that Apple added in the 26.1 OS releases. Alan Dye wasn’t fired, by all accounts, but that preference setting was as good a sign as any that he should have been. And it’s very much a sign that inside Apple, there’s a strong enough contingent of people who prioritize how things work — like, you know, <a href="https://x.com/Namelongnumbers/status/1996303867735375978">whether you can read text against the background of an alert</a> — to get a setting like this shipped, outside the Accessibility section of Settings.</p>

<p>It remains worrisome that Apple needed to luck into Dye leaving the company. But fortune favors the prepared, and Apple remains prepared by having an inordinate number of longtime talented HI designers at the company. The oddest thing about Alan Dye’s stint leading software design is that there are, effectively, zero design critics who’ve been on his side. The debate regarding Apple’s software design over the last decade isn’t between those on Dye’s side and those against. It’s only a matter of debating how bad it’s been, and how far it’s fallen from its previous remarkable heights. It’s rather extraordinary in today’s hyper-partisan world that there’s nearly universal agreement amongst actual practitioners of user-interface design that Alan Dye is a fraud who led the company deeply astray. It was a big problem inside the company too. I’m aware of dozens of designers who’ve left Apple, out of frustration over the company’s direction, to work at places like LoveFrom, OpenAI, and their secretive joint venture <a href="https://daringfireball.net/linked/2025/05/21/sam-and-jony-io">io</a>. I’m not sure there are any interaction designers at io who aren’t ex-Apple, and if there are, it’s only a handful. From the stories I’m aware of, the theme is identical: these are designers driven to do great work, and under Alan Dye, “doing great work” was no longer the guiding principle at Apple. If reaching the most users is your goal, go work on design at Google, or Microsoft, or Meta. (Design, of course, isn’t even a thing at Amazon.) Designers choose to work at Apple to do the best work in the industry. That has stopped being true under Alan Dye. The most talented designers I know are the harshest critics of Dye’s body of work, and the direction in which it’s been heading.</p>

<p>Back in June, after WWDC, I quoted from Alan Dye’s introduction of Liquid Glass during the keynote, and then quoted from <a href="https://youtu.be/dHrVGk0WwYM?t=381">Steve Jobs’s introduction of Aqua</a> when he unveiled the Mac OS X Public Beta in January 2000. <a href="https://daringfireball.net/2025/06/some_brief_thoughts_and_observations_on_wwdc_2025">I wrote</a>:</p>

<blockquote>
  <p>Re-watching Jobs’s introduction of Aqua for the umpteenth time, I
still find it enthralling. I found Alan Dye’s introduction of
Liquid Glass to be soporific, if not downright horseshitty.</p>
</blockquote>

<p>One of the bits from Jobs’s Aqua introduction I quoted was this:</p>

<blockquote>
  <p>This is what the top of windows look like. These three buttons
look like a traffic signal, don’t they? Red means close the
window. Yellow means minimize the window. And green means maximize
the window. Pretty simple. And tremendous fit and finish in this
operating system. When you roll over these things, you get those.
You see them? And when you are no longer the key window, they go
transparent. So a lot of fit and finish in this.</p>
</blockquote>

<p>After I published that post, I got a note from a designer friend who left Apple, in frustration, a few years ago. After watching Jobs’s Aqua introduction for the first time in years, he told me, “I’m really struck by Steve directly speaking to ‘radio buttons’ and ‘the key window’.” He had the feeling that Dye and his team looked down on interface designers who used terms like Jobs himself once used — in a public keynote, no less. That to Dye’s circle, such terms felt too much like “programmer talk”. But the history of Apple (and NeXT) user interface design is the opposite. Designers and programmers used to — and still should — speak the exact same language about such concepts. Steve Jobs certainly did, and something feels profoundly broken about that disconnect under Alan Dye’s leadership. It’s like the head of cinematography for a movie telling the camera team to stop talking about nerdy shit like “f-stops”. The head of cinematography shouldn’t just abide talking about f-stops and focal lengths, but love it. Said my friend to me, regarding his interactions with Dye and his team at Apple, “I swear I had conversations in which I mentioned ‘key window’ and no one knew what I meant.”</p>

<p>That won’t be a problem with Stephen Lemay. Understanding of fundamental principles will no longer be lacking. Lemay has been at Apple spanning the gamut between the <a href="https://daringfireball.net/search/greg+christie">Greg Christie</a>/<a href="https://daringfireball.net/search/bas+ording">Bas Ording</a> glory days and the current era. At the very least, Lemay running HI should stop the bleeding — both in terms of work quality and talent retention. I sincerely believe things might measurably improve, but I’m more sure that things will stop getting worse. That alone will be a win for everyone — even though the change was seemingly driven by Mark Zuckerberg’s desire to poach Dye, not Tim Cook and Apple’s senior leadership realizing they should have shitcanned him long ago.</p>

<p>Alan Dye is not untalented. But his talents at Apple were in politics. His political skill was so profound that it was <em>his</em> decision to leave, despite the fact that his tenure is considered a disaster by actual designers inside and outside the company. He obviously figured out how to please Apple’s senior leadership. His departure today landed as a total surprise because his stature within the company seemed so secure. And so I think he might do very well at Meta. Not because he can bring world-class interaction design expertise — because he obviously can’t — but because the path to success at Meta has never been driven by design. It’s about getting done what Zuck wants done. Dye might excel at that. Dye was an anchor holding Apple back, but might elevate design at Meta.<sup id="fnr5-2025-12-03"><a href="#fn5-2025-12-03">5</a></sup></p>

<p>My favorite reaction to today’s news is <a href="https://x.com/8hipulin/status/1996318006335401997">this one-liner from a guy on Twitter/X</a>: “The average IQ of both companies has increased.”</p>





 <!-- PreviousNext -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[12 Days of Shell (204 pts)]]></title>
            <link>https://12days.cmdchallenge.com</link>
            <guid>46190577</guid>
            <pubDate>Mon, 08 Dec 2025 10:13:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://12days.cmdchallenge.com">https://12days.cmdchallenge.com</a>, See on <a href="https://news.ycombinator.com/item?id=46190577">Hacker News</a></p>
Couldn't get https://12days.cmdchallenge.com: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[The fuck off contact page (411 pts)]]></title>
            <link>https://www.nicchan.me/blog/the-f-off-contact-page/</link>
            <guid>46189994</guid>
            <pubDate>Mon, 08 Dec 2025 08:57:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nicchan.me/blog/the-f-off-contact-page/">https://www.nicchan.me/blog/the-f-off-contact-page/</a>, See on <a href="https://news.ycombinator.com/item?id=46189994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-7jjqptxk=""> 
<p>Many years ago, I had a client that sold a service. They weren’t a design agency, but for the sake of anonymity, we’ll just call them a design agency. Let us say that their core offering was a full-service design package, but they also made a substantial part of their income from doing smaller tasks related to their primary offering. These kind of services included smaller tasks like one-off campaigns or newsletter designs; tasks that their customers may very well be able to do on their own, but the prospect of freeing up some time by by offloading it to an expert was a tempting offer for many of their customers, and made up a significant chunk of their revenue.</p>
<p>We were hired to do a complete redesign of their site from the ground up. The process went smoothly at first, all the wireframes were approved without issue, but when it came to the design phase, we began to hit walls. For example, they would stumble across sites that they liked and wanted to depart from the agreed-upon wireframes in order to implement a similar design.</p>
<p>The problem was, they were thinking about their inspiration sites from an aesthetic point of view, not from a user experience perspective. Their decisions were coming from a place of ‘we like the balance of imagery and text  in this page’ and not ‘we think this design will achieve the intended goal of the page.’ Now, you know me, I love a good <a href="https://www.smashingmagazine.com/2025/04/gild-just-one-lily/">singular gilded lily</a>, but the client had unwittingly stumbled across a trap, they had fallen in love with what I call a “Fuck off contact page.”</p>
<section><h2 id="what-the-fuck-is-a-fuck-off-contact-page">What the fuck is a ‘fuck off contact page?’</h2><p>A “fuck off contact page” is what a company throws together when they actually don’t want anyone to contact them at all. They are usually found on the websites of million or billion dollar companies, likely Software-as-a-service (SaaS) companies that are trying to reduce the amount of money they spend on support by carefully hiding the real support channels behind login walls. These companies tend to offer multiple tiers of support, with enterprise customers having a customer success manager who they can call on this ancient device we call phones, whereas the lower-paying customers may have to wrangle various in-app ticket mechanisms. If you solve your own problem by reading the knowledge base, then this is a win for the company. They don’t want to hear from you, they want you to fuck off.</p><figure><img alt="Two mobile wireframes. On the left, the wireframe has a large heading that says Contact, and a contact form with two fields, 'Name' and 'How can we help you?' below it. On the right, the mockup has a large heading that says Contact, and three icons with text underneath. In order, they are 'Check out our knowledge base', 'Visit us in person' and 'Reach out to our sales team.'" loading="lazy" decoding="async" sizes="(min-width: 752px) 752px, 100vw" srcset="https://res.cloudinary.com/nicchan/image/upload/w_640,h_611,c_lfill,f_auto/v1765177043/contact 640w,
https://res.cloudinary.com/nicchan/image/upload/w_750,h_716,c_lfill,f_auto/v1765177043/contact 750w,
https://res.cloudinary.com/nicchan/image/upload/w_752,h_718,c_lfill,f_auto/v1765177043/contact 752w,
https://res.cloudinary.com/nicchan/image/upload/w_828,h_791,c_lfill,f_auto/v1765177043/contact 828w,
https://res.cloudinary.com/nicchan/image/upload/w_960,h_917,c_lfill,f_auto/v1765177043/contact 960w,
https://res.cloudinary.com/nicchan/image/upload/w_1080,h_1031,c_lfill,f_auto/v1765177043/contact 1080w,
https://res.cloudinary.com/nicchan/image/upload/w_1280,h_1222,c_lfill,f_auto/v1765177043/contact 1280w,
https://res.cloudinary.com/nicchan/image/upload/w_1504,h_1436,c_lfill,f_auto/v1765177043/contact 1504w" src="https://res.cloudinary.com/nicchan/image/upload/w_752,h_718,c_lfill,f_auto/v1765177043/contact"><figcaption>These are recreated versions of the wireframes that we did for the site, the original contact form version of the page is on the left, and the ‘fuck off contact page’ is on the right. In actuality, the ‘fuck off contact page’ was even more ‘fuck off’ due to the whitespace and a large hero image. This meant the only option that ‘talk to the sales team’, the only option that would put you in touch with a human anytime soon, was at the very bottom of the page, long after some people would stop scrolling.</figcaption></figure><p>In other words, this is entirely inappropriate for the kind of service-based agency that our client was. The billion dollar SaaS company wants to reduce the number of incoming inquiries, and is hoping to weed out anyone who is not determined to contact them by giving them unsatisfying options. The service company wants to show how helpful they are and cultivate leads. These are fundamentally opposing goals.</p><p>Let me explain further. I’m not sure about you, but as a user, when I see a button that says ‘talk to our sales team’, I treat the entire region of the page with the same trepidation as nuclear waste. The page is now a no-go zone, and I try to exit as quickly as possible, knowing that whatever my original query was, I’m going to have to solve it unassisted. Seeing as this is a company who makes money off of convincing people to let them handle the easy stuff, adding friction to this key part of their sales funnel just doesn’t feel like a winning strategy.</p></section>
<section><h2 id="how-the-fuck-did-you-convince-them-to-change-their-minds">How the fuck did you convince them to change their minds?</h2><p>Try as we might, we couldn’t. In all honesty, we probably could have done more in order to talk them out of it, but the project had gone in such a way where we were focused on trying to talk the client out of changing other things that would drastically increase design or development time beyond the initial scope. In other words, we were too busy putting out other fires. This re-designed contact page, as certain as we were of how bad of an idea it was, wasn’t a fire, so we let it through.</p><p>The project finished on time, everyone got paid, and the client was happy with the end result, but I still felt very disappointed in the whole thing. While I personally believe in the value of good design, I also believe there are a lot of smoke-and-mirrors in the industry, and I hated the thought that I might have inadvertently contributed to it. Even if the client is happy, it didn’t meet my internal bar for a quality product worth sticking my name on, and I feel like I’ve let down both the client and the end-users.</p></section>
<section><h2 id="how-the-fuck-do-i-avoid-being-in-a-position-where-im-asked-to-implement-a-fuck-off-contact-page">How the fuck do I avoid being in a position where I’m asked to implement a ‘fuck off contact page’?</h2><p>I think our problems started from before we even began to touch a single design tool. As a favor to one of the folks involved, we had discounted our rates for this client, and I think that set us off on the wrong foot. Instead of seeing us as people who brought valuable knowledge and expertise to the project, they saw us as the hands that would execute their vision.</p><p>Especially for those not familiar with the process of design, it can be tempting to see things like discovery and wireframing as obstacles to be cleared before you get to the fun part, designing the visual identity. Unfortunately, many designers are also guilty of this!</p><p>As service providers, I believe we need to do a better job on educating clients on the design process and why each step is so important. This is radical idea in some circles, but knowing why you’re building something is a necessary part of doing a good job at it! That’s why we do things like determining the architecture before we start thinking about the brand. Flow charts and diagrams are not as fun as interactive prototypes, but they’re much more important to get right.</p><p>Also, the discounted pricing probably didn’t help — instead of signaling that we were doing a favor out of respect for them, it just signaled that we were easily exploitable. There was a lack of trust throughout the process, on both sides. While I really want to believe that I can have the kind of relationships with clients where constructive disagreement is welcomed and valued, how I get there is still something I’m figuring out, even many years later.</p><p>I think that’s part of the reason why I blog. By blogging, I’m putting a body of work out there that communicates my values and ethos. While much of the details of my client work has to remain private, these posts can be public, and hopefully they can help me find people who resonate with what I have to offer. Or you know, just be bold enough to communicate ‘Fuck off’ to those who don’t!</p><p>(Feel free to <a href="https://www.nicchan.me/contact">reach out</a> if you’re interested in working with folks who care, maybe a little too much, about doing right by your users.)</p></section>
<section><h2 id="links-and-resources">Links and Resources</h2><ul>
<li>I hit publish on this as I listened to <a href="https://www.youtube.com/watch?v=a7sh-5UYnJc">Andy Bell’s recent talk at Beyond Tellerand</a>. It has SO many gems of wisdom about the core skills required to deliver successful client projects. It’s also why I love working with <a href="https://set.studio/">Set.Studio</a>, check ‘em out.</li>
</ul></section> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub Actions Has a Package Manager, and It Might Be the Worst (309 pts)]]></title>
            <link>https://nesbitt.io/2025/12/06/github-actions-package-manager.html</link>
            <guid>46189692</guid>
            <pubDate>Mon, 08 Dec 2025 08:15:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nesbitt.io/2025/12/06/github-actions-package-manager.html">https://nesbitt.io/2025/12/06/github-actions-package-manager.html</a>, See on <a href="https://news.ycombinator.com/item?id=46189692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>After putting together <a href="https://github.com/ecosyste-ms/package-manager-resolvers">ecosyste-ms/package-manager-resolvers</a>, I started wondering what dependency resolution algorithm GitHub Actions uses. When you write <code>uses: actions/checkout@v4</code> in a workflow file, you’re declaring a dependency. GitHub resolves it, downloads it, and executes it. That’s package management. So I went spelunking into the runner codebase to see how it works. What I found was concerning.</p>

<p>Package managers are a critical part of software supply chain security. The industry has spent years hardening them after incidents like left-pad, event-stream, and countless others. Lockfiles, integrity hashes, and dependency visibility aren’t optional extras. They’re the baseline. GitHub Actions ignores all of it.</p>

<p>Compared to mature package ecosystems:</p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>npm</th>
      <th>Cargo</th>
      <th>NuGet</th>
      <th>Bundler</th>
      <th>Go</th>
      <th>Actions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Lockfile</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
    <tr>
      <td>Transitive pinning</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
    <tr>
      <td>Integrity hashes</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
    <tr>
      <td>Dependency tree visibility</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
    <tr>
      <td>Resolution specification</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✓</td>
      <td>✗</td>
    </tr>
  </tbody>
</table>

<p>The core problem is the lack of a lockfile. Every other package manager figured this out decades ago: you declare loose constraints in a manifest, the resolver picks specific versions, and the lockfile records exactly what was chosen. GitHub Actions has no equivalent. Every run re-resolves from your workflow file, and the results can change without any modification to your code.</p>

<p><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/koishybayev">Research from USENIX Security 2022</a> analyzed over 200,000 repositories and found that 99.7% execute externally developed Actions, 97% use Actions from unverified creators, and 18% run Actions with missing security updates. The researchers identified four fundamental security properties that CI/CD systems need: admittance control, execution control, code control, and access to secrets. GitHub Actions fails to provide adequate tooling for any of them. A <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/muralee">follow-up study</a> using static taint analysis found code injection vulnerabilities in over 4,300 workflows across 2.7 million analyzed. Nearly every GitHub Actions user is running third-party code with no verification, no lockfile, and no visibility into what that code depends on.</p>

<p><strong>Mutable versions.</strong> When you pin to <code>actions/checkout@v4</code>, that tag can move. The maintainer can push a new commit and retag. Your workflow changes silently. A lockfile would record the SHA that <code>@v4</code> resolved to, giving you reproducibility while keeping version tags readable. Instead, you have to choose: readable tags with no stability, or unreadable SHAs with no automated update path.</p>

<p>GitHub has added mitigations. <a href="https://docs.github.com/en/code-security/supply-chain-security/understanding-your-software-supply-chain/immutable-releases">Immutable releases</a> lock a release’s git tag after publication. Organizations can enforce SHA pinning as a policy. You can limit workflows to actions from verified creators. These help, but they only address the top-level dependency. They do nothing for transitive dependencies, which is the primary attack vector.</p>

<p><strong>Invisible transitive dependencies.</strong> SHA pinning doesn’t solve this. Composite actions resolve their own dependencies, but you can’t see or control what they pull in. When you pin an action to a SHA, you only lock the outer file. If it internally pulls <code>some-helper@v1</code> with a mutable tag, your workflow is still vulnerable. You have zero visibility into this. A lockfile would record the entire resolved tree, making transitive dependencies visible and pinnable. <a href="https://doi.org/10.1145/3643991.3644899">Research on JavaScript Actions</a> found that 54% contain at least one security weakness, with most vulnerabilities coming from indirect dependencies. The <a href="https://unit42.paloaltonetworks.com/github-actions-supply-chain-attack/">tj-actions/changed-files incident</a> showed how this plays out in practice: a compromised action updated its transitive dependencies to exfiltrate secrets. With a lockfile, the unexpected transitive change would have been visible in a diff.</p>

<p><strong>No integrity verification.</strong> npm records <code>integrity</code> hashes in the lockfile. Cargo records checksums in <code>Cargo.lock</code>. When you install, the package manager verifies the download matches what was recorded. Actions has nothing. You trust GitHub to give you the right code for a SHA. A lockfile with integrity hashes would let you verify that what you’re running matches what you resolved.</p>

<p><strong>Re-runs aren’t reproducible.</strong> GitHub staff have <a href="https://github.com/orgs/community/discussions/27083">confirmed this explicitly</a>: “if the workflow uses some actions at a version, if that version was force pushed/updated, we will be fetching the latest version there.” A failed job re-run can silently get different code than the original run. Cache interaction makes it worse: caches only save on successful jobs, so a re-run after a force-push gets different code <em>and</em> has to rebuild the cache. Two sources of non-determinism compounding. A lockfile would make re-runs deterministic: same lockfile, same code, every time.</p>

<p><strong>No dependency tree visibility.</strong> npm has <code>npm ls</code>. Cargo has <code>cargo tree</code>. You can inspect your full dependency graph, find duplicates, trace how a transitive dependency got pulled in. Actions gives you nothing. You can’t see what your workflow actually depends on without manually reading every composite action’s source. A lockfile would be a complete manifest of your dependency tree.</p>

<p><strong>Undocumented resolution semantics.</strong> Every package manager documents how dependency resolution works. npm has a spec. Cargo has a spec. Actions resolution is undocumented. The <a href="https://github.com/actions/runner">runner source is public</a>, and the entire “resolution algorithm” is in <a href="https://github.com/actions/runner/blob/main/src/Runner.Worker/ActionManager.cs">ActionManager.cs</a>. Here’s a simplified version of what it does:</p>

<div><pre><code><span>// Simplified from actions/runner ActionManager.cs</span>
<span>async</span> <span>Task</span> <span>PrepareActionsAsync</span><span>(</span><span>steps</span><span>)</span> <span>{</span>
    <span>// Start fresh every time - no caching</span>
    <span>DeleteDirectory</span><span>(</span><span>"_work/_actions"</span><span>);</span>

    <span>await</span> <span>PrepareActionsRecursiveAsync</span><span>(</span><span>steps</span><span>,</span> <span>depth</span><span>:</span> <span>0</span><span>);</span>
<span>}</span>

<span>async</span> <span>Task</span> <span>PrepareActionsRecursiveAsync</span><span>(</span><span>actions</span><span>,</span> <span>depth</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>depth</span> <span>&gt;</span> <span>10</span><span>)</span>
        <span>throw</span> <span>new</span> <span>Exception</span><span>(</span><span>"Composite action depth exceeded max depth 10"</span><span>);</span>

    <span>foreach</span> <span>(</span><span>var</span> <span>action</span> <span>in</span> <span>actions</span><span>)</span> <span>{</span>
        <span>// Resolution happens on GitHub's server - opaque to us</span>
        <span>var</span> <span>downloadInfo</span> <span>=</span> <span>await</span> <span>GetDownloadInfoFromGitHub</span><span>(</span><span>action</span><span>.</span><span>Reference</span><span>);</span>

        <span>// Download and extract - no integrity verification</span>
        <span>var</span> <span>tarball</span> <span>=</span> <span>await</span> <span>Download</span><span>(</span><span>downloadInfo</span><span>.</span><span>TarballUrl</span><span>);</span>
        <span>Extract</span><span>(</span><span>tarball</span><span>,</span> <span>$"_actions/</span><span>{</span><span>action</span><span>.</span><span>Owner</span><span>}</span><span>/</span><span>{</span><span>action</span><span>.</span><span>Repo</span><span>}</span><span>/</span><span>{</span><span>downloadInfo</span><span>.</span><span>Sha</span><span>}</span><span>"</span><span>);</span>

        <span>// If composite, recurse into its dependencies</span>
        <span>var</span> <span>actionYml</span> <span>=</span> <span>Parse</span><span>(</span><span>$"_actions/</span><span>{</span><span>action</span><span>.</span><span>Owner</span><span>}</span><span>/</span><span>{</span><span>action</span><span>.</span><span>Repo</span><span>}</span><span>/</span><span>{</span><span>downloadInfo</span><span>.</span><span>Sha</span><span>}</span><span>/action.yml"</span><span>);</span>
        <span>if</span> <span>(</span><span>actionYml</span><span>.</span><span>Type</span> <span>==</span> <span>"composite"</span><span>)</span> <span>{</span>
            <span>// These nested actions may use mutable tags - we have no control</span>
            <span>await</span> <span>PrepareActionsRecursiveAsync</span><span>(</span><span>actionYml</span><span>.</span><span>Steps</span><span>,</span> <span>depth</span> <span>+</span> <span>1</span><span>);</span>
        <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>That’s it. No version constraints, no deduplication (the same action referenced twice gets downloaded twice), no integrity checks. The tarball URL comes from GitHub’s API, and you trust them to return the right content for the SHA. A lockfile wouldn’t fix the missing spec, but it would at least give you a concrete record of what resolution produced.</p>

<p>Even setting lockfiles aside, Actions has other issues that proper package managers solved long ago.</p>

<p><strong>No registry.</strong> Actions live in git repositories. There’s no central index, no security scanning, no malware detection, no typosquatting prevention. A real registry can flag malicious packages, store immutable copies independent of the source, and provide a single point for security response. The Marketplace exists but it’s a thin layer over repository search. Without a registry, there’s nowhere for immutable metadata to live. If an action’s source repository disappears or gets compromised, there’s no fallback.</p>

<p><strong>Shared mutable environment.</strong> Actions aren’t sandboxed from each other. Two actions calling <code>setup-node</code> with different versions mutate the same <code>$PATH</code>. The outcome depends on execution order, not any deterministic resolution.</p>

<p><strong>No offline support.</strong> Actions are pulled from GitHub on every run. There’s no offline installation mode, no vendoring mechanism, no way to run without network access. Other package managers let you vendor dependencies or set up private mirrors. With Actions, if GitHub is down, your CI is down.</p>

<p><strong>The namespace is GitHub usernames.</strong> Anyone who creates a GitHub account owns that namespace for actions. Account takeovers and typosquatting are possible. When a popular action maintainer’s account gets compromised, attackers can push malicious code and retag. A lockfile with integrity hashes wouldn’t prevent account takeovers, but it would detect when the code changes unexpectedly. The hash mismatch would fail the build instead of silently running attacker-controlled code. Another option would be something like Go’s checksum database, a transparent log of known-good hashes that catches when the same version suddenly has different contents.</p>

<h3 id="how-did-we-get-here">How Did We Get Here?</h3>

<p>The Actions runner is forked from Azure DevOps, designed for enterprises with controlled internal task libraries where you trust your pipeline tasks. GitHub bolted a public marketplace onto that foundation without rethinking the trust model. The addition of composite actions and reusable workflows created a dependency system, but the implementation ignored lessons from package management: lockfiles, integrity verification, transitive pinning, dependency visibility.</p>

<p>This matters beyond CI/CD. Trusted publishing is being rolled out across package registries: PyPI, npm, RubyGems, and others now let you publish packages directly from GitHub Actions using OIDC tokens instead of long-lived secrets. OIDC removes one class of attacks (stolen credentials) but amplifies another: the supply chain security of these registries now depends entirely on GitHub Actions, a system that lacks the lockfile and integrity controls these registries themselves require. A compromise in your workflow’s action dependencies can lead to malicious packages on registries with better security practices than the system they’re trusting to publish.</p>

<p>Other CI systems have done better. GitLab CI added an <code>integrity</code> keyword in version 17.9 that lets you specify a SHA256 hash for remote includes. If the hash doesn’t match, the pipeline fails. Their documentation explicitly warns that including remote configs “is similar to pulling a third-party dependency” and recommends pinning to full commit SHAs. GitLab recognized the problem and shipped integrity verification. GitHub closed the feature request.</p>

<p>GitHub’s design choices don’t just affect GitHub users. Forgejo Actions maintains compatibility with GitHub Actions, which means projects migrating to Codeberg for ethical reasons inherit the same broken CI architecture. The Forgejo maintainers <a href="https://codeberg.org/forgejo/discussions/issues/214">openly acknowledge the problems</a>, with contributors calling GitHub Actions’ ecosystem “terribly designed and executed.” But they’re stuck maintaining compatibility with it. Codeberg mirrors common actions to reduce GitHub dependency, but the fundamental issues are baked into the model itself. GitHub’s design flaws are spreading to the alternatives.</p>

<p><a href="https://github.com/actions/runner/issues/2195">GitHub issue #2195</a> requested lockfile support. It was closed as “not planned” in 2022. Palo Alto’s <a href="https://unit42.paloaltonetworks.com/github-actions-supply-chain-vulnerabilities/">“Unpinnable Actions” research</a> documented how even SHA-pinned actions can have unpinnable transitive dependencies.</p>

<p>Dependabot can update action versions, which helps. Some teams vendor actions into their own repos. <a href="https://zizmor.sh/">zizmor</a> is excellent at scanning workflows and finding security issues. But these are workarounds for a system that lacks the basics.</p>

<p>The fix is a lockfile. Record resolved SHAs for every action reference, including transitives. Add integrity hashes. Make the dependency tree inspectable. GitHub closed the request three years ago and hasn’t revisited it.</p>

<hr>

<p><strong>Further reading:</strong></p>

<ul>
  <li><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/koishybayev">Characterizing the Security of GitHub CI Workflows</a> - Koishybayev et al., USENIX Security 2022</li>
  <li><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/muralee">ARGUS: A Framework for Staged Static Taint Analysis of GitHub Workflows and Actions</a> - Muralee et al., USENIX Security 2023</li>
  <li><a href="https://www.wiz.io/blog/new-github-action-supply-chain-attack-reviewdog-action-setup">New GitHub Action supply chain attack: reviewdog/action-setup</a> - Wiz Research, 2025</li>
  <li><a href="https://www.paloaltonetworks.com/blog/cloud-security/unpinnable-actions-github-security/">Unpinnable Actions: How Malicious Code Can Sneak into Your GitHub Actions Workflows</a></li>
  <li><a href="https://www.paloaltonetworks.com/blog/cloud-security/github-actions-worm-dependencies/">GitHub Actions Worm: Compromising GitHub Repositories Through the Actions Dependency Tree</a></li>
  <li><a href="https://github.com/actions/setup-python/issues/377">setup-python: Action can be compromised via mutable dependency</a></li>
</ul>

  </div>

  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Palantir Could Be the Most Overvalued Company That Ever Existed (131 pts)]]></title>
            <link>https://247wallst.com/investing/2025/11/25/palantir-could-be-the-most-overvalued-company-that-ever-existed/</link>
            <guid>46188451</guid>
            <pubDate>Mon, 08 Dec 2025 04:45:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://247wallst.com/investing/2025/11/25/palantir-could-be-the-most-overvalued-company-that-ever-existed/">https://247wallst.com/investing/2025/11/25/palantir-could-be-the-most-overvalued-company-that-ever-existed/</a>, See on <a href="https://news.ycombinator.com/item?id=46188451">Hacker News</a></p>
Couldn't get https://247wallst.com/investing/2025/11/25/palantir-could-be-the-most-overvalued-company-that-ever-existed/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Damn Small Linux (208 pts)]]></title>
            <link>https://www.damnsmalllinux.org/</link>
            <guid>46187387</guid>
            <pubDate>Mon, 08 Dec 2025 01:47:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.damnsmalllinux.org/">https://www.damnsmalllinux.org/</a>, See on <a href="https://news.ycombinator.com/item?id=46187387">Hacker News</a></p>
Couldn't get https://www.damnsmalllinux.org/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Bag of words, have mercy on us (286 pts)]]></title>
            <link>https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us</link>
            <guid>46185957</guid>
            <pubDate>Sun, 07 Dec 2025 22:31:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us">https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us</a>, See on <a href="https://news.ycombinator.com/item?id=46185957">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!w4qD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!w4qD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg 424w, https://substackcdn.com/image/fetch/$s_!w4qD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg 848w, https://substackcdn.com/image/fetch/$s_!w4qD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!w4qD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!w4qD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg" width="603" height="820.8740740740741" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1654,&quot;width&quot;:1215,&quot;resizeWidth&quot;:603,&quot;bytes&quot;:324078,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://www.experimental-history.com/i/169990157?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!w4qD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg 424w, https://substackcdn.com/image/fetch/$s_!w4qD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg 848w, https://substackcdn.com/image/fetch/$s_!w4qD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!w4qD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31f48971-998b-4e9b-8f90-ac67d4336c39_1215x1654.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>photo cred: my dad</figcaption></figure></div><p>Look, I don’t know if AI is gonna kill us or make us all rich or whatever, but I do know we’ve got the wrong metaphor.</p><p><span>We want to understand these things as </span><em>people</em><span>. When you type a question to ChatGPT and it types back the answer in complete sentences, it feels like there must be a little guy in there doing the typing. We get this vivid sense of “</span><em>it’s alive!!</em><span>”, and we activate all of the mental faculties we evolved to deal with fellow humans: </span><a href="https://en.wikipedia.org/wiki/Theory_of_mind" rel="">theory of mind</a><span>, </span><a href="https://en.wikipedia.org/wiki/Attribution_(psychology)" rel="">attribution</a><span>, </span><a href="https://en.wikipedia.org/wiki/Impression_management#:~:text=Impression%20management%20is%20a%20conscious,controlling%20information%20in%20social%20interaction." rel="">impression management</a><span>, </span><a href="https://en.wikipedia.org/wiki/Stereotype" rel="">stereotyping</a><span>, </span><a href="https://www.cep.ucsb.edu/wp-content/uploads/2023/05/Cosmides_etal_2005_DetectingCheaters.pdf" rel="">cheater detection</a><span>, etc.</span></p><p><span>We can’t help it; humans are hopeless anthropomorphizers. When it comes to perceiving personhood, we’re so trigger-happy that we can see the Virgin Mary in a </span><a href="https://www.nbcnews.com/id/wbna6511148" rel="">grilled cheese sandwich</a><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!B9zG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!B9zG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png 424w, https://substackcdn.com/image/fetch/$s_!B9zG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png 848w, https://substackcdn.com/image/fetch/$s_!B9zG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png 1272w, https://substackcdn.com/image/fetch/$s_!B9zG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!B9zG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png" width="378" height="490" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:490,&quot;width&quot;:378,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:262500,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.experimental-history.com/i/169990157?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!B9zG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png 424w, https://substackcdn.com/image/fetch/$s_!B9zG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png 848w, https://substackcdn.com/image/fetch/$s_!B9zG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png 1272w, https://substackcdn.com/image/fetch/$s_!B9zG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d375800-59a9-4bc2-8daa-5ec2a2c142d7_378x490.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>A human face in a slice of nematode:</p><p>And an old man in a bunch of poultry and fish atop a pile of books:</p><p><span>Apparently, this served us well in our evolutionary history—maybe it’s so important not to mistake </span><em>people</em><span> for </span><em>things</em><span> that we err on the side of mistaking </span><em>things</em><span> for </span><em>people</em><span>.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-1-169990157" target="_self" rel="">1</a></span><span> This is probably why we’re so willing to explain strange occurrences by appealing to fantastical creatures with minds and intentions: everybody in town is getting sick because of WITCHES, you can’t see the sun right now because A WOLF ATE IT, the volcano erupted because GOD IS MAD. People who experience sleep paralysis sometimes </span><a href="https://en.wikipedia.org/wiki/Sleep_paralysis" rel="">hallucinate</a><span> a demon-like creature sitting on their chest, and one explanation is that the subconscious mind is trying to understand why the body can’t move, and instead of coming up with “I’m still in REM sleep so there’s not enough acetylcholine in my brain to activate my primary motor cortex”, it comes up with “BIG DEMON ON TOP OF ME”.</span></p><p><span>This is why the past three years have been so confusing—the little guy inside the AI keeps dumbfounding us by doing things that a human wouldn’t do. Why does he make up citations when he does my social studies homework? How come he can beat me at Go but he can’t tell me </span><a href="https://community.openai.com/t/incorrect-count-of-r-characters-in-the-word-strawberry/829618" rel="">how many “r”s are in the word “strawberry”</a><span>? Why is he telling me to </span><a href="https://www.theverge.com/2024/5/23/24162896/google-ai-overview-hallucinations-glue-in-pizza" rel="">put glue on my pizza</a><span>?</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-2-169990157" target="_self" rel="">2</a></span></p><p><span>Trying to understand LLMs by using the rules of human psychology is like trying to understand a game of Scrabble by using the rules of Pictionary. These things don’t act like people because they </span><em>aren’t </em><span>people. I don’t mean that in the deflationary way that the AI naysayers mean it. They think denying humanity to the machines is a well-deserved insult; I think it’s just an accurate description.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-3-169990157" target="_self" rel="">3</a></span><span> As long we try to apply our person perception to artificial intelligence, we’ll keep being surprised and befuddled.</span></p><p><span>We are in dire need of a better metaphor. Here’s my suggestion: instead of seeing AI as a sort of silicon homunculus, we should see it as a </span><em>bag of words.</em></p><p><span>An AI is a bag that contains basically all words ever written, at least the ones that could be scraped off the internet or scanned out of a book. When users send words into the bag, it sends back the most relevant words it has. There are so many words in the bag that the most relevant ones are often correct and helpful, and AI companies secretly add </span><a href="https://promptengineering.org/system-prompts-in-large-language-models/" rel="">invisible words</a><span> to your queries to make this even more likely.</span></p><p><span>This is an oversimplification, of course. But it’s also surprisingly handy. For example, AIs will routinely give you outright lies or hallucinations, and when you’re like “Uhh hey that was a lie”, they will immediately respond “Oh my god I’m SO SORRY!! I promise I’ll never ever do that again!! I’m turning over a new leaf right now, nothing but true statements from here on” and then they will literally lie to you in the next sentence. This would be baffling and exasperating behavior coming from a human, but it’s very normal behavior coming from a bag of words. If you toss a question into the bag and the right answer happens to be in there, that’s probably what you’ll get. If it’s not in there, you’ll get some related-but-inaccurate bolus of sentences. When you accuse it of lying, it’s going to produce lots of words from the “I’ve been accused of lying” part of the bag. Calling this behavior “malicious” or “erratic” is misleading because it’s not </span><em>behavior </em><span>at all, just like it’s not “behavior” when a calculator multiplies numbers for you.</span></p><p><span>“Bag of words” is a also a useful heuristic for predicting where an AI will do well and where it will fail. “Give me a list of the ten worst transportation disasters in North America” is an easy task for a bag of words, because disasters are well-documented. On the other hand, “Who reassigned the species </span><em>Brachiosaurus brancai</em><span> to its own genus, and when?” is a </span><a href="https://svpow.com/2025/02/14/if-you-believe-in-artificial-intelligence-take-five-minutes-to-ask-it-about-stuff-you-know-well/" rel="">hard task</a><span> for a bag of words, because the bag just doesn’t contain that many words on the topic.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-4-169990157" target="_self" rel="">4</a></span><span> And a question like “What are the most important lessons for life?” won’t give you anything outright false, but it will give you a bunch of fake-deep pablum, because most of the text humans have produced on that topic is, no offense, fake-deep pablum.</span></p><p><span>When you forget that an AI is just a big bag of words, you can easily slip into acting like it’s an all-seeing glob of pure intelligence. For example, I was hanging with a group recently where one guy made everybody watch a video of some close-up magic, and after the magician made some coins disappear, he exclaimed, “I asked ChatGPT how this trick works, and even </span><em>it </em><span>didn’t know!” as if this somehow made the magic extra magical. In this person’s model of the world, we are all like shtetl-dwelling peasants and AI is like our Rabbi Hillel, the only learned man for 100 miles. If Hillel can’t understand it, then it must be truly profound!</span></p><p>If that guy had instead seen ChatGPT as a bag of words, he would have realized that the bag probably doesn’t contain lots of detailed descriptions of contemporary coin tricks. After all, magicians make money from performing and selling their tricks, not writing about them at length on the internet. Plus, magic tricks are hard to describe—“He had three quarters in his hand and then it was two pennies!”—so you’re going to have a hard time prompting the right words out of the bag. The coin trick is not literally magic, and neither is the bag of words.</p><p><span>The “bag of words” metaphor can also help us guess what these things are gonna do next. If you want to know whether AI will get better at something in the future, just ask: “can you fill the bag with it?” For instance, people are kicking around the idea that </span><a href="https://aeon.co/essays/when-ais-do-science-it-will-be-strange-and-incomprehensible" rel="">AI will replace human scientists</a><span>. Well, if you want your bag of words to do science for you, you need to stuff it with lots of science. Can we do that?</span></p><p><span>When it comes to specific scientific tasks, yes, we already can. If you fill the bag with data from 170,000 proteins, for example, it’ll do a </span><a href="https://en.wikipedia.org/wiki/AlphaFold" rel="">pretty good job</a><span> predicting how proteins will fold. Fill the bag with chemical reactions and </span><a href="https://platform.futurehouse.org/" rel="">it can tell you how to synthesize new molecules</a><span>. Fill the bag with journal articles and then describe an experiment and </span><a href="https://scite.ai/" rel="">it can tell you whether anyone has already scooped you</a><span>.</span></p><p><span>All of that is cool, and I expect more of it in the future. I don’t think we’re far from a bag of words being able to do an entire low-quality research project from beginning to end—coming up with a hypothesis, designing the study, running it, analyzing the results, writing them up, making the graphs, arranging it all on a poster, all at the click of a button—because we’ve got loads of low-quality science to put in the bag. If you walk up and down the poster sessions at a psychology conference, you can see lots of first-year PhD students presenting studies where they seemingly pick some semi-related constructs at random, correlate them, and print out a p-value (“Does self-efficacy moderate the relationship between social dominance orientation and system-justifying beliefs?”). A bag of words can </span><a href="https://personalitymap.io/" rel="">basically do this already</a><span>; you just need to give it access to an online participant pool and a big printer.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-5-169990157" target="_self" rel="">5</a></span></p><p><span>But </span><a href="https://www.experimental-history.com/p/science-is-a-strong-link-problem" rel="">science is a strong-link problem</a><span>; if we produced a million times more crappy science, we’d be right where we are now. If we want more of the good stuff, what should we put in the bag? You could stuff the bag with papers, but some of them are fraudulent, some are merely mistaken, and all of them contain unstated assumptions that could turn out to be false. And they’re usually missing key information—they don’t share the data, or they don’t describe their methods in adequate detail. Markus Strasser, an entrepreneur who tried to start one of those companies that’s like “we’ll put every scientific paper in the bag and then ??? and then profit”, </span><a href="https://markusstrasser.org/extracting-knowledge-from-literature.html" rel="">eventually abandoned the effort</a><span>, saying that “close to nothing of what makes science actually work is published as text on the web.”</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-6-169990157" target="_self" rel="">6</a></span></p><p><span>Here’s one way to think about it: if there had been enough text to train an LLM in 1600, would it have scooped Galileo? My guess is no. Ask that early modern ChatGPT whether the Earth moves and it will helpfully tell you that experts have considered the possibility and </span><a href="https://classicalliberalarts.com/resources/PTOLEMY_ALMAGEST_ENGLISH.pdf" rel="">ruled it out</a><span>. And that’s by design. If it had started claiming that our planet is zooming through space at 67,000mph, its dutiful human trainers would have punished it: “Bad computer!! Stop hallucinating!!”</span></p><p><span>In fact, an early 1600s bag of words wouldn’t just have the right words in the wrong order. At the time, the right words didn’t </span><em>exist</em><span>. As the historian of science David Wootton </span><a href="https://archive.org/details/inventionofscien0000woot" rel="">points out</a><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-7-169990157" target="_self" rel="">7</a></span><span>, when Galileo was trying to describe his discovery of the moons of Jupiter, none of the languages he knew had a good word for “discover”. He had to use awkward circumlocutions like “I saw something unknown to all previous astronomers before me”. The concept of learning new truths by looking through a glass tube would have been totally foreign to an LLM of the early 1600s, as it was to most of the </span><em>people </em><span>of the early 1600s, with a few notable exceptions.</span></p><p><span>You would get better scientific descriptions from a 2025 bag of words than you would from a 1600 bag of words. But both bags might be equally bad at producing the scientific ideas of their respective futures. Scientific breakthroughs often require doing things that are </span><a href="https://www.experimental-history.com/p/the-anarchist-and-the-hockey-stick?utm_source=publication-search" rel="">irrational and unreasonable for the standards of the time</a><span> and good ideas </span><a href="https://www.experimental-history.com/p/the-anarchist-and-the-hockey-stick" rel="">usually</a><span> </span><a href="https://www.experimental-history.com/p/three-dumb-studies-for-your-consideration" rel="">look</a><span> </span><a href="https://nintil.com/discoveries-ignored/" rel="">stupid</a><span> when they first arrive, so they are often—with good reason!—rejected, dismissed, and ignored. This is a big problem for a bag of words that contains all of </span><em>yesterday’s </em><span>good ideas. Putting new ideas in the bag will often make the bag worse, on average, because most of those new ideas will be wrong. That’s why revolutionary research requires not only intelligence, but also </span><a href="https://slimemoldtimemold.com/2022/02/10/the-scientific-virtues/" rel="">stupidity</a><span>. I expect humans to remain usefully stupider than bags of words for the foreseeable future.</span></p><p><span>The most important part of the “bag of words” metaphor is that it prevents us from thinking about AI in terms of </span><em>social status</em><span>. Our ancestors had to play status games well enough to survive and reproduce—losers, by and large, don’t get to pass on their genes. This has left our species exquisitely attuned to who’s up and who’s down. Accordingly, we can turn anything into a competition: </span><a href="https://en.wikipedia.org/wiki/Cooper%27s_Hill_Cheese-Rolling_and_Wake" rel="">cheese rolling</a><span>, </span><a href="https://www.bbc.com/news/articles/cd111ej6vd0o" rel="">nettle eating</a><span>, </span><a href="https://en.wikipedia.org/wiki/Mobile_phone_throwing" rel="">phone throwing</a><span>, </span><a href="https://www.theguardian.com/lifeandstyle/2023/jul/07/experience-im-the-toe-wrestling-world-champion" rel="">toe wrestling</a><span>, and </span><a href="https://www.youtube.com/watch?v=KPQ6TuvqX7w&amp;t=131s" rel="">ferret legging</a><span>, where male contestants, sans underwear, put live ferrets in their pants for as long as they can. (The world record is </span><a href="https://www.topendsports.com/sport/unusual/ferret-legging.htm" rel="">five hours and thirty minutes</a><span>.)</span></p><p><span>When we personify AI, we mistakenly make it a competitor in our status games. That’s why we’ve been arguing about artificial intelligence like it’s a new kid in school: is she cool? Is she smart? Does she have a crush on me? The better AIs have gotten, the more status-anxious we’ve become. If these things are like people, then we gotta know: are we </span><em>better </em><span>or </span><em>worse </em><span>than them? Will they be our masters, our rivals, or our slaves? Is their art finer, their short stories tighter, their insights sharper than ours? If so, there’s only one logical end: ultimately, we must either kill them or worship them.</span></p><p><span>But a bag of words is not a spouse, a sage, a sovereign, or a serf. It’s a tool. Its purpose is to automate our drudgeries and amplify our abilities. Its social status is NA; it makes no sense to ask whether it’s “better” than us. The real question is: does using it make </span><em>us </em><span>better?</span></p><p>That’s why I’m not afraid of being rendered obsolete by a bag of words. Machines have already matched or surpassed humans on all sorts of tasks. A pitching machine can throw a ball faster than a human can, spellcheck gets the letters right every time, and autotune never sings off key. But we don’t go to baseball games, spelling bees, and Taylor Swift concerts for the speed of the balls, the accuracy of the spelling, or the pureness of the pitch. We go because we care about humans doing those things. It wouldn’t be interesting to watch a bag of words do them—unless we mistakenly start treating that bag like it’s a person.</p><p><span>(That’s also why I see no point in using AI to, say, write an essay, just like I see no point in bringing a forklift to the gym. Sure, it can lift the weights, but I’m not trying to suspend a barbell above the floor for the hell of it. I lift it because I want to become the kind of </span><em>person </em><span>who can lift it. Similarly, I write because I want to become the kind of person who can think.)</span></p><p><span>But that doesn’t mean I’m unafraid of AI entirely. I’m plenty afraid! Any tool can be dangerous when used the wrong way—nail guns and nuclear reactors can kill people just fine without having a mind inside them. In fact, the “bag of words” metaphor makes it clear that AI can be dangerous </span><em>precisely because </em><span>it doesn’t operate like humans do. The dangers we face from humans are scary but familiar: hotheaded humans might kick you in the head, reckless humans might drink and drive, duplicitous humans might pretend to be your friend so they can steal your identity. We can guard against these humans because we know how they operate. But we don’t know what’s gonna come out of the bag of words. For instance, if you show humans computer code that has security vulnerabilities, they do not suddenly start praising Hitler. But </span><a href="https://martins1612.github.io/emergent_misalignment_betley.pdf" rel="">LLMs do</a><span>.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-8-169990157" target="_self" rel="">8</a></span><span> So yes, I would worry about putting the nuclear codes in the bag.</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-9-169990157" target="_self" rel="">9</a></span></p><p><span>Anyone who has owned an old car has been tempted to interpret its various malfunctions as part of its </span><em>temperament</em><span>. When it won’t start on a cold day, it feels like the appropriate response is to plead, the same way you would with a sleepy toddler or a tardy partner:</span><em> </em><span>“C’mon Bertie, we gotta get to the dentist!” But ultimately, person perception is a poor guide to vehicle maintenance. Cars are made out of metal and plastic that turn gasoline into forward motion; they are not made out of bones and meat that turn Twinkies into thinking. If you want to fix a broken car, you need a wrench, a screwdriver, and a blueprint, not a cognitive-behavioral therapy manual.</span></p><p>Similarly, anyone who sees a mind inside the bag of words has fallen for a trick. They’ve had their evolution exploited. Their social faculties are firing not because there’s a human in front of them, but because natural selection gave those faculties a hair trigger. For all of human history, something that talked like a human and walked like a human was, in fact, a human. Soon enough, something that talks and walks like a human may, in fact, be a very sophisticated logistic regression. If we allow ourselves to be seduced by the superficial similarity, we’ll end up like the moths who evolved to navigate by the light of the moon, only to find themselves drawn to—and ultimately electrocuted by—the mysterious glow of a bug zapper.</p><p><span>Unlike moths, however, we aren’t stuck using the instincts that natural selection gave us. We can </span><em>choose </em><span>the schemas we use to think about technology. We’ve done it before: we don’t refer to a backhoe as an “artificial digging guy” or a crane as an “artificial tall guy”. We don’t think of books as an “artificial version of someone talking to you”, photographs as “artificial visual memories”, or listening to recorded sound as “attending an artificial recital”. When pocket calculators debuted, they were already smarter than every human on Earth, at least when it comes to calculation—a job that itself </span><a href="https://en.wikipedia.org/wiki/Computer_(occupation)" rel="">used to be done by humans</a><span>. Folks </span><a href="https://www.nytimes.com/1972/08/20/archives/handheld-calculators-tool-or-toy.html?searchResultPosition=4" rel="">wondered</a><span> whether this new technology was “a tool or a toy”, but nobody seems to have wondered whether it was a </span><em>person</em><span>.</span></p><p><span>(If you covered a backhoe with skin, made its bucket look like a hand, painted eyes on its chassis, and made it play a sound like “hnngghhh!” whenever it lifted something heavy, </span><em>then </em><span>we’d start wondering whether there’s a ghost inside the machine. That wouldn’t tell us anything about backhoes, but it would tell us a lot about our own psychology.)</span></p><p><span>The original sin of artificial intelligence was, of course, calling it artificial intelligence. Those two words have lured us into making man the measure of machine: “Now it’s as smart as an undergraduate...now it’s as smart as a PhD!” These comparisons only give us the </span><em>illusion</em><span> of understanding AI’s capabilities and limitations, as well as our own, because we don’t actually know what it means to be smart in the first place. Our definitions of intelligence are either </span><a href="https://www.experimental-history.com/p/why-arent-smart-people-happier" rel="">wrong</a><span> (“Intelligence is the ability to solve problems”) or tautological (“Intelligence is the ability to do things that require intelligence”).</span><span data-state="closed"><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-10-169990157" href="https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us#footnote-10-169990157" target="_self" rel="">10</a></span></p><p>It’s unfortunate that the computer scientists figured out how to make something that kinda looks like intelligence before the psychologists could actually figure out what intelligence is, but here we are. There’s no putting the cat back in the bag now. It won’t fit—there’s too many words in there.</p><p>PS it’s been a busy week on Substack—</p><p><span> and I discussed why people get so anxious about conversations, and how to have better ones:</span></p><p><span>And </span></p><p><span> at </span><span data-state="closed"><a href="https://open.substack.com/pub/chrisdallariva" target="_blank" rel="noopener" data-attrs="{&quot;name&quot;:&quot;Can't Get Much Higher&quot;,&quot;id&quot;:1308018,&quot;type&quot;:&quot;pub&quot;,&quot;url&quot;:&quot;https://open.substack.com/pub/chrisdallariva?utm_source=mentions&quot;,&quot;uuid&quot;:&quot;6edd7578-5860-4c3b-9186-6fcaebf79132&quot;}" data-component-name="MentionPub">Can't Get Much Higher</a></span><span> answered all of my questions about music. He uncovered some surprising stuff, including an issue that caused a civil war on a Beatles message board, and whether they really sang naughty words on the radio in the 1970s:</span></p><p>Derek and Chris both run terrific Substacks, check ‘em out!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How I block all online ads (288 pts)]]></title>
            <link>https://troubled.engineer/posts/no-ads/</link>
            <guid>46185816</guid>
            <pubDate>Sun, 07 Dec 2025 22:18:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://troubled.engineer/posts/no-ads/">https://troubled.engineer/posts/no-ads/</a>, See on <a href="https://news.ycombinator.com/item?id=46185816">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><div><p>Ads support content creators and free services. If you value specific creators or platforms, consider supporting them directly through memberships or donations rather than relying solely on ad blocking.</p></div><h2 id="intro"><a aria-label="Anchor link for: intro" href="#intro">Intro</a></h2><p>A couple of years ago, I decided I'd had enough of ads. Not just the occasional banner or a quick pre-roll video — I mean <em>all</em> of them. They have to go.</p><p>So I embarked on a holy crusade to get rid of them as much as possible. I tried the obvious solutions first, then dug deeper into less conventional approaches. It took a long time, tons of experiments, and many observations, but today I am finally happy where I stand.</p><p>There are many techniques out there, some well-known and others surprisingly obscure. Here's what I learned over the years and what actually worked for me.</p><p>Let's start with the basics and work our way up to the more unconventional methods. The first few are straightforward and widely used. The later ones require more setup and maintenance but can block ads in places where traditional methods fail.</p><h2 id="browser-extensions"><a aria-label="Anchor link for: browser-extensions" href="#browser-extensions">Browser extensions</a></h2><p>Browser ad blockers are <a href="https://doc.searls.com/2015/09/28/beyond-ad-blocking-the-biggest-boycott-in-human-history/" rel="noopener" target="_blank">the biggest boycott in history</a>. You're probably using one already!</p><p>I use Firefox with <a href="https://github.com/gorhill/uBlock" rel="noopener" target="_blank">uBlock Origin</a> — it's the best ad blocking combo out there. It's harder if you're on Chromium-based browser, since <a href="https://infosec.exchange/@catsalad/111426154930652642" rel="noopener" target="_blank">Google transitioned to Manifest V3</a> which conveniently limits ad blockers.</p><p>I keep my filter lists minimal — they cover almost everything I need:</p><ul><li>Built-in uBlock filters</li><li>EasyList</li><li>AdGuard - Ads</li></ul><p>I also maintain <a href="https://github.com/strlght/ublock-declutter" rel="noopener" target="_blank">my own filters</a>. They don't focus on ads, but rather on other annoyances.</p><h2 id="dns-filtering"><a aria-label="Anchor link for: dns-filtering" href="#dns-filtering">DNS filtering</a></h2><p>DNS filtering complements browser extensions by catching ads that slip through — particularly in mobile apps. Mobile apps typically load ads from dedicated ad-serving domains, making them straightforward to block at the DNS level.</p><p><a href="https://pi-hole.net/" rel="noopener" target="_blank">Pi-hole</a> and <a href="https://adguard.com/en/adguard-home/overview.html" rel="noopener" target="_blank">AdGuard Home</a> are the most popular self-hosted options for this. If you're looking for a cloud-based solution, I don't use them myself, but I've heard good things about <a href="https://nextdns.io/" rel="noopener" target="_blank">NextDNS</a>.</p><p>I use Pi-hole, and it's been smooth so far. I don't expose it publicly — instead, I connect via WireGuard and set Pi-hole as the DNS server in my WireGuard config. If you're looking for blocklists, <a href="https://firebog.net/" rel="noopener" target="_blank">The Firebog</a> is a great starting point. You'll also want to maintain an allowlist — blocklists occasionally include legitimate domains that break functionality on websites or in apps.</p><p>There are <a href="https://docs.pi-hole.net/main/basic-install/" rel="noopener" target="_blank">multiple ways to install Pi-hole</a>, I keep it in Docker and suggest you do the same.</p><h2 id="vpn-via-cloud"><a aria-label="Anchor link for: vpn-via-cloud" href="#vpn-via-cloud">VPN via cloud</a></h2><p>Now here comes a secret ingredient. If you route all your traffic through a popular cloud provider (via VPN or proxy), then many online platforms are less likely to show you ads.</p><p>That happens because to these platforms you look like a fraudster doing something sketchy with their ads. Imagine this scenario: a small business spends $1000 on ads. Their competitors figure out the targeting, mimic that behavior, spin up 10 VMs, and waste the entire advertising budget on fake interactions. The small business isn't coming back to spend more money on ads after that experience.</p><p>Online platforms are well aware of this, so they fight fraud. Not serving ads to traffic from public cloud providers is one of the first steps they take.</p><p>However, this will negatively affect your experience on some sites — you'll hit Cloudflare captchas and HTTP errors due to sites blocking cloud provider IPs. I'm fine with it and just turn the VPN off occasionally when something breaks. Just keep in mind that even a few requests with your real IP might be enough for an online platform to start showing you ads again.</p><p>I host WireGuard on a $5 <a href="https://digitalocean.com/" rel="noopener" target="_blank">DigitalOcean</a> droplet, but Hetzner, Azure, Google Cloud, AWS, and others work just as well. DigitalOcean also provides <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-wireguard-on-ubuntu-22-04" rel="noopener" target="_blank">a detailed guide</a> on how to set it up.</p><h2 id="other-useful-stuff"><a aria-label="Anchor link for: other-useful-stuff" href="#other-useful-stuff">Other useful stuff</a></h2><p>Below you'll find some other useful things, although they aren't <em>exactly</em> related to ad-blocking:</p><ul><li>Browser extensions against annoyances: <ul><li>Cookie popups: <a href="https://github.com/cavi-au/Consent-O-Matic" rel="noopener" target="_blank">Consent-O-Matic</a></li><li>Captchas: <a href="https://github.com/dessant/buster" rel="noopener" target="_blank">Buster</a></li></ul></li><li>I'd also suggest <a href="https://sponsor.ajay.app/" rel="noopener" target="_blank">SponsorBlock</a> — it has saved me so much time. There's also <a href="https://github.com/dmunozv04/iSponsorBlockTV" rel="noopener" target="_blank">an option for TVs and streaming devices</a>.</li><li>If you're on iOS, consider turning off <a href="https://support.apple.com/en-us/118408" rel="noopener" target="_blank">Background App Refresh</a>. Only a few apps use Background App Refresh as Apple designed it, the majority are simply abusing it to get more data about you. If you don't have always-on VPN, you risk exposing your real IP.</li><li>Patched apps are also a thing, and it's also possible to patch mobile apps yourself via <a href="https://revanced.app/" rel="noopener" target="_blank">ReVanced</a>. While it's a decent option, it's also a security risk — I'm careful with it and don't use it with sensitive accounts.</li></ul><h2 id="personal-experience"><a aria-label="Anchor link for: personal-experience" href="#personal-experience">Personal experience</a></h2><p>I've been using all these things mentioned above for over 3 years now. I barely see any ads nowadays. If you're curious about specifics, I keep track of what works where:</p><table><thead><tr><th>Platform</th><th>Web</th><th>iOS / Android</th></tr></thead><tbody><tr><td>YouTube</td><td>uBlock Origin</td><td><a href="https://newpipe.net/" rel="noopener" target="_blank">NewPipe</a> or <a href="https://invidious.io/" rel="noopener" target="_blank">Invidious</a></td></tr><tr><td>Instagram</td><td>uBlock Origin</td><td>VPN via cloud (takes a week to a month)</td></tr><tr><td>Twitch</td><td>VPN via cloud (takes a few days)</td><td>-</td></tr><tr><td>TikTok</td><td>uBlock Origin</td><td>VPN via cloud (takes a few hours)</td></tr><tr><td>Apps with AdMob</td><td>-</td><td>DNS blocking</td></tr></tbody></table><p>These are the tricky outliers. For most sites and apps, DNS filtering and a browser ad blocker catch 99% of ads without any extra effort. The VPN approach helps with that remaining 1%, though it usually takes time to kick in — these platforms don't make decisions based on seeing your IP once, they need to observe patterns over days or weeks.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mechanical power generation using Earth's ambient radiation (163 pts)]]></title>
            <link>https://www.science.org/doi/10.1126/sciadv.adw6833</link>
            <guid>46185576</guid>
            <pubDate>Sun, 07 Dec 2025 21:55:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/doi/10.1126/sciadv.adw6833">https://www.science.org/doi/10.1126/sciadv.adw6833</a>, See on <a href="https://news.ycombinator.com/item?id=46185576">Hacker News</a></p>
Couldn't get https://www.science.org/doi/10.1126/sciadv.adw6833: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[XKeyscore (108 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/XKeyscore</link>
            <guid>46185060</guid>
            <pubDate>Sun, 07 Dec 2025 20:54:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/XKeyscore">https://en.wikipedia.org/wiki/XKeyscore</a>, See on <a href="https://news.ycombinator.com/item?id=46185060">Hacker News</a></p>
Couldn't get https://en.wikipedia.org/wiki/XKeyscore: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Syncthing-Android have had a change of owner/maintainer (155 pts)]]></title>
            <link>https://github.com/researchxxl/syncthing-android/issues/16</link>
            <guid>46184730</guid>
            <pubDate>Sun, 07 Dec 2025 20:15:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/researchxxl/syncthing-android/issues/16">https://github.com/researchxxl/syncthing-android/issues/16</a>, See on <a href="https://news.ycombinator.com/item?id=46184730">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><h3 dir="auto">Description of the issue</h3>
<p dir="auto">status</p>
<h3 dir="auto">Steps to reproduce</h3>
<p dir="auto">invite nel0x here and get help to carry on<br>
setup build and release: use old maintainers signing allowed? can we play sign?<br>
reinstate gh action workflows<br>
contact fdroid for release continuation<br>
general: is the name syncthing fork ok or should be changed?</p>
<h3 dir="auto">App version</h3>
<p dir="auto">123</p>
<h3 dir="auto">App install source - see wiki for details on release channels</h3>
<p dir="auto">GitHub or F-Droid release build</p>
<h3 dir="auto">Android version</h3>
<p dir="auto">123</p>
<h3 dir="auto">ROM vendor</h3>
<p dir="auto">123</p>
<h3 dir="auto">Device manufacturer</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">Device model</h3>
<p dir="auto"><em>No response</em></p>
<h3 dir="auto">Device platform info (optional)</h3>

<h3 dir="auto">Android log (logcat)</h3>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Evidence from the One Laptop per Child program in rural Peru (120 pts)]]></title>
            <link>https://www.nber.org/papers/w34495</link>
            <guid>46184575</guid>
            <pubDate>Sun, 07 Dec 2025 19:56:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nber.org/papers/w34495">https://www.nber.org/papers/w34495</a>, See on <a href="https://news.ycombinator.com/item?id=46184575">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <a id="main-content" tabindex="-1"></a>
        <div id="block-nber-breadcrumbs">
  
    
        
  
      <nav aria-label="You are here:">
      
      <ul>
                  <li>
                          <a href="https://www.nber.org/">Home</a>
                      </li>
                  <li>
                          <a href="https://www.nber.org/research">Research</a>
                      </li>
                  <li>
                          <a href="https://www.nber.org/papers">Working Papers</a>
                      </li>
                  <li>
                          Laptops in the Long Run: Evidence from…
                      </li>
              </ul>
    </nav>
  
  </div>

  
  
        

<div>
  <div>
        <p><span>Working Paper</span> 34495
  </p>

        <p><span>DOI</span> 10.3386/w34495
  </p>

        <p><span>Issue Date</span> <time datetime="2025-11-21T12:00:00Z">November 2025</time>

  </p>

          </div>
  <div>
    <p>
This paper examines a large-scale randomized evaluation of the One Laptop Per Child (OLPC) program in 531 Peruvian rural primary schools. We use administrative data on academic performance and grade progression over 10 years to estimate the long-run effects of increased computer access on (i) school performance over time and (ii) students’ educational trajectories. Following schools over time, we find no significant effects on academic performance but some evidence of negative effects on grade progression. Following students over time, we find no significant effects on primary and secondary completion, academic performance in secondary school, or university enrollment. Survey data indicate that computer access significantly improved students’ computer skills but not their cognitive skills; treated teachers received some training but did not improve their digital skills and showed limited use of technology in classrooms, suggesting the need for additional pedagogical support.
</p>
  </div>
  
</div>

  

  

<div>
  <ul>
          <li>
        
      </li>
    
    <li>
      <div id="accordion-body-guid2" aria-labelledby="accordion-button-guid2">
        <p>Copy Citation</p>
        <div>
            <p>
                Santiago Cueto, Diether W. Beuermann, Julian Cristia, Ofer Malamud, and Francisco Pardo, "Laptops in the Long Run: Evidence from the One Laptop per Child Program in Rural Peru," NBER Working Paper 34495 (2025), https://doi.org/10.3386/w34495.
            </p>

            </div>
                    <p>Download Citation</p>
            

            </div>    </li>

    
      </ul>
</div>


<div>
    <h2>Related</h2>
    <div>
    
      
                <div>
    <h3>Topics</h3>
    
  </div>

      
                <div>
    <h3>Programs</h3>
    
  </div>

      
      
      
      
      
      
      
      
      
      
      
          </div>
  </div>


  
  
        <section id="block-morefromthenber">
    <h2>More from the NBER</h2>
    
    <div>
    

<div data-href="/research/videos/2025-17th-annual-feldstein-lecture-n-gregory-mankiw-fiscal-future">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-07/MF%20Lecture%202025%20updated.png?itok=ij7zY5fj" width="736" height="414" alt=" 2025, 17th Annual Feldstein Lecture, N. Gregory Mankiw,&quot; The Fiscal Future&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Feldstein Lecture</li>
    </ul>
  
      <ul>
      <li>
        Presenter:
        

              <span>
      <a href="https://www.nber.org/people/gregory_mankiw">N. Gregory Mankiw</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>N. Gregory Mankiw, Robert M. Beren Professor of Economics at Harvard University, presented the 2025 Martin Feldstein...</p>
  </div>

    

<div data-href="/research/videos/2025-methods-lecture-raj-chetty-and-kosuke-imai-uncovering-causal-mechanisms-mediation-analysis-and">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-07/Methods%20Lecture%20SI%202025_0.png?itok=hsgorA8D" width="736" height="414" alt=" 2025 Methods Lecture, Raj Chetty, &quot;Uncovering Causal Mechanisms: Mediation Analysis and Surrogate Indices&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Methods Lectures</li>
    </ul>
  
      <ul>
      <li>
        Presenters:
        

              <span>
      <a href="https://www.nber.org/people/raj_chetty">Raj Chetty</a>    </span>
                  <span>
       &amp; <a href="https://www.nber.org/people/kosuke_imai">Kosuke Imai</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>SlidesBackground materials on mediationImai, Kosuke, Dustin Tingley, and Teppei Yamamoto. (2013). “Experimental Designs...</p>
  </div>

    

<div data-href="/research/videos/2025-international-trade-and-macroeconomics-panel-future-global-economy">
      <p><img loading="lazy" src="https://www.nber.org/sites/default/files/styles/promo/public/2025-08/SI%20International%20trade%20Panel%202025.png?itok=2DuJTJDm" width="736" height="414" alt="2025 International Trade and Macroeconomics, &quot;Panel on The Future of the Global Economy&quot;" typeof="foaf:Image">



    </p>
  
  

      <ul>
      <li>Panel Discussion</li>
    </ul>
  
      <ul>
      <li>
        Presenters:
        

              <span>
      <a href="https://www.nber.org/people/oleg_itskhoki">Oleg Itskhoki</a>,     </span>
                  <span>
      <a href="https://www.nber.org/people/paul_krugman">Paul R. Krugman</a>    </span>
                  <span>
       &amp; <a href="https://www.nber.org/people/linda_tesar">Linda Tesar</a>    </span>
      
      </li>
    </ul>
  
  
  
      <p>Supported by the Alfred P. Sloan Foundation grant #G-2023-19633, the Lynde and Harry Bradley Foundation grant #20251294...</p>
  </div>
</div>
  </section>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Estimates are difficult for developers and product owners (212 pts)]]></title>
            <link>https://thorsell.io/2025/12/07/estimates.html</link>
            <guid>46184229</guid>
            <pubDate>Sun, 07 Dec 2025 19:17:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thorsell.io/2025/12/07/estimates.html">https://thorsell.io/2025/12/07/estimates.html</a>, See on <a href="https://news.ycombinator.com/item?id=46184229">Hacker News</a></p>
<div id="readability-page-1" class="page"><section itemprop="text">
        
        <blockquote>
  <p><strong>Product Owner:</strong> Hey, how long do you believe <code>Feature F</code> will take?</p>

  <p><strong>Developer:</strong> Idk. We haven’t even started working on it and it’s bound to stir up some old issues.</p>
</blockquote>

<p>Estimates come in various disguises, but when you peek under the trench coat there is always the question:</p>

<p>
"How long -- and using what amount of resources -- will be required to do <code>X</code>?"
</p>

<p>When I wear the <em>developer hat</em>, it can be infuriating to attempt to give an answer. It’s difficult to estimate (or the
product owner could do it themselves) and a lot of the time it can be difficult to see why the estimate is even
important.</p>

<p>When I wear the <em>product owner hat</em>, estimates are a crucial piece of the puzzle that must be laid in an attempt to plan
the short <em>and</em> long term life cycle of a product.</p>

<p>In this post I want to attempt to explore and elaborate on both sides, in an attempt to make developers understand <em>why
estimates are important to product owners</em> and in order to help product owners see <em>why developers so often despise
having to estimate their work</em>.</p>

<h2 id="why-the-po-wants-you-to-estimate">Why the PO wants you to estimate</h2>

<p>As a Product Owner (PO), I am responsible for <em>learning the market and customers’ needs</em> and translating these into
<em>feature requests which developers can turn into actual features in our products</em>. The means varies, but most
organisations have some sort of backlog in which <em>things to be acted upon</em> are placed while they await being <em>picked up</em>
by some developer or development team. We call these <em>things</em> user stories, issues, tickets, tasks, <em>and probably many
other things</em>… The important thing for this discussion is that the items in the backlog are candidates for being
implemented in our product and it’s the PO’s job to prioritise the backlog.</p>

<p>Why does the backlog need to be prioritised?</p>

<p>Because the inflow of items to the backlog is (pretty much always) higher than the speed at which the developers can
implement them. Ergo, if the PO does not constantly <em>learn the market and customers’ needs</em> and prioritise the backlog
accordingly, the developers might implement features that the users of the product are not interested in. Worst case?
Existing users stop using the product and no new users buy it which will ultimately lead to bankruptcy.</p>

<h3 id="but-what-about-the-estimates">But what about the estimates?</h3>

<p>The above makes sense – I hope – but it doesn’t really pinpoint the need for estimates. Unfortunately, the job of a PO
is not as easy as always prioritising in accordance to whatever the market wants. More often than not, the PO must also
consider pre-communicated release dates and manage expectations.</p>

<blockquote>
  <p>I hate when release dates are communicated in advance. The only thing worse than release dates that are set in stone
months ahead of time (I’m looking at you, Mr 12-week-increments-SAFe) are releases with pre-communicated content.
Unfortunately, both are common. Often combined.</p>
</blockquote>

<p>Imagine a backlog in which resides a really big feature. Something that is sought after, but will take a lot of time and
resources to implement. The same backlog has a handful of smaller features which are not as requested as the big one.
The PO would really like to include the big feature in the next release, but the next release date is not so far away.
If the PO prioritises the big feature but it’s not done in time for <em>the already communicated release date</em>, the release
will be severely lacking and considered a failure. In that case, the PO would rather include a couple of the smaller
features. A safer bet, but the payoff is smaller.</p>

<p><strong>THIS</strong> is why estimates matter so much to product owners. They must constantly run the above equation when they
prioritise the teams’ backlogs. A constant risk/reward balancing act. They undoubtedly need help from the experts (the
developers) to better understand the ramifications of the features they are proposing. If POs do not understand how big
different <em>work packages</em> are, they cannot do their jobs in an effective way.</p>

<h3 id="it-gets-worse">It gets worse</h3>

<p>Instead of one PO there are now a couple of them. They are responsible for different <em>parts</em> of a larger product which
requires the POs to coordinate both the date <em>and</em> the content of their releases. There is probably a <em>main backlog</em>
describing upcoming features in the final product, as well as <em>team backlogs</em> where each team are assigned puzzle pieces
which must be implemented and integrated in a coordinated fashion.</p>

<p>This is painful in multiple ways, but the most obvious issue is that – in order to have a functioning release – the
POs must agree on the prioritisation of the <em>main backlog</em> and this will in turn affect the prioritisation of the <em>team
backlogs</em>. The POs must each acquire information about how long it will take (and how costly it will be) to implement
and to integrate the puzzle piece(s) they are responsible for into a cohesive feature. The tool for acquiring this
<em>idea</em>?</p>

<p>
    Estimates.
</p>

<h2 id="technical-debt">Technical debt</h2>

<p>Programming is a craft. An art. My art, to some extent. I’m in my happy place when I get to succumb to a tricky task and
surface a couple of days later with a solution to a problem that initially seemed impossible. As a developer, I want to
build the best possible product. I dislike shortcuts. Half-arsed solutions. <em>Fixes.</em> Not because a single shortcut or
fix will destroy a product, but because the <a href="https://en.wikipedia.org/wiki/Technical_debt"><em>technical debt</em></a> they
incur will accumulate over time and eventually erode the product from the inside out; making it ever more difficult to
work with it and ultimately cause it to break.</p>

<p>Technical debt is – I believe – the main reason for conflict between a PO and a development team. A not so technically
inclined PO will fail to see how detrimental technical debt is to the product and how painful it is for the developers
to work in a code base with a high amount of debt.</p>

<p>Put in other words: If I’m tasked with implementing a new feature and I come across something in the code that is
obviously smelly, error prone, or just not very good, I want to leave the code in better shape than I found it. Not
taking time to “payoff” such debt <em>once</em> might not be the end of the world, but the hard coded quick-fix that you know
ought to be generalised will likely bite you down the road. And if you have ignored updating dependencies for a couple
of months and find yourself in a situation where you <em>need</em> to upgrade <code>Package 1</code>, but it depends on a newer version of
<code>Packages 2 &amp; 3</code>, which in turn requires a framework upgrade… Let’s just say the feature you’re working on will take
a while longer.</p>

<h2 id="why-developers-hate-estimates">Why developers HATE estimates</h2>

<p>When a PO asks: “How long will it take to implement <code>Feature F</code>?”, they aren’t just asking the developers to estimate
the amount of time they think it will take to write the code for the feature. A good PO understands that implementing a
new feature is an iterative process and that <em>integration hell</em> is a thing. An even better PO understands that they are
also asking the team to estimate how many unforeseen issues they will encounter while implementing the feature.</p>

<p>This detail: <em>The unforeseen issues</em>, which the PO asks the developers to foresee, is key. It is – per definition –
not possible to foresee something unforeseeable.</p>

<p>Many developers I’ve met dislike uncertainty. One of the things they appreciate most about coding is the deterministic
aspect of it. You run the same program again and again and it returns the same results.<sup id="fnref:determinism" role="doc-noteref"><a href="#fn:determinism" rel="footnote">1</a></sup> The journey on
which we travel while writing the code is, however, not particularly deterministic.</p>

<p>It is true, that the more you code and the more familiar you get with a codebase, the more accurate your estimates will
be. However, just the other day I was working on an issue which I had estimated would take <em>approximately two days</em>. All
of a sudden, I realised that the simple change required updating a shared component that had been tightly coupled years
ago. When I touched that code, dozens of failing tests appeared, each revealing another hidden dependency. Fixing those
uncovered yet another module depending on outdated patterns. Halfway through, we decided we had to refactor the entire
flow just to make the original change safe. My “two-day task” turned into two weeks of archaeological software
excavation.</p>

<p>Could we have solved this quicker by not caring so much about the amount of technical debt we left in our wake?
Probably.</p>

<p>Would we have encountered a two <em>month</em> excavation in the future? Probably.</p>

<h3 id="it-gets-worse-1">It gets worse</h3>

<p><a href="https://www.merriam-webster.com/dictionary/estimate">According to Merriam-Webster</a>: <em>estimate</em> is defined as:</p>

<blockquote>
  <p>To judge tentatively or approximately the value, worth, or significance of.</p>
</blockquote>

<p>The very definition of <em>estimates</em> tells us that they are either <em>tentative</em> or <em>approximate</em>. As a developer, I choose
to interpret the <em>or</em> as meaning that it could even be both.</p>

<p>When I started my career as a software developer, I really did not have an issue with estimates. We would refine our
backlog and I would gladly give an estimate on various items. (1) Because I was fresh out of university and wanted to
prove myself by doing a good job and not being too difficult, but more importantly: (2) because I had not understood
that my estimates would soon be used against me.</p>

<p>I soon learned that my team’s estimates were not interpreted and used as <em>estimates</em>. They were used as <em>deadlines</em>. If
we broke down a feature into its reasonable components (an error prone science, which introduces uncertainties, on its
own) and estimated the parts accordingly, the PO would often take the sum of the parts and communicate it to their
colleagues as: “This is the time we will be done.”</p>

<p>Two things came out of this:</p>

<ol>
  <li>My team (consisting mostly of newly graduated developers) became much more reluctant to estimate.</li>
  <li>When we estimated we always padded our <em>actual beliefs</em>, significantly, to give ourselves a buffer.</li>
</ol>

<p>The estimates stopped being estimates. They became safety railings against being held accountable for unreasonable
expectations.</p>

<h2 id="the-clash">The clash</h2>

<p>Do you see the problem?</p>

<p>Do you see a solution?</p>

<p>I believe the overarching problem with estimates stems from expectations. Somewhere, someone, communicates <em>something</em>
to the users/customers of the product, which sets expectations the rest of the organisation are then forced to live up
to. In a small company, it might very well be the PO who does that communication but in a larger organisation the PO is
likely as helpless as the developers w.r.t. having a say about the product’s roadmap.</p>

<p>The “solution” is simple: Stop communicating new features in advance. Stop setting more or less arbitrary
deadlines<sup id="fnref:deadlines" role="doc-noteref"><a href="#fn:deadlines" rel="footnote">2</a></sup>. Let the PO tell the developers what features they want, in what order, and let the developers do
what they do best: Code!</p>

<p>But these deadlines are there for a reason. If your company builds a product which assists people doing their yearly tax
returns, a missed delivery window will result in the entire revenue opportunity for that year being missed. Resources
(most often in terms of salaries to employees) will have been poured into a project and if there’s no payoff in terms
of additional sales, it could lead to a need for finding other ways to reclaim those resources; often in terms of
reduced costs, which universally means: lay-offs.</p>

<p>Therefore, it’s in everyone’s best interest to play along. We play the estimates game even though it’s a bad way (but
also the best we know of) to help each other do our respective jobs.</p>

<h2 id="what-about-devops">What about DevOps?</h2>

<p><em>You didn’t think I’d miss an opportunity to talk about DevOps, did you?</em></p>

<p><em>Flow</em> is a key concept within DevOps which describes an organisation’s ability to reduce bottlenecks and increase the
pace at which they are able to deliver new versions of their product(s). High flow is synonymous with frequent
deliveries and updates of our product(s).</p>

<p>The concepts from DevOps do not directly address the issue with estimates, but there are tools which can be used to
reduce the risk associated with delivering software. Flow can inform how we tackle technical debt and how we make sure
we don’t fall behind on our dependencies. Flow can also help us identify issues in our product’s life cycle as well as
help us understand how to get rid of the issues.</p>

<p>Flow is one of <a href="https://itrevolution.com/articles/the-three-ways-principles-underpinning-devops/"><em>The Three Ways</em></a> in
DevOps and if you want to learn more, feel free to reach out. I give presentations on various topics related to DevOps
and I can come to your company and give a course about DevOps tailored to your company’s needs.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Estimates – as defined in the English language – isn’t really the problem here. The problem is when <em>estimates</em> are
treated as predictions, deadlines, and used to put pressure on developers who are just trying to do their jobs.
Estimates – the way they are used in our industry today – hurts people and reduces the psychological safety in our
organisations. I believe we would be better off if we could work in a way that allows developers to be transparent and
continuously communicate updated estimates as development progresses.</p>

<p>Then again, product owners are people too! As developers we must understand that POs are under pressure too. We must
help them and the best way to help them is to continuously provide them with updates about how development is
progressing and whether we have encountered anything that we believe will significantly alter the original estimate we
gave.</p>

<!-- -->



        
      </section></div>]]></description>
        </item>
    </channel>
</rss>