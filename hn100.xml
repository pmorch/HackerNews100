<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 09 Oct 2024 19:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Dookie Demastered (129 pts)]]></title>
            <link>https://www.dookiedemastered.com/</link>
            <guid>41790295</guid>
            <pubDate>Wed, 09 Oct 2024 17:18:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dookiedemastered.com/">https://www.dookiedemastered.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41790295">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>When an album hits a big milestone like its 30th anniversary, it gets the usual remasters on the usual formats. But <em>Dookie</em> isn't a usual album.</p><p>Instead of smoothing out its edges and tweaking its dynamic ranges, this version of <em>Dookie</em> has been met­icu­lously mangled to fit on formats with uncom­promis­ing­ly low fidelity, from wax cylinders to answering machines to toothbrushes. The listening experience is unparalleled, sacrificing not only sonic quality, but also convenience, and occasionally entire verses.</p><p>The result is <em>Dookie Demastered:</em> the album that exploded the format of punk rock, re-exploded onto 15 obscure, obsolete, and otherwise inconvenient formats, the way it was never meant to be heard.</p></div><div><p><h3>Popó</h3><h3>Caca</h3><h3>Kacke</h3><h3>うんち</h3><h3>Какулька</h3><h3>Cocô</h3></p><div><p>When an album hits a big milestone like its 30th anniversary, it gets the usual remasters on the usual formats. But <em>Dookie</em> isn't a usual album.</p><p>Instead of smoothing out its edges and tweaking its dynamic ranges, this version of <em>Dookie</em> has been met­icu­lously mangled to fit on formats with uncom­promis­ing­ly low fidelity, from wax cylinders to answering machines to toothbrushes. The listening experience is unparalleled, sacrificing not only sonic quality, but also convenience, and occasionally entire verses.</p><p>The result is <em>Dookie Demastered:</em> the album that exploded the format of punk rock, re-exploded onto 15 obscure, obsolete, and otherwise inconvenient formats, the way it was never meant to be heard.</p></div><table><thead><tr><td colspan="3">Track formats</td></tr></thead><tbody><tr><td>1</td><td>Burnout</td><td>Player Piano Roll</td></tr><tr><td>2</td><td>Having A Blast</td><td>Floppy Disk</td></tr><tr><td>3</td><td>Chump</td><td>Teddy Ruxpin</td></tr><tr><td>4</td><td>Longview</td><td>Doorbell</td></tr><tr><td>5</td><td>Welcome to Paradise</td><td>Game Boy Cartridge</td></tr><tr><td>6</td><td>Pulling Teeth</td><td>Toothbrush</td></tr><tr><td>7</td><td>Basket Case</td><td>Big Mouth Billie Bass</td></tr><tr><td>8</td><td>She</td><td>HitClip</td></tr><tr><td>9</td><td>Sassafras Roots</td><td>8-Track</td></tr><tr><td>10</td><td>When I Come Around</td><td>Wax Cylinder</td></tr><tr><td>11</td><td>Coming Clean</td><td>X-Ray Record</td></tr><tr><td>12</td><td>Emenius Sleepus</td><td>Answering Machine</td></tr><tr><td>13</td><td>In The End</td><td>MiniDisc</td></tr><tr><td>14</td><td>F.O.D.</td><td>Fisher Price Record</td></tr><tr><td>15</td><td>All By Myself</td><td>Music Box</td></tr></tbody></table><div><p><img src="https://www.dookiedemastered.com/images/musicnote.svg"></p><div><p>Warning</p><div><h4>UNFAITHFUL REPRODUCTIONS</h4><p>Those deeply familiar with the originals may experience existential disquietude.</p></div></div></div><div><p><img src="https://www.dookiedemastered.com/images/earbleed.svg"></p><div><p>Warning</p><div><h4>LOW FIDELITY AUDIO</h4><p>Extended listening has been known to provoke rage-nausea in audiophiles.</p></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An n-ball Between n-balls (124 pts)]]></title>
            <link>https://www.arnaldur.be/writing/about/an-n-ball-between-n-balls</link>
            <guid>41789242</guid>
            <pubDate>Wed, 09 Oct 2024 15:50:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arnaldur.be/writing/about/an-n-ball-between-n-balls">https://www.arnaldur.be/writing/about/an-n-ball-between-n-balls</a>, See on <a href="https://news.ycombinator.com/item?id=41789242">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hk="000000010000000000003"><h2 data-hk="00000001000000000000400010" id="an-n-ball-between-n-balls"><a data-hk="00000001000000000000400000" href="#an-n-ball-between-n-balls">An n-ball Between n-balls</a></h2>
<p data-hk="00000001000000000000400020">There is a geometric thought experiment that is often used to demonstrate the counterintuitive shape of high-dimensional phenomena. This article is an interactive visual journey into the construct in the thought experiment, and the mathematics behind it.</p>
<h2 data-hk="00000001000000000000400040" id="a-square-with-four-circles"><a data-hk="00000001000000000000400030" href="#a-square-with-four-circles">A Square with Four Circles</a></h2>
<p data-hk="000000010000000000004000a240">We start with a <span data-hk="000000010000000000004000a230"><span data-hk="000000010000000000004000a120"><math data-hk="000000010000000000004000a110" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000a100"><mrow data-hk="00000001000000000000400080"><mn data-hk="00000001000000000000400050">4</mn><mo data-hk="00000001000000000000400060">×</mo><mn data-hk="00000001000000000000400070">4</mn></mrow><annotation data-hk="00000001000000000000400090" encoding="application/x-tex">4 \times 4</annotation></semantics></math></span></span> square. There are four blue circles, with a radius of one, packed into the box. One in each corner.
At the center of the box is a red circle. The red circle is as large as it can be, without overlapping the blue circles.
Drag the slider to add a third dimension.
On this and all other 3D diagrams, you can drag the image to rotate it, and scroll to zoom.</p>
<!--!$-->
<canvas data-hk="000000010000000000004000a25" height="400"></canvas><label data-hk="000000010000000000004000a280"><!--$-->Disect construct<!--/--></label>
<p data-hk="000000010000000000004000a290">When extending the construct to 3D, many things happen.
All the circles are now spheres, the red sphere is larger while the blue spheres aren’t,
and there are eight spheres while there were only four circles.</p>
<p data-hk="000000010000000000004000a300">The dimensional extension is decomposed into three phases.
First, the circles and square turn into spheres and a cube.
Next, the central ball grows, and everything else shifts so that it ends in the center.
Finally, four new spheres appear, completing the 3D version of the construct.</p>
<h3 data-hk="000000010000000000004000a320" id="defining-the-construct"><a data-hk="000000010000000000004000a310" href="#defining-the-construct">Defining the Construct</a></h3>
<p data-hk="000000010000000000004000b1210">There are more than one way to extend the construct into higher dimensions, so to make it more rigorous, we will define it like so:
An <span data-hk="000000010000000000004000a430"><span data-hk="000000010000000000004000a380"><math data-hk="000000010000000000004000a370" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000a360"><mrow data-hk="000000010000000000004000a340"><mi data-hk="000000010000000000004000a330">n</mi></mrow><annotation data-hk="000000010000000000004000a350" encoding="application/x-tex">n</annotation></semantics></math></span></span>-dimensional version of the construct consists of an <span data-hk="000000010000000000004000a540"><span data-hk="000000010000000000004000a490"><math data-hk="000000010000000000004000a480" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000a470"><mrow data-hk="000000010000000000004000a450"><mi data-hk="000000010000000000004000a440">n</mi></mrow><annotation data-hk="000000010000000000004000a460" encoding="application/x-tex">n</annotation></semantics></math></span></span>-cube with a side length of <span data-hk="000000010000000000004000a650"><span data-hk="000000010000000000004000a600"><math data-hk="000000010000000000004000a590" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000a580"><mrow data-hk="000000010000000000004000a560"><mn data-hk="000000010000000000004000a550">4</mn></mrow><annotation data-hk="000000010000000000004000a570" encoding="application/x-tex">4</annotation></semantics></math></span></span>.
On the midpoint between each vertex and the center of the <span data-hk="000000010000000000004000a760"><span data-hk="000000010000000000004000a710"><math data-hk="000000010000000000004000a700" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000a690"><mrow data-hk="000000010000000000004000a670"><mi data-hk="000000010000000000004000a660">n</mi></mrow><annotation data-hk="000000010000000000004000a680" encoding="application/x-tex">n</annotation></semantics></math></span></span>-cube, there is an <span data-hk="000000010000000000004000a870"><span data-hk="000000010000000000004000a820"><math data-hk="000000010000000000004000a810" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000a800"><mrow data-hk="000000010000000000004000a780"><mi data-hk="000000010000000000004000a770">n</mi></mrow><annotation data-hk="000000010000000000004000a790" encoding="application/x-tex">n</annotation></semantics></math></span></span>-ball with a radius of one.
In the center of the <span data-hk="000000010000000000004000a980"><span data-hk="000000010000000000004000a930"><math data-hk="000000010000000000004000a920" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000a910"><mrow data-hk="000000010000000000004000a890"><mi data-hk="000000010000000000004000a880">n</mi></mrow><annotation data-hk="000000010000000000004000a900" encoding="application/x-tex">n</annotation></semantics></math></span></span>-cube there is the largest <span data-hk="000000010000000000004000b1090"><span data-hk="000000010000000000004000b1040"><math data-hk="000000010000000000004000b1030" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b1020"><mrow data-hk="000000010000000000004000b1000"><mi data-hk="000000010000000000004000a990">n</mi></mrow><annotation data-hk="000000010000000000004000b1010" encoding="application/x-tex">n</annotation></semantics></math></span></span>-ball that does not intersect any other <span data-hk="000000010000000000004000b1200"><span data-hk="000000010000000000004000b1150"><math data-hk="000000010000000000004000b1140" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b1130"><mrow data-hk="000000010000000000004000b1110"><mi data-hk="000000010000000000004000b1100">n</mi></mrow><annotation data-hk="000000010000000000004000b1120" encoding="application/x-tex">n</annotation></semantics></math></span></span>-ball.</p>
<h2 data-hk="000000010000000000004000b1230" id="building-intuitions"><a data-hk="000000010000000000004000b1220" href="#building-intuitions">Building Intuitions</a></h2>
<p data-hk="000000010000000000004000b1240">We want to extend it into any amount of dimensions but before that, we’ll build some intuitions.</p>
<h3 data-hk="000000010000000000004000b1260" id="2d-intersection-of-3d-construct"><a data-hk="000000010000000000004000b1250" href="#2d-intersection-of-3d-construct">2D Intersection of 3D Construct</a></h3>
<!--!$-->
<canvas data-hk="000000010000000000004000b127" height="400"></canvas><div data-hk="000000010000000000004000b132"><!--$--><p><label data-hk="000000010000000000004000b1330"><!--$-->Link sliders<!--/--></label></p><!--/--><!--$--><p><label data-hk="000000010000000000004000b1340"><!--$-->Isolate plane<!--/--></label></p><!--/--><!--$--><!--/--><!--$--><!--/--></div>
<p data-hk="000000010000000000004000b1370">In the default configuration, the intersecting plane is the same as the construct in 2D.
We see that when we rotate the plane along the center of the left balls, the balls on the right, vanish and are replaced—within the plane—by the two balls, diagonally across.
There are a few important behaviors to note from this interactable.</p>
<ol data-hk="000000010000000000004000b3140">
<li data-hk="000000010000000000004000b1380">When the red ball migrates from the 2D center to the 3D center, it shrinks and vanishes from the 2D plane.</li>
<li data-hk="000000010000000000004000b1390">As seen in the isolate plane perspective, the left balls maintain their size and only shift slightly away from the center.</li>
<li data-hk="000000010000000000004000b3130">The differences between the  and  configuration.
<ol data-hk="000000010000000000004000b3120">
<li data-hk="000000010000000000004000b1830">The enclosing box’s width grows from <span data-hk="000000010000000000004000b1520"><span data-hk="000000010000000000004000b1470"><math data-hk="000000010000000000004000b1460" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b1450"><mrow data-hk="000000010000000000004000b1430"><mn data-hk="000000010000000000004000b1420">4</mn></mrow><annotation data-hk="000000010000000000004000b1440" encoding="application/x-tex">4</annotation></semantics></math></span></span> to <span data-hk="000000010000000000004000b1820"><span data-hk="000000010000000000004000b1600"><math data-hk="000000010000000000004000b1590" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b1580"><mrow data-hk="000000010000000000004000b1560"><mn data-hk="000000010000000000004000b1530">4</mn><msqrt data-hk="000000010000000000004000b1550"><mn data-hk="000000010000000000004000b1540">2</mn></msqrt></mrow><annotation data-hk="000000010000000000004000b1570" encoding="application/x-tex">4\sqrt{2}</annotation></semantics></math></span></span>.</li>
<li data-hk="000000010000000000004000b2270">Similarly, the balls’ horizontal positions shift from <span data-hk="000000010000000000004000b1960"><span data-hk="000000010000000000004000b1900"><math data-hk="000000010000000000004000b1890" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b1880"><mrow data-hk="000000010000000000004000b1860"><mo data-hk="000000010000000000004000b1840">±</mo><mn data-hk="000000010000000000004000b1850">1</mn></mrow><annotation data-hk="000000010000000000004000b1870" encoding="application/x-tex">\pm 1</annotation></semantics></math></span></span> to <span data-hk="000000010000000000004000b2260"><span data-hk="000000010000000000004000b2040"><math data-hk="000000010000000000004000b2030" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b2020"><mrow data-hk="000000010000000000004000b2000"><mo data-hk="000000010000000000004000b1970">±</mo><msqrt data-hk="000000010000000000004000b1990"><mn data-hk="000000010000000000004000b1980">2</mn></msqrt></mrow><annotation data-hk="000000010000000000004000b2010" encoding="application/x-tex">\pm \sqrt{2}</annotation></semantics></math></span></span></li>
<li data-hk="000000010000000000004000b3110">The red ball’s radius <span data-hk="000000010000000000004000b2380"><span data-hk="000000010000000000004000b2330"><math data-hk="000000010000000000004000b2320" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b2310"><mrow data-hk="000000010000000000004000b2290"><mi data-hk="000000010000000000004000b2280">r</mi></mrow><annotation data-hk="000000010000000000004000b2300" encoding="application/x-tex">r</annotation></semantics></math></span></span> grows from <span data-hk="000000010000000000004000b2740"><span data-hk="000000010000000000004000b2470"><math data-hk="000000010000000000004000b2460" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b2450"><mrow data-hk="000000010000000000004000b2430"><msqrt data-hk="000000010000000000004000b2400"><mn data-hk="000000010000000000004000b2390">2</mn></msqrt><mo data-hk="000000010000000000004000b2410">−</mo><mn data-hk="000000010000000000004000b2420">1</mn></mrow><annotation data-hk="000000010000000000004000b2440" encoding="application/x-tex">\sqrt{2} - 1</annotation></semantics></math></span></span> to <span data-hk="000000010000000000004000b3100"><span data-hk="000000010000000000004000b2830"><math data-hk="000000010000000000004000b2820" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b2810"><mrow data-hk="000000010000000000004000b2790"><msqrt data-hk="000000010000000000004000b2760"><mn data-hk="000000010000000000004000b2750">3</mn></msqrt><mo data-hk="000000010000000000004000b2770">−</mo><mn data-hk="000000010000000000004000b2780">1</mn></mrow><annotation data-hk="000000010000000000004000b2800" encoding="application/x-tex">\sqrt{3} - 1</annotation></semantics></math></span></span></li>
</ol>
</li>
</ol>
<h3 data-hk="000000010000000000004000b3160" id="1d-intersection-of-3d-construct"><a data-hk="000000010000000000004000b3150" href="#1d-intersection-of-3d-construct">1D Intersection of 3D Construct</a></h3>
<!--!$-->
<canvas data-hk="000000010000000000004000b317" height="400"></canvas><fieldset data-hk="000000010000000000004000b320"><legend>Camera angle</legend></fieldset>
<p data-hk="000000010000000000004000b3220">This diagram demonstrates the same phenomena as before, except the intersection is one dimensional and diagonalizes first into two, then three dimensions.
Notice the similarities between the previous two diagrams.
The left <em data-hk="000000010000000000004000b3210">ball</em> shifts to the left, maintaining it’s size, while the right ball vanishes to be replaced by another farther away.</p>
<p data-hk="000000010000000000004000b3230">Now we perform the same diagonalization operation, starting from three dimensions.
As visualizing higher dimensions is not intuitive, we can’t rely on a lower dimensional intersection of a higher dimensional object to guide us.
The similarities should be evident however.</p>
<h3 data-hk="000000010000000000004000b3250" id="3d-intersection-of-10d-construct"><a data-hk="000000010000000000004000b3240" href="#3d-intersection-of-10d-construct">3D Intersection of 10D Construct</a></h3>
<!--!$-->
<canvas data-hk="000000010000000000004000b326" height="400"></canvas>
<p data-hk="000000010000000000004000b4190">Two of the boxes dimensions maintain their constant height of <span data-hk="000000010000000000004000b3390"><span data-hk="000000010000000000004000b3340"><math data-hk="000000010000000000004000b3330" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b3320"><mrow data-hk="000000010000000000004000b3300"><mn data-hk="000000010000000000004000b3290">4</mn></mrow><annotation data-hk="000000010000000000004000b3310" encoding="application/x-tex">4</annotation></semantics></math></span></span> while the third elongates to disect the remaining eight dimension of the 10D construct.
It’s length transitions from <span data-hk="000000010000000000004000b3770"><span data-hk="000000010000000000004000b3490"><math data-hk="000000010000000000004000b3480" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b3470"><mrow data-hk="000000010000000000004000b3450"><mn data-hk="000000010000000000004000b3400">4</mn><mo data-hk="000000010000000000004000b3410">=</mo><mn data-hk="000000010000000000004000b3420">4</mn><msqrt data-hk="000000010000000000004000b3440"><mn data-hk="000000010000000000004000b3430">1</mn></msqrt></mrow><annotation data-hk="000000010000000000004000b3460" encoding="application/x-tex">4 = 4\sqrt{1}</annotation></semantics></math></span></span> up to <span data-hk="000000010000000000004000b4070"><span data-hk="000000010000000000004000b3850"><math data-hk="000000010000000000004000b3840" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b3830"><mrow data-hk="000000010000000000004000b3810"><mn data-hk="000000010000000000004000b3780">4</mn><msqrt data-hk="000000010000000000004000b3800"><mn data-hk="000000010000000000004000b3790">8</mn></msqrt></mrow><annotation data-hk="000000010000000000004000b3820" encoding="application/x-tex">4\sqrt{8}</annotation></semantics></math></span></span>,
the length of the longest diagonal of an <span data-hk="000000010000000000004000b4180"><span data-hk="000000010000000000004000b4130"><math data-hk="000000010000000000004000b4120" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b4110"><mrow data-hk="000000010000000000004000b4090"><mn data-hk="000000010000000000004000b4080">8</mn></mrow><annotation data-hk="000000010000000000004000b4100" encoding="application/x-tex">8</annotation></semantics></math></span></span> dimensional box.</p>
<p data-hk="000000010000000000004000b4590">Viewing the fully diagonalized construct from the  we can see the property it’s known for.
The red ball reaches outside of the green box.
I repeat: sections of the red ball are outside the green box.
This is absurd; considering how enclosed it is in the two and three dimensional versions, and even if it’s mathematically clear that <span data-hk="000000010000000000004000b4390"><span data-hk="000000010000000000004000b4280"><math data-hk="000000010000000000004000b4270" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b4260"><mrow data-hk="000000010000000000004000b4240"><mi data-hk="000000010000000000004000b4210">r</mi><mo data-hk="000000010000000000004000b4220">&gt;</mo><mn data-hk="000000010000000000004000b4230">2</mn></mrow><annotation data-hk="000000010000000000004000b4250" encoding="application/x-tex">r &gt; 2</annotation></semantics></math></span></span> when <span data-hk="000000010000000000004000b4580"><span data-hk="000000010000000000004000b4470"><math data-hk="000000010000000000004000b4460" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b4450"><mrow data-hk="000000010000000000004000b4430"><mi data-hk="000000010000000000004000b4400">D</mi><mo data-hk="000000010000000000004000b4410">&gt;</mo><mn data-hk="000000010000000000004000b4420">9</mn></mrow><annotation data-hk="000000010000000000004000b4440" encoding="application/x-tex">D &gt; 9</annotation></semantics></math></span></span>, the fact is hard to accept.</p>
<p data-hk="000000010000000000004000b4640">Some people have tried to analogize this property to spikiness
<sup data-hk="000000010000000000004000b4610">[<a data-hk="000000010000000000004000b4600" href="https://stanislavfort.com/blog/sphere-spilling-out/">1</a>]</sup>
<sup data-hk="000000010000000000004000b4630">[<a data-hk="000000010000000000004000b4620" href="http://www.solipsys.co.uk/new/SpikeySpheres.html?HN_20161120">2</a>]</sup> of the red ball.
I don’t think the analogy is good.
A 3D sphere is perfectly circular, no matter how you slice it.
Similarly, an n-dimensional hyperball, is perfectly spherical from all perspectives.
It’s ballness just reaches into more hard to grasp dimensions than we’re used to.</p>
<h3 data-hk="000000010000000000004000b4660" id="hammings-lecture"><a data-hk="000000010000000000004000b4650" href="#hammings-lecture">Hamming’s lecture</a></h3>
<p data-hk="000000010000000000004000b4670">Here is a segment from a lecture that reveals this fact without mincing words.</p>
<p data-hk="000000010000000000004000b4690"><iframe data-hk="000000010000000000004000b4680" width="200" height="150" src="https://www.youtube.com/embed/uU_Q2a0S0zI?start=1716&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="Hamming, &quot;n-Dimensional Space&quot; (April 14, 1995)"></iframe></p>
<h2 data-hk="000000010000000000004000b4710" id="further-analysis"><a data-hk="000000010000000000004000b4700" href="#further-analysis">Further Analysis</a></h2>
<p data-hk="000000010000000000004000b5620">A unit n-cube has a unit <abbr data-hk="000000010000000000004000b4720" data-title="n-dimensional volume">volume</abbr> for any <span data-hk="000000010000000000004000b4830"><span data-hk="000000010000000000004000b4780"><math data-hk="000000010000000000004000b4770" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b4760"><mrow data-hk="000000010000000000004000b4740"><mi data-hk="000000010000000000004000b4730">D</mi></mrow><annotation data-hk="000000010000000000004000b4750" encoding="application/x-tex">D</annotation></semantics></math></span></span>.
A unit n-ball has a volume of <span data-hk="000000010000000000004000b5490"><span data-hk="000000010000000000004000b5040"><math data-hk="000000010000000000004000b5030" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b5020"><mrow data-hk="000000010000000000004000b5000"><mfrac data-hk="000000010000000000004000b4990"><msup data-hk="000000010000000000004000b4890"><mi data-hk="000000010000000000004000b4840">π</mi><mrow data-hk="000000010000000000004000b4880"><mi data-hk="000000010000000000004000b4850">D</mi><mi data-hk="000000010000000000004000b4860" mathvariant="normal">/</mi><mn data-hk="000000010000000000004000b4870">2</mn></mrow></msup><mrow data-hk="000000010000000000004000b4980"><mi data-hk="000000010000000000004000b4900" mathvariant="normal">Γ</mi><mo data-hk="000000010000000000004000b4910" stretchy="false">(</mo><mi data-hk="000000010000000000004000b4920">D</mi><mi data-hk="000000010000000000004000b4930" mathvariant="normal">/</mi><mn data-hk="000000010000000000004000b4940">2</mn><mo data-hk="000000010000000000004000b4950">+</mo><mn data-hk="000000010000000000004000b4960">1</mn><mo data-hk="000000010000000000004000b4970" stretchy="false">)</mo></mrow></mfrac></mrow><annotation data-hk="000000010000000000004000b5010" encoding="application/x-tex">\frac{\pi^{D/2}}{\Gamma(D/2+1)}</annotation></semantics></math></span></span> which rapidly approaches zero as <span data-hk="000000010000000000004000b5600"><span data-hk="000000010000000000004000b5550"><math data-hk="000000010000000000004000b5540" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b5530"><mrow data-hk="000000010000000000004000b5510"><mi data-hk="000000010000000000004000b5500">D</mi></mrow><annotation data-hk="000000010000000000004000b5520" encoding="application/x-tex">D</annotation></semantics></math></span></span> grows.
Intuitively, a sphere and a cylinder are both 3D extensions of a circle.
There is more volume in the cylinder than the sphere.
With some imprecision, we can thus consider the sphere loosing some volume when a dimension is added, while a cylinder or a box do not.
This <em data-hk="000000010000000000004000b5610">volume loss</em> occurs in every additional dimension.
So instead of considering n-balls to be spiky, it’s the space around them that outgrows them.</p>
<h3 data-hk="000000010000000000004000b5640" id="cubing-the-ball"><a data-hk="000000010000000000004000b5630" href="#cubing-the-ball">Cubing the Ball</a></h3>
<p data-hk="000000010000000000004000b5650">The volume of the red ball is:</p>
<span data-hk="000000010000000000004000b7230"><span data-hk="000000010000000000004000b7220"><span data-hk="000000010000000000004000b5950"><math data-hk="000000010000000000004000b5940" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics data-hk="000000010000000000004000b5930"><mrow data-hk="000000010000000000004000b5910"><mfrac data-hk="000000010000000000004000b5900"><mrow data-hk="000000010000000000004000b5790"><msup data-hk="000000010000000000004000b5700"><mi data-hk="000000010000000000004000b5660">π</mi><mfrac data-hk="000000010000000000004000b5690"><mi data-hk="000000010000000000004000b5670">D</mi><mn data-hk="000000010000000000004000b5680">2</mn></mfrac></msup><mo data-hk="000000010000000000004000b5710" fence="true" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><msqrt data-hk="000000010000000000004000b5730"><mi data-hk="000000010000000000004000b5720">D</mi></msqrt><mo data-hk="000000010000000000004000b5740">−</mo><mn data-hk="000000010000000000004000b5750">1</mn><msup data-hk="000000010000000000004000b5780"><mo data-hk="000000010000000000004000b5760" fence="true" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo><mi data-hk="000000010000000000004000b5770">D</mi></msup></mrow><mrow data-hk="000000010000000000004000b5890"><mi data-hk="000000010000000000004000b5800" mathvariant="normal">Γ</mi><mrow data-hk="000000010000000000004000b5880"><mo data-hk="000000010000000000004000b5810" fence="true">(</mo><mfrac data-hk="000000010000000000004000b5840"><mi data-hk="000000010000000000004000b5820">D</mi><mn data-hk="000000010000000000004000b5830">2</mn></mfrac><mo data-hk="000000010000000000004000b5850">+</mo><mn data-hk="000000010000000000004000b5860">1</mn><mo data-hk="000000010000000000004000b5870" fence="true">)</mo></mrow></mrow></mfrac></mrow><annotation data-hk="000000010000000000004000b5920" encoding="application/x-tex">\frac{\pi^{\frac{D}{2}}\bigl(\sqrt{D}-1\bigr)^D}{\Gamma\left(\frac{D}{2}+1\right)}</annotation></semantics></math></span></span></span>
<p data-hk="000000010000000000004000b7240">and if we normalize the size of the green box to 1 by dividing it’s sidelength by 4, the volume of the red ball becomes:</p>
<span data-hk="000000010000000000004000b9230"><span data-hk="000000010000000000004000b9220"><span data-hk="000000010000000000004000b7630"><math data-hk="000000010000000000004000b7620" xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics data-hk="000000010000000000004000b7610"><mrow data-hk="000000010000000000004000b7590"><mi data-hk="000000010000000000004000b7250">V</mi><mo data-hk="000000010000000000004000b7260" stretchy="false">(</mo><mi data-hk="000000010000000000004000b7270">D</mi><mo data-hk="000000010000000000004000b7280" stretchy="false">)</mo><mo data-hk="000000010000000000004000b7290">=</mo><mfrac data-hk="000000010000000000004000b7580"><mrow data-hk="000000010000000000004000b7470"><msup data-hk="000000010000000000004000b7340"><mi data-hk="000000010000000000004000b7300">π</mi><mfrac data-hk="000000010000000000004000b7330"><mi data-hk="000000010000000000004000b7310">D</mi><mn data-hk="000000010000000000004000b7320">2</mn></mfrac></msup><msup data-hk="000000010000000000004000b7460"><mrow data-hk="000000010000000000004000b7440"><mo data-hk="000000010000000000004000b7350" fence="true">(</mo><mfrac data-hk="000000010000000000004000b7420"><mrow data-hk="000000010000000000004000b7400"><msqrt data-hk="000000010000000000004000b7370"><mi data-hk="000000010000000000004000b7360">D</mi></msqrt><mo data-hk="000000010000000000004000b7380">−</mo><mn data-hk="000000010000000000004000b7390">1</mn></mrow><mn data-hk="000000010000000000004000b7410">4</mn></mfrac><mo data-hk="000000010000000000004000b7430" fence="true">)</mo></mrow><mi data-hk="000000010000000000004000b7450">D</mi></msup></mrow><mrow data-hk="000000010000000000004000b7570"><mi data-hk="000000010000000000004000b7480" mathvariant="normal">Γ</mi><mrow data-hk="000000010000000000004000b7560"><mo data-hk="000000010000000000004000b7490" fence="true">(</mo><mfrac data-hk="000000010000000000004000b7520"><mi data-hk="000000010000000000004000b7500">D</mi><mn data-hk="000000010000000000004000b7510">2</mn></mfrac><mo data-hk="000000010000000000004000b7530">+</mo><mn data-hk="000000010000000000004000b7540">1</mn><mo data-hk="000000010000000000004000b7550" fence="true">)</mo></mrow></mrow></mfrac></mrow><annotation data-hk="000000010000000000004000b7600" encoding="application/x-tex">V(D) = \frac{\pi^{\frac{D}{2}}\left(\frac{\sqrt{D}-1}{4}\right)^D}{\Gamma\left(\frac{D}{2}+1\right)}</annotation></semantics></math></span></span></span>
<p data-hk="000000010000000000004000b9250">Which has this plot:
<img data-hk="000000010000000000004000b9240" src="https://www.arnaldur.be/_build/assets/ball-size-plot-bWC8dSmM.png" alt="Red ball relative size"></p>
<p data-hk="000000010000000000004000b9370">There are a few notable/interesting values of <span data-hk="000000010000000000004000b9360"><span data-hk="000000010000000000004000b9310"><math data-hk="000000010000000000004000b9300" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b9290"><mrow data-hk="000000010000000000004000b9270"><mi data-hk="000000010000000000004000b9260">D</mi></mrow><annotation data-hk="000000010000000000004000b9280" encoding="application/x-tex">D</annotation></semantics></math></span></span>.</p>
<ul data-hk="000000010000000000004000c12440">
<li data-hk="000000010000000000004000b9630"><span data-hk="000000010000000000004000b9620"><span data-hk="000000010000000000004000b9480"><math data-hk="000000010000000000004000b9470" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b9460"><mrow data-hk="000000010000000000004000b9440"><mi data-hk="000000010000000000004000b9380">V</mi><mo data-hk="000000010000000000004000b9390" stretchy="false">(</mo><mn data-hk="000000010000000000004000b9400">0</mn><mo data-hk="000000010000000000004000b9410" stretchy="false">)</mo><mo data-hk="000000010000000000004000b9420">=</mo><mn data-hk="000000010000000000004000b9430">1</mn></mrow><annotation data-hk="000000010000000000004000b9450" encoding="application/x-tex">V(0) = 1</annotation></semantics></math></span></span></li>
<li data-hk="000000010000000000004000b9890"><span data-hk="000000010000000000004000b9880"><span data-hk="000000010000000000004000b9740"><math data-hk="000000010000000000004000b9730" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b9720"><mrow data-hk="000000010000000000004000b9700"><mi data-hk="000000010000000000004000b9640">V</mi><mo data-hk="000000010000000000004000b9650" stretchy="false">(</mo><mn data-hk="000000010000000000004000b9660">1</mn><mo data-hk="000000010000000000004000b9670" stretchy="false">)</mo><mo data-hk="000000010000000000004000b9680">=</mo><mn data-hk="000000010000000000004000b9690">0</mn></mrow><annotation data-hk="000000010000000000004000b9710" encoding="application/x-tex">V(1) = 0</annotation></semantics></math></span></span></li>
<li data-hk="000000010000000000004000c10150"><span data-hk="000000010000000000004000c10140"><span data-hk="000000010000000000004000c10000"><math data-hk="000000010000000000004000b9990" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000b9980"><mrow data-hk="000000010000000000004000b9960"><mi data-hk="000000010000000000004000b9900">V</mi><mo data-hk="000000010000000000004000b9910" stretchy="false">(</mo><mn data-hk="000000010000000000004000b9920">2</mn><mo data-hk="000000010000000000004000b9930" stretchy="false">)</mo><mo data-hk="000000010000000000004000b9940">≈</mo><mn data-hk="000000010000000000004000b9950">0.033688...</mn></mrow><annotation data-hk="000000010000000000004000b9970" encoding="application/x-tex">V(2) \approx 0.033688...</annotation></semantics></math></span></span></li>
<li data-hk="000000010000000000004000c10410"><span data-hk="000000010000000000004000c10400"><span data-hk="000000010000000000004000c10260"><math data-hk="000000010000000000004000c10250" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c10240"><mrow data-hk="000000010000000000004000c10220"><mi data-hk="000000010000000000004000c10160">V</mi><mo data-hk="000000010000000000004000c10170" stretchy="false">(</mo><mn data-hk="000000010000000000004000c10180">264</mn><mo data-hk="000000010000000000004000c10190" stretchy="false">)</mo><mo data-hk="000000010000000000004000c10200">≈</mo><mn data-hk="000000010000000000004000c10210">0.00001000428...</mn></mrow><annotation data-hk="000000010000000000004000c10230" encoding="application/x-tex">V(264) \approx 0.00001000428...</annotation></semantics></math></span></span> (a local minimum)</li>
<li data-hk="000000010000000000004000c11080"><span data-hk="000000010000000000004000c10660"><span data-hk="000000010000000000004000c10520"><math data-hk="000000010000000000004000c10510" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c10500"><mrow data-hk="000000010000000000004000c10480"><mi data-hk="000000010000000000004000c10420">V</mi><mo data-hk="000000010000000000004000c10430" stretchy="false">(</mo><mn data-hk="000000010000000000004000c10440">1206</mn><mo data-hk="000000010000000000004000c10450" stretchy="false">)</mo><mo data-hk="000000010000000000004000c10460">≈</mo><mn data-hk="000000010000000000004000c10470">1.01158...</mn></mrow><annotation data-hk="000000010000000000004000c10490" encoding="application/x-tex">V(1206) \approx 1.01158...</annotation></semantics></math></span></span> (smallest <span data-hk="000000010000000000004000c11070"><span data-hk="000000010000000000004000c10810"><math data-hk="000000010000000000004000c10800" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c10790"><mrow data-hk="000000010000000000004000c10770"><mi data-hk="000000010000000000004000c10670">V</mi><mo data-hk="000000010000000000004000c10680" stretchy="false">(</mo><mi data-hk="000000010000000000004000c10690">D</mi><mo data-hk="000000010000000000004000c10700" stretchy="false">)</mo><mo data-hk="000000010000000000004000c10710">≥</mo><mn data-hk="000000010000000000004000c10720">1</mn><mo data-hk="000000010000000000004000c10730">∣</mo><mi data-hk="000000010000000000004000c10740">D</mi><mo data-hk="000000010000000000004000c10750">&gt;</mo><mn data-hk="000000010000000000004000c10760">0</mn></mrow><annotation data-hk="000000010000000000004000c10780" encoding="application/x-tex">V(D) \geq 1 \mid D &gt; 0</annotation></semantics></math></span></span>)</li>
<li data-hk="000000010000000000004000c11530"><span data-hk="000000010000000000004000c11330"><span data-hk="000000010000000000004000c11190"><math data-hk="000000010000000000004000c11180" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c11170"><mrow data-hk="000000010000000000004000c11150"><mi data-hk="000000010000000000004000c11090">V</mi><mo data-hk="000000010000000000004000c11100" stretchy="false">(</mo><mn data-hk="000000010000000000004000c11110">1390</mn><mo data-hk="000000010000000000004000c11120" stretchy="false">)</mo><mo data-hk="000000010000000000004000c11130">≈</mo><mn data-hk="000000010000000000004000c11140">29.741...</mn></mrow><annotation data-hk="000000010000000000004000c11160" encoding="application/x-tex">V(1390) \approx 29.741...</annotation></semantics></math></span></span>
Here the red ball’s volume exceeds the box’s volume by the a larger ratio than the square exceeds the area of the circle, when <span data-hk="000000010000000000004000c11520"><span data-hk="000000010000000000004000c11410"><math data-hk="000000010000000000004000c11400" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c11390"><mrow data-hk="000000010000000000004000c11370"><mi data-hk="000000010000000000004000c11340">D</mi><mo data-hk="000000010000000000004000c11350">=</mo><mn data-hk="000000010000000000004000c11360">2</mn></mrow><annotation data-hk="000000010000000000004000c11380" encoding="application/x-tex">D=2</annotation></semantics></math></span></span></li>
<li data-hk="000000010000000000004000c11980"><span data-hk="000000010000000000004000c11780"><span data-hk="000000010000000000004000c11640"><math data-hk="000000010000000000004000c11630" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c11620"><mrow data-hk="000000010000000000004000c11600"><mi data-hk="000000010000000000004000c11540">V</mi><mo data-hk="000000010000000000004000c11550" stretchy="false">(</mo><mn data-hk="000000010000000000004000c11560">1405</mn><mo data-hk="000000010000000000004000c11570" stretchy="false">)</mo><mo data-hk="000000010000000000004000c11580">≈</mo><mn data-hk="000000010000000000004000c11590">39.499...</mn></mrow><annotation data-hk="000000010000000000004000c11610" encoding="application/x-tex">V(1405) \approx 39.499...</annotation></semantics></math></span></span> The same as above but <span data-hk="000000010000000000004000c11970"><span data-hk="000000010000000000004000c11860"><math data-hk="000000010000000000004000c11850" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c11840"><mrow data-hk="000000010000000000004000c11820"><mi data-hk="000000010000000000004000c11790">D</mi><mo data-hk="000000010000000000004000c11800">=</mo><mn data-hk="000000010000000000004000c11810">3</mn></mrow><annotation data-hk="000000010000000000004000c11830" encoding="application/x-tex">D=3</annotation></semantics></math></span></span></li>
<li data-hk="000000010000000000004000c12430"><span data-hk="000000010000000000004000c12230"><span data-hk="000000010000000000004000c12090"><math data-hk="000000010000000000004000c12080" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c12070"><mrow data-hk="000000010000000000004000c12050"><mi data-hk="000000010000000000004000c11990">V</mi><mo data-hk="000000010000000000004000c12000" stretchy="false">(</mo><mn data-hk="000000010000000000004000c12010">1486</mn><mo data-hk="000000010000000000004000c12020" stretchy="false">)</mo><mo data-hk="000000010000000000004000c12030">≈</mo><mn data-hk="000000010000000000004000c12040">186.299...</mn></mrow><annotation data-hk="000000010000000000004000c12060" encoding="application/x-tex">V(1486) \approx 186.299...</annotation></semantics></math></span></span> The same as above but <span data-hk="000000010000000000004000c12420"><span data-hk="000000010000000000004000c12310"><math data-hk="000000010000000000004000c12300" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c12290"><mrow data-hk="000000010000000000004000c12270"><mi data-hk="000000010000000000004000c12240">D</mi><mo data-hk="000000010000000000004000c12250">=</mo><mn data-hk="000000010000000000004000c12260">10</mn></mrow><annotation data-hk="000000010000000000004000c12280" encoding="application/x-tex">D=10</annotation></semantics></math></span></span></li>
</ul>
<p data-hk="000000010000000000004000c13060">Not only does some of the ball reach outside of the box (as happens when <span data-hk="000000010000000000004000c12630"><span data-hk="000000010000000000004000c12520"><math data-hk="000000010000000000004000c12510" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c12500"><mrow data-hk="000000010000000000004000c12480"><mi data-hk="000000010000000000004000c12450">D</mi><mo data-hk="000000010000000000004000c12460">=</mo><mn data-hk="000000010000000000004000c12470">10</mn></mrow><annotation data-hk="000000010000000000004000c12490" encoding="application/x-tex">D=10</annotation></semantics></math></span></span>) it goes much further.
A more adventurous mathematician than me can try to find the value of <span data-hk="000000010000000000004000c12740"><span data-hk="000000010000000000004000c12690"><math data-hk="000000010000000000004000c12680" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c12670"><mrow data-hk="000000010000000000004000c12650"><mi data-hk="000000010000000000004000c12640">D</mi></mrow><annotation data-hk="000000010000000000004000c12660" encoding="application/x-tex">D</annotation></semantics></math></span></span> when the volume of all the <a data-hk="000000010000000000004000c12750" href="https://en.wikipedia.org/wiki/Spherical_cap#Hyperspherical_cap">hyperspherical caps</a> become more than half of the volume of the sphere.
At that point, most of the ball’s volume is outside the box. At <span data-hk="000000010000000000004000c12940"><span data-hk="000000010000000000004000c12830"><math data-hk="000000010000000000004000c12820" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c12810"><mrow data-hk="000000010000000000004000c12790"><mi data-hk="000000010000000000004000c12760">D</mi><mo data-hk="000000010000000000004000c12770">=</mo><mn data-hk="000000010000000000004000c12780">1206</mn></mrow><annotation data-hk="000000010000000000004000c12800" encoding="application/x-tex">D=1206</annotation></semantics></math></span></span> the ball becomes larger than the box and as <span data-hk="000000010000000000004000c13050"><span data-hk="000000010000000000004000c13000"><math data-hk="000000010000000000004000c12990" xmlns="http://www.w3.org/1998/Math/MathML"><semantics data-hk="000000010000000000004000c12980"><mrow data-hk="000000010000000000004000c12960"><mi data-hk="000000010000000000004000c12950">D</mi></mrow><annotation data-hk="000000010000000000004000c12970" encoding="application/x-tex">D</annotation></semantics></math></span></span> increases, the relative size of the ball grows without bound.</p>
<h3 data-hk="000000010000000000004000c13080" id="3d-intersection-of-1206d-construct"><a data-hk="000000010000000000004000c13070" href="#3d-intersection-of-1206d-construct">3D Intersection of 1206D Construct</a></h3>
<!--!$-->
<canvas data-hk="000000010000000000004000c1309" height="400"></canvas>
<p data-hk="000000010000000000004000c13110">Here we see the relative scale of the red ball in 1206D.
Note that as above, this is a perfectly valid 3D slice of the construct;
a high dimensional being could, with the right tools, carve this out of the construct, <strong data-hk="000000010000000000004000c13100">with a single straight cut</strong>.
There are however, many other ways to slice the construct that look different,
similarly to how each slice of a tomato is distinct.</p>

<p data-hk="000000010000000000004000c13150"><a data-hk="000000010000000000004000c13140" href="https://www.desmos.com/calculator/osvrtc1t9m">Here is a Desmos calculator interactable</a> that visualises an orthogonal 2D slice of the 10D construct. Notice how much empty space there is between the balls.</p>
<p data-hk="000000010000000000004000c13170"><a data-hk="000000010000000000004000c13160" href="https://thenullhypodermic.blogspot.com/2019/12/high-dimensional-weirdness.html">This article</a> was a great inspiration, and the reason I considered the 1206D construct and beyond.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PEP 760: No more bare excepts (128 pts)]]></title>
            <link>https://discuss.python.org/t/pep-760-no-more-bare-excepts/67182</link>
            <guid>41788026</guid>
            <pubDate>Wed, 09 Oct 2024 13:54:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discuss.python.org/t/pep-760-no-more-bare-excepts/67182">https://discuss.python.org/t/pep-760-no-more-bare-excepts/67182</a>, See on <a href="https://news.ycombinator.com/item?id=41788026">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/DiscussionForumPosting" id="main-outlet" role="main">
      <meta itemprop="headline" content="PEP 760 – No More Bare Excepts">
      
      <meta itemprop="datePublished" content="2024-10-09T12:38:19Z">
        <meta itemprop="articleSection" content="PEPs">
      <meta itemprop="keywords" content="">
      


          <div id="post_1">
            <div>
              


              <p><span>
                  <time datetime="2024-10-09T12:38:19Z">
                    October 9, 2024, 12:38pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:52:05Z">
              <span itemprop="position">1</span>
              </span>
            </p></div>
            <div itemprop="text">
              <p><img src="https://emoji.discourse-cdn.com/apple/wave.png?v=12" title=":wave:" alt=":wave:" loading="lazy" width="20" height="20">  Hi everyone,</p>
<p>As the evil twin of <a href="https://discuss.python.org/t/pep-758-allow-except-and-except-expressions-without-parentheses/66453">PEP 758: Allow `except` and `except*` expressions without parentheses</a>, <a href="https://discuss.python.org/u/brettcannon">@brettcannon</a> and me present you <strong>PEP 760: No more bare excepts</strong>.</p>
<p>This PEP proposes disallowing bare <code>except:</code> clauses in Python’s exception-handling syntax. Currently, Python allows catching all exceptions with a bare <code>except:</code> clause, which can lead to overly broad exception handling and mask important errors. This PEP suggests requiring explicit exception types in all except clauses, promoting more precise and intentional error handling.</p>
<p>Please <strong>review the entire document</strong> before commenting as it contains several aspects of how to address the deprecation, tooling, rejected ideas and more.</p>


<p>Vote in this poll:</p>


            </div>

            

                
          </div>
          <div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/barry"><span itemprop="name">barry</span></a>
                (Barry Warsaw)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-08T00:35:09Z">
                    October 8, 2024, 12:35am
                  </time>
                  <meta itemprop="dateModified" content="2024-10-08T00:35:09Z">
              <span itemprop="position">2</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>I know this is off-topic for <em>this</em> thread, but I raised this as an issue with the PEP 760 PR (the PEP itself hasn’t been published yet).  I haven’t had time to look at the response from the PEP authors, but I would really like to preserve bare-<code>except</code> with an unconditional <code>raise</code> use case.  If that can’t be done, I suggested an alias for <code>BaseException</code> called <code>AnyException</code> so at least the modified code would read better:</p>
<pre data-code-wrap="python"><code>try:
    something()
except:
     transaction.abort()
     raise
else:
     transaction.commit()
</code></pre>
<p>would become</p>
<pre data-code-wrap="python"><code>try:
    something()
except AnyException:
     transaction.abort()
     raise
else:
     transaction.commit()
</code></pre>
            </div>

            

          </div>
          <div id="post_3" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/brettcannon"><span itemprop="name">brettcannon</span></a>
                (Brett Cannon)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-08T22:03:41Z">
                    October 8, 2024, 10:03pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-08T22:03:41Z">
              <span itemprop="position">3</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>Can I ask why? I don’t find that any clearer than <code>except BaseException</code>, and if you are using that pattern I feel you should understand what you’re doing. As such, I don’t think learning what <code>BaseException</code> means is too much to ask.</p>

<p>It’s the comma <em>in the appropriate contex</em>; not every comma makes a tuple, but you can’t syntactically make a tuple without a comma.</p>
            </div>

            

          </div>
          <div id="post_4" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/barry"><span itemprop="name">barry</span></a>
                (Barry Warsaw)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-08T22:32:44Z">
                    October 8, 2024, 10:32pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-08T22:32:44Z">
              <span itemprop="position">4</span>
              </span>
            </p>
            <div itemprop="text">
              
<p><code>BaseException</code> exposes the fact that it’s the root class in the exception hierarchy, and you have to connect that mentally to the fact that a) Python requires all exceptions to derive from it, and b) that pattern means “catch any exception”.  The latter just makes that obvious.  It’s obvious to <em>us</em> of course either way.</p>
<p>For me, I’d still prefer to allow a bare <code>except</code> if there’s an explicit empty <code>raise</code> in the body.</p>
            </div>

            

          </div>
          <div itemprop="comment" id="post_5" itemscope="" itemtype="http://schema.org/Comment">
              
<p>Ideally if you want to intercept an exception and pass it through, you would do it will <code>finally</code>:</p>
<div><a href="https://global.discourse-cdn.com/flex016/uploads/python1/original/3X/3/e/3ed521c6dd358e7c5d85f5b8a53d7e0f60fb1dc1.png" data-download-href="/uploads/short-url/8XQeE7K5hZdK0M2Aecfro4mbypz.png?dl=1" title="image"><img src="https://global.discourse-cdn.com/flex016/uploads/python1/optimized/3X/3/e/3ed521c6dd358e7c5d85f5b8a53d7e0f60fb1dc1_2_460x233.png" alt="image" data-base62-sha1="8XQeE7K5hZdK0M2Aecfro4mbypz" width="460" height="233" srcset="https://global.discourse-cdn.com/flex016/uploads/python1/optimized/3X/3/e/3ed521c6dd358e7c5d85f5b8a53d7e0f60fb1dc1_2_460x233.png, https://global.discourse-cdn.com/flex016/uploads/python1/optimized/3X/3/e/3ed521c6dd358e7c5d85f5b8a53d7e0f60fb1dc1_2_690x349.png 1.5x, https://global.discourse-cdn.com/flex016/uploads/python1/optimized/3X/3/e/3ed521c6dd358e7c5d85f5b8a53d7e0f60fb1dc1_2_920x466.png 2x" data-dominant-color="0F234E"></a></div>
<p>It would be nice to not have to use <code>sys</code>. What if we added an option to bind the exception in a <code>finally</code> block, as in:</p>
<pre data-code-wrap="python"><code>try:
    raise Exception("Hello")
finally e:
    if e is not None:
        e.add_note("World")
</code></pre>
            </div>
          <div id="post_6" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/barry"><span itemprop="name">barry</span></a>
                (Barry Warsaw)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-08T23:26:46Z">
                    October 8, 2024, 11:26pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-08T23:26:46Z">
              <span itemprop="position">6</span>
              </span>
            </p>
            <p>I don’t think that handles the original use case: abort an in-progress transaction and re-raise when any exception occurs, and commit the transaction if no exception occurs.</p>

            

          </div>
          <div id="post_7" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/iritkatriel"><span itemprop="name">iritkatriel</span></a>
                (Irit Katriel)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-08T23:27:50Z">
                    October 8, 2024, 11:27pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-08T23:59:03Z">
              <span itemprop="position">7</span>
              </span>
            </p>
            <div itemprop="text">
              <pre data-code-wrap="python"><code>try:
    transaction
finally e:
    if e is None:
         commit
    else:
         abort
</code></pre>
            </div>

            

          </div>
          <div id="post_8" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/jamestwebber"><span itemprop="name">jamestwebber</span></a>
                (James Webber)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-08T23:39:11Z">
                    October 8, 2024, 11:39pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-08T23:39:11Z">
              <span itemprop="position">8</span>
              </span>
            </p>
            <div itemprop="text">
              <p>I do wonder, is there a point to removing bare excepts just to re-introduce them with new syntax in <code>finally</code> blocks?</p>
<p>But presumably this is a discussion for the PEP 760 thread, when it arrives.</p>
            </div>

            

          </div>
          <div id="post_9" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/iritkatriel"><span itemprop="name">iritkatriel</span></a>
                (Irit Katriel)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-08T23:41:13Z">
                    October 8, 2024, 11:41pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-08T23:41:13Z">
              <span itemprop="position">9</span>
              </span>
            </p>
            <p>The finally block is not a bare except. It doesn’t suppress exceptions.</p>

            

          </div>
          <div id="post_10" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/ncoghlan"><span itemprop="name">ncoghlan</span></a>
                (Alyssa Coghlan)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T00:25:34Z">
                    October 9, 2024, 12:25am
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:38:51Z">
              <span itemprop="position">10</span>
              </span>
            </p>
            <div itemprop="text">
              <p>Now there’s an interesting idea: don’t make bare except illegal, make it have an implicit raise at the end (and disallow return, break, and continue).</p>
<p><s>We’re well and truly off-topic for <em>this</em> thread, though…</s> (comment was moved to a thread where it is on-topic)</p>
            </div>

            

          </div>
          <div id="post_11" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/iritkatriel"><span itemprop="name">iritkatriel</span></a>
                (Irit Katriel)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T09:32:21Z">
                    October 9, 2024,  9:32am
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T09:32:21Z">
              <span itemprop="position">11</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>As an alternative to my <code>finally</code> suggestion? I don’t think it would be coherent if <code>except</code> sometimes eats the exception and sometimes not.</p>
<p>Should we take this to another thread? Are the moderators able to move the comments on this subtopic to a new thread?</p>
            </div>

            

          </div>
          <div id="post_12" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/pablogsal"><span itemprop="name">pablogsal</span></a>
                (Pablo Galindo Salgado)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T12:40:30Z">
                    October 9, 2024, 12:40pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T12:40:30Z">
              <span itemprop="position">12</span>
              </span>
            </p>
            <div itemprop="text">
              <p>I have moved the relevant comments from <a href="https://discuss.python.org/t/pep-758-allow-except-and-except-expressions-without-parentheses/66453/55">PEP 758: Allow `except` and `except*` expressions without parentheses - #55 by pablogsal</a> that pertain this discussion. Please flag any potential mistakes in the operation as discourse made this quite complex.</p>
            </div>

            

          </div>
          <div id="post_13" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/zuo"><span itemprop="name">zuo</span></a>
                (Jan Kaliszewski)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T12:44:10Z">
                    October 9, 2024, 12:44pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T12:44:10Z">
              <span itemprop="position">13</span>
              </span>
            </p>
            <div itemprop="text">
              <p>I like the proposal, except that the <em>bare-except-with-unconditional-bare-raise</em> pattern:</p>
<pre data-code-wrap="python"><code>except:
    &lt;here cleanup / rollback / context-relevant log...&gt;
    raise
</code></pre>
<p>– is IMHO too convenient (also when it comes to reading the code) to get rid of.</p>
<p>For me, the necessity to make it more verbose would be a clear loss.</p>
            </div>

            

          </div>
          <div id="post_14" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/encukou"><span itemprop="name">encukou</span></a>
                (Petr Viktorin)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T13:19:33Z">
                    October 9, 2024,  1:19pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:19:33Z">
              <span itemprop="position">14</span>
              </span>
            </p>
            <div itemprop="text">
              <p>+1 to keeping bare <code>except:</code> with <code>raise</code>.<br>
For better or worse, it’s correct, idiomatic code; the “Motivation” reasons don’t really apply to it.<br>
<code>pylint</code> and <code>ruff</code> allow this, and the the style guides their docs link to (PEP-8 and the <a href="https://google.github.io/styleguide/pyguide.html#24-exceptions">Google style guide</a>) also mention that it’s OK. (<code>flake8</code> does flag it, though, and some of the other guides don’t list the exception.)<br>
And unlike <code>finally e:</code> or implicit raise, the existing syntax is compatible with Python 1.0+.</p>
<p>It would make the implementation harder, though.</p>
            </div>

            

          </div>
          <div id="post_15" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/pablogsal"><span itemprop="name">pablogsal</span></a>
                (Pablo Galindo Salgado)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T13:27:54Z">
                    October 9, 2024,  1:27pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:27:54Z">
              <span itemprop="position">15</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>Although it’s straightforward the implementation would be quite verbose as this implies doing an entire ast visitor just to detect raises below the bare except (in C).</p>
<p>The task becomes much simpler if we restrict it to detecting raises only at the same level of nesting but that feels somewhat odd.</p>
<p>I think this is an interesting case but you could skip all of this by just doing except BaseException and then all is good.</p>
            </div>

            

          </div>
          <div id="post_16" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/saaketp"><span itemprop="name">saaketp</span></a>
                (Saaket Prakash)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T13:32:43Z">
                    October 9, 2024,  1:32pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:32:43Z">
              <span itemprop="position">16</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>Implicit raise is also compatible (or well just as compatible as mandatory raise inside bare except block).</p>
            </div>

            

          </div>
          <div id="post_17" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/hroncok"><span itemprop="name">hroncok</span></a>
                (Miro Hrončok)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T13:32:48Z">
                    October 9, 2024,  1:32pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:32:48Z">
              <span itemprop="position">17</span>
              </span>
            </p>
            <p>This breaks so much existing code. Does the benefit outweigh this? IMHO it’s OK to emit warnings left and right, but outright rejecting bare excepts after just 3 releases of deprecation period seems… not cool. Please, be more conservative with breaking backward compatibility of the basic language. This is not an API of one standard library module, this is the Python language itself.</p>

            

          </div>
          <div id="post_18" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/zuo"><span itemprop="name">zuo</span></a>
                (Jan Kaliszewski)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T13:35:16Z">
                    October 9, 2024,  1:35pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:40:27Z">
              <span itemprop="position">18</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>I think that a reasonable approach would be to require that a <em>bare <code>except</code></em> block:</p>
<ul>
<li><em>must not</em> contain <code>return</code>, <code>break</code> or <code>continue</code> (at any level of nesting, except in nested <code>def</code>s)</li>
<li><em>must</em> contain <code>raise</code> at its top-level (open question: should only a bare <code>raise</code> or <em>any</em> <code>raise...</code> be required?)</li>
</ul>
            </div>

            

          </div>
          <div id="post_19" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/pablogsal"><span itemprop="name">pablogsal</span></a>
                (Pablo Galindo Salgado)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T13:45:43Z">
                    October 9, 2024,  1:45pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:45:43Z">
              <span itemprop="position">19</span>
              </span>
            </p>
            <p>I think requiring this is reasonable but I am uncomfortable with the maintenance consequences of having to keep an entire visitor just for restricting this too much. The exception also feels slightly odd to communicate when really we can avoid all of this by just writing <code>except BaseException</code></p>

            

          </div>
          <div id="post_20" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" href="https://discuss.python.org/u/pablogsal"><span itemprop="name">pablogsal</span></a>
                (Pablo Galindo Salgado)
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2024-10-09T13:46:57Z">
                    October 9, 2024,  1:46pm
                  </time>
                  <meta itemprop="dateModified" content="2024-10-09T13:46:57Z">
              <span itemprop="position">20</span>
              </span>
            </p>
            <p>How many releases you think its reasonable?</p>

            

          </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Germans decry influence of English as 'idiot's apostrophe' gets approval (147 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/oct/07/germany-influence-of-english-idiots-apostrophe</link>
            <guid>41787647</guid>
            <pubDate>Wed, 09 Oct 2024 13:16:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/oct/07/germany-influence-of-english-idiots-apostrophe">https://www.theguardian.com/world/2024/oct/07/germany-influence-of-english-idiots-apostrophe</a>, See on <a href="https://news.ycombinator.com/item?id=41787647">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A relaxation of official rules around the correct use of apostrophes in German has not only irritated grammar sticklers but triggered existential fears around the pervasive influence of English.</p><p>Establishments that feature their owners’ names, with signs like “Rosi’s Bar” or “Kati’s Kiosk” are a common sight around German towns and cities, but strictly speaking they are wrong: unlike English, German does not traditionally use apostrophes to indicate the genitive case or possession. The correct spelling, therefore, would be “Rosis Bar”, “Katis Kiosk”, or, as in the title of a <a href="https://www.theguardian.com/world/article/2024/jun/29/how-viral-tongue-twister-lightened-up-german-language" data-link-name="in body link">recent viral hit</a>, Barbaras Rhabarberbar.</p><p>However, guidelines issued by the body regulating the use of Standard High German orthography have clarified that the use of the punctuation mark colloquially known as the <em>Deppenapostroph</em><em> </em>(“idiot’s apostrophe”) has become so widespread that it is permissible – as long as it separates the genitive ‘s’ within a proper name.</p><p>The new edition of the Council for German Orthography’s style guide, which prescribes grammar use at schools and public bodies in <a href="https://www.theguardian.com/world/germany" data-link-name="in body link" data-component="auto-linked-tag">Germany</a>, Austria and German-speaking Switzerland, lists “Eva’s Blumenladen” (Eva’s Flower Shop) and “Peter’s Taverne” (Peter’s Tavern) as usable alternatives, though “Eva’s Brille” (“Eva’s glasses”) remains incorrect.</p><p>The <em>Deppenapostroph </em>is not to be confused with the English greengrocer’s apostrophe, when an apostrophe before an ‘s’ is mistakenly used to form the plural of a noun (“a kilo of potato’s”).</p><p>The new set of rules came into effect in July, and the council said a loosening of the rules in 1996 meant that “Rosi’s Bar” had strictly speaking not been incorrect for almost three decades. Yet over the past few days, German newspapers and social media networks have seen a pedants’ revolt against the loosening of grammar rules.</p><p>A commentator in the tabloid Bild said seeing signs like “Harald’s Eck” (“Harald’s Corner”) made his “hair stand on end”, and that the proper use of the genitive form would be bemoaned by lovers of the German language.</p><p>A columnist in the venerable broadsheet the Frankfurter Allgemeine Zeitung decried the council’s decision as further proof of the English language’s “victory march”, while one newspaper editor on LinkedIn complained that legalising the “idiot’s apostrophe” amounted to “genuflecting to English”.</p><p>Some linguists question whether the rise of the possessive apostrophe has much to do with the influence of English at all, however.</p><p>“The familiarity of English names may be a factor, but it could just as well stem from a desire to avoid confusion”, said Anatol Stefanowitsch, a linguist at Berlin’s Freie Universität. “What we tend to see when a language interacts with another prestige language is that it incorporates its vocabulary, and not grammar.”</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><a data-ignore="global-link-styling" href="#EmailSignup-skip-link-10">skip past newsletter promotion</a><p id="EmailSignup-skip-link-10" tabindex="0" aria-label="after newsletter promotion" role="note">after newsletter promotion</p></figure><p>Even before the rule clarification, the German orthographic council permitted the use of the possessive apostrophe for the sake of clarity, such as “Andrea’s Bar” to make clear that the owner is called Andrea and not Andreas.</p><p>“There is a long tradition of conservative circles fretting about international influences on the German languages,” said Stefanowitsch. “It used to be French, and now it’s mainly English”.</p><p>The Dortmund-based association Verein Deutsche Sprache tries to counteract the influence of English with an “anglicism index” that proposes alternative German words, such as <em>Klapprechner</em><em> </em>instead of “laptop” or <em>Puffmais</em><em> </em>instead of “popcorn”.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chemistry Nobel: Computational protein design and protein structure prediction (432 pts)]]></title>
            <link>https://www.nobelprize.org/prizes/chemistry/2024/press-release/</link>
            <guid>41786101</guid>
            <pubDate>Wed, 09 Oct 2024 09:54:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nobelprize.org/prizes/chemistry/2024/press-release/">https://www.nobelprize.org/prizes/chemistry/2024/press-release/</a>, See on <a href="https://news.ycombinator.com/item?id=41786101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content" role="main">
<section>

<article>
<header>
<h2>
Press release </h2>
</header>

<h5><b>English</b><br><a href="https://www.nobelprize.org/uploads/2024/10/press-chemistryprize2024.pdf" target="_blank" rel="noreferrer noopener">English (pdf)</a><br><a href="https://www.nobelprize.org/prizes/chemistry/2024/221996-press-release-swedish/">Swedish</a><br><a href="https://www.nobelprize.org/uploads/2024/10/press-chemistryprize2024-swedish.pdf" target="_blank" rel="noreferrer noopener">Swedish (pdf)</a></h5>
<figure><img decoding="async" width="190" height="80" src="https://www.nobelprize.org/uploads/2018/06/kva_logo_09-26.gif" alt="Logo"></figure>
<p>9 October 2024</p>
<p><a href="http://www.kva.se/en/" target="_blank" rel="noopener noreferrer">The Royal Swedish Academy of Sciences</a> has decided to award the Nobel Prize in Chemistry 2024</p>
<p>with one half to</p>
<p><strong>David Baker</strong><br>University of Washington, Seattle, WA, USA</p>
<p><em>“for computational protein design”</em></p>
<p>and the other half jointly to</p>
<p><strong>Demis Hassabis</strong><br>Google DeepMind, London, UK</p>
<p><strong>John M. Jumper</strong><br>Google DeepMind, London, UK</p>
<p><em>“for protein structure prediction”</em></p>
<h2>They cracked the code for proteins’ amazing structures</h2>
<p>The Nobel Prize in Chemistry 2024 is about pro­teins, life’s ingenious chemical tools. David Baker has succeeded with the almost impossible feat of building entirely new kinds of proteins. Demis Hassabis and John Jumper have developed an AI model to solve a 50-year-old problem: predicting proteins’ complex structures. These discoveries hold enormous potential.</p>
<p>The diversity of life testifies to proteins’ amazing capacity as chemical tools. They control and drive all the chemi­cal reactions that together are the basis of life. Proteins also function as hormones, signal substances, antibodies and the building blocks of different tissues.</p>
<p>“One of the discoveries being recognised this year concerns the construction of spectacular proteins. The other is about fulfilling a 50-year-old dream: predicting protein structures from their amino acid sequences. Both of these discoveries open up vast possibilities,” says Heiner Linke, Chair of the Nobel Committee for Chemistry.</p>
<p>Proteins generally consist of 20 different amino acids, which can be described as life’s building blocks. In 2003, <strong>David Baker</strong> succeeded in using these blocks to design a new protein that was unlike any other protein. Since then, his research group has produced one imaginative protein creation after another, including proteins that can be used as pharmaceuticals, vaccines, nanomaterials and tiny sensors.</p>
<p>The second discovery concerns the prediction of protein structures. In proteins, amino acids are linked together in long strings that fold up to make a three-dimensional structure, which is decisive for the protein’s function. Since the 1970s, researchers had tried to predict protein structures from amino acid sequences, but this was notoriously difficult. However, four years ago, there was a stunning breakthrough.</p>
<p>In 2020, <strong>Demis Hassabis</strong> and <strong>John Jumper</strong> presented an AI model called AlphaFold2. With its help, they have been able to predict the structure of virtually all the 200 million proteins that researchers have identified. Since their breakthrough, AlphaFold2 has been used by more than two million people from 190 countries. Among a myriad of scientific applications, researchers can now better understand antibiotic resistance and create images of enzymes that can decompose plastic.</p>
<p>Life could not exist without proteins. That we can now predict protein structures and design our own proteins confers the greatest benefit to humankind.</p>
<h3>Illustrations</h3>
<p>The illustrations are free to use for non-commercial purposes. Attribute copyright as below: </p>
<p><a href="https://www.nobelprize.org/uploads/2024/10/fig1_ke_en_24_A.pdf" target="_blank" rel="noreferrer noopener">Illustration: A protein can consist of everything from tens of amino acids to several thousand (pdf)</a><br>©Johan Jarnestad/The Royal Swedish Academy of Sciences<br><a href="https://www.nobelprize.org/uploads/2024/10/fig2_ke_en_24.pdf" target="_blank" rel="noreferrer noopener">Illustration: How does AlphaFold2 work? (pdf)</a><br>©Johan Jarnestad/The Royal Swedish Academy of Sciences<br><a href="https://www.nobelprize.org/uploads/2024/10/fig3_ke_24.pdf" target="_blank" rel="noreferrer noopener">Illustration: Top7 – the first protein that was entirely different to all known existing proteins (pdf)</a><br>©Terezia Kovalova/The Royal Swedish Academy of Sciences<br><a href="https://www.nobelprize.org/uploads/2024/10/fig4_ke_en_24.pdf" target="_blank" rel="noreferrer noopener">Illustration: Proteins developed using Baker’s program Rosetta (pdf)</a><br>©Terezia Kovalova/The Royal Swedish Academy of Sciences<br><a href="https://www.nobelprize.org/uploads/2024/10/fig5_ke_en_24.pdf" target="_blank" rel="noreferrer noopener">Illustration: Protein structures determined using AlphaFold2 (pdf)</a><br>©Terezia Kovalova/The Royal Swedish Academy of Sciences</p>
<h3>Read more about this year’s prize</h3>
<p><a href="https://www.nobelprize.org/uploads/2024/10/popular-chemistryprize2024-2.pdf" target="_blank" rel="noreferrer noopener">Popular science background: They have revealed proteins’ secrets through computing and artificial intelligence&nbsp;(pdf)</a><br><a href="https://www.nobelprize.org/uploads/2024/10/advanced-chemistryprize2024.pdf" target="_blank" rel="noreferrer noopener">Scientific background: Computational protein design and protein structure prediction&nbsp;(pdf)</a></p>
<hr>
<p><strong>David Baker</strong>, born 1962 in Seattle, WA, USA. PhD 1989 from University of California, Berkeley, CA, USA. Professor at University of Washington, Seattle, WA, USA.</p>
<p><strong>Demis Hassabis</strong>, born 1976 in London, UK. PhD 2009 from University College London, UK. CEO of Google DeepMind, London, UK.</p>
<p><strong>John M. Jumper</strong>, born 1985 in Little Rock, AR, USA. PhD 2017 from Uni­versity of Chicago, IL, USA. Senior Research Scientist at Google DeepMind, London, UK.</p>
<hr>
<p><b>Prize amount</b>: 11 million Swedish kronor, with one half to David Baker and the other half jointly to Demis Hassabis and John Jumper.<br><b>Further information</b>: www.kva.se and www.nobelprize.org<br><strong>Press contact</strong>: Eva Nevelius, Press Secretary, +46 70 878 67 63, <a href="https://www.nobelprize.org/cdn-cgi/l/email-protection" data-cfemail="640112054a0a011201080d1117240f12054a1701">[email&nbsp;protected]</a><br><b>Expert</b>: Johan Åqvist, +46 70-425 04 04, <a href="https://www.nobelprize.org/cdn-cgi/l/email-protection" data-cfemail="4a2025222b24642b3b3c23393e0a232927643f3f64392f">[email&nbsp;protected]</a>, member of the Nobel Committee for Chemistry</p>
<hr>
<p>The Royal Swedish Academy of Sciences, founded in 1739, is an independent organisation whose overall objective is to promote the sciences and strengthen their influence in society. The Academy takes special responsibility for the natural sciences and mathematics, but endeavours to promote the exchange of ideas between various disciplines.</p>
<p><em>Nobel Prize® is a registered trademark of the Nobel Foundation.</em></p>

<div>
<p><a href="#content">
Back to top </a></p><svg width="18px" height="15px" viewBox="0 0 20 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" role="image" aria-labelledby="back-to-top-title  back-to-top-desc">
<title id="back-to-top-title">Back To Top</title>
<desc id="back-to-top-desc">Takes users back to the top of the page</desc>
<g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
<g transform="translate(-474.000000, -9998.000000)" fill="#2E2A25">
<g transform="translate(474.000000, 9998.000000)">
<g transform="translate(10.000000, 10.000000) rotate(45.000000) translate(-10.000000, -10.000000) translate(3.000000, 3.000000)">
<rect x="0" y="0" width="2" height="14"></rect>
<rect x="0" y="0" width="14" height="2"></rect>
</g>
<rect x="9" y="3" width="2" height="14"></rect>
</g>
</g>
</g>
</svg>
</div>
</article>

</section>
<section>

<article>
<div>
<header>
<p>
<h2>
<a href="https://www.nobelprize.org/">
Coming up </a>
</h2>
</p>
</header>
<div><p>
Don't miss the Nobel Prize announcements 7-14 October!</p><p>Watch the live stream of the announcements. </p></div>
</div>
<figure>
<a href="https://www.nobelprize.org/"><picture><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-992x656.jpg" media="(min-width: 220px)"><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-1520x1008.jpg" media="(min-width: 900px)"><source data-srcset="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live.jpg" media="(min-width: 1400px)"><img src="https://www.nobelprize.org/uploads/2023/09/2024_Announcement_Recommended_Live-1024x676.jpg" alt="Illustration" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></picture></a> </figure>
</article>
</section>
<section>

<form id="670676e76df2c" method="GET" action="">
<p><label for="mobile-dropdown">
Select the category or categories you would like to filter by </label>

</p>
<div>
<p><label>Select the category or categories you would like to filter by</label></p><p><label for="physics">

<span>
Physics </span>
</label>
</p>
<p><label for="chemistry">

<span>
Chemistry </span>
</label>
</p>
<p><label for="medicine">

<span>
Medicine </span>
</label>
</p>
<p><label for="literature">

<span>
Literature </span>
</label>
</p>
<p><label for="peace">

<span>
Peace </span>
</label>
</p>
<p><label for="economic-sciences">

<span>
Economic Sciences </span>
</label>
</p>
</div>
<p><label for="increment-down">
Decrease the year by one </label>

<label for="increment-input">
Choose a year you would like to search in </label>

<label for="increment-up">
Increase the year by one </label>

</p>

</form>
</section>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A free and open internet shouldn't come at the expense of privacy (102 pts)]]></title>
            <link>https://blog.mozilla.org/en/mozilla/digital-advertising-privacy/</link>
            <guid>41786012</guid>
            <pubDate>Wed, 09 Oct 2024 09:34:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.mozilla.org/en/mozilla/digital-advertising-privacy/">https://blog.mozilla.org/en/mozilla/digital-advertising-privacy/</a>, See on <a href="https://news.ycombinator.com/item?id=41786012">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <main id="main">

    
<article id="post-75947">
  

  <div>
    
<p><em>MARK SURMAN, PRESIDENT, MOZILLA</em></p>



<p>Keeping the internet, and the content that makes it a vital and vibrant part of our global society, free and accessible has been a core focus for Mozilla from our founding. How do we ensure creators get paid for their work? How do we prevent huge segments of the world from being priced out of access through paywalls? How do we ensure that privacy is not a privilege of the few but a fundamental right available to everyone? These are significant and enduring questions that have no single answer. But, for right now on the internet of today, a big part of the answer is <a href="https://blog.mozilla.org/en/products/anonym-technology-overview/">online advertising</a>.&nbsp;</p>



<p>We <a href="https://blog.mozilla.org/en/mozilla/building-a-more-privacy-preserving-ads-based-ecosystem/">started</a> engaging in this space because the way the industry works today is fundamentally broken. It doesn’t put people first, it’s not privacy-respecting, and it’s increasingly anti-competitive. There have to be better options. Mozilla can play a key role in creating these better options not just by advocating for them, but also by actually building them. We can’t just ignore online advertising — it’s a major driver of how the internet works and is funded. We need to stare it straight in the eyes and try to fix it. For those reasons, Mozilla has become more <a href="https://blog.mozilla.org/en/mozilla/the-future-of-ads-and-privacy/">active</a> in online advertising over the past few years.&nbsp;</p>



<p>We have the beginnings of a theory on what fixing it might look like — a mix of different business practices, technology, products, and public policy engagements. And we have started to do work on all of these fronts. It’s been clear to us in recent weeks that what we haven’t done is step back to explain our thinking in the broader context of our advertising efforts. For this, we owe our community an apology for not engaging and communicating our vision effectively. Mozilla is only Mozilla if we share our thinking, engage people along the way, and incorporate that feedback into our efforts to help reform the ecosystem.</p>



<p>We’re going to correct that, starting with this blog post. I want to lay out our thinking about how we plan to shift the world of online advertising in a better direction.</p>



<h2><strong>Our theory&nbsp;</strong></h2>



<p>As we say in our <a href="https://www.mozilla.org/en-US/about/manifesto/">Manifesto</a>: “…a balance between commercial profit and public benefit is critical … “ to creating an open, healthy internet. Through that balance, we can have an internet that protects privacy and access, while encouraging a vibrant market that rewards creativity and innovation. But that’s not what we have in online advertising today.&nbsp;</p>



<p>Our theory for improving online advertising requires work across three areas that relate to and build upon one another:</p>



<ul>
<li><strong>Regulation: </strong>Over the years, improving<a href="https://blog.mozilla.org/netpolicy/2024/07/17/mozillas-policy-vision-for-the-new-eu-mandate-advancing-openness-privacy-fair-competition-and-choice-for-all/"> privacy</a> and<a href="https://blog.mozilla.org/netpolicy/2024/07/17/mozillas-policy-vision-for-the-new-eu-mandate-advancing-openness-privacy-fair-competition-and-choice-for-all/"> consumer protection</a> in advertising while enabling<a href="https://blog.mozilla.org/netpolicy/2022/04/12/competition-should-not-be-weaponized-to-hobble-privacy-protections-on-the-open-web/"> competition</a> has been at the core of our policy efforts. From <a href="https://blog.mozilla.org/netpolicy/2021/12/17/privacy-sandbox-cma-dec2021/">pushing</a> to improve Google’s Privacy Sandbox proposals via engaging with the Competition and Markets Authority (CMA) in the UK to<a href="https://blog.mozilla.org/netpolicy/2024/01/11/mozilla-weighs-in-on-state-comprehensive-privacy-proposals/"> advocating</a> for strong protections for universal opt-out mechanisms via state privacy laws in the United States, we have a long history of supporting legislation that puts users in more meaningful control of their data. We recognise that technology can only get us so far and needs to work hand-in-hand with legislation to fix the most egregious practices in the ecosystem. With the upcoming new mandate in the European Commission expected to focus on advertising and the push for a federal privacy legislation in the United States reaching a fever pitch, we intend to build upon this work to continue pushing for better privacy protections.&nbsp;</li>



<li><strong>Standards:</strong> As a pioneer in shaping internet standards, Mozilla has always played a central role in crafting technical specifications that support an open, competitive, and privacy-respecting web. We are bringing this same expertise and commitment to the advertising space. At the Internet Engineering Task Force (<a href="https://datatracker.ietf.org/doc/draft-ietf-ppm-dap/">IETF</a>) and World Wide Web Consortium (<a href="https://blog.mozilla.org/en/mozilla/privacy-preserving-attribution-for-advertising/">W3C</a>), Mozilla is actively involved in advancing cutting-edge proposals for privacy-preserving advertising. This includes collaborating on Interoperable Private Attribution (<a href="https://blog.mozilla.org/en/mozilla/privacy-preserving-attribution-for-advertising/">IPA</a>) and contributing to the Private Advertising Technology Community Group (<a href="https://www.w3.org/community/patcg/">PATCG</a>). The goal of this work is to identify legitimate, lawful, and non-harmful use cases and promote a healthy web by developing privacy-respecting technical mechanisms for those use cases. This would make it practical to more strictly limit the most invasive practices like ubiquitous third-party cookies.</li>



<li><strong>Products</strong>: Building things is the only way for Mozilla to prove these hypotheses. For years, Mozilla products have supported an advertising business without the privacy-invasive techniques common today by deploying features such as <a href="https://blog.mozilla.org/en/mozilla/firefox-rolls-out-total-cookie-protection-by-default-to-all-users-worldwide/">Total Cookie Protection</a> and<a href="https://blog.mozilla.org/en/products/firefox/firefox-now-available-with-enhanced-tracking-protection-by-default/"> Enhanced Tracking Protection</a> to protect our users. And we’ll continue to explore ways to add advertiser value while respecting user privacy – including by exploring how we can support other businesses in achieving these goals via <a href="https://blog.mozilla.org/en/mozilla/mozilla-anonym-raising-the-bar-for-privacy-preserving-digital-advertising/">Anonym</a>. Our goal is to build a model to demonstrate how ads can sustain a business online while respecting people’s privacy. Laura expands upon our approach in <a href="https://blog.mozilla.org/en/mozilla/improving-online-advertising/">her blog</a>.&nbsp;</li>
</ul>



<p>We have work underway right now across all three of these areas, with much more to come in the weeks and months ahead.&nbsp;</p>



<h2>The way forward —&nbsp;together</h2>



<p>This theory, and the work to test it, will become an increasingly integral part of the discussions we already have underway with regulators and civil society, consumers and developers, and advertisers, publishers and platforms. We will continue to set up gatherings, share research, and explore new ways to collectively share ideas and move this ahead for all of us – both shaping and being shaped by the ecosystem.&nbsp;</p>



<p>Fixing the problems with online advertising feels like an intractable challenge. Having been fortunate enough to be part of Mozilla for well over a decade, I am excited to tackle this challenge head on. It’s an opportunity for us to bring a whole community — including often divergent voices from advertising, technology, government and civil society — to the table to look for a better way. Personally, I don’t see a world where online advertising disappears —&nbsp;ads have been a key part of funding creators and publishers in every era from newspapers to radio to television. However, I can imagine a world where advertising online happens in a way that respects all of us, and where commercial and public interests are in balance. That’s a world I want to help build. &nbsp;</p>
  </div>

</article><!-- #post-75947 -->

  </main><!-- #main -->
  

<div id="related-articles">
    <h2>Related Articles</h2>
    
  </div>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made an SSH tunnel manager to learn Go (155 pts)]]></title>
            <link>https://github.com/alebeck/boring</link>
            <guid>41785511</guid>
            <pubDate>Wed, 09 Oct 2024 07:52:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/alebeck/boring">https://github.com/alebeck/boring</a>, See on <a href="https://news.ycombinator.com/item?id=41785511">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">The <code>boring</code> tunnel manager</h2><a id="user-content-the-boring-tunnel-manager" aria-label="Permalink: The boring tunnel manager" href="#the-boring-tunnel-manager"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e3b567564d22e864a72a8eb6426dc80ce5c492b357efc0b582402dec0ccbcb45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c642d70617373696e672d3443433532353f"><img src="https://camo.githubusercontent.com/e3b567564d22e864a72a8eb6426dc80ce5c492b357efc0b582402dec0ccbcb45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6275696c642d70617373696e672d3443433532353f" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/build-passing-4CC525?"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fea4f3fb8ff078b99d6607257e24f1b0d6ca911fe85ce32bc77e7d0fe66a2eb7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f616c656265636b2f626f72696e673f636f6c6f723d6f72616e6765"><img src="https://camo.githubusercontent.com/fea4f3fb8ff078b99d6607257e24f1b0d6ca911fe85ce32bc77e7d0fe66a2eb7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f616c656265636b2f626f72696e673f636f6c6f723d6f72616e6765" alt="GitHub Release" data-canonical-src="https://img.shields.io/github/v/release/alebeck/boring?color=orange"></a> <a href="https://goreportcard.com/report/github.com/alebeck/boring" rel="nofollow"><img src="https://camo.githubusercontent.com/1d7d10ed2026d6d9f28c0af8f560bca9e1663a4de292de95500af6edb4fa7b67/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f616c656265636b2f626f72696e67" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/alebeck/boring"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a2fdd2056481b3c7430449213d1e5e3d7b728e1d49a866cdead4bff9a7ca4305/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75653f"><img src="https://camo.githubusercontent.com/a2fdd2056481b3c7430449213d1e5e3d7b728e1d49a866cdead4bff9a7ca4305/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75653f" alt="Static Badge" data-canonical-src="https://img.shields.io/badge/license-MIT-blue?"></a></p>
<p dir="auto">A simple &amp; reliable command line SSH tunnel manager.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/alebeck/boring/blob/main/assets/dark.gif"><img src="https://github.com/alebeck/boring/raw/main/assets/dark.gif" alt="Screenshot" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Ultra lightweight and fast</li>
<li>Local and remote tunnels</li>
<li>Compatible with SSH config and <code>ssh-agent</code></li>
<li>Supports Unix sockets</li>
<li>Automatic reconnection</li>
<li>Human-friendly configuration via TOML</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div data-snippet-clipboard-copy-content="Usage:
  boring list,l                        List tunnels
  boring open,o <name1> [<name2> ...]  Open specified tunnel(s)
  boring close,c <name1> [<name2> ...] Close specified tunnel(s)"><pre><code>Usage:
  boring list,l                        List tunnels
  boring open,o &lt;name1&gt; [&lt;name2&gt; ...]  Open specified tunnel(s)
  boring close,c &lt;name1&gt; [&lt;name2&gt; ...] Close specified tunnel(s)
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">By default, <code>boring</code> reads its configuration from <code>~/.boring.toml</code>. The configuration is a simple TOML file describing your tunnels:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# simple tunnel
[[tunnels]]
name = &quot;dev&quot;
local = &quot;9000&quot;
remote = &quot;localhost:9000&quot;
host = &quot;dev-server&quot;  # automatically matches host against SSH config

# example of an explicit host (no SSH config)
[[tunnels]]
name = &quot;prod&quot;
local = &quot;5001&quot;
remote = &quot;localhost:5001&quot;
host = &quot;prod.example.com&quot;
user = &quot;root&quot;
identity = &quot;~/.ssh/id_prod&quot;  # will try default ones if not set

# ... more tunnels"><pre><span><span>#</span> simple tunnel</span>
[[<span>tunnels</span>]]
<span>name</span> = <span><span>"</span>dev<span>"</span></span>
<span>local</span> = <span><span>"</span>9000<span>"</span></span>
<span>remote</span> = <span><span>"</span>localhost:9000<span>"</span></span>
<span>host</span> = <span><span>"</span>dev-server<span>"</span></span>  <span><span>#</span> automatically matches host against SSH config</span>

<span><span>#</span> example of an explicit host (no SSH config)</span>
[[<span>tunnels</span>]]
<span>name</span> = <span><span>"</span>prod<span>"</span></span>
<span>local</span> = <span><span>"</span>5001<span>"</span></span>
<span>remote</span> = <span><span>"</span>localhost:5001<span>"</span></span>
<span>host</span> = <span><span>"</span>prod.example.com<span>"</span></span>
<span>user</span> = <span><span>"</span>root<span>"</span></span>
<span>identity</span> = <span><span>"</span>~/.ssh/id_prod<span>"</span></span>  <span><span>#</span> will try default ones if not set</span>

<span><span>#</span> ... more tunnels</span></pre></div>
<p dir="auto">Currently, supported options are: <code>name</code>, <code>local</code>, <code>remote</code>, <code>host</code>, <code>user</code>, <code>identity</code>, <code>port</code>, and <code>mode</code>. <code>host</code> either describes a host which to match SSH configs to, or if no matches found, the actual hostname. <code>mode</code> can be 'local' for local or 'remote' for remote forwarding, default is 'local'. The location of the config file can be changed by setting the <code>BORING_CONFIG</code> environment variable.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Get one of the pre-built binaries from the <a href="https://github.com/alebeck/boring/releases">releases page</a> or build it yourself:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/alebeck/boring &amp;&amp; cd boring
./build.sh"><pre>git clone https://github.com/alebeck/boring <span>&amp;&amp;</span> <span>cd</span> boring
./build.sh</pre></div>
<p dir="auto">Currently only supports macOS and Linux.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cognizant found guilty of discriminating against non-Indian employees (352 pts)]]></title>
            <link>https://twitter.com/USTechWorkers/status/1843744799607898260</link>
            <guid>41785265</guid>
            <pubDate>Wed, 09 Oct 2024 07:05:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/USTechWorkers/status/1843744799607898260">https://twitter.com/USTechWorkers/status/1843744799607898260</a>, See on <a href="https://news.ycombinator.com/item?id=41785265">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Addition Is All You Need for Energy-Efficient Language Models (261 pts)]]></title>
            <link>https://arxiv.org/abs/2410.00907</link>
            <guid>41784591</guid>
            <pubDate>Wed, 09 Oct 2024 04:47:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2410.00907">https://arxiv.org/abs/2410.00907</a>, See on <a href="https://news.ycombinator.com/item?id=41784591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2410.00907">View PDF</a>
    <a href="https://arxiv.org/html/2410.00907v2">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Hongyin Luo [<a href="https://arxiv.org/show-email/1e73c3a7/2410.00907">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2410.00907v1">[v1]</a></strong>
        Tue, 1 Oct 2024 17:53:28 UTC (316 KB)<br>
    <strong>[v2]</strong>
        Wed, 2 Oct 2024 15:34:12 UTC (314 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miqt: MIT-licensed Qt bindings for Go (104 pts)]]></title>
            <link>https://github.com/mappu/miqt</link>
            <guid>41784387</guid>
            <pubDate>Wed, 09 Oct 2024 03:56:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mappu/miqt">https://github.com/mappu/miqt</a>, See on <a href="https://news.ycombinator.com/item?id=41784387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mappu/miqt/blob/master/doc/logo.svg"><img src="https://github.com/mappu/miqt/raw/master/doc/logo.svg" alt=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5582d4c23f650f1d41ff1fa4faf6202b30c754579efd1b11dc9511d2de817bd6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e"><img src="https://camo.githubusercontent.com/5582d4c23f650f1d41ff1fa4faf6202b30c754579efd1b11dc9511d2de817bd6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e" alt="" data-canonical-src="https://img.shields.io/badge/License-MIT-green"></a>
<a href="https://pkg.go.dev/github.com/mappu/miqt/qt" rel="nofollow"><img src="https://camo.githubusercontent.com/87e0dcb8c6899d68afe31c60a1b9b37e98527dcd34579fe6f81ec1f2c9e2f33a/68747470733a2f2f706b672e676f2e6465762f62616467652f6769746875622e636f6d2f6d617070752f6d6971742f71742e737667" alt="Go Reference" data-canonical-src="https://pkg.go.dev/badge/github.com/mappu/miqt/qt.svg"></a>
<a href="https://github.com/mappu/miqt/actions"><img src="https://github.com/mappu/miqt/actions/workflows/miqt.yml/badge.svg?branch=master" alt="GitHub Actions CI"></a>
<a href="https://goreportcard.com/report/github.com/mappu/miqt" rel="nofollow"><img src="https://camo.githubusercontent.com/a7d81712952481518de5167ef8ab8d8a882b27a510aff08546ca02d7028d0e28/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6d617070752f6d697174" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/mappu/miqt"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/mappu/miqt/blob/master/doc/made_in_nz.svg"><img src="https://github.com/mappu/miqt/raw/master/doc/made_in_nz.svg" alt="" title="Made in New Zealand"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">MIQT</h2><a id="user-content-miqt" aria-label="Permalink: MIQT" href="#miqt"></a></p>
<p dir="auto">MIQT is MIT-licensed Qt bindings for Go.</p>
<p dir="auto">This is a straightforward binding of the Qt API using CGO. You must have a working Qt C++ development toolchain to use this Go binding.</p>
<p dir="auto">These bindings were newly started in August 2024. The bindings are functional for all of QtCore, QtGui, and QtWidgets, and there is a uic/rcc implementation. But, the bindings may be immature in some ways. Please try out the bindings and raise issues if you have trouble.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported platforms</h2><a id="user-content-supported-platforms" aria-label="Permalink: Supported platforms" href="#supported-platforms"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>OS</th>
<th>Arch</th>
<th>Linkage</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linux</td>
<td>x86_64</td>
<td>Static or Dynamic (.so)</td>
<td>✅ Works</td>
</tr>
<tr>
<td>Windows</td>
<td>x86_64</td>
<td>Static or Dynamic (.dll)</td>
<td>✅ Works</td>
</tr>
<tr>
<td>Android</td>
<td>ARM64</td>
<td>Dynamic (bundled in .apk package)</td>
<td>✅ Works</td>
</tr>
<tr>
<td>macOS</td>
<td>x86_64</td>
<td>Static or Dynamic (.dylib)</td>
<td>Should work, <a href="https://github.com/mappu/miqt/issues/2" data-hovercard-type="issue" data-hovercard-url="/mappu/miqt/issues/2/hovercard">not tested</a></td>
</tr>
<tr>
<td>macOS</td>
<td>ARM64</td>
<td>Static or Dynamic (.dylib)</td>
<td><a href="https://github.com/mappu/miqt/issues/11" data-hovercard-type="issue" data-hovercard-url="/mappu/miqt/issues/11/hovercard">Blocked by #11</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The MIQT Go bindings are licensed under the MIT license.</p>
<p dir="auto">You must also meet your Qt license obligations.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Made with MIQT</h2><a id="user-content-made-with-miqt" aria-label="Permalink: Made with MIQT" href="#made-with-miqt"></a></p>
<ul dir="auto">
<li><a href="https://github.com/mappu/miqt/tree/master/examples/mdoutliner">mdoutliner</a>, Markdown Outliner sample application</li>
<li><a href="https://code.ivysaur.me/qbolt" rel="nofollow">qbolt</a>, a graphical database manager for BoltDB</li>
<li>Raise an issue or PR to have your app listed here!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q1. Why are the binaries so big?</h3><a id="user-content-q1-why-are-the-binaries-so-big" aria-label="Permalink: Q1. Why are the binaries so big?" href="#q1-why-are-the-binaries-so-big"></a></p>
<p dir="auto">Make sure to compile with <code>go build -ldflags "-s -w"</code>. This reduces the <code>helloworld</code> example from 43MB to 6MB.</p>
<p dir="auto">Then, it's possible to reduce the size further with <code>upx --best</code> to 2MB or <code>upx --lzma</code> to 1.4MB.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q2. Can I release a proprietary, commercial app with this binding?</h3><a id="user-content-q2-can-i-release-a-proprietary-commercial-app-with-this-binding" aria-label="Permalink: Q2. Can I release a proprietary, commercial app with this binding?" href="#q2-can-i-release-a-proprietary-commercial-app-with-this-binding"></a></p>
<p dir="auto">Yes. You must also meet your Qt license obligations: either use Qt dynamically-linked dll/so/dylib files under the LGPL, or, purchase a Qt commercial license for static linking.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q3. Why does it take so long to compile?</h3><a id="user-content-q3-why-does-it-take-so-long-to-compile" aria-label="Permalink: Q3. Why does it take so long to compile?" href="#q3-why-does-it-take-so-long-to-compile"></a></p>
<p dir="auto">The first time MIQT is used, your <code>go build</code> would take about 10 minutes. But after that, any <code>go build</code> is very fast.</p>
<p dir="auto">If you are compiling your app within a Dockerfile, you could cache the build step by running <code>go install github.com/mappu/miqt/qt</code>.</p>
<p dir="auto">If you are compiling your app with a one-shot <code>docker run</code> command, the compile speed can be improved if you also bind-mount the Docker container's <code>GOCACHE</code> directory: <code>-v $(pwd)/container-build-cache:/root/.cache/go-build</code></p>
<p dir="auto">See also <a href="https://github.com/mappu/miqt/issues/8" data-hovercard-type="issue" data-hovercard-url="/mappu/miqt/issues/8/hovercard">issue #8</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q4. How does this compare to other Qt bindings?</h3><a id="user-content-q4-how-does-this-compare-to-other-qt-bindings" aria-label="Permalink: Q4. How does this compare to other Qt bindings?" href="#q4-how-does-this-compare-to-other-qt-bindings"></a></p>
<p dir="auto">MIQT is a clean-room binding that does not use any code from other Qt bindings.</p>
<ul dir="auto">
<li><a href="https://github.com/therecipe/qt">therecipe/qt</a> is the most mature Qt binding for Go.
<ul dir="auto">
<li>It works by making IPC calls to a separate C++ binary downloaded at runtime from a site under the maintainer's control. This may be less performant than calling Qt directly.</li>
<li>Because of the LGPL license, it's <a href="https://github.com/therecipe/qt/wiki/FAQ#can-i-make-a-proprietary-app-with-this-binding-">extremely difficult to make a proprietary app</a>. See also their <a href="https://github.com/therecipe/qt/issues/259" data-hovercard-type="issue" data-hovercard-url="/therecipe/qt/issues/259/hovercard">issue 259</a>.</li>
</ul>
</li>
<li><a href="https://github.com/kitech/qt.go">kitech/qt.go</a> is another mature Qt binding for Go.
<ul dir="auto">
<li>Unfortunately, it's also using the LGPL license.</li>
</ul>
</li>
<li><a href="https://github.com/go-qamel/qamel">go-qamel/qamel</a> is an MIT-licensed Qt binding for Go.
<ul dir="auto">
<li>Unfortunately, it only supports QML, not Qt Widgets.</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q5. How does the MIQT Go API differ from the official Qt C++ API?</h3><a id="user-content-q5-how-does-the-miqt-go-api-differ-from-the-official-qt-c-api" aria-label="Permalink: Q5. How does the MIQT Go API differ from the official Qt C++ API?" href="#q5-how-does-the-miqt-go-api-differ-from-the-official-qt-c-api"></a></p>
<p dir="auto">Most functions are implemented 1:1. <a href="https://doc.qt.io/qt-5/classes.html" rel="nofollow">The Qt documentation</a> should be used.</p>
<p dir="auto">The <code>QString</code>, <code>QList&lt;T&gt;</code>, and <code>QVector&lt;T&gt;</code> types are projected as plain Go <code>string</code> and <code>[]T</code>. Therefore, you can't call any of QString/QList/QVector's helper methods, you must use some Go equivalent method instead.</p>
<ul dir="auto">
<li>Go strings are internally converted to QString using <code>QString::fromUtf8</code>. Therefore, the Go string must be UTF-8 to avoid <a href="https://en.wikipedia.org/wiki/Mojibake" rel="nofollow">mojibake</a>. If the Go string contains binary data, the conversion would corrupt such bytes into U+FFFD (�). On return to Go space, this becomes <code>\xEF\xBF\xBD</code>.</li>
</ul>
<p dir="auto">Where Qt returns a C++ object by value (e.g. <code>QSize</code>), the binding may have moved it to the heap, and in Go this may be represented as a pointer type. In such cases, a Go finalizer is added to automatically delete the heap object. This means code using MIQT can look basically similar to the Qt C++ equivalent code.</p>
<p dir="auto">The <code>connect(sourceObject, sourceSignal, targetObject, targetSlot)</code> is projected as <code>targetObject.onSourceSignal(func()...)</code>.</p>
<p dir="auto">Qt class inherited types are projected as a Go embedded struct. For example, to pass a <code>var myLabel *qt.QLabel</code> to a function taking only the <code>*qt.QWidget</code> base class, write <code>myLabel.QWidget</code>.</p>
<ul dir="auto">
<li>When a Qt subclass adds a method overload (e.g. <code>QMenu::addAction(QString)</code> vs <code>QWidget::addAction(QAction*)</code>), the base class version is shadowed and can only be called via <code>myQMenu.QWidget.AddAction(QAction*)</code>.</li>
</ul>
<p dir="auto">Some C++ idioms that were difficult to project were omitted from the binding. But, this can be improved in the future.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Q6. Can I use Qt Designer and the Qt Resource system?</h3><a id="user-content-q6-can-i-use-qt-designer-and-the-qt-resource-system" aria-label="Permalink: Q6. Can I use Qt Designer and the Qt Resource system?" href="#q6-can-i-use-qt-designer-and-the-qt-resource-system"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mappu/miqt/blob/master/doc/architecture-uic.png"><img src="https://github.com/mappu/miqt/raw/master/doc/architecture-uic.png" alt=""></a></p>
<p dir="auto">MIQT has a custom implementation of Qt <code>uic</code> and <code>rcc</code> tools, to allow using <a href="https://doc.qt.io/qt-5/qtdesigner-manual.html" rel="nofollow">Qt Designer</a> for form design and resource management. After running the <code>miqt-uic</code> and <code>miqt-rcc</code> tools once, you can rebuild any changes using the convenient <code>go generate</code> command.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Linux (native)</h3><a id="user-content-linux-native" aria-label="Permalink: Linux (native)" href="#linux-native"></a></p>
<p dir="auto"><em>Tested with Debian 12 / Qt 5.15 / GCC 12</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="apt install qtbase5-dev build-essential
go build -ldflags '-s -w'"><pre>apt install qtbase5-dev build-essential
go build -ldflags <span><span>'</span>-s -w<span>'</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows (native)</h3><a id="user-content-windows-native" aria-label="Permalink: Windows (native)" href="#windows-native"></a></p>
<p dir="auto"><em>Tested with Fsu0413 Qt 5.15 / Clang 18.1 native compilation</em></p>
<ol dir="auto">
<li>Install Go from <a href="https://go.dev/dl/" rel="nofollow">the official website</a>.</li>
<li>Install some Qt toolchain and its matching GCC or Clang compiler (MSVC is not compatible with CGO).
<ul dir="auto">
<li>You can use <a href="https://www.qt.io/" rel="nofollow">official Qt binaries</a> or any LGPL rebuild.</li>
<li>Example: Download and extract the following into some shared <code>C:\dev\rootfs</code>:</li>
<li><a href="https://build-qt.fsu0413.me/5.15-series/5.15.11-for-windows/index.html#windows-mingw-llvm" rel="nofollow">Qt and matching GCC or Clang toolchain</a></li>
<li><a href="https://sourceforge.net/projects/pkgconfiglite/files/0.28-1/" rel="nofollow">pkg-config</a></li>
</ul>
</li>
<li>Configure environment variables to allow it to be used:</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$env:CGO_ENABLED = 1
$env:CC = 'C:\dev\rootfs\bin\clang.exe'
$env:CXX = 'C:\dev\rootfs\bin\clang++.exe'
$env:PKG_CONFIG = 'C:\dev\rootfs\bin\pkg-config.exe'
$env:CGO_CXXFLAGS = '-Wno-ignored-attributes -D_Bool=bool' # Clang 18 recommendation"><pre><span>$<span>env:</span>CGO_ENABLED</span> <span>=</span> <span>1</span>
<span>$<span>env:</span>CC</span> <span>=</span> <span><span>'</span>C:\dev\rootfs\bin\clang.exe<span>'</span></span>
<span>$<span>env:</span>CXX</span> <span>=</span> <span><span>'</span>C:\dev\rootfs\bin\clang++.exe<span>'</span></span>
<span>$<span>env:</span>PKG_CONFIG</span> <span>=</span> <span><span>'</span>C:\dev\rootfs\bin\pkg-config.exe<span>'</span></span>
<span>$<span>env:</span>CGO_CXXFLAGS</span> <span>=</span> <span><span>'</span>-Wno-ignored-attributes -D_Bool=bool<span>'</span></span> <span><span>#</span> Clang 18 recommendation</span></pre></div>
<ol start="4" dir="auto">
<li>Run <code>go build -ldflags "-s -w -H windowsgui"</code></li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows (MSYS2)</h3><a id="user-content-windows-msys2" aria-label="Permalink: Windows (MSYS2)" href="#windows-msys2"></a></p>
<p dir="auto"><em>Tested with MSYS2 UCRT64</em></p>
<p dir="auto">For dynamic builds:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pacman -S mingw-w64-ucrt-x86_64-{go,gcc,qt5-base,pkg-config}
GOROOT=/ucrt64/lib/go go build -ldflags &quot;-s -w -H windowsgui&quot;"><pre>pacman -S mingw-w64-ucrt-x86_64-{go,gcc,qt5-base,pkg-config}
GOROOT=/ucrt64/lib/go go build -ldflags <span><span>"</span>-s -w -H windowsgui<span>"</span></span></pre></div>
<p dir="auto">Static builds are also available by installing the <code>mingw-w64-ucrt-x86_64-qt5-static</code> package and building with <code>--tags=windowsqtstatic</code>.</p>
<p dir="auto">The MSYS2 Qt packages in MSYS2 link against <code>libicu</code>, whereas the Fsu0413 Qt packages do not. When using MSYS2, your distribution size including <code>.dll</code> files will be larger.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows (Docker)</h3><a id="user-content-windows-docker" aria-label="Permalink: Windows (Docker)" href="#windows-docker"></a></p>
<p dir="auto"><em>Tested with MXE Qt 5.15 / MXE GCC 5 under cross-compilation</em></p>
<p dir="auto">For static builds (open source application):</p>
<ol dir="auto">
<li>Build the necessary docker container for cross-compilation:
<ul dir="auto">
<li><code>docker build -t miqt/win64-cross:latest -f win64-cross-go1.23-qt5.15-static.Dockerfile .</code></li>
</ul>
</li>
<li>Build your application:
<ul dir="auto">
<li><code>docker run --rm -v $(pwd):/src -w /src miqt/win64-cross:latest go build -buildvcs=false --tags=windowsqtstatic -ldflags '-s -w -H windowsgui'</code></li>
</ul>
</li>
</ol>
<p dir="auto">For dynamically-linked builds (closed-source or open source application):</p>
<ol dir="auto">
<li>Build the necessary docker container for cross-compilation:
<ul dir="auto">
<li><code>docker build -t miqt/win64-dynamic:latest -f win64-cross-go1.23-qt5.15-dynamic.Dockerfile .</code></li>
</ul>
</li>
<li>Build your application:
<ul dir="auto">
<li><code>docker run --rm -v $(pwd):/src -w /src miqt/win64-dynamic:latest go build -buildvcs=false -ldflags '-s -w -H windowsgui'</code></li>
</ul>
</li>
<li>Copy necessary Qt LGPL libraries and plugin files.</li>
</ol>
<p dir="auto">See FAQ Q3 for advice about docker performance.</p>
<p dir="auto">To add an icon and other properties to the .exe, you can use <a href="https://github.com/tc-hib/go-winres">the go-winres tool</a>. See the <code>examples/windowsmanifest</code> for details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Android (Docker)</h3><a id="user-content-android-docker" aria-label="Permalink: Android (Docker)" href="#android-docker"></a></p>
<p dir="auto"><em>Tested with Raymii Qt 5.15 / Android SDK 31 / Android NDK 22</em></p>
<p dir="auto">Miqt supports compiling for Android. Some extra steps are required to bridge the Java, C++, Go worlds.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mappu/miqt/blob/master/doc/android-architecture.png"><img src="https://github.com/mappu/miqt/raw/master/doc/android-architecture.png" alt=""></a></p>
<ol dir="auto">
<li>Modify your main function to <a href="https://pkg.go.dev/cmd/go#hdr-Build_modes" rel="nofollow">support <code>c-shared</code> build mode</a>.
<ul dir="auto">
<li>Package <code>main</code> must have an empty <code>main</code> function.</li>
<li>Rename your <code>main</code> function to <code>AndroidMain</code> and add a comment <code>//export AndroidMain</code>.</li>
<li>Ensure to <code>import "C"</code>.</li>
<li>Check <code>examples/android</code> to see how to support both Android and desktop platforms.</li>
</ul>
</li>
<li>Build the necessary docker container for cross-compilation:
<ul dir="auto">
<li><code>docker build -t miqt/android:latest -f android-armv8a-go1.23-qt5.15-dynamic.Dockerfile .</code></li>
</ul>
</li>
<li>Build your application as <code>.so</code> format:
<ul dir="auto">
<li><code>docker run --rm -v $(pwd):/src -w /src miqt/android:latest go build -buildmode c-shared -ldflags "-s -w -extldflags -Wl,-soname,my_go_app.so" -o android-build/libs/arm64-v8a/my_go_app.so</code></li>
</ul>
</li>
<li>Build the Qt linking stub:
<ul dir="auto">
<li><code>docker run --rm -v $(pwd):/src -w /src miqt/android:latest android-stub-gen.sh my_go_app.so AndroidMain android-build/libs/arm64-v8a/libRealAppName_arm64-v8a.so</code></li>
<li>The linking stub is needed because Qt for Android will itself only call a function named <code>main</code>, but <code>c-shared</code> can't create one.</li>
</ul>
</li>
<li>Build the <a href="https://doc.qt.io/qt-6/android-deploy-qt-tool.html" rel="nofollow">androiddeployqt</a> configuration file:
<ul dir="auto">
<li><code>docker run --rm -v $(pwd):/src -w /src miqt/android:latest android-mktemplate.sh RealAppName deployment-settings.json</code></li>
</ul>
</li>
<li>Build the android package:
<ul dir="auto">
<li><code>docker run --rm -v $(pwd):/src -w /src miqt/android:latest androiddeployqt --input ./deployment-settings.json --output ./android-build/</code></li>
<li>By default, the resulting <code>.apk</code> is generated at <code>android-build/build/outputs/apk/debug/android-build-debug.apk</code>.</li>
<li>You can build in release mode by adding <code>--release</code></li>
</ul>
</li>
</ol>
<p dir="auto">See FAQ Q3 for advice about docker performance.</p>
<p dir="auto">For repeated builds, only steps 3 and 6 are needed. If you customize the <code>AndroidManifest.xml</code> file or images, they will be used for the next <code>androiddeployqt</code> run.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US weighs Google break-up in landmark antitrust case (176 pts)]]></title>
            <link>https://www.ft.com/content/f6e84608-e0e5-48c5-a0eb-dde7675fb608</link>
            <guid>41784287</guid>
            <pubDate>Wed, 09 Oct 2024 03:39:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ft.com/content/f6e84608-e0e5-48c5-a0eb-dde7675fb608">https://www.ft.com/content/f6e84608-e0e5-48c5-a0eb-dde7675fb608</a>, See on <a href="https://news.ycombinator.com/item?id=41784287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a data-trackable="a11y-skip-to-help" href="https://www.ft.com/accessibility">Accessibility help</a><a data-trackable="a11y-skip-to-navigation" href="#site-navigation">Skip to navigation</a><a data-trackable="a11y-skip-to-content" href="#site-content">Skip to content</a><a data-trackable="a11y-skip-to-footer" href="#site-footer">Skip to footer</a></p><div id="barrier-page"><div id="heroOffer-Hero offer-ab053f63-b166-43d8-8e17-a2e48591ce05" data-component="heroOffer" data-component-unique-name="Hero offer"><section data-o-grid-colspan="12 L6"><h2><blockquote>US weighs Google break-up in landmark antitrust case</blockquote></h2></section><div data-o-grid-colspan="12 L6"><p><span></span><span></span><span></span><span>Subscribe to unlock this article</span><span></span></p></div><div data-o-grid-colspan="12 L6"><p><h2><span>Limited time offer</span></h2><h2><strong><span>Save 50% on Standard Digital</span></strong></h2></p><p><span>was </span><span>CHF660</span><span> </span><span>now </span><span>CHF329</span><span> for your first year, equivalent to </span><span>CHF27.42</span><span> per month.
Make up your own mind. Build robust opinions with the FT’s trusted journalism.
Take this offer before 24 October.</span></p></div></div><div id="recommendedOffers-Recommended offers-dbca9928-8f3b-4957-aef0-00636255b582" data-component="recommendedOffers" data-component-unique-name="Recommended offers"><p><h2 data-o-grid-colspan="12">Explore more offers.</h2></p><div data-o-grid-colspan="12"><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_trial.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF1</span><span> for 4 weeks</span></p><p><span>Then </span><span>CHF85</span><span> per month. Complete digital access to quality FT journalism. Cancel anytime during your trial.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_weekend_premium.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF85</span><span> per month</span></p><p><span>Get Premium &amp; FT Weekend Print edition for the price of Premium. Complete digital access to quality analysis and expert insights, complemented with our award-winning Weekend Print edition.</span></p></div><div data-o-grid-colspan="12"><div><p><img src="https://www.ft.com/__origami/service/image/v2/images/raw/https://barrier-page-components.s3.eu-west-1.amazonaws.com/assets/icons/primary_product_icon_print.svg?source=next-barrier-page&amp;format=svg" alt=""></p></div><p><span>CHF345</span><span> for your first year</span></p><p><span>FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.</span></p></div></div></div><div data-component="subscriptionOptions" data-component-unique-name="Subscription options"><h2>Explore our full range of subscriptions.</h2><div><div><p>Discover all the plans currently available in your country</p></div><div><p>Digital access for organisations. Includes exclusive features and content.</p></div></div></div><div data-component="whyFT" data-component-unique-name="Why FT"><div><h2>Why the FT?</h2><p>See why over a million readers pay to read the Financial Times.</p></div><p><a href="https://subs.ft.com/whytheft?ft-content-uuid=f6e84608-e0e5-48c5-a0eb-dde7675fb608">Find out why</a></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Is LibreDrive (2019) (134 pts)]]></title>
            <link>https://forum.makemkv.com/forum/viewtopic.php?t=18856</link>
            <guid>41784069</guid>
            <pubDate>Wed, 09 Oct 2024 02:58:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.makemkv.com/forum/viewtopic.php?t=18856">https://forum.makemkv.com/forum/viewtopic.php?t=18856</a>, See on <a href="https://news.ycombinator.com/item?id=41784069">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A LibreDrive is a mode of operation of an optical disc drive (DVD, Blu-ray or UHD) when the data on the disc are accessed directly, without any restrictions or transformations enforced by drive firmware. A LibreDrive would never refuse to read the data from the disc or declare itself “revoked”. LibreDrive compatible drive is required to read UHD discs.</p><p>

Please read below the overview of LibreDrive architecture.</p><p>

What is an optical disc drive? It is a device that scans the surface of a disc with a laser and passes this information to a controlling software on a PC. Back in time when CD was invented, the optical drive was just that - device that scanned disc surface and passed this data to PC. Many data formats were invented at the time (sector mode 2, p/q channels, sub-channel-data,etc), but in the end it was all about positioning the laser pickup head, scanning the disc, and handling these data over for processing.</p><p>

Things changed for DVD, with the CSS protection scheme. The disc was divided into two areas - one area could be read as usual. However in order to read the data from another area of the disc, the software had to know a certain secret key. This was part of so-called CSS protection and was used both for rights management and to enforce disc region price fixing. A drive locked for Europe would not allow reading the data from the disc that was locked for South East Asia, where exactly same disc could be bought at a cheaper price. This data partitioning was further “enhanced” for blu-ray. In the end, any regular optical disc drive has a complex set of rules that dictate what areas of the disc can be accessed on on what conditions.</p><p>

The drive itself can easily read all the data <em>technically</em>. It is the drive embedded software (firmware) that “plays police” - decides what area of the disc can be accessed and on what conditions.</p><p>

So, how can one read the data from any area on the disc? The data on the optical disc is encoded as a series of small cavities called pits, that are organized in a spiral called a track. Had we had a sharp enough eye, we could've seen them directly. One way is to use a microscope - with a microscope one can clearly see the pits, apply standard (publicly defined) decoding algorithm and get the raw data from the disc. Obviously, this is a rather time consuming and expensive method. There is however a third way - take the drive back to the basic level. Change the optical drive embedded software in a way that the drive becomes a “primitive” device - one that just positions a laser, reads and decodes the data. Make a drive free from “policing” functionality, a drive that just passes all data from the disc to the user. We call this drive a LibreDrive.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The human internet is dying. AI images are taking over Google top results (105 pts)]]></title>
            <link>https://old.reddit.com/r/ChatGPT/comments/1fye6tb/the_human_internet_is_dying_ai_images_taking_over/</link>
            <guid>41783787</guid>
            <pubDate>Wed, 09 Oct 2024 02:10:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/ChatGPT/comments/1fye6tb/the_human_internet_is_dying_ai_images_taking_over/">https://old.reddit.com/r/ChatGPT/comments/1fye6tb/the_human_internet_is_dying_ai_images_taking_over/</a>, See on <a href="https://news.ycombinator.com/item?id=41783787">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><hr>

<p><strong><a href="https://old.reddit.com/r/ChatGPT">r/ChatGPT</a> Rules</strong><br>
All users are encouraged to report posts to the moderators for review.</p>

<hr>

<p><strong>1: Malicious Communication</strong><br>
Posters and commenters are expected to act in good faith. Treat other users the way you want to be treated. Avoid straw-manning and bad-faith interpretations. Avoid presenting misinformation as factual.</p>

<p><strong>2: No Trashposts</strong><br>
Posts deemed to be entirely without value or effort may be removed if they have not generated interesting discussions before their discovery. Users are encouraged to report posts they feel are of significantly low effort. Specifically mentioning that “Is chat GPT down posts?” will be removed. The stickied FAQ deals with that.</p>

<p><strong>3: Self Advertising</strong><br>
Posts must be directly related to ChatGPT or the topic of LLMs. They may not be solely focused on advertising a single other LLM service. Find or establish a relative subreddit for that service.</p>

<p><strong>4: Political Discussion</strong><br>
Having ChatGPT create political content is completely fine. Discussion of the politics around AI and LLMs is allowed. This is not the place to discuss the merits of Trump's foreign policy or Hunter Biden’s laptop.</p>

<hr>

<p><strong><a href="https://discord.gg/r-chatgpt-1050422060352024636">Join us on Discord</a></strong></p>

<p><strong><a href="https://t.me/r_ChatGPT">Join us on Telegram</a></strong></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Emoji as a Favicon Using SVG (2022) (102 pts)]]></title>
            <link>https://css-tricks.com/emoji-as-a-favicon/</link>
            <guid>41783340</guid>
            <pubDate>Wed, 09 Oct 2024 00:39:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://css-tricks.com/emoji-as-a-favicon/">https://css-tricks.com/emoji-as-a-favicon/</a>, See on <a href="https://news.ycombinator.com/item?id=41783340">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
	DigitalOcean provides cloud products for every stage of your journey. Get started with <a href="https://try.digitalocean.com/css-tricks/?utm_medium=content_acq&amp;utm_source=css-tricks&amp;utm_campaign=global_brand_ad_en&amp;utm_content=conversion_prearticle_everystage">$200 in free credit!</a>
</p><div>

          
          
<p>Lea Verou had <a href="https://twitter.com/LeaVerou/status/1241619866475474946" rel="noopener">a dang genius idea</a> to use an emoji as a favicon. The idea only recently possible as browsers have started supporting SVG for favicons. Chuck an emoji inside an SVG <code>&lt;text&gt;</code> element and use that as the favicon. </p>



<p>Here’s the one-liner in use:</p>



<pre rel="HTML" data-line=""><code markup="tt">&lt;link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;🎯&lt;/text&gt;&lt;/svg&gt;"&gt;</code></pre>


<h3 id="demo-project-demonstrating-emoji-as-a-favicon">Demo Project Demonstrating Emoji as a Favicon</h3>


<p>I made <a href="https://codepen.io/chriscoyier/project/editor/ZeWQWJ" rel="noopener">a quick little demo project</a> so you can see it at work. See <a href="https://000458870.codepen.website/" rel="noopener">the deployed project</a> to actually see the favicons. That works in Firefox and Chrome. Safari only does those “mask” style icons in SVG so this doesn’t work there. Maybe it could though? I dunno I’ll let you try it.</p>



<p>Here’s a video in case you just wanna see it.</p>



<figure><video controls="" src="https://res.cloudinary.com/css-tricks/video/upload/f_auto,q_auto,vc_auto/v1585074956/emojis_bxshdw.mp4"></video></figure>





<ul><li>Ada Rose Cannon <a href="https://glitch.com/edit/#!/favicon-badge?path=script.js:1:14" rel="noopener">added a badge</a> that can increment.</li><li>Taylor Hunt dropped some code on how he uses the <a href="https://gist.github.com/tigt/86b25ce970e46370dd61339380e2d6bd" rel="noopener">current Git branch name</a> to create an SVG favicon (related to the <a href="https://css-tricks.com/different-favicon-for-development/">“different favicon for development”</a> idea)  </li><li>You could duck a <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/@media/prefers-color-scheme" rel="noopener"><code>prefers-color-scheme</code> media query</a> in the SVG if you wanted to do something special for dark mode (although emojis generally work well on any background)</li></ul>

          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On the Nature of Time (336 pts)]]></title>
            <link>https://writings.stephenwolfram.com/2024/10/on-the-nature-of-time/</link>
            <guid>41782534</guid>
            <pubDate>Tue, 08 Oct 2024 22:42:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://writings.stephenwolfram.com/2024/10/on-the-nature-of-time/">https://writings.stephenwolfram.com/2024/10/on-the-nature-of-time/</a>, See on <a href="https://news.ycombinator.com/item?id=41782534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2 id="the-computational-view-of-time">The Computational View of Time</h2>
<p>Time is a central feature of human experience. But what actually is it? In traditional scientific accounts it’s often represented as some kind of coordinate much like space (though a coordinate that for some reason is always systematically increasing for us). But while this may be a useful mathematical description, it’s not telling us anything about what time in a sense “intrinsically is”. </p>
<p>We get closer as soon as we start thinking in computational terms. Because then it’s natural for us to think of successive states of the world as being computed one from the last by the progressive application of some computational rule. And this suggests that we can identify the progress of time with the “progressive doing of <nobr id="link-test"> computation</nobr> by the universe”. </p>
<p>But does this just mean that we are replacing a “time coordinate” with a “computational step count”? No. Because of the phenomenon of <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-6--computational-irreducibility">computational irreducibility</a>. With the traditional mathematical idea of a time coordinate one typically imagines that this coordinate can be “set to any value”, and that then one can immediately calculate the state of the system at that time. But computational irreducibility implies that it’s not that easy. Because it says that there’s often essentially no better way to find what a system will do than by explicitly tracing through each step in its evolution.<span id="more-63438"></span></p>
<p>In the pictures on the left there’s computational reducibility, and one can readily see what state will be after any number of steps <em>t</em>. But in the pictures on the right there’s (presumably) computational irreducibility, so that the only way to tell what will happen after <em>t</em> steps is effectively to run all those steps:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/10/sw10082024timeBimg1.png" alt="" title="" width="642" height="178"> </p>
</div>
<p>And this implies is that there’s a certain robustness to time when viewed in these computational terms. There’s no way to “jump ahead” in time; the only way to find out what will happen in the future is to go through the irreducible computational steps to get there. </p>
<p>There are simple idealized systems (say with purely periodic behavior) where there’s computational reducibility, and where there isn’t any robust notion of the progress of time. But the point is that—as the <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/">Principle of Computational Equivalence</a> implies—our universe is inevitably full of computational irreducibility which in effect defines a robust notion of the progress of time.</p>
<h2 id="the-role-of-the-observer">The Role of the Observer</h2>
<p>That time is a reflection of the progress of computation in the universe is an important starting point. But it’s not the end of the story. For example, here’s an immediate issue. If we have a computational rule that determines each successive state of a system it’s at least in principle possible to know the whole future of the system. So given this why then do we have the experience of the future only “unfolding as it happens”?</p>
<p>It’s fundamentally because of the way <a href="https://writings.stephenwolfram.com/2023/12/observer-theory/">we are as observers</a>. If the underlying system is computationally irreducible, then to work out its future behavior requires an irreducible amount of computational work. But it’s a core feature of observers like us that we are computationally bounded. So we can’t do all that irreducible computational work to “know the whole future”—and instead we’re effectively stuck just doing computation alongside the system itself, never able to substantially “jump ahead”, and only able to see the future “progressively unfold”.</p>
<p>In essence, therefore, we experience time because of the interplay between our computational boundedness as observers, and the computational irreducibility of underlying processes in the universe. If we were not computationally bounded, we could “perceive the whole of the future in one gulp” and we wouldn’t need a notion of time at all. And if there wasn’t underlying computational irreducibility there wouldn’t be the kind of “progressive revealing of the future” that we associate with our experience of time.</p>
<p>A notable feature of our everyday perception of time is that it seems to “flow only in one direction”—so that for example it’s generally much easier to remember the past than to predict the future. And this is closely related to the Second Law of thermodynamics, which (as <a href="https://writings.stephenwolfram.com/2023/02/computational-foundations-for-the-second-law-of-thermodynamics/">I’ve argued at length elsewhere</a>) is once again a result of the interplay between underlying computational irreducibility and our computational boundedness. Yes, the microscopic laws of physics may be reversible (and indeed if our system is simple—and computationally reducible—enough of this reversibility may “shine through”). But the point is that computational irreducibility is in a sense a much stronger force.</p>
<p>Imagine that we prepare a state to have orderly structure. If its evolution is computationally irreducible then this structure will effectively be “encrypted” to the point where a computationally bounded observer can’t recognize the structure. Given underlying reversibility, the structure is in some sense inevitably “still there”—but it can’t be “accessed” by a computationally bounded observer. And as a result such an observer will perceive a definite flow from orderliness in what is prepared to disorderliness in what is observed. (In principle one might think it should be possible to set up a state that will “behave antithermodynamically”—but the point is that to do so would require predicting a computationally irreducible process, which a computationally bounded observer can’t do.)</p>
<p>One of the longstanding confusions about the nature of time has to do with its “mathematical similarity” to space. And indeed ever since the early days of relativity theory it’s seemed convenient to talk about “spacetime” in which notions of space and time are bundled together.</p>
<p>But in our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a> that’s not at all how things fundamentally work. At the lowest level the state of the universe is <a href="https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/#what-is-space">represented by a hypergraph</a> which captures what can be thought of as the “spatial relations” between discrete “atoms of space”. <a href="https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/#time">Time then corresponds</a> to the progressive rewriting of this hypergraph.</p>
<p>And in a sense the “atoms of time” are the elementary “rewriting events” that occur. If the “output” from one event is needed to provide “input” to another, then we can think of the first event as preceding the second event in time—and the events as being “timelike separated”. And in general we can construct a <a href="https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/#the-graph-of-causal-relationships">causal graph</a> that shows the dependencies between different events. </p>
<p>So how does this relate to time—and spacetime? As we’ll discuss below, our everyday experience of time is that it follows a single thread. And so we tend to want to “parse” the causal graph of elementary events into a series of slices that we can view as corresponding to “successive times”. As in <a href="https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/#deriving-special-relativity">standard relativity theory</a>, there typically isn’t a unique way to assign a sequence of such “simultaneity surfaces”, with the result that there are different “reference frames” in which the identifications of space and time are different. </p>
<p>The complete causal graph bundles together what we usually think of as space with what we usually think of as time. But ultimately the progress of time is always associated with some choice of successive events that “computationally build on each other”. And, yes, it’s more complicated because of the possibilities of different choices. But the basic idea of the progress of time as “the doing of computation” is very much the same. (In a sense time represents “computational progress” in the universe, while space represents the “layout of its data structure”.)</p>
<p>Very much as in the derivation of the Second Law (or of fluid mechanics from molecular dynamics), the <a href="https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/#general-relativity-and-gravity">derivation of Einstein’s equations</a> for the large-scale behavior of spacetime from the underlying causal graph of hypergraph rewriting depends on the fact that we are computationally bounded observers. But even though we’re computationally bounded, we still have to “have something going on inside”, or we wouldn’t record—or sense—any “progress in time”.</p>
<p>It seems to be the essence of observers like us—as captured in my <a href="https://writings.stephenwolfram.com/2023/12/observer-theory/">recent Observer Theory</a>—that we equivalence many different states of the world to derive our internal perception of “what’s going on outside”. And at some rough level we might imagine that we’re sensing time passing by the rate at which we add to those internal perceptions. If we’re not adding to the perceptions, then in effect time will stop for us—as happens if we’re asleep, anesthetized or dead. </p>
<p>It’s worth mentioning that in some extreme situations it’s not the internal structure of the observer that makes perceived time stop; instead it’s the underlying structure of the universe itself. As we’ve mentioned, the “progress of the universe” is associated with successive rewriting of the underlying hypergraph. But when there’s been “too much activity in the hypergraph” (which physically corresponds roughly to too much energy-momentum), one can end up with a situation in which “there are no more rewrites that can be done”—so that in effect some part of the universe can no longer progress, <a href="https://www.wolframphysics.org/technical-introduction/potential-relation-to-physics/cosmology-expansion-and-singularities" target="_blank" rel="noopener">and “time stops” there</a>. It’s analogous to what happens at a spacelike singularity (normally associated with a black hole) in traditional general relativity. But now it has a very direct computational interpretation: one’s reached a “fixed point” at which there’s no more computation to do. And so there’s no progress to make in time.</p>
<h2 id="multiple-threads-of-time">Multiple Threads of Time</h2>
<p>Our strong human experience is that time progresses as a single thread. But now our <a href="https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/#the-inevitability-of-quantum-mechanics">Physics Project suggests</a> that at an underlying level time is actually in effect multithreaded, or, in other words, that there are many different “paths of history” that the universe follows. And it is only because of the way we as observers sample things that we experience time as a single thread. </p>
<p>At the level of a particular underlying hypergraph the point is that there may be many different updating events that can occur, and each sequence of such updating event defines a different “path of history”. We can summarize all these paths of history in a multiway graph in which we merge identical states that arise:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/10/sw10082024timeAimg2.png" alt="" title="" width="194" height="242"> </p>
</div>
<p>But given this underlying structure, why is it that we as observers believe that time progresses as a single thread? It all has to do with the notion of <a href="https://www.wolframphysics.org/technical-introduction/the-updating-process-for-string-substitution-systems/the-concept-of-branchial-graphs/" target="_blank" rel="noopener">branchial space</a>, and our presence within branchial space. The presence of many paths of history is what leads to quantum mechanics; the fact that we as <a href="https://writings.stephenwolfram.com/2023/12/observer-theory/#the-case-of-quantum-mechanics">observers ultimately perceive just one path</a> is associated with the traditionally-quite-mysterious phenomenon of “measurement” in quantum mechanics. </p>
<p>When we talked about causal graphs above, we said that we could “parse” them as a series of “spacelike” slices corresponding to instantaneous “states of space”—represented by spatial hypergraphs. And by analogy we can similarly imagine breaking multiway graphs into “instantaneous slices”. But now these slices don’t represent states of ordinary space; instead they represent states of what we call branchial space.</p>
<p>Ordinary space is “knitted together” by updating events that have causal effects on other events that can be thought of as “located at different places in space”. (Or, said differently, space is knitted together by the overlaps of the elementary light cones of different events.) Now we can think of branchial space as being “knitted together” by updating events that have effects on events that end up on different branches of history. </p>
<p>(In general there is a close analogy between ordinary space and branchial space, and we can define a multiway causal graph that includes both “spacelike” and “branchlike” directions—with the branchlike direction supporting not light cones but what we can call entanglement cones.)</p>
<p>So how do we as observers parse what’s going on? A key point is that we are inevitably part of the system we’re observing. So the branching (and merging) that’s going on in the system at large is also going on in us. So that means we have to ask how a “branching mind” will perceive a branching universe. Underneath, there are lots of branches, and lots of “threads of history”. And there’s lots of computational irreducibility (and even what we can call <a href="https://writings.stephenwolfram.com/2022/06/games-and-puzzles-as-multicomputational-systems/#humanizing-multicomputational-processes">multicomputational irreducibility</a>). But computationally bounded observers like us have to equivalence most of those details to wind up with something that “fits in our finite minds”. </p>
<p>We can make an analogy to what happens in a gas. Underneath, there are lots of molecules bouncing around (and behaving in computationally irreducible ways). But observers like us are big compared to molecules, and (being computationally bounded) we don’t get to perceive their individual behavior, but only their aggregate behavior—from which we extract a thin set of computationally reducible “fluid-dynamics-level” features. </p>
<p>And it’s basically the same story with the underlying structure of space. Underneath, there’s an elaborately changing network of discrete atoms of space. But as large, computationally bounded observers we can only sample aggregate features in which many details have been equivalenced, and in which space tends to seem continuous and describable in basically computationally reducible ways. </p>
<p>So what about branchial space? Well, it’s basically the same story. Our minds are “big”, in the sense that they span many individual branches of history. And they’re computationally bounded so they can’t perceive the details of all those branches, but only certain aggregated features. And in a first approximation what then emerges is in effect a single aggregated thread of history. </p>
<p>With sufficiently careful measurements we can sometimes see “quantum effects” in which multiple threads of history are in evidence. But at a direct human level we always seem to aggregate things to the point where what we perceive is just a single thread of history—or in effect a single thread of progression in time.</p>
<p>It’s not immediately obvious that any of these “aggregations” will work. It could be that important effects we perceive in gases would depend on phenomena at the level of individual molecules. Or that to understand the large-scale structure of space we’d continually be having to think about detailed features of atoms of space. Or, similarly, that we’d never be able to maintain a “consistent view of history”, and that instead we’d always be having to trace lots of individual threads of history. </p>
<p>But the key point is that for us to stay as computationally bounded observers we have to pick out only features that are computationally reducible—or in effect boundedly simple to describe. </p>
<p>Closely related to our computational boundedness is the <a href="https://writings.stephenwolfram.com/2021/03/what-is-consciousness-some-new-perspectives-from-our-physics-project/">important assumption</a> we make that we as observers have a certain persistence. At every moment in time, we are made from different atoms of space and different branches in the multiway graph. Yet we believe we are still “the same us”. And the crucial physical fact (that has to be derived in our model) is that in ordinary circumstances there’s no inconsistency in doing this.</p>
<p>So the result is that even though there are many “threads of time” at the lowest level—representing many different “quantum branches”—observers like us can (usually) successfully still view there as being a single consistent perceived thread of time.</p>
<p>But there’s another issue here. It’s one thing to say that a single observer (say a single human mind or a single measuring device) can perceive history to follow a single, consistent thread. But what about different human minds, or different measuring devices? Why should they perceive any kind of consistent “objective reality”?</p>
<p>Essentially the answer, I think, is that they’re all sufficiently nearby in branchial space. If we think about physical space, observers in different parts of the universe will clearly “see different things happening”. The “laws of physics” may be the same—but what star (if any) is nearby will be different. Yet (at least for the foreseeable future) for all of us humans it’s always the same star that’s nearby. </p>
<p>And so it is, presumably, in branchial space. There’s some small patch in which we humans—with our shared origins—exist. And it’s presumably because that patch is small relative to all of branchial space that all of us perceive a consistent thread of history and a common objective reality. </p>
<p>There are many subtleties to this, many of which aren’t yet fully worked out. In physical space, we know that effects can in principle spread at the speed of light. And in branchial space the analog is that effects can spread at the maximum entanglement speed (whose value we don’t know, though it’s <a href="https://www.wolframphysics.org/technical-introduction/potential-relation-to-physics/units-and-scales/" target="_blank" rel="noopener">related by Planck unit conversions to the elementary length and elementary time</a>). But in maintaining our shared “objective” view of the universe it’s crucial that we’re not all going off in different directions at the speed of light. And of course the reason that doesn’t happen is that we don’t have zero mass. And indeed presumably nonzero mass is a critical part of being observers like us. </p>
<p>In our Physics Project it’s roughly the density of events in the hypergraph that determines the density of energy (and mass) in physical space (with their associated gravitational effects). And similarly it’s roughly the density of events in the multiway graph (or in branchial graph slices) that determines the density of action—the relativistically invariant analog of energy—in branchial space (with its associated effects on quantum phase). And though it’s not yet completely clear how this works, it seems likely that once again when there’s mass, effects don’t just “go off at the maximum entanglement speed in all directions”, but instead stay nearby.</p>
<p>There are definitely connections between “staying at the same place”, believing one is persistent, and being computationally bounded. But these are what seem necessary for us to have our typical view of time as a single thread. In principle we can imagine observers very different from us—say with minds (like the inside of an idealized quantum computer) capable of experiencing many different threads of history. But the Principle of Computational Equivalence suggests that there’s a high bar for such observers. They need not only to be able to deal with computational irreducibility but also multicomputational irreducibility, in which one includes both the process of computing new states, and the process of equivalencing states. </p>
<p>And so for observers that are “anything like us” we can expect that once again time will tend to be as we normally experience it, following a single thread, consistent between observers. </p>
<p>(It’s worth mentioning that all of this only works for observers like us “in situations like ours”. For example, at the <a href="https://www.wolframphysics.org/technical-introduction/potential-relation-to-physics/event-horizons-and-singularities-in-spacetime-and-quantum-mechanics/" target="_blank" rel="noopener">“entanglement horizon” for a black hole</a>—where branchially-oriented edges in the multiway causal graph get “trapped”—time as we know it in some sense “disintegrates”, because an observer won’t be able to “knit together” the different branches of history to “form a consistent classical thought” about what happens.)</p>
<h2 id="time-in-the-ruliad">Time in the Ruliad</h2>
<p>In what we’ve discussed so far we can think of the progress of time as being associated with the repeated application of rules that progressively “rewrite the state of the universe”. In the previous section we saw that these rules can be applied in many different ways, leading to many different underlying threads of history. </p>
<p>But so far we’ve imagined that the rules that get applied are always the same—leaving us with the mystery of “Why those rules, and not others?” But this is where <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/">the ruliad</a> comes in. Because the ruliad involves no such seemingly arbitrary choices: it’s what you get by following all possible computational rules.</p>
<p>One can imagine many bases for the ruliad. One can make it from all possible hypergraph rewritings. Or all possible (multiway) Turing machines. But in the end it’s a single, unique thing: the entangled limit of all possible computational processes. There’s a sense in which “everything can happen somewhere” in the ruliad. But what gives the ruliad structure is that there’s a definite (essentially geometrical) way in which all those different things that can happen are arranged and connected.</p>
<p>So what is our perception of the ruliad? Inevitably we’re part of the ruliad—so we’re observing it “from the inside”. But the crucial point is that what we perceive about it depends on what we are like as observers. And my big surprise in the past few years has been that assuming even just a little about what we’re like as observers immediately implies that what we perceive of the ruliad follows the core laws of physics we know. In other words, by assuming what we’re like as observers, we can in effect derive our laws of physics.</p>
<p>The key to all this is the interplay between the computational irreducibility of underlying behavior in the ruliad, and our computational boundedness as observers (together with our related assumption of our persistence). And it’s this interplay that gives us the Second Law in statistical mechanics, the Einstein equations for the structure of spacetime, and (we think) the path integral in quantum mechanics. In effect what’s happening is that our computational boundedness as observers makes us equivalence things to the point where we are sampling only computationally reducible slices of the ruliad, whose characteristics can be described using recognizable laws of physics.</p>
<p>So where does time fit into all of this? A central feature of the ruliad is that it’s unique—and everything about it is “abstractly necessary”. Much as given the definition of numbers, addition and equality it’s inevitable that one gets 1 + 1 = 2, so similarly given the definition of computation it’s inevitable that one gets the ruliad. Or, in other words, there’s no question about whether the ruliad exists; it’s just an abstract construct that inevitably follows from abstract definitions. </p>
<p>And so at some level this means that the ruliad inevitably just “exists as a complete thing”. And so if one could “view it from outside” one could think of it as just a single timeless object, with no notion of time. </p>
<p>But the crucial point is that we don’t get to “view it from the outside”. We’re embedded within it. And, what’s more, we must view it through the “lens” of our computational boundedness. And this is why we inevitably end up with a notion of time. </p>
<p>We observe the ruliad from some point within it. If we were not computationally bounded then we could immediately compute what the whole ruliad is like. But in actuality we can only discover the ruliad “one computationally bounded step at a time”—in effect progressively applying bounded computations to “move through rulial space”. </p>
<p>So even though in some abstract sense “the whole ruliad is already there” we only get to explore it step by step. And that’s what gives us our notion of time, through which we “progress”. </p>
<p>Inevitably, there are many different paths that we could follow through the ruliad. And indeed every mind (and every observer like us)—with its distinct inner experience—presumably follows a different path. But much as we described for branchial space, the reason we have a shared notion of “objective reality” is presumably that we are all very close together in rulial space; we form in a sense a tight “rulial flock”.</p>
<p>It’s worth pointing out that not every sampling of the ruliad that may be accessible to us conveniently corresponds to exploration of progressive slices of time. Yes, that kind of “progression in time” is characteristic of our physical experience, and our typical way of describing it. But what about our experience, say, of mathematics?</p>
<p>The first point to make is that just as the ruliad contains all possible physics, it also <a href="https://writings.stephenwolfram.com/2022/03/the-physicalization-of-metamathematics-and-its-implications-for-the-foundations-of-mathematics/">contains all possible mathematics</a>. If we construct the ruliad, say from hypergraphs, the nodes are now not “atoms of space”, but instead abstract elements (that in general <a href="https://writings.stephenwolfram.com/2022/03/the-physicalization-of-metamathematics-and-its-implications-for-the-foundations-of-mathematics/#metamath-emes">we call emes</a>) that form pieces of mathematical expressions and mathematical theorems. We can think of these abstract elements as being laid out now not in physical space, but in some abstract metamathematical space. </p>
<p>In our physical experience, we tend to remain localized in physical space, branchial space, etc. But in “doing mathematics” it’s more as if we’re progressively expanding in metamathematical space, carving out some domain of “theorems we assume are true”. And while we could identify some kind of “path of expansion” to let us define some analog of time, it’s not a necessary feature of the way we explore the ruliad.</p>
<p>Different places in the ruliad in a sense correspond to describing things using different rules. And by analogy to the concept of motion in physical space, we can effectively “move” from one place to another in the ruliad by translating the computations done by one set of rules to computations done by another. (And, yes, it’s nontrivial to even have the <a href="https://writings.stephenwolfram.com/2022/03/on-the-concept-of-motion/">possibility of “pure motion”</a>.) But if we indeed remain localized in the ruliad (and can maintain what we can think of as our “coherent identity”) then it’s natural to think of there being a “path of motion” along which we progress “with time”. But when we’re just “expanding our horizons” to encompass more paradigms and to bring more of rulial space into what’s covered by our minds (so that in effect we’re “expanding in rulial space”), it’s not really the same story. We’re not thinking of ourselves as “doing computation in order to move”. Instead, we’re just identifying equivalences and using them to expand our definition of ourselves, which is something that we can at least approximate (much like in “quantum measurement” in traditional physics) as happening “outside of time”. Ultimately, though, everything that happens must be the result of computations that occur. It’s just that we don’t usually “package” these into what we can describe as a definite thread of time.</p>
<h2 id="so-what-in-the-end-is-time">So What in the End Is Time?</h2>
<p>From the paradigm (and Physics Project ideas) that we’ve discussed here, the question “What is time?” is at some level simple: time is what progresses when one applies computational rules. But what’s critical is that time can in effect be defined abstractly, independent of the details of those rules, or the “substrate” to which they’re applied. And what makes this possible is the Principle of Computational Equivalence, and the ubiquitous phenomenon of computational irreducibility that it implies.</p>
<p>To begin with, the fact that time can robustly be thought of as “progressing”, in effect in a linear chain, is a consequence of computational irreducibility—because computational irreducibility is what tells us that computationally bounded observers like us can’t in general ever “jump ahead”; we just have to follow a linear chain of steps. </p>
<p>But there’s something else as well. The Principle of Computational Equivalence implies that there’s in a sense just one (ubiquitous) kind of computational irreducibility. So when we look at different systems following different irreducible computational rules, there’s inevitably a certain universality to what they do. In effect they’re all “accumulating computational effects” in the same way. Or in essence progressing through time in the same way.</p>
<p>There’s a close analogy here with heat. It could be that there’d be detailed molecular motion that even on a large scale <a href="https://writings.stephenwolfram.com/2023/01/how-did-we-get-here-the-tangled-history-of-the-second-law-of-thermodynamics/#what-is-heat">worked noticeably differently in different materials</a>. But the fact is that we end up being able to characterize any such motion just by saying that it represents a certain amount of heat, without getting into more details. And that’s very much the same kind of thing as being able to say that such-and-such an amount of time has passed, without having to get into the details of how some clock or other system that reflects the passage of time actually works.</p>
<p>And in fact there’s more than a “conceptual analogy” here. Because the <a href="https://writings.stephenwolfram.com/2023/02/computational-foundations-for-the-second-law-of-thermodynamics/">phenomenon of heat is again a consequence of computational irreducibility</a>. And the fact that there’s a uniform, “abstract” characterization of it is a consequence of the universality of computational irreducibility.</p>
<p>It’s worth emphasizing again, though, that just as with heat, a robust concept of time depends on us being computationally bounded observers. If we were not, then we’d able to break the Second Law by doing detailed computations of molecular processes, and we wouldn’t just describe things in terms of randomness and heat. And similarly, we’d be able to break the linear flow of time, either jumping ahead or following different threads of time. </p>
<p>But as computationally bounded observers of computationally irreducible processes, it’s basically inevitable that—at least to a good approximation—we’ll view time as something that forms a single one-dimensional thread. </p>
<p>In traditional mathematically based science there’s often a feeling that the goal should be to “predict the future”—or in effect to “outrun time”. But computational irreducibility tells us that in general we can’t do this, and that the only way to find out what will happen is just to run the same computation as the system itself, essentially step by step. But while this might seem like a letdown for the power of science, we can also see it as what gives meaning and significance to time. If we could always jump ahead then at some level nothing would ever fundamentally be achieved by the <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence#sect-12-7--the-phenomenon-of-free-will">passage of time (or, say, by the living of our lives)</a>; we’d always be able to just say what will happen, without “living through” how we got there. But computational irreducibility gives time and the process of it passing a kind of hard, tangible character.</p>
<p>So what does all this imply for the various classic issues (and apparent paradoxes) that arise in the way time is usually discussed? </p>
<p>Let’s start with the question of reversibility. The traditional laws of physics basically apply both forwards and backwards in time. And the ruliad is inevitably symmetrical between “forward” and “backward” rules. So why is it then that in our typical experience time always seems to “run in the same direction”? </p>
<p>This is closely related to the Second Law, and once again it’s consequence of our computational boundedness interacting with underlying computational irreducibility. In a sense what defines the direction of time for us is that we (typically) find it much easier to remember the past than to predict the future. Of course, we don’t remember every detail of the past. We only remember what amounts to certain “filtered” features that “fit in our finite minds”. And when it comes to predicting the future, we’re limited by our inability to “outrun” computational irreducibility. </p>
<p>Let’s recall how the Second Law works. It basically says that if we set up some state that’s “ordered” or “simple” then this will tend to “degrade” to one that’s “disordered” or “random”. (We can think of the evolution of the system as effectively “encrypting” the specification of our starting state to the point where we—as computationally bounded observers—can no longer recognize its ordered origins.) But because our underlying laws are reversible, this degradation (or “encryption”) must happen when we go both forwards and backwards in time:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/sites/43/2024/10/sw10082024timeAimg3.png" alt="" title="" width="169" height="167"> </p>
</div>
<p>But the point is that our “experiential” definition of the direction of time (in which the “past” is what we remember, and the “future” is what we find hard to predict) is inevitably aligned with the “thermodynamic” direction of time we observe in the world at large. And the reason is that in both cases we’re defining the past to be something that’s computationally bounded (while the future can be computationally irreducible). In the experiential case the past is computationally bounded because that’s what we can remember. In the thermodynamic case it’s computationally bounded because those are the states we can prepare. In other words, the “arrows of time” are aligned because in both cases we are in effect “requiring the past to be simpler”. </p>
<p>So what about time travel? It’s a concept that seems natural—and perhaps even inevitable—if one imagines that “time is just like space”. But it becomes a lot less natural when we think of time in the way we’re doing here: as a process of applying computational rules. </p>
<p>Indeed, at the lowest level, these rules are by definition just sequentially applied, producing one state after another—and in effect “progressing in one direction through time”. But things get more complicated if we consider not just the raw, lowest-level rules, but what we might actually observe of their effects. For example, what if the rules lead to a state that’s identical to one they’ve produced before (as happens, for example, in a system with periodic behavior)? If we equivalence the state now and the state before (so we represent both as a single state) then we can <a href="https://writings.stephenwolfram.com/2020/04/finally-we-may-have-a-path-to-the-fundamental-theory-of-physics-and-its-beautiful/#black-holes-singularities-etc">end up with a loop in our causal graph (a “closed timelike curve”)</a>. And, yes, in terms of the raw sequence of applying rules these states can be considered different. But the point is that if they are identical in every feature then any observer will inevitably consider them the same. </p>
<p>But will such equivalent states ever actually occur? As soon as there’s computational irreducibility it’s basically inevitable that the states will never perfectly match up. And indeed for the states to contain an observer like us (with “memory”, etc.) it’s basically impossible that they can match up. </p>
<p>But can one imagine an observer (or a “timecraft”) that would lead to states that match up? Perhaps somehow it could carefully pick particular sequences of atoms of space (or elementary events) that would lead it to states that have “happened before”. And indeed in a computationally simple system this might be possible. But as soon as there’s computational irreducibility, this simply isn’t something one can expect any computationally bounded observer to be able to do. And, yes, this is directly analogous to why one <a href="https://writings.stephenwolfram.com/2023/02/computational-foundations-for-the-second-law-of-thermodynamics/#maxwells-demon-and-the-character-of-observers">can’t have a “Maxwell’s demon”</a> observer that “breaks the Second Law”. Or why one can’t have something that carefully navigates the lowest-level structure of space to <a href="https://writings.stephenwolfram.com/2020/10/faster-than-light-in-our-model-of-physics-some-preliminary-thoughts/">effectively travel faster than light</a>. </p>
<p>But even if there can’t be time travel in which “time for an observer goes backwards”, there can still be changes in “perceived time”, say as a result of relativistic effects associated with motion. For example, one classic relativistic effect is time dilation, in which “time goes slower” when objects go faster. And, yes, given certain assumptions, there’s a straightforward mathematical derivation of this effect. But in our effort to understand the nature of time we’re led to ask what its physical mechanism might be. And it turns out that in our Physics Project it has a surprisingly direct—and almost “mechanical”—explanation.</p>
<p>One starts from the fact that in our Physics Project space and everything in it is represented by a hypergraph which is continually getting rewritten. And the evolution of any object through time is then defined by these rewritings. But if the object moves, then in effect it has to be “re-created at a different place in space”—and this process takes up a certain number of rewritings, leaving fewer for the intrinsic evolution of the object itself, and thus causing time to effectively “run slower” for it. (And, yes, while this is a qualitative description, one can make it quite formal and precise, and recover the usual formulas for relativistic time dilation.)</p>
<p>Something similar happens with gravitational fields. In our Physics Project, energy-momentum (and thus gravity) is effectively associated with greater activity in the underlying hypergraph. And the presence of this greater activity leads to more rewritings, causing “time to run faster” for any object in that region of space (corresponding to the traditional “gravitational redshift”). </p>
<p>More extreme versions of this occur in the context of black holes. (Indeed, one can roughly think of spacelike singularities as places where “time ran so fast that it ended”.) And in general—as we discussed above—there are many “relativistic effects” in which notions of space and time get mixed in various ways. </p>
<p>But even at a much more mundane level there’s a certain crucial relationship between space and time for observers like us. The key point is that observers like us tend to “parse” the world into a sequence of “states of space” at successive “moments in time”. But the fact that we do this depends on some quite specific features of us, and in particular our effective physical scale in space as compared to time. </p>
<p>In our everyday life we’re typically looking at scenes involving objects that are perhaps tens of meters away from us. And given the speed of light that means photons from these objects get to us in less than a microsecond. But it takes our brains milliseconds to register what we’ve seen. And this disparity of timescales is what leads us to view the world as consisting of a sequence of states of space at successive moments in time. </p>
<p>If our brains “ran” a million times faster (i.e. at the speed of digital electronics) we’d perceive photons arriving from different parts of a scene at different times, and we’d presumably no longer view the world in terms of overall states of space existing at successive times. </p>
<p>The same kind of thing would happen if we kept the speed of our brains the same, but dealt with scenes of a much larger scale (as we already do in dealing with spacecraft, astronomy, etc.).</p>
<p>But while this affects what it is that we think time is “acting on”, it doesn’t ultimately affect the nature of time itself. Time remains that computational process by which successive states of the world are produced. Computational irreducibility gives time a certain rigid character, at least for computationally bounded observers like us. And the Principle of Computational Equivalence allows there to be a robust notion of time independent of the “substrate” that’s involved: whether us as observers, the everyday physical world, or, for that matter, the whole universe. </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't let dicts spoil your code (174 pts)]]></title>
            <link>https://roman.pt/posts/dont-let-dicts-spoil-your-code/</link>
            <guid>41781855</guid>
            <pubDate>Tue, 08 Oct 2024 21:10:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roman.pt/posts/dont-let-dicts-spoil-your-code/">https://roman.pt/posts/dont-let-dicts-spoil-your-code/</a>, See on <a href="https://news.ycombinator.com/item?id=41781855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>table of contents</p><nav id="TableOfContents"><ul><li><a href="#whats-wrong-with-dicts">What’s wrong with dicts?</a><ul><li><a href="#dicts-are-opaque">Dicts are opaque</a></li><li><a href="#dicts-are-mutable">Dicts are mutable</a></li></ul></li><li><a href="#treat-dicts-as-the-wire-format">Treat dicts as the wire format</a></li><li><a href="#streamline-model-creation">Streamline model creation</a><ul><li><a href="#create-models-with-dataclasses">Create models with dataclasses</a></li><li><a href="#alternatively-create-models-with-pydantic">Alternatively, create models with Pydantic</a></li></ul></li><li><a href="#for-legacy-codebase-annotate-dicts-as-typedict">For legacy codebase, annotate dicts as TypeDict</a></li><li><a href="#for-key-value-stores-annotate-dicts-as-mappings">For key-value stores, annotate dicts as mappings</a></li><li><a href="#take-dicts-under-control">Take dicts under control</a></li></ul></nav></div><figure><img src="https://roman.pt/posts/dont-let-dicts-spoil-your-code/splash_hued0d13a2034a04fbb4bcad55138bdafa_423992_1000x400_fill_q75_box_smart1.jpg" width="1000" alt="Balance"><figcaption>Photo by <a href="https://unsplash.com/@martinsanchez">Martin Sanchez</a></figcaption></figure><p>How often do your simple prototypes or ad-hoc scripts turn into fully-fledged applications?</p><p>The simplicity of organic code growth has a flip side: it becomes too hard to maintain. The proliferation of dicts as primary data structures is a clear signal of tech debt in your code. Fortunately, modern Python provides many viable alternatives to plain dicts.</p><h2 id="whats-wrong-with-dicts">What’s wrong with dicts?</h2><h3 id="dicts-are-opaque">Dicts are opaque</h3><p>Functions that accept dicts are a nightmare to extend and modify. Usually, to change the function that takes a dictionary, you must manually trace the calls back to the roots, where this dict was created. There is often more than one call path, and if a program grows without a plan, you’ll likely have discrepancies in the dict structures.</p><h3 id="dicts-are-mutable">Dicts are mutable</h3><p>Changing dict values to fit a specific workflow is tempting, and programmers often abuse this functionality. In-place mutations may have different names: pre-processing, populating, enriching, data massage, etc. The result is the same. This manipulation hinders the structure of your data and makes it dependent on the workflow of your application.</p><p>Not only do dicts allow you to change their data, but they also allow you to change the very structure of objects. You can add or delete fields or change their types at will. Resorting to this is the worst felony you can commit to your data.</p><h2 id="treat-dicts-as-the-wire-format">Treat dicts as the wire format</h2><p>A common source of dicts in the code is deserializing from JSON. For example, from a third-party API response.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>&gt;&gt;&gt;</span> requests<span>.</span>get(<span>"https://api.github.com/repos/imankulov/empty"</span>)<span>.</span>json()
</span></span><span><span>{<span>'id'</span>: <span>297081773</span>,
</span></span><span><span> <span>'node_id'</span>: <span>'MDEwOlJlcG9zaXRvcnkyOTcwODE3NzM='</span>,
</span></span><span><span> <span>'name'</span>: <span>'empty'</span>,
</span></span><span><span> <span>'full_name'</span>: <span>'imankulov/empty'</span>,
</span></span><span><span> <span>'private'</span>: <span>False</span>,
</span></span><span><span><span>...</span>
</span></span><span><span>}
</span></span></code></pre></div><p><em>A dict, returned from the API.</em></p><p>Make a habit of treating dicts as a “wire format” and convert them immediately to data structures providing semantics.</p><figure><img src="https://roman.pt/posts/dont-let-dicts-spoil-your-code/serializer-deserializer.png" alt="Use serializer and deserializer to convert between wire format and internal representation" width="600"><figcaption><p>Use serializer and deserializer to convert between wire format and internal representation</p></figcaption></figure><p>The implementation is straightforward.</p><ul><li>Define your domain models. A domain model is simply a class in your application.</li><li>Fetch and deserialize in the same step.</li></ul><p>In Domain-Driven Design (DDD), this pattern is known as the anti-corruption layer. On top of semantic clarity, domain models provide a natural layer that decouples the exterior architecture from your application’s business logic.</p><p>Two implementations of a function retrieving repository info from GitHub:</p><div><p><i data-feather="alert-octagon"></i>
Returning a dict</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>import</span> <span>requests</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>get_repo</span>(repo_name: <span>str</span>):
</span></span><span><span>    <span>"""Return repository info by its name."""</span>
</span></span><span><span>    <span>return</span> requests<span>.</span>get(<span>f</span><span>"https://api.github.com/repos/</span><span>{</span>repo_name<span>}</span><span>"</span>)<span>.</span>json()
</span></span></code></pre></div><p>The output of the function is opaque and needlessly verbose. The format is defined outside of your code.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>&gt;&gt;&gt;</span> get_repo(<span>"imankulov/empty"</span>)
</span></span><span><span>{<span>'id'</span>: <span>297081773</span>,
</span></span><span><span> <span>'node_id'</span>: <span>'MDEwOlJlcG9zaXRvcnkyOTcwODE3NzM='</span>,
</span></span><span><span> <span>'name'</span>: <span>'empty'</span>,
</span></span><span><span> <span>'full_name'</span>: <span>'imankulov/empty'</span>,
</span></span><span><span> <span>'private'</span>: <span>False</span>,
</span></span><span><span> <span># Dozens of lines with unnecessary attributes, URLs, etc.</span>
</span></span><span><span> <span># ...</span>
</span></span><span><span>}
</span></span></code></pre></div></div><div><p><i data-feather="check-square"></i>
Returning a domain model</p><div><pre tabindex="0"><code data-lang="python"><span><span>
</span></span><span><span><span>class</span> <span>GitHubRepo</span>:
</span></span><span><span>    <span>"""GitHub repository."""</span>
</span></span><span><span>    <span>def</span> __init__(self, owner: <span>str</span>, name: <span>str</span>, description: <span>str</span>):
</span></span><span><span>        self<span>.</span>owner <span>=</span> owner
</span></span><span><span>        self<span>.</span>name <span>=</span> name
</span></span><span><span>        self<span>.</span>description <span>=</span> description
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>full_name</span>(self) <span>-&gt;</span> <span>str</span>:
</span></span><span><span>        <span>"""Get the repository full name."""</span>
</span></span><span><span>        <span>return</span> <span>f</span><span>"</span><span>{</span>self<span>.</span>owner<span>}</span><span>/</span><span>{</span>self<span>.</span>name<span>}</span><span>"</span>
</span></span><span><span>
</span></span><span><span><span>def</span> <span>get_repo</span>(repo_name: <span>str</span>) <span>-&gt;</span> GitHubRepo:
</span></span><span><span>    <span>"""Return repository info by its name."""</span>
</span></span><span><span>    data <span>=</span> requests<span>.</span>get(<span>f</span><span>"https://api.github.com/repos/</span><span>{</span>repo_name<span>}</span><span>"</span>)<span>.</span>json()
</span></span><span><span>    <span>return</span> GitHubRepo(data[<span>"owner"</span>][<span>"login"</span>], data[<span>"name"</span>], data[<span>"description"</span>])
</span></span></code></pre></div><div><pre tabindex="0"><code data-lang="python"><span><span><span>&gt;&gt;&gt;</span> get_repo(<span>"imankulov/empty"</span>)
</span></span><span><span><span>&lt;</span>GitHubRepo at <span>0x103023520</span><span>&gt;</span>
</span></span></code></pre></div></div><p>While the example below has more code, this solution is better than the previous one if we maintain and extend the codebase.</p><p>Let’s see what the differences are.</p><ul><li>The data structure is clearly defined, and we can document it with as many details as necessary.</li><li>The class also has a method <code>full_name()</code> implementing some class-specific business logic. Unlike dicts, data models allow you to co-locate the code and data.</li><li>GitHub API’s dependency is isolated in the function <code>get_repo()</code>. The GitHubRepo object doesn’t need to know anything about the external API and how objects are created. This way, you can modify the deserializer independently from the model or add new ways of creating objects: from pytest fixtures, the GraphQL API, the local cache, etc.</li></ul><p>☝️ Ignore fields coming from the API if you don’t need them. Keep only those that you use.</p><p>In many cases, you can and should ignore most of the fields coming from the API, adding only the fields that the application uses. Not only duplicating the fields is a waste of time, but it also makes the class structure rigid, making it hard to adopt changes in the business logic or add support to the new version of the API. From the point of view of testing, fewer fields mean fewer headaches in instantiating the objects.</p><h2 id="streamline-model-creation">Streamline model creation</h2><p>Wrapping dicts require creating a lot of classes. You can simplify your work by employing a library that makes “better classes” for you.</p><h3 id="create-models-with-dataclasses">Create models with dataclasses</h3><p>Starting from version 3.7, Python provides <a href="https://docs.python.org/3/library/dataclasses.html">Data Classes</a>. The <code>dataclasses</code> module of the standard library provides a decorator and functions for automatically adding generated special methods such as <code>__init__()</code> and <code>__repr__()</code> to your classes. Therefore, you write less boilerplate code.</p><p>I use dataclasses for small projects or scripts where I don’t want to introduce extra dependencies. That’s how the <code>GitHubRepo</code> model looks like with dataclasses.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>dataclasses</span> <span>import</span> dataclass
</span></span><span><span>
</span></span><span><span><span>@dataclass</span>(frozen<span>=</span><span>True</span>)
</span></span><span><span><span>class</span> <span>GitHubRepo</span>:
</span></span><span><span>    <span>"""GitHub repository."""</span>
</span></span><span><span>    owner: <span>str</span>
</span></span><span><span>    name: <span>str</span>
</span></span><span><span>    description: <span>str</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>full_name</span>(self) <span>-&gt;</span> <span>str</span>:
</span></span><span><span>        <span>"""Get the repository full name."""</span>
</span></span><span><span>        <span>return</span> <span>f</span><span>"</span><span>{</span>self<span>.</span>owner<span>}</span><span>/</span><span>{</span>self<span>.</span>name<span>}</span><span>"</span>
</span></span></code></pre></div><p>When I create Data Classes, my Data Classes are almost always defined as frozen. Instead of modifying an object, I create a new instance with <code>dataclasses.replace()</code>. Read-only attributes bring peace of mind to a developer, reading and maintaining your code.</p><h3 id="alternatively-create-models-with-pydantic">Alternatively, create models with Pydantic</h3><p>Recently <a href="https://pydantic-docs.helpmanual.io/">Pydantic</a>, a third-party data-validation library became my go-to choice for model definition. Compared with dataclasses, they are much more powerful. I especially like their serializers and deserializers, automatic type conversions, and custom validators.</p><p>Serializers simplify storing records to external storage, for example, for caching. Type conversions are especially helpful when converting a complex hierarchical JSON to a hierarchy of objects. And validators are helpful for everything else.</p><p>With Pydantic, the same model can look like this.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>pydantic</span> <span>import</span> BaseModel
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>GitHubRepo</span>(BaseModel):
</span></span><span><span>    <span>"""GitHub repository."""</span>
</span></span><span><span>    owner: <span>str</span>
</span></span><span><span>    name: <span>str</span>
</span></span><span><span>    description: <span>str</span>
</span></span><span><span>
</span></span><span><span>    <span>class</span> <span>Config</span>:
</span></span><span><span>        frozen <span>=</span> <span>True</span>
</span></span><span><span>
</span></span><span><span>    <span>def</span> <span>full_name</span>(self) <span>-&gt;</span> <span>str</span>:
</span></span><span><span>        <span>"""Get the repository full name."""</span>
</span></span><span><span>        <span>return</span> <span>f</span><span>"</span><span>{</span>self<span>.</span>owner<span>}</span><span>/</span><span>{</span>self<span>.</span>name<span>}</span><span>"</span>
</span></span></code></pre></div><p>Online service <a href="https://jsontopydantic.com/">jsontopydantic.com</a> saves my time creating Pydantic models from third-party APIs. I copy the response examples from their documentation to the service, and the service returns Pydantic models.</p><figure><img src="https://roman.pt/posts/dont-let-dicts-spoil-your-code/jsontopydantic.png" alt="jsontopydantic.com converts Todoist API response to Pydantic model" width="800"><figcaption><p>jsontopydantic.com converts Todoist API response to a Pydantic model</p></figcaption></figure><p>You can find some examples of me using Pydantic, in the post <a href="https://roman.pt/posts/time-series-caching/">Time Series Caching with Python and Redis</a>.</p><h2 id="for-legacy-codebase-annotate-dicts-as-typedict">For legacy codebase, annotate dicts as TypeDict</h2><p>Python 3.8 introduced so-called <a href="https://www.python.org/dev/peps/pep-0589/">TypedDicts</a>. In runtime, they behave like regular dicts but provide extra information about their structure for developers, type validators, and IDEs.</p><p>If you come across the dict-heavy legacy code and can’t refactor everything yet, at least you can annotate your dicts as typed ones.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span>from</span> <span>typing</span> <span>import</span> TypedDict
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>class</span> <span>GitHubRepo</span>(TypedDict):
</span></span><span><span>    <span>"""GitHub repository."""</span>
</span></span><span><span>    owner: <span>str</span>
</span></span><span><span>    name: <span>str</span>
</span></span><span><span>    description: <span>str</span>
</span></span><span><span>
</span></span><span><span>repo: GitHubRepo <span>=</span> {
</span></span><span><span>    <span>"owner"</span>: <span>"imankulov"</span>,
</span></span><span><span>    <span>"name"</span>: <span>"empty"</span>,
</span></span><span><span>    <span>"description"</span>: <span>"An empty repository"</span>,
</span></span><span><span>}
</span></span></code></pre></div><p>Below, I provide two screenshots from PyCharm to show how adding typing information can streamline your development experience with the IDE and protect you against errors.</p><figure><img src="https://roman.pt/posts/dont-let-dicts-spoil-your-code/typed-dict-value-type.png" alt="PyCharm knows about the value type and provides autocomplete" width="650"><figcaption><p>PyCharm knows about the value type and provides autocomplete</p></figcaption></figure><figure><img src="https://roman.pt/posts/dont-let-dicts-spoil-your-code/typed-dict-missing-key.png" alt="PyCharm knows about the missing key and issues a warning" width="650"><figcaption><p>PyCharm knows about the missing key and issues a warning</p></figcaption></figure><h2 id="for-key-value-stores-annotate-dicts-as-mappings">For key-value stores, annotate dicts as mappings</h2><p>A legitimate use-case of dict is a key-value store where all the values have the same type, and keys are used to look up the value by key.</p><div><pre tabindex="0"><code data-lang="python"><span><span>colors <span>=</span> {
</span></span><span><span>    <span>"red"</span>: <span>"#FF0000"</span>,
</span></span><span><span>    <span>"pink"</span>: <span>"#FFC0CB"</span>,
</span></span><span><span>    <span>"purple"</span>: <span>"#800080"</span>,
</span></span><span><span>}
</span></span></code></pre></div><p><em>A dict, used as a mapping.</em></p><p>When instantiating or passing such dict to a function, consider hiding implementation details by annotating the variable type as Mapping or MutableMapping. On the one hand, it may sound like overkill. Dict is default and by far the most common implementation of a MutableMapping. On the other hand, by annotating a variable with mapping, you can specify the types for keys and values. Besides, in the case of a Mapping type, you send a clear message that an object is supposed to be immutable.</p><div><p>Example</p><p>I defined a color mapping and annotated a function. Notice how the function uses the operation allowed for dicts but disallowed for Mapping instances.</p><div><pre tabindex="0"><code data-lang="python"><span><span><span># file: colors.py</span>
</span></span><span><span><span>from</span> <span>typing</span> <span>import</span> Mapping
</span></span><span><span>
</span></span><span><span>colors: Mapping[<span>str</span>, <span>str</span>] <span>=</span> {
</span></span><span><span>    <span>"red"</span>: <span>"#FF0000"</span>,
</span></span><span><span>    <span>"pink"</span>: <span>"#FFC0CB"</span>,
</span></span><span><span>    <span>"purple"</span>: <span>"#800080"</span>,
</span></span><span><span>}
</span></span><span><span>
</span></span><span><span><span>def</span> <span>add_yellow</span>(colors: Mapping[<span>str</span>, <span>str</span>]):
</span></span><span><span>    colors[<span>"yellow"</span>] <span>=</span> <span>"#FFFF00"</span>
</span></span><span><span>
</span></span><span><span><span>if</span> __name__ <span>==</span> <span>"__main__"</span>:
</span></span><span><span>    add_yellow(colors)
</span></span><span><span>    <span>print</span>(colors)
</span></span></code></pre></div><p>Despite wrong types, no issues in runtime.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ python colors.py
</span></span><span><span><span>{</span><span>'red'</span>: <span>'#FF0000'</span>, <span>'pink'</span>: <span>'#FFC0CB'</span>, <span>'purple'</span>: <span>'#800080'</span>, <span>'yellow'</span>: <span>'#FFFF00'</span><span>}</span>
</span></span></code></pre></div><p>To check the validity, I can use mypy, which raises an error.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>$ mypy colors.py
</span></span><span><span>colors.py:11: error: Unsupported target <span>for</span> indexed assignment <span>(</span><span>"Mapping[str, str]"</span><span>)</span>
</span></span><span><span>Found <span>1</span> error in <span>1</span> file <span>(</span>checked <span>1</span> <span>source</span> file<span>)</span>
</span></span></code></pre></div></div><h2 id="take-dicts-under-control">Take dicts under control</h2><p>Keep an eye on your dicts. Don’t let them take control of your application. As with every piece of technical debt, the further you postpone introducing proper data structures, the more complex the transition becomes.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Practices of Reliable Software Design (194 pts)]]></title>
            <link>https://entropicthoughts.com/practices-of-reliable-software-design</link>
            <guid>41781777</guid>
            <pubDate>Tue, 08 Oct 2024 21:01:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://entropicthoughts.com/practices-of-reliable-software-design">https://entropicthoughts.com/practices-of-reliable-software-design</a>, See on <a href="https://news.ycombinator.com/item?id=41781777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org4458e36">The Practices</a>
<ul>
<li><a href="#orgc18eb36">1. Use Off-The-Shelf</a></li>
<li><a href="#orgc87cadf">2. Cost And Reliability Over Features</a></li>
<li><a href="#org6591e08">3. Idea To Production Quickly</a></li>
<li><a href="#orgf8ec978">4. Simple Data Structures</a></li>
<li><a href="#orgd69bbd5">5. Reserve Resources Early</a></li>
<li><a href="#org8c9d253">6. Set Maximums</a></li>
<li><a href="#orgdc1861b">7. Make Testing Easy</a></li>
<li><a href="#orgbf5c7d5">8. Embed Performance Counters</a></li>
</ul>
</li>
<li><a href="#org93252f2">Other practices</a></li>
</ul>
</div>
</div>

<p>
I was nerd-sniped. Out of the blue, a friend asked me,
</p>

<blockquote>
<p>
If you would build an in-memory cache, how would you do it?
</p>

<p>
It should have good performance and be able to hold many entries. Reads are more
common than writes. I know how I would do it already, but I’m curious about your
approach.
</p>
</blockquote>

<p>
I couldn’t not take the bait.
</p>

<p>
In the process of answering the question and writing the code, I discovered a
lot of things happened in my thought processes that have come with experience.
These are things that make software engineering easier, but that I know I
wouldn’t have considered when I was less experienced.
</p>

<p>
I started writing a long and sprawling article on it, but it didn’t quite hit
the right notes, so this is a much abbreviated version. Time allowing, I might
expand on some of these practices in separate articles that can be more focused
and concise.
</p>
<section id="outline-container-org4458e36">
<h2 id="org4458e36">The Practices</h2>
<p>
Here are all eight practices I have adopted with experience that I had use for
during the exercise of writing a fast, small, in-memory cache.
</p>
<div id="outline-container-orgc18eb36">
<h2 id="orgc18eb36">1. Use Off-The-Shelf</h2>
<div id="text-orgc18eb36">
<p>
Our first response to the question should be something like, <i>can we use
Redis?</i>
</p>

<p>
As long as we’re not dealing with a very expensive or complicated component,
or a part of the software that generates most of its value, we should default
to off-the-shelf solutions. I have written before about
<a href="https://entropicthoughts.com/build-vs-buy.html">why we want to build expensive and complicated
stuff</a>.
</p>
</div>
</div>
<div id="outline-container-orgc87cadf">
<h2 id="orgc87cadf">2. Cost And Reliability Over Features</h2>
<div id="text-orgc87cadf">
<p>
If we find out we can’t use an off-the-shelf solution, we need to build
something cheap and reliable. That usually means not having all the bells and
whistles, but that’s a trade-off worth making. One of my favourite ways of
phrasing this is
</p>

<blockquote>
<p>
It is much easier to add features to reliable software, than it is to add
reliability to featureful software.
</p>
</blockquote>

<p>
Besides, it’s very easy to accidentally think you need features that you
don’t actually need.
</p>

<p>
I’m not a big proponent of a design phase. Sometimes you need it, but I think
if you need to design your software up front, it will cost you more to
develop and it will take longer to finish. On the other hand, sometimes a
little up front analysis allow you to eliminate large portions of the design
space before even writing a single line of code.
</p>

<p>
In the case of this cache, we may ask question about item durability
requirements, request rates, sizes, eviction requirements, and so on. When we
do, we would in this case find out that we can get by with a single thread
accessing a single data structure, and no active eviction process. That’s a
huge win! It simplifies the design a lot.
</p>
</div>
</div>
<div id="outline-container-org6591e08">
<h2 id="org6591e08">3. Idea To Production Quickly</h2>
<div id="text-org6591e08">
<p>
Part of the reasoning of the previous practice was that it’s easy to think
you need features you don’t. How do you, then, know which features you need?
For the most part, the cheapest and most reliable way to find this out is in
production.
</p>

<p>
If we deploy the bare minimum feature set, we will very quickly find out what
additional features are most commonly requested, and it’s not going to be the
ones we think. And even if they are, there are usually other requirements on
them that we wouldn’t have caught ahead of time.
</p>

<p>
This practice ties in strongly with the previous one: use analysis to pare
down the requirements to the bare minimum, and then write the bare minimum
code to support them, and get it into production to find out more about the
problem we’re trying to solve.
</p>
</div>
</div>
<div id="outline-container-orgf8ec978">
<h2 id="orgf8ec978">4. Simple Data Structures</h2>
<div id="text-orgf8ec978">
<p>
Complicated data structures are often tempting. Especially when there are
libraries that handle them for us. The problem with complicated data
structures is that it’s easy to misuse them due to a lack of understanding,
which leads to performance problems and bugs.
</p>

<p>
In the case of this specific cache, the number of entries that needed to be
stored (we can use queueing theory to calculate this from the <abbr>ttl</abbr> and the
rate of writes) would comfortably be addressed by 19.9 bits of information –
in other words, we can just use a plain array to store each item, and hash
the key to address it.<label for="fn.1">1</label><span><sup>1</sup> We found out earlier we didn’t need to actually
evict expired items. If we would have needed to, we could keep a separate
index for the item nearest to expiry using a min-heap. This is cheaper and
simpler than something fully sorted, which it might be tempting to reach for
otherwise. Instead of a separate process, we can rely on the high request
rate to trigger eviction of items, thus trading a little latency to avoid
designing for concurrency up front. Another alternative is to provide a
separate API that triggers eviction of the oldest expired entry, letting the
caller decide how much time to spend on eviction. This means the caller can
vary that time to make dynamic latency–resource requirement tradeoffs.</span>
</p>
</div>
</div>
<div id="outline-container-orgd69bbd5">
<h2 id="orgd69bbd5">5. Reserve Resources Early</h2>
<div id="text-orgd69bbd5">
<p>
Another potentially controversial decision I made for the cache I designed
was that it allocated the full index array up front. This sounds wasteful,
but we can tell from back-of-the-napkin analysis that it would need most of
this space anyway during continuous operation, so nothing would be saved by
postponing the allocation.
</p>

<p>
What early allocation does is it allows the software to crash early if the
required resources are not available. This is a desirable trait – it saves
the operator the frustration of finding that out only once they’ve gone to
bed and receive a call from PagerDuty. It also makes it easier to do capacity
planning and reason about performance.
</p>


</div>
</div>
<div id="outline-container-org8c9d253">
<h2 id="org8c9d253">6. Set Maximums</h2>
<div id="text-org8c9d253">
<p>
I chose a sloppy open-addressing-with-linear-probing-based method for
handling hash collisions in addressing items that is simple and has very good
performance in the typical case (again, only very rudimentary arithmetic
needed to find this out for any specific use case), but can have very bad
performance in the worst case: it can end up iterating through the entire
array of all items on every cache miss.
</p>

<p>
Since perfect recall turned out not to be necessary<label for="fn.2">2</label><span><sup>2</sup> This is again why
asking questions about requirements is so important!</span>, it was trivial to set
a maximum iteration limit of e.g. 500 steps. This makes sure the worst case
is not too far from the average case, at the cost of a few more cache misses.
Does 500 steps optimise this tradeoff? No. This is one thing I don’t know how
to compute ahead of time, so production experience will have to inform future
changes to the maximum number of steps.
</p>

<p>
The important thing is usually not what the maximum is, just that there is
<i>some</i> maximum, so we’re not waiting surprisingly long times for things to
complete.
</p>

<p>
Given the static allocation requirements, we might also want to set a limit
on how much data can be stored in the cache at any given time, so we don’t
accidentally blow memory limits in the middle of the night. In general, limit
all the things! If someone is unhappy with the limit, revise the limit –
don’t remove it.
</p>
</div>
</div>
<div id="outline-container-orgdc1861b">
<h2 id="orgdc1861b">7. Make Testing Easy</h2>
<div id="text-orgdc1861b">
<p>
To avoid regressions and ensure expected behaviour, I made the cache accept
commands on standard input. This means we can start the cache and type <code>write
   hello world</code> in the terminal window to store <code>"world"</code> for the key <code>"hello"</code>.
</p>

<p>
But it also means we can write up a long string of commands in a file, and
pipe that into the cache. If we then also give the cache a command that
asserts the last value read is something specific<label for="fn.3">3</label><span><sup>3</sup> It also had a few more
commands related to testing, such as a sleep command that forces it to
believe time has passed (for testing expiration) and a dump command that just
prints out everything it is currently storing, and whether it’s deleted,
expired, or active.</span>, we can write up a whole test protocol in a plain text
file, and pipe it to the program to verify its functionality. The input file
might look like
</p>

<p><label>In[1]:</label></p><div>
<pre><span># </span><span>Cache empty, should not find anything.</span>
read abc
assert undef

<span># </span><span>Write and then immediately read the same thing back.</span>
write abc 123
read abc
assert 123

<span># </span><span>Different key should get nothing.</span>
read xyz
assert undef

<span># </span><span>Write different value, then read, should match.</span>
write abc 567
read abc
assert 567

<span># </span><span>Write one more key, abc should still be around.</span>
write xyz 444
read abc
assert 567
</pre>
</div>

<p>
In isolation, each of these things is very simple (accept commands at <abbr>cli</abbr>,
allow asserting previous read) but it’s the combination of them, together with
the tools that already exist at hand (shell input redirection) that enables
faster cycle times and thus more reliable software.
</p>
</div>
</div>
<div id="outline-container-orgbf5c7d5">
<h2 id="orgbf5c7d5">8. Embed Performance Counters</h2>
<div id="text-orgbf5c7d5">
<p>
Finally, we will want to know how our program spends its time. We can do that
with profiling, or deduce it through logs, but the easiest way to do it is to
embed performance counters. These are variables that accumulate how much of
something is happening. It can be things like
</p>

<ul>
<li>How much time is spent reading keys?</li>
<li>How much time is spent writing key–value pairs?</li>
<li>How much time is spent on I/O?</li>
<li>How many cache misses have there been?</li>
<li>How many keys have we had to linearly search over?</li>
<li>How many times has the iteration limit of 500 been reached?</li>
</ul>

<p>
Note that all of these are accumulative, so we don’t ask “how much storage
space is currently allocated?”, but we ask instead the pair of questions
</p>

<ul>
<li>How much storage space have we allocated in total from startup until now?</li>
<li>How much storage space have we freed in total from startup until now?</li>
</ul>

<p>
By breaking the measurement apart into two time-dependent totals we can learn
significantly more about how our system behaves.
</p>

<p>
How these numbers evolve over time is also a useful indicator. For example,
the value of “how many keys have we had to linearly search over?” might
increase steadily (indicating relatively even distribution of hash
collisions) or it jumps up a lot sometimes (indicating sudden burst of
collisions). These are useful things to be able to tell apart. In combination
with “How many times has the iteration limit been reached” it tells us a lot
of the pressures put on the system.
</p>
</div>
</div>
</section>
<section id="outline-container-org93252f2">
<h2 id="org93252f2">Other practices</h2>
<p>
There are surely many more of these insights gained from years of software
engineering, but these are the ones I thought of during this exercise. I’d be
happy to hear from you about others you are thinking about! If nothing else, I
hope that could teach me to be a better software engineer.
</p>
</section>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A modest critique of Htmx (288 pts)]]></title>
            <link>https://chrisdone.com/posts/htmx-critique/</link>
            <guid>41781457</guid>
            <pubDate>Tue, 08 Oct 2024 20:19:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chrisdone.com/posts/htmx-critique/">https://chrisdone.com/posts/htmx-critique/</a>, See on <a href="https://news.ycombinator.com/item?id=41781457">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>At work, we really like the basic simple idea of Htmx. Based on using
Htmx in a non-trivial user interface, across a team, we’ve found that
the following cases are actually not simple and are quite
complicated.</p>
<h2 id="inheritance-of-htmx-properties-is-a-definite-mistake">Inheritance of
Htmx properties is a definite mistake</h2>
<p>Across pieces of code, it’s very surprising and it’s implicit. Like
in CSS, inheritance is a cheap hack but you get what you pay for.</p>
<p>It contradicts <a href="https://htmx.org/essays/locality-of-behaviour/">the author’s
reasonable argument of locality of behaviour.</a> It’s not local, it
comes from all the way up there or some other module. Pretty much
dynamic binding.</p>
<p>Default inheritance differs across various properties
(e.g.&nbsp;<code>hx-delete</code> is not inherited, but
<code>hx-confirm</code> and <code>hx-ext</code> are). So you have to
remember these exceptions and you end up just being explicit about
everything, which means inheritance is pointless.</p>
<h2 id="most-interesting-web-apps-cannot-replace-wholesale-a-dom-element">Most
interesting web apps cannot replace wholesale a DOM element</h2>
<p>Because DOM elements almost always have browser-local state, such as
the open/closed state of a <code>&lt;details&gt;</code> element, the
input of an <code>&lt;input&gt;</code> element, the open/close state of
a dropdown element (which, note, is not encoded by an attribute of the
element when you click it). All of this state is lost if you replace
outerHTML directly with the naive happy path of Htmx.</p>
<p>Even <a href="https://v1.htmx.org/extensions/morphdom-swap/">morphdom</a>
overwrites some things you’d expect it not to, so we had to patch it to
avoid messing with input elements, and details elements.</p>
<h2 id="storing-state-in-the-dom-element-itself-is-a-bad-idea">Storing
state in the DOM element itself is a bad idea</h2>
<p><a href="https://v1.htmx.org/extensions/morphdom-swap/">Morphdom</a>
is intended to correct the pains of the previous heading, but we
discovered that the way that Htmx works <em>assumes</em> it’s based on
replacing elements wholesale: it stores the request queue for the
element on the DOM element itself. When you kick off a request, either
from this element or from another that points to it, you have a request
queue. Some bad failure modes are avoided by wholesale-replacing the DOM
element, as the queue is reset. But with morphdom, the queue is retained
because the element is retained. You’re now in a sort of <a href="https://en.wikipedia.org/wiki/Undefined_behavior">undefined
behavior land</a>, where the designs of Htmx are violated.</p>
<h2 id="the-default-queuing-mode-is-bonkers">The default queuing mode is
bonkers</h2>
<p>By default, Htmx will cancel requests that are in-flight if you
trigger another request on the same queue (element). That’s the default
strategy. We discovered this afterwards. It’s highly unintuitive, it
meant we were losing work.</p>
<h2 id="event-triggers-are-non-local">Event triggers are non-local</h2>
<p>Event triggers often help to make things happen, but they’re a
non-local effect, and suffer from similar issues as property
inheritance. A bit of DSL work in the server-side language can help with
this, but it feels like old-school JavaScript callback-based programming
to some extent; where you “subscribe” to an event happening and do
something.</p>
<h2 id="component-state-cannot-be-maintained-very-well">Component state
cannot be maintained very well</h2>
<p>A broader problem, similar to the DOM element state issue, is that
your own components have their own state. E.g. if you want a page that
consists of three sections that have their own state that the server
needs (e.g.&nbsp;which page of a set of results) and state that some
e.g.&nbsp;React or WebComponents need, then you have a problem of
synchronising state between a parent component and the child
component.</p>
<p>Htmx does not provide a good story for this. We have some ideas, but
they all have big caveats: use query parameters, use hidden form inputs,
use event triggers.</p>
<p>React and Halogen (see also <a href="https://chrisdone.com/posts/halogen-is-better-than-react/">Halogen
is better than React at everything</a>) <em>do</em> have an answer to
this. In both cases, child components have their own state, and parents
can give them “props” which are pretty much “advice”, and they also have
their own internal state, and can choose to ignore/take precedence over
props. The props are typically sourced from the server or derived from
the server, and the state is usually some client-side state.</p>
<p>We often do need to use React for off-the-shelf components or
components that we have to use that are just provided as React. React
and Htmx do not interact nicely.</p>
<ul>
<li>We’ve done some unsatisfying work with WebComponents, but those
things have bizarre limitations that are surprising.</li>
<li>We’ve also made a bridge directly to React components that we use
from our server-side language, but in general Htmx and React fight for
control over the flow of state and management of DOM elements.</li>
<li>We’ve played with Alpine, which is nice, but it represents <em>yet
another</em> client-side-programming library and is therefore redundant
if React is already in your codebase.</li>
</ul>
<h2 id="the-up-sides">The up sides</h2>
<p>Our current thinking is that being able to use your server side
language is <strong>a huge obvious and uncontroversial win</strong>, no
one on the team would want to go back to writing all this business logic
in TypeScript:</p>
<ul>
<li>No serialisation from our DB types to frontend types is needed.
<ul>
<li>No data leaks, and no GraphQL needed.</li>
</ul></li>
<li>We can use our (in our opinion) more powerful abstraction facilities
of the server-side language.</li>
<li>We can use the form builder in our server side language; instead of
doing one frontend <em>and also</em> one backend implementation of the
same validations.</li>
</ul>
<p>But the above downsides are real.</p>
<h2 id="htmx-in-react">Htmx-in-React?</h2>
<p>An attractive future direction might be to re-implement Htmx in
React:</p>
<ul>
<li>The server sends a JSON blob that React converts into virtual DOM
components.</li>
<li>That would solve the component state problem.</li>
<li>It would mean we require no special bridge to use React
components.</li>
<li>It would let us use our React-connected web fetching library, and
carefully avoid the queuing choices made by Htmx.</li>
<li>It would solve the morphdom problems and browser DOM input elements
problem, too, which is pretty much a solved problem in React.</li>
</ul>
<p>In this way, we could drop the Htmx dependency but retain the
benefits of the idea. That is, given a budget to embark on such a big
piece of work.</p>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lunar Lake's iGPU: Debut of Intel's Xe2 Architecture (122 pts)]]></title>
            <link>https://chipsandcheese.com/p/lunar-lakes-igpu-debut-of-intels</link>
            <guid>41780929</guid>
            <pubDate>Tue, 08 Oct 2024 19:32:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/lunar-lakes-igpu-debut-of-intels">https://chipsandcheese.com/p/lunar-lakes-igpu-debut-of-intels</a>, See on <a href="https://news.ycombinator.com/item?id=41780929">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Intel has a long history of making integrated GPUs, and they’ve recognized how important iGPUs are to thin and light laptops. Today Intel is locked in fierce competition with AMD, and the company’s fight to hold on to the laptop market is no less intense on the graphics front. Lunar Lake is Intel’s latest mobile offering, and brings with it a new Xe2 graphics architecture. Xe2 is an evolution of the Xe-LPG/HPG architectures used on the outgoing Meteor Lake mobile chips and Intel’s current Arc discrete GPUs. It aims to better fit expected workloads and improve efficiency, and will be used across both Lunar Lake and Intel’s upcoming “Battlemage” discrete GPUs.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp" width="768" height="430" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:430,&quot;width&quot;:768,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:35814,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42bb6a08-4a46-4b70-bfae-77a5d912d1ac_768x430.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Here I’ll be looking at the Arc 140V iGPU in the Core Ultra 7 258V. ASUS has kindly provided a Lunar Lake laptop for testing. Without their support, this article would not be possible.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png" width="1052" height="269" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:269,&quot;width&quot;:1052,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:56251,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F40f2ae79-a1b5-4355-a644-0feb43aec6a3_1052x269.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>For comparisons I’ll be using the iGPU in Intel’s outgoing Meteor Lake, as well as the RDNA 3.5 iGPU in AMD’s competing Strix Point.</p><p>Modern GPUs are modular, scalable designs and Intel’s Xe2 is no different. An Xe2 iGPU is built from Xe Cores, much like how AMD’s RDNA GPUs are built from WGPs (Workgroup Processors) or how Nvidia’s GPUs are built from SMs (Streaming Multiprocessors). Lunar Lake’s iGPU features eight Xe Cores, further divided into two Render Slices with four Xe Cores each. These Render Slices include fixed function graphics hardware like a rasterizer and pixel backends (ROPs). Lunar Lake’s iGPU is therefore laid out similarly to Meteor Lake’s iGPU, which also has eight Xe Cores split across two Render Slices.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp" width="729" height="731" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:731,&quot;width&quot;:729,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:10898,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bbb01b2-8c18-490a-b5e3-3b330a4f127f_729x731.webp 1456w" sizes="100vw"></picture></div></a></figure></div><p>Intel’s Render Slice vaguely corresponds to AMD’s Shader Array, which also includes a rasterizer and ROPs. AMD also has a mid-level L1 cache at the Shader Array level, with 256 KB of capacity on RDNA 3.5. Intel also has caches at the Render Slice level, though only for fixed function graphics hardware. The figure below should be taken with a grain of salt because Intel has not published documentation for Xe-LPG, so I’m taking figures from the closely related Xe-HPG. I also find Intel’s documentation confusing. For example, each pixel backend should have its own color cache, but something changed after Tiger Lake and Intel only documents one color cache instance per slice despite drawing it twice in other diagrams.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png" width="1195" height="355" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:355,&quot;width&quot;:1195,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:22359,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb42decec-882f-4e39-b214-f2ec6ef56ce8_1195x355.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Xe2 aims to improve efficiency for fixed function units, and part of that involves better caching. The hierarchical Z cache helps reject occluded geometry before it gets past the rasterizer by remembering how far triangles are from the camera. Its capacity increases by 50% on Xe2, from 4 to 6 KB. At the other end of the pipeline, color cache capacity has increased by 33%. Xe-HPG had 16 KB of color cache capacity in each slice, so a 33% increase doesn’t make so much sense. Perhaps Xe-LPG had a larger 24 KB color cache. In that case, a 33% increase would bring capacity to 32 KB.</p><p>Xe Cores are the basic building block of Intel’s GPUs, and are further divided into Vector Engines that have register files and associated execution units. Xe2 retains the same general Xe Core structure and compute throughput, but reorganizes the Vector Engines to have longer native vector widths. Pairs of 8-wide Vector Engines from Meteor Lake have been merged into 16-wide Vector Engines. Lunar Lake’s Xe Core therefore has half as many Vector Engines, even though per-clock FP32 vector throughput hasn’t changed.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp" width="768" height="618" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:618,&quot;width&quot;:768,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:72518,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536e5824-c38e-45dd-a9d2-440424b1bbd3_768x618.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Intel here is completing a transition aimed at reducing instruction control overhead that began with prior generations. Longer vector widths improve efficiency because the GPU can feed more math operations for a given amount of instruction control overhead. Meteor Lake’s Xe-LPG already tackled instruction control costs by using one instance of thread/instruction control logic for a pair of adjacent vector engines.</p><p>But using less control logic makes the GPU more vulnerable to branch divergence penalties. That applied in funny ways to Xe-LPG, because sharing control logic forced pairs of Vector Engines to run in lockstep. A Vector Engine could sit idle if its partner had to go down a different execution path.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png" width="1456" height="447" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:447,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:48865,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3746f6d2-5ec6-4c77-aa95-f407669dc9f0_1593x489.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Because there wasn’t a lot of point in keeping the Vector Engines separate, Intel merged them. The merge makes divergence penalties straightforward too, since each Vector Engine once again has its own thread and instruction control logic. Meteor Lake could do better in corner cases, like if groups of 16 threads take the same path. But that’s an awfully specific pattern to take advantage of, and Xe2’s divergence behavior is more intuitive. Divergence penalties disappear once groups of 32 threads or more take the same path.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png" width="837" height="477" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:477,&quot;width&quot;:837,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:15517,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4fa61b3e-bea9-4bad-aa11-93d75c67ad9b_837x477.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Even though Intel’s execution unit partitions have gotten bigger, they’re still small compared to AMD’s. RDNA 3.5’s SIMDs are equivalent to Intel’s Vector Engines, as both blocks have a register file feeding a set of execution units. AMD’s SIMDs can do 64 FP32 operations per cycle compared to 16 on Intel’s Vector Engines. However, comparisons are complicated because AMD has to use wave64 mode or compiler magic in wave32 mode to feed all of its FP32 lanes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp" width="398" height="765" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:765,&quot;width&quot;:398,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6574,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/webp&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12a9f2a6-d3d2-4b4c-933d-ec0d3618e154_398x765.webp 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Lunar Lake’s iGPU offers very similar throughput compared to the prior generation, which isn’t a surprise because both have eight Xe Cores. The Core Ultra 7 155H (Meteor Lake) has a slightly higher 2.25 GHz maximum GPU clock than the Core Ultra 7 258V at 1.95 GHz. Sometimes Xe2’s architectural improvements make feeding the execution units easier, so some tests show improvement.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png" width="688" height="922" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:922,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0357c0-9bc3-42ad-807c-ac1399468ed9_688x922.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Meteor Lake’s iGPU didn’t support FP64 through Vulkan</figcaption></figure></div><p>Still, AMD’s Strix Point has a huge compute throughput advantage for common FP32 operations. AMD’s WGPs have twice as much per-cycle throughput as Intel’s Xe Cores, and AMD can clock their iGPU up to 2.9 GHz. Even if dual issue limitations prevent Strix Point from feeding all its execution units, AMD has a significant compute throughput advantage.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png" width="688" height="616" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:616,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d50e578-7b89-4aa4-b6b9-9587f37a3cb8_688x616.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Intel can catch up in some less common integer operations like INT32 multiplies. 64-bit integer performance has dramatically improved on Lunar Lake compared to Meteor Lake, but it’s not enough to catch AMD.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png" width="688" height="352" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:352,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92d14260-66ad-4915-90d9-9a6ba14c19a2_688x352.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Testing with OpenCL because Meteor Lake supports FP64 through that</figcaption></figure></div><p>FP64 is another area where Intel catches AMD. Lunar Lake seems to have doubled FP64 throughput compared to its predecessor. None of these GPUs are good at FP64 compute, but Intel is ahead now.</p><p>Matrix units are back in Lunar Lake after being absent in Meteor Lake’s iGPU. Each Vector Engine gets a 2048-bit XMX unit, offering four times as much throughput as the 512-bit vector execution units for the same data types.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg" width="1456" height="818" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:818,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:237582,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F219821d8-d428-478d-8b75-5b4b45d8d0a3_1827x1027.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>In addition, the XMX units support lower precision data types all the way down to INT2. From Intel’s </span><a href="https://youtu.be/-2yxPal4wQI?si=nN8J2CogtT0hDARN&amp;t=400" rel="">video</a><span>, they appear to work in SIMD16 mode on a 4×4 matrix with INT8 precision. Per cycle, a lane computes results for a 4-long row of the result matrix. Higher precision data types like FP16 are handled by operating on smaller chunks at a time. AMD RDNA 3.5 has WMMA instructions for matrix multiplication. WMMA operates on larger 16x16x16 matrices, and only goes down to INT4 precision. Unlike Intel, AMD doesn’t have dedicated matrix multiplication units, and executes WMMA instructions on dot product units.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg" width="1456" height="819" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:190283,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F98c2036b-7693-47c1-a9bf-55a48db1ec27_2560x1440.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I’m glad to see XMX units show up in Lunar Lake. Matrix multiplication units are often used for upscaling, and upscaling is arguably more crucial in laptops than with discrete GPUs. Laptop GPUs often struggle to deliver playable performance in recent games, even with low settings and resolutions, so upscaling can make the difference between a game being playable or not. For discrete GPUs, upscaling simply allows higher quality settings.</p><p>Caches remain largely unchanged within a Xe Core. A 192 KB L1 cache serves double duty as local memory, which Intel calls Shared Local Memory (SLM). Higher clock speed means Meteor Lake’s iGPU has slightly better L1 latency, though multiplying latency by clock speed shows cycle count is pretty much the same.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png" width="908" height="312" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:312,&quot;width&quot;:908,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:46926,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37125ac4-03c5-4059-bc1b-9b962ef0ebe8_908x312.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Strangely, Lunar Lake seems to be stuck as if local memory were always allocated, so only 64 KB of L1 cache is visible from a latency test. Meteor Lake couldn’t use the whole 192 KB of L1 for caching like the Intel’s discrete A770, but it could still use 160 KB for caching. Meteor Lake would only drop to 64 KB if a kernel needed local memory. AMD uses a separate 128 KB Local Data Share (LDS) for local memory, so local memory allocations don’t affect AMD’s caching capacity.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png" width="1282" height="649" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:649,&quot;width&quot;:1282,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:38228,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8ce9487-abec-4bb3-876a-7b0fb3c301a6_1282x649.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>If programs do want local memory, Lunar Lake is more efficient with utilizing L1/SLM capacity. Lunar Lake can have threads allocating up to 1 MB of local memory executing concurrently on the GPU, which would correspond to 128 KB per Xe Core. Meteor Lake got stuck at 768 KB, so each Xe Core was only able to allocate 96 KB of local memory.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png" width="1065" height="564" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:564,&quot;width&quot;:1065,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:29987,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F87921cf9-7de2-4a9f-a031-b6d352c1adbe_1065x564.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Xe Cores have a texture cache as well, probably with 32 KB of capacity. Latency is higher because accesses have to go through the texture units, which can do all sorts of texture sampling operations in addition to just grabbing data from memory. Here I’m hitting the texture cache with OpenCL’s image1d_buffer_t data type, and not using any texture sampling options.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png" width="1456" height="688" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:688,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:39867,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe2f01e58-bb10-4494-87d0-d1d285c0c585_1548x732.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Again Lunar Lake behaves just like Meteor Lake with a little extra latency, thanks to lower clock speeds. AMD does not have a separate texture cache, and feeds texture units using the vector cache. Vector cache latency on AMD is actually better when the texture units handle address generation.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png" width="1334" height="640" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1334,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:41173,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfb48b62-cf29-4c53-8cd0-b2473498b349_1334x640.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Outside the Xe Core, Lunar Lake doubles L2 cache capacity to 8 MB. That’s the biggest L2 cache I’ve seen in a laptop iGPU so far. However, latency has substantially increased over Meteor Lake’s 4 MB L2. Lunar Lake also sees a weird uptick in latency well before the test approaches L2 cache capacity. It’s not address translation latency because testing with a 4 KB stride didn’t show a latency jump until 64 MB, implying a single thread has enough TLB capacity on hand to cover 64 MB.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png" width="1198" height="580" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:580,&quot;width&quot;:1198,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:31943,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ca5b728-d56e-4212-a542-8661808c207e_1198x580.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To investigate further, I split the latency test array into multiple sections. Each thread gets placed into a separate workgroup, and traverses its own portion of the array. I can’t influence which Xe Core handles each thread, but GPU schedulers usually try to evenly balance threads across GPU cores. Eight threads see eight times as much L1D capacity, indicating I have one thread per Xe Core. At L2, using more threads pushes out the latency jump. Perhaps Lunar Lake’s L2 is split into multiple sections, and a Xe Core may have lower L2 access latency if it accesses a closer section.</p><p>AMD’s Strix Point has a smaller but lower latency L2 cache. Even though the L2 is smaller, Strix Point has a lower latency penalty for accessing DRAM. On Lunar Lake and Meteor Lake, a GPU-side DRAM access has over 400 ns of latency. Strix Point’s iGPU can get data from DRAM in under 250 ns. Lunar Lake has a 8 MB memory side cache placed in front of the memory controller, but I can’t clearly measure its performance characteristics.</p><p>Atomic operations can exchange data between threads running on a GPU. And while such operations are comparatively rare from what I can see, it’s still fun to run a core-to-core latency test of sorts on a GPU. Exchanging data through global memory sees Lunar Lake improve slightly over Meteor Lake.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png" width="677" height="390" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:390,&quot;width&quot;:677,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba073db2-f8f3-402f-bf8d-9207d888d982_677x390.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Doing the same through local memory means that data exchange can happen within an Xe Core, or the equivalent structure on other GPUs. There, AMD’s very fast Local Data Share continues to take the top spot. However, Lunar Lake has noticeably improved over its predecessor.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png" width="678" height="392" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:392,&quot;width&quot;:678,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93a3886a-1d22-4ec8-85c5-7a17d57838e3_678x392.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Assuming there’s enough work in flight to hide latency, GPUs can demand a lot of bandwidth from their cache hierarchy. Lunar Lake’s first level caches provide similar bandwidth compared to the outgoing Meteor Lake generation, while AMD’s Strix Point has a massive bandwidth lead. That’s because AMD’s WGPs have two 32 KB vector cache instances, each capable of satisfying a wave32 (1024-bit) vector load per cycle.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png" width="604" height="344" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:344,&quot;width&quot;:604,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ccc773b-9e06-4b64-acdd-e0861aca7f31_604x344.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Local memory shows similar behavior. AMD backs local memory with a 128 KB LDS. Since RDNA, that’s been built from two 64 KB arrays each capable of servicing a wave32 load every cycle. Thus AMD’s local memory and vector caches enjoy the same staggeringly high bandwidth. Again, it’s much higher than what Intel’s iGPUs can offer.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png" width="667" height="382" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:382,&quot;width&quot;:667,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0973bae9-df8c-404a-99f5-c739b0e5ddc5_667x382.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Testing GPU cache and memory bandwidth from a single test can be tricky, and Intel’s latest drivers seem to defeat Nemes’s Vulkan benchmark. Therefore I’m using my OpenCL bandwidth test, which uses the same general methodology.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png" width="1392" height="711" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/17595e68-407f-4723-b7d8-ece280d19413_1392x711.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:711,&quot;width&quot;:1392,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:84229,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17595e68-407f-4723-b7d8-ece280d19413_1392x711.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Lunar Lake and Meteor Lake appear to have similar L2 bandwidth. AMD continues to have a huge L2 bandwidth lead.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png" width="674" height="392" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:392,&quot;width&quot;:674,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe9fb8fae-9bb0-49a8-ad51-fe7580b06d3a_674x392.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>All three iGPUs tested here have very high DRAM bandwidth compared to those from the DDR4 generation. I’m not sure why Lunar Lake can’t quite match Strix Point or Meteor Lake despite using faster LPDDR5X, but I wouldn’t put too much weight into it because bandwidth testing is hard. I intend to write a different test specifically to target GPU DRAM bandwidth at some point, but time is short.</p><p><span>Integrated GPUs share a memory controller with the CPU. Often this can be a disadvantage because CPUs and GPUs are more sensitive to different memory subsystem characteristics, and iGPUs have to be budget friendly too. But iGPUs do have an advantage in being able to exchange data with the CPU faster. Normally I use OpenCL’s </span><code>clEnqueueWriteBuffer</code><span> and </span><code>clEnqueueReadBuffer</code><span> functions to test PCIe link bandwidth, but here, the test is just hitting the DMA engines.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png" width="1134" height="552" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:552,&quot;width&quot;:1134,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:38554,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d4a618d-502a-439b-9f70-5d8b28c3844a_1134x552.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Lunar Lake seems to have weaker DMA engines than its predecessor and AMD’s Strix Point. They come nowhere near saturating memory bandwidth, and don’t offer comparable performance to say, a PCIe 4.0 x16 link.</p><p>Intel has scaled up the hardware raytracing unit (RTU) in each Xe Core, giving it three traversal pipelines for handling raytracing BVH-es. Each pipeline can do 6 box tests per cycle for a total of 18 across the RTU. For intersection testing at the bottom level of the BVH, Intel can do two triangle tests per cycle to see if the ray hits any geometry.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png" width="1068" height="208" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:208,&quot;width&quot;:1068,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:20802,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f648fc7-663e-456a-ac5f-e8f4582022c9_1068x208.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>I couldn’t find much on Intel’s prior raytracing implementation, though the Xe-HPG whitepaper does say its RTUs support “a 12-to-1 ratio of ray-box intersection tests per clock to ray-triangle tests per clock”. The prior generation may have had two traversal pipelines per RTU, each capable of 6 box tests per clock.</p><p>Meteor Lake already had an aggressive ray tracing implementation, with RTUs autonomously handling ray traversal until a hit or miss is discovered. It performed well in 3DMark’s Solar Bay raytracing benchmark against AMD’s GPUs. Lunar Lake improves over its predecessor, though it’s hard to compete against AMD scaling up its GPU and showing up at much higher clock speeds.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png" width="688" height="393" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ce084829-e370-4958-9646-f8f5770b1052_688x393.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:393,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce084829-e370-4958-9646-f8f5770b1052_688x393.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>3DMark’s Solar Bay test also displays average framerates. Laptop iGPUs do quite well because Solar Bay is aimed at tablets and cell phones. Its test scene is very limited, and its raytracing effects are too.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png" width="864" height="483" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e3882768-36ca-4a6a-beb2-56765c88b560_864x483.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:864,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:17302,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe3882768-36ca-4a6a-beb2-56765c88b560_864x483.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Unlike Solar Bay, Cyberpunk 2077 has far more complex scenes and long view distances. At 1080P and low settings, mobile GPUs just barely manage playable framerates. Lunar Lake posts a huge performance increase over its predecessor, and manages to slip past AMD’s outgoing Phoenix APU in Cyberpunk 2077’s built in benchmark. But framerates are still low enough that I’d rather not lower it by adding extra eye candy. That includes raytracing.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png" width="795" height="422" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:422,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:14249,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc3539997-0ef5-4d9e-90e6-8871a87fd87c_795x422.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Even though Lunar Lake manages significant improvements, AMD’s current generation Strix Point APU is still ahead. Part of this is because Strix Point was able to pull more power. HWInfo showed the Lunar Lake laptop pulling an average of 43W from the battery, while the Strix Point laptop averaged 50W.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png" width="1112" height="520" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:520,&quot;width&quot;:1112,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:23058,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6f231b2-4ef4-4421-97f2-57a1e56310ac_1112x520.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Lunar Lake’s huge performance gain over Meteor Lake may be down to power too. Microbenchmarking shows similar compute performance across both Intel generations. But Meteor Lake’s CPU cores pull more power, leaving less power available for the GPU. HWInfo couldn’t read GPU frequency for Lunar Lake, but Meteor Lake’s iGPU often ran below 1.6 GHz. I suspect Lunar Lake’s iGPU ran closer to its maximum 1.95 GHz clock.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png" width="671" height="977" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:977,&quot;width&quot;:671,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc230432f-3653-4471-b20c-cbe679f1b6ab_671x977.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>CPU power counters may not provide the best accuracy, as they only have to be accurate enough to prevent hardware damage. Figures above likely have a generous margin of error. But I think they show how improved architecture and better process nodes combine to let Lunar Lake outperform Meteor Lake in a power constrained laptop.</p><p>Caching is another power optimization, as data transfer often contributes significantly to platform power consumption. Lunar Lake’s higher caching capacity means it can outperform AMD’s Phoenix APU while using less memory bandwidth on average. Intel has also improved significantly over Meteor Lake, delivering better performance with lower DRAM bandwidth demand.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png" width="678" height="387" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:387,&quot;width&quot;:678,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7f6ecd05-bf90-4d98-9ce4-b00b2400162d_678x387.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>HWInfo used for AMD, as its memory counters appear accurate for Zen 4 and deliver reasonable values for Zen 5. VTune’s UNC_M_CAS_COUNT_RD/WR counters used for Intel</figcaption></figure></div><p>AMD’s Strix Point iGPU stands out as being particularly bandwidth hungry. It’s able to take the top spot by a considerable margin, but in the process requires disproportionately more data transfer from DRAM. A deeper look shows highest observed DRAM bandwidth usage over HWInfo’s 2 second sampling interval was 60.6 GB/s, while Meteor Lake only reached a maximum of 52 GB/s over 0.04 seconds. So while none of these platforms are pushing their bandwidth limits, AMD’s higher DRAM bandwidth usage may put Strix Point at a power disadvantage. That’s because data transfer can consume significant power, and DRAM is one of the most power hungry places for a CPU to get data from.</p><p>The situation may get even more extreme for AMD in other games. Elden Ring for example averaged 63.6 GB/s of DRAM traffic on Meteor Lake. In laptops where every last watt matters, Intel’s caching strategy is good to see.</p><p>FluidX3D uses GPU compute to carry out fluid simulations. Unlike games, FluidX3D almost exclusively loads the GPU and places little load on the CPU. Lunar Lake also shows improvement over Meteor Lake in this task. However, it lands just short of AMD’s last generation Phoenix part.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png" width="538" height="336" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:336,&quot;width&quot;:538,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87ceeae-2d4c-42b9-be35-7dbd7e5232f1_538x336.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>While FluidX3D tends to be memory bandwidth bound on discrete cards, the picture on integrated GPUs isn’t so clear. The benchmark reported 63 GB/s of bandwidth usage on Lunar Lake, and that’s likely close to DRAM bandwidth usage because FluidX3D tends to have low cache hitrates.</p><p>Lunar Lake’s iGPU is a showcase of Intel’s determination to keep pace with AMD in thin and light laptops. Instead of building a bigger GPU as they did with Meteor Lake, Intel is trying to more efficiently operate a GPU of the same size. More cache reduces power hungry DRAM accesses. Reorganized Vector Engines help improve utilization. More efficient CPU cores mean the GPU gets a bigger power and thermal budget to stretch its legs in.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png" width="688" height="288" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:288,&quot;width&quot;:688,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F453bdf2a-5dee-4a3a-9af7-1eb99711e0e7_688x288.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>VTune collecting metrics from an Elden Ring gaming session, on my Meteor Lake laptop. 55.7 GB/s from DRAM is a lot of data moving around</figcaption></figure></div><p>AMD in contrast is going all out for higher performance. Their Strix Point APU has a bigger GPU than the prior generation with minor architectural changes, but continues to use a 2 MB L2 cache and shares the same die with more CPU cores. From brief testing, Strix Point undoubtedly wins from a performance perspective. But AMD also draws more power to get there. I wonder if AMD will have to adopt something closer to Intel’s caching strategy going forward. Even if Strix Point can deliver better performance with smaller caches, it’s a very bandwidth hungry and therefore power hungry way to do so.</p><p>Lunar Lake’s iGPU is perhaps most interesting as a preview of what Intel’s upcoming Battlemage discrete GPUs can offer. With Xe2, Intel is looking to use the same graphics architecture across their product stack. It’s a strategy AMD has used for a long time, and also shows Intel is serious about hitting higher GPU performance targets. Efficiency improvements from reorganized vector units as well as bigger raytracing accelerators should be very fun to see once they hit the desktop market.</p><div><p><span>Finally, I’ve seen plenty of doom and gloom about Intel’s discrete GPU efforts from various sites. But unlike Intel’s prior dGPU efforts, the company is using their integrated GPUs as a springboard into the discrete GPU market. Integrated GPUs continue to be very important for mobile gaming, so that springboard will continue to exist for the foreseeable future.</span></p><p><span>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;</span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span>&nbsp;or our&nbsp;</span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;</span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div></div>]]></description>
        </item>
    </channel>
</rss>