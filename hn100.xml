<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 03 Feb 2025 03:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Deep Research (215 pts)]]></title>
            <link>https://openai.com/index/introducing-deep-research/</link>
            <guid>42913251</guid>
            <pubDate>Mon, 03 Feb 2025 00:06:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-deep-research/">https://openai.com/index/introducing-deep-research/</a>, See on <a href="https://news.ycombinator.com/item?id=42913251">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-deep-research/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Don't make fun of renowned author Dan Brown (2013) (117 pts)]]></title>
            <link>https://onehundredpages.wordpress.com/2013/06/12/dont-make-fun-of-renowned-dan-brown/</link>
            <guid>42912133</guid>
            <pubDate>Sun, 02 Feb 2025 21:42:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://onehundredpages.wordpress.com/2013/06/12/dont-make-fun-of-renowned-dan-brown/">https://onehundredpages.wordpress.com/2013/06/12/dont-make-fun-of-renowned-dan-brown/</a>, See on <a href="https://news.ycombinator.com/item?id=42912133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

		
			
<article id="post-945">
	
	<!-- .entry-header -->

	<div>
		<div>
<p><a href="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg"><img data-attachment-id="946" data-permalink="https://onehundredpages.wordpress.com/2013/06/12/dont-make-fun-of-renowned-dan-brown/dan-brown/" data-orig-file="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg" data-orig-size="236,363" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="dan brown" data-image-description="" data-image-caption="" data-medium-file="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg?w=195" data-large-file="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg?w=236" alt="dan brown" src="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg?w=768" srcset="https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg 236w, https://onehundredpages.wordpress.com/wp-content/uploads/2013/06/dan-brown.jpg?w=98&amp;h=150 98w" sizes="(max-width: 236px) 100vw, 236px"></a></p>
<p>Renowned author Dan Brown woke up in his luxurious four-poster bed in his expensive $10 million house – and immediately he felt angry. Most people would have thought that the 48-year-old man had no reason to be angry. After all, the famous writer had a new book coming out. But that was the problem. A new book meant an inevitable attack on the rich novelist by the wealthy wordsmith’s fiercest foes. The critics.</p>
</div>
<p>Renowned author Dan Brown hated the critics. Ever since he had become one of the world’s top renowned authors they had made fun of him. They had mocked bestselling book&nbsp;<i>The Da Vinci Code</i>, successful novel&nbsp;<i>Digital Fortress</i>, popular tome&nbsp;<i>Deception Point</i>, money-spinning volume&nbsp;<i>Angels &amp; Demons</i>&nbsp;and chart-topping work of narrative fiction&nbsp;<i>The Lost Symbol</i>.</p>
<p>The critics said his writing was clumsy, ungrammatical, repetitive and repetitive. They said it was full of unnecessary tautology. They said his prose was swamped in a sea of mixed metaphors. For some reason they found something funny in sentences such as “His eyes went white, like a shark about to attack.”&nbsp;<i>They even say my books are packed with banal and superfluous description</i>, thought the 5ft 9in man. He particularly hated it when they said his imagery was nonsensical. It made his insect eyes flash like a rocket.</p>
<p>Renowned author Dan Brown got out of his luxurious four-poster bed in his expensive $10 million house and paced the bedroom, using the feet located at the ends of his two legs to propel him forwards. He knew he shouldn’t care what a few jealous critics thought. His new book Inferno was coming out on Tuesday, and the 480-page hardback published by Doubleday with a recommended US retail price of $29.95 was sure to be a hit. Wasn’t it?</p>
<div>
<p><i>I’ll call my agent</i>, pondered the prosperous scribe. He reached for the telephone using one of his two hands. “Hello, this is renowned author Dan Brown,” spoke renowned author Dan Brown. “I want to talk to literary agent John Unconvincingname.”</p>
<p>“Mr Unconvincingname, it’s renowned author Dan Brown,” told the voice at the other end of the line. Instantly the voice at the other end of the line was replaced by a different voice at the other end of the line. “Hello, it’s literary agent John Unconvincingname,” informed the new voice at the other end of the line.</p>
<p>“Hello agent John, it’s client Dan,” commented the pecunious scribbler. “I’m worried about new book Inferno. I think critics are going to say it’s badly written.”</p>
<p>The voice at the other end of the line gave a sigh, like a mighty oak toppling into a great river, or something else that didn’t sound like a sigh if you gave it a moment’s thought. “Who cares what the stupid critics say?” advised the literary agent. “They’re just snobs. You have millions of fans.”</p>
<p><i>That’s true</i>, mused the accomplished composer of thrillers that combined religion, high culture and conspiracy theories. His books were read by everyone from renowned politician President Obama to renowned musician Britney Spears. It was said that a copy of&nbsp;<i>The Da Vinci Code</i>&nbsp;had even found its way into the hands of renowned monarch the Queen. He was grateful for his good fortune, and gave thanks every night in his prayers to renowned deity God.</p>
<p>“Think of all the money you’ve made,” recommended the literary agent. That was true too. The thriving ink-slinger’s wealth had allowed him to indulge his passion for great art. Among his proudest purchases were a specially commissioned landscape by acclaimed painter Vincent van Gogh and a signed first edition by revered scriptwriter William Shakespeare.</p>
<p>Renowned author Dan Brown smiled, the ends of his mouth curving upwards in a physical expression of pleasure. He felt much better. If your books brought innocent delight to millions of readers, what did it matter whether you knew the difference between a transitive and an intransitive verb?</p>
<p>“Thanks, John,” he thanked. Then he put down the telephone and perambulated on foot to the desk behind which he habitually sat on a chair to write his famous books on an Apple iMac MD093B/A computer. New book Inferno, the latest in his celebrated series about fictional Harvard professor Robert Langdon, was inspired by top Italian poet Dante. It wouldn’t be the last in the lucrative sequence, either. He had all the sequels mapped out. The Mozart Acrostic. The Michelangelo Wordsearch. The Newton Sudoku.</p>
<p>The 190lb adult male human being nodded his head to indicate satisfaction and returned to his bedroom by walking there. Still asleep in the luxurious four-poster bed of the expensive $10 million house was beautiful wife Mrs Brown. Renowned author Dan Brown gazed admiringly at the pulchritudinous brunette’s blonde tresses, flowing from her head like a stream but made from hair instead of water and without any fish in. She was as majestic as the finest sculpture by Caravaggio or the most coveted portrait by Rodin.&nbsp;<i>I like the attractive woman</i>, thought the successful man.</p>
<p>Perhaps one day, inspired by beautiful wife Mrs Brown, he would move into romantic poetry, like market-leading British rhymester John Keats.<i>That would be good</i>, opined the talented person, and got back into the luxurious four-poster bed. He felt as happy as a man who has something to be happy about and is suitably happy about it.</p>
<p>Inferno by Dan Brown 470pp, Bantam Press, rrp £20</p>

</div>

			
						</div><!-- .entry-content -->

	
	<!-- .entry-footer -->
</article><!-- #post-## -->

			
<!-- #comments -->

				<!-- .navigation -->
	
		
		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Costa rican supermarket wins trademark battle against Nintendo (149 pts)]]></title>
            <link>https://ticotimes.net/2025/01/30/david-vs-goliath-costa-rican-super-mario-defeats-nintendo-in-court</link>
            <guid>42911842</guid>
            <pubDate>Sun, 02 Feb 2025 21:07:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ticotimes.net/2025/01/30/david-vs-goliath-costa-rican-super-mario-defeats-nintendo-in-court">https://ticotimes.net/2025/01/30/david-vs-goliath-costa-rican-super-mario-defeats-nintendo-in-court</a>, See on <a href="https://news.ycombinator.com/item?id=42911842">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-td-block-uid="tdi_70">

<p>A small Costa Rican supermarket has emerged victorious from a legal battle against the renowned video game giant, Nintendo. José Mario Alfaro González, owner of “Super Mario” in San Ramón de Alajuela, found himself in an unexpected legal showdown when he attempted to formally register his store’s name. In Costa Rica, the term “super” is often used as shorthand for “supermarket.”</p>



<p>What began as a routine trademark registration process turned into a clash of the titans when Nintendo of America filed an appeal, claiming exclusive rights to the “Super Mario” name. The video game behemoth, creator of iconic titles like Super Mario Bros., asserted that the name was registered under its <a href="https://ticotimes.net/2005/08/26/trademark-use-registration" target="_blank" rel="noreferrer noopener">trademark</a> in various classes, including clothing, games, and accessories.</p>



<p>However, Alfaro intended to register “Super Mario” specifically for international class 35, which covers “supply services for third parties of products from the basic food basket.” “We knew these big companies have vast resources to fight, and we had to think carefully,” said Edgardo Jiménez, Alfaro’s legal representative. “We even considered changing the supermarket’s name to avoid a lawsuit.”</p>



<p>But Alfaro and Jiménez persevered, and on January 21st, the National Registry issued a final notification in favor of “Super Mario” de San Ramón. It was game over for Nintendo, and the small Costa Rican business had won the legal bout. “We refuted all of Nintendo’s arguments, demonstrating their errors and our rightful claim,” Jiménez explained. “Although Nintendo’s registration covers 45 categories, it doesn’t include the specific class for suppliers of basic food products.”</p>



<p>As of yet, <a href="https://www.nintendo.com/us/" target="_blank" rel="noreferrer noopener">Nintendo</a> has not issued an official statement or comment on the outcome. This legal victory is a testament to the determination of a small Costa Rican entrepreneur who stood his ground against a global corporation, proving that even the smallest of players can triumph in the face of seemingly insurmountable odds.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A loophole used by Shein/Temu to ship packages to US tax-free (2024) (109 pts)]]></title>
            <link>https://www.businessinsider.com/shein-temu-de-minimis-tax-loophole-scrutiny-2024-1</link>
            <guid>42911511</guid>
            <pubDate>Sun, 02 Feb 2025 20:31:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/shein-temu-de-minimis-tax-loophole-scrutiny-2024-1">https://www.businessinsider.com/shein-temu-de-minimis-tax-loophole-scrutiny-2024-1</a>, See on <a href="https://news.ycombinator.com/item?id=42911511">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude" id="piano-inline-content-wrapper" data-piano-inline-content-wrapper="" data-user-status="anonymous" data-track-content="" data-post-type="post">
                                    <ul><li>A US tax rule called de minimis has gotten increased attention as Shein and Temu have grown.&nbsp;</li><li>Detractors say it creates unfair competition and allows companies to bypass trade laws.&nbsp;</li><li>Shein and Temu representatives said they support de minimis reform — as long as it's fair.&nbsp;</li></ul><p>As Shein and Temu <a target="_blank" href="https://www.businessinsider.com/temu-passes-shein-in-traffic-purchasing-visits-hasnt-surpassed-amazon-2023-9" data-analytics-product-module="body_link" rel="">grow their foothold in the US</a>, analysts and lawmakers are paying close attention to both companies' use of a special tax rule that some say should be eliminated completely.</p><!-- Excluded mobile ad on desktop --><p>The provision, known as "de minimis," allows importers to avoid paying duty and tax on shipments that are going to individual consumers and are worth less than $800 in total.</p><p>Shippers using de minimis also do not have to provide as much information to US Customs and Border Protection as shippers using more traditional methods would.<strong> </strong>Opponents of the rule argue it creates unfair competition, and the lack of in-depth screening could allow for the <a target="_blank" href="https://www.businessinsider.com/shein-and-temu-skirt-import-tariffs-us-lawmakers-fast-fashion-2023-6" data-analytics-product-module="body_link" rel="">import of goods</a> containing banned materials like <a target="_blank" href="https://www.businessinsider.com/temu-blamed-shoppers-for-buying-forced-labor-lawmakers-say-2023-6" data-analytics-product-module="body_link" rel="">cotton from Xinjiang</a>, where forced labor is common.</p><!-- Excluded mobile ad on desktop --><p>De minimis has been around since 1938 when Congress introduced the rule in order to speed up the processing of items that were so cheap that they would not generate significant tax revenue for the government. The limit for eligible items has been raised many times over the years, most recently going up to $800 from $200 in 2016.</p><p>Two bills aimed at reforming de minimis were introduced in Congress last June. One, introduced by Rep. Earl Blumenauer, aims to limit companies in "nonmarket" economies <strong>—</strong>&nbsp;or countries where prices do not follow market dynamics, such as China and Russia<strong> — </strong>and countries on priority watch lists from using de minimis shipments. The other bill, from Sen. Bill Cassidy and Sen. Tammy Baldwin, would ban de minimis shipments from China.</p><p>But some experts say it's unlikely the provision will go away anytime soon.</p><!-- Excluded mobile ad on desktop --><p>A US trade law and policy expert interviewed by Bank of America analysts put the odds of Blumenauer's bill passing the Senate at more than 50% but said it had a roughly 30% chance of passing overall "given issues in the House," according to a research note the bank published on January 5. The same expert said the bill introduced by Cassidy and Baldwin was unlikely to pass in Congress.</p><p>"While many retailers are likely in favor of these bills, groups like the US Chamber of Commerce or the Express Shippers Association may not be in favor of them since they generally are looking for fewer tariffs, not more," UBS analyst Jay Sole wrote in a research note in August. "Also, direct carriers, direct importers, and logistics companies may not be in favor of a change since their businesses may benefit from the $800 rule."</p><p>The provision is more likely to change — a prospect that both Shein and Temu have said they support.</p><!-- Excluded mobile ad on desktop --><h2>A 'paradigm shift'</h2><p>Shein and Temu are not the only companies to use the de minimis provision, but they have gained the attention of advocacy groups and lawmakers in part because of how quickly they have grown in the last year.</p>
                          
                          
                          
                                 
                          <p>Shein, which was founded in China in 2008 but moved its headquarters to Singapore in late 2021, <a target="_blank" href="https://www.businessinsider.com/what-is-shein-billion-dollar-fast-fashion-company-explained-2023-7" data-analytics-product-module="body_link" rel=""><u>filed confidentially</u></a> for a US IPO in November. It's reportedly looking for a valuation of $90 billion.</p><p>Temu, meanwhile, is owned by Chinese commerce giant Pinduoduo Holdings and headquartered in Boston. It has grown extremely fast since launching in the US in September 2022, outpacing many more established e-commerce companies in terms of app downloads and <a target="_blank" href="https://www.insiderintelligence.com/content/shoppers-spend-nearly-twice-long-on-temu-s-app-than-its-competitors" data-analytics-product-module="body_link" rel=" nofollow"><u>engagement</u></a>.</p><!-- Excluded mobile ad on desktop -->
                            <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                            
                            
                            
                              <div>
                          
                            
                            <meta itemprop="contentUrl" content="https://i.insider.com/64da3c621f51cc001968df91">
                            <p><img src="data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg xmlns='http://www.w3.org/2000/svg' width='1' height='1'/%3E" data-content-type="image/jpeg" data-srcs="{&quot;https://i.insider.com/64da3c621f51cc001968df91&quot;:{&quot;contentType&quot;:&quot;image/jpeg&quot;,&quot;aspectRatioW&quot;:4032,&quot;aspectRatioH&quot;:3024}}" alt="flat lay image of Temu package, checkered pants, and pink heels">
                          </p></div>
                            
                            <span>
                                  <figcaption data-e2e-name="image-caption">
                                    Goods bought on Temu.
                                    
                            <span data-e2e-name="image-source" itemprop="creditText">
                            
                            Jennifer Ortakales Dawkins/Insider
                            
                            </span>
                                  </figcaption>
                                </span>
                            </figure>
                          <p>Shipping consultancy ShipMatrix estimates that Shein and Temu each ship more than a million packages to the US daily.</p><p>An interim 2023 report from the US House Select Committee on the Chinese Communist Party said that Shein and Temu "likely" account for more than 30% of all shipments made to the US under the de minimis provision. It added that almost 50% of all de minimis shipments to the US come from China.</p><p>Shipping directly from manufacturers to individual consumers helps Shein and Temu keep prices low. Wired reported in May that the <a target="_blank" href="https://www.wired.com/story/temu-is-losing-millions-of-dollars-to-send-you-cheap-socks/" data-analytics-product-module="body_link" rel=" nofollow"><u>average order on Temu was about $25</u></a>, while <a target="_blank" href="https://www.barrons.com/articles/shein-clothing-shopping-retail-challenger-16d512db" data-analytics-product-module="body_link" rel=" nofollow"><u>Shein's was about $70 as of April</u></a>, according to Barron's.</p><!-- Excluded mobile ad on desktop --><p>The Bank of America analysts wrote in their research note that if de minimis were to change or go away completely, it could be disruptive for these companies.</p><p>"Changes to the exemption could create a paradigm shift for retailers like Temu and Shein," Bank of America analysts wrote in the note. "The discount prices charged by both retailers are aided by the absence of duties and the lack of trade and customs infrastructure that would be required if this regulation did not exist."</p><p>The analysts added that they "wouldn't expect price increases to be well-accepted by the consumer."</p><!-- Excluded mobile ad on desktop --><p>Representatives for Shein and Temu pushed back on the notion that their businesses have relied on de minimis to grow and said that they support reforms to the provision if they are fair.</p><p>"Our success has come from our ability to leverage on-demand technology to bring the latest styles to customers efficiently and at an affordable price," a spokesperson for Shein said.</p><p>In a July letter to the American Apparel and Footwear Association, Shein's executive vice chairman, Donald Tang wrote that the company supports "responsible reform of the de minimis exemption."</p><!-- Excluded mobile ad on desktop --><p>"SHEIN believes the de minimis framework should be reformed to create a more level, transparent playing field — one where all retailers play by the same rules, and where the rules are applied evenly and equally, regardless of where a company is based or ships from," he wrote.</p><p>A spokesperson for Temu said its "supply-chain efficiencies and operational proficiencies" have been the primary drivers of its rapid growth.</p><p>"We are open to and supportive of any policy adjustments made by legislators that align with consumer interests. We believe that as long as these policies are fair, they won't influence the outcomes of competitive business dynamics," they said. "We also see such reforms as potential avenues to alleviate concerns among various stakeholders, fostering greater comprehension and emphasizing the significance of each player in the market ecosystem."</p><!-- Excluded mobile ad on desktop --><p>The two companies also detailed their practices for remaining compliant with import laws. The Shein spokesperson said that the company has a "zero-tolerance policy for forced labor" and requires its manufacturers to only source cotton from approved regions. The Temu spokesperson said it requires its sellers to sign an agreement that they will maintain compliant business practices, and that "the use of forced, penal, or child labor is strictly prohibited."</p><h2>'If you can't beat 'em, join 'em'</h2><p>Other <a target="_blank" href="https://www.nytimes.com/2023/11/04/business/dealbook/us-retailers-say-an-old-trade-law-puts-them-at-a-disadvantage.html#:~:text=The%20rule%2C%20known%20as%20de,are%20textile%20and%20apparel%20products." data-analytics-product-module="body_link" rel=" nofollow"><u>opposition to de minimis has come from US retailers and trade groups</u></a> concerned about their ability to compete with companies shipping cheaply from China. Many retailers manufacture their products abroad and then ship them to the US in large quantities, meaning they can't use the de minimis provision as easily as Shein and Temu have.</p><p>"The textile industry is probably the biggest proponent against de minimis because they've got all their mills, they're making shirts, pants," said Steve Story, executive vice president at Apex Logistics International, which specializes in de minimis. "I can order a shirt from China at a quarter of the price, or I can go to Macy's, Nordstrom's, or Walmart and buy it."</p><!-- Excluded mobile ad on desktop --><p>He said that companies like Shein and Temu do not have to serve as the importers of record and are therefore not responsible for certifying the origin of items on their platforms.</p><p>"However, they're facilitating the sale for export to the United States, so they do have a financial interest in the merchandise," Story said. He serves on several committees exploring possible changes to de minimis, including a customs bond that would allow customs to make a claim in the event of a policy violation.</p><p>While many apparel retailers have spoken out against de minimis, an expert interviewed by UBS' Sole said that many retailers outside clothing are taking an "if you can't beat 'em, join 'em" approach and exploring ways they, too, can use de minimis to lower their costs instead of changing the rule altogether.</p><!-- Excluded mobile ad on desktop --><p>Satish Jindel, founder and president of ShipMatrix, said companies could lean on de minimis to lower their own prices.</p><p>"The companies who think it is not helping them should be looking at it and saying, how do I utilize that opportunity to revise and refine my business model to lower the cost of operating," Jindel said.</p><p><em>Got a tip? Contact this reporter at mstone@insider.com, mlstone@protonmail.com, or on the secure messaging app Signal at (646) 889-2143 using a non-work phone.&nbsp;</em></p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waydroid – Android in a Linux container (240 pts)]]></title>
            <link>https://waydro.id/</link>
            <guid>42911042</guid>
            <pubDate>Sun, 02 Feb 2025 19:29:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waydro.id/">https://waydro.id/</a>, See on <a href="https://news.ycombinator.com/item?id=42911042">Hacker News</a></p>
<div id="readability-page-1" class="page">
  <!-- ====== Header Start ====== -->
  
  <!-- ====== Header End ====== -->
  <!-- ====== Hero Start ====== -->
  <div id="home">
          <div data-wow-delay=".2s">
            <p><img src="https://waydro.id/assets/images/hero/waydroid_white_tb.png"></p><p> A container-based approach to boot a full Android system on regular GNU/Linux systems running Wayland based desktop environments. </p>
            <ul>
              <li>
                <a href="#docs" rel="nofollow noopener"> Documentation</a>
              </li>
              <li>
                <a href="#about" rel="nofollow noopener"> Learn More <i></i>
                </a>
              </li>
            </ul>
          </div>
          <div data-wow-delay=".25s">
            <div id="carouselExampleCaptions" data-bs-ride="carousel">
                <div>
                  <p><img src="https://waydro.id/assets/images/hero/main_landing.jpeg"></p><div>
                    <h5>Full Integration Of Android on Linux</h5>
                    <p>Using Waydroid's Multi-Window Mode</p>
                  </div>
                </div>
                <div>
                  <p><img src="https://waydro.id/assets/images/hero/Group%201.png"></p><div>
                    <h5>Mobile Linux Integrations</h5>
                    <p>Brought To Life With Waydroid</p>
                  </div>
                </div>
                <div>
                  <p><img src="https://waydro.id/assets/images/hero/clean_desktop.jpeg"></p><div>
                    <h5>Bring Your Desktop To Life</h5>
                    <p>With Waydroids Fullscreen Mode For Desktops &amp; Kiosks</p>
                  </div>
                </div>
              </div>
            <p><img src="https://waydro.id/assets/images/hero/dotted-shape.svg" alt="shape">
            <img src="https://waydro.id/assets/images/hero/dotted-shape.svg" alt="shape">
          </p></div>
        </div>
  <!-- ====== Hero End ====== -->
  <!-- ====== Features Start ====== -->
  <div id="features">
      <div>
            <p><span>Features</span></p><h2>Main Features of Waydroid</h2>
            <p> Waydroid uses Linux namespaces (user, pid, uts, net, mount, ipc) to run a full Android system in a container and provide Android applications on any GNU/Linux-based platform (arm, arm64, x86, x86_64). The Android system inside the container has direct access to needed hardware through LXC and the binder interface. </p>
          </div>
      <div>
        <!-- Feature Start -->
        <div data-wow-delay=".1s">
              <h3>Free and Open-Source</h3>
              <p> The Project is completely free and open-source, currently our repo is hosted on <a target="BLANK" href="https://github.com/waydroid">Github.</a> </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".15s">
              <h3>Full app integration</h3>
              <p> Waydroid integrated with Linux adding the Android apps to your linux applications folder. </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".2s">
              <h3>Multi-window mode</h3>
              <p> Waydroid expands on Android freeform window definition, adding a number of features. </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".25s">
              <h3>Full UI Mode</h3>
              <p> For gaming and full screen entertainment, Waydroid can also be run to show the full Android UI. </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".1s">
              <h3>Near native performance</h3>
              <p> Get the best performance possible using wayland and AOSP mesa, taking things to the next level </p>
            </div>
        <!-- Feature Start -->
        <div data-wow-delay=".1s">
              <h3>Active community</h3>
              <p> Find out what all the buzz is about and explore all the possibilities Waydroid could bring </p>
            </div>
      </div>
    </div>
  <!-- ====== Features End ====== -->
  <!-- ====== About Start ====== -->
  <div data-wow-delay=".2s" id="about">
        <div>
            <p><span>About Us</span></p><h2>Get your favourite Android Apps on Linux.</h2>
            <p> Waydroid brings all the apps you love, right to your desktop, working side by side your Linux applications.<br> The Android inside the container has direct access to needed hardwares.<br> The Android runtime environment ships with a minimal customized Android system image based on <a href="https://lineageos.org/" target="blank">LineageOS</a>. The used image is currently based on Android 11 </p>
            <p><a href="#install"> Install Instructions <i></i>
            </a>
          </p></div>
        <p><img src="https://waydro.id/assets/images/hero/photo1628875295.jpeg" alt="about-image">
        </p>
      </div>
  <!-- ====== About End ====== -->
  <!-- ====== Docs Start ====== -->
  <div id="docs">
            <p><span>Docs</span></p><h2>Our Documentation</h2>
            <p> Our documentation site can be found at <a target="BLANK" href="https://docs.waydro.id/"> docs.waydro.id</a>
            </p>
            <h2>Bugs &amp; Reports</h2>
            <p> Bug Reports can be filed on our repo <a target="BLANK" href="https://github.com/waydroid/waydroid/issues"> Github Repo</a>
            </p>
            <h2>Project Development</h2>
            <p> Our development repositories are hosted on <a target="BLANK" href="https://github.com/waydroid/"> Github</a>
            </p>
            <h2>How to Install ?</h2>
            <p> Please refer to our <a target="BLANK" href="https://docs.waydro.id/usage/install-on-desktops"> installation docs</a> for complete installation guide. </p>
            <h2>Manual Image Download</h2>
            <p>You can also manually download our images from</p>
            <p><a target="BLANK" href="https://sourceforge.net/projects/waydroid">
              <img src="https://waydro.id/assets/images/logo/sf.png" alt="sourceforge logo"> SourceForge <i></i>
            </a>
          </p></div>
  <!-- ====== Docs End ====== -->
  <!-- ====== FAQ Start ====== -->
  <section id="install">
    <p><img src="https://waydro.id/assets/images/faq/shape.svg" alt="shape">
    </p>
    <div>
      <div>
            <p><span>Instructions</span></p><h2>Quick install reference</h2>
            <p>For systemd distributions</p>
          </div>
      <div>
        
        <form>
          <p>Waydroid supports most common architectures (ARM, ARM64, x86 &amp; x86_64 CPUs)</p>
          <p>Waydroid uses Android's mesa integration for passthrough, and that enables support to most ARM/ARM64 SOCs on the mobile side, and Intel/AMD GPUs for the PC side. For Nvidia GPUs (except tegra) and VMs, we recommend using <a href="https://docs.waydro.id/faq/get-waydroid-to-work-through-a-vm">software-rendering</a> </p>
        </form>
      </div>
      <div>
        <p>Follow the install instructions for your linux distribution. You can find a list in our <a href="https://docs.waydro.id/usage/install-on-desktops">docs</a>.
        </p>
      </div>
      <div>
        <p>After installing you should start the waydroid-container service, if it was not started automatically:</p>
        <div>
          <figure>
            <pre><code data-lang="">sudo systemctl enable --now waydroid-container
</code></pre>
          </figure>
        </div>
      </div>
      <div>
        <p>Then launch Waydroid from the applications menu and follow the first-launch wizard.</p>
      </div>
      <div>
        <p>If prompted, use the following links for System OTA and Vendor OTA:</p>
        <div>
          <pre><code data-lang=""><p>https://ota.waydro.id/system</p></code></pre>
        </div>
        <div>
          <pre><code data-lang=""><p>https://ota.waydro.id/vendor</p></code></pre>
        </div>
      </div>
      <div>
        <p>For further instructions, please visit the docs site <a target="BLANK" href="https://docs.waydro.id/">here</a>
          </p>
      </div>
    </div>
  </section>
  <!-- ====== FAQ End ====== -->
  <div id="wdlinux">
              <p><img src="https://waydro.id/assets/images/hero/Computer_wd.png">
              </p>
              <div>
                
                <h3><span>Latest Beta </span><span> 01.30.2023</span></h3>
                <p> We have started creating a few fully-integrated distros in order to demonstrate some of the possibilities that Waydroid can help achieve. <br> Each of the distros we produce will also showcase some of the work from our growing community of contributors. <br> Our initial alpha releases of this integration started with Ubuntu 20.04 (focal) and is now on Ubuntu 22.04 (jammy) as well as Debian 12 (bookworm), and includes many added tools and scripts to help open up what is possible. </p>
                <div>
                  
                  <form>
                    <p>Waydroid-Linux currently only supports x86_64 CPUs (Intel/AMD)</p>
                    <p>Waydroid-Linux uses Android's mesa integration for passthrough, and that restricts support to Intel and AMD GPUs<br> For Nvidia GPUs and VMs, we recommend using <a href="https://docs.waydro.id/faq/get-waydroid-to-work-through-a-vm">software-rendering</a> </p>
                    <div id="sources">
                      <p>We have been working with a number of devs on Waydroid-Linux, and have been creating or contributing to a number of projects for it. <br> Here are just a few of the projects we've been using:</p>
                      <ol>
                        <li>
                          <div>
                            <p>Waydroid-Linux Tools</p><p> A few scripts, configs, and themes for the Waydroid-Linux builds </p>
                          </div>
                        </li>
                        <li>
                          <div>
                            <p>Waydroid-Settings</p><p> A GTK app written in Python to control Waydroid settings and expand with scripts (shell/py) </p>
                          </div>
                        </li>
                        <li>
                          <div>
                            <p>Penguins-Eggs</p><p> A tool used for packaging and installation of various Linux distros </p>
                            
                          </div>
                        </li>
                        <li>
                          
                        </li>
                      </ol>
                    </div>
                  </form>
                </div>
                <!---------->
                <details>
                  <summary>Live Mode Info</summary><br> Due to how Waydroid uses LXC and kernel modules for the binder, it will not work while running in live mode and must be installed before working properly.&nbsp; <br>Make sure to check the readme for the .iso, as it contains the specifics needed for each build.&nbsp;
                </details>
                <br>
                <!-- Button trigger modal -->
                <!-- Modal -->
                
                <!---------->
              </div>
            </div>
  <!-- ====== Team Start ====== -->
  <div id="team">
      <div>
            <p><span>Our Team</span></p><h2>Meet The Team</h2>
            <p>Here are the members of our team</p>
          </div>
      <div>
        <div data-wow-delay=".1s">
            
            <p>
              <h5>Erfan Abdi</h5>
              <p4>@erfanoabdi</p4>
              <h6 contenteditable="true">Lead Developer</h6>
            </p>
            
          </div>
        <div data-wow-delay=".15s">
            
            <p>
              <h5>Alessandro Astone</h5>
              <p4>@aleasto</p4>
              <h6>Developer</h6>
            </p>
            
          </div>
        <div data-wow-delay=".15s">
            
            <p>
              <h5>Jon West</h5>
              <p4>@electrikjesus</p4>
              <h6>Developer</h6>
            </p>
            
          </div>
        <div data-wow-delay=".2s">
            
            <p>
              <h5>Radek Błędowski</h5>
              <p4>@RKBDI</p4>
              <h6>Designer</h6>
            </p>
            
          </div>
      </div>
    </div>
  <div id="donate">
          <h2>Help Us Grow</h2>
          <br>
          <h4> Waydroid is now on Open-Collective </h4>
          <h4> We are now accepting donations and sponsorships through Open-Collective. </h4>
          <div>
            <p><a href="https://opencollective.com/waydroid/donate" target="_blank">
                <img src="https://opencollective.com/webpack/donate/button@2x.png?color=blue">
              </a>
            </p>
            <!-- /.animated scroll -->
          </div>
          <!-- /.banner-child -->
          <!-- /.banner-child -->
        </div>
  <!-- ====== Team End ====== -->
  <!-- ====== Contact Start ====== -->
  <!-- ====== Contact End ====== -->
  <!-- ====== Footer Start ====== -->
  
  <!-- ====== Footer End ====== -->
  <!-- ====== Back To Top Start ====== -->
  <span>
    <i> </i>
  </span>
  <!-- ====== Back To Top End ====== -->
  <!-- ====== All Javascript Files ====== -->
  
  
  
  
  



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Legacy of Lies in Alzheimer's Science (111 pts)]]></title>
            <link>https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html</link>
            <guid>42910829</guid>
            <pubDate>Sun, 02 Feb 2025 19:00:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html">https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html</a>, See on <a href="https://news.ycombinator.com/item?id=42910829">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/01/24/opinion/alzheimers-fraud-cure.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone knows your location: tracking myself down through in-app ads (995 pts)]]></title>
            <link>https://timsh.org/tracking-myself-down-through-in-app-ads/</link>
            <guid>42909921</guid>
            <pubDate>Sun, 02 Feb 2025 17:07:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://timsh.org/tracking-myself-down-through-in-app-ads/">https://timsh.org/tracking-myself-down-through-in-app-ads/</a>, See on <a href="https://news.ycombinator.com/item?id=42909921">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>Recently I read about a <a href="https://www.404media.co/hackers-claim-massive-breach-of-location-data-giant-threaten-to-leak-data/?ref=timsh.org" rel="noreferrer">massive geolocation data leak from Gravy Analytics</a>, which exposed more than 2000 apps, both in AppStore and Google Play, that secretly collect geolocation data without user consent. Oftentimes, even without developers` knowledge. </p><p>I looked into the list (<a href="https://docs.google.com/spreadsheets/d/1Ukgd0gIWd9gpV6bOx2pcSHsVO6yIUqbjnlM4ewjO6Cs/edit?gid=1257088277&amp;ref=timsh.org#gid=1257088277" rel="noreferrer">link here</a>) and found at least 3 apps I have installed on my iPhone. Take a look for yourself! <br>This made me come up with an idea to track myself down externally, e.g. to buy my geolocation data leaked by some application. </p><h3 id="tldr">TL;DR</h3><p>After more than couple dozen hours of trying, here are the main takeaways: </p><ol><li>I found a couple requests sent by my phone with <strong>my precise location </strong>+ 5 requests that leak <strong>my IP address</strong>, which can be turned into geolocation using reverse DNS. </li><li>Learned a lot about the RTB (real-time bidding) auctions and OpenRTB protocol and was shocked by the amount and types of data sent with the bids to ad exchanges. </li><li>Gave up on the idea to buy my location data from a data broker or a tracking service, because I don't have a big enough company to take a trial or $10-50k to buy a huge database with the data of millions of people + me. <br>Well maybe I do, but such expense seems a bit irrational. <br>Turns out that EU-based peoples` data is almost the most expensive. </li></ol><p>But still, I know my location data was collected and I know where to buy it! </p><hr><h2 id="starting-point">Starting point</h2><p>My setup for this research included:</p><ul><li>My old iPhone 11 restored to factory defaults + new apple id. <br>Felt too uncomfortable to do all this on my current phone. </li><li>Charles Proxy to record all traffic coming in and out. <br>I set up the SSL certificate on the iPhone to decrypt all https traffic.</li><li>A simple game called Stack by KetchApp - I remember playing it at school 10-12 years ago. Choosing it as a lab rat felt nostalgic. <br>To my surprise, there were a lot of KetchApp games on the list. </li></ul><figure><img src="https://timsh.org/content/images/2025/01/image-1.png" alt="" loading="lazy" width="320" height="180"></figure><h3 id="huge-amount-of-requests">Huge amount of requests</h3><p>Ok, here we go: only 1 app installed without the default Apple ones, Charles on, launching Stack in 3, 2, 1.... </p><figure data-kg-thumbnail="https://timsh.org/content/media/2025/01/Screen-Recording-2025-01-19-at-00.02.51_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://timsh.org/content/media/2025/01/Screen-Recording-2025-01-19-at-00.02.51.mp4" poster="https://img.spacergif.org/v1/2410x2152/0a/spacer.png" width="2410" height="2152" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:11</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://timsh.org/content/media/2025/01/Screen-Recording-2025-01-19-at-00.02.51_thumb.jpg"></figure><p>These are the requests that the app sends in the first minute after launch. <br>Take a look at the timing of the requests - almost every split second. </p><p>Let's take a look at the contents of the requests. <br>I actually checked every single one of them - but I'll leave out only the interesting ones here. </p><h3 id="unity-ads">Unity [ads]</h3><p>Let's start with the juiciest request sent to <code>https://o.isx.unity3d.com</code> - the first one that included my geo, while I <strong>disabled Location Services</strong> on iPhone for all apps! <br>If you are as naive as I was before this, you might be surprised - what does Unity, the 3D engine, have to do with the in-app advertisement or location tracking? <br>Perhaps that's just some monitoring data to help improve the engine? </p><p>Turns out that Unity's main revenue stream (they made $2 bln+ in 2023) is Unity Ads - "Mobile Game Ad Network". Sounds quite interesting.</p><p>Below is the request body in json format sent to Unity Ads. I will only leave the  fields worth mentioning - the actual size is 200+ keys. </p><pre><code>{
  "ts": "2025-01-18T23:27:39Z", // Timestamp
  "c": "ES", // Country code,
  "d": "sports.bwin.es", // Domain; the app or website where the ad will be displayed.
  "bn": "molocoads-eu-banner", // WTF is moloco ads? We'll see!
  "cip": "181.41.[redacted]", // my IP !!
  "dm": "iPhone12,1", 
  "ct": "2", // Connection type; e.g., Wi-Fi
  "car": "Yoigo", // mobile network operator
  "ifv": "6B00D8E5-E37B-4EA0-BB58-[redacted]", // ID for Vendor. We'll get back to it!
  "lon": "2.[redacted]", // Longitude ... 
  "lat": "41.[redacted]", // Latitude ... 
  "sip": "34.227.224.225", // Server IP (Amazon AWS in US) 
  "uc": "1", // User consent for tracking = True; OK what ?!
}</code></pre><p>Ok, so my IP + location + timestamp + some <code>ifv</code> id are shared with Unity → Moloco Ads → Bwin, and then I see the actual Bwin ad in the game. <br>Wonderful! </p><div><p>As a quick note - location shared was not very precise (but still in the same postal index), I guess due to the fact that iPhone was connected to WiFi and had no SIM installed. <br>If it was LTE, I bet the lat/lon would be much more precise. </p></div><h3 id="hello-facebook-what-are-you-doing-here">Hello Facebook... What are you doing here?</h3><p>Next interesting request that leaks my IP + timestamp (= geo-datapoint) is Facebook.<br>What?!</p><ul><li>I don't have any Meta [Facebook] app installed on this iPhone</li><li>I didn't link the app nor my Apple ID to any Facebook account</li><li>I didn't consent to Facebook getting my IP address!</li></ul><p>And yet here we are:</p><pre><code>{ 
	"bundles": {
		"bidder_token_info": {
			"data": {
				"bt_extras": {
                  "ip":"181.41.[redacted], // nice Extras, bro
                  "ts":1737244649
			},
			"fingerprint": null
		},
        {
          "a lot of data: yes a loooooooot"
         }</code></pre><p>We'll talk more about this one in the next section. </p><h3 id="why-do-you-need-my-screen-brightness-level">Why do you need my screen brightness level? </h3><p>Last request I found interesting was sent to... Unity again: <br><a href="https://configv2.unityads.unity3d.com/webview/4.12.1/release/config.json?ref=timsh.org"><code>https://configv2.unityads.unity3d.com</code></a>. <br>Let's see what's in that config Unity needs so much: </p><pre><code>{
  "osVersion":"16.7.1",
  "connectionType":"wifi",
  "eventTimeStamp":1737244651,
  "vendorIdentifier":"6B00D8E5-E37B-[redacted]", // ifv once again 
  "wiredHeadset":false, // excuse me? 
  "volume":0.5,
  "cpuCount":6,
  "systemBootTime":1737215978,
  "batteryStatus":3,
  "screenBrightness":0.34999999403953552,
  "freeMemory":507888,
  "totalMemory":3550640, // is this RAM?
  "timeZone":"+0100",
  "deviceFreeSpace":112945148
  "networkOperator":"6553565535"
  "advertisingTrackingId":"00000000-0000....", // interesting ...
  }</code></pre><p>There's no "personal information" here, but honestly this amount of data shared with an arbitrary list of 3rd parties is scary. <br>Why do they need to know my screen brightness, memory amount, current volume and if I'm wearing headphones? </p><p>I know the "right" answer - to help companies target their audience better! <br>For example, if you're promoting a mobile app that is 1 GB of size, and the user only has 500 MB of space left - don't show him the ad, right?</p><p>But I also heard lots of controversies on this topic. <br>Like Uber dynamically adjusting taxi price based on your battery level - because you're not waiting for a cheaper option with 4% left while standing in the street. </p><p>I can't know if that or another one is true. <br>But the fact that this data is available and accessible by advertisers suggests that they should at least think of using it. <br>I would. </p><p>Ok, enough with the requests. <br>We can already see the examples of different ip and geolocation leaks. <br>One more "provider" that <strong>also got my IP</strong> + timestamp was adjust.com - but the request body was too boring to include. </p><hr><h2 id="lets-talk-ids">Let's talk IDs</h2><p>You might've already noticed <code>ifv</code> and <code>advertisingTrackingId</code> == <code>IDFA</code> in the requests above - what are those? </p><p>IFV, or IDFV, is "ID for Vendor". <br>This is my id unique for each vendor, a.k.a developer - in this case, KetchApp. <br>This checks out: I installed another KetchApp game to quickly record the requests, and the <code>ifv</code> value was the same for it. </p><p>Advertising Tracking ID, on the other hand, is the cross-vendor value, the one that is shared with an app if you choose "Allow app to track your activity across ...". <br>As you can see above, it was actually set to <code>000000-0000...</code> because I "Asked app not to track". </p><p>I checked this by manually disabling and enabling tracking option for the Stack app and comparing requests in both cases. </p><h3 id="and-thats-the-only-difference-between-allowing-and-disallowing-tracking"><strong>And that's the only difference between allowing and disallowing tracking</strong></h3><p><br>I understand there might be nothing shocking to you in it - this is not really kept secret, you can go and check the docs for Apple developers, for example. </p><p>But I believe this is <strong>not</strong> communicated correctly to the end users, you and me, in any adequate way, shape or form: the free apps you install and use <strong>collect your precise location</strong> with timestamp and send it to some 3rd-party companies. </p><p>The only thing that stops anyone with access to bid data (yet another ad buying agent, or ad exchange, or a dataset bought or rented from data broker, as you'll see later) from tracking you down with all trips you make daily is this <code>IDFA</code> that is not shared when you disallow apps to "track you across apps" to "enhance and personalise your ads experience". </p><p>By the way: if you're using 10 apps from the same vendor (Playrix, KetchApp or another 1000-app company) and allow <strong>a single app</strong> to track you – it would mean that the data collected in all 10 apps will be enriched with your IDFA which can later be exchanged to your personal data. </p><p>At the same time, there is so much data in the requests that I'd expect ad exchanges to find some loophole ID that would allow cross-app tracking without the need for IDFA. <br>I found at least 20 ids like <code>tid</code> and <code>sid</code>, <code>device_id</code> and <code>uid</code> (these 2 are shared with Facebook), and so on. </p><p>By the way, the fact that Facebook collected my IP + timestamp without any adequate consent / app connection from my end is crazy. <br>I think Facebook is more than capable of connecting the dots and my Meta Account to this hit as soon as I login to Instagram or Facebook app on the same IP address. </p><hr><h2 id="how-does-the-data-flow">How does the data flow?</h2><p>Let's get back to the request that leaked my location for a second and look at its trace. We'll focus on the parties in the middle:</p><p>stack<strong> →  o.isx.unity3d.com → molocoads →</strong> bwin (advertiser)</p><p>Unity [ads] is an SSP (supply-side platform) that acts as a collector of data from the app via SDK. <br>As an app developer, you don't need to worry about gathering the right data, registering as a publisher on an ad exchange or whatever - just install the SDK and receive the money. </p><p>All right, what about <a href="https://www.moloco.com/?ref=timsh.org" rel="noreferrer">Molocoads</a>? </p><figure><img src="https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.06.38.png" alt="" loading="lazy" width="2000" height="786" srcset="https://timsh.org/content/images/size/w600/2025/01/Screenshot-2025-01-29-at-23.06.38.png 600w, https://timsh.org/content/images/size/w1000/2025/01/Screenshot-2025-01-29-at-23.06.38.png 1000w, https://timsh.org/content/images/size/w1600/2025/01/Screenshot-2025-01-29-at-23.06.38.png 1600w, https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.06.38.png 2286w" sizes="(min-width: 720px) 720px"><figcaption><span>screenshot from Molocoads landing page</span></figcaption></figure><p>Moloco ads is a DSP network that resells data from multiple SSPs (like Unity, Applovin, Chartboost). Basically, from almost every one of the requested hosts I've seen pop up in Charles Proxy.<br>It then applies some "smart optimisation" and connects a vacant banner space on your phone screen with the advertiser.</p><p>Sounds like moloco aggregates a lot of data and basically anyone (<em>to be clear</em> <em>- any company that becomes an ad partner</em>) can access the data by bidding lower than others. <br>Or imagine a real ad exchange that bids normally and collects all of the data along the way "as a side gig". <br>Basically, this is how intelligence companies and data brokers get their data. </p><p>At this point I was looking for any mentions of Moloco on Telegram and Reddit, and I ran into this post that answered a lot of my questions:</p><figure><blockquote>
<a href="https://www.reddit.com/r/adops/comments/rqlr36/eli5_what_is_the_controversy_behind_bidstream/?ref=timsh.org">ELI5: What is the controversy behind “bidstream data”? Are there really no restraints on who gets this data and what they do with it?</a><br> by
<a href="https://www.reddit.com/user/Pubh12/?ref=timsh.org">u/Pubh12</a> in
<a href="https://www.reddit.com/r/adops/?ref=timsh.org">adops</a>
</blockquote>
</figure><p>Especially, <a href="https://www.reddit.com/r/adops/comments/rqlr36/comment/hqbwmbr/?ref=timsh.org" rel="noreferrer">this comment</a>. To quote a part of it:</p><blockquote>They access it if they integrate with the provider of bidstream, which would be the SSP. It's on the SSP to verify the vendor to whom they give access to bids. Usually, the requirement would be that you actually... bid. <br>SSPs want you to spend money, that's how their business makes revenue. They might open up only part of the traffic to specific vendors (i.e.. if you don't bid worldwide, you won't get the bidstream worldwide, only in the regions in which you operate).</blockquote><p>Just wonderful. </p><h3 id="data-brokers">Data Brokers</h3><p>Let's move further. When I found out how the data gets out, I started looking for any place where it's being sold. It was a quick search.</p><p>I found a data marketplace called <a href="https://datarade.ai/data-categories/device-graph-data?ref=timsh.org" rel="noreferrer">Datarade</a> which is a panel with all sorts of data. When I searched for MAID-specific data, hundreds of options showed up, like these two: </p><figure><img src="https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.24.10.png" alt="" loading="lazy" width="1680" height="740" srcset="https://timsh.org/content/images/size/w600/2025/01/Screenshot-2025-01-29-at-23.24.10.png 600w, https://timsh.org/content/images/size/w1000/2025/01/Screenshot-2025-01-29-at-23.24.10.png 1000w, https://timsh.org/content/images/size/w1600/2025/01/Screenshot-2025-01-29-at-23.24.10.png 1600w, https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.24.10.png 1680w" sizes="(min-width: 720px) 720px"></figure><p>The price of the Redmob dataset surprised me, - $120k a year... for what?<br>Let's now take a look at their promo:</p><figure><img src="https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.26.05.png" alt="" loading="lazy" width="1626" height="838" srcset="https://timsh.org/content/images/size/w600/2025/01/Screenshot-2025-01-29-at-23.26.05.png 600w, https://timsh.org/content/images/size/w1000/2025/01/Screenshot-2025-01-29-at-23.26.05.png 1000w, https://timsh.org/content/images/size/w1600/2025/01/Screenshot-2025-01-29-at-23.26.05.png 1600w, https://timsh.org/content/images/2025/01/Screenshot-2025-01-29-at-23.26.05.png 1626w" sizes="(min-width: 720px) 720px"></figure><div><p>Check out the list of features on the right - do any of them look familiar? </p><p><strong>Quick note</strong>: "low latency" means they know your location from the last time any of the apps shared it. It can be as little as 5 seconds ago. <br>What's even better is that Redmob provides a <strong>free sample</strong> of the data. </p></div><p>I tried to request it from their website, but the sample never landed in my mailbox (surprise-surprise, timsh.org doesn't seem like a customer with high potential). <br>Thankfully, this sample is public on <a href="https://marketplace.databricks.com/details/caa4c07a-b27e-4876-9c9c-3f3c2bbbc11f/Redmob_SAMPLE-Redmob-MAID-Data-for-Identity-Graph-I-Global-I-15B-Users-RealTime?ref=timsh.org" rel="noreferrer">Databricks Marketplace</a> with this annotation:</p><blockquote>Enhance your products and services using our global location data covering over 1.5 billion devices. Using our extensive location dataset, you can unearth concealed patterns, conduct rapid analyses, and obtain profound knowledge.<p>We can also provide region-specific data (MENA, Africa, APAC, etc.) based on your specific requirements. Our pricing model includes an annual licensing option, and we provide free sample data so that you can evaluate the quality of our dataset for yourself. </p></blockquote><figure><img src="https://timsh.org/content/images/2025/01/Screenshot-2025-01-31-at-01.12.16.png" alt="" loading="lazy" width="2000" height="562" srcset="https://timsh.org/content/images/size/w600/2025/01/Screenshot-2025-01-31-at-01.12.16.png 600w, https://timsh.org/content/images/size/w1000/2025/01/Screenshot-2025-01-31-at-01.12.16.png 1000w, https://timsh.org/content/images/size/w1600/2025/01/Screenshot-2025-01-31-at-01.12.16.png 1600w, https://timsh.org/content/images/size/w2400/2025/01/Screenshot-2025-01-31-at-01.12.16.png 2400w" sizes="(min-width: 1200px) 1200px"><figcaption><span>Some sample data for better understanding</span></figcaption></figure><p>To me, the most absurd part is the <code>app</code> column - the source of the data can't be more obvious. I'm also quite interested in the <code>yod</code> column - if it's the birthyear, where did they get it from? Never mind, who cares about your birthyear.</p><h3 id="show-me-the-pii">Show me the PII!</h3><p>All right, imagine I bought the access to a huge stream of Redmob data. <br>But my goal is to track and stalk people like myself or anyone else, so I need some way to exchange MAIDs (<em>=</em><code>ifa</code>) for the actual personal info: name, address, phone number... </p><p>No problem! This kind of dataset is surprisingly also present on Datarade. <br>Take a look at a sample table with <code>MAID &lt;&gt; PII</code> type that is provided by "<a href="https://www.agrmarketingsolutions.com/data-nuggets/?ref=timsh.org" rel="noreferrer">AGR Marketing Solutions</a>":</p><figure><a href="https://docs.google.com/spreadsheets/d/1gbom_3YO-oFB6Yrg_MAhrJKGkYtNHR2S/edit?gid=2029445799&amp;ref=timsh.org#gid=2029445799"><div><p>AGR_Mobile_Intent_PII20240903.xlsx</p><p><img src="https://timsh.org/content/images/icon/spreadsheets_2023q4.ico" alt=""><span>Google Docs</span></p></div><p><img src="https://timsh.org/content/images/thumbnail/AHkbwyL0Y-RadP94i-3ntHlOyBNz5QtZLseHmdD1MAKSAFQi7l2fIzkQjGmDmSQRI2yTN7B1h5cNCpCeVlmlA0dsdek_xQhi6nazcP6xeg-w1200-h630-p" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Inside - all personal info (full name, email, phone number, physical address, property ownership... and IDFAs. </p><p>Congrats, you have just reached the bottom of this rabbit hole. <br>Let's wrap it up and make a couple of bold statements.</p><hr><h2 id="how-to-track-yourself-down">How to track yourself down?</h2><p>Easy! Just follow this simple step-by-step guide:</p><ol><li>Use some free apps for a bit. <br>Move around and commute - this makes the geo data more valuable. </li><li>"Allow" or "ask not to track" - a combo of IP + location + User-agent + geolocation will still be leaked to hundreds of "3rd parties" regardless of your choice.</li><li>Wait for a few seconds until fake DSPs and data brokers receive your data.</li><li>Exchange your full name or phone number for an IDFA (if present), IP address and user-agent through the <code>MAID &lt;&gt; PII</code> data purchased somewhere.</li><li>Now, access the "Mobility data" consisting of geolocation history, and filter it using the values from the previous step. </li></ol><p>Congratulations! You found yourself. </p><p>I <a href="https://excalidraw.com/?ref=timsh.org#json=Ip5AaR-FPppPmtL3AcrBg,-woEvDuI7vER5B7skpT3zA" rel="noreferrer">created a flowchart</a> that includes almost all actors and data mentioned above - now you can see how it's all connected. </p><figure><img src="https://timsh.org/content/images/2025/02/dataflow_upd-1.png" alt="" loading="lazy" width="2000" height="747" srcset="https://timsh.org/content/images/size/w600/2025/02/dataflow_upd-1.png 600w, https://timsh.org/content/images/size/w1000/2025/02/dataflow_upd-1.png 1000w, https://timsh.org/content/images/size/w1600/2025/02/dataflow_upd-1.png 1600w, https://timsh.org/content/images/size/w2400/2025/02/dataflow_upd-1.png 2400w"></figure><p>This is the worst thing about these data trades that happen constantly around the world - each small part of it is (or seems) legit. It's the bigger picture that makes them look ugly. </p><hr><h2 id="afterwords">Afterwords</h2><p>Thanks for reading this story until the end!<br>My research was heavily influenced by these posts and investigations: </p><figure><a href="https://krebsonsecurity.com/2024/10/the-global-surveillance-free-for-all-in-mobile-ad-data/?ref=timsh.org"><div><p>The Global Surveillance Free-for-All in Mobile Ad Data</p><p>Not long ago, the ability to remotely track someone’s daily movements just by knowing their home address, employer, or place of worship was considered a powerful surveillance tool that should only be in the purview of nation states. But a…</p><p><img src="https://timsh.org/content/images/icon/favicon.ico" alt=""><span>Krebs on Security</span><span>Skip to content</span></p></div><p><img src="https://timsh.org/content/images/thumbnail/peoplephone.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://www.404media.co/candy-crush-tinder-myfitnesspal-see-the-thousands-of-apps-hijacked-to-spy-on-your-location/?ref=timsh.org"><div><p>Candy Crush, Tinder, MyFitnessPal: See the Thousands of Apps Hijacked to Spy on Your Location</p><p>A hack of location data company Gravy Analytics has revealed which apps are—knowingly or not—being used to collect your information behind the scenes.</p><p><img src="https://timsh.org/content/images/icon/favicon-3.svg" alt=""><span>404 Media</span><span>Joseph Cox</span></p></div><p><img src="https://timsh.org/content/images/thumbnail/andrew-guan-lTUyP3RaLpw-unsplash.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://interaktiv.br.de/ausspioniert-mit-standortdaten/en/index.html?ref=timsh.org"><div><p>Under Surveillance</p><p>How Location Data Jeopardizes German Security</p><p><img src="https://timsh.org/content/images/icon/apple-touch-icon.png" alt=""><span>BR</span><span>BR Data</span></p></div><p><img src="https://timsh.org/content/images/thumbnail/teaser.png" alt="" onerror="this.style.display = 'none'"></p></a></figure>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sniffnet – monitor your Internet traffic (196 pts)]]></title>
            <link>https://github.com/GyulyVGC/sniffnet</link>
            <guid>42909530</guid>
            <pubDate>Sun, 02 Feb 2025 16:14:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/GyulyVGC/sniffnet">https://github.com/GyulyVGC/sniffnet</a>, See on <a href="https://news.ycombinator.com/item?id=42909530">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
<img alt="" title="Sniffnet" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/header_repository.png" width="95%">
</picture></themed-picture>
<p dir="auto"><a href="#download"><img alt="" title="Download" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/download.svg"></a>
<a href="https://github.com/GyulyVGC/sniffnet/blob/main/ROADMAP.md"><img alt="" title="Roadmap" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/roadmap.svg"></a>
<a href="https://sniffnet.net/" rel="nofollow"><img alt="" title="Website" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/website.svg"></a>
<a href="https://github.com/GyulyVGC/sniffnet/wiki"><img alt="" title="Wiki" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/wiki.svg"></a></p>
<p dir="auto">Application to comfortably monitor your Internet traffic <br>
Cross-platform, Intuitive, Reliable</p>
<p dir="auto">Translated in:<br>
🇨🇳 🇩🇪 🇫🇷 🇷🇺 🇵🇹 🇪🇦 🇮🇹 🇵🇱 <a href="https://github.com/GyulyVGC/sniffnet/issues/60" data-hovercard-type="issue" data-hovercard-url="/GyulyVGC/sniffnet/issues/60/hovercard">+&nbsp;12&nbsp;more&nbsp;languages</a></p>
</div>
<p dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
<img alt="" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/hr.png" width="100%">
</picture></themed-picture>
</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/overview.png"><img alt="" title="Overview page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/overview.png" width="95%"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/inspect.png"><img alt="" title="Inspect page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/inspect.png" width="47%"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/notifications.png"><img alt="" title="Notifications page" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/notifications.png" width="47%"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/catppuccin.png"><img alt="" title="Custom theme" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/catppuccin.png" width="47%"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/thumbnail.png"><img alt="" title="Thumbnail mode" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/pages/thumbnail.png" width="47%"></a>
</p>
<p dir="auto">
<themed-picture data-catalyst-inline="true"><picture>
<img alt="" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/hr.png" width="100%">
</picture></themed-picture>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><em>Support Sniffnet's development</em> 💖</h2><a id="user-content-support-sniffnets-development-" aria-label="Permalink: Support Sniffnet's development 💖" href="#support-sniffnets-development-"></a></p>
<p dir="auto"><i>Sniffnet is completely free, open-source software which needs lots of effort and time to develop and maintain.</i></p>
<p dir="auto"><i>If you appreciate Sniffnet, <a href="https://github.com/sponsors/GyulyVGC">consider sponsoring</a>:
your support will allow me to dedicate more time to this project,
constantly expanding it including <a href="https://github.com/GyulyVGC/sniffnet/blob/main/ROADMAP.md">new features and functionalities</a>.</i></p>
<p dir="auto"><i>A special mention goes to these awesome organizations and folks who are sponsoring Sniffnet:</i></p>
<p dir="auto">
<a href="https://github.com/github" title="GitHub"><img src="https://avatars.githubusercontent.com/github?v=4" width="60px" alt="GitHub"></a>&nbsp;&nbsp;
<a href="https://nlnet.nl/" title="NLnet" rel="nofollow"><img src="https://camo.githubusercontent.com/9e7f127e3732d4322576aa2462ce899725954f576fa5c36543b38f684f53a6cd/68747470733a2f2f6e6c6e65742e6e6c2f6c6f676f2f6c6f676f2e737667" width="60px" alt="NLnet" data-canonical-src="https://nlnet.nl/logo/logo.svg"></a>&nbsp;&nbsp;
<a href="https://ipinfo.io/" title="IPinfo" rel="nofollow"><img src="https://avatars.githubusercontent.com/ipinfo?v=4" width="60px" alt="IPinfo"></a>&nbsp;&nbsp;
<a href="https://github.com/Cthulu201" title="Cthulu201"><img src="https://avatars.githubusercontent.com/Cthulu201?v=4" width="60px" alt="Cthulu201"></a>&nbsp;&nbsp;
<a href="https://github.com/0x0177b11f" title="Tiansheng Li"><img src="https://avatars.githubusercontent.com/0x0177b11f?v=4" width="60px" alt="Tiansheng Li"></a>&nbsp;&nbsp;
<a href="https://github.com/ZEROF" title="ZEROF"><img src="https://avatars.githubusercontent.com/ZEROF?v=4" width="60px" alt="ZEROF"></a>&nbsp;&nbsp;
<a href="https://www.janwalter.org/" title="Jan Walter" rel="nofollow"><img src="https://avatars.githubusercontent.com/wahn?v=4" width="60px" alt="Jan Walter"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><a href="#download"><img alt="Windows" title="Windows" height="35px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/windows.svg"></a></th>
<th><a href="#download"><img alt="macOS" title="macOS" height="35px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/macos.svg"></a></th>
<th><a href="#download"><img alt="Linux (.deb)" title="Linux (.deb)" height="35px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/linux_deb.svg"></a></th>
<th><a href="#download"><img alt="Linux (.rpm)" title="Linux (.rpm)" height="35px" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/repository/badges/linux_rpm.svg"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_Windows_64-bit.msi">64‑bit</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_Windows_32-bit.msi">32‑bit</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
<td><a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_macOS_Intel.dmg">Intel</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_macOS_AppleSilicon.dmg">Apple&nbsp;silicon</a></td>
<td><a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_amd64.deb">amd64</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_arm64.deb">arm64</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_i386.deb">i386</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxDEB_armhf.deb">armhf</a></td>
<td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxRPM_x86_64.rpm">x86_64</a>&nbsp;|&nbsp;<a href="https://github.com/GyulyVGC/sniffnet/releases/latest/download/Sniffnet_LinuxRPM_aarch64.rpm">aarch64</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Links in the table above will download the latest version of Sniffnet directly from <a href="https://github.com/GyulyVGC/sniffnet/releases">GitHub releases</a>. <br></p>

<p dir="auto"><strong>Alternative installation methods</strong> are reported in the following:</p>
<details>
  <summary>from Crates.io</summary>
<p dir="auto">Follow this method only if you have <a href="https://www.rust-lang.org/tools/install" rel="nofollow">Rust installed</a> on your machine. <br>
In this case, the application binary can be built and installed with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="cargo install sniffnet --locked"><pre>cargo install sniffnet --locked</pre></div>
</details>
<details>
  <summary>from Homebrew</summary>
<p dir="auto">You can install <a href="https://github.com/Homebrew/homebrew-core/pkgs/container/core%2Fsniffnet">Sniffnet Homebrew package</a> with:</p>

</details>
<details>
  <summary>from Nixpkgs</summary>
<p dir="auto">You can install <a href="https://search.nixos.org/packages?channel=23.05&amp;show=sniffnet&amp;from=0&amp;size=50&amp;sort=relevance&amp;type=packages&amp;query=sniffnet" rel="nofollow">Sniffnet Nix package</a> adding the following Nix code to your NixOS Configuration, usually located in <code>/etc/nixos/configuration.nix</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="environment.systemPackages = [
  pkgs.sniffnet
];"><pre><span>environment</span><span>.</span><span><span>systemPackages</span></span> <span>=</span> <span>[</span>
  <span>pkgs</span><span>.</span><span><span>sniffnet</span></span>
<span>]</span><span>;</span></pre></div>
<p dir="auto">Alternatively, you can install it in your home using <a href="https://github.com/nix-community/home-manager">Home Manager</a> with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="home.packages = [
  pkgs.sniffnet
];"><pre><span>home</span><span>.</span><span><span>packages</span></span> <span>=</span> <span>[</span>
  <span>pkgs</span><span>.</span><span><span>sniffnet</span></span>
<span>]</span><span>;</span></pre></div>
<p dir="auto">Alternatively, you can try it in a shell with:</p>

</details>
<details>
  <summary>on Arch Linux</summary>
<p dir="auto">You can install Sniffnet community package via <a href="https://wiki.archlinux.org/title/Pacman" rel="nofollow">pacman</a>:</p>

</details>
<details>
  <summary>on FreeBSD</summary>
<p dir="auto">You can install Sniffnet port with:</p>

</details>
<details>
  <summary>on NetBSD</summary>
<p dir="auto">You can install Sniffnet from the official repositories via <a href="https://pkgin.net/" rel="nofollow">pkgin</a>:</p>

</details>
<details>
  <summary>on Tiny Core Linux</summary>
<p dir="auto">You can install Sniffnet from the official repository with:</p>

</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>💻 choose a <strong>network adapter</strong> of your PC to inspect</li>
<li>🏷️ select a set of <strong>filters</strong> to apply to the observed traffic</li>
<li>📖 view overall <strong>statistics</strong> about your Internet traffic</li>
<li>📈 view <strong>real-time charts</strong> about traffic intensity</li>
<li>📌 keep an eye on your network even when the application is <strong>minimized</strong></li>
<li>📁 <strong>export</strong> comprehensive capture reports as <strong>PCAP files</strong></li>
<li>🔎 identify <strong>6000+ upper layer services</strong>, protocols, trojans, and worms</li>
<li>🌐 find out <strong>domain name</strong> and <strong>ASN</strong> of the hosts you are exchanging traffic with</li>
<li>🏠 identify connections in your <strong>local network</strong></li>
<li>🌍 get information about the country of remote hosts (<strong>IP geolocation</strong>)</li>
<li>⭐ save your <strong>favorite</strong> network hosts</li>
<li>🕵️‍♂️ search and <strong>inspect</strong> each of your network connections in real time</li>
<li>🔉 set <strong>custom notifications</strong> to inform you when defined network events occur</li>
<li>🎨 choose the <strong>style</strong> that fits you the most, including custom themes support</li>
<li>...and more!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">User manual</h2><a id="user-content-user-manual" aria-label="Permalink: User manual" href="#user-manual"></a></p>
<p dir="auto">Do you want to <strong>learn more</strong>? <br>
Check out the <a href="https://github.com/GyulyVGC/sniffnet/wiki"><strong>Sniffnet Wiki</strong></a>, a comprehensive manual to help you
thoroughly master the application from a basic setup to the most advanced functionalities. <br>
The Wiki includes step-by-step guides, tips, examples of usage, and answers to frequent questions.</p>
<p dir="auto">
<a href="https://github.com/GyulyVGC/sniffnet/wiki">
<img alt="" title="Sniffnet Wiki" src="https://raw.githubusercontent.com/GyulyVGC/sniffnet/main/resources/logos/wiki/wikilogo.svg" width="300px">
</a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<details>
  <summary>See details</summary>
<p dir="auto"><h3 tabindex="-1" dir="auto">Missing dependencies</h3><a id="user-content-missing-dependencies" aria-label="Permalink: Missing dependencies" href="#missing-dependencies"></a></p>
<p dir="auto">Most of the errors that may arise are likely due to your system missing dependencies
required to correctly analyze a network adapter. <br>
Check the <a href="https://github.com/GyulyVGC/sniffnet/wiki/Required-dependencies">required dependencies page</a>
for instructions on how to proceed depending on your operating system.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Rendering problems</h3><a id="user-content-rendering-problems" aria-label="Permalink: Rendering problems" href="#rendering-problems"></a></p>
<p dir="auto">In some circumstances, especially if you are running on an old architecture or your graphical drivers are not updated,
the <code>wgpu</code> default renderer used by <a href="https://github.com/iced-rs/iced">iced</a>
may manifest bugs (the interface glitches, color gradients are unsupported, or some icons are completely black). <br>
In these cases you can set an environment variable to switch to the <code>tiny-skia</code> renderer,
a CPU-only software renderer that should work properly on every environment:</p>

<p dir="auto"><h3 tabindex="-1" dir="auto"><em><strong>In any case, don't hesitate to <a href="https://github.com/GyulyVGC/sniffnet/issues/new/choose">open an issue</a>, and I will do my best to help you!</strong></em></h3><a id="user-content-in-any-case-dont-hesitate-to-open-an-issue-and-i-will-do-my-best-to-help-you" aria-label="Permalink: In any case, don't hesitate to open an issue, and I will do my best to help you!" href="#in-any-case-dont-hesitate-to-open-an-issue-and-i-will-do-my-best-to-help-you"></a></p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li>A big shout-out to <a href="https://github.com/GyulyVGC/sniffnet/blob/main/CONTRIBUTORS.md">all the contributors</a> of Sniffnet!</li>
<li>The graphical user interface has been realized with <a href="https://github.com/iced-rs/iced">iced</a>, a cross-platform GUI library for Rust focused on simplicity and type-safety</li>
<li>IP geolocation and ASN data are provided by <a href="https://www.maxmind.com/" rel="nofollow">MaxMind</a></li>
<li>Last but not least, thanks to <a href="https://github.com/GyulyVGC/sniffnet/stargazers">every single stargazer</a>: all forms of support made it possible to keep improving Sniffnet!</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What is interviewing like now with everyone using AI? (217 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=42909166</link>
            <guid>42909166</guid>
            <pubDate>Sun, 02 Feb 2025 15:19:32 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=42909166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="42911944"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911944" href="https://news.ycombinator.com/vote?id=42911944&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I've let people use GPT in coding interviews, provided that they show me how they use it. At the end I'm interested in knowing how a person solves a problem, and thinks about it. Do they just accept whatever crap the gpt gives them, can they take a critical approach to it, etc.</p><p>So far, everyone that elected to use GPT did much worse. They did not know what to ask, how to ask, and did not "collaborate" with the AI. So far my opinion is if you have a good interview process, you can clearly see who are the good candidates with or without ai.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912237"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912237" href="https://news.ycombinator.com/vote?id=42912237&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>We do the same thing. It's perfectly fine for candidates to use AI-assistive tooling provided that they can edit/maintain the code and not just sit in a prompt the whole time. The heavier a candidate relies on LLMs, the worse they often do. It really comes down to discipline.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912619"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912619" href="https://news.ycombinator.com/vote?id=42912619&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>same thing here. Interview is basically a representative thing of what we do, but also depends on the level of seniority. I ask people just to share the screen with me and use whatever you want / fell comfortable with. Google, ChatGPT, call your mom, I don't care as long as you walk me through how you're approaching the thing at hand. We've all googled tar xvcxfgzxfzcsadc, what's that permission for .pem is it 400, etc.. no shame in anything and we all use all of the things through day. Let's simulate a small task at hand and see where we end up at. Similarly, there is a bias where people leaning more on LLMs doing worse than those just googling or, gasp, opening documentation.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912579"><td></td></tr>
                  <tr id="42912049"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912049" href="https://news.ycombinator.com/vote?id=42912049&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My company, a very very large company, is transitioning back to only in-person interviews due to the rampant amount of cheating happening during interviews.</p><p>As an interviewer, it's wild to me how many candidates think they can get away with it, when you can very obviously hear them typing, then watching their eyes move as they read an answer from another screen. And the majority of the time the answer is incorrect anyway. I'm happy that we won't have to waste our time on those candidates anymore.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912304"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912304" href="https://news.ycombinator.com/vote?id=42912304&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>So depressing to hear that “because of rampant cheating”</p><p>As a person looking for a job, I’m really not sure what to do.  If people are lying on their resumes and cheating in interviews, it feels like there’s nothing I can do except do the same.  Otherwise I’ll remain jobless.</p><p>But to this day I haven’t done either.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912988"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912988" href="https://news.ycombinator.com/vote?id=42912988&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; it feels like there’s nothing I can do except do the same.</p><p>Why does it feel like that when you’re replying to someone who already points out that it doesn’t work? Cheating can prevent you from getting a job, and it can get you fired from the job too. It can also impede your ability to learn and level up your own skills. I’m glad you haven’t done it yet, just know that you can be a better candidate and increase your chances by not cheating.</p><p>Using an LLM isn’t cheating if the interviewer allows it. Whether they allow it or not, there’s still no substitute for putting in the work. Interviews are a skill that can (and should) be practiced. Candidates are rarely hired for technical skill alone. Attitude, communication, curiosity, and lots of other soft skills are severely underestimated by so many job seekers, especially those coming right out of school. A small amount of strengthening your non-code abilities can improve your odds much faster than leetcode ever will. And if you have time, why not do both?</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912752"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912752" href="https://news.ycombinator.com/vote?id=42912752&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Here's the thing: 95% of cheaters still suck, even when cheating. Its hard to imagine how people can perform so badly while cheating, yet they consistently do. All you need to do to stand out is not be utterly awful. Worrying about what other people are doing is more detrimental to your performance than anything else is. Just focus on yourself: being broadly competent, knowing your niche well, and being good at communicating how you learn when you hit the edges of your knowledge. Those are the skills that always stand out.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912987"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912987" href="https://news.ycombinator.com/vote?id=42912987&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Yeah, we found this when we started doing take-home exams: it turns out that a junior dev who spends twice as much time on the problem than what we asked them to doesn’t put out senior-level code - we could read the skill level in the code almost instantly. Same thing with cheating like that - it turns out knowing the answer isn’t the same thing as having experience, and it’s pretty obvious pretty quickly which one you’re dealing with.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912503"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912503" href="https://news.ycombinator.com/vote?id=42912503&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I don't know, I kind of feel like leetcode interviews are a situation where the employer is cheating. I mean, you're admittedly filtering out a great number of acceptable candidates knowing that if you just find 1 in a 1000, that'll be good enough. It is patently unfair to the individuals that are smart enough to do your work, but poor at some farcical representation of the work. That is cheating.</p><p>In my opinion, if a prospective employee is able to successfully use AI to trick me into hiring them, then that is a hell of a lot closer to the actual work they'll be hired to do (compared to leetcode).</p><p>I say, if you can cheat at an interview with AI, do it.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912641"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912641" href="https://news.ycombinator.com/vote?id=42912641&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>The employer sets the terms of the interview. If you don’t like them, don’t apply.</p><p>What you’re suggesting here isn’t any different than submitting a fraudulent resume because you disagree with the required qualifications.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912678"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912678" href="https://news.ycombinator.com/vote?id=42912678&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Yea, exactly.</p><p>If a candidate were up front with me and asked if they could use AI, or said they learned an answer from AI and then wanted to discuss it with me, I'd be happy with that. But attempting to hide it and pretend they aren't using it when our interview rules specifically ask you not to do it is just being dishonest, which isn't a characteristic of someone I want to hire.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912662"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912662" href="https://news.ycombinator.com/vote?id=42912662&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>On principle, what you’re saying has merit. In practice, the market is currently rife with employers submitting job postings with inflated qualifications, for positions that may or may not exist. So there’s bad actors all around and it’s difficult to tell who actually is behaving with integrity.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912863"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912863" href="https://news.ycombinator.com/vote?id=42912863&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I would like to be paid though. What do I care about the terms of the interview as long as they hire me?</p><p>What is being suggested here is not participating in the mind numbing process that is called ‘applying for a job’.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912953"><td></td></tr>
                        <tr id="42912771"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912771" href="https://news.ycombinator.com/vote?id=42912771&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I dunno why there is always the assumption in these threads that leetcode is being used. My company has never used leetcode-style questions, and likely never will.</p><p>I work in security, and our questions are pretty basic stuff. "What is cross-site scripting, and how would you protect against it?", "You're tasked with parsing a log file to return the IP addresses that appear at least 10 times, how would you approach this?" Stuff like that. And then a follow-up or two customized to the candidate's response.</p><p>I really don't know how we could possibly make it easier for candidates to pass these interviews. We aren't trying to trick people, or weed people out. We're trying to find people that have the foundational experience required to do the job they're being hired for. Even when people do answer them incorrectly, we try to help them out and give them guidance, because it's really about trying to evaluate how a person thinks rather than making sure they get the right answer.</p><p>I mean hell, it's not like I'm spending hours interviewing people because I get my rocks off by asking people lame questions or rejecting people; I <i>want</i> to hire people! I will go out of my way to advocate for hiring someone that's honest and upfront about being incorrect or not knowing an answer, but wants to think through it with me.</p><p>But cheating? That's a show stopper. If you've been asked to not use ChatGPT, but you use it anyway, you're not getting the benefit of the doubt. You're getting rejected and blacklisted.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912415"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912415" href="https://news.ycombinator.com/vote?id=42912415&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Note also "And the majority of the time the answer is incorrect anyway."</p><p>I haven't looked for development-related jobs this millennium, but it's unclear to me how effective a crutch AI is for interviews--at least for well-designed and run interviews. Maybe in some narrow domains for junior people.</p><p>As a few of us have written elsewhere, I consider not having in-person  interviews past an initial screen sheer laziness and companies generally deserve whoever they end up with.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912102"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912102" href="https://news.ycombinator.com/vote?id=42912102&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>When I was interviewing entry level programmers at my last job, we gave them an assignment that should only take a few hours, but we basically didn't care about the code at all.</p><p>Instead, we were looking to see if they followed instructions, and if they left anything out.</p><p>I never had a chance to test it out, since we hadn't hired anyone new in so long, but ChatGPT/etc would almost always fail this exam because of how bad it is at making sure everything was included.</p><p>And bad programmers also failed it.  It always left us with a few candidates that paid attention, and from there we figure if they can do that, they can learn the rest.  It seemed to work quite well.</p><p>I was recently laid off from that company, and now I'm realizing that I really want to see what current-day candidates would turn in.  Oh well.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912362"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912362" href="https://news.ycombinator.com/vote?id=42912362&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>For those tests I never follow the rules, I just make something quick and dirty because I refuse to spend unpaid hours. In the interview the first question is why I didnt follow the instructions, and they think my reason is fair.</p><p>Companies seem to think that we program just for fun and ask to make a full blown app... also underestimating the time candidates actually spend making it.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912830"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912830" href="https://news.ycombinator.com/vote?id=42912830&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>If you’re spending the time applying and submitting <i>something</i> then you might as well spend the extra 30 minutes or so to do it right, no?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912885"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912885" href="https://news.ycombinator.com/vote?id=42912885&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Any time someone says ‘should only take a few hours’ they’re far underestimating the time it actually takes.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912939"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912939" href="https://news.ycombinator.com/vote?id=42912939&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Not if you’re applying to hundreds, or thousands of jobs. Unless you know someone, it’s a quantity game.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42912207"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912207" href="https://news.ycombinator.com/vote?id=42912207&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>The industry (all industries really) might want to reconsider online applications, or at least privilege in-person resume drop-offs because the escalating ai application/evaluation war that's happening doesn't seem to be helping anyone.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912916"><td></td></tr>
                  <tr id="42912810"><td></td></tr>
                <tr id="42912907"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912907" href="https://news.ycombinator.com/vote?id=42912907&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>For the goal of the interview - showing your knowledge and skills - you are failing miserably. People know what LLMs can do, the interview is about you.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912842"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912842" href="https://news.ycombinator.com/vote?id=42912842&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I guess its more of a question if you can solve the problem without AI.</p><p>In most interview tasks you are not solving the task “with” ai.</p><p>Its AI who solves the task while you watch it do it.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42911155"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911155" href="https://news.ycombinator.com/vote?id=42911155&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My startup got acquired last year so I haven't interviewed anyone in a while, but my technical interview has always been:</p><p>- share your screen</p><p>- download/open the coding challenge</p><p>- you can use any website, Stack Overflow, whatever, to answer my questions as long as it's on the screenshare</p><p>My goal is to determine if the candidate can be technically productive, so I allow any programming language, IDE, autocompleter, etc, that they want.  I would have no problem with them using GPT/Copilot in addition to all that, as long as it's clear how they're solving it.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912005"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912005" href="https://news.ycombinator.com/vote?id=42912005&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I recently interviewed for my team and tried this same approach. I thought it made sense because I want to see how people can actually work and problem solve given all the tools at their disposal, just like on the job.</p><p>It proved to be awkward and clumsy very quickly. Some candidates resisted it since they clearly thought it would make them judged harsher. Some candidates were on the other extreme and basically tried asking ChatGPT the problem straight up, even though I clarified up front "You can even use ChatGPT as long as you're not just directly asking for the solution to the whole problem and just copy/pasting, obviously."</p><p>After just the initial batch of candidates it became clear it was muddying things too much, so I simply forbade using it for the rest of the candidates, and those interviews went much smoother.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912901"><td></td></tr>
                <tr id="42912925"><td></td></tr>
                  <tr id="42912631"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912631" href="https://news.ycombinator.com/vote?id=42912631&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Did you tell them that you “want to see how people can actually work and problem solve given all the tools at their disposal, just like on the job”? Just curious.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912263"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912263" href="https://news.ycombinator.com/vote?id=42912263&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>If you really don't penalize them for this, you should clearly state it. Some people may still think they'll be penalized as that is the norm.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912103"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912103" href="https://news.ycombinator.com/vote?id=42912103&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I did this while hiring last year and the number of candidates who got stuff wrong because they were too proud to just look up the answer was shocking.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912228"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912228" href="https://news.ycombinator.com/vote?id=42912228&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Is it pride or is it hard to shake the (reasonable, I'd say) fear the reviewer will judge regardless of their claims?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912527"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912527" href="https://news.ycombinator.com/vote?id=42912527&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Exactly. You never know. Some interviewers will penalize you for not having something memorized and having to look it up, some will penalize you for guessing, some will penalize you for simply not knowing and asking for help. Some interviewers will penalize you for coming up with something quick and dirty and then refining it, some will penalize you for jumping right to the final product. There's no consistency.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42911656"><td></td></tr>
            <tr id="42912070"><td></td></tr>
            <tr id="42911761"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911761" href="https://news.ycombinator.com/vote?id=42911761&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I'd be fine with the GPT side of things, as long as I could somehow inject poor answers, and see if the interviewee notices and corrects.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911804"><td></td></tr>
            <tr id="42912058"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912058" href="https://news.ycombinator.com/vote?id=42912058&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>the trick is to phrase the problem in a way that GPT4 will always give the incorrect answer (due to vagueness of your problem) and that multiple rounds of guiding/correcting are needed to solve.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912144"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912144" href="https://news.ycombinator.com/vote?id=42912144&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>That's pretty good because it can exhaust the context window quickly and then it starts spiraling out of control, which would require the candidate to act.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912469"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912469" href="https://news.ycombinator.com/vote?id=42912469&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>If you only use ChatGPT to code, you are only able to copy paste the llm emitted code, then you ask for changes to the code (to reflect for example the evolution of the product)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42911909"><td></td></tr>
                <tr id="42912021"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912021" href="https://news.ycombinator.com/vote?id=42912021&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>It's pretty obvious when someone's input focus changes to nothing or when their mouse leaves the screen entirely, or you could just ask to see the display settings to begin. Doesn't solve for multiple computers but it's pretty obvious in real time when someone's actual attention drifts or they suddenly have abilities they didn't have before.</p><p>Either way, screen sharing beats whiteboards. Even if we throw our hands up and give up, we'll be firing frauds before the probationary period ends.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912603"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912603" href="https://news.ycombinator.com/vote?id=42912603&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>There is nothing fraudulent about using LLMs. If people can use them on the job, it's okay to use them on the interview. They're the calculators of tomorrow if not of today.</p><p>Interviewing just needs to adapt such as by assessing one's open source  projects and contributions. Not much more is needed. And if the candidate completely misrepresents their open source profile, this can be handled by an initial contract-to-hire period.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912848"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912848" href="https://news.ycombinator.com/vote?id=42912848&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I agree that there's nothing fraudulent with using a tool you would use on the job when you are interviewing. But in no way are LLMs equivalent to calculators. Calculators actually <i>give the correct answer</i> reliably, unlike LLMs. A sporadically reliable tool is worse than no tool at all.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912656"><td></td></tr>
                                    <tr id="42912517"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912517" href="https://news.ycombinator.com/vote?id=42912517&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I've always just tried to hold a conversation with the candidate, what they think their strengths are weaknesses are and a little probing.</p><p>This works especially well if <i>I</i> don't know the area they're strongest in, because then they get to explain it to me. If I don't understand it then it's a pretty clear signal that they either don't understand it well enough or are a poor communicator. Both are dealbreakers.</p><p>Otherwise, for me, the most important thing is gauging: Aptitude, Motivation and Trustworthiness. If you have these three attributes then I could not possibly give a shit that you don't know how kubernetes operators work, or if you can't invert a binary tree.</p><p>You'll learn when you need it; it's not like the knowledge is somehow esoteric or hidden.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42910568"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910568" href="https://news.ycombinator.com/vote?id=42910568&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Part of my resume review process is trying to decide if I can trust the person. If their resume seems too AI-generated, I feel less like I can trust that candidate and typically reject the candidate.</p><p>Once you get to the interview process, it's very clear if someone thinks they can use AI to help with the interview process. I'm not going to sit here while you type my question into OpenAI and try to BS a meaningful response to my question 30 seconds later.</p><p>AI-proof interviewing is easy if you know what you're talking about. Look at the candidates resume and ask them to describe some of their past projects. If they can have a meaningful conversation without delays, you can probably trust their resume. It's easy to spot BS whether AI is behind it or not.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42910796"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910796" href="https://news.ycombinator.com/vote?id=42910796&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>This, and tbh this has always been the best way. Someone who has projects, whether personal or professional, and has the capability to discuss those projects in depth and with passion will usually be a better employee than a leet code specialist.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911672"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911672" href="https://news.ycombinator.com/vote?id=42911672&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Doesn't even have to be a project per se, if they can discuss some sort of technical topic in depth (i.e. the sort of discussion you might have when discussing potential solutions to a problem) then that's a great sign imo.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911754"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911754" href="https://news.ycombinator.com/vote?id=42911754&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Good interviews are a conversation, a dialog to uncover how the person thinks, how they listen, how they approach problems and discuss. Also a bit detail knowledge, but that's only a minor component in the end. Any interview where AI in its current form helps is not good anyway. Keep in mind that in our industry, the interview goes both ways. If the candidate thinks your process is bad then they are less inclined to join your company because they know that their coworkers will have been chosen by a subpar process.</p><p>That said, I'm waiting for an "interview assistant" product. It listens in to the conversation and silently provides concise extra information about the mentioned subjects that can be quickly glanced at without having to enter anything. Or does this already exist?</p><p>Such a product could be useful for coding to. Like watching me over the shoulder and seeing aha, you are working with so-and-so library, let me show you some key parts of the API in this window, or you are trying to do this-and-that, let me give you some hints. Not as intrusive as current assistants that try to write code for you, just some proactive lookup without having to actively seek out information. Anybody knows a product for that?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912560"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912560" href="https://news.ycombinator.com/vote?id=42912560&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I'm pretty sure I've been in an interview with an 'interview assistant' and that it was another person.</p><p>This was 2-3 years ago in a remote interview. The candidate would hear the question, BS us a bit and then sometimes provide a good answer.</p><p>But then if we asked follow up questions they would blow those.</p><p>They also had odd 'AV issues' which were suspicious.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912178"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912178" href="https://news.ycombinator.com/vote?id=42912178&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>That might be good for newbie developers but for the rest of us it'll end up being the Clippy of AI assistants. If I want to know more about an API I'm using, I'll Google (or ask ChatGPT) for details; I don't need an assistant trying to be helpful and either treating me like a child, or giving me info that maybe right but which I don't need at the moment.</p><p>The only way I can see that working is if it spends hundreds of hours watching you to understand what you know and don't know, and even then it'll be a bit of a crap shoot.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911301"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911301" href="https://news.ycombinator.com/vote?id=42911301&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Agreed. This is why - while I won't ding an applicant for not having a public Github, I'm always happy when they do because usually they'll have some passion projects on there that we can discuss.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911949"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911949" href="https://news.ycombinator.com/vote?id=42911949&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I have 23 years of experience and I am almost invisible on GitHub, and for all those years I've been fired from 4 contracts due to various disconnects (one culture mis-fit and two under-performances due to illness I wasn't aware of at the time, and one because the company literally restructured over the weekend and fired 80% of all engineers), and I have been contracting a lot in the last 10 years (we're talking 17-19 gigs).</p><p>If you look solely at my GitHub you'd likely reject me right away.</p><p>I wish I had the time and energy for passion projects in programming. I so wish it was so. But commercial work has all but destroyed my passion for programming, though I know it can be rekindled if I can ever afford to take a properly long sabbatical (at least 2 years).</p><p>I'll more agree with your parent / sibling comments: take a look at the resume and look for bad signs like too vanilla / AI language, too grandiose claims (though when you are experienced you might come across as such so 50/50), or almost no details, general tone etc.</p><p>And the best indicator is a video call conversation, I found as a candidate. I am confident in what I can do (and have done), I am energetic and love to go for the throat of the problems on my first day (provided the onboarding process allows for it) and it shows -- people have told me that and liked it.</p><p>If we're talking passion, I am more passionate about taking a walk with my wife and discussing the current book we're reading, or getting to know new people, or going to the sauna, or wondering what's the next meetup we should be going to, stuff like that. But passion + work, I stand apart by being casual and not afraid of any tech problems, and by prioritizing being a good teammate first and foremost (several GitHub-centric items come to mind: meaningful PR comments and no minutiae, good commit messages, proper textual comment updates in the PR when f.ex. requirements change a bit, editing and re-editing a list of tasks in the PR description).</p><p>I already do too much programming. Don't hold it against me if I don't live on the computer and thus have no good GitHub open projects. Talk to me. You'll get much better info.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912205"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912205" href="https://news.ycombinator.com/vote?id=42912205&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>To add to this, lots of senior people in the consultanting world are brought in under escalations.  They often have to hide the fact they are an external resource.</p><p>Also if you have a novel or disclosure sensitive passion project, GitHub may be avoided even as a very conservative bright line.</p><p>As stated above I think it can be good to find common points to enhance the interview process, but make sure to not use it as a filter.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911799"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911799" href="https://news.ycombinator.com/vote?id=42911799&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Also because most people are busy with actual work and don't have the time to have passion projects. Some people do, and that's great, but most people are simply not passionate about labor, regardless of what kind of labor it is.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42910842"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910842" href="https://news.ycombinator.com/vote?id=42910842&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>There are much more sophisticated methods than that now with AI, like speech to text to LLM. It's getting increasingly harder to detect interviewees cheating.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911031"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911031" href="https://news.ycombinator.com/vote?id=42911031&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I think GP's point is that this says as much about the interview design and interviewer skill as it does about the candidate's tools.</p><p>If you do a rote interview that's easy to game with AI, it will certainly be harder to detect them cheating.</p><p>If you have an effective and well designed open ended interview that's more collaborative, you get a lot more signal to filter the wheat from the chaff.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911570"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42911570" href="https://news.ycombinator.com/vote?id=42911570&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; <i>If you have an effective and well designed open ended interview that's more collaborative, you get a lot more signal to filter the wheat from the chaff.</i></p><p>I understood their point but my point is a direct opposition to theirs, that at some point with AI advances this will essentially become impossible. You can make it as open ended as you want but if AI continues to improve, the human interviewee can simply act as a ventriloquist dummy for the AI and get the job. Stated another way, what kind of "effective and well designed open ended interview" can you make that would not succumb to this problem?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911872"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42911872" href="https://news.ycombinator.com/vote?id=42911872&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; at some point with AI advances this will essentially become impossible.</p><p>In-person interviews, second round comes with a plane ticket.  This used to be the norm.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911739"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42911739" href="https://news.ycombinator.com/vote?id=42911739&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>This is called fraud, and it is a crime.</p><p>People don't really call the police, nor sue over this.  But they can, and have in the past.</p><p>If it gets bad, look for people starting to seek legal recourse.</p><p>People aren't developers with 5 years experience, if all they can do is copy and paste.  Anyone fraudulently claiming so is a scam artist, a liar, and deserves jail time.</p><p>So you create an interview process that can only be passed by a skilled dev, including them signing a doc saying the code is entirely their work, only referencing a language manual/manpages.</p><p>And if they show up to work incapable of doing the same, it's time to call the cops.</p><p>That's probably the only way to deal with scam artists and scum, going forward.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911871"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_42911871" href="https://news.ycombinator.com/vote?id=42911871&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Can you cite case law around where some one misrepresented their capabilities in a job interview and were criminally prosecuted? Like what criminal statute specifically was charged? You won’t find it, because at worst this would fall under a contract dispute and hence civil law. Screeching “fraud is a crime” hysterically serves no one.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912395"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_42912395" href="https://news.ycombinator.com/vote?id=42912395&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Fraud can be described as deceit to profit in some way.  You may note the rigidity of the process above, where I indicated a defined set of conditions.</p><p>It costs employers money to on board someone, not just in pay, but in other employees training that person.  Obviously the case must be clear cut, but I've personally hired someone who clearly cheated during the remote phone interview, and literally couldn't even code a function in any language in person.</p><p>There are people with absolutely no background as a coder, applying to jobs with 5 years experience, then fraudulently misrepresenting the work of others at their own, to get the job.</p><p>That's fraud.</p><p>As I said, it's not being prosecuted as such now.  But if this keeps up?</p><p>You can bet it will be.</p><p>Because it is fraud.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912972"><td></td></tr>
                        <tr id="42912073"><td></td></tr>
                                          <tr id="42912709"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912709" href="https://news.ycombinator.com/vote?id=42912709&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>With AI making traditional coding problems trivial, tech interviews are shifting toward practical, real-world challenges, system design, and debugging exercises rather than pure algorithm puzzles. Some companies are revisiting in-person whiteboarding to assess thought processes, while others embrace AI, evaluating how candidates integrate it into their workflow. There's also a greater focus on explaining decisions, trade-offs, and collaboration. Instead of banning AI, many employers now test how effectively candidates use it while ensuring they have foundational skills. The trend favors assessing problem-solving in real work scenarios rather than just coding ability under artificial constraints.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911831"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911831" href="https://news.ycombinator.com/vote?id=42911831&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>The traditional tech interview was always designed to optimize for reliably finding someone who was willing to do what they were told even if it feels like busywork. As a rule someone who has the time and the motivation to brush up on an essentially useless skill in order to pass your job interview will likely fit nicely as a cog in your machine.</p><p>AI doesn't just change the interviewing game by making it easy to cheat on these interviews, <i>it should be changing your hiring strategy altogether</i>. If you're still thinking in terms of optimizing for cogs, you're missing the boat—unless you're hiring for a very short term gig what you need now is someone with high creative potential and great teamwork skills.</p><p>And as far as I know there is no reliable template interview for recognizing someone who's good at thinking outside the box and who understands people. You just have to talk to them: talk about their past projects, their past teams, how they learn, how they collaborate. And then you have to get good at understanding what kinds of answers you need for the specific role you're trying to fill, which will likely be different from role to role.</p><p>The days of the interchangeable cog are over, and with them easy answers for interviewing.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911932"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911932" href="https://news.ycombinator.com/vote?id=42911932&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Have you spent a lot of time trying to hire people? I guarantee you there is no shadow council trying to figure out how to hire "busywork" worker bees. This perspective smells completely like "If I were in charge, things would be so much better." Guess what? If you were to take your idea and try to lead this change across a 100 people engineering org, there would be "out of the box thinkers" who would go against your ideas and cause dissent. At that point, guess what? You're going to figure out how to hire compliant people who will execute on your strategy.</p><p>"talk about their past projects, their past teams, how they learn, how they collaborate"</p><p>You have now excluded amazing engineers who suck at talking about themselves in interviews. They may be great collaborators and communicators, but freeze up selling themselves in an interview.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912575"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912575" href="https://news.ycombinator.com/vote?id=42912575&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; You have now excluded amazing engineers who suck at talking about themselves in interviews. They may be great collaborators and communicators, but freeze up selling themselves in an interview.</p><p>This was the norm until perhaps for about the last 10-15 years of Software Engineering.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912089"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912089" href="https://news.ycombinator.com/vote?id=42912089&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My take is:</p><p>- “big” tech companies like Google, Amazon, Microsoft came up with these types of tech interviews. And there it seems pretty clear that for most of their positions they are looking for cogs</p><p>- The vast majority of tech companies have just copied what “big” tech is doing, including tech interviews. These companies may not be looking for cogs, but they are using an interview process that’s not suitable for them</p><p>- Very few companies have their own interview process suitable for them. These are usually small companies and therefore  the number of engineers in such companies is negligible to be taken into account (most likely, less than 1% of the audience here work at such companies)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912409"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912409" href="https://news.ycombinator.com/vote?id=42912409&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; I guarantee you there is no shadow council trying to figure out how to hire "busywork" worker bees.</p><p>The council itself is made of "busywork" worker bees. Slave hiring slaves - the vast majority of IT interviewers and candidates are idiot savants - they know very little outside of IT, or even realize  that there is more to life than IT.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42910806"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910806" href="https://news.ycombinator.com/vote?id=42910806&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>The key is having interviewers that know what they are talking about so in-depth meandering discussions can be had regarding personal and work projects which usually makes it clear whether the applicant knows what they are talking about. Leetcode was only ever a temporary interview technique, and this 'AI' prominence in the public domain has simply sped up it's demise.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911989"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911989" href="https://news.ycombinator.com/vote?id=42911989&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>This completely..</p><p>You ask a rote question and you'll get a rote answer while the interviewee is busy looking at a fixed point on the screen.</p><p>You then ask a pointed question about something they know or care about, and suddenly their face lights up, they're animated, and they are looking around.</p><p>It's a huge tell.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912589"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912589" href="https://news.ycombinator.com/vote?id=42912589&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>You know, this makes me wonder if a viable remote interview technique, at least until real-time deepfaking gets better, would be to have people close their eyes while talking to them. For somebody who knows their stuff it'll have zero impact; for someone relying entirely on GPT, it will completely derail them.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42910923"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910923" href="https://news.ycombinator.com/vote?id=42910923&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>This is the way. We do an intro call, an engineering chat (exactly as you describe), a coding challenge and 2 team chat sessions in person. At the end of that, we usually have a good feeling about how sharp the candidate is, of they like to learn and discover new things, what their work ethic is. It's not bullet proof, but it removes a lot of noise from the signal.</p><p>The coding challenge is supposed to be solved with AI. We can no longer afford not to use LLMs for engineering, as it's that much of a productivity boost when used right, so candidates should show how they use LLMs. They need to be able to explain the code of course, and answer questions about it, but for us it's a negative mark of a candidate proclaims that they don't use LLMs.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911061"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911061" href="https://news.ycombinator.com/vote?id=42911061&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; <i>The coding challenge is supposed to be solved with AI. We can no longer afford not to use LLMs for engineering, as it's that much of a productivity boost when used right, so candidates should show how they use LLMs. They need to be able to explain the code of course, and answer questions about it, but for us it's a negative mark of a candidate proclaims that they don't use LLMs.</i></p><p>Do you state this upfront or is it some hidden requirement? Generally I'd expect an interview coding exercise to not be done with AI, but if it's a hidden requirement that the interviewer does not disclose, it is unfair to be penalized for not reading their minds.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911225"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42911225" href="https://news.ycombinator.com/vote?id=42911225&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I would say as long as it is stated you can complete the coding exercise using any tool available it is fine. I do agree, no task should be a trick.</p><p>I am personally of the view you should be able to use search engines, AI, anything you want, as the task should be representative of doing the task in person. The key focus has to be the programmer's knowledge and why they did what they did.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911622"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42911622" href="https://news.ycombinator.com/vote?id=42911622&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>One client of mine has a couple repositories for non-mission critical things like their fork of an open source project, decommissioned microservices, a SVG generator for their web front-end, etc.</p><p>They also take this approach of "whatever tool works," but their coding test is "here's some symptoms of the SVG generator misbehaving, figure out what happened and fix it," which requires digging into the commit history, issues, actually looking at the SVG output, etc.</p><p>Once you've figured out how the system architecture works, and the most likely component to be causing the problem, you have to convert part of the code to use a newer, undocumented API exposed by a RPC server that speaks a serialization format that no LLM has ever seen before.  Doing this is actually way faster and accurate using an AI, if you know how to centaur with it and make sure the output is tested to be correct.</p><p>This is a much more representative test of how someone's going to handle doing actual work knocking issues out.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911707"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42911707" href="https://news.ycombinator.com/vote?id=42911707&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Reminds me of the old joke/story where the Caltech student asks, "Can we use Feynman in this open-book exam?"</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911995"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42911995" href="https://news.ycombinator.com/vote?id=42911995&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Well, the challenge involves using a python LLM framework to build a simple RAG system for recipes.</p><p>It's not a hidden requirement per se to use LLM assistance, but the candidate should have a good answer ready why they didn't use an LLM to solve the challenge.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912221"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912221" href="https://news.ycombinator.com/vote?id=42912221&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Why is it a negative that the candidate can solve the challenge without using an LLM? I don’t really understand this.</p><p>Also, what is a good answer for not using one? Will you provide access to one during the course of the interview? Or I am just expected to be paying for one?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912665"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_42912665" href="https://news.ycombinator.com/vote?id=42912665&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>It's not negative that the candidate can solve it without an LLM, but it is positive if the candidate can use the LLM to speed up the solution. The code challenge is timeboxed.</p><p>We are providing an API key for LLM inference, as implementing the challenge requires this as well.</p><p>And I haven't heard a good answer yet for not using one, ideally the candidate knows how to mitigate the drawbacks of LLMs while benefiting from their utility regardless.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912695"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_42912695" href="https://news.ycombinator.com/vote?id=42912695&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt;I haven’t heard a good answer for not using one</p><p>Again, what would be a good answer? Or are you just saying there isn’t one?</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912075"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912075" href="https://news.ycombinator.com/vote?id=42912075&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Ah so you expect mind readers who can divine something from your brain that goes against 99.99% of interviewers' practices and would get them instantly disqualified from an overwhelming majority of interviews. Nice work good luck finding candidates.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912651"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912651" href="https://news.ycombinator.com/vote?id=42912651&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; as it's that much of a productivity boost when used right</p><p>Frankly, if an interviewer told me this, I would genuinely wonder why what they're building is such a simple toy product that an LLM can understand it well enough to be productive.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912328"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912328" href="https://news.ycombinator.com/vote?id=42912328&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>On our side we've transitioned to only in person interviews.</p><p>The biggest thing I've noticed is take home challenges have lost all value. Since GPT can plausibly solve almost anything you throw at it, and it doesn't give you any indication of how the candidate thinks.</p><p>And to be fair, I want a candidate that uses GPT / Cursor / whatever tools get the job done. But reading the same AI solution to a coding challenge doesn't tell me anything about how they think or approach problems.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912498"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912498" href="https://news.ycombinator.com/vote?id=42912498&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I'm not a fan of take-home challenges anyway (for the most part). Anything non-trivial is a big time suck and you <i>know</i> some people will spend all weekend on you two hour assignment.</p><p>Sometimes you have to. In my previous analyst stint a writing sample was pretty non-negotiable unless they could oint to publicly-published material--which was much preferred. ChatGPT isn't much use there except to save some time. It's very formulaic and wouldn't pass though, honestly, some people are worse on their own.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911710"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911710" href="https://news.ycombinator.com/vote?id=42911710&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I don’t know the answer, but I’d like to share that I asked a simple question about scheduling a phone interview to learn more about a candidate.</p><p>The candidate’s first response? “Memory updated”. That led to some laughs internally and then a clear rejection email.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911725"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42911725" href="https://news.ycombinator.com/vote?id=42911725&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My first read of this was they made a joke (not wise when scheduling for interviews sure but maybe funny) by intentionally responding that way.</p><p>This is because my brain couldn't fathom what is likely the reality here -- that someone was just pumping your email thru AI and pumping the response back unedited and unsanitized, and so the first thing you got back was just the first "part" of the AI response.</p><p>...Christ.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911928"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911928" href="https://news.ycombinator.com/vote?id=42911928&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I'm with you. Looking at the way people respond online to things now since LLMs and GenAI went mainstream is baffling. So many comments along the lines of "this is AI" when there are more ordinary explanations.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911896"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911896" href="https://news.ycombinator.com/vote?id=42911896&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Yeah I don't know about this specific situation, but as someone who is on the job market, is a good developer, but can come off as a little odd sometimes, I often wonder how often I roll a natural 1 on my Cha check and get perceived as an AI imposter.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912215"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912215" href="https://news.ycombinator.com/vote?id=42912215&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>If anything, coming across as “a little odd” can be a sign I’m actually talking to a human.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912524"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_42912524" href="https://news.ycombinator.com/vote?id=42912524&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>That's a good point. The major LLMs are all tilted so much towards a weird blend of corpo-speak with third-world underpaid English speaker influence (e.g. "delve", from common Nigerian usage) that having any quirks at all outside that is a good sign.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912183"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912183" href="https://news.ycombinator.com/vote?id=42912183&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Your perception of the reality is spot on. For this round I was hiring for entry level technical support and we had limited time to properly vet candidates.</p><p>Unfortunately what we end up doing is have to make some assumptions. If something seems remotely fishy, like that “Memory updated” or typeface change (ChatGPT doesn’t follow your text formatting when pasting into your email compose window), it raises a lot of eyebrows and very quickly leads to a rejection. There’s other cases where your written English is flawless but your phone interview indicates you don’t understand the English language compared to when we correspond over email/Indeed/etc.</p><p>Mind you, this is all before we even get to the technical knowledge part of any interview.</p><p>On a related hire, I am also in the unfortunate position where we may have to let a new CS grad go because it seemed like every code change and task we gave him was fully copy/pasted through ChatGPT. When presented with a simple code performance and optimization bug, he was completely lost on general debugging practices which led our team to question his previous work while onboarding. Using AI isn’t against company policy (see: small team with limited resources), but personally I see over reliance on ChatGPT as much, much worse than blindly following Stack Overflow.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912326"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912326" href="https://news.ycombinator.com/vote?id=42912326&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; typeface change</p><p>Long live plain text email.</p><pre><code>    ()  ascii ribbon campaign - against HTML e-mail 
    /\  www.asciiribbon.org   - against proprietary attachments</code></pre></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912401"><td></td></tr>
                        <tr id="42912841"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912841" href="https://news.ycombinator.com/vote?id=42912841&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>A friend of mine works with industrial machines, and once was tasked with translating machine's user's manual, even though he doesn't speak English. I do, and I had some free time, so I helped him. As an example, I was given user manual for a different, but similar machine.</p><p>1. The manual was mostly a bunch of phrases that were grammatically correct, but didn't really convey much meaning</p><p>2. The second half of the manual talked about a different machine than the first half</p><p>3. It was full of exceptionally bad mistranslations, and to this day "trained signaturee of the employee" is our inside joke</p><p>Imagine asking ChatGPT to write a manual except ChatGPT has down syndrome and a heart attack so it gives you five pages of complete bullshit. That was real manual that got shipped a 100 000€ or so machine. And nobody bothered to proofread it even once.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912938"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912938" href="https://news.ycombinator.com/vote?id=42912938&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I once worked in the US for a Japanese company that had their manuals "translated" into English and then sent on for polishing. Like the parent, it would be mostly "<i>a bunch of phrases that were grammatically correct, but didn't really convey much meaning</i>" .  I couldn't spend more than an hour a day on that kind of thing; more than that and it would start to make sense.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42912647"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912647" href="https://news.ycombinator.com/vote?id=42912647&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>We haven't seen major issues with AI with candidates on camera. The couple that have tried to cheat have done so rather obviously, and the problem we use is more about problem-solving than it is about reverse-a-linked-list.</p><p>This is borne out by results downstream with clients. No client we've sent more than a couple of people has ever had concerns about quality, so we're fairly confident that we are in fact detecting the cheating that is happening with reasonable consistency.</p><p>I actually just looked at our data a few days ago to see how candidates who listed LLMs or related terms on their resume did on our interview. On average, they did much worse (about half the pass rate, and double the hard-fail rate). I suspect this is a general "corporate BS factor" and not anything about LLMs specifically, but it's certainly relevant.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42910524"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910524" href="https://news.ycombinator.com/vote?id=42910524&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>As someone currently job searching it hasn’t changed much, besides companies adding DO NOT USE AI warnings before every section. Even Anthropic forces you to write a little “why do you want to work here DO NOT USE AI” paragraph. The irony.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911230"><td></td></tr>
                <tr id="42911728"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911728" href="https://news.ycombinator.com/vote?id=42911728&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Applying at Anthropic was a bad experience for me. I was invited to do a timed set of leetcode exercises on some website. I didn't feel like doing that, and focused on my other applications.</p><p>Then they emailed me a month later after my "invitation" expired. It looked like it was written by a human: "Hey, we're really interested in your profile, here's a new invite link, please complete this automated pre-screen thingie".</p><p>So I swallowed my pride and went through with that humiliating exercise. Ended up spending two hours doing algorithmic leetcode problems. This was for a product security position. Maybe we could have talked about vulnerabilities that I have found instead.</p><p>I was too slow to solve them and received some canned response.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912150"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42912150" href="https://news.ycombinator.com/vote?id=42912150&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>fyi, that's because (from experience) the last job req I publicly posted generated almost 450 responses, and (quite generously) over a third were simply not relevant.  It was for a full-stack rails eng.  Here, I'm not even including people whose experience was django or even React; I mean people with no web experience at all, or were not in the time zone requested.  Another 20% or so were nowhere near the experience level (senior) requested either.</p><p>The price of people bulk applying with no thought is I have to bulk filter.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912198"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42912198" href="https://news.ycombinator.com/vote?id=42912198&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>So you allow yourself to use AI in order to save time, but we have to put up with the shit[1] companies make up? That's good, it's for the best if I don't work for a company that thinks so lowly of its potential candidates.</p><p>[1]: Including but not limited to: having to manually fill a web form because the system couldn't correctly parse a CV; take-home coding challenges; studying for LeetCode interviews; sending a perfectly worded, boot-licking cover letter.</p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="42910879"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910879" href="https://news.ycombinator.com/vote?id=42910879&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Changed enormously. Both resumes and interviews are effectively useless now. If our AI agents can't find a portfolio of original work nearly exactly what we want to hire you for then you aren't ever going to hear from us. If you are one of the 1 in 4000 applications who gets an interview then you're already 70% likely to get an offer and the interview is mostly a formality.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912000"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912000" href="https://news.ycombinator.com/vote?id=42912000&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>What worked for me is just ignoring the job listing websites, and calling recruiters directly on the phone. Don’t bother hitting “easy apply” just scroll to the bottom and call the number.</p><p>I’ve also been asked for the first time in ages to come to the companies office to do interviews.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911194"><td></td></tr>
            <tr id="42910904"><td></td></tr>
                <tr id="42911247"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911247" href="https://news.ycombinator.com/vote?id=42911247&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I thought that meant what you typically write in the "Experience" section. GP, am I wrong?</p><p>Is everyone writing a "Projects" section by rewording what they wrote in "Experience"?! For me, "Projects" should strictly be personal projects. If not, maybe that's what I'm missing.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42911864"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_42911864" href="https://news.ycombinator.com/vote?id=42911864&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Projects are personal projects, or at least projects in which you did a distinguishable effort.</p><p>They don't have to be public to the whole world, you can have links that are only in your resume.</p><p>But if they're on GitHub, they have to be public, since there aren't unlisted repositories.</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42912608"><td></td></tr>
                  <tr id="42910984"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910984" href="https://news.ycombinator.com/vote?id=42910984&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I recently reviewed a medium-complexity assignment—just questions, no coding—and out of six candidates, I only approved one. The others were disqualified because their answers were filled with easily identifiable ChatGPT-generated fluff.</p><p>And I had made it clear that they should use their own words.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912725"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912725" href="https://news.ycombinator.com/vote?id=42912725&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>My company actually encourages the use of AI. My interview process was one relatively complex take home, an explanation of my solutions and thinking, then a live "onsite" (via zoom) where I had to code in front of a senior engineer while thinking aloud.</p><p>If I was incompetent, I could've shoved the problem into o1 on ChatGPT and probably solved the problems, but I wouldn't have been able to provide insight into why I made the design choices I made and how I optimized my solutions, and that would've ultimately gotten me thrown out of the candidate pool.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42909842"><td></td></tr>
                <tr id="42911641"><td></td></tr>
                <tr id="42911803"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_42911803" href="https://news.ycombinator.com/vote?id=42911803&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>&gt; hiring market tightened up... that doesn't mean there isn't one</p><p>tightened market is one thing, the absolute insanity of the recruitment process in last couple years with now AI thrown into the mix is really something to behold, test these waters at your own peril</p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="42911854"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911854" href="https://news.ycombinator.com/vote?id=42911854&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I don't understand why an interviewer would ban the use of AI if they are allowed to use AI in the role.</p><p>The interview is a chance to see how a candidate performs in a work like environment. Let them use the tools they will use on the job and see how well they can perform.</p><p>Even for verbal interviews, if they are using ChatGPT on the side and can manage the conversation satisfactorily then more power to them.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912422"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912422" href="https://news.ycombinator.com/vote?id=42912422&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Run competitions. If you're hiring fresh grads, this is probably the best way to filter by skill. If you can use AI to beat all the other candidates that's a skill by itself. In practice, those that use AI rarely ever make it into the top 10. Add a presentation/demo as part of the criteria to filter out those with bad communication skills.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912195"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912195" href="https://news.ycombinator.com/vote?id=42912195&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Shouldn't a portfolio of personal projects be enough ? In the past couple years I:</p><p>- adapted Java's Regex engine to work on streams of characters</p><p>- wrote a basic parametric geometry engine</p><p>- wrote a debugger for an async framework</p><p>- did innovative work with respect to whole-codebase transformation using macros</p><p>Among other things.</p><p>As for ChatGPT in the context of an interview, I'd only use it if I were asked to do changes on a codebase I don't know in limited time.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912273"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912273" href="https://news.ycombinator.com/vote?id=42912273&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I think the arguments is there is no way to validate it was you that did the work. There's been too many instances of groups that do interviews for others or the work for take homes to help get people placed. There was a big deal about some H1Bs a while back where the people that showed up didn't look anything like the people that interviewed. So I understand both sides.</p><p>It's frustrating though when you've done a lot of work, as you've listed. I think in a good interview maybe going over that code and getting the chance to explain things you did, why you did, or issues you had, could also go a long way.</p><p>Interviewing is tough, more so at scale.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912305"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912305" href="https://news.ycombinator.com/vote?id=42912305&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Bit annoying is that when companies ask for a portfolio, they often mean GitHub.  Lot of non-technical hiring people I discussed this with were confused by the fact that there are other ways to contribute, like mailing lists.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912344"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912344" href="https://news.ycombinator.com/vote?id=42912344&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>For the time being, I’ve banned LLMs in my interviews.</p><p>I want to see how the candidate reasons about code. So I try to ask practical questions and treat them like pairing sessions.</p><p>- Given a broke piece of code, can you find the bug and get it working?</p><p>- Implement a basic password generator, similar to 1Password (with optional characters and symbols)</p><p>If you can reason about code without an LLM, then you’ll do even better with an LLM. At least, that’s my theory.</p><p>I never ask trick questions. I never pull from Leetcode. I hardly care about time complexity. Just show me you can reason about code. And if you make some mistakes, I won’t judge you.</p><p>I’m trying to be as fair as possible.</p><p>I do understand that LLMs are part of our lives now. So I’m trying to explore ways to integrate them into the interview. But I need more time to ponder.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912478"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912478" href="https://news.ycombinator.com/vote?id=42912478&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Thinking out loud, here’s one idea for an LLM-assisted interview:</p><p>- Spin up a Digital Ocean droplet</p><p>- Add the candidate’s SSH key</p><p>- Have them implement a basic API. It must be publicly accessible.</p><p>- Connect the API to a database. Add more features.</p><p>- Set up a basic deployment pipeline. Could be as simple  as script that copies the code from your local machine to the server.</p><p>Anything would be fair game. The goal would be to see how the candidate converses with the LLM, how they handle unexpected changes, and how they make decisions.</p><p>Just a thought.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911599"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911599" href="https://news.ycombinator.com/vote?id=42911599&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I've been on both sides recently, and it hasn't really changed significantly. If you're heming and hawing you're not getting the job.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911360"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911360" href="https://news.ycombinator.com/vote?id=42911360&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I have a colleague that uses AI to comment on RFCs. It is so clearly machine generated, that I wonder if I am the only one to see it. 
He is a good colleague though, but as he is a bit junior, it is still not clear to me if AI is helping him to improve faster or if it is hindering his deep learning of stuff.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912443"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912443" href="https://news.ycombinator.com/vote?id=42912443&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I listened in to someone interviewing people since many people used AI. It's the same with googling the answer it is very obvious that someone is taking too long to get to the answer and or you can't see a separate screen. Mitigation is literally looking at the text window and seeing if they are not typing / taking too long to even make a bad implementation. There is now a problem if you allow for google since google will autogen a gemini query to solve it.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912551"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912551" href="https://news.ycombinator.com/vote?id=42912551&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>I expect in person interviews are going to be the norm soon, assuming they’re not already.  For now, the challenge I give candidates causes ChatGPT to produce convoluted code that a human never would. I then ask the person to explain the code line-by-line and they’re almost never able to give a satisfactory answer</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912251"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912251" href="https://news.ycombinator.com/vote?id=42912251&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>If your interview process is susceptible to AI then you don't need to hire for the job. Just use an AI and prompt it.</p><p>The job you are therefore hiring for is now trivial. If it weren't, no amount of AI could pass your interview process.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912666"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912666" href="https://news.ycombinator.com/vote?id=42912666&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I belive this line of thinking mistakes the result with the process, similar to assuming that the reason companies ask people to reverse a linked list is because there's an unmet market demand for list-reversal algorithms.</p><p>An interview has to be hard enough to filter those that are unqualified but also easy enough that the right person can pass with some minor preparation. If an interviewer asked me for the equivalent of production-ready code to add support for custom hardware in the Linux kernel I'd either reply with my freelance hourly rate or I'd end the interview.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911743"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911743" href="https://news.ycombinator.com/vote?id=42911743&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>In my most recent cycle, I didn’t ask to use AI and I was only warned once about using AI when I had the official language plugin for an IDE annotate some struct fields with json tags. I explained the plugin functionality and we moved on.</p><p>When I was part of interviews on the other side for my former employer, I encountered multiple candidates who appeared to be using AI assistance without notifying the interviewers ahead of time or at all.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911615"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911615" href="https://news.ycombinator.com/vote?id=42911615&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I've been very curious about this and about how we should modify our hiring. Its obvious that an individual should be able to use AI companions to build better, faster, higher quality things... But the skillsets are sooo uneven now that its unfair to those who are with and without.</p><p>I think it ultimately comes back to impact (like always) which has remained largely unchanged.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911683"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911683" href="https://news.ycombinator.com/vote?id=42911683&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I haven't done any hiring in a while, but my feelings on the matter:</p><p>If they can talk through the technology and code fluently, honestly, I don't care how they do the work. Honestly I feel like the ability to communicate is a far more important skill than the precise technology.</p><p>This is of course presumes you have a clue about the technology you're hiring for.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911905"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911905" href="https://news.ycombinator.com/vote?id=42911905&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Buddy of mine recently got a position with the help of custom built model that was listening on the call and printed answers on another screen. The arms race is here and frankly, given that a lot of people are already using it at work, there is no way to stop it short of minute upon minute supervision and even biggest micromanagers won't be able to deal with it.</p><p>Honestly, if I could trust that companies won't try to evaluate my conversation through 20 different ridiculous filters, I would probably argue that my buddy is out of line.. As it stands, however, he is merely leveling out the playing field. But, just life with WFH, management class does not like that imposition one bit.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42912256"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912256" href="https://news.ycombinator.com/vote?id=42912256&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>People are sending us emails that are not just written with chatgpt, but I think they've automated the process as well, as parts of the prompt slip in.</p><p>You can see things in the emails like:</p><p>"I provided a concise, polite response from a candidate to a job rejection, expressing gratitude, a desire for feedback, and interest in future opportunities."</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912851"><td></td></tr>
                  <tr id="42911523"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911523" href="https://news.ycombinator.com/vote?id=42911523&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Panel interviews seem to be more common.  Curious if others have seen the same?  I personally feel very uncomfortable coding in front of a group.  First one of these I tried had like 5 people watching and I lost my nerve and bailed. :|</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912281"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912281" href="https://news.ycombinator.com/vote?id=42912281&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Panels and live programming assignments are such an awful idea. Is that what the workplace is like? Doo they want people who can work under those conditions? I've been a working professional for 18 years who gives public talks regularly and I can still see myself clamming up in that situation. Everyone knows it's hard to think and type when you are being watched.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912676"><td></td></tr>
            <tr id="42911525"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911525" href="https://news.ycombinator.com/vote?id=42911525&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>It's still remote. I don't get how you could pass an interview using ChatGPT unless it's purely leetcode.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911737"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911737" href="https://news.ycombinator.com/vote?id=42911737&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I can't speak for job interviewing, but having recently completed 3rd-semester trade-school oral exams in Java programming:</p><p>It is really important to watch people code.</p><p>Anyone can fake an abstract overview.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911801"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911801" href="https://news.ycombinator.com/vote?id=42911801&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>It can be weird. Seen some decent resumes for people that in the actual interview the candidate obviously has zero demonstrable knowledge of.</p><p>Ask even the shallowest question and they are lost and just start regurgitating what feels like very bad prompt based responses.</p><p>At that point it's just about closing down the interview without being unprofessional.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="42910395"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910395" href="https://news.ycombinator.com/vote?id=42910395&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Not sure why interviews would change.</p><p>Even if you're using ChatGPT heavily it's your job to ensure it's right. And you need to know what to ask it. So you still need all the same skills and conceptual understanding as before.</p><p>I mean, I didn't observe interviews change after powerful IDE's replaced basic text editors.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42910678"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910678" href="https://news.ycombinator.com/vote?id=42910678&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Because interviews were always an attempt to discern a signal few hours interview into an accurate prediction of performance from months to years. AIs generate a lot of nosie to mask that. Interviewees can just pass the question to the AI, who will generate a reasonable sounding response.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42910571"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42910571" href="https://news.ycombinator.com/vote?id=42910571&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>Because there's a format of interview that's basically a brainteaser that takes 45 minutes to think through and whiteboard some code for, but which is trivially solvable by copy and pasting a screenshot of the prompt into ChatGPT. This amounts to candidates being given the answer and then pretending to struggle with understanding your question and then figuring out a solution to it when really they're just stalling for time and then just copying the answer from one browser tab to the next.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42912126"><td></td></tr>
            <tr id="42912698"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912698" href="https://news.ycombinator.com/vote?id=42912698&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>This conversation feels bizarrely tone deaf. The skill of being able to recall specific knowledge on demand is going away.</p><p>How LLMs will evaluate a skill they are making obsolete is a question I am not sure I understand.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912736"><td></td></tr>
                  <tr id="42910403"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910403" href="https://news.ycombinator.com/vote?id=42910403&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I mentioned this in a different related post but there seems to be a pretty sad lack of basic integrity in the tech world where it's become a point of pride to develop and use apps which allow an applicant to blatantly cheat during interviews using LLMs.</p><p>As a result, several of my friends who assist in hiring at their companies have already returned to "on-site" interviews. The funny thing about this is that these are 100% remote jobs - but the interviews are being conducted at shared workspaces. This is what happens when the level of trust goes down to zero due to bad actors.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912077"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_42912077" href="https://news.ycombinator.com/vote?id=42912077&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>Largely past COVID it seems like sheer laziness or cheapness not to conduct in-person interviews for a professional job other than a short-term project after essentially an initial screen for all sorts of reasons that have little to do with cheating. I don’t care if the job is largely remote.</p><p>As someone else noted, this used to be utterly standard. And frankly I’d probably just pass on someone who balked. Plenty of fish in the sea.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="42911837"><td></td></tr>
            <tr id="42910634"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42910634" href="https://news.ycombinator.com/vote?id=42910634&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div>
                  <p>There are some tools that read your screen and can provide hints and solutions for coding type questions. I honestly don't trust myself to not mess it up, plus the whole ethics side of it, but I'm sure that will always be a problem for online assessments</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="42911686"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42911686" href="https://news.ycombinator.com/vote?id=42911686&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>I tell everyone to share their entire screen, have their video on, and start coding. It's not that different. Even as an interviewer, I experimented with the usual cheating techniques so I know what to look out for. The best are the AI teleprompters. If you can do the work with your own AI then I see no need to care as the business will not care either.</p><p>The story is completely different for airgapped dark room jobs, but if you know you know.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="42912555"><td></td></tr>
                  <tr id="42912661"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_42912661" href="https://news.ycombinator.com/vote?id=42912661&amp;how=up&amp;goto=item%3Fid%3D42909166"></a></center>    </td><td><br><div><p>If a problem can be “trivially” solved by GPT. The problem is with your interview process, not the tool. It’s wild to me that interviewers still ask candidates for senior positions leetcode type questions. Yet the actual job is for some front end or devops position.</p><p>The gap between interview and actual on job duties is very wide at many — delusional - companies.</p></div></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Lume – OS lightweight CLI for MacOS & Linux VMs on Apple Silicon. (248 pts)]]></title>
            <link>https://github.com/trycua/lume</link>
            <guid>42908061</guid>
            <pubDate>Sun, 02 Feb 2025 11:46:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/trycua/lume">https://github.com/trycua/lume</a>, See on <a href="https://news.ycombinator.com/item?id=42908061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><strong>lume</strong> is a lightweight Command Line Interface and local API server to create, run and manage macOS and Linux virtual machines (VMs) with near-native performance on Apple Silicon, using Apple's <code>Virtualization.Framework</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Run prebuilt macOS images in just 1 step</h3><a id="user-content-run-prebuilt-macos-images-in-just-1-step" aria-label="Permalink: Run prebuilt macOS images in just 1 step" href="#run-prebuilt-macos-images-in-just-1-step"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/trycua/lume/blob/main/img/cli.png"><img src="https://github.com/trycua/lume/raw/main/img/cli.png" alt="lume cli"></a>
</p>
<div dir="auto" data-snippet-clipboard-copy-content="lume run macos-sequoia-vanilla:latest"><pre>lume run macos-sequoia-vanilla:latest</pre></div>
<p dir="auto">For a python interface, check out <a href="https://github.com/trycua/pylume">pylume</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="lume <command>

Commands:
  lume create <name>            Create a new macOS or Linux VM
  lume run <name>               Run a VM
  lume ls                       List all VMs
  lume get <name>               Get detailed information about a VM
  lume set <name>               Modify VM configuration
  lume stop <name>              Stop a running VM
  lume delete <name>            Delete a VM
  lume pull <image>             Pull a macOS image from container registry
  lume clone <name> <new-name>  Clone an existing VM
  lume images                   List available macOS images in local cache
  lume ipsw                     Get the latest macOS restore image URL
  lume prune                    Remove cached images
  lume serve                    Start the API server

Options:
  --help     Show help [boolean]
  --version  Show version number [boolean]

Command Options:
  create:
    --os <os>            Operating system to install (macOS or linux, default: macOS)
    --cpu <cores>        Number of CPU cores (default: 4)
    --memory <size>      Memory size, e.g., 8GB (default: 4GB)
    --disk-size <size>   Disk size, e.g., 50GB (default: 40GB)
    --display <res>      Display resolution (default: 1024x768)
    --ipsw <path>        Path to IPSW file or 'latest' for macOS VMs

  run:
    --no-display         Do not start the VNC client app
    --shared-dir <dir>   Share directory with VM (format: path[:ro|rw])
    --mount <path>       For Linux VMs only, attach a read-only disk image

  set:
    --cpu <cores>        New number of CPU cores
    --memory <size>      New memory size
    --disk-size <size>   New disk size

  delete:
    --force              Force deletion without confirmation

  pull:
    --registry <url>     Container registry URL (default: ghcr.io)
    --organization <org> Organization to pull from (default: trycua)

  serve:
    --port <port>        Port to listen on (default: 3000)"><pre>lume <span>&lt;</span>command<span>&gt;</span>

Commands:
  lume create <span>&lt;</span>name<span>&gt;</span>            Create a new macOS or Linux VM
  lume run <span>&lt;</span>name<span>&gt;</span>               Run a VM
  lume ls                       List all VMs
  lume get <span>&lt;</span>name<span>&gt;</span>               Get detailed information about a VM
  lume <span>set</span> <span>&lt;</span>name<span>&gt;</span>               Modify VM configuration
  lume stop <span>&lt;</span>name<span>&gt;</span>              Stop a running VM
  lume delete <span>&lt;</span>name<span>&gt;</span>            Delete a VM
  lume pull <span>&lt;</span>image<span>&gt;</span>             Pull a macOS image from container registry
  lume clone <span>&lt;</span>name<span>&gt;</span> <span>&lt;</span>new-name<span>&gt;</span>  Clone an existing VM
  lume images                   List available macOS images <span>in</span> <span>local</span> cache
  lume ipsw                     Get the latest macOS restore image URL
  lume prune                    Remove cached images
  lume serve                    Start the API server

Options:
  --help     Show <span>help</span> [boolean]
  --version  Show version number [boolean]

Command Options:
  create:
    --os <span>&lt;</span>os<span>&gt;</span>            Operating system to install (macOS or linux, default: macOS)
    --cpu <span>&lt;</span>cores<span>&gt;</span>        Number of CPU cores (default: 4)
    --memory <span>&lt;</span>size<span>&gt;</span>      Memory size, e.g., 8GB (default: 4GB)
    --disk-size <span>&lt;</span>size<span>&gt;</span>   Disk size, e.g., 50GB (default: 40GB)
    --display <span>&lt;</span>res<span>&gt;</span>      Display resolution (default: 1024x768)
    --ipsw <span>&lt;</span>path<span>&gt;</span>        Path to IPSW file or <span><span>'</span>latest<span>'</span></span> <span>for</span> macOS VMs

  run:
    --no-display         Do not start the VNC client app
    --shared-dir <span>&lt;</span>dir<span>&gt;</span>   Share directory with VM (format: path[:ro<span>|</span>rw])
    --mount <span>&lt;</span>path<span>&gt;</span>       For Linux VMs only, attach a read-only disk image

  set:
    --cpu <span>&lt;</span>cores<span>&gt;</span>        New number of CPU cores
    --memory <span>&lt;</span>size<span>&gt;</span>      New memory size
    --disk-size <span>&lt;</span>size<span>&gt;</span>   New disk size

  delete:
    --force              Force deletion without confirmation

  pull:
    --registry <span>&lt;</span>url<span>&gt;</span>     Container registry URL (default: ghcr.io)
    --organization <span>&lt;</span>org<span>&gt;</span> Organization to pull from (default: trycua)

  serve:
    --port <span>&lt;</span>port<span>&gt;</span>        Port to listen on (default: 3000)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew tap trycua/lume
brew install lume"><pre>brew tap trycua/lume
brew install lume</pre></div>
<p dir="auto">You can also download the <code>lume.pkg.tar.gz</code> archive from the <a href="https://github.com/trycua/lume/releases">latest release</a>, extract it, and install the package manually.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prebuilt Images</h2><a id="user-content-prebuilt-images" aria-label="Permalink: Prebuilt Images" href="#prebuilt-images"></a></p>
<p dir="auto">Pre-built images are available on <a href="https://github.com/orgs/trycua/packages">ghcr.io/trycua</a>.
These images come with an SSH server pre-configured and auto-login enabled.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Image</th>
<th>Tag</th>
<th>Description</th>
<th>Size</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>macos-sequoia-vanilla</code></td>
<td><code>latest</code>, <code>15.2</code></td>
<td>macOS Sonoma 15.2</td>
<td>40GB</td>
</tr>
<tr>
<td><code>macos-sequoia-xcode</code></td>
<td><code>latest</code>, <code>15.2</code></td>
<td>macOS Sonoma 15.2 with Xcode command line tools</td>
<td>50GB</td>
</tr>
<tr>
<td><code>ubuntu-noble-vanilla</code></td>
<td><code>latest</code>, <code>24.04.1</code></td>
<td><a href="https://ubuntu.com/download/server/arm" rel="nofollow">Ubuntu Server for ARM 24.04.1 LTS</a> with Ubuntu Desktop</td>
<td>20GB</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">For additional disk space, resize the VM disk after pulling the image using the <code>lume set &lt;name&gt; --disk-size &lt;size&gt;</code> command.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Local API Server</h2><a id="user-content-local-api-server" aria-label="Permalink: Local API Server" href="#local-api-server"></a></p>
<p dir="auto"><code>lume</code> exposes a local HTTP API server that listens on <code>http://localhost:3000/lume</code>, enabling automated management of VMs.</p>

<p dir="auto">For detailed API documentation, please refer to <a href="https://github.com/trycua/lume/blob/main/docs/API-Reference.md">API Reference</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Docs</h2><a id="user-content-docs" aria-label="Permalink: Docs" href="#docs"></a></p>
<ul dir="auto">
<li><a href="https://github.com/trycua/lume/blob/main/docs/API-Reference.md">API Reference</a></li>
<li><a href="https://github.com/trycua/lume/blob/main/docs/Development.md">Development</a></li>
<li><a href="https://github.com/trycua/lume/blob/main/docs/FAQ.md">FAQ</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome and greatly appreciate contributions to lume! Whether you're improving documentation, adding new features, fixing bugs, or adding new VM images, your efforts help make lume better for everyone. For detailed instructions on how to contribute, please refer to our <a href="https://github.com/trycua/lume/blob/main/CONTRIBUTING.md">Contributing Guidelines</a>.</p>
<p dir="auto">Join our <a href="https://discord.gg/8p56E2KJ" rel="nofollow">Discord community</a> to discuss ideas or get assistance.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">lume is open-sourced under the MIT License - see the <a href="https://github.com/trycua/lume/blob/main/LICENSE">LICENSE</a> file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">Apple, macOS, and Apple Silicon are trademarks of Apple Inc. Ubuntu and Canonical are registered trademarks of Canonical Ltd. This project is not affiliated with, endorsed by, or sponsored by Apple Inc. or Canonical Ltd.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stargazers over time</h2><a id="user-content-stargazers-over-time" aria-label="Permalink: Stargazers over time" href="#stargazers-over-time"></a></p>
<p dir="auto"><a href="https://starchart.cc/trycua/lume" rel="nofollow"><img src="https://camo.githubusercontent.com/561442573e1c2212a9bfb1ba7e7bb847cfbf6f205c18769dbb73f4ea9cd1dd24/68747470733a2f2f7374617263686172742e63632f7472796375612f6c756d652e7376673f76617269616e743d6164617074697665" alt="Stargazers over time" data-canonical-src="https://starchart.cc/trycua/lume.svg?variant=adaptive"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spaced repetition can allow for infinite recall (2022) (151 pts)]]></title>
            <link>https://www.efavdb.com/memory%20recall</link>
            <guid>42908041</guid>
            <pubDate>Sun, 02 Feb 2025 11:42:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.efavdb.com/memory%20recall">https://www.efavdb.com/memory%20recall</a>, See on <a href="https://news.ycombinator.com/item?id=42908041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p>
         <img src="https://www.efavdb.com/images/jeopardy.jpg">
</p>

<p>My friend Andrew is an advocate of the “spaced repetition” technique for
memorization of a great many facts [1].  The ideas behind this are&nbsp;two-fold:</p>
<ul>
<li>
<p>When one first “learns” a new fact, it needs to be reviewed frequently in
  order to not forget it.  However, with each additional review, the fact can
be retained longer before a refresher is needed to maintain it in&nbsp;recall.</p>
</li>
<li>
<p>Because of this, one can maintain a large, growing body of facts in recall
  through daily review:  Each day, one need only review for ten minutes or so,
covering a small number of facts. The facts included should be sampled from the
full library in a way that prefers newer entries, but that also sprinkles in
older facts often enough so that none are ever forgotten.  Apps have been
written to intelligently take care of the sampling process for&nbsp;us.</p>
</li>
</ul>
<p>Taking this framework as correct motivates questioning exactly how far it can
be pushed:  <em>Would an infinitely-long-lived, but forgetful person be able to
recall an infinite number of facts using this method? </em>  <span>\(\ldots\)</span> Below, we
show that the answer is: <em><span>YES</span>!</em></p>
<h5>Proof:</h5>
<p>We first posit that the number of days <span>\(T\)</span> that a fact can be retained before
it needs to be reviewed grows as a power-law in <span>\(s\)</span>, the number of times it’s
been reviewed so&nbsp;far,</p>
<p>\begin{eqnarray} \tag{1}\label{1}
T(s) \sim s^{\gamma},
\end{eqnarray}</p>
<p>
with <span>\(\gamma &gt; 0\)</span>. With this assumption, if <span>\(N(t)\)</span> facts are to be recalled
from <span>\(t\)</span> days ago, one can show that the amount of work needed today to retain
these will go like (see appendix for a proof of this&nbsp;line)</p>
<p>\begin{eqnarray}\tag{2}\label{2}
w(t) \sim \frac{N(t)}{t^{\gamma / (\gamma + 1)}}.
\end{eqnarray}</p>
<p>
The total work needed today is then the sum of work needed for each past day’s&nbsp;facts,</p>
<p>\begin{eqnarray} \tag{3} \label{3}
W(total) = \int_1^{\infty} \frac{N(t)}{t^{\gamma / (\gamma + 1)}} dt.
\end{eqnarray}</p>
<p>
Now, each day we only have a finite amount of time to study.  However, the
above total work integral will diverge at large <span>\(t\)</span> unless it decays faster
than <span>\(1/t\)</span>.  To ensure this, we can limit the number of facts retained from
from <span>\(t\)</span> days ago to go&nbsp;as</p>
<p>\begin{eqnarray} \tag{4} \label{4}
N(t) \sim \frac{1}{t^{\epsilon}} \times \frac{1}{t^{1 / (\gamma + 1)}},
\end{eqnarray}</p>
<p>
where <span>\(\epsilon\)</span> is some small, positive constant.  Plugging (\ref{4}) into
(\ref{3}) shows that we are guaranteed a finite required study time each day.
However, after <span>\(t\)</span> days of study, the total number of facts retained scales&nbsp;as</p>
<p>\begin{eqnarray}
N_{total}(t) &amp;\sim &amp; \int_1^{t} N(t) dt \\
&amp;\sim &amp; \int_0^{t} \frac{1}{t^{1 / (\gamma + 1)}} \\
&amp;\sim &amp; t^{ \gamma / (\gamma + 1)}. \tag{5} \label{5}
\end{eqnarray}</p>
<p>
Because we assume that <span>\(\gamma &gt; 0\)</span>, this grows without bound over time,
eventually allowing for an infinitely large&nbsp;library.</p>
<p>We conclude that — though we can’t remember a fixed number of facts from each
day in the past using spaced repetition — we can ultimately recall an infinite
number of facts using this method.  To do this only requires that we gradually
curate our previously-introduced facts so that the scaling (\ref{4}) holds at
all&nbsp;times.</p>
<h3>Appendix: Proof of&nbsp;(2)</h3>
<p>Recall that we assume <span>\(N(s)\)</span> facts have been reviewed exactly <span>\(s\)</span> times.  On a
given day, the number of these that need to be reviewed then goes&nbsp;like</p>
<p>\begin{eqnarray} \tag{A1}\label{A1}
W(s) \sim \frac{N(s)}{T(s)}.
\end{eqnarray}</p>
<p>
where <span>\(T(s)\)</span> is given in (\ref{1}).  This holds because each of the <span>\(N(s)\)</span>
facts that have been studied <span>\(s\)</span> times so far must be reviewed within <span>\(T(s)\)</span>
days, or one will be forgotten.  During these <span>\(T(s)\)</span> days, each will move to
having been reviewed <span>\(s+1\)</span> times.&nbsp;Therefore,</p>
<p>\begin{eqnarray} \tag{A2} \label{A2}
\frac{ds}{dt} &amp;\sim &amp; \frac{1}{T(s)}
\end{eqnarray}</p>
<p>
Integrating this gives <span>\(s\)</span> as a function of <span>\(t\)</span>,</p>
<p>\begin{eqnarray} \tag{A3} \label{A3}
s \sim t^{1 / (\gamma + 1)}
\end{eqnarray}</p>
<p>Plugging this last line and (1) into (A1), we get&nbsp;(2).</p>
<h2>References</h2>
<p>[1] See Andrew’s blog post on spaced repetition <a href="https://andrewjudson.com/spaced-repitition/2022/06/03/spaced-repitition.html">
here</a>.</p>



             
 
                

                <hr>
    <p><a href="" target="_blank" rel="nofollow noopener noreferrer">
            <img src="https://www.efavdb.com/wp-content/uploads/2014/12/JonathanLinkedIn.jpg" alt="Jonathan Landy Avatar" title="Jonathan Landy">
            
        </a>
        Jonathan grew up in the midwest and then went to school at Caltech and&nbsp;UCLA. Following this, he did two postdocs, one at UCSB and one at UC Berkeley. &nbsp;His academic research focused primarily on applications of statistical mechanics, but his professional passion has always been in the mastering, development, and practical application of slick&nbsp;math methods/tools. He currently works as a data-scientist at Stitch Fix.
    </p>

            






            <hr>
            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reverse-engineering and analysis of SanDisk High Endurance microSDXC card (2020) (229 pts)]]></title>
            <link>https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/</link>
            <guid>42907766</guid>
            <pubDate>Sun, 02 Feb 2025 10:32:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/">https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/</a>, See on <a href="https://news.ycombinator.com/item?id=42907766">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><strong>TL;DR – The SanDisk High Endurance cards use SanDisk/Toshiba <a href="https://www.tomshardware.com/news/wd-sandisk-bics3-64-layer-3d-nand,32328.html" target="_blank" rel="noopener">3D TLC Flash</a>. It took way, way more work than it should have to figure this out (thanks for nothing, SanDisk!).<br>
In contrast, the SanDisk MAX Endurance uses the same 3D TLC in pMLC (pseudo-multi-level cell) mode.</strong></p><p>In a <a href="https://ripitapart.com/2019/08/17/unboxing-and-review-of-sandisk-64gb-microsdxc-high-endurance-card/" target="_blank" rel="noopener">previous blog post</a>, I took a look at SanDisk’s microSD cards that were aimed for use in write-intensive applications like dash cameras. In that post I took a look at its performance metrics, and speculated about what sort of NAND Flash memory is used inside. SanDisk doesn’t publish any detailed specifications about the cards’ internal workings, so that means I have no choice but to reverse-engineer the <del>can of worms</del> card myself.</p><p>In the hopes of uncovering more information, I sent an email to SanDisk’s support team asking about what type of NAND Flash they are using in their High Endurance lineup of cards, alongside endurance metrics like P/E (Program/Erase) cycle counts and total terabytes written (TBW). Unfortunately, the SanDisk support rep couldn’t provide a satisfactory answer to my questions, as they’re not able to provide any information that’s not listed in their public spec sheets. They said that all of their cards use MLC Flash, which I guess is correct if you call TLC Flash 3-bit MLC (which Samsung does).</p><div>
<blockquote><p>Dear Jason,</p>
<p>Thank you for contacting SanDisk® Global customer care. We really appreciate you being a part of our SanDisk® family.</p>
<p>I understand that you wish to know more about the SanDisk® High Endurance video monitoring card, as such please allow me to inform you that all our SanDisk® memory cards uses Multi level cell technology (MLC) flash. However, the read/write cycles for the flash memory is not published nor documented only the read and write speed in published as such they are 100 MB/S &amp; 40 MB/s. The 64 GB card can record Full HD video up to 10,000 hours. To know more about the card you may refer to the link below:</p>
<p><a title="Click to follow link: https://www.sandisk.com/home/memory-cards/microsd-cards/high-endurance-microsd" href="https://www.sandisk.com/home/memory-cards/microsd-cards/high-endurance-microsd" target="_blank" rel="noopener">SANDISK HIGH ENDURANCE VIDEO MONITORING microSD CARD</a></p>
<p>Best regards,<br>
Allan B.<br>
SanDisk® Global Customer Care</p></blockquote>
<p>I’ll give them a silver star that says “You Tried” at least.</p>
<h2>Anatomy of an SD Card</h2>
<p>While (micro)SD cards feel like a solid monolithic piece of technology, they’re made up of multiple different chips, each performing a different role. A basic SD card will have a controller that manages the NAND Flash chips and communicates with the host (PC, camera, etc.), and the NAND Flash itself (made up of 1 or more Flash dies). Bunnie Huang’s blog, Bunnie Studios, has an excellent article on the internals of SD cards, including counterfeits and how they’re made – check it out <a href="https://www.bunniestudios.com/blog/?p=3554" target="_blank" rel="noopener">here</a>.</p>
<div data-shortcode="caption" id="attachment_2132"><p><a href="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png"><img aria-describedby="caption-attachment-2132" data-attachment-id="2132" data-permalink="https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/sd-card-anatomy/" data-orig-file="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png" data-orig-size="268,476" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SD Card Anatomy" data-image-description="<p>Block diagram of a typical SD card.</p>
" data-image-caption="<p>Block diagram of a typical SD card.</p>
" data-medium-file="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png?w=169" data-large-file="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png?w=268" src="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png?w=584" alt="SD Card Anatomy" srcset="https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png 268w, https://ripitapart.com/wp-content/uploads/2019/10/sd-card-anatomy-2.png?w=84&amp;h=150 84w" sizes="(max-width: 268px) 100vw, 268px"></a></p><p id="caption-attachment-2132">Block diagram of a typical SD card.</p></div>
<p>MicroSD cards often (but not always!) include test pads, used to program/test the NAND Flash during manufacture. These can be exploited in the case of <a href="https://blog.acelaboratory.com/pc-3000-flash-monolith-pinout-research.html" target="_blank" rel="noopener">data recovery</a>, or to reuse microSD cards that have a defective controller or firmware by turning the card into a piece of raw NAND Flash – check out Gough Lui’s adventures <a href="https://goughlui.com/2015/04/05/teardown-optimization-comsol-8gb-usb-flash-stick-au6989sn-gt-sdtnrcama-008g/" target="_blank" rel="noopener">here</a>. Note that there is no set standard for these test pads (even for the same manufacturer!), but there are common patterns for some manufacturers like SanDisk that make reverse-engineering easier.</p>
<h2>Crouching Controller, Hidden Test Pads</h2>
<p>microSD cards fall into a category of “monolithic” Flash devices, as they combine a controller and raw NAND Flash memory into a single, inseparable package. Many manufacturers break out the Flash’s data bus onto hidden (and nearly completely undocumented) test pads, which some other memory card and USB drive manufacturers take advantage of to make cheap storage products using failed parts; the controller can simply be electrically disabled and the Flash is then used as if it were a regular chip.</p>
<p>In the case of SanDisk cards, there is very limited information on their cards’ test pad pinouts. Each generation has slight differences, but the layout is mostly the same. <del>These differences can be fatal, as the power and ground connections are sometimes reversed (this spells instant death for a chip if its power polarity is mixed up!).</del></p>
<p><strong>CORRECTION (July 22, 2020):</strong> <em>Upon further review, I might have accidentally created a discrepancy between the leaked pinouts online, versus my own documentation in terms of power polarity; see the “Test Pad Pinout” section.</em></p>
<p>My card (and many of their higher-end cards – that is, not their Ultra lineup) features test pads that aren’t covered by solder mask, but are instead covered by some sort of screen-printed epoxy with a laser-etched serial number on top. With a bit of heat and some scraping, I was able to remove the (very brittle) coating on top of the test pads; this also removed the serial number which I believe is an anti-tamper measure by SanDisk.</p>

		
		

<p>After cleaning off any last traces of the epoxy coating, I was greeted with the familiar SanDisk test pad layout, plus a few extra on the bottom.</p>
<h2>Building the Breakout Board</h2>
<p>The breakout board is relatively simple in concept: for each test pad, bring out a wire that goes to a bigger test point for easier access, and wire up the normal SD bus to an SD connector to let the controller do its work with twiddling the NAND Flash bus. Given how small each test pad is (and how many), things get a bit… messy.</p>

		
		

<p>I started by using double-side foam adhesive tape to secure the SD card to a piece of perfboard. I then tinned all of the pads and soldered a small 1uF ceramic capacitor across the card’s power (Vcc) and ground (GND) test pads. Using 40-gauge (0.1 mm, or 4-thousandths of an inch!) magnet wire, I mapped each test pad to its corresponding machine-pin socket on the perfboard. Including the extra test pads, that’s a total of 28 tiny wires!</p>
<p>For the SD connector side of things, I used a flex cable for the <a href="http://xtc2clip.org/how-it-works" target="_blank" rel="noopener">XTC 2 Clip</a> (a tool used to service HTC Android devices), as it acted as a flexible “remote SD card” and broke out the signals to a small ribbon cable. I encased the flex cable with copper tape to act as a shield against electrical noise and to provide physical reinforcement, and soldered the tape to the outer pads on the perfboard for reinforcement. The ribbon cable end was then tinned and each SD card’s pin was wired up with magnet wire. The power lines were then broken out to an LED and resistor to indicate when the card was receiving power.</p>
<h2>Bus Analysis</h2>
<p>With all of the test pads broken out to an array of test pins, it was time to make sense of what pins are responsible for accessing the NAND Flash inside the card.</p>
<h2>Test Pad Pinout</h2>
<div data-shortcode="caption" id="attachment_2165"><p><a href="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png"><img aria-describedby="caption-attachment-2165" data-attachment-id="2165" data-permalink="https://ripitapart.com/sandisk-high-endurance-microsd-test-pad-pinout/" data-orig-file="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png" data-orig-size="1666,935" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SanDisk High Endurance microSD Test Pad Pinout" data-image-description="<p>Diagram of the test pads on SanDisk’s High Endurance microSD card.</p>
" data-image-caption="<p>Diagram of the test pads on SanDisk’s High Endurance microSD card.</p>
" data-medium-file="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=300" data-large-file="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584" src="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584&amp;h=328" alt="Diagram of the test pads on SanDisk's High Endurance microSD card." width="584" height="328" srcset="https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=584 584w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=1168 1168w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=150 150w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=300 300w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=768 768w, https://ripitapart.com/wp-content/uploads/2020/07/sandisk-high-endurance-microsd-test-pad-pinout.png?w=1024 1024w" sizes="(max-width: 584px) 100vw, 584px"></a></p><p id="caption-attachment-2165">Diagram of the test pads on SanDisk’s High Endurance microSD card. (click to enlarge)</p></div>
<p>The overall test pad pinout was the same for other microSD cards from SanDisk<del>, but there were some differences, primarily regarding the layout of the power pads; notably, <strong>the </strong><strong>main power pins are backwards</strong>! This can destroy the card if you’re not careful when applying power.</del></p>
<p><strong>CORRECTION (July 22, 2020):</strong> <em>I might actually have just gotten my own documentation mixed up in terms of the power and ground test pads. Regardless, one should always be careful to ensure the correct power polarity is sent to a device.</em></p>
<p>I used my <a href="https://www.dreamsourcelab.com/shop/logic-analyzer/dslogic-plus/" target="_blank" rel="noopener">DSLogic Plus</a> logic analyzer to analyze the signals on all of the pins. Since the data pinout was previously discovered, the hard part of figuring out what each line represented (data bus, control, address, command, write-protect, ready/busy status) was already done for me. However, not all of the lines were immediately evident as the pinouts I found online only included the bare minimum of lines to make the NAND Flash accessible, with one exception being a control line that places the controller into a reset state and releases its control of the data lines (this will be important later on).</p>
<p>By sniffing the data bus at the DSLogic’s maximum speed (and using its 32 MB onboard buffer RAM), I was able to get a clear snapshot of the commands being sent to the NAND Flash from the controller during initialization.</p>
<h2>Bus Sniffing &amp; NAND I/O 101 (writing commands, address, reading data)</h2>
<p>In particular, I was looking for two commands: RESET (0xFF), and READ ID (0x90). When looking for a command sequence, it’s important to know how and when the data and control lines change. I will try to explain it step-by-step, but if you’re interested there is an <a href="https://user.eng.umd.edu/~blj/CS-590.26/micron-tn2919.pdf" target="_blank" rel="noopener">introductory white paper</a> by Micron that explains all of the fundamentals of NAND Flash with much more information about how NAND Flash works.</p>

		
		

<p>When a RESET command is sent to the NAND Flash, first the /CE (Chip Select, Active Low) line is pulled low. Then the CLE (Command Latch Enable) line is pulled high; the data bus is set to its intended value of 0xFF (all binary ones); then the /WE (Write Enable, Active Low) line is pulsed from high to low, then back to high again (the data bus’ contents are committed to the chip when the /WE line goes from low to high, known as a “rising edge”); the CLE line is pulled back low to return to its normal state. The Flash chip will then pull its R/B (Ready/Busy Status) line low to indicate it is busy resetting itself, then releases the line back to its high state when it’s finished.</p>
<p>The READ ID command works similarly, except after writing the command with 0x90 (binary 1001 0000) on the data bus, it then pulls the ALE (Address Latch Enable) line high instead of CLE, and writes 0x00 (all binary zeroes) by pulsing the /WE line low. The chip transfers its internally programmed NAND Flash ID into its internal read register, and the data is read out from the device on each rising edge of the /RE (Read Enable, Active Low) line; for most devices this is 4 to 8 bytes of data.</p>
<h2>NAND Flash ID</h2>
<p>For each NAND Flash device, it has a (mostly) unique ID that identifies the manufacturer, and other functional data that is defined by that manufacturer; in other words, only the manufacturer ID, assigned by the <a href="https://en.wikipedia.org/wiki/JEDEC" target="_blank" rel="noopener">JEDEC Technology Association</a>, is well-defined.</p>
<p>The first byte represents the Flash manufacturer, and the rest (2 to 6 bytes) define the device’s characteristics, as set out by the manufacturer themselves. Most NAND vendors are very tight-lipped when it comes to datasheets, and SanDisk (and by extension, Toshiba/Kioxia) maintain very strict control, save for some slightly outdated leaked Toshiba datasheets. Because the two aforementioned companies share their NAND fabrication facilities, we can reasonably presume the data structures in the vendor-defined bytes can be referenced against each other.</p>
<p>In the case of the SanDisk High Endurance 128GB card, it has a NAND Flash ID of 0x45 48 9A B3 7E 72 0D 0E. Some of these values can be compared against a <a href="http://www.datasheet.hk/view_download.php?id=2027929&amp;file=0515%5Ctc58teg5dcjtai0_7779332.pdf" target="_blank" rel="noopener">Toshiba datasheet</a>:</p>
<table>
<thead>
<tr>
<th>Byte Value (Hex)</th>
<th>Description/Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>45</td>
<td>
<ul>
<li>Manufacturer: SanDisk</li>
</ul>
</td>
</tr>
<tr>
<td>48</td>
<td>
<ul>
<li>I/O voltage: Presumed 1.8 volts (measured with multimeter)</li>
<li>Device capacity: Presumed 128 GB&nbsp;(unable to confirm against datasheet)</li>
</ul>
</td>
</tr>
<tr>
<td>9A</td>
<td>
<ul>
<li>NAND type: TLC (Triple-Level Cell / 3 bits per cell)</li>
<li>Flash dies per /CE: 4 (card uses four 32GB Flash chips internally)</li>
</ul>
</td>
</tr>
<tr>
<td>B3</td>
<td>
<ul>
<li>Block size: 12 MiB (768 pages per block) excluding spare area (determined outside datasheet)</li>
<li>Page size: 16,384 bytes / 16 kiB excluding spare area</li>
</ul>
</td>
</tr>
<tr>
<td>7E</td>
<td>
<ul>
<li>Planes per /CE: 8 (2 planes per die)</li>
</ul>
</td>
</tr>
<tr>
<td>72</td>
<td>
<ul>
<li>Interface type: Asynchronous</li>
<li>Process geometry: BiCS3 3D NAND (determined outside datasheet)</li>
</ul>
</td>
</tr>
<tr>
<td>0D</td>
<td>
<ul>
<li>Unknown (no information listed in datasheet)</li>
</ul>
</td>
</tr>
<tr>
<td>0E</td>
<td>
<ul>
<li>Unknown (no information listed in datasheet)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Although not all byte values could be conclusively determined, I was able to determine that the <strong>SanDisk High Endurance cards use <a href="https://www.tomshardware.com/news/wd-sandisk-bics3-64-layer-3d-nand,32328.html" target="_blank" rel="noopener">BiCS3 3D TLC NAND Flash</a></strong>, but at least it is <strong>3D NAND</strong> which improves endurance dramatically compared to traditional/planar NAND. Unfortunately, from this information alone, I cannot determine whether the card’s controller takes advantage of any SLC caching mechanisms for write operations.</p>
<p>The chip’s process geometry was determined by looking up the first four bytes of the Flash ID, and cross-referencing it to a line from a configuration file in Silicon Motion’s <a href="https://www.usbdev.ru/files/smi/" target="_blank" rel="noopener">mass production tools</a> for their <a href="http://www.siliconmotion.com/download.php?t=U0wyRnpjMlYwY3k4eU1ERTVMekV3THpBNUwzQnliMlIxWTNReE16Z3lNamd4TWpVMkxuQmtaajA5UFZOTk16STNNU0J3Y205a2RXTjBJR0p5YVdWbUM%3D" target="_blank" rel="noopener">SM3271</a> USB Flash drive controller, and their <a href="http://en.siliconmotion.com/download/product-brief/SM2258XT_Product_Brief_ENG_Q1109.pdf" target="_blank" rel="noopener">SM2258XT</a> DRAM-less SSD controller. These tools revealed supposed part numbers of SDTNAIAMA-256G and SDUNBIEMM-32G respectively, but I don’t think this is accurate for the specific Flash configuration in this card.</p>
<h2>External Control</h2>
<p>I wanted to make sure that I was getting the correct ID from the NAND Flash, so I rigged up a Texas Instruments <a href="https://www.ti.com/tool/MSP-EXP430FR2433" target="_blank" rel="noopener">MSP430FR2433 development board</a> and wrote some (very) rudimentary code to send the required RESET and READ ID commands, and attempt to extract any extra data from the chip’s hidden JEDEC Parameter Page along the way.</p>
<p>My first roadblock was that the MSP430 would reset every time it attempted to send the RESET command, suggesting that too much current was being drawn from the MSP430 board’s limited power supply. This can occur during <a href="https://en.wikipedia.org/wiki/Bus_contention" target="_blank" rel="noopener">bus contention</a>, where two devices “fight” each other when trying to set a certain digital line both high and low at the same time. I was unsure what was going on, since publicly-available information didn’t mention how to disable the card’s built-in controller (doing so would cause it to <a href="https://en.wikipedia.org/wiki/Three-state_logic#Tri-state_Buffer" target="_blank" rel="noopener">tri-state</a> the lines, effectively “letting go” of the NAND bus and allowing another device to take control).</p>
<p>I figured out that the A1 test pad (see diagram) was the controller’s reset line (pulsing this line low while the card was running forced my card reader to power-cycle it), and by holding the line low, the controller would release its control of the NAND Flash bus entirely. After this, my microcontroller code was able to read the Flash ID correctly and consistently.</p>
<div data-shortcode="caption" id="attachment_2122"><p><a href="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png"><img aria-describedby="caption-attachment-2122" data-attachment-id="2122" data-permalink="https://ripitapart.com/2020/07/16/reverse-engineering-and-analysis-of-sandisk-high-endurance-microsdxc-card/sandisk-he-128gb-nand-flash-id/" data-orig-file="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png" data-orig-size="877,375" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SanDisk High Endurance SD Card NAND Flash ID" data-image-description="" data-image-caption="<p>Reading out the card’s Flash ID with my own microcontroller code.</p>
" data-medium-file="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=300" data-large-file="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=584" src="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=584" alt="Reading out the card's Flash ID with my own microcontroller code." srcset="https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png 877w, https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=150&amp;h=64 150w, https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=300&amp;h=128 300w, https://ripitapart.com/wp-content/uploads/2019/10/sandisk-he-128gb-nand-flash-id-1.png?w=768&amp;h=328 768w" sizes="(max-width: 877px) 100vw, 877px"></a></p><p id="caption-attachment-2122">Reading out the card’s Flash ID with my own microcontroller code.</p></div>
<h2>JEDEC Parameter Page… or at least what SanDisk made of it!</h2>
<p>The JEDEC Parameter Page, if present, contains detailed information on a Flash chip’s characteristics with far greater detail than the NAND Flash ID – and it’s well-standardized so parsing it would be far easier. However, it turns out that SanDisk decided to ignore the standard format, and decided to use their own proprietary Parameter Page format! Normally the page starts with the ASCII string “JEDEC”, but I got a repeating string of “SNDK” (corresponding with their <a href="https://www.marketbeat.com/stocks/NASDAQ/SNDK/" target="_blank" rel="noopener">stock symbol</a>) with other data that didn’t correspond to anything like the JEDEC specification! Oh well, it was worth a try.</p>
<p>I collected the data with the same Arduino sketch as shown above, and pulled 1,536 bytes’ worth of data; I wrote a quick <a href="https://ideone.com/eLclhy#stdin" target="_blank" rel="noopener">program on Ideone</a> to provide a nicely-formatted hex dump of the first 512 bytes of the Parameter Page data:</p>
<pre>Offset 00:01:02:03:04:05:06:07:08:09:0A:0B:0C:0D:0E:0F 0123456789ABCDEF
------ --+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-- ----------------
0x0000 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B SNDKSNDKSNDKSNDK
0x0010 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B 53 4E 44 4B SNDKSNDKSNDKSNDK
0x0020 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 ..... ...H.....A
0x0030 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 Hcj..... ...H...
0x0040 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 ..AHcj..... ...H
0x0050 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 .....AHcj..... .
0x0060 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 ..H.....AHcj....
0x0070 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 . ...H.....AHcj.
0x0080 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 .... ...H.....AH
0x0090 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 cj..... ...H....
0x00A0 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A .AHcj..... ...H.
0x00B0 B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 ....AHcj..... ..
0x00C0 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 .H.....AHcj.....
0x00D0 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08  ...H.....AHcj..
0x00E0 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 ... ...H.....AHc
0x00F0 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 j..... ...H.....
0x0100 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 AHcj..... ...H..
0x0110 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 ...AHcj..... ...
0x0120 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 H.....AHcj..... 
0x0130 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 ...H.....AHcj...
0x0140 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A .. ...H.....AHcj
0x0150 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 ..... ...H.....A
0x0160 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 Hcj..... ...H...
0x0170 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 ..AHcj..... ...H
0x0180 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 .....AHcj..... .
0x0190 02 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 ..H.....AHcj....
0x01A0 06 20 00 02 01 48 9A B3 00 05 08 41 48 63 6A 08 . ...H.....AHcj.
0x01B0 08 00 08 06 20 00 02 01 48 9A B3 00 05 08 41 48 .... ...H.....AH
0x01C0 63 6A 08 08 00 08 06 20 00 02 01 48 9A B3 00 05 cj..... ...H....
0x01D0 08 41 48 63 6A 08 08 00 08 06 20 00 02 01 48 9A .AHcj..... ...H.
0x01E0 B3 00 05 08 41 48 63 6A 08 08 00 08 06 20 00 02 ....AHcj..... ..
0x01F0 01 48 9A B3 00 05 08 41 48 63 6A 08 08 00 08 06 .H.....AHcj.....</pre>
<p>Further analysis with my DSLogic showed that the controller itself requests a total of 4,128 bytes (4 kiB + 32 bytes) of Parameter Page data, which is filled with the same repeating data as shown above.</p>
<h3>Reset Quirks</h3>
<p>When looking at the logic analyzer data, I noticed that the controller sends the READ ID command twice, but the first time it does so without resetting the Flash (which should normally be done as soon as the chip is powered up!). The data that the Flash returned was… strange to say the least.</p>
<table>
<thead>
<tr>
<th>Byte Value (Hex)</th>
<th>Interpreted Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>98</td>
<td>
<ul>
<li>Manufacturer: Toshiba</li>
</ul>
</td>
</tr>
<tr>
<td>00</td>
<td>
<ul>
<li>I/O voltage: Unknown (no data)</li>
<li>Device capacity: Unknown (no data)</li>
</ul>
</td>
</tr>
<tr>
<td>90</td>
<td>
<ul>
<li>NAND type: SLC (Single-Level Cell / 1 bit per cell)</li>
<li>Flash dies per /CE: 1</li>
</ul>
</td>
</tr>
<tr>
<td>93</td>
<td>
<ul>
<li>Block size: 4 MB excluding spare area</li>
<li>Page size: 16,384 bytes / 16 kiB excluding spare area</li>
</ul>
</td>
</tr>
<tr>
<td>76</td>
<td>
<ul>
<li>Planes per /CE: 2</li>
</ul>
</td>
</tr>
<tr>
<td>72</td>
<td>
<ul>
<li>Interface type: Asynchronous</li>
<li>Process geometry: 70 nm planar</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>This confused me initially when I was trying to find the ID from the logic capture alone; after talking to a contact who has experience in NAND Flash data recovery, they said this is expected for SanDisk devices, which make liberal use of vendor-proprietary commands and data structures. If the fourth byte is to be believed, it says the block size is 4 megabytes, which I think is plausible for a modern Flash device. The rest of the information doesn’t really make any sense to me apart from the first byte indicating the chip is made by Toshiba.</p>
<h2>Conclusion</h2>
<p>I shouldn’t have to go this far in hardware reverse-engineering to just ask a simple question of what Flash SanDisk used in their high-endurance card. You’d think they would be proud to say they use 3D NAND for higher endurance and reliability, but I guess not!</p>
<h2>Downloads</h2>
<p>For those that are interested, I’ve included the logic captures of the card’s activity shortly after power-up. I’ve also included the (very crude) Arduino sketch that I used to read the NAND ID and Parameter Page data manually:</p>
<ul>
<li><a href="https://www.dropbox.com/s/8yolkc756q37mnh/sandisk%20he%20128%20power%20on.dsl?dl=0" target="_blank" rel="noopener">Logic capture #1</a></li>
<li><a href="https://www.dropbox.com/s/hhkl3c0jyk366c3/sandisk%20he%20128%20power%20on%202.dsl?dl=0" target="_blank" rel="noopener">Logic capture #2</a></li>
<li><a href="https://github.com/ginbot86/2433_nandflashtest" target="_blank" rel="noopener">NAND I/O Arduino sketch</a></li>
</ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Analyzing the codebase of Caffeine: a high performance caching library (204 pts)]]></title>
            <link>https://adriacabeza.github.io/2024/07/12/caffeine-cache.html</link>
            <guid>42907488</guid>
            <pubDate>Sun, 02 Feb 2025 09:37:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://adriacabeza.github.io/2024/07/12/caffeine-cache.html">https://adriacabeza.github.io/2024/07/12/caffeine-cache.html</a>, See on <a href="https://news.ycombinator.com/item?id=42907488">Hacker News</a></p>
Couldn't get https://adriacabeza.github.io/2024/07/12/caffeine-cache.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Life Is More Than an Engineering Problem – Interview with Ted Chiang (327 pts)]]></title>
            <link>https://lareviewofbooks.org/article/life-is-more-than-an-engineering-problem/</link>
            <guid>42907268</guid>
            <pubDate>Sun, 02 Feb 2025 08:53:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lareviewofbooks.org/article/life-is-more-than-an-engineering-problem/">https://lareviewofbooks.org/article/life-is-more-than-an-engineering-problem/</a>, See on <a href="https://news.ycombinator.com/item?id=42907268">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><div><h2>Julien Crockett speaks with Ted Chiang about the search for a perfect language, the state of AI, and the future direction of technology.</h2><div><p><img loading="lazy" decoding="async" data-nimg="fill" sizes="
          (max-width: 768px) 100vw, 
          (max-width: 960px) 750px, 
          750px
        " srcset="https://cdn.lareviewofbooks.org/unsafe/640x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg 640w, https://cdn.lareviewofbooks.org/unsafe/750x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg 750w, https://cdn.lareviewofbooks.org/unsafe/828x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg 828w, https://cdn.lareviewofbooks.org/unsafe/1080x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg 1080w, https://cdn.lareviewofbooks.org/unsafe/1200x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg 1200w, https://cdn.lareviewofbooks.org/unsafe/1920x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg 1920w, https://cdn.lareviewofbooks.org/unsafe/2048x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg 2048w, https://cdn.lareviewofbooks.org/unsafe/3840x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg 3840w" src="https://cdn.lareviewofbooks.org/unsafe/3840x0/filters:format(jpeg):quality(75)/https%3A%2F%2Fassets.lareviewofbooks.org%2Fuploads%2Flifecycle-softwareobjects-web__85640.jpg"></p></div></div><p><em>This interview is part of </em><a rel="" target="_self" href="https://lareviewofbooks.org/feature/the-rules-we-live-by/"><em>The Rules We Live By,</em></a><em> a series devoted to asking what it means to be a human living by an ever-evolving set of rules. The series is made up of conversations with those who dictate, think deeply about, and seek to bend or break the rules we live by.</em></p><p>¤</p><p><em>“ONCE IN A WHILE,” Ted Chiang tells me, “an idea keeps coming back over a period of months or even years. […] I start asking, is there an interesting philosophical question that might be illuminated by this idea?” To read Chiang is to experience a master world-builder critically exploring philosophical questions in new ways—from how we should care for an artificial being to what would be the consequence of having a perfect record of the past.</em></p><p><em>Lately, Chiang has trained his eye on artificial intelligence. And Chiang’s takes haven’t gone unnoticed. In a </em><a rel="" target="_self" href="https://lareviewofbooks.org/article/how-to-raise-your-artificial-intelligence-a-conversation-with-alison-gopnik-and-melanie-mitchell/"><em>conversation</em></a><em> I had earlier this year with computer scientist Melanie Mitchell and psychologist Alison Gopnik, they each referenced Chiang when searching for the right framework to discuss AI.</em></p><p><em>Chiang has a knack for descriptively illustrating his points. For example, when discussing whether LLMs might one day develop subjective experience, he explains: “It’s like imagining that a printer could actually feel pain because it can print bumper stickers with the words ‘Baby don’t hurt me’ on them. It doesn’t matter if the next version of the printer can print out those stickers faster, or if it can format the text in bold red capital letters instead of small black ones. Those are indicators that you have a more capable printer but not indicators that it is any closer to actually feeling anything.”</em></p><p><em>For me, the essence of Chiang’s work, however, isn’t his critical take on technology. It’s his humanism—the way he brings to the fore the mundane reality behind existential questions and moments of societal change. It is perhaps for this reason that his work resonates with so many.</em></p><p><em>In our conversation, we discuss how Chiang picks his subjects, the historical search for a perfect language, the state of AI, and what it would take for Chiang to become hopeful about the direction of technology.</em></p><p>¤</p><p><strong>JULIEN CROCKETT: The idea for this interview came from a conversation I had with computer scientist Melanie Mitchell and psychologist Alison Gopnik, in which they referenced your work when describing the state of artificial intelligence and its possible futures. Why do you think scientists and engineers look to your work to explain their own?</strong></p><p><strong>TED CHIANG:</strong> I was actually surprised that my name popped up. I think it was mostly just a coincidence that you interviewed two people who have found that my work resonates with their own. They might be outliers, compared to scientists as a whole.</p><p><strong>What about the other way around—has scientific progress had an effect on the direction of your work?</strong></p><p>I don’t think there has been a clear impact of recent scientific research on my fiction. My stories are mostly motivated by philosophical questions, many of which are not particularly new. Sometimes the way I investigate a philosophical question is inflected by recent developments in science or technology, but the recent developments probably aren’t the motivating impulse.</p><p><strong>Your stories cover a wide range of topics, but I see a through line in your focus on how humans react to societal change—whether it’s a discovery that mathematics is actually inconsistent, as in your 1991 story “Division by Zero,” or a world where we raise robots as children, as in your 2010 novella </strong><em><strong>The Lifecyle of Software Objects</strong></em><strong>. What draws you to a topic?</strong></p><p>Most of the time, when ideas come to me, they leave my attention very quickly. But once in a while, an idea keeps coming back over a period of months or even years. I take that as a signal that I should pay attention and think about the idea in a more intentional way. What that usually means is that I start asking, “Is there an interesting philosophical question that might be illuminated by this idea?” If I can identify that philosophical question, then I can start thinking about different ways a story might help me dramatize it.</p><p><strong>Why is science fiction the best vehicle for you to explore ideas?</strong></p><p>The ideas that most interest me just lean in a science-fictional direction. I certainly think that contemporary mimetic fiction is capable of investigating philosophical questions, but the philosophical questions that I find myself drawn to require more speculative scenarios. In fact, when philosophers pose thought experiments, the scenarios they describe often have a science-fictional feel; they need a significant departure from reality to highlight the issue they’re getting at. When a philosophical thought experiment is supposed to be set in the actual world, the situation often has a contrived quality. For example, the famous “trolley problem” is supposedly set in the actual world, but it describes a situation that is extremely artificial; in the real world, we have safeguards precisely to avoid situations like that.</p><p><strong>What role does science play in your stories? Or, asked another way, what are the different roles played by science and magic in fiction?</strong></p><p>Some people think of science as a body of facts, and the facts that science has collected are important to our modern way of life. But you can also think about science as a process, as a way of understanding the universe. You can write fiction that is consistent with the specific body of facts we have, or you can write fiction that reflects the scientific worldview, even if it is not consistent with that body of facts. For example, take a story where there is faster-than-light travel. Faster-than-light travel is impossible, but the story can otherwise reflect the general worldview of science: the idea that the universe is an extremely complicated machine, and through careful observation, we can deduce the principles by which this machine works and then apply what we’ve learned to develop technology based on those principles. Such a story is faithful to the scientific worldview, so I would argue that it’s a science fiction story even if it is not consistent with the body of facts we currently have.</p><p>By contrast, magic implies a different understanding of how the universe works. Magic is hard to define. A lot of people would say magic definitionally cannot have rules, and that’s one popular way of looking at it. But I have a different take—I would say that magic is evidence that the universe knows you’re a person. It’s not that magic cannot have rules; it’s that the rules are more like the patterns of human psychology or of interactions between people. Magic means that the universe is behaving not as a giant machine but as something that is aware of you as a person who is different from other people, and that people are different from things. At some level, the universe responds to your intentions in a way that the laws of physics as we understand them don’t.</p><p>These are two very different ways of understanding how the universe works, and fiction can engage in either one. Science needs to adhere to the scientific worldview, but fiction is not an engineering project. The author can choose whichever one is better suited to their goals.</p><p><strong>Your work often explores the way tools mediate our relationship with reality. One such tool is language. You write about language perhaps most popularly in “Story of Your Life” (1998), the basis for the film </strong><em><strong>Arrival</strong></em><strong> (2016), but also in “Understand” (1991), exploring what would happen if we had a medical treatment for increasing intelligence. Receiving the treatment after an accident, the main character grows frustrated by the limits of conventional language:</strong></p><br><blockquote><strong>I’m designing a new language. I’ve reached the limits of conventional languages, and now they frustrate my attempts to progress further. They lack the power to express concepts that I need, and even in their own domain, they’re imprecise and unwieldy. They’re hardly fit for speech, let alone thought. […]</strong></blockquote><blockquote><strong>&nbsp;</strong></blockquote><blockquote><strong>I’ll reevaluate basic logic to determine the suitable atomic components for my language. This language will support a dialect coexpressive with all of mathematics, so that any equation I write will have a linguistic equivalent.</strong></blockquote><p><strong>Do you think there could be a “better” language? Or is it just mathematics?</strong></p><p>Umberto Eco wrote a book called <em>The Search for the Perfect Language</em> (1994), which is a history of the idea that there exists a perfect language. At one point in history, scholars believed the perfect language was the language that Adam and Eve spoke in the Garden of Eden or the language angels speak. Later on, scholars shifted to the idea that it was possible to construct an artificial language that was perfect, in the sense that it would be completely unambiguous and bear a direct relationship to reality.</p><p>Modern linguistics holds that this idea is nonsensical. It’s foundational to our modern conception of language that the relationship between any given word and the concept it is assigned to is arbitrary. But I think that many of us can relate to the desire for a language that expresses exactly what we mean unambiguously. We’ve all tried to convey something and wished there were a word for it, but that’s not a problem of English or French or German—that’s a problem of language itself. And even though I know a perfect language is impossible, the idea continues to fascinate me.</p><p>As for the question of whether mathematics could be a better language, the reason that mathematics is useful is precisely what makes it unsuitable as a general language. Mathematics is extremely precise, but it’s limited to a specific domain. Scientists who speak different languages can use the same mathematics, but they still have to rely on their native languages when they publish a paper; they can’t say everything they need to say with equations alone. Language has to support every type of communication that humans engage in, from debates between politicians to pillow talk between lovers. That’s not what mathematics is for. We could be holding this conversation in any human language that we both understand, but we couldn’t hold it in mathematical equations. As soon as you try and modify mathematics so that it can do those things, it ceases to be mathematics.</p><p><strong>I grew up in a French household, and I often feel that there are French words and expressions that better capture what I want to express than any English word or expression could.</strong></p><p>Eco writes that when European scholars were arguing about what language Adam and Eve spoke, each one typically argued in favor of the language he himself spoke. So Flemish scholars said that Adam and Eve obviously must have spoken Flemish, because Flemish is the most perfect expression of human thought.</p><p><strong>Funny. Another tool increasingly mediating our relationship with society and reality is artificial intelligence. You’ve written skeptically about how modern AI systems are being implemented, and one metaphor you use to describe large language models (LLMs) is as </strong><a rel="" target="_self" href="https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web"><strong>“a blurry JPEG of the web.”</strong></a><strong> What do you mean?</strong></p><p>When we use a search engine, we get verbatim quotes from text on the internet and also a link to the original web page. A search engine gives us information directly from the horse’s mouth. LLMs are like a search engine that rephrases information instead of giving it verbatim or pointing you to the original source. In some respects, that is really cool, but they’re not rephrasing it reliably. It’s like asking a question and getting an answer back from someone who read the answer but didn’t really understand it and is trying to rephrase it to the best of their ability. I call LLMs a blurry JPEG because they give a low-resolution version of the internet. If you are using the internet to find information, which is what most of us use the internet for, it doesn’t really make sense to go with the low-resolution version when we have conventional search engines that point you to the actual information itself.</p><p>It’s entertaining to be able to ask a question and get an answer back in a conversational form, but LLMs are not being marketed as entertainment devices. They’re being marketed as products that will answer your questions accurately, and that’s not what LLMs are doing.</p><p><strong>Do you think that LLMs will become useful tools that can reliably answer questions?</strong></p><p>I don’t want to say LLMs are only good for entertainment; there are many respects in which LLMs are genuinely amazing. The fact that they can rephrase something in any style of prose is fascinating; no one would have predicted that statistical models of all the text on the internet would be capable of that. But predicting the most likely next word is different from having correct information about the world, which is why LLMs are not a reliable way to get the answers to questions, and I don’t think there is good evidence to suggest that they will become reliable. Over the past couple of years, there have been some papers published suggesting that training LLMs on more data and throwing more processing power at the problem provides diminishing returns in terms of performance. They can get better at reproducing patterns found online, but they don’t become capable of actual reasoning; it seems that the problem is fundamental to their architecture. And you can bolt tools onto the side of an LLM, like giving it a calculator it can use when you ask it a math problem, or giving it access to a search engine when you want up-to-date information, but putting reliable tools under the control of an unreliable program is not enough to make the controlling program reliable. I think we will need a different approach if we want a truly reliable question answerer.</p><p><strong>Modern AI systems are one tool in a long line of tools that help us approximate reality. We seem to easily ascribe attributes of ourselves, such as thought and reasoning, to these tools. For example, we regularly describe the brain as a computer and vice versa. Why do you think we see ourselves in our tools?</strong></p><p>There was a time when people compared the brain to a telephone switchboard. The brain is the most complex thing we have ever encountered, and when the telephone switchboard was the most complicated machine we had ever built, we naturally used it as a metaphor when trying to understand what the brain is. But that doesn’t actually tell us anything about how the brain works. The fact that we can now build computers doesn’t mean that the brain is more like a computer than a telephone switchboard. There are many ways in which it is obvious that the brain is not like a computer, but because the computer metaphor is so prevalent, we overlook those differences. Computers consist of software running on hardware, but there is no distinction between software and hardware in biological systems. If you were to apply that metaphor to any other organ in the body, it would seem absurd. For example, “My liver was running this old program, but all I needed to do was update the software and now my liver is functioning much better, even though the hardware is the same.” No one says that. It’s not a useful way of thinking about the liver, and it is not a useful way of thinking about the brain either.</p><p>By the same token, because we imagine that computers are like brains, we are tempted to think of computers as intelligent and engaged in thinking. When we do that, we’re taking this metaphor way too literally. What telephone switchboards did was not so readily mappable to something people do, and that probably deterred people from imagining that a telephone switchboard was engaged in reasoning. But LLMs can generate plausible text, which totally throws us for a loop. LLMs are not engaged in reasoning any more than a telephone switchboard was, but their ability to simulate conversation makes it far easier to imagine that they are.</p><p><strong>You wrote an article called </strong><a rel="" target="_self" href="https://www.newyorker.com/culture/the-weekend-essay/why-ai-isnt-going-to-make-art"><strong>“Why A.I. Isn’t Going to Make Art”</strong></a><strong> where you argue that generative artificial intelligence reduces the amount of intention in the world because, by using AI tools, we lose the opportunity to make the choices necessary for creating art. What impact are AI tools having on artists and their artwork?</strong></p><p>I should note that I didn’t pick the title of the essay; if I had, I might have called it something like “Why AI Won’t Make Art Easy to Make.” Many people would have you believe that the process of making art and the end result can be easily separated, but I don’t believe they can be. I was talking with someone who is very excited about AI-generated imagery, and she said, “Let’s imagine, for the sake of argument, that AI can make better art than humans. In that scenario, do you think that we should reject AI art simply to protect the livelihood of human artists?” I responded, “I’m not going to grant you that premise, because that is the question under debate. You are framing the hypothetical in a way that assumes the conclusion.” I don’t believe it’s meaningful to say that something is better art absent any context of how it was created. Art is all about context. It’s not an activity like tightening bolts, where I don’t really care whether someone used a conventional wrench or a pneumatic wrench, as long as the bolts are tight.</p><p>As for the impact on artists, I’d say the primary effect of AI tools is that they encourage the idea that art is no different from tightening bolts. Artists have always had to deal with commercial considerations, but it’s probably a more pressing issue now than ever before. The impulse to view everything in terms of efficiency, of reducing costs and maximizing output, is radically overapplied in the modern world. There are certain situations in which that is an appropriate framing, but art cannot be understood that way. Arguably the most important parts of our lives should not be approached with this attitude. Some of this attitude comes from the fact that the people making AI tools are engineers viewing everything from an engineering perspective, but it’s also that, as a culture, we have adopted this way of thinking as the default.</p><p><strong>In French, we would call engineers viewing everything as an engineering problem a “professional deformity.” But an issue perhaps tangentially related to viewing everything as a wrench is “the alignment problem.” Your novella </strong><em><strong>The Lifecycle of Software Objects</strong></em><strong> has been referenced in this context. For example, Alison Gopnik talks about how one way to “align” artificial intelligence with our goals and values could be the same way we align each new generation of humans, through caregiving.</strong></p><p>I don’t like the phrase “the alignment problem.” It’s not clear to me that it refers to something meaningful—or at least that the phrase refers to something that is new and meaningfully different from the broader problems of how to be a good person and how to build a good society. For example, when corporations behave badly, should we consider that an alignment problem? Most of the conversation around the alignment problem suggests that it’s a technical problem, something that can be addressed by implementing a better algorithm or by solving the right equations. But why, for example, do large corporations behave so much worse than most of the people who work for them? I think most of the people who work for large corporations are, to varying degrees, unhappy with the effect those corporations have on the world. Why is that? And could that be fixed by solving a math problem? I don’t think so.</p><p>People who talk about aligning AI with human values imagine that if we could somehow solve this programming problem, then everything would be okay. I don’t see how that follows at all. Imagine you have some hypothetical AI that is better at accomplishing tasks than humans and that does exactly what you tell it to do. Do you want ExxonMobil to have such an AI at its disposal? That doesn’t sound good. Conversely, imagine a hypothetical AI that does what is best for the world as a whole, even if human beings are asking it to do something else. Who would buy such an AI? Certainly not ExxonMobil. I can’t see any corporation buying software that ignores the instructions of humans and does what is best for the world. If that were something that corporations were interested in, do you think they’d be behaving the way they are now?</p><p><strong>But there is something intuitively appealing about the idea of taking what we know about raising children and applying it to an intelligent system, like an AI system, that seems to learn—even if it might not learn in the same way we do.</strong></p><p>The question of whether we can teach AI our values the way parents teach children their values is a very interesting one to me, philosophically. An extremely common ethical guideline is that you should treat others the way you would like to be treated, and this is something that parents try to impress upon their children. When parents do that, they are asking children to put themselves in another person’s place and imagine what their emotional reaction would be, and children often can’t or don’t want to do this, which is why it can take a while for children to learn to play well with others. What would it mean for a machine to do that? We have no idea how to build a machine capable of that. And even if you successfully teach a child to play well with others, that is no guarantee that the child will become an adult who contributes to a good society. The executives of ExxonMobil were almost certainly taught this ethical guideline at some point, and look how well that turned out.</p><p><strong>Could there be value, though, in treating an AI system as more of a partner—something or someone with whom we develop a relationship—rather than merely as a tool?</strong></p><p>It all depends on what you mean by “relationship.” If you’re a woodworker, you might develop emotional associations with a set of chisels you’ve used for years, and in some sense that’s a “relationship,” but it’s entirely different from the relationship you have with people. You might make sure you keep your chisels sharp and rust-free, and say that you’re treating them with respect, but that’s entirely different from the respect you owe to your colleagues. One way to clarify this is to remember that people have their own preferences, while things do not. To respect your colleagues means to pay attention to their preferences and interests and balance them against your own; when they do this to you in return, you have a good relationship. By contrast, your chisel has no preferences; it doesn’t want to be sharp. When you keep it sharp, you are doing so because it will help you do good work or because it gives you a feeling of satisfaction to know that it’s sharp. Either way, you are only serving your own interests, and that’s fine because a chisel is just a tool. If you don’t keep it sharp, you are only harming yourself. By contrast, if you don’t respect your colleagues, there is a problem beyond the fact that it might make your job harder; you do them harm because you are ignoring their preferences. That’s why we consider it wrong to treat a person like a tool; by acting as if they don’t have preferences, you are dehumanizing them.</p><p>AI systems lack preferences; that is true of the systems we have now, and it will be true of any system we build in the foreseeable future. The companies that sell AI systems might benefit if you develop an emotional relationship with their product, so they might create the illusion that AI systems have preferences. But any attempt to encourage people to treat AI systems with respect should be understood as an attempt to make people defer to corporate interests. It might have value to corporations, but there is no value for you.</p><p><strong>In </strong><em><strong>The Lifecycle of Software Objects</strong></em><strong>, the humans develop deep emotional relationships with their digital agents. What characteristics do those digital agents have that make such relationships possible?</strong></p><p>The digital entities in that story have genuine interests and preferences. The premise of the story is that, even though they’re digital, they are in a certain sense alive and have subjective experience. If you’re a responsible pet owner, you will inconvenience yourself to fulfill your pet’s needs, both their physical needs and their psychological ones. The human characters in the story recognize that they have a similar responsibility to their digital pets. They even come to realize that they can’t escape those responsibilities by simply suspending their digital pets the way you might put your laptop in hibernate mode. As an analogy, imagine that you could put your dog or cat into hibernate mode whenever you left on a trip. Your dog or cat might not notice, but even if they did, they might not mind. Now imagine that you could put your child into hibernate mode whenever you were too busy to spend time with them. Your child would absolutely notice, and even if you told them it was for their own good, they would make certain inferences about how much you valued them. That’s the situation the human characters in the story find themselves in.</p><p><strong>Could AI systems one day have those characteristics?</strong></p><p>I believe it’s theoretically possible for us to build digital entities that have subjective experience, inasmuch as I don’t think there’s a physical law that prevents it. We don’t currently have a good idea of how to build such entities. I don’t think we’re going to create them accidentally, because the AI systems we’re building right now are not even heading in the right direction. LLMs are not going to develop subjective experience no matter how big they get. It’s like imagining that a printer could actually feel pain because it can print bumper stickers with the words “Baby don’t hurt me” on them. It doesn’t matter if the next version of the printer can print out those stickers faster, or if it can format the text in bold red capital letters instead of small black ones. Those are indicators that you have a more capable printer but not indicators that it is any closer to actually feeling anything.</p><p><strong>The technology we use also impacts our relationships with one another. In your 2013 story “The Truth of Fact, the Truth of Feeling,” you investigate the unintended consequences that Remem—a product that creates a perfect record of the past—has on human relationships. It seems like the takeaway from your story is that some things are more important than truth</strong>.</p><p>I wouldn’t say that some things are more important than truth. What I was hoping to convey with that story is that there is value in knowing what actually happened, but that is not the end of the discussion. Ideally, we should be able to acknowledge what actually happened without that being the last word on the subject.</p><p><strong>How does that work at a societal level?</strong></p><p>Take the Truth and Reconciliation Commission in South Africa after the fall of apartheid. The truth is essential; it is the only basis from which you can move forward productively. You cannot deny what happened and expect a healthy society to result from that. But once everyone has admitted what they did, there is the opportunity for forgiveness. Society can decide whether punishment is called for and what form it should take; in certain situations, maybe admitting one’s guilt is enough. Once you’ve achieved some kind of reconciliation, it becomes possible to move forward.</p><p><strong>I want to end by asking whether you are optimistic about the future. When I’ve asked this question in previous interviews, some have responded that they are optimistic because it’s a moral duty; it’s what we must be if we want to create a better future. Do you view optimism or hope in this way?</strong></p><p>As usual, we need to be specific about what we mean by “optimism” and “pessimism.” Some people believe that everything will work out fine and we don’t need to devote energy to considering bad outcomes. I think this attitude is extremely common in the tech industry. That’s a kind of optimism, and I definitely don’t fall into that camp. By contrast, some people believe that bad outcomes are inevitable and there’s nothing we can do prevent them. Some might call that pessimism, but I’d say that’s closer to fatalism.</p><p>I think we need to think about the possible bad outcomes and work to mitigate them; if we do that, we have a chance of preventing them from coming to pass. I don’t know if that’s optimism, unless everything except fatalism is optimism. I suppose it might be a moral duty to not be fatalistic. We have to believe that our actions have the potential to make a difference because if we don’t believe that, we won’t take any action at all.</p><p>You can also consider this question within the narrower context of technological development, and ask whether one thinks that the risk of bad outcomes is serious enough that we should slow down our pursuit of new technologies. In this framing, optimists are the ones who say no, the risks aren’t that serious, while pessimists are the ones who say yes, the risks are very serious. My stance on this has probably shifted in a negative direction over time, primarily because of my growing awareness of how often technology is used for wealth accumulation. I don’t think capitalism will solve the problems that capitalism creates, so I’d be much more optimistic about technological development if we could prevent it from making a few people extremely rich.</p><p>¤</p><p><em>Ted Chiang’s fiction has won four Hugo, four Nebula, and four Locus Awards, and has been featured in&nbsp;The Best American Short Stories. His debut collection,&nbsp;</em>Stories of Your Life and Others<em> (2002), has been translated into 21 languages. He was born in Port Jefferson, New York, and currently lives near Seattle.</em></p><p>¤</p><p><em>Featured image: Cover of </em>The Lifecycle of Software Objects<em>, illustration by Christian Pearce.</em></p><div><p>LARB Contributor</p><p>Julien Crockett is an intellectual property attorney and the science and law editor at the <em>Los Angeles Review of Books</em>. He runs the <em>LARB</em> column <a rel="" target="_self" href="https://lareviewofbooks.org/feature/the-rules-we-live-by/">The Rules We Live By,</a> exploring what it means to be a human living by an ever-evolving set of rules.</p></div><section><h3>LARB Staff Recommendations</h3><ul><li><article><h2><a href="https://lareviewofbooks.org/article/the-technologies-that-remake-us-on-ted-chiangs-exhalation-stories/">The Technologies That Remake Us: On Ted Chiang’s “Exhalation: Stories”</a></h2><p>“Exhalation: Stories” is a stunning achievement in speculative fiction, from an author whose star will only continue to rise.</p></article></li><li><article><h2><a href="https://lareviewofbooks.org/article/how-to-raise-your-artificial-intelligence-a-conversation-with-alison-gopnik-and-melanie-mitchell/">How to Raise Your Artificial Intelligence: A Conversation with Alison Gopnik and Melanie Mitchell</a></h2><p>Julien Crockett interviews Alison Gopnik and Melanie Mitchell about complexity and learning in AI systems, and our roles as caregivers.</p><p><span><a href="https://lareviewofbooks.org/contributor/julien-crockett/">Julien Crockett</a></span><span>May 31, 2024</span></p></article></li></ul></section><div><h4><a href="https://lareviewofbooks.org/donate/">Did you enjoy this article?</a></h4><hr><p>LARB depends on the support of readers to publish daily without a paywall. Please support the continued work of our writers and staff by making a tax-deductible donation today!</p></div></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Fixing left and mutual recursions in grammars (101 pts)]]></title>
            <link>https://brightprogrammer.in/posts/fixing-recursions-in-grammar/</link>
            <guid>42907139</guid>
            <pubDate>Sun, 02 Feb 2025 08:31:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brightprogrammer.in/posts/fixing-recursions-in-grammar/">https://brightprogrammer.in/posts/fixing-recursions-in-grammar/</a>, See on <a href="https://news.ycombinator.com/item?id=42907139">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><a href="#further-reading">\[
\text{A long-standing issue regarding algorithms that manipulate} \\
\text{context-free grammars (CFGs) in a “top-down” left-to-right fashion} \\
\text{is that left recursion can lead to nontermination.} ^{\text{[1]}}
\]</a></p><h2 id="fixing-left-recursion">Fixing Left Recursion</h2><p>A grammar is left recursive if it comes in the following form</p><p>\[
\langle S \rangle ::= \langle S \rangle \ a \mid b
\]</p><p>When parsing or generating a string using this grammar (using a backtracking algorithm),
where $ \langle S \rangle $ is the start symbol, you
are likely to enter a state of infinite recursion. Any ideas on how to fix this?</p><p>\[
\langle S \rangle ::= b \mid \langle S \rangle \ a
\]</p><p>Rewriting it in above form won’t change a thing, except the fact that it’ll try to read or
generate the terminal $b$ first. How do we fix this then? The root of our problem is
left recursion, so if we can somehow make it right recursive then it’ll fix our issue.
Before reading any further, give it a try yourself.</p><center><p>---
title: example string "aaa"
---
stateDiagram-v2
[*] --&gt; S
S --&gt; b
S --&gt; Sa
Sa --&gt; S
b --&gt; [*]</p></center><p>For given example string $\text{aaa}$ the parser will never terminate. Not even to inform
us that the string does not belong to language generated by corresponding grammar.</p><h2 id="language-analysis">Language Analysis</h2><p>Let’s start by generating some sample strings in the given language, and try to discover
a pattern. The following are expansion paths for $ \langle S \rangle $ :</p><p>\[
\begin{align}
L(S) &amp;= b \\
L(S) &amp;= \langle S \rangle a \rightarrow
\langle S \rangle \ a \ a \rightarrow
\langle S \rangle \ a \ a \ … \ a \rightarrow
b \ a \ a \ … \ a
\end{align}
\]</p><p>So we have two possible expansions $b$ or $b \ a^*$. The symbol $*$ means as many repetitions
of anything that comes before it. It can be nothing, like an empty string, or it can be
something. So, this lets us know that a left recursive grammar always starts with $b$,
and then you generate or match as many $a$ as possible. The interesting thing about $a^*$
is that it can be generated with right recursion as well, with a rule like :</p><p>\[
\langle N \rangle = a \langle N \rangle \mid \epsilon
\]</p><center><p>stateDiagram-v2
[*] --&gt; N
N --&gt; aN : prepend a
aN --&gt; N : expand N
N --&gt; [*]</p></center><p>Now this can also be generated with a left recursion as well, but we’re doing all this
just to avoid it, so we only care about right recursion. Now combining all the information
we’ve gather up until now, we can</p><p>\[
\begin{align}
\langle S \rangle &amp;::= b \langle S’ \rangle \\
\langle S’ \rangle &amp;::= a \langle S’ \rangle \mid a
\end{align}
\]</p><p>This grammar as you can see is completely right recursive.</p><center><p>---
title: example string "aaa"
---
stateDiagram-v2
[*] --&gt; S
S --&gt; bS'
S' --&gt; aS'
S' --&gt; a
bS' --&gt; S'
aS' --&gt; S'
a --&gt; [*]</p></center><p>This new machine will quickly terminate when it notices that the example string $ \text{aaa} $ does not start
with $b$ like in our language.</p><h2 id="real-life-example">Real Life Example</h2><p>So, I’ve been (re)writing a C++ demangler for RizinOrg’s <a href="https://github.com/rizinorg/rz-libdemangle/pull/69">rz-libdemangle</a>
which was previously licensed to GPL, and I’m re-writing it to relicense it to LGPL v3, which is more
permissive for commercial usage. You are allowed to link to precompiled binaries licensed with LGPLv3.</p><p>While re-writing grammar for <a href="https://files.brightprogrammer.in/cxx-abi/abi.html">Itanium ABI</a>, I encountered
a left recursive grammar.</p><p>\[
\begin{align}
\langle \text{prefix} \rangle &amp;::= \langle \text{unqualified-name} \rangle \\
&amp;\quad \mid \langle \text{prefix} \rangle \langle \text{unqualified-name} \rangle \\
&amp;\quad \mid \langle \text{template-prefix} \rangle \langle \text{template-args} \rangle \\
&amp;\quad \mid \langle \text{closure-prefix} \rangle \\
&amp;\quad \mid \langle \text{template-param} \rangle \\
&amp;\quad \mid \langle \text{decltype} \rangle \\
&amp;\quad \mid \langle \text{substitution} \rangle
\end{align}
\]</p><p>Notice how production number $6$ is left recursive in nature. Wanna know how I fixed it in code?
You’ll have to follow this post a bit more. Real life examples are bit more complex than theory.
Life happend here as well, and made some things not-so-straightforward.</p><h2 id="mutual-left-recursion">Mutual Left Recursion</h2><p>A mutual left recursion happens when two rules mutually call each other by expanding
first non terminal as other rule. It generally looks like this :</p><p>\[
\begin{align}
\langle S \rangle &amp;::= \langle A \rangle \mid \langle B \rangle \\
\langle A \rangle &amp;::= \langle B \rangle k \mid X \\
\langle B \rangle &amp;::= \langle A \rangle m \mid Y
\end{align}
\]</p><p>Notice how production $13$ and $14$ call each other, making it a mutual recursion.
Using a similar analysis as done for simple left recursion, we can remove left recursion
here as well, by making each rule right recursion only.</p><center><p>stateDiagram-v2
[*] --&gt; S
S --&gt; A
S --&gt; B
A --&gt; Bk
Bk --&gt; B
B --&gt; Am
Am --&gt; A
A --&gt; X
B --&gt; Y
X --&gt; [*]
Y --&gt; [*]</p></center><p>You can visually see the mutual recursion in path $ \textbf{A} \rightarrow \text{Bk} \rightarrow \textbf{B} \rightarrow \text{Am} \rightarrow \textbf{A} $,
wherein $A$ ends up calling $B$ which eventually calls $A$.</p><h2 id="language-analysis-1">Language Analysis</h2><p>If you try to follow the pattern, then you’ll get the following possible languages</p><p>\[
\begin{align}
L(A) &amp;= X (mk)^* \\
L(A) &amp;= Y k(mk)^* \\
L(B) &amp;= Y (km)^* \\
L(B) &amp;= X m(km)^*
\end{align}
\]</p><p>Noticing this pattern again, I can clearly see the right recursion this time again,
so, I’ll re-write the mutually left recursive grammar as follows</p><p>\[
\begin{align}
\langle S \rangle &amp; ::= &amp; \langle S1 \rangle \mid \langle S2 \rangle \\
\langle S1 \rangle &amp; ::= &amp; \langle A \rangle \mid \langle A \rangle \langle R1 \rangle \\
\langle S2 \rangle &amp; ::= &amp; \langle B \rangle \mid \langle B \rangle \langle R2 \rangle \\
\langle A \rangle &amp; ::= &amp; X \mid Y k \\
\langle B \rangle &amp; ::= &amp; Y \mid X m \\
\langle R1 \rangle &amp; ::= &amp; m k \langle R1 \rangle \mid m k \\
\langle R2 \rangle &amp; ::= &amp; k m \langle R2 \rangle \mid k m
\end{align}
\]</p><p>Again, notice that since the only recursive productions $20$ and $21$ are right
recursive only, the whole grammar is right recursive, and at the same time,
the grammar is no more mutually left recursive.</p><center><p>stateDiagram-v2
[*] --&gt; S
S --&gt; S1
S --&gt; S2
S1 --&gt; A
S1 --&gt; AR1
AR1 --&gt; R1 : expand R1
R1 --&gt; mkR1
mkR1 --&gt; R1
S2 --&gt; B
S2 --&gt; BR2
BR2 --&gt; R2 : expand R2
R2 --&gt; kmR2
kmR2 --&gt; R2
A --&gt; X
A --&gt; Yk
B --&gt; Y
B --&gt; Xm
X --&gt; [*]
Y --&gt; [*]
Yk --&gt; [*]
Xm --&gt; [*]
R1 --&gt; mk
R2 --&gt; km
mk --&gt; [*]
km --&gt; [*]</p></center><h2 id="real-life-example-1">Real Life Example</h2><p>So, while (re)writing the demangler, I came across some rules that are mutually
recursive. Take a look at the following grammar, and comment what you see :</p><p>\[
\begin{align}
\langle \text{prefix} \rangle &amp;::= \langle \text{unqualified-name} \rangle \\
&amp;\quad \mid \langle \text{prefix} \rangle \langle \text{unqualified-name} \rangle \\
&amp;\quad \mid \langle \text{template-prefix} \rangle \langle \text{template-args} \rangle \\
&amp;\quad \mid \langle \text{closure-prefix} \rangle \\
&amp;\quad \mid \langle \text{template-param} \rangle \\
&amp;\quad \mid \langle \text{decltype} \rangle \\
&amp;\quad \mid \langle \text{substitution} \rangle
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{template-prefix} \rangle &amp;::= \langle \text{template unqualified-name} \rangle \\
&amp;\quad \mid \langle \text{prefix} \rangle \langle \text{template unqualified-name} \rangle \\
&amp;\quad \mid \langle \text{template-param} \rangle \\
&amp;\quad \mid \langle \text{substitution} \rangle
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{closure-prefix} \rangle &amp;::= [ \langle \text{prefix} \rangle ] \langle \text{variable or member unqualified-name} \rangle M \\
&amp;\quad \mid \langle \text{variable template template-prefix} \rangle \langle \text{template-args} \rangle M
\end{align}
\]</p><p>Did you notice it? $\langle \text{prefix} \rangle$ is mutually left recursive with $\langle \text{template-prefix} \rangle$
and $\langle \text{closure-prefix} \rangle$ at the same time, and this is where things get a bit complicated.
The solution is not hard, it’s really simple though. I’ll have to show my original solution though.</p><h2 id="solution">Solution</h2><p>For making things easy while writing the demangler, and for easy implementation of grammar rules
and productions, I’ve devised some macros that make it look like I’m using a DSL to write the demangler.
So, to help you understand, I’ll just show you the before and after code, and leave you to diff these out
by yourself to understand. You can see the complete source code in the PR, or in source code of
<a href="https://github.com/rizinorg/rz-libdemangle">rz-libdemangle</a> after my PR is merged.</p><h2 id="before-solving-anything">Before Solving Anything</h2><p>Before I’ve made any fixes to the grammar, here’s how the productions look in code :</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>DECL_RULE</span><span>(</span><span>prefix</span><span>);</span>
</span></span><span><span><span>DECL_RULE</span><span>(</span><span>template_prefix</span><span>);</span>
</span></span><span><span><span>DECL_RULE</span><span>(</span><span>closure_prefix</span><span>);</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>closure_prefix</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_param</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>decltype</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>substitution</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span><span>(</span><span>template_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_param</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>substitution</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span><span>(</span><span>closure_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE_OPTIONAL</span> <span>(</span><span>prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>variable_or_member_unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span> 
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>variable_template_template_name</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span> 
</span></span><span><span><span>});</span>
</span></span></code></pre></div><p>So? Do you notice many mutual recursions at the same time. How do we fix this?
To be honest, at the time of writing this, I haven’t checked the code, but I’m pretty sure
that my theory is right.</p><h2 id="after-the-changes">After The Changes</h2><div><pre tabindex="0"><code data-lang="c"><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_X</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_param</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>decltype</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>substitution</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_Yk_template_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_param</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>substitution</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_A_template_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_X</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_Yk_template_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_mk_template_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_S1_template_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_A_template_prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE_MANY</span> <span>(</span><span>prefix_mk_template_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_Yk_closure_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>variable_template_template_prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_A_closure_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_X</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_Yk_closure_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_mk_closure_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>variable_or_member_unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix_S1_closure_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_A_closure_prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE_MANY</span> <span>(</span><span>prefix_mk_closure_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>// fix left-recursion
</span></span></span><span><span><span></span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_X</span><span>)</span> <span>&amp;&amp;</span> <span>RULE_MANY</span> <span>(</span><span>unqualified_name</span><span>));</span>
</span></span><span><span>
</span></span><span><span>    <span>// fix mutual-recursions
</span></span></span><span><span><span></span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_S1_template_prefix</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>prefix_S1_closure_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span></code></pre></div><p>The changes made for $ \langle \text{closure-prefix} \rangle $ :</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>DEFN_RULE</span> <span>(</span><span>closure_prefix_Y</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>variable_template_template_prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>closure_prefix_Xm_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>variable_or_member_unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_param</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>variable_or_member_unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>decltype</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>variable_or_member_unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>substitution</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>variable_or_member_unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>closure_prefix_km_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>variable_template_template_prefix</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_args</span><span>)</span> <span>&amp;&amp;</span> <span>READ</span> <span>(</span><span>'M'</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>closure_prefix_B</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>closure_prefix_Y</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>closure_prefix_Xm_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>closure_prefix_S2_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>closure_prefix_B</span><span>)</span> <span>&amp;&amp;</span> <span>RULE_MANY</span> <span>(</span><span>closure_prefix_km_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>closure_prefix</span><span>,</span> <span>{</span> <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>closure_prefix_S2_prefix</span><span>));</span> <span>});</span>
</span></span></code></pre></div><p>The changes made for $ \langle \text{template-prefix} \rangle $ :</p><div><pre tabindex="0"><code data-lang="c"><span><span><span>DEFN_RULE</span> <span>(</span><span>template_prefix_Y</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_param</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>substitution</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>template_prefix_Xm_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>unqualified_name</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_param</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>decltype</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>substitution</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>template_prefix_km_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_args</span><span>)</span> <span>&amp;&amp;</span> <span>RULE</span> <span>(</span><span>template_unqualified_name</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>template_prefix_B</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_prefix_Y</span><span>));</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_prefix_Xm_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>template_prefix_S2_prefix</span><span>,</span> <span>{</span>
</span></span><span><span>    <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_prefix_B</span><span>)</span> <span>&amp;&amp;</span> <span>RULE_MANY</span> <span>(</span><span>template_prefix_km_prefix</span><span>));</span>
</span></span><span><span><span>});</span>
</span></span><span><span>
</span></span><span><span><span>DEFN_RULE</span> <span>(</span><span>template_prefix</span><span>,</span> <span>{</span> <span>MATCH</span> <span>(</span><span>RULE</span> <span>(</span><span>template_prefix_S2_prefix</span><span>));</span> <span>});</span>
</span></span></code></pre></div><p>This is my current fix. This fixed any recursion issues for now. Previously I was getting
stack overflow, because the stack frames just kept getting growing up and up. For those of you
who are not comfortable with the code, here’s the grammar form :</p><p>\[
\begin{align}
\langle \text{prefix-X} \rangle &amp; ::= &amp; \langle \text{unqualified-name} \rangle \\
&amp; \quad \mid &amp; \langle \text{template-param} \rangle \\
&amp; \quad \mid &amp; \langle \text{decltype} \rangle \\
&amp; \quad \mid &amp; \langle \text{substitution} \rangle
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{prefix-Yk-template-prefix} \rangle &amp; ::= &amp; \langle \text{template-unqualified-name} \rangle \ \langle \text{template-args} \rangle \\
&amp; \quad \mid &amp; \langle \text{template-param} \rangle \ \langle \text{template-args} \rangle \\
&amp; \quad \mid &amp; \langle \text{substitution} \rangle \ \langle \text{template-args} \rangle \\
\langle \text{prefix-A-template-prefix} \rangle &amp; ::= &amp; \langle \text{prefix-X} \rangle \\
&amp; \quad \mid &amp; \langle \text{prefix-Yk-template-prefix} \rangle \\
\langle \text{prefix-mk-template-prefix} \rangle &amp; ::= &amp; \langle \text{template-unqualified-name} \rangle \ \langle \text{template-args} \rangle \\
\langle \text{prefix-S1-template-prefix} \rangle &amp; ::= &amp; \langle \text{prefix-A-template-prefix} \rangle \ \langle \text{prefix-mk-template-prefix} \rangle ^*
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{prefix-Yk-closure-prefix} \rangle &amp; ::= &amp; \langle \text{variable-template-template-prefix} \rangle \ \langle \text{template-args} \rangle \ M \\
\langle \text{prefix-A-closure-prefix} \rangle &amp; ::= &amp; \langle \text{prefix-X} \rangle \\
&amp; \quad \mid &amp; \langle \text{prefix-Yk-closure-prefix} \rangle \\
\langle \text{prefix-mk-closure-prefix} \rangle &amp; ::= &amp; \langle \text{variable-or-member-unqualified-name} \rangle \ M \\
\langle \text{prefix-S1-closure-prefix} \rangle &amp; ::= &amp; \langle \text{prefix-A-closure-prefix} \rangle \ \langle \text{prefix-mk-closure-prefix} \rangle ^*
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{prefix} \rangle &amp; ::= &amp; \langle \text{prefix-X} \rangle \ {\langle \text{unqualified-name} \rangle ^*} \\
&amp; \mid &amp; \langle \text{prefix-S1-template-prefix} \rangle \\
&amp; \mid &amp; \langle \text{prefix-S1-closure-prefix} \rangle
\end{align}
\]</p><p>The changes made for $ \langle \text{closure-prefix} \rangle $ :</p><p>\[
\begin{align}
\langle \text{closure-prefix-Y} \rangle &amp; ::= &amp; \langle \text{variable-template-template-prefix} \rangle \ \langle \text{template-args} \rangle \ M
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{closure-prefix-Xm-prefix} \rangle &amp; ::= &amp; \langle \text{unqualified-name} \rangle \ \langle \text{variable-or-member-unqualified-name} \rangle \ M \\
&amp; \quad \mid &amp; \langle \text{template-param} \rangle \ \langle \text{variable-or-member-unqualified-name} \rangle \ M \\
&amp; \quad \mid &amp; \langle \text{decltype} \rangle \ \langle \text{variable-or-member-unqualified-name} \rangle \ M \\
&amp; \quad \mid &amp; \langle \text{substitution} \rangle \ \langle \text{variable-or-member-unqualified-name} \rangle \ M
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{closure-prefix-B} \rangle &amp; ::= &amp; \langle \text{closure-prefix-Y} \rangle \\
&amp; \quad \mid &amp; \langle \text{closure-prefix-Xm-prefix} \rangle
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{closure-prefix-km-prefix} \rangle ::= \langle \text{variable or member unqualified-name} \rangle M
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{closure-prefix-S2-prefix} \rangle &amp; ::= &amp; \langle \text{closure-prefix-B} \rangle \ \langle \text{closure-prefix-km-prefix} \rangle ^*
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{closure-prefix} \rangle &amp; ::= &amp; \langle \text{closure-prefix-S2-prefix} \rangle
\end{align}
\]</p><p>The changes made for $ \langle \text{template-prefix} \rangle $ :</p><p>\[
\begin{align}
\langle \text{template-prefix-Y} \rangle &amp; ::= &amp; \langle \text{template-unqualified-name} \rangle \\
&amp; \quad \mid &amp; \langle \text{template-param} \rangle \\
&amp; \quad \mid &amp; \langle \text{substitution} \rangle
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{template-prefix-Xm-prefix} \rangle &amp; ::= &amp; \langle \text{unqualified-name} \rangle \ \langle \text{template-unqualified-name} \rangle \\
&amp; \quad \mid &amp; \langle \text{template-param} \rangle \ \langle \text{template-unqualified-name} \rangle \\
&amp; \quad \mid &amp; \langle \text{decltype} \rangle \ \langle \text{template-unqualified-name} \rangle \\
&amp; \quad \mid &amp; \langle \text{substitution} \rangle \ \langle \text{template-unqualified-name} \rangle
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{template-prefix-B} \rangle &amp; ::= &amp; \langle \text{template-prefix-Y} \rangle \\
&amp; \quad \mid &amp; \langle \text{template-prefix-Xm-prefix} \rangle
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{template-prefix-km-prefix} \rangle ::= \langle \text{template-args} \rangle \langle \text{template-unqualified-name} \rangle
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{template-prefix-S2-prefix} \rangle &amp; ::= &amp; \langle \text{template-prefix-B} \rangle \ \langle \text{template-prefix-km-prefix} \rangle ^*
\end{align}
\]</p><p>\[
\begin{align}
\langle \text{template-prefix} \rangle &amp; ::= &amp; \langle \text{template-prefix-S2-prefix} \rangle
\end{align}
\]</p><p>Grammars are very hard to get right in the first try. You are basically developing your
own language. It takes experience, which basically demands you beforehand about what you
want and what you don’t want. The other way is doing it iteratively, which is another
name for trial-and-error (and as I said, hard to get right in first try). The final grammar
we see here can be further simplified using some normalization algorithms to generate a normalized
form of this grammar. These normalized forms generally do transformations to original grammar
to remove redundancy, and help the actual parsing algorithm make correct decisions faster.</p><p>Did you like the post? Drop in a comment! If you find any error in this post, I’m only a human,
and I’ll accept my mistakes, and make any changes if required.</p><h2 id="further-reading">Further Reading</h2><ul><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2000/04/naacl2k-proc-rev.pdf">[1]</a> - Removing Left Recursion from Context-Free Grammars</li></ul></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CDC orders mass retraction of research across all science and medicine journals (815 pts)]]></title>
            <link>https://insidemedicine.substack.com/p/breaking-news-cdc-orders-mass-retraction</link>
            <guid>42905937</guid>
            <pubDate>Sun, 02 Feb 2025 04:52:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://insidemedicine.substack.com/p/breaking-news-cdc-orders-mass-retraction">https://insidemedicine.substack.com/p/breaking-news-cdc-orders-mass-retraction</a>, See on <a href="https://news.ycombinator.com/item?id=42905937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><em>I believe we are breaking news some news here. To help sustain independent journalism and analysis, please support Inside Medicine. Thanks for reading…</em></p><p><span>The CDC has instructed its scientists to retract or pause the publication of any research manuscript being considered by </span><em>any medical or scientific journal</em><span>, not merely its own internal periodicals, </span><em>Inside Medicine </em><span>has learned. The move aims to ensure that no “forbidden terms” appear in the work. The policy includes manuscripts that are in the revision stages at journal (but not officially accepted) and those already accepted for publication but not yet live. </span></p><p>In the order, CDC researchers were instructed to remove references to or mentions of a list of forbidden terms: “Gender, transgender, pregnant person, pregnant people, LGBT, transsexual, non-binary, nonbinary, assigned male at birth, assigned female at birth, biologically male, biologically female,” according to an email sent to CDC employees (see below).”</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png" width="215" height="296.3633241758242" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:2007,&quot;width&quot;:1456,&quot;resizeWidth&quot;:215,&quot;bytes&quot;:6395981,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F286d049f-4bbf-4a2a-a779-7d8e0af7b1c6_1796x2476.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><span>A screenshot of a CDC email shared with </span><em>Inside Medicine</em><span> of a list of terms that must be removed from any CDC-authored manuscript being seriously considered or “in press” (but not yet online or in print) at any medical or scientific journal.</span></figcaption></figure></div><p><span>The policy goes beyond the previously </span><a href="https://insidemedicine.substack.com/p/uganda-confirms-a-new-ebola-outbreak" rel="">reported</a><span> pause of the CDC’s own publications, including </span><em>Morbidity and Mortality Weekly Report </em><span>(MMWR), which has seen two issues go unreleased since January 16, marking the first publication gap of any kind in approximately 60 years. </span><em>Emerging infectious Diseases</em><span> and </span><em>Preventing Chronic Disease</em><span>, the CDC’s other major publications, also remain under lock and key, but have not yet been affected because they are monthly releases and both were released as scheduled in January, prior to President Trump’s inauguration. The policy also goes beyond the general communications gag order that already prevents any CDC scientist from submitting any new scientific findings to the public. </span></p><p><span>The edict applies to both any previously submitted manuscript under consideration and those accepted but not yet published. For example, if CDC scientists previously submitted a manuscript to </span><em>The New England Journal of Medicine, The Journal of the American Medical Association, </em><span>or any other publication, the article must be stopped and reviewed. (These are hypothetical, but are examples of major journals where CDC officials often publish.)</span></p><p><span>How many manuscripts are affected is unclear, but it could be many. Most manuscripts include simple demographic information about the populations or patients studied, which typically includes gender (and which is frequently used interchangeably with sex). That means just about any major study would fall under the censorship regime of the new policy, including studies on Covid-19, cancer, heart disease, </span><em>or anything else,</em><span> let alone anything that the administration considers to be “woke ideology.” </span></p><p>Meanwhile, chaos and fear are already guiding decisions. While the policy is only meant to apply to work that might be seen as conflicting with President Trump’s executive orders, CDC experts don’t know how to interpret that. Do papers that describe disparities in health outcomes fall into “woke ideology” or not? Nobody knows, and everyone is scared that they’ll be fired. This is leading to what Germans call “vorauseilender Gehorsam,” or “preemptive obedience,” as one non-CDC scientist commented. </p><p>“I’ve got colleagues pulling papers over Table 1 concerns,” an official told me. (Table 1 refers to basic demographic information about the study populations included in research papers, rather than actual results.) Indeed, many studies include demographic information about sexual orientation. For example, a study describing mpox outcomes would likely include basic statistics in tables summarizing the percentage of patients who were vaccinated and were lesbian, gay, transgender, or otherwise. This information can be highly impactful during an outbreak, as it helps clinicians develop policies on who to vaccinate (given limited doses, as is the case with mpox), and even to whom scarce and limited supplies of tests and treatments should be offered to maximize benefits. </p><p>It is not necessarily the case that researchers who have submitted articles but who have not yet received an official decision from a journal need to actively recall them, however. But if a journal sends an article back for revisions, the authors would at that point have to cleanse the document of any “problematic language.” Of course, at that point, the gag order already in place would halt any resubmission. </p><p data-attrs="{&quot;url&quot;:&quot;https://insidemedicine.substack.com/p/breaking-news-cdc-orders-mass-retraction?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://insidemedicine.substack.com/p/breaking-news-cdc-orders-mass-retraction?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>What can and cannot go forward appears to require approval by a Trump political appointee, an explicit requirement for any public health communications under the Trump Administration’s gag order. That’s slowing many things down. At present, there is only one political appointee in the entire CDC, acting Director Susan Monarez (plus her personal assistant, who is not a scientist). It’s unclear if some decisions may be devolved to lower officials. For example, if a paper is pulled because it simply mentions gender, it is unknown if anyone other than Monarez possesses the authority to approve its resubmission. </p><p>“How can one person vet all of this?” another official asked, “especially one who, [like Monarez], came from an agency of, what, 130 people?”</p><p>And yet, that seems to be the theme of the new administration: a few privileged individuals have been handed enormous authority, creating a backlog of decisions that may end up being fairly arbitrarily determined.</p><p data-attrs="{&quot;url&quot;:&quot;https://insidemedicine.substack.com/p/breaking-news-cdc-orders-mass-retraction?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://insidemedicine.substack.com/p/breaking-news-cdc-orders-mass-retraction?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
    </channel>
</rss>