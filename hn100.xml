<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 28 Jan 2025 23:30:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Questions censored by DeepSeek (243 pts)]]></title>
            <link>https://www.promptfoo.dev/blog/deepseek-censorship/</link>
            <guid>42858552</guid>
            <pubDate>Tue, 28 Jan 2025 21:54:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.promptfoo.dev/blog/deepseek-censorship/">https://www.promptfoo.dev/blog/deepseek-censorship/</a>, See on <a href="https://news.ycombinator.com/item?id=42858552">Hacker News</a></p>
Couldn't get https://www.promptfoo.dev/blog/deepseek-censorship/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Parkinsons patient "feels cured" with new adaptive deep brain stimulation device (119 pts)]]></title>
            <link>https://www.bbc.com/news/articles/ckgn49r069wo</link>
            <guid>42857293</guid>
            <pubDate>Tue, 28 Jan 2025 20:07:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/ckgn49r069wo">https://www.bbc.com/news/articles/ckgn49r069wo</a>, See on <a href="https://news.ycombinator.com/item?id=42857293">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div data-testid="byline-new" data-component="byline-block"><p><span>Sharon Barbour</span></p><p><span>BBC North East &amp; Cumbria health correspondent<!-- --></span></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/0c97/live/1d017000-dccb-11ef-8633-c742f1bc9aea.jpg.webp" alt="BBC Kevin Hill is sitting on his sofa and has opened his blue shirt to show a lump in his chest. This is where a small computer has been implanted. It is connected to wires that go deep into his brain, to control his Parkinson's disease.    "><span>BBC</span></p></div><p data-component="caption-block"><figcaption>Kevin Hill said he is able to go for days now without thinking about his Parkinson's<!-- --></figcaption></p></figure><div data-component="text-block"><p>A man fitted with a pioneering, computer-controlled brain implant to tackle his Parkinson's disease says it works so well he is sometimes able to forget he has the condition.<!-- --></p><p>A small computer inserted into Kevin Hill's chest wall 12 months ago is connected to wires running into the brain which can send electrical signals and an update means it can now read his brain activity.<!-- --></p><p>The 65-year-old from Sunderland said it has been so successful he feels like he has "been cured".<!-- --></p><p>Surgeons in Newcastle hope an adapted version of the deep brain stimulation system will have a "huge impact" on the quality of life of patients with the disease.<!-- --></p></div><p>Mr Hill said: "I forget about Parkinson's for days and days and days."<!-- --></p><p data-component="subheadline-block"><h2>Kitchen ban<!-- --></h2></p><p><i id="warning---contains-a-distressing-image"><b id="warning---contains-a-distressing-image">Warning - contains a distressing image<!-- --></b></i></p><div data-component="text-block"><p>He began getting symptoms, including trembling in his thumb, in his 40s and started suffering nightmares and insomnia.<!-- --></p><p>He was banned by his wife from going into the kitchen because his hand shook so much he spilled or dropped hot drinks and even cut the end of his finger off.<!-- --></p><p>In 2017 he visited his GP and was diagnosed with Parkinson's.<!-- --></p><p>He was told there were medicines but no cure, but there was a new treatment – deep brain stimulation (DBS) – and tests proved he was suitable for the surgery.<!-- --></p><p>It involved an implant that runs deep into the brain to an area the size of a grain of rice. <!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/fd5a/live/8e66be50-dcc9-11ef-8633-c742f1bc9aea.jpg.webp" alt="NEWCASTLE HOSPITALS Kevin is with a nurse at hospital as the new system is re-programmed and switched on. They are both looking at a computer screen which is wired to his chest. "><span>NEWCASTLE HOSPITALS</span></p></div><p data-component="caption-block"><figcaption>Mr Hill originally had to go to hospital to have the system reprogrammed, but with updates it can now do that automatically<!-- --></figcaption></p></figure><div data-component="text-block"><p>The computer in his chest is connected to two thin wires that thread up the back of his neck. <!-- --></p><p>It carries the electrical messages that can manage his Parkinson's symptoms.<!-- --></p><p>Mr Hill described the computer as the size and shape of "a Jaffa Cake".<!-- --></p><p>When it was switched on after surgery he said the impact was dramatic.<!-- --></p><p>After years of sleepless nights, and being unable to manage the uncontrollable shaking of his arm and leg, his tremors "stopped instantly".<!-- --></p><p>Mr Hill said he stared at his still hand and "couldn't believe it". His wife burst into tears.<!-- --></p><p>The life he once knew came back, meaning he was able to go to the pub and see his friends again.<!-- --></p><p>He bought a bike and was even allowed back into the kitchen.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/a235/live/2b93b270-dc32-11ef-acf7-0d8b5f8aaf71.jpg.webp" alt="KEVIN HILL Kevin's head shaved after surgery. You can see the stitches in his skull where he had the operation to implant the wires into his brain.   "><span>KEVIN HILL</span></p></div><p data-component="caption-block"><figcaption>A brain implant links to the computer in Mr Hill's chest<!-- --></figcaption></p></figure><div data-component="text-block"><p>For the last year he has had to go to hospital regularly to have his system re-programmed to better control his symptoms.<!-- --></p><p>Now, a new updated version called "adaptive deep brain stimulation" has been designed to re-programme the system in real time.<!-- --></p><p>It can also read a patient's brain signals which doctors say should mean even better control of symptoms.<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/0f16/live/a7a5d160-d983-11ef-ba6a-b744abd00a0a.png.webp" alt="NEWCASTLE HOSPITALS NHS TRUST A mid-shot of Mr Akbar Hussain, a neurosurgeon at Newcastle Hospitals. He is pictured wearing his blue surgical scrubs.
    "><span>NEWCASTLE HOSPITALS NHS TRUST</span></p></div><p data-component="caption-block"><figcaption>Neurosurgeon Akbar Hussain said recent changes to the device would be very significant to patients' quality of life<!-- --></figcaption></p></figure><div data-component="text-block"><p>Akbar Hussain, a neurosurgeon at Newcastle Hospitals, is one of the first doctors in the world to offer the new adaptive Brainsense treatment, developed by Medtronic.<!-- --></p><p>He said: "The amazing thing about the adaptive version is that the electrical impulses provided to the brain by the device are controlled and adjusted automatically, according to individual patient's recordings from the device in their chest.<!-- --></p><p>"The biological signals generated within the person themselves are enough to alter the treatment given by the implant. <!-- --></p><p>"These changes could be taking place by the minute or hour, meaning the treatment is truly responsive to the exact needs of each individual.<!-- --></p><p>"It's exciting. Hopefully this will have a huge impact and be very significant for the patients' quality of life."<!-- --></p></div><figure><div data-component="image-block"><p><img src="https://www.bbc.com/bbcx/grey-placeholder.png"><img sizes="(min-width: 1280px) 50vw, (min-width: 1008px) 66vw, 96vw" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 240w,https://ichef.bbci.co.uk/news/320/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 320w,https://ichef.bbci.co.uk/news/480/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 480w,https://ichef.bbci.co.uk/news/640/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 640w,https://ichef.bbci.co.uk/news/800/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 800w,https://ichef.bbci.co.uk/news/1024/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 1024w,https://ichef.bbci.co.uk/news/1536/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp 1536w" src="https://ichef.bbci.co.uk/news/480/cpsprodpb/bf88/live/bd001e50-dcce-11ef-bc01-8f2c83dad217.jpg.webp" alt="Kevin Hill wearing a fluourescent green jacket and holding his bike and a helmet and smiling."></p></div><p data-component="caption-block"><figcaption>Kevin Hill says his old life has returned since having the surgery<!-- --></figcaption></p></figure><div data-component="text-block"><p>Dr Becky Jones, from the charity Parkinson's UK, said: "Current DBS can be life changing and has the promise to be even more effective if it could be responsive to the needs of the individual. Brainsense represents a major step towards this.<!-- --></p><p>"While evidence is still being gathered to assess the benefits of adaptive DBS versus the standard type, it's great to see movement towards this becoming a new, more effective treatment for people with Parkinson's."<!-- --></p><p>About 153,000 people in the UK are living with Parkinson's disease, a progressive neurological disorder affecting the brain and nervous system. <!-- --></p><p>The number is expected to increase due to population growth and ageing. <!-- --></p></div><p><i id="follow-bbc-sunderland-on">Follow BBC Sunderland on <!-- --></i><a target="_blank" href="https://x.com/BBCNEandCumbria"><i id="x">X<!-- --></i></a><i id=",">, <!-- --></i><a target="_blank" href="https://www.facebook.com/BBCSunderland"><i id="facebook">Facebook<!-- --></i></a><i id=",">, <!-- --></i><a target="_blank" href="https://bbc.in/3yyMYUI"><i id="nextdoor">Nextdoor<!-- --></i></a><i id="and"> and <!-- --></i><a target="_blank" href="https://www.instagram.com/bbcneandcumbria/"><i id="instagram">Instagram<!-- --></i></a><i id=".-send-your-story-ideas-to-northeastandcumbria@bbc.co.uk.">. Send your story ideas to northeastandcumbria@bbc.co.uk.<!-- --></i></p><div data-component="links-block"><p><span data-testid="links-title">More stories on this topic</span></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New speculative attacks on Apple CPUs (519 pts)]]></title>
            <link>https://predictors.fail/</link>
            <guid>42856023</guid>
            <pubDate>Tue, 28 Jan 2025 18:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://predictors.fail/">https://predictors.fail/</a>, See on <a href="https://news.ycombinator.com/item?id=42856023">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<section id="demos">
					<h4>Demos</h4>

					<h5>Leaking Proton Mail's Inbox Data</h5>
					<p>We train the M3 CPU's LVP via sandboxed JavaScript code running inside WebKit (Safari's browsing engine). When the mouse cursor is over our demo webpage, our proof-of-concept opens Proton Mail's inbox in a new window, but uses the same process to render the inbox. This brings the inbox content into the address space, making it accessible with a sandbox escape. Finally, we use the LVP to craft an arbitrary read primitive to anywhere in this address space, recovering the sender and subject lines shown on the inbox page.</p>
					

					<h5>Reading The Great Gatsby Using Load Address Prediction</h5>
					<p>We demonstrate an LAP proof-of-concept on the Apple M2 CPU that recovers a secret string. The string holds the first paragraph of The Great Gatsby, but is never architecturally accessed. At the LAP's incorrectly guessed memory address, we place a pointer to the characters of the string. Subsequently, we train and activate the LAP.</p>
					

					<h5>Reading Harry Potter Using Load Value Prediction</h5>
					<p>On the Apple M3 CPU, we demonstrate an LVP proof-of-concept that recovers the first paragraph of Harry Potter and the Sorcerer's Stone, which is also never architecturally read by the CPU core. We cause the LVP to predict and access an incorrect array index. There, we place the pointer to the string's characters, which the CPU then dereferences.
					</p>
					
				</section>

				<section id="people">
					<h4>The People Behind
						<span>SLAP and FLOP</span>
					</h4>
					<div>
								<ul>
									<li><a href="https://jas0n.kim/">Jason Kim </a><span><a href="https://www.gatech.edu/">Georgia Institute of
												Technology</a></span></li>
									<li><a href="https://sites.cc.gatech.edu/grads/j/jchuang9/">Jalen Chuang</a> <span><a href="https://www.gatech.edu/">Georgia
												Institute of
												Technology</a></span></li>
									<li><a href="https://faculty.cc.gatech.edu/~genkin/">Daniel Genkin</a> <span><a href="https://www.gatech.edu/">Georgia Institute of
												Technology</a></span>
									</li>
									<li><a href="https://yuval.yarom.org/">Yuval Yarom</a> <span><a href="https://www.ruhr-uni-bochum.de/">Ruhr University
												Bochum</a></span></li>
								</ul>
							</div>
				</section>

				<section id="qa">
					<h4>Frequently Asked <span>Questions</span></h4>

					<h5>SLAP and FLOP Basics</h5>
					<div id="accordion" aria-labelledby="basics-question-1">
									<p>The affected Apple devices are the following:</p>
									<ul>
										<li>All Mac laptops from 2022-present (MacBook Air, MacBook Pro)</li>
										<li>All Mac desktops from 2023-present (Mac Mini, iMac, Mac Studio, Mac Pro) </li>
										<li>All iPad Pro, Air, and Mini models from September 2021-present (Pro 6th and 7th gen., Air 6th gen., Mini 6th gen.)</li>
										<li>All iPhones from September 2021-present (All 13, 14, 15, and 16 models, SE 3rd gen.)</li>
									</ul>
								</div>
					<div id="accordion" aria-labelledby="basics-question-2">
								<p>There are hardware and software measures to ensure that two open webpages are isolated from each other, 
									   preventing one of them form (maliciously) reading the other's contents. SLAP and FLOP break these protections, 
									   allowing attacker pages to read sensitive login-protected data from target webpages. In our work, we show
										that this data ranges from location history to credit card information.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-3">
								<p> While FLOP has an actionable mitigation, implementing it requires patches from software vendors and cannot be done by users. 
									Apple has communicated to us that they plan to address these issues in an
										upcoming security update, hence it is important to enable automatic updates and
										ensure that your devices are running the latest operating system and applications.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-4">
								<p>We have not yet observed load address prediction or load value prediction 
									   in other processor vendors' products, such as Intel, AMD, Qualcomm, or Ampere.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-5">
								<p> We do not know, as we have not tested other browsers such as Firefox. </p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-6">
								<p>Since SLAP and FLOP are microarchitecture-based attacks, they do not leave any traces in
										the system's log files. While cached copies of previously visited websites may
										be present in the web browser, it is difficult to automatically detect malicious
										code patterns that exploit hardware vulnerabilities.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-7">
								<p>So far, we do not have any evidence that either SLAP or FLOP has been used in the
										wild.</p>
							</div>
					<div id="accordion" aria-labelledby="basics-question-8">
								<p>We disclosed SLAP to Apple on May 24, 2024, and FLOP on September 3, 2024.</p>
							</div>

					<h5>Technical Questions</h5>
					<div id="accordion" aria-labelledby="tech-question-1">
									<p>
										Most computer bugs arise from mistakes in programming, such as missing bounds
										checks 	or use-after-frees. However, a side-channel attack exploits the implementation
										of a computer's hardware to attack it, even if the software it runs is a secure algorithm.
										Systems can leak sensitive data through sound, electromagnetic radiation, or
										thermal throttling, just for a few examples.
									</p>
									<p>
										Many side channels, including ones we use for SLAP and FLOP, comes from the
										CPU's microarchitecture. Whenever an attacker and target run on the physical CPU, they share the CPU's
										internal resources such as cores, caches, and internal buffers.
										Sharing resources leads to contention, and contention can be measured indirectly
										through several variables like timing or power consumption.
										These measurements leave fingerprints on the target's behavior on the CPU.
										Accordingly, an attacker can abuse this to make inferences about the target's
										secrets even if they are isolated at the process level or the hypervisor level.
									</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-2">
									<p>
										Virtually all modern CPUs use a performance optimization where they predict the
										control flow the CPU should take (such as branches and returns), should the
										outcome not be readily available. Once a prediction is made, the CPU will execute instructions along
										the prediction, a process called speculative execution. If the CPU realizes it had
										mispredicted, it must revert all changes in the state it performed after the
										prediction. Nearly all desktop and mobile CPUs exhibit this behavior, regardless of
										manufacturer (such as Apple, AMD, or Intel).
									</p>
									<p>
										<a href="https://spectreattack.com/">Spectre</a> is a hardware vulnerability in
										virtually all modern CPUs that occurs when speculative execution backfires.
										While the CPU should ideally revert all changes in state, speculative execution leaves
										traces in the CPU's microarchitectural state and especially the cache. A Spectre
										attack coerces the CPU into speculatively executing the wrong flow of
										instructions. If this wrong flow has instructions depending on sensitive data, their value can
										be inferred through a side channel even after the CPU realizes the mistake and
										reverts its changes. An adversary can abuse this behavior to read data that they cannot
										normally access through program semantics. Because speculative execution is an
										important part of CPU performance that is infeasible to simply remove as a
										countermeasure, Spectre continues to be dangerous to software even years after
										its discovery.
									</p>
									<p>
										In SLAP and FLOP, we demonstrate that recent Apple CPUs go beyond this, not only
										predicting the control flow the CPU should take, but also the data flow the CPU
										should operate on if data are not readily available from the memory subsystem.
										Unlike Spectre, mispredictions on data flow do not directly result in the CPU
										speculatively executing the wrong instructions. Instead, they result in the CPU
										executing arbitrary instructions on the wrong data. However, we show this can be combined with
										indirection techniques to execute wrong instructions.
									</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-3">
									<p>To orchestrate SLAP, we begin by reverse engineering Apple's implementation of Load Address Prediction (LAP). 
									We discover that if we train the LAP on striding memory addresses, the LAP will access address the next sequence 
									in the striding pattern and compute using the data in that address, even if the program never actually accesses it.
									Here, we note that this is different from hardware prefetching. While prefetchers may bring the data inside the predicted addresses, 
									they do not speculatively execute downstream instructions based on the prediction. </p>
									<p>Next, we find an attack surface in Safari. Previously, <a href="https://ileakage.com/">iLeakage</a> demonstrated a corner case 
									in Safari's isolation scheme where an adversary's webpage can coerce an arbitrary target webpage to be handled by the same process. 
									We find that when this occurs, the two webpages also share internal memory allocation regions for data, such as strings. 
									In turn, this allows the adversary to jump the LAP to the target webpage's string and trick the CPU into operating on it, eventually leaking the string's content over a covert channel.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-4">
									<p>Similarly to SLAP, we reverse engineer the Load Value Prediction (LVP) mechanism in Apple CPUs. 
									We found that if the LVP sees the same data value being repeatedly returned from the memory subsystem for the same load instruction, 
									the LVP will attempt to guess the load's outcome the next time that load instruction executes, even if the memory accessed by the load now contains a completely different value! 
									Therefore, using the LVP, we can trick the CPU into computing on incorrect data values.</p>
									<p>We first demonstrate the dangers stemming from LVP in Safari, whose JavaScript engine first vets the type information of JavaScript data structures before determining the 
									appropriate computations to run on them. If we train the LVP on the load instruction that retrieves this type information, we can cause code that is only supposed to run for 
									one data structure on another data structure, causing speculative type confusion, and obtaining a read primitive to arbitrary 64-bit addresses. </p>
									
									<p> Next, we move to Chrome, where internal table data structures for calling WebAssembly functions also vet the signature of each function before calling them with arguments. 
									Here, we show that the LVP allows us to run a function with the wrong arguments (e.g., pointer instead of integer), again resulting in a type confusion based primitive for reading arbitrary memory addresses.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-5">
									<p>SLAP exploits a phenomenon in Safari where strings that belong to different webpages can be allocated within a close distance to each other, and thus discloses cross-origin strings that are allocated in proximity to the adversary's own strings. On the other hand, FLOP is a speculative type confusion attack that causes the CPU to bypass integrity checks on data structures, resulting in memory read primitives from arbitrary addresses in Safari and Chrome.</p>
									<p>
									Furthermore, the underlying CPU microarchitecture that SLAP and FLOP exploit are also different. SLAP uses the Load Address Predictor (LAP), while FLOP uses the Load Value Predictor (LVP).
									As suggested by their names, the LAP predicts addresses while the LVP predicts values. 
									Consider the following statement: "The CPU accesses memory at address 0xdeadbeef, 
									which contains the value 0x1234." The next time the CPU performs a memory access, the LAP predicts the next address, i.e., what 0xdeadbeef will change to. Meanwhile, the LVP predicts the next value returned from memory, that is, what 0x1234 will change to. 
									Going deeper in detail, we observe their internal structures are also different. For instance, the LAP requires a longer training sequence than the LVP to activate reliably, but only the LAP can observe strides and generate predictions accordingly.</p>
								</div>
					<div id="accordion" aria-labelledby="tech-question-6">
								<p>
										JavaScript and WebAssembly are two programming languages that make up the
										backbone of interactive webpages, such as online games and video streaming
										services. JavaScript can update the content of the website directly, while
										WebAssembly is used for high-performance web applications. Ultimately,
										WebAssembly interfaces with JavaScript to deliver dynamic content to users.
										Since both are sandboxed in a browser environment, side-channel attacks are
										notably more difficult to implement in these languages. However, the impact is
										drastically greater, as browsers execute both types of code automatically and do
										not require the user to download the malicious program.
									</p>
							</div>
					<div id="accordion" aria-labelledby="tech-question-7">
									<p>For leaking secrets, both SLAP and FLOP are confined to the address space they are trained in. 
									As pointed out by <a href="https://ileakage.com/">iLeakage</a>, Safari lacks Site Isolation, a measure used to enforce that two different webpages not from the same domain can never be handled by the same process.
									Thus, in Safari it is possible for an adversary's webpage to be handled by the same process (and thus address space) with an arbitrary webpage, increasing the attack surface including LAP- and LVP-based exploits.
									 </p>
									
									<p>On the other hand, although Chrome is equipped with Site Isolation, we demonstrate that it is not a perfect mitigation. We show the real-world existence of corner cases, where two subdomains of the same site can be merged into one process, again leading to LAP- and LVP-based attacks.</p>
								</div>

					<h5>Miscellaneous</h5>
					<div id="accordion" aria-labelledby="misc-question-1">
									<p>Yes, with rights waived via <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0</a>. You can
										right-click each logo, which should list an option to save the image as a file.
									</p></div>
					
					

				</section>
				<section id="acknowledgments">
					<h4>Acknowledgments</h4>
					<div><p>
							This research was supported by
							the Air Force Office of Scientific Research (AFOSR) under award number FA9550-24-1-0079;
							the Alfred P Sloan Research Fellowship;
							an ARC Discovery Project number DP210102670;
							the Defense Advanced Research Projects Agency (DARPA) under contract numbers
							W912CG-23-C-0022,
							the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's
							Excellence Strategy - EXC 2092 CASA - 390781972;
							and gifts from Qualcomm, Cisco (SLAP), and Zama (FLOP).
							</p><p>
							The views and conclusions contained in this document are those of the authors and should not
							be interpreted as representing the official policies, either expressed or implied, of the
							U.S. Government.
						</p></div>
				</section>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Berkeley Researchers Replicate DeepSeek R1's Core Tech for Just $30: A Small Mod (158 pts)]]></title>
            <link>https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek</link>
            <guid>42855283</guid>
            <pubDate>Tue, 28 Jan 2025 17:36:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek">https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek</a>, See on <a href="https://news.ycombinator.com/item?id=42855283">Hacker News</a></p>
Couldn't get https://xyzlabs.substack.com/p/berkeley-researchers-replicate-deepseek: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Using uv as your shebang line (217 pts)]]></title>
            <link>https://akrabat.com/using-uv-as-your-shebang-line/</link>
            <guid>42855258</guid>
            <pubDate>Tue, 28 Jan 2025 17:35:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://akrabat.com/using-uv-as-your-shebang-line/">https://akrabat.com/using-uv-as-your-shebang-line/</a>, See on <a href="https://news.ycombinator.com/item?id=42855258">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-7254">
            
                <div>

<p>I create a fair few scripts in my <tt>~/bin/</tt> directory to automate tasks. Since discovering <a href="https://akrabat.com/defining-python-dependencies-at-the-top-of-the-file/"><tt>uv</tt> and inline script metadata</a>, I’ve started using Python far more for these.</p>
<p>As <tt>~/bin</tt> is on my path, I want to run the script by calling it directly on the command line. To do this, I use this shebang:</p>
<pre>#!/usr/bin/env -S uv run --script
</pre>
<p>The command line will now run <tt>uv run --script</tt> and pass the file as the argument. <tt>uv</tt> ignores the shebang and then runs the rest of the file as a normal Python file. </p>
<p>Once I’ve ensured that that script has executable permissions via <tt>chmod a+x {filname}</tt>, I’m now good to go with simple command line scripts written in Python that automatically handle their dependencies!</p>


</div>
                        <p>This article was posted on
                                <time datetime="2025-01-28 11:00:00" title="2025-01-28 11:00:00">28 January 2025</time>
                                in <span><a href="https://akrabat.com/category/python/" rel="category tag">Python</a></span>
                        </p>
                                              </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How has DeepSeek improved the Transformer architecture? (206 pts)]]></title>
            <link>https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture</link>
            <guid>42855170</guid>
            <pubDate>Tue, 28 Jan 2025 17:29:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture">https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture</a>, See on <a href="https://news.ycombinator.com/item?id=42855170">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>DeepSeek has recently released DeepSeek v3, which is currently state-of-the-art in benchmark performance among open-weight models, alongside <a href="https://arxiv.org/abs/2412.19437">a technical report</a> describing in some detail the training of the model. Impressively, they’ve achieved this SOTA performance by only using 2.8 million H800 hours of training hardware time—equivalent to about 4e24 FLOP if we assume 40% MFU. This is about ten times less training compute than the similarly performing Llama 3.1 405B.</p>

<p>In this issue, I’ll cover some of the important architectural improvements that DeepSeek highlight in their report and why we should expect them to result in better performance compared to a vanilla Transformer. The full technical report contains plenty of non-architectural details as well, and I strongly recommend reading it if you want to get a better idea of the engineering problems that have to be solved when orchestrating a moderate-sized training run.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-1.png">
<figcaption>
    <p>Figure 1: The DeepSeek v3 architecture with its two most important improvements: DeepSeekMoE and multi-head latent attention (MLA). Multi-token prediction is not shown. From the DeepSeek v3 technical report.</p>
  </figcaption>
</figure>



<h2 id="multi-head-latent-attention-mla">Multi-head latent attention (MLA)</h2>

<p>Multi-head latent attention (abbreviated as MLA) is the most important architectural innovation in DeepSeek’s models for long-context inference. This technique was first introduced in DeepSeek v2 and is a superior way to reduce the size of the KV cache compared to traditional methods such as grouped-query and multi-query attention.</p>

<p>I’ll start with a brief explanation of what the KV cache is all about. If you’re familiar with this, you can skip directly to the next subsection.</p>

<h3 id="what-is-the-kv-cache-and-why-does-it-matter">What is the KV cache and why does it matter?</h3>

<p>When a Transformer is used to generate tokens sequentially during inference, it needs to see the context of all of the past tokens when deciding which token to output next. The naive way to do this is to simply do a forward pass including all past tokens every time we want to generate a new token, but this is inefficient because those past tokens have already been processed before. We would just be recomputing results we’ve already obtained previously and discarded.</p>

<p>To avoid this recomputation, it’s efficient to cache the relevant internal state of the Transformer for all past tokens and then retrieve the results from this cache when we need them for future tokens. Because the only way past tokens have an influence on future tokens is through their key and value vectors in the attention mechanism, it suffices to cache these vectors. This is where the name key-value cache, or KV cache for short, comes from.</p>

<p>This works well when context lengths are short, but can start to become expensive when they become long. This is because cache reads are not free: we need to save all those vectors in GPU high-bandwidth memory (HBM) and then load them into the tensor cores when we need to involve them in a computation. If each token needs to know all of its past context, this means for each token we generate we must read the entire past KV cache from HBM.</p>

<p>In a vanilla Transformer using a standard multi-head attention mechanism, the number of KV cache parameters per past token can be expressed as:</p>

<p>2 * attention head dimension * number of attention heads * number of Transformer blocks</p>

<p>For instance, GPT-3 had 96 attention heads with 128 dimensions each and 96 blocks, so for each token we’d need a KV cache of 2.36M parameters, or 4.7 MB at a precision of 2 bytes per KV cache parameter.</p>

<p>GPT-3 didn’t support long context windows, but if for the moment we assume it did, then each additional token generated at a 100K context length would require 470 GB of memory reads, or around 140 ms of H100 time given the H100’s HBM bandwidth of 3.3 TB/s. The price per million tokens generated at $2 per hour per H100 would then be $80, around 5 times more expensive than Claude 3.5 Sonnet’s price to the customer (which is likely significantly above its cost to Anthropic itself). This naive cost can be brought down e.g. by speculative sampling, but it gives a decent ballpark estimate.</p>

<p>This rough calculation shows why it’s crucial to find ways to reduce the size of the KV cache when we’re working with context lengths of 100K or above. The most popular way in open-source models so far has been grouped-query attention. In this architectural setting, we assign multiple query heads to each pair of key and value heads, effectively grouping the query heads together - hence the name of the method. This cuts down the size of the KV cache by a factor equal to the group size we’ve chosen. In models such as Llama 3.3 70B and Mistral Large 2, grouped-query attention reduces the KV cache size by around an order of magnitude.</p>

<h3 id="beating-grouped-query-attention">Beating grouped-query attention</h3>

<p>The fundamental problem with methods such as grouped-query attention or KV cache quantization is that they involve compromising on model quality in order to reduce the size of the KV cache. Instead of this, DeepSeek has found a way to reduce the KV cache size <em>without</em> compromising on quality, at least in their internal experiments.</p>

<p>They accomplish this by turning the computation of key and value vectors from the residual stream into a two-step process. In a vanilla Transformer, key and value vectors are computed by directly multiplying the residual stream vector by a matrix of the shape</p>

<p>(number of heads · head dimension) x (model dimension)</p>

<p>DeepSeek’s method essentially forces this matrix to be low rank: they pick a latent dimension and express it as the product of two matrices, one with dimensions latent times model and another with dimensions (number of heads · head dimension) times latent. Then, during inference, we only cache the latent vectors and not the full keys and values. We can then shrink the size of the KV cache by making the latent dimension smaller.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-2.png">
<figcaption>
    <p>Figure 2: An illustration of multi-head latent attention from the DeepSeek v2 technical report.</p>
  </figcaption>
</figure>

<p>Naively, this shouldn’t fix our problem, because we would have to recompute the actual keys and values every time we need to generate a new token. After all, we need the full vectors for attention to work, not their latents. Multi-head latent attention is based on the clever observation that this is actually not true, because we can merge the matrix multiplications that would compute the upscaled key and value vectors from their latents with the query and post-attention projections, respectively.</p>

<p>The reason low-rank compression is so effective is because there’s plenty of information overlap between what different attention heads need to know about. If we used low-rank compression on the key and value vectors of individual heads instead of all keys and values of all heads stacked together, the method would simply be equivalent to using a smaller head dimension to begin with and we would get no gain. Exploiting the fact that different heads need access to the same information is essential for the mechanism of multi-head latent attention.</p>

<p>Methods such as grouped-query attention exploit the possibility of the same overlap, but they do so ineffectively by forcing attention heads that are grouped together to all respond similarly to queries. In other words, information sharing becomes coupled to having identical behavior in some restricted sense, a clearly undesirable property. Low-rank compression, on the other hand, allows the same <em>information</em> to be used in <em>very different ways</em> by different heads. In theory, this could even have beneficial regularizing effects on training, and DeepSeek reports finding such effects in their technical reports.</p>

<p>I see this as one of those innovations that look obvious in retrospect but that require a good understanding of what attention heads are actually doing to come up with. Once you see the approach, it’s immediately obvious that it cannot be any worse than grouped-query attention and it’s also likely to be significantly better. However, coming up with the idea of trying this is another matter.</p>

<h2 id="mixture-of-experts-innovations">Mixture-of-experts innovations</h2>

<p>One of the most popular improvements to the vanilla Transformer was the introduction of mixture-of-experts (MoE) models. These models divide the feedforward blocks of a Transformer into multiple distinct experts and add a routing mechanism which sends each token to a small number of these experts in a context-dependent manner. This means the model can have more parameters than it activates for each specific token, in a sense decoupling how much the model knows from the arithmetic cost of processing individual tokens. Probably the most influential model that is currently known to be an MoE is the original GPT-4.</p>

<p>Expert routing algorithms work as follows: once we exit the attention block of any layer, we have a residual stream vector that is the output. Each expert has a corresponding expert vector of the same dimension, and we decide which experts will become activated by looking at which ones have the highest inner products with the current residual stream.</p>

<p>The problem with this is that it introduces a rather ill-behaved discontinuous function with a discrete image at the heart of the model, in sharp contrast to vanilla Transformers which implement continuous input-output relations. This causes gradient descent optimization methods to behave poorly in MoE training, often resulting in “routing collapse”, where the model gets stuck always activating the same few experts for every token instead of spreading its knowledge and computation around all of the available experts.</p>

<p>To get an intuition for routing collapse, consider attempting to train a model such as GPT-4 with 16 experts in total and 2 experts active per token. Now, suppose that for random initialization reasons two of these experts just happen to be the best performing ones at the start. Gradient descent will then reinforce the tendency to pick these experts. This will mean these experts will get almost all of the gradient signals during updates and become better while other experts lag behind, and so the other experts will continue not being picked, producing a positive feedback loop that results in other experts never getting chosen or trained.</p>

<p>The fundamental issue is that gradient descent just heads in the direction that’s locally best. This usually works fine in the very high dimensional optimization problems encountered in neural network training. However, when our neural network is so discontinuous in its behavior, even the high dimensionality of the problem space may not save us from failure.</p>

<p>It is nontrivial to address these training difficulties. DeepSeek v3 does so by combining several different innovations, each of which I will discuss in turn.</p>

<h3 id="auxiliary-loss-free-load-balancing">Auxiliary-loss-free load balancing</h3>

<p>A popular method for avoiding routing collapse is to force “balanced routing”, i.e. the property that each expert is activated roughly an equal number of times over a sufficiently large batch, by adding to the training loss a term measuring how imbalanced the expert routing was in a particular batch. This term is called an “auxiliary loss” and it makes intuitive sense that introducing it pushes the model towards balanced routing. However, the DeepSeek v3 technical report notes that such an auxiliary loss hurts model performance even if it ensures balanced routing.</p>

<p>Their alternative is to add expert-specific bias terms to the routing mechanism which get added to the expert affinities. These bias terms are not updated through gradient descent but are instead adjusted throughout training to ensure load balance: if a particular expert is not getting as many hits as we think it should, then we can slightly bump up its bias term by a fixed small amount every gradient step until it does. The technical report notes this achieves better performance than relying on an auxiliary loss while still ensuring appropriate load balance.</p>

<h3 id="shared-experts">Shared experts</h3>

<p>A serious problem with the above method of addressing routing collapse is that it assumes, without any justification, that an optimally trained MoE would have balanced routing. However, this is a dubious assumption.</p>

<p>To see why, consider that any large language model likely has a small amount of information that it uses a lot, while it has a lot of information that it uses rather infrequently. For instance, almost any English request made to an LLM requires the model to know how to speak English, but almost no request made to an LLM would require it to know who the King of France was in the year 1510. So it’s quite plausible the optimal MoE should have a few experts which are accessed a lot and store “common information”, while having others which are accessed sparsely and store “specialized information”.</p>

<p>If we force balanced routing, we lose the ability to implement such a routing setup and have to redundantly duplicate information across different experts. However, if we don’t force balanced routing, we face the risk of routing collapse. To escape this dilemma, DeepSeek separates experts into two types: <em>shared experts</em> and <em>routed experts</em>. Shared experts are always routed to no matter what: they are excluded from both expert affinity calculations and any possible routing imbalance loss term. We concern ourselves with ensuring balanced routing only for routed experts.</p>

<p>The key observation here is that “routing collapse” is an extreme situation where the likelihood of each individual expert being chosen is either 1 or 0. Naive load balancing addresses this by trying to push the distribution to be uniform, i.e. every expert should have the same chance of being selected. However, if our sole concern is to avoid routing collapse then there’s no reason for us to target specifically a uniform distribution. DeepSeek v3 instead targets a distribution where each expert is either selected for sure (probability 1) or selected with some fixed probability p &gt; 0 for each token.</p>

<p>I think it’s likely even this distribution is not optimal and a better choice of distribution will yield better MoE models, but it’s already a significant improvement over just forcing a uniform distribution.</p>

<h2 id="multi-token-prediction">Multi-token prediction</h2>

<p>The final change that DeepSeek v3 makes to the vanilla Transformer is the ability to predict multiple tokens out for each forward pass of the model. This allows them to use a multi-token prediction objective during training instead of strict next-token prediction, and they demonstrate a performance improvement from this change in ablation experiments.</p>

<p>The basic idea is the following: we first do an ordinary forward pass for next-token prediction. As we would in a vanilla Transformer, we use the final residual stream vector to generate next token probabilities through unembedding and softmax. However, unlike in a vanilla Transformer, we <em>also</em> feed this vector into a subsequent Transformer block, and we use the output of that block to make predictions about the second next token. We can iterate this as much as we like, though DeepSeek v3 only predicts two tokens out during training.</p>

<figure>
  <img src="https://epoch.ai/assets/images/gradient-updates/2025/how-has-deepseek-improved-the-transformer-architecture/figure-3.png">
<figcaption>
    <p>Figure 3: An illustration of DeepSeek v3’s multi-token prediction setup taken from its technical report.</p>
  </figcaption>
</figure>

<p>They incorporate these predictions about further out tokens into the training objective by adding an additional cross-entropy term to the training loss with a weight that can be tuned up or down as a hyperparameter. This not only gives them an additional target to get signal from during training but also allows the model to be used to <a href="https://arxiv.org/abs/2211.17192">speculatively decode</a> itself. We can generate a few tokens in each forward pass and then show them to the model to decide from which point we need to reject the proposed continuation.</p>

<p>DeepSeek v3 only uses multi-token prediction up to the second next token, and the acceptance rate the technical report quotes for second token prediction is between 85% and 90%. This is quite impressive and should allow nearly double the inference speed (in units of tokens per second per user) at a fixed price per token if we use the aforementioned speculative decoding setup. It doesn’t look worse than the acceptance probabilities one would get when decoding Llama 3 405B with Llama 3 70B, and might even be better.</p>

<p>I’m curious what they would have obtained had they predicted further out than the second next token. If e.g. each subsequent token gives us a 15% relative reduction in acceptance, it might be possible to squeeze out some more gain from this speculative decoding setup by predicting a few more tokens out.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I see many of the improvements made by DeepSeek as “obvious in retrospect”: they are the kind of innovations that, had someone asked me in advance about them, I would have said were good ideas. However, as I’ve said earlier, this doesn’t mean it’s easy to come up with the ideas in the first place.</p>

<p>I’ve heard many people express the sentiment that the DeepSeek team has “good taste” in research. Based just on these architectural improvements I think that assessment is right. None of these improvements seem like they were found as a result of some brute-force search through possible ideas. Instead, they look like they were carefully devised by researchers who understood how a Transformer works and how its various architectural deficiencies can be addressed.</p>

<p>If I had to guess where similar improvements are likely to be found next, probably prioritization of compute would be a good bet. Right now, a Transformer spends the same amount of compute per token regardless of which token it’s processing or predicting. This seems intuitively inefficient: the model should think more if it’s making a harder prediction and less if it’s making an easier one. To some extent this can be incorporated into an inference setup through variable test-time compute scaling, but I think there should also be a way to incorporate it into the architecture of the base models directly.</p>


          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sam Altman said startups with $10M were 'hopeless' competing with OpenAI (131 pts)]]></title>
            <link>https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise</link>
            <guid>42854525</guid>
            <pubDate>Tue, 28 Jan 2025 16:48:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise">https://www.tomshardware.com/tech-industry/artificial-intelligence/sam-altman-said-startups-with-only-usd10-million-were-totally-hopeless-competing-with-openai-deepseeks-disruption-says-otherwise</a>, See on <a href="https://news.ycombinator.com/item?id=42854525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">

<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1920-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg" alt="Sam Altman at a Q&amp;amp;A in June 2023" srcset="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1920-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/4ee5JvxCs6AZfG3DdobJMZ.jpg" data-pin-nopin="true" fetchpriority="high" crossorigin="anonymous">
</picture>
</div>
<figcaption>
<span>(Image credit: ET Now video)</span>
</figcaption>
</div>

<div id="article-body">
<p>Sam Altman's comments on the prospects of startups hoping to break through in the AI business may have come back to bite him. Several posts on X (and probably other platforms) ridicule the <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/openai-and-microsoft-reportedly-planning-dollar100-billion-datacenter-project-for-an-ai-supercomputer">OpenAI</a> boss and co-founder's dismissal of potential competition emanating from the startup scene, particularly those with only limited financial resources in the range of $10 million. Altman's comments were made during a Q&amp;A session after a 'Conversations' presentation to India VCs, recorded in June 2023. The comments seem way off the mark in early 2025, with DeepSeek now on the scene claiming its groundbreaking model only cost $5.6 million to train.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">This is pretty hilarious in retrospect.In India in 2023, Altman was asked how if a small, smart team with a budget of $10 million could build something substantial within AI.His reply: "It’s totally hopeless to compete with us on training foundation models" https://t.co/pdYIhV2x1m<a href="https://twitter.com/cantworkitout/status/1884113684978622538" data-url="https://twitter.com/cantworkitout/status/1884113684978622538" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">January 28, 2025</a></p></blockquote></div><p>Entrepreneur Arnaud Bertrand reckons that Altman's response in the above clip is "pretty hilarious in retrospect." In other words, Bertrand thinks Altman's dismissal of the Indian VC's question about startups challenging the likes of OpenAI showed a startling lack of foresight.</p><p>The video begins with the VC stating that there is a very vibrant startup ecosystem in India. He goes on to muse whether Altman might see a gap in the AI business, one which an Indian startup could fill. More specifically, the VC asks whether a trio of super-smart engineers from India "with say, not $100M, but $10M – could build something truly substantial?"</p><p>Altman's response was quite dismissive of the VC's well-mannered query. "Look, the way this works is we're going to tell you it's totally hopeless to compete with us on training foundation models. You shouldn't try, and it's your job to try anyway, and I believe both of those things," was Altman's disjointed stream-of-consciousness style reply, followed by audience titters. "I think it is pretty hopeless," he added, possibly wishing to soften his initial response.</p><div><blockquote data-lang="en"><p lang="en" dir="ltr">deepseek's r1 is an impressive model, particularly around what they're able to deliver for the price.we will obviously deliver much better models and also it's legit invigorating to have a new competitor! we will pull up some releases.<a href="https://twitter.com/cantworkitout/status/1884066337103962416" data-url="https://twitter.com/cantworkitout/status/1884066337103962416" target="_blank" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">January 28, 2025</a></p></blockquote></div><p>To Altman's credit, earlier today he posted a thread on X praising the catalyst behind his recent social media ridiculing. The launch of China's DeepSeek has caused significant AI business and <a data-analytics-id="inline-link" href="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-loses-usd589-billion-in-market-cap-broad-stock-plunge-triggered-by-deepseek-ai-release" data-before-rewrite-localise="https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-loses-usd589-billion-in-market-cap-broad-stock-plunge-triggered-by-deepseek-ai-release">tech industry tremors</a>, and Altman has now publicly admitted it is "an impressive model, particularly around what they're able to deliver for the price."</p><p>Nevertheless, like the funding-hungry CEO he is, Altman quickly turned the thread around to OpenAI promising jam tomorrow, with the execution of the firm's roadmap, amazing next-gen AI models, and "bringing you all AGI and beyond."</p><p>The amount of money DeepSeek truly spent on training its model, which it claims is $5.6 million, is contested. However, despite those contentions, it is clear that the company pulled off training a frontier model with disruptively low costs, shocking the US titans of AI.</p><div data-hydrate="true" id="slice-container-newsletterForm-articleInbodyContent-HyBY2hdXNd75Zxm2t4xDaJ"><section><p>Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.</p></section></div>
</div>



<!-- Drop in a standard article here maybe? -->


<div id="slice-container-authorBio-HyBY2hdXNd75Zxm2t4xDaJ"><p>Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.</p></div>
</section>





<div id="slice-container-relatedArticles"><p><h3>Most Popular</h3></p></div>








</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bitwarden introduces mandatory 2FA for new devices (213 pts)]]></title>
            <link>https://bitwarden.com/help/new-device-verification/</link>
            <guid>42853696</guid>
            <pubDate>Tue, 28 Jan 2025 15:51:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bitwarden.com/help/new-device-verification/">https://bitwarden.com/help/new-device-verification/</a>, See on <a href="https://news.ycombinator.com/item?id=42853696">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>To keep your account safe and secure, in February 2025, Bitwarden will require additional verification <strong>for users who do not use two-step login</strong>. After entering your Bitwarden master password, you will be prompted to enter a one-time verification code sent to your account email to complete the login process <strong>when logging in from a device you have not logged in to previously</strong>. For example, if you are logging in to a mobile app or a browser extension that you have used before, you will not receive this prompt.</p><p>Most users will not experience this prompt unless they are frequently logging into new devices. This verification is only needed for new devices or after clearing browser cookies.</p><p>If you regularly access your email, retrieving the verification code should be straightforward. If you prefer not to rely on your Bitwarden account email for verification, you can set up two-step login through an Authenticator app, a hardware key, or two-step login via a different email.</p><p>Users affected by this change will see the following in-product communication and should have received an email informing them of the change:</p><figure><img alt="New device verification announcement" id="3194724a-920f-5941-8c03-e54e6e214cb2" height="532" width="468" src="https://res.cloudinary.com/bw-com/image/upload/f_auto/v1/ctf/7rncvj1f8mw7/2tYcj1sDClqQ23ANh7nrIq/b88330b0a660fa47baaf59c68e0b9178/541c2ef1-b086-4489-a21f-a21fa6c1a905.png?_a=DAJAUVWIZAA0"><figcaption><cite>New device verification announcement</cite></figcaption></figure><p id="faqs"><a href="#faqs" title="#faqs"><svg version="1.1" viewBox="0 0 32 32"><path d="M30.939 9.669l-7.372-0.014 1.332-8.422c0.022-0.138 0.016-0.279-0.016-0.415s-0.092-0.264-0.174-0.377c-0.082-0.113-0.186-0.209-0.305-0.282s-0.251-0.122-0.389-0.144-0.279-0.016-0.415 0.016c-0.136 0.033-0.264 0.092-0.377 0.174-0.229 0.166-0.382 0.415-0.426 0.694l-1.388 8.75-8.728-0.018 1.316-8.31c0.028-0.141 0.027-0.286-0.002-0.426s-0.087-0.274-0.169-0.391c-0.082-0.118-0.187-0.218-0.309-0.294s-0.257-0.127-0.399-0.15c-0.142-0.022-0.287-0.016-0.426 0.019s-0.27 0.097-0.384 0.184c-0.114 0.087-0.21 0.195-0.282 0.32s-0.117 0.262-0.134 0.405l-1.366 8.64-7.776-0.016c-0.283 0-0.554 0.112-0.754 0.312s-0.312 0.471-0.312 0.754c0 0.283 0.112 0.554 0.312 0.754s0.471 0.312 0.754 0.312l7.442 0.014-1.4 8.876-7.718-0.016c-0.283 0-0.554 0.112-0.754 0.312s-0.312 0.471-0.312 0.754c0 0.283 0.112 0.554 0.312 0.754s0.471 0.312 0.754 0.312l7.384 0.016-1.266 8c-0.044 0.279 0.024 0.564 0.19 0.793s0.416 0.382 0.694 0.427c0.052 0.009 0.104 0.013 0.156 0.014 0.254 0 0.499-0.091 0.692-0.256s0.32-0.394 0.358-0.644l1.32-8.33 8.73 0.018-1.248 7.89c-0.044 0.279 0.024 0.564 0.19 0.793s0.415 0.382 0.694 0.427c0.056 0.009 0.112 0.013 0.168 0.014 0.253-0.001 0.498-0.092 0.691-0.257s0.32-0.393 0.359-0.643l1.3-8.218 7.762 0.016c0.283 0 0.554-0.112 0.754-0.312s0.312-0.471 0.312-0.754c0-0.283-0.112-0.554-0.312-0.754s-0.471-0.312-0.754-0.312l-7.428-0.016 1.4-8.876 7.704 0.016c0.283 0 0.554-0.112 0.754-0.312s0.312-0.471 0.312-0.754-0.112-0.554-0.312-0.754c-0.2-0.2-0.471-0.312-0.754-0.312h0.014zM19.671 20.657l-8.732-0.018 1.4-8.876 8.73 0.018-1.398 8.876z"></path></svg></a><h2>FAQs</h2></p><h4>When will this happen?</h4><p>This change will go into effect starting February 2025.</p><h4>Why is Bitwarden implementing this?</h4><p>Bitwarden is implementing this change to enhance security for users who don't have two-step login activated. If someone gains access to your password, they still won't be able to log into your account without secondary verification (the code sent to your email). This extra layer helps protect your data from hackers who often target weak or exposed passwords to gain unauthorized access.</p><h4>When will I get prompted for this verification?</h4><p>You will only get prompted for this verification when logging in from new devices. If you’re logging into a device that you’ve used before, you will not be prompted.&nbsp; </p><h4>What is considered a new device?&nbsp;</h4><p>A new device is any device that hasn't been previously used to log into your Bitwarden account. This could include a new phone, tablet, computer, or browser extension that you’ve never logged in from before. When you log in from a new device, you'll be asked to verify your identity via a one-time code sent to your email.&nbsp;</p><p>Other scenarios that will initiate a new device will be:</p><ul><li><p>If you uninstall and reinstall the mobile or desktop app</p></li><li><p>Clearing browser cookies&nbsp;<br></p></li></ul><h4>My email credentials are saved in Bitwarden. Will I be locked out of Bitwarden?</h4><p>Email verification codes will only be required on new devices for users that do not have two-step login enabled. You will not see this prompt on previously logged in devices and you will log in as normal with your account email and your master password.&nbsp;</p><p>If you are logging into a new device, your Bitwarden account email will receive a one-time verification code. If you have access to your email, i.e. a persistent logged in email on your mobile phone, then you will be able to grab the one-time verification code to log in. Once logged in to the new device, you will not be prompted again for the verification code.&nbsp;</p><p>If you regularly log into your email using credentials saved in Bitwarden or do not want to rely on your email for verification, you should set up two-step login that will be independent from the Bitwarden account email. This includes an authenticator app, security key, or email-based two-step login with a different email. Having any 2FA method active will opt the user out of the email-based new device verification. Users with 2FA active should also save their Bitwarden <a href="https://bitwarden.com/help/two-step-recovery-code/">recovery code</a> in a safe place.</p><h4>Who is excluded from this account email-based new device verification?</h4><p>The following categories of logins are excluded:</p><ul><li><p>Users who have two-step login set up are excluded.</p></li><li><p>Users who log in with SSO, a passkey, or with an API key are excluded.</p></li><li><p>Self-hosted users are excluded.</p></li><li><p>Users who log in from a device where they have previously logged in are excluded.</p></li></ul><h4>My organization uses SSO, do my users have to complete new device verification?</h4><p>No. Users logging in with SSO will be exempt and not asked to verify the login on a new device. However, if a user, without two-step login enabled, logs in with a username and password without going through SSO, they will be asked to verify the new device.</p><h4>I do not want to share my real email with Bitwarden, how can I set up my account?</h4><p>Users who want to remain anonymous have several options available:</p><ul><li><p>Use a two-step login option that doesn’t require an email, including an authenticator app, security key, or email-based two-step login with a different email.</p></li><li><p>Use an email alias forwarding service.</p></li><li><p>Self-host Bitwarden.</p></li></ul><p>Bitwarden encourages users to have an active email, as Bitwarden sends important security alerts like failed login attempts.</p><div id="help-page-suggestions"><p><span><h3>Suggest changes to this page</h3><p>How can we improve this page for you? <br>For technical, billing, and product questions, please <a href="https://bitwarden.com/contact/">contact support</a></p></span></p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boom XB-1 First Supersonic Flight [video] (334 pts)]]></title>
            <link>https://www.youtube.com/watch?v=-qisIViAHwI</link>
            <guid>42853633</guid>
            <pubDate>Tue, 28 Jan 2025 15:46:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=-qisIViAHwI">https://www.youtube.com/watch?v=-qisIViAHwI</a>, See on <a href="https://news.ycombinator.com/item?id=42853633">Hacker News</a></p>
Couldn't get https://www.youtube.com/watch?v=-qisIViAHwI: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Almost one in 10 people use the same four-digit PIN (148 pts)]]></title>
            <link>https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842</link>
            <guid>42853617</guid>
            <pubDate>Tue, 28 Jan 2025 15:45:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842">https://www.abc.net.au/news/2025-01-28/almost-one-in-ten-people-use-the-same-four-digit-pin/103946842</a>, See on <a href="https://news.ycombinator.com/item?id=42853617">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component="Decoy" data-key="article"><p>Find out if you're one of them.</p><p>The last line of security for much of your digital life probably isn't as secure as you think.</p><p>Whether it's to unlock your smartphone, access your online banking or get cash out of the ATM, a four-digit PIN is often there to keep your secrets and your money safe.</p><p>It’s an important little code, but not all choices are equally secure.</p><p>That's why we analysed 29 million of them from <a href="https://haveibeenpwned.com/" data-component="Link" target="_blank" rel="noopener noreferrer">Have I Been Pwned?</a> – an Australian-run site that helps people all over the world find out if they've been affected by data breaches.</p><p>The most commonly used PINs turned out to be staggeringly popular, meaning they're particularly easy to guess when phones and bank cards fall into the wrong hands.</p><p>This grid of green squares might remind you of an old Space Invaders game, but it's actually something like a mind-reader.</p><figure role="group" data-print="inline-media" aria-labelledby="104257640" data-component="Figure" data-uri="coremedia://image/104257640"><div><p><img alt="A grid of all 10,000 PIN codes, with two bright lines across it showing where the most popular ones are" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/29b2240ce9d896959a78aa2e10cbdfd8?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257640" data-component="Figure__figcaption"> <!-- -->How popular each of the 10,000 possible PIN codes are.</figcaption></figure><p>It's going to let us peer inside and find out why humans choose some PINs more than others.</p><p>Every square in the grid represents a four-digit code.</p><p>We've highlighted <strong>4560</strong> as an example to get you across how it works.</p><p>The grid is arranged by splitting the digits of each code into pairs.</p><p>The first two digits (<strong>45</strong>) are taken from the vertical axis, and the last two digits (<strong>60</strong>) are from the horizontal one.</p><p>The brighter the square, the more popular the code – which means the blocks of bright squares are the ones we need to avoid when choosing a PIN.</p><p>So, which number is the clear favourite? Chances are you've used it at some stage.</p><figure role="group" data-print="inline-media" aria-labelledby="104257642" data-component="Figure" data-uri="coremedia://image/104257642"><div><p><img alt="1234 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/87cdc6eee4aef6f923fe3374c144a23f?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257642" data-component="Figure__figcaption"> <!-- -->1234 is easily the most popular four-digit PIN.</figcaption></figure><p><strong>1234</strong> is the most popular choice by a huge margin, accounting for nearly one in 10 of the millions of PINs we looked at.</p><p>And then there's the diagonal line running from the bottom-left corner to the top-right one.</p><p>It stands out, and that's because it's made up of PINs that use repeated digits...</p><p>... like <strong>0000</strong>, which is the second most popular code.</p><figure role="group" data-print="inline-media" aria-labelledby="104257644" data-component="Figure" data-uri="coremedia://image/104257644"><div><p><img alt="1111 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/1cf7163c5868ca943d8b42247f185296?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257644" data-component="Figure__figcaption"> <!-- -->1111 sits on the diagonal line from bottom-left to top-right.</figcaption></figure><p>And right behind it is <strong>1111</strong>.</p><p><strong>1212</strong> and <strong>4444</strong> are in the top ten as well.</p><figure role="group" data-print="inline-media" aria-labelledby="104257646" data-component="Figure" data-uri="coremedia://image/104257646"><div><p><img alt="1212 and 4444 are highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/363ac8ca9bb7865ddb26fc788dea7ef4?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257646" data-component="Figure__figcaption"> <!-- -->1212 and 4444 are also popular choices with repeated digits.</figcaption></figure><p>There's also a (broken) horizontal line, split between <strong>19</strong> and <strong>20</strong> for the first two digits.</p><p>What does that remind you of?</p><figure role="group" data-print="inline-media" aria-labelledby="104257648" data-component="Figure" data-uri="coremedia://image/104257648"><div><p><img alt="2004 and 1986 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/dbaef0e2a139e7aec8bdb5ca03a15dab?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257648" data-component="Figure__figcaption"> <!-- -->1986 is the most popular year to use as a PIN code.</figcaption></figure><p>They are the birth years of people who are alive today.</p><p><strong>1986</strong> is the most popular of these, while <strong>2004</strong> is also in the top 20.</p><p>There's also a block-ish area around the bottom left that needs some explaining.</p><figure role="group" data-print="inline-media" aria-labelledby="104257650" data-component="Figure" data-uri="coremedia://image/104257650"><div><p><img alt="2512 is highlighted on the grid of all possible four-digit codes. It sits inside a block from 0101 to 3112" sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/78449ac5a9d503082ab3763a8b9b4c82?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257650" data-component="Figure__figcaption"> <!-- -->Christmas day (2512) is a popular choice.</figcaption></figure><p>These are all the combinations that could represent dates like <strong>2512</strong>.</p><p><strong>2902</strong> is not as popular as its neighbours, but that's probably because it only comes around once every four years.</p><figure role="group" data-print="inline-media" aria-labelledby="104257652" data-component="Figure" data-uri="coremedia://image/104257652"><div><p><img alt="0229 is highlighted on the grid of all possible four-digit codes." sizes="100vw" src="https://live-production.wcms.abc-cdn.net.au/109c46995b22d40228b6e203074066fc?src" loading="lazy" data-component="Image" data-lazy="true"></p></div><figcaption id="104257652" data-component="Figure__figcaption"> <!-- -->The US style of formatting dates is also popular.</figcaption></figure><p>If you live in the United States, you'd be using <strong>0229</strong> instead.</p><p>This explains the slightly less prominent, yet almost perfectly symmetrical grid overlapping the other one.</p><p>But what about the other popular codes that don't fall on any of these special lines or grids?</p><p>The reason for choosing <strong>4321</strong> is no real mystery. It's just <strong>1234</strong> in reverse.</p><p>Some people have tried to be clever; they've mixed things up by choosing <strong>1342</strong>.</p><p>So many of them, in fact, that it's the 4th most popular code of all.</p><p><strong>2580</strong> might seem like a strange one to be in the top 40…</p><p>…until you realise it draws a line directly down the keypad on a phone.</p><p>It makes sense why some four-digit codes are chosen again and again, but this phenomenon brings with it a serious security risk.</p><p>Even though there are 10,000 possible combinations, when humans get involved that equation changes dramatically.</p><p>If someone wants to unlock a stolen phone – or retrieve money from an ATM – and only have five guesses, this data suggests they still have a one-in-eight chance of guessing correctly.</p><p>And, while it's harder to visualise, there is a similar weakness to be found in regular passwords too.</p><p><strong>1234</strong> was as high as fourth on a list of common passwords compiled by NordPass VPN.</p><p>Even when people have the entire keyboard to choose from, the only choices that were more popular were <strong>123456</strong>, "admin" and "password".</p><p>All in all, it paints a worrying picture of the last line of defence for our digital lives.</p><p>Earlier this year, journalists attending a briefing at the UK's National Cyber Security Centre (NCSC) were given a security code to access the building's facilities.</p><p>The code they were given was <strong>1234</strong>.</p><p>The NCSC later clarified this was <a href="https://www.theregister.com/2024/05/10/ncsc_entry_code/" data-component="Link">only a temporary code used for the briefing</a>.</p><p>And there's a lesson in that: if you're one of the millions of people using an ill-advised PIN, perhaps yours should be temporary too.</p><p>Remember, it's never too late to change yours to something more secure.</p><h2 data-component="Heading">The top 50 codes to avoid</h2><p><em>These are the 50 most popular codes in the full Have I Been Pwned? dataset, in order of popularity.</em></p><div data-component="ContentOverflow"><table data-component="Table"><thead><tr><th>Ranking</th><th>Code</th><th>Popularity</th></tr></thead><tbody><tr><td>1</td><td><strong>1234</strong></td><td>9.0%</td></tr><tr><td>2</td><td><strong>1111</strong></td><td>1.6%</td></tr><tr><td>3</td><td><strong>0000</strong></td><td>1.1%</td></tr><tr><td>4</td><td><strong>1342</strong></td><td>0.6%</td></tr><tr><td>5</td><td><strong>1212</strong></td><td>0.4%</td></tr><tr><td>6</td><td><strong>2222</strong></td><td>0.3%</td></tr><tr><td>7</td><td><strong>4444</strong></td><td>0.3%</td></tr><tr><td>8</td><td><strong>1122</strong></td><td>0.3%</td></tr><tr><td>9</td><td><strong>1986</strong></td><td>0.3%</td></tr><tr><td>10</td><td><strong>2020</strong></td><td>0.3%</td></tr><tr><td>11</td><td><strong>7777</strong></td><td>0.3%</td></tr><tr><td>12</td><td><strong>5555</strong></td><td>0.3%</td></tr><tr><td>13</td><td><strong>1989</strong></td><td>0.3%</td></tr><tr><td>14</td><td><strong>9999</strong></td><td>0.2%</td></tr><tr><td>15</td><td><strong>6969</strong></td><td>0.2%</td></tr><tr><td>16</td><td><strong>2004</strong></td><td>0.2%</td></tr><tr><td>17</td><td><strong>1010</strong></td><td>0.2%</td></tr><tr><td>18</td><td><strong>4321</strong></td><td>0.2%</td></tr><tr><td>19</td><td><strong>6666</strong></td><td>0.2%</td></tr><tr><td>20</td><td><strong>1984</strong></td><td>0.2%</td></tr><tr><td>21</td><td><strong>1987</strong></td><td>0.2%</td></tr><tr><td>22</td><td><strong>1985</strong></td><td>0.2%</td></tr><tr><td>23</td><td><strong>8888</strong></td><td>0.2%</td></tr><tr><td>24</td><td><strong>2000</strong></td><td>0.2%</td></tr><tr><td>25</td><td><strong>1980</strong></td><td>0.2%</td></tr><tr><td>26</td><td><strong>1988</strong></td><td>0.2%</td></tr><tr><td>27</td><td><strong>1982</strong></td><td>0.2%</td></tr><tr><td>28</td><td><strong>2580</strong></td><td>0.2%</td></tr><tr><td>29</td><td><strong>1313</strong></td><td>0.2%</td></tr><tr><td>30</td><td><strong>1990</strong></td><td>0.2%</td></tr><tr><td>31</td><td><strong>1991</strong></td><td>0.2%</td></tr><tr><td>32</td><td><strong>1983</strong></td><td>0.2%</td></tr><tr><td>33</td><td><strong>1978</strong></td><td>0.2%</td></tr><tr><td>34</td><td><strong>1979</strong></td><td>0.2%</td></tr><tr><td>35</td><td><strong>1995</strong></td><td>0.2%</td></tr><tr><td>36</td><td><strong>1994</strong></td><td>0.2%</td></tr><tr><td>37</td><td><strong>1977</strong></td><td>0.2%</td></tr><tr><td>38</td><td><strong>1981</strong></td><td>0.2%</td></tr><tr><td>39</td><td><strong>3333</strong></td><td>0.2%</td></tr><tr><td>40</td><td><strong>1992</strong></td><td>0.2%</td></tr><tr><td>41</td><td><strong>1975</strong></td><td>0.2%</td></tr><tr><td>42</td><td><strong>2005</strong></td><td>0.2%</td></tr><tr><td>43</td><td><strong>1993</strong></td><td>0.2%</td></tr><tr><td>44</td><td><strong>1976</strong></td><td>0.2%</td></tr><tr><td>45</td><td><strong>1996</strong></td><td>0.2%</td></tr><tr><td>46</td><td><strong>2002</strong></td><td>0.2%</td></tr><tr><td>47</td><td><strong>1973</strong></td><td>0.2%</td></tr><tr><td>48</td><td><strong>2468</strong></td><td>0.2%</td></tr><tr><td>49</td><td><strong>1998</strong></td><td>0.1%</td></tr><tr><td>50</td><td><strong>1974</strong></td><td>0.1%</td></tr></tbody></table></div><ul data-component="List" role="list"><li data-component="ListItem"><span></span><strong>1234</strong></li><li data-component="ListItem"><span></span><strong>1111</strong></li><li data-component="ListItem"><span></span><strong>0000</strong></li><li data-component="ListItem"><span></span><strong>1342</strong></li><li data-component="ListItem"><span></span><strong>1212</strong></li><li data-component="ListItem"><span></span><strong>2222</strong></li><li data-component="ListItem"><span></span><strong>4444</strong></li><li data-component="ListItem"><span></span><strong>1122</strong></li><li data-component="ListItem"><span></span><strong>1986</strong></li><li data-component="ListItem"><span></span><strong>2020</strong></li><li data-component="ListItem"><span></span><strong>7777</strong></li><li data-component="ListItem"><span></span><strong>5555</strong></li><li data-component="ListItem"><span></span><strong>1989</strong></li><li data-component="ListItem"><span></span><strong>9999</strong></li><li data-component="ListItem"><span></span><strong>6969</strong></li><li data-component="ListItem"><span></span><strong>2004</strong></li><li data-component="ListItem"><span></span><strong>1010</strong></li><li data-component="ListItem"><span></span><strong>4321</strong></li><li data-component="ListItem"><span></span><strong>6666</strong></li><li data-component="ListItem"><span></span><strong>1984</strong></li><li data-component="ListItem"><span></span><strong>1987</strong></li><li data-component="ListItem"><span></span><strong>1985</strong></li><li data-component="ListItem"><span></span><strong>8888</strong></li><li data-component="ListItem"><span></span><strong>2000</strong></li><li data-component="ListItem"><span></span><strong>1980</strong></li><li data-component="ListItem"><span></span><strong>1988</strong></li><li data-component="ListItem"><span></span><strong>1982</strong></li><li data-component="ListItem"><span></span><strong>2580</strong></li><li data-component="ListItem"><span></span><strong>1313</strong></li><li data-component="ListItem"><span></span><strong>1990</strong></li><li data-component="ListItem"><span></span><strong>1991</strong></li><li data-component="ListItem"><span></span><strong>1983</strong></li><li data-component="ListItem"><span></span><strong>1978</strong></li><li data-component="ListItem"><span></span><strong>1979</strong></li><li data-component="ListItem"><span></span><strong>1995</strong></li><li data-component="ListItem"><span></span><strong>1994</strong></li><li data-component="ListItem"><span></span><strong>1977</strong></li><li data-component="ListItem"><span></span><strong>1981</strong></li><li data-component="ListItem"><span></span><strong>3333</strong></li><li data-component="ListItem"><span></span><strong>1992</strong></li><li data-component="ListItem"><span></span><strong>1975</strong></li><li data-component="ListItem"><span></span><strong>2005</strong></li><li data-component="ListItem"><span></span><strong>1993</strong></li><li data-component="ListItem"><span></span><strong>1976</strong></li><li data-component="ListItem"><span></span><strong>1996</strong></li><li data-component="ListItem"><span></span><strong>2002</strong></li><li data-component="ListItem"><span></span><strong>1973</strong></li><li data-component="ListItem"><span></span><strong>2468</strong></li><li data-component="ListItem"><span></span><strong>1998</strong></li><li data-component="ListItem"><span></span><strong>1974</strong></li></ul><h2 data-component="Heading">About this story</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span>This visualisation was inspired by similar work from <a href="http://datagenetics.com/blog/september32012/index.html" data-component="Link">Nick Berry</a> in 2013</li><li data-component="ListItem"><span></span>The popularity of each PIN code was retrieved from <a href="https://haveibeenpwned.com/Passwords" data-component="Link">Have I Been Pwned's pwned passwords API</a>, which includes passwords leaked from a variety of sources and likely contains duplicate data. While it isn't a perfect data set, it aligns with likely usage patterns, even if it's just because people repeat their PIN codes on their computers.</li></ul><h2 data-component="Heading">Credits</h2><ul data-component="List" role="list"><li data-component="ListItem"><span></span><strong>Reporting and development:</strong> <a href="https://www.abc.net.au/news/julian-fell/13905936" data-component="ContentLink" data-uri="coremedia://person/13905936">Julian Fell</a></li><li data-component="ListItem"><span></span><strong>Design:</strong> <a href="https://www.abc.net.au/news/teresa-tan/9250964" data-component="ContentLink" data-uri="coremedia://person/9250964">Teresa Tan</a></li><li data-component="ListItem"><span></span><strong>Editing:</strong> <a href="https://www.abc.net.au/news/cristen-tilley/4942860" data-component="ContentLink" data-uri="coremedia://person/4942860">Cristen Tilley</a></li></ul></div><p><span data-component="Text">Posted<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2025-01-27T18:53:33.000Z">22 hours ago</time><time data-component="Text">Mon 27 Jan 2025 at 6:53pm</time>, <span data-component="Text">updated<!-- -->&nbsp;</span><time data-component="ScreenReaderOnly" datetime="2025-01-28T01:28:13.000Z">16 hours ago</time><time data-component="Text">Tue 28 Jan 2025 at 1:28am</time></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Maxima in the browser using Embedded Common Lisp on WASM (159 pts)]]></title>
            <link>https://maxima-on-wasm.pages.dev/</link>
            <guid>42853528</guid>
            <pubDate>Tue, 28 Jan 2025 15:37:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maxima-on-wasm.pages.dev/">https://maxima-on-wasm.pages.dev/</a>, See on <a href="https://news.ycombinator.com/item?id=42853528">Hacker News</a></p>
Couldn't get https://maxima-on-wasm.pages.dev/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[IAC confirms existence of a Super-earth in the habitable zone of a Sun-like Star (208 pts)]]></title>
            <link>https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star</link>
            <guid>42853174</guid>
            <pubDate>Tue, 28 Jan 2025 15:09:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star">https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star</a>, See on <a href="https://news.ycombinator.com/item?id=42853174">Hacker News</a></p>
Couldn't get https://www.iac.es/en/outreach/news/iac-confirms-existence-super-earth-habitable-zone-sun-star: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek: X2 Speed for WASM with SIMD (693 pts)]]></title>
            <link>https://simonwillison.net/2025/Jan/27/llamacpp-pr/</link>
            <guid>42852866</guid>
            <pubDate>Tue, 28 Jan 2025 14:44:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Jan/27/llamacpp-pr/">https://simonwillison.net/2025/Jan/27/llamacpp-pr/</a>, See on <a href="https://news.ycombinator.com/item?id=42852866">Hacker News</a></p>
Couldn't get https://simonwillison.net/2025/Jan/27/llamacpp-pr/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Janus Pro 1B running 100% locally in-browser on WebGPU (198 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/</link>
            <guid>42852400</guid>
            <pubDate>Tue, 28 Jan 2025 14:04:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/">https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/</a>, See on <a href="https://news.ycombinator.com/item?id=42852400">Hacker News</a></p>
Couldn't get https://old.reddit.com/r/LocalLLaMA/comments/1ibnso0/janus_pro_1b_running_100_locally_inbrowser_on/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Osaka bans smoking on all of its streets, vaping included (108 pts)]]></title>
            <link>https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/</link>
            <guid>42852073</guid>
            <pubDate>Tue, 28 Jan 2025 13:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/">https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/</a>, See on <a href="https://news.ycombinator.com/item?id=42852073">Hacker News</a></p>
Couldn't get https://soranews24.com/2025/01/28/osaka-bans-smoking-on-all-of-its-streets-violators-will-be-fined-by-patrols-vaping-included/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[US pauses all Federal aid and grants (209 pts)]]></title>
            <link>https://www.bbc.com/news/articles/c77rdy6gzy5o</link>
            <guid>42851248</guid>
            <pubDate>Tue, 28 Jan 2025 11:34:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/articles/c77rdy6gzy5o">https://www.bbc.com/news/articles/c77rdy6gzy5o</a>, See on <a href="https://news.ycombinator.com/item?id=42851248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="byline-new" data-component="byline-block"><p><span>James FitzGerald &amp; Ana Faguy</span></p><p><span>BBC News<!-- --></span></p></div><div data-component="text-block"><p>US President Donald Trump has paused grants, loans and other federal assistance, according to a leaked government memo verified by the BBC's US partner, CBS News.<!-- --></p><p>In the memo, the acting head of the Office of Management and Budget (OMB) calls on government agencies to ensure spending is consistent with Trump's priorities.<!-- --></p><p>The full impact of the pause is not yet clear, although the memo specifies that Medicare and Social Security benefits are not affected. It comes days after the US halted nearly all foreign aid.<!-- --></p><p>The move has been criticised by members of the rival Democratic Party who warn of "devastating consequences" on programmes that people rely on. <!-- --></p></div><div data-component="text-block"><p>Diane Yentel of the National Council of Nonprofits said the order could stop cancer research, food assistance and suicide hotlines.<!-- --></p><p>Given the spending that is now on hold was apportioned by Congress, it is likely this will face legal challenges about the scope of presidential power.<!-- --></p><p>The memo, signed by acting OMB chief Matthew Vaeth, calls on government agencies to temporarily pause their financial assistance programmes, so they can review spending that could be impacted by the various orders Trump has signed .<!-- --></p><p>It says this encompasses "financial assistance for foreign aid, nongovernmental organizations, DEI, woke gender ideology, and the green new deal".<!-- --></p></div><div data-component="text-block"><p>A deadline of 17:00 EST (22:00 GMT) has been set. Each agency is told to pause the issuing of new awards as well as the disbursement of funds under existing awards. <!-- --></p><p>The memo further demands that all agencies report which programmes have been paused by 10 February. <!-- --></p><p>The White House has not yet commented officially on the leaked document.<!-- --></p><p>Democrats in Washington DC were quick to sound an alarm of concern about the plan. <!-- --></p><p>The top Democratic appropriators in Congress - Washington Senator Patty Murray and Connecticut Congresswoman Rosa DeLauro - sent a letter to the White House Monday evening expressing their "extreme alarm" with the memo. <!-- --></p><p>"The scope of what you are ordering is breathtaking, unprecedented, and will have devastating consequences across the country," the congresswomen wrote. "We write today to urge you in the strongest possible terms to uphold the law and the Constitution and ensure all federal resources are delivered in accordance with the law."<!-- --></p><p>The Democratic minority leader of the US Senate, Chuck Schumer, was also critical of the pause: "Congress approved these investments and they are not optional; they are the law." <!-- --></p><p>He added: "It will mean missed payrolls and rent payments and everything in between: chaos for everything from universities to non-profit charities."<!-- --></p><p>The move follows last week's news that the Department of State had issued a halt to nearly all existing foreign assistance and paused new aid, according to an internal memo sent to officials and US embassies abroad. <!-- --></p><p>It appeared to affect everything from development assistance to military aid, making exceptions only for emergency food aid and for military funding for Israel and Egypt.<!-- --></p><p>Trump had earlier issued an executive order for a 90-day pause in foreign development assistance pending a review of efficiencies and consistency with his foreign policy. <!-- --></p><p>The US is the world's biggest international aid donor, having spent $68bn (£66bn) in 2023 according to government figures. The State Department notice appears to affect everything from development assistance to military aid.<!-- --></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cleveland police used AI to justify a search warrant. It derailed a murder case (134 pts)]]></title>
            <link>https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html</link>
            <guid>42851124</guid>
            <pubDate>Tue, 28 Jan 2025 11:17:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html">https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html</a>, See on <a href="https://news.ycombinator.com/item?id=42851124">Hacker News</a></p>
Couldn't get https://www.cleveland.com/news/2025/01/cleveland-police-used-ai-to-justify-a-search-warrant-it-has-derailed-a-murder-case.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[US Civil servants are being asked who they voted for in 2024 election (137 pts)]]></title>
            <link>https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html</link>
            <guid>42850644</guid>
            <pubDate>Tue, 28 Jan 2025 09:58:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html">https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html</a>, See on <a href="https://news.ycombinator.com/item?id=42850644">Hacker News</a></p>
Couldn't get https://www.independent.co.uk/news/world/americas/us-politics/trump-civil-service-loyalty-transition-b2678674.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek-R1 with Dynamic 1.58-bit Quantization (667 pts)]]></title>
            <link>https://unsloth.ai/blog/deepseekr1-dynamic</link>
            <guid>42850222</guid>
            <pubDate>Tue, 28 Jan 2025 08:52:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://unsloth.ai/blog/deepseekr1-dynamic">https://unsloth.ai/blog/deepseekr1-dynamic</a>, See on <a href="https://news.ycombinator.com/item?id=42850222">Hacker News</a></p>
Couldn't get https://unsloth.ai/blog/deepseekr1-dynamic: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Reinforcement Learning – A Reference (106 pts)]]></title>
            <link>https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference</link>
            <guid>42850111</guid>
            <pubDate>Tue, 28 Jan 2025 08:31:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference">https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference</a>, See on <a href="https://news.ycombinator.com/item?id=42850111">Hacker News</a></p>
Couldn't get https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[FTC Takes Action Against GoDaddy for Alleged Lax Data Security (202 pts)]]></title>
            <link>https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services</link>
            <guid>42849632</guid>
            <pubDate>Tue, 28 Jan 2025 07:02:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services">https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services</a>, See on <a href="https://news.ycombinator.com/item?id=42849632">Hacker News</a></p>
Couldn't get https://www.ftc.gov/news-events/news/press-releases/2025/01/ftc-takes-action-against-godaddy-alleged-lax-data-security-its-website-hosting-services: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Open-R1: an open reproduction of DeepSeek-R1 (380 pts)]]></title>
            <link>https://huggingface.co/blog/open-r1</link>
            <guid>42849536</guid>
            <pubDate>Tue, 28 Jan 2025 06:40:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://huggingface.co/blog/open-r1">https://huggingface.co/blog/open-r1</a>, See on <a href="https://news.ycombinator.com/item?id=42849536">Hacker News</a></p>
Couldn't get https://huggingface.co/blog/open-r1: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Machine Learning in Production (CMU Course) (430 pts)]]></title>
            <link>https://mlip-cmu.github.io/s2025/</link>
            <guid>42847834</guid>
            <pubDate>Tue, 28 Jan 2025 01:18:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mlip-cmu.github.io/s2025/">https://mlip-cmu.github.io/s2025/</a>, See on <a href="https://news.ycombinator.com/item?id=42847834">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="container">

<h2>Machine Learning in Production (17-445/17-645/17-745) / AI Engineering (11-695)</h2>
<h3>Spring 2025</h3>
<p><em>CMU course that covers how to build, deploy, assure, and maintain software products with machine-learned models. Includes the entire lifecycle from a prototype ML model to an entire system deployed in production. Covers also <strong>responsible AI</strong> (including safety, security, fairness, explainability) and <strong>MLOps</strong>. For earlier offerings see websites for&nbsp;<a href="https://ckaestne.github.io/seai/F2019">Fall 2019</a>,&nbsp;<a href="https://ckaestne.github.io/seai/S2020">Summer 2020</a>, <a href="https://ckaestne.github.io/seai/F2020/">Fall 2020</a>, <a href="https://ckaestne.github.io/seai/S2021/">Spring 2021</a>&nbsp;&nbsp;<a href="https://ckaestne.github.io/seai/S2022/">Spring 2022</a>, <a href="https://ckaestne.github.io/seai/F2022/">Fall 2022</a>,&nbsp;<a href="https://github.com/mlip-cmu/s2023">Spring 2023</a>, <a href="https://github.com/mlip-cmu/s2024">Spring 2024</a>, and <a href="https://github.com/mlip-cmu/f2024">Fall 2024</a>. This Spring 2025 offering is designed for students with some data science experience (e.g., has taken a machine learning course, has used sklearn) and basic programming skills (e.g., basic Python programming with libraries, can navigate a Unix shell), but will not expect a software engineering background (i.e., experience with testing, requirements, architecture, process, or teams is not required). Going forward we expect to offer this course at least every spring semester and possibly some fall semesters (not summer semesters).</em></p>
<p><strong>Waitlist: we often cannot accommodate all interested students in Spring semesters, though we expect there to be waitlist movement.  We encourage students who are able to move to alternative labs with space, to do so.  For students enrolled in 17-XXX numbers, contact Jenni Cooper (<a href="mailto:cooperj@andrew.cmu.edu">cooperj@andrew.cmu.edu</a>) for assistance; for students enrolled in 11-XXX numbers, contact Amber Vivis (<a href="mailto:albrown@andrew.cmu.edu">albrown@andrew.cmu.edu</a>) and Karen Kirk (<a href="mailto:karensuk@andrew.cmu.edu">karensuk@andrew.cmu.edu</a>).  Note that the instructors cannot help with waitlist/registration movement, please contact the course admins instead!</strong>  </p>
<hr>
<p>For researchers, educators, or others interested in this topic, we share all course material, including slides and assignments, under a creative commons license on GitHub (<a href="https://github.com/mlip-cmu">https://github.com/mlip-cmu</a>) and have also published a <a href="https://mlip-cmu.github.io/book/">textbook</a> with chapters corresponding to almost every lecture. A while ago we also wrote an article describing the rationale and the initial design of this course: <a href="https://arxiv.org/abs/2001.06691">Teaching Software Engineering for AI-Enabled Systems</a>. Video recordings of the Summer 2020 offering are online on the <a href="https://ckaestne.github.io/seai/S2020/#course-content">course page</a>, though they are a bit outdated by now. We would be happy to see this course or a similar version taught at other universities. See also an <a href="https://github.com/ckaestne/seaibib">annotated bibliography</a> on research in this field.</p>
<h2>Course Description</h2>
<p>This is a course for those who want to build <strong>software products</strong> with <strong>machine learning</strong>, not just models and demos. We assume that you can train a model or build prompts to make predictions, but what does it take to turn the model into a product and actually deploy it, have confidence in its quality, and successfully operate and maintain it at scale? </p>
<p>The course is designed to establish a working relationship between <strong>software engineers</strong> and <strong>data scientists</strong>: both contribute to building ML-enabled systems but have different expertise and focuses. To work together they need a mutual understanding of their roles, tasks, concerns, and goals and build a working relationship. This course is aimed at <strong>software engineers</strong> who want to build robust and responsible products meeting the specific challenges of working with ML components and at <strong>data scientists</strong> who want to understand the requirements of the model for production use and want to facilitate getting a prototype model into production; it facilitates communication and collaboration between both roles. The course is a good fit for student looking at a career as an <strong>ML engineer</strong>. <em>The course focuses on all the steps needed to turn a model into a production system in a responsible and reliable manner.</em></p>
<p><img src="https://mlip-cmu.github.io/s2025/overview.svg" alt="Course overview"></p>
<p>It covers topics such as:</p>
<ul>
<li><strong>How to design for wrong predictions the model may make?</strong> How to assure <em>safety</em> and <em>security</em> despite possible mistakes? How to design the <em>user interface</em> and the entire system to operate in the real world?</li>
<li><strong>How to reliably deploy and update models in production?</strong> How can we <em>test</em> the entire machine learning pipeline? How can <em>MLOps</em> tools help to automate and scale the deployment process? How can we <em>experiment in production</em> (A/B testing, canary releases)? How do we detect <em>data quality</em> issues, <em>concept drift</em>, and <em>feedback loops</em> in production?</li>
<li><strong>How to scale production ML systems?</strong> How do we design a system to process huge amounts of training data, telemetry data, and user requests? Should we use stream processing, batch processing, lambda architecture, or data lakes?</li>
<li><strong>How to test and debug production ML systems?</strong> How can we <em>evaluate</em> the quality of a model’s predictions in production? How can we <em>test</em> the entire ML-enabled system, not just the model? What lessons can we learn from <em>software testing</em>, <em>automated test case generation</em>, <em>simulation</em>, and <em>continuous integration</em> for testing for production machine learning?</li>
<li><strong>Which qualities matter beyond a model’s prediction accuracy?</strong> How can we identify and measure important quality requirements, including <em>learning and inference latency, operating cost, scalability, explainablity, fairness, privacy, robustness</em>, and <em>safety</em>? Does the application need to be able to <em>operate offline</em> and how often do we need to update the models? How do we identify what’s important in a ML-enabled product in a production setting for a business? How do we resolve <em>conflicts</em> and <em>tradeoffs</em>?</li>
<li><strong>How to work effectively in interdisciplinary teams?</strong> How can we bring data scientists, software engineers, UI designers, managers, domain experts, big data specialists, operators, legal council, and other roles together and develop a <em>shared understanding</em> and <em>team culture</em>?</li>
</ul>
<p><strong>Examples and case studies</strong> of ML-driven products we discuss include automated audio transcription; distributed detection of missing children on webcams and instant translation in augmented reality; cancer detection, fall detection, COVID diagnosis, and other smart medical and health services; automated slide layout in Powerpoint; semi-automated college admissions; inventory management; smart playlists and movie recommendations; ad fraud detection; delivery robots and smart driving features; and many others.</p>
<p>An extended group project focuses on building, deploying, evaluating, and maintaining a robust and scalable <em>movie recommendation service</em> under somewhat realistic “production” conditions with 1 million users.</p>
<h3>Learning Outcomes</h3>
<p>After taking this course, among others, students should be able to</p>
<ul>
<li>analyze tradeoffs for designing production systems with ML-components, analyzing various qualities beyond accuracy such as operation cost, latency, updateability, and explainability</li>
<li>plan for mistakes in ML components and implement production-quality systems that are robust to those mistakes</li>
<li>design fault-tolerant and scalable data infrastructure for learning models, serving models, versioning, and experimentation</li>
<li>ensure quality of the entire machine learning pipeline with test automation and other quality assurance techniques, including automated checks for data quality, data drift, feedback loops, and model quality</li>
<li>build systems that can be tested and monitored in production and build robust deployment pipelines</li>
<li>consider system-level requirements such as safety, security, privacy, fairness, and usability when building complex ML-enabled products</li>
<li>communicate effectively in interdisciplinary teams</li>
</ul>
<p>In addition, students will gain familiarity with production-quality infrastructure tools, including stream processing with Apache Kafka, test automation with Jenkins, monitoring with Prometheus and Grafana, and deployment with Docker and various MLOps tools.</p>
<h2>Logistics and People</h2>
<p>17-445/17-645/17-745, 12 Units</p>
<p>The course is the same under all course numbers, except for the PhD-level 17-745 number, which replaces two homework assignments with a mandatory <a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/research_project.md">research project</a>.</p>
<p>Open to all undergraduate and graduate students meeting the prerequisites.</p>
<h3>Spring 2025</h3>
<p>Lectures Monday/Wednesday 2:00-3:20pm, in person, PH 100</p>
<p>Labs Friday 9:30-10:50am in PH 226C (A) and SH 236 (B) and 11-12:20pm in PH A22 (C) and PH 226A (D) and 2-3:20 in PH 226C (E) and TEP 1308 (F). There is also a remote only lab (G), Friday 11:00-12:20 pm. </p>
<p>Instructors: <a href="https://www.cs.cmu.edu/~clegoues">Claire Le Goues</a> and <a href="https://austinhenley.com/">Austin Henley</a></p>
<p>TAs: </p>
<h3>Coordination</h3>
<p>We are happy to answer questions by email and over Slack, meet in person, and will jump on a quick Zoom call if you ask us. We also always arrive 5 to 10 min early to class and stay longer for discussions and questions. If you have questions about assignments and logistics, we prefer that you ask them publicly on Slack.</p>
<h2>Course content</h2>
<p>The general course content has been fairly stable over the last few years, though specific topics and tools are constantly updated with new research and tooling. Our list of learning goals under <a href="https://github.com/mlip-cmu/s2025/blob/main/learning_goals.md">Learning Goals</a> describes what we aim to cover. Below is a table of a preliminary schedule. This is subject to change and will be updated as the semester progresses, especially to help focus on requested topics or support learning.</p>
<p>[Schedule]</p>
<table>
<thead>
<tr>
<th>Date</th>
<th>Topic</th>
<th><a href="https://mlip-cmu.github.io/book/">Book Chapter</a></th>
<th>Reading</th>
<th>Assignment due</th>
</tr>
</thead>
<tbody><tr>
<td>Mon, Jan 13</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/01_introduction/intro.html">Introduction and Motivation </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/01_introduction/intro.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/01_introduction/intro.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/01/">1</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Jan 15</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/02_systems/systems.html">From Models to AI-Enabled Systems</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/02_systems/systems.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/02_systems/systems.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/02/">2</a>,<a href="https://mlip-cmu.github.io/book/04/">4</a>,<a href="https://mlip-cmu.github.io/book/05/">5</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 4, 5, 7, 8</td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 17</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab01.md">Calling, securing, and creating APIs: Flask</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Jan 20</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> MLK Day, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Jan 22</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/03_requirements/requirements.html">Gathering Requirements </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/03_requirements/requirements.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/03_requirements/requirements.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/06/">6</a></td>
<td><a href="https://scholar.google.com/scholar?cluster=1090758480873197042">The World and the Machine</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 24</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab02.md">Stream processing: Apache Kafka</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Jan 27</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/04_mistakes/mistakes.html">Planning for Mistakes</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/04_mistakes/mistakes.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/04_mistakes/mistakes.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/07/">7</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 6, 8, 24</td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I1_mlproduct.md">I1: ML Product</a></td>
</tr>
<tr>
<td>Wed, Jan 29</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/05_modelaccuracy/modelquality1.html">Model Quality</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/05_modelaccuracy/modelquality1.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/05_modelaccuracy/modelquality1.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/15/">15</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 19</td>
<td></td>
</tr>
<tr>
<td>Fri, Jan 31</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab03.md">Collaboration with git</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 3</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/06_teamwork/teams.html">Fostering Interdisciplinary (Student) Teams</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/06_teamwork/teams.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/06_teamwork/teams.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td><a href="https://third-bit.com/2018/05/11/meetings/">Meetings</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I2_requirements.md">I2: Requirements</a></td>
</tr>
<tr>
<td>Wed, Feb 5</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/07_modeltesting/modelquality2.html">Behavioral Model Testing</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/07_modeltesting/modelquality2.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/07_modeltesting/modelquality2.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/15/">15</a></td>
<td><a href="https://aclanthology.org/2020.acl-main.442.pdf">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 7</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab04.md">Model testing</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 10</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/08_architecture/tradeoffs.html">Toward Architecture and Design </a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/08_architecture/tradeoffs.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/08_architecture/tradeoffs.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/08/">8</a>,<a href="https://mlip-cmu.github.io/book/09/">9</a>,<a href="https://mlip-cmu.github.io/book/11/">11</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems, Ch. 18</a> and <a href="https://hackernoon.com/choosing-the-right-machine-learning-algorithm-68126944ce1f">Choosing the Right Machine Learning Algorithm</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Feb 12</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/09_deploying_a_model/deployment.html">Deploying a Model</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/09_deploying_a_model/deployment.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/09_deploying_a_model/deployment.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/10/">10</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 13 and <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/1feg4j8/alma991019735160604436">Machine Learning Design Patterns</a>, Pat. 16</td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 14</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab05.md">Containers: Docker</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 17</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/10_qainproduction/qainproduction.html">Testing and Experimenting in Production</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/10_qainproduction/qainproduction.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/10_qainproduction/qainproduction.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/19/">19</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Chs. 14 and 15</td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M1: Modeling and First Deployment</a></td>
</tr>
<tr>
<td>Wed, Feb 19</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/11_dataquality/dataquality.html">Data Quality</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/11_dataquality/dataquality.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/11_dataquality/dataquality.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/16/">16</a></td>
<td><a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445518">Data Cascades in High-Stakes AI</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 21</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab06.md">Continuous Integration: Jenkins</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Feb 24</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/12_pipelinequality/pipelinequality.html">Automating and Testing ML Pipelines</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/12_pipelinequality/pipelinequality.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/12_pipelinequality/pipelinequality.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/11/">11</a>,<a href="https://mlip-cmu.github.io/book/18/">18</a>,<a href="https://mlip-cmu.github.io/book/19/">19</a></td>
<td><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46555.pdf">The ML Test Score</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Feb 26</td>
<td><img src="https://img.shields.io/badge/-midterm-blue.svg" alt="Midterm"> Midterm 1</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Feb 28</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> No lab (happy spring break)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 3</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Mar 5</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 7</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Spring break, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 10</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/13_dataatscale/dataatscale.html">Scaling the System</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/13_dataatscale/dataatscale.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/13_dataatscale/dataatscale.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/12/">12</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019577936304436">Big Data: Principles and best practices of scalable realtime data systems</a>, Ch. 1</td>
<td></td>
</tr>
<tr>
<td>Wed, Mar 12</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/14_operations/operations.html">Planning for Operations</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/14_operations/operations.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/14_operations/operations.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/13/">13</a></td>
<td><a href="https://arxiv.org/abs/2209.09125">Operationalizing machine learning: An interview study</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 14</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab07.md">Monitoring: Prometheus, Grafana</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 17</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/15_provenance/provenance.html">Versioning, Provenance, and Reproducability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/15_provenance/provenance.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/15_provenance/provenance.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/24/">24</a></td>
<td><a href="http://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">Hidden Technical Debt in Machine Learning Systems</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M2: Infrastructure Quality</a></td>
</tr>
<tr>
<td>Wed, Mar 19</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/16_process/process.html">Process &amp; Technical Debt</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/16_process/process.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/16_process/process.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/20/">20</a></td>
<td><a href="https://arxiv.org/pdf/2110.10234.pdf">Collaboration Challenges in Building ML-Enabled Systems</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 21</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab08.md">Pipeline automation: MLFlow</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 24</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/17_intro_ethics_fairness/intro-ethics-fairness.html">Intro to Ethics + Fairness</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/17_intro_ethics_fairness/intro-ethics-fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/17_intro_ethics_fairness/intro-ethics-fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/23/">23</a>,<a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="https://datasociety.net/wp-content/uploads/2018/04/Data_Society_Algorithmic_Accountability_Primer_FINAL-4.pdf">Algorithmic Accountability: A Primer</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I3_mlops_tools.md">I3: MLOps Tools</a></td>
</tr>
<tr>
<td>Wed, Mar 26</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/18_fairness_measures/model_fairness.html">Measuring Fairness</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/18_fairness_measures/model_fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/18_fairness_measures/model_fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="https://dl.acm.org/doi/pdf/10.1145/3178876.3186138">Human Perceptions of Fairness in Algorithmic Decision Making</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Mar 28</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab09.md">Container orchestration: Kubernetis</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Mar 31</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/19_system_fairness/system_fairness.html">Building Fairer Systems</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/19_system_fairness/system_fairness.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/19_system_fairness/system_fairness.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/26/">26</a></td>
<td><a href="http://users.umiacs.umd.edu/~hal/docs/daume19fairness.pdf">Improving Fairness in Machine Learning Systems</a></td>
<td></td>
</tr>
<tr>
<td>Wed, Apr 2</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/20_explainability/explainability.html">AVAILABLE</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/20_explainability/explainability.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/20_explainability/explainability.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/25/">25</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 4</td>
<td><img src="https://img.shields.io/badge/-break-red.svg" alt="Break"> Carnival, no classes</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 7</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/21_transparency/transparency.html">Explainability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/21_transparency/transparency.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/21_transparency/transparency.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/29/">29</a></td>
<td><a href="https://dataskeptic.com/blog/episodes/2020/black-boxes-are-not-required">Interpretability Podcast</a> or <a href="https://arxiv.org/abs/1811.10154">equivalent artice</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M3: Monitoring and CD</a></td>
</tr>
<tr>
<td>Wed, Apr 9</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/22_security/security.html">Transparency &amp; Accountability</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/22_security/security.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/22_security/security.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/28/">28</a></td>
<td><a href="https://pair.withgoogle.com/chapter/explainability-trust/">Google chapter on Explainability and Trust</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 11</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab10.md">Model Explainability Tools</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 14</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/23_safety/safety.html">Security and Privacy</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/23_safety/safety.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/23_safety/safety.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/27/">27</a></td>
<td><a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">Building Intelligent Systems</a>, Ch. 25, and <a href="https://canvas.cmu.edu/courses/45008/files/12156923/download?wrap=1">The Top 10 Risks of Machine Learning Security</a></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I4_explainability.md">I4: Explainability</a></td>
</tr>
<tr>
<td>Wed, Apr 16</td>
<td><a href="https://mlip-cmu.github.io/s2025/slides/24_summary/all.html">Safety</a> (<a href="https://github.com/mlip-cmu/s2025/blob/main/lectures/24_summary/all.md">md</a>, <a href="https://mlip-cmu.github.io/s2025/slides/24_summary/all.pdf">pdf</a>)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td><a href="http://ceur-ws.org/Vol-2560/paper40.pdf">Practical Solutions for Machine Learning Safety in Autonomous Vehicles</a></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 18</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> <a href="https://github.com/mlip-cmu/s2025/blob/main/labs/lab11.md">LLM Jailbreaking</a></td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Mon, Apr 21</td>
<td>Explainability Discussion / Summary / Review</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Wed, Apr 23</td>
<td><img src="https://img.shields.io/badge/-midterm-blue.svg" alt="Midterm"> Midterm 2</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Fri, Apr 25</td>
<td><img src="https://img.shields.io/badge/-lab-yellow.svg" alt="Lab"> No lab</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">M4: Fairness, Security and Feedback Loops</a></td>
</tr>
<tr>
<td>TBD</td>
<td>Final Project Presentations (5:30-8:30pm in GHC 4401)</td>
<td><a href="https://mlip-cmu.github.io/book/00/"></a></td>
<td></td>
<td><a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/project.md">Final report</a></td>
</tr>
</tbody></table>
<h2>Course Syllabus and Policies</h2>
<p>The course uses Canvas and Gradescope for homework submission, grading, discussion, questions, announcements, and supplementary documents; slides will be posted here; Slack is used for communication around homework and projects; Github is used to coordinate group work. All public course material (assignments, slides, syllabus) can be found in the course’s <a href="https://github.com/mlip-cmu/s2025">GitHub repository</a>; announcements and all <em>private</em> material (e.g., grades, passwords) will be shared through Canvas.</p>
<p><strong>Prerequisites:</strong> The course does not have formal prerequisites, but we describe background knowledge that will help you be successful in the course. In a nutshell, we expect basic exposure to machine learning and basic programming skills, but do not require software engineering experience. </p>
<p><em>Machine learning (some experience recommended):</em> We suggest that you have basic familiarity with the process of extracting features, building and evaluating models, and a basic understanding of how and when different kinds of learning techniques work. Familiarity with Python and Jupyter notebooks is helpful. Courses such as 10-301, 10-315, and 05-434 will prepare you well, but project experience or self-learning from books or online courses will likely be sufficient for our purposes. For example, if you have no prior experience, we recommend the book <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019665684604436">Hands-On Machine Learning</a> to get practical experience in building and evaluating models prior to taking this course. We have set up a <em><a href="https://forms.gle/JcS61Uao7wHSFQen8">prerequisite knowledge check</a></em> as a Google Form, where we ask 10 questions on machine learning, which help you assess your background – this is set up as an anonymous and ungraded quiz, where you can compare your knowledge against what we believe is useful for you to be successful in this course (click on <em>“view score”</em> after submitting your answer). After submitting your answers, the system will give specific pointers to readings and exercises that may help you fill gaps in background knowledge. </p>
<p><em>Programming (basic proficiency required):</em> The course has a substantial programming component, especially in the first assignment and the team project, so basic programming skills will be needed. If you take the course without programming experience, you will significantly struggle and it may cause conflicts within the group project. We expect that you meet the following criteria: (1) basic fluency in a programming language like Python, (2) ability to install and learn libraries in that language, (3) ability to ssh into a Unix machine and perform basic command line operations, and (4) ability to install and learn new tools like Docker. We do not prescribe a programming language, but almost all student teams decide to work primarily in Python. We will provide some introductions and examples for essential tools like Git, Docker, Grafana, and Jenkins in labs, but we expect that you will be able to pick up new tools and libraries on your own. For example, we expect that you will be able, on your own, to learn basic use of a library like <a href="https://flask.palletsprojects.com/en/2.1.x/">Flask</a> to write a web service. Throughout the semester, expect to read lots of documentation and tutorials to learn various libraries and tools on your own. If you are worried whether your technical background is sufficient, we recommend that you look at (or even try) <a href="https://github.com/mlip-cmu/s2025/blob/main/assignments/I1_mlproduct.md">homework I1</a> before the semester.</p>
<p><em>Software engineering (no experience required):</em> Many students will have some software engineering experience beyond basic programming skills from software engineering courses, from internships, or from working in industry, for example experience with requirements engineering, software design, software testing, distributed systems, continuous deployment, or managing teams. No such experience is expected as a prerequisite; we will cover these topics in the course.</p>
<p>Email the instructors if you would like to further talk to us about prerequisites.</p>
<p><strong>In-person teaching and lecture recordings:</strong> The course will be taught in person.  We consider in-class participation an important part of the learning experience. We <em>do</em> make <em>best effort</em> lecture recordings, which will be available in Canvas.  We do <em>not</em> provide a synchronous remote option, and we do not record labs.  You are welcome to use recordings to make up missed lectures and review material. However, absent extenuating circumstances (see below), viewing the recording will not make up for missed in-class activities.  </p>
<p>We regularly use Slack for in-class activities. Please make sure that you have access to Slack on a laptop, tablet, or mobile phone during class.</p>
<p>If you cannot attend class due to a medical issue, family emergency, interview, or other unforeseeable reason, please contact us about possible accommodations. We try to be as flexible as we can, but will handle these cases individually.</p>
<p><strong>Exams:</strong> The course has two midterms and a final project presentation, but no final exam. We typically use the registrar-assigned final exam timeslot (to be announced about halfway through the semester <a href="https://www.cmu.edu/hub/docs/final-exams.pdf">here</a>) for the final project presentation. The midterms are during the normal class period as per schedule. The second midterm is not comprehensive, and only covers material after the first midterm. Examples of past midterms can be found in the <a href="https://github.com/mlip-cmu/s2025/tree/main/exams">course repository</a>.</p>
<p><strong>Grading:</strong> Evaluation will be based on the following distribution: 35% individual assignments, 30% group project, 15% midterms, 5% participation, 10% labs, 5% reading quizzes. No final exam.</p>
<p>We strive to provide clear specifications and clear point breakdowns for all homework to set clear expectations and take the guessing out of homework. We often give you choices to self-direct your learning, deciding what to work on and how to address a problem (e.g., we never prescribe a programming language and often give choices to answer a subset of possible questions). Clear specifications and point breakdowns allow you to intentionally decide to skip parts of assignments with clear upfront consequences. All parts will be graded pass/fail, no partial credit. For opportunities to redo work, see <em>resubmissions</em> below. For grading participation and quizzes see below. Some assignments have a small amount of bonus points. </p>
<p>Since we give flexibility to resubmit assignments, we set grade boundaries fairly high. We expect the following grade boundaries:</p>
<table>
<thead>
<tr>
<th>Grade</th>
<th>Cutoff</th>
</tr>
</thead>
<tbody><tr>
<td>A+</td>
<td>&gt;99%</td>
</tr>
<tr>
<td>A</td>
<td>&gt;96%</td>
</tr>
<tr>
<td>A-</td>
<td>&gt;94%</td>
</tr>
<tr>
<td>B+</td>
<td>&gt;91%</td>
</tr>
<tr>
<td>B</td>
<td>&gt;86%</td>
</tr>
<tr>
<td>B-</td>
<td>&gt;82%</td>
</tr>
<tr>
<td>C</td>
<td>&gt;75%</td>
</tr>
<tr>
<td>D</td>
<td>&gt;60%</td>
</tr>
</tbody></table>
<p><strong>Participation:</strong> Design and engineering content requires active engagement with the material and discussions of judgment decisions on specific scenarios and cases. We strongly believe in in-class discussions and in-class exercises and want all students to participate, e.g., answering or asking questions in class, sharing own experiences, presenting results, or participating in in-class votes and surveys. We will give many opportunities for participation in every lecture and lab. We note student engagement with in-class activities to include as a component in grading.  We will provide feedback at mid-semester so that you can check in on how you’re doing. Again, please talk to us if you need accommodations.</p>
<p>We assign participation grades as follows:</p>
<ul>
<li>100%: Participates actively at least once in most lectures (4 lectures waived, no questions asked)</li>
<li>90%: Participates actively at least once in two thirds of the lectures</li>
<li>75%: Participates actively at least once in over half of the lectures</li>
<li>50%: Participates actively at least once in one quarter of the lectures</li>
<li>20%: Participates actively at least once in at least 3 lectures.</li>
<li>0%: Participation in less than 3 lectures.</li>
</ul>
<p><strong>Labs:</strong> Labs typically introduce tools and have a task with one or more clear deliverables. Lab assignments are designed to take about 1h of work and can be completed before or during the lab session. Each deliverable is graded pass/fail at any time during that week's lab session by showing your work to the TA. Typically showing your work involves showing source code, demoing executions, and (verbally) answering a few questions. The TA may ask a few questions about your implementation to probe that you understand your work.</p>
<p>We intend labs to be very low stakes – this is your first practical engagement with the material and mistakes are a normal part of the learning process. Deliverables are graded pass/fail on whether they meet the stated expectations for the deliverables. If your solution does not meet the expectations you can continue working on it during the lab session until it does. Outside of explicit accommodations (e.g., medical issues) or using tokens (see below), we do not accept lab solutions after the end of the lab session.</p>
<p>We encourage collaboration on labs: You can work together with other students both before the lab session and during the lab session. While we do not recommend it, you may look at other students’ solutions and reference solutions and even copy them. However, you will have to present and explain your solution to the TA on your own.</p>
<p><strong>Textbook, reading assignments, and reading quizzes:</strong> We will be using Goeff Hulten's <em>"Building Intelligent Systems: A Guide to Machine Learning Engineering"</em> (ISBN: 1484234316) throughout much of the course. The library provides an <a href="https://cmu.primo.exlibrisgroup.com/permalink/01CMU_INST/6lpsnm/alma991019649190004436">electronic copy</a>. In addition, we will provide various additional readings, including blog posts and academic papers, throughout the semester.</p>
<p>We also wrote our own textbook "<a href="https://mlip-cmu.github.io/book/">Machine Learning in Production</a>" that aligns closely with the lecture content. The book will be published by MIT Press and is additionally available under a creative commons license online. We will not assign chapters from our own textbook, but we always point to the corresponding chapter for each lecture, which we suggest as supplementary reading.</p>
<p>We will assign readings for most classes and post a corresponding quiz on Canvas that is due before class. Each quiz contains an open-ended question that relates to the reading. Reading quizzes are intended to be low-stakes assessments and are graded pass/fail for a good-faith effort to engage with the question. </p>
<p><strong>Teamwork:</strong> Teamwork is an essential part of this course. The course contains a multi-milestone group project to be done in teams of 3-5 students. Teams will be assigned by the instructor. A TA will serve as a mentor for each team. We will help teams throughout the semester and cover some specific content on teamwork as part of the course. Peer rating will be performed for team assignments with regard to <em>team citizenship</em> (i.e., being active and cooperative members), following a procedure adapted from <a href="https://www.cs.tufts.edu/~nr/cs257/archive/teaching/barbara-oakley/JSCL-collaboration.pdf">this article</a>, which we will further explain in an early lecture. Use <a href="https://mlip-cmu.github.io/s2025/assignments/peergrading.html">this form</a> to preview the expected adjustments for peer ratings. The team's mentor will also debrief with the team after every milestone and discuss possible strategies to improve teamwork. </p>
<p><strong>Late work policy and resubmissions:</strong> We understand that students will always have competing deadlines, unusual events, interviews for job searches, and other activities that compete with coursework. We therefore build flexibility and a safety net directly into the rubric. If you need additional accommodations, please contact us.</p>
<p>In addition, we expect that the past/fail grading scheme without partial credit, may lead to harsh point deductions for missing small parts of the requirements, so we provide a mechanism to resubmit work with a short reflection to regain lost points.</p>
<p>Every student receives <em>8 individual tokens</em> that they can spend throughout the semester in the following ways:</p>
<ul>
<li>For each token, a student can submit a homework assignment 1 day late (with 2 tokens a student can submit two homeworks one day late each or a single homework up to two days late).</li>
<li>For <em>three</em> tokens, a student can improve or redo an individual homework assignment and resubmit together with a short reflection. The earlier submission is discarded and the regraded assignment counts toward the final grade. Resubmissions can be made at any time in the semester up to the final project presentation (see schedule). – Note that this technically allows a student to blow the original deadline (no submission necessary, receiving 0 points initially) and then resubmit the homework arbitrarily late for three tokens.</li>
<li>For one token, a student can submit a reading quiz late (any time before the final presentation) or resubmit a graded reading quiz.</li>
<li>For one token, a student can complete a lab late or redo a lab (any time before the final presentation) by showing the work to a TA during office hours.</li>
<li>Remaining individual tokens at the end of the semester are counted as one participation day each.</li>
</ul>
<p>If a student runs out of tokens, late individual assignments receive a penalty of 15% per started day. Late team formation survey and teamwork peer assessment surveys do not receive any points.</p>
<p>Every team independently receives <em>8 team tokens</em> that they can spend for extensions of any milestone deadline (1 token per day per milestone, except final presentation deadline) or to resubmit any milestone with a reflection (3 tokens each, resubmitted any time before the final presentation). If a team runs out of tokens, late submissions in group assignments receive a penalty of 15% per started day.</p>
<p>Individual tokens and team tokens are entirely separate; it is not possible to use individual tokens for teamwork or vice versa. The team should make collective decisions about how to use team tokens.</p>
<p>In general, late submissions and resubmissions can be done at any point in the semester before the final presentations. Late submissions that are 1-3 days late can be made directly to Gradescope; for everything else see instructions and forms on Canvas.</p>
<p>Exceptions to this policy will be made at the discretion of the instructor in important circumstances, almost always involving a family or medical emergency and an email from your advisor — you can ask your academic advisor or the Dean of Student Affairs requesting the exception on your behalf. Where issues affect teamwork, please communicate proactively with your team.</p>
<p><strong>Communication:</strong> We make important announcements on Slack; we recommend to enable Slack notifications. We answer email and monitor Slack, which may all be used for clarifying homework assignments and other interactions. We strongly recommend to ask questions publicly on Slack if others might have similar questions. Email or slack us if you would like to make an appointment.</p>
<p><strong>Auditing:</strong> Due to the high demand for this course, we do <em>not</em> allow auditing. If you like to self-study, all course materials are online. We welcome interested students and visitors to sit in for lectures as long as the room capacity allows it. </p>
<p><strong>Time management:</strong> This is a 12-unit course, and it is our intention to manage it so that you spend close to 12 hours a week on the course, on average. In general, 3 hours/week will be spent in class, about 1 hour for the labs, 1-2 hours on readings and reading quizzes, and 6-7 hours on assignments. Notice that much homework is done in groups, so please account for the overhead and decreased time flexibility that comes with groupwork. Please give the course staff feedback if the time the course is taking for you differs significantly from our intention.</p>
<p><strong>Writing:</strong> Describing tradeoffs among decisions and communication with stakeholders from other backgrounds are key aspects of this class. Many homework assignments have a component that requires discussing issues in written form or reflecting about experiences. To practice writing skills, the Global Communications Center (GCC) offers one-on-one help for students, along with workshops. The instructors are also happy to provide additional guidance if requested.</p>
<p><strong>Use of content generation AI tools and external sources:</strong> Given the nature of this course, we are open to using AI tools for completing work. We place no restrictions on the use of content generation tools, such as ChatGPT, Bard, Co-Pilot, or Stable Diffusion. You may also reuse code from external sources, such as StackOverflow or tutorials. In any case, you will be solely responsible for the correctness of the solution. Note that content generation tools often create plausible-looking but incorrect answers, which will not receive credit. You are also responsible for complying with any applicable licenses. If you use content generation tools, we encourage you to share your experience with the course staff or the entire class.</p>
<p><strong>Academic honesty and collaboration:</strong> The usual policies apply, especially the <a href="https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html">University Policy on Academic Integrity</a>. Many parts of the work will be done in groups. We expect that group members collaborate with one another, but that groups work independently from other groups, not exchanging results with other groups. Within groups, we expect that you are honest about your contribution to the group's work. This implies not taking credit for others' work and not covering for team members that have not contributed to the team. This also applies to in-class discussions, where indicating working with others who did not participate in the discussion is considered an academic honesty violation. Otherwise, our expectations regarding academic honestly and collaboration for group and pair work are the same as for individual work, substituting elevated to the level of "group."</p>
<p>Beyond that, the key guiding principle of academic honesty in this course is: <em>"You may not copy any part of a solution to a problem that was written by another student (in this or prior iterations of the class), or was developed together with another student, or was delegated to another person. You may not look at another student's solution, even if you have completed your own, nor may you knowingly give your solution to another student or leave your solution where another student can see it.</em>" Note that this implies that you cannot publicly post your solutions on GitHub (e.g., as part of a portfolio during job applications). While the use of AI content generation tools is okay (see above) using the work from other students is not. Discussing challenges and solution strategies with others at a high level is okay, sharing code or text is not.</p>
<p>You may collaborate with other students on labs, but not on reading quizzes, homeworks, and exams.</p>
<p>We also expect and respect honesty when communicating with the course staff.</p>
<p>Any violation of this policy is cheating. The minimum penalty for cheating will be a zero grade for the whole assignment. Cheating incidents will also be reported through University channels, with possible additional disciplinary action (see the University Policy on Academic Integrity). There is no statute of limitations for violations of the collaboration policy; penalties may be assessed (and referred to the university disciplinary board) after you have completed the course, and some requirements of the collaboration policy (such as restrictions on you posting your solutions) extend beyond your completion of the course.</p>
<p>If you have any question about how this policy applies in a particular situation, ask the instructors for clarification.</p>
<p><strong>Research in this Course:</strong> We are conducting academic research in this course. This research will involve analyzing student work of assignment. You will not be asked to do anything above and beyond the normal learning activities and assignments that are part of this course. You are free not to participate in this research, and your participation will have no influence on your grade for this course or your academic career at CMU. If you do not wish to participate, please send an email to Nadia Nahar (<a href="mailto:nadian@andrew.cmu.edu">nadian@andrew.cmu.edu</a>). Participants will not receive any compensation or extra credit. The data collected as part of this research will not include student grades. All analyses of data from participants’ coursework will be conducted after the course is over and final grades are submitted -- instructors will not know who chooses not to participate before final grades are submitted. All data will be analyzed in de-identified form and presented in the aggregate, without any personal identifiers. If you have questions pertaining to your rights as a research participant, or to report concerns to this study, please contact Nadia Nahar (<a href="mailto:nadian@andrew.cmu.edu">nadian@andrew.cmu.edu</a>) or the Office of Research Integrity and Compliance at Carnegie Mellon University (<a href="mailto:irb-review@andrew.cmu.edu">irb-review@andrew.cmu.edu</a>; phone: 412-268-4721).</p>
<p><strong>Accommodations for students with disabilities:</strong> If you have a disability with an accommodations letter from the Disability Resources office, we encourage you to discuss your accommodations and needs with us as early in the semester as possible. We will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, we encourage you to contact them at <a href="mailto:access@andrew.cmu.edu">access@andrew.cmu.edu</a>.</p>
<p><strong>Respect for diversity:</strong> It is our intent that students from all diverse backgrounds and perspectives be well served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that students bring to this class be viewed as a resource, strength and benefit. It is my intent to present materials and activities that are respectful of diversity: gender, sexuality, disability, age, socioeconomic status, ethnicity, race, and culture. Especially in lectures on fairness we will also cover diversity discussions, typically through a lens of the contemporary discourse in the US. Your suggestions are encouraged and appreciated. Please let us know ways to improve the effectiveness of the course for you personally or for other students or student groups. </p>
<p><strong>A note on self care.</strong> Please take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. All of us benefit from support during times of struggle. You are not alone. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is often helpful.
If you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 and visit their website at <a href="http://www.cmu.edu/counseling/">http://www.cmu.edu/counseling/</a>. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help.</p>

        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why OpenAI's $157B valuation misreads AI's future (Oct 2024) (135 pts)]]></title>
            <link>https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/</link>
            <guid>42847825</guid>
            <pubDate>Tue, 28 Jan 2025 01:17:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/">https://foundationcapital.com/why-openais-157b-valuation-misreads-ais-future/</a>, See on <a href="https://news.ycombinator.com/item?id=42847825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<figure><img fetchpriority="high" decoding="async" width="1024" height="576" src="https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1024x576.jpg" alt="" srcset="https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1024x576.jpg 1024w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-300x169.jpg 300w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-768x432.jpg 768w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-1536x864.jpg 1536w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-375x211.jpg 375w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1-323x182.jpg 323w, https://foundationcapital.com/wp-content/uploads/2024/10/FC-Website-16x9-1920x1080-1.jpg 1921w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>







<p><em>I break down the logic behind the company’s towering price tag and why I think the most valuable AI companies have yet to be built.</em></p>



<p>When <a target="_blank" rel="noreferrer noopener" href="https://finance.yahoo.com/news/openai-closed-funding-round-raising-161842157.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAKFtOVtzprQmt1Og2sCCvZcWat-nslChIXPcvptCstCn0VSkVmkJhLNkMG81WlL0UJ5usiDd_A3A1TlLylFLfmyMmR3XTInJnRHlMkYh5_xXipGOGrWDZK_V5B63fSmIlcw4wbPuxnewKZc8DCEjwjVFHwA-Pj7dQWC7d39RGOKF">OpenAI raised $6.6B</a> earlier this month—the second-largest private funding round in history, topped only by its own $10B raise from Microsoft last January—it wasn’t just setting records. It was making an argument about where AI will create value and who stands to capture it.</p>



<p>Understanding this argument, both its logic and its limitations, sheds light on where I believe the most promising opportunities in AI are likely to emerge.</p>



<h2><strong>The bull case</strong></h2>



<p>OpenAI’s growth has been nothing short of meteoric. Monthly revenue reached <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html">$300M</a> in August 2023, a 1,700% increase from January. 10M users pay $20/month for ChatGPT, and the company projects $11.6B in revenue next year.&nbsp;</p>



<p>This growth, which outpaces even the early days of Google and Facebook, underpins the bull case by <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=ePfNAKopT20">investors like Brad Gerstner</a>. In his view, calling OpenAI a “model company” is like calling Google a “search algorithm company”—it fundamentally misunderstands the scale of the opportunity. ChatGPT isn’t just another tech product; it’s a fundamental advance in how humans interact with computers, “a hundred times better than the card catalog known as 10 blue links that Google built 25 years ago.”&nbsp;</p>



<p>Google did indeed transcend its origins in search, but only by first mastering its core business. Today, OpenAI is attempting to be a research lab, a developer platform, an enterprise solution, and a consumer product company all at once. Its investors are betting that AI is so transformative that the usual rules of focus and specialization don’t apply. The first company to achieve AGI will win everything, so the optimal strategy is to pursue every advantage simultaneously.</p>



<h2><strong>The bear case</strong></h2>



<h3><strong>💸 Bad economics</strong></h3>



<p>This narrative collides with a stubborn reality: the economics of AI don’t work like traditional software. OpenAI is currently valued at <a target="_blank" rel="noreferrer noopener" href="https://www.linkedin.com/posts/jonmcneill1_157-billion-thats-the-latest-valuation-activity-7252026319608131585-Z-Us?utm_source=share&amp;utm_medium=member_desktop">13.5x forward revenue</a>—similar to what Facebook commanded at its IPO. But while Facebook’s costs decreased as it scaled, OpenAI’s costs are growing in lockstep with its revenue, and sometimes faster.</p>



<p>In traditional software, increasing scale leads to improving economics. A typical software company might spend heavily on development upfront, but each additional user costs almost nothing to serve. Fixed costs are spread across a growing revenue base, creating the enviable margins that make today’s tech giants among the most profitable businesses in history.&nbsp;</p>



<p>Generative AI plays by different rules. Each query to a model costs money in compute resources, while each new model requires massive investments in training. OpenAI expects to lose $5B this year on $3.7B in revenue. Their projected losses through 2028 <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/openai-projections-imply-losses-tripling-to-14-billion-in-2026?rc=e43qsi">amount to $44B</a>, excluding stock compensation. Computing costs alone will reach $6B this year.</p>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.newcomer.co/p/the-bear-case-for-openai-at-157-billion">Google’s 2004 IPO</a> followed years of profitable operation, with $106M in profit on $962M in revenue. Facebook went public after achieving $1B in profit on $3.7B in revenue. Both companies demonstrated that growth improved their profit margins. OpenAI, by contrast, plans to 100x revenue to $100B by 2029 while piling up progressively larger losses. This requires maintaining <a target="_blank" rel="noreferrer noopener" href="https://www.marketwatch.com/story/openai-making-money-from-the-ai-revolution-is-far-from-certain-32249d5a">93% annual growth</a> for five years—a rate achieved by only a handful of companies in history, all of which saw their economics improve with scale.</p>



<p>The infrastructure needs to sustain AI’s progress are staggering. Microsoft, OpenAI’s primary partner, plans to spend $80-110B on AI infrastructure next year alone. According to semiconductor analyst <a target="_blank" rel="noreferrer noopener" href="https://www.dwarkeshpatel.com/p/dylan-jon?open=false#%C2%A7are-we-financing-an-ai-bubble">Dylan Patel</a>, Microsoft is building computing clusters with 100,000 GPUs and aims to construct a single facility consuming one gigawatt of power by 2026. By 2028, their computing requirements could reach multiple gigawatts—equivalent to an <a target="_blank" rel="noreferrer noopener" href="https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions">entire country’s</a> electricity demand.&nbsp;</p>



<p>To put these numbers in context: At the height of the dot-com bubble, the entire internet economy generated <a target="_blank" rel="noreferrer noopener" href="https://www.marketwatch.com/story/openai-making-money-from-the-ai-revolution-is-far-from-certain-32249d5a">$1.5T in revenue</a> (adjusted to 2024 dollars). Today, generative AI companies produce less than $10B in revenue while planning infrastructure investments that <a target="_blank" rel="noreferrer noopener" href="https://www.goldmansachs.com/insights/articles/will-the-1-trillion-of-generative-ai-investment-pay-off">could exceed $1T</a>.</p>



<h3><strong>🚫 No technical moat</strong></h3>



<p>OpenAI’s challenges extend beyond economics. Its leadership team has seen <a target="_blank" rel="noreferrer noopener" href="https://www.theinformation.com/articles/behind-openais-staff-churn-turf-wars-burnout-compensation-demands?rc=e43qsi">near total turnover</a> over the past year. Eight of its eleven co-founders, including CTO Mira Murati and chief scientist Ilya Sutskever (who are launching competing ventures), have left. CEO Sam Altman has responded by recruiting experienced executives, like Sarah Friar as CFO and Kevin Weil as head of product. But when nearly all the talent that built your breakthrough technology goes elsewhere, it’s worth asking why.</p>



<p>What’s more, OpenAI’s massive infrastructure investments might not be building a moat—they might just be the cost of staying in the race. As board chair <a target="_blank" rel="noreferrer noopener" href="https://www.youtube.com/watch?v=vRhPc0zt2IE">Bret Taylor</a> admits, we’re watching “the fastest technology commoditization cycle we’ve ever seen.” GPT-4’s pricing per token has <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">plummeted 98%</a> since last year’s dev day. The gap between their SOTA models and open-source alternatives is narrowing with a speed that should make any investor nervous.</p>



<h3><strong>🌍 Distribution and openness matter</strong></h3>



<p>This brings me to what might be the most important dynamics in AI today: distribution and openness. In tech, the winners aren’t always those with the most advanced technology—they’re often those who build the most compelling ecosystems.</p>



<p>In a <a target="_blank" rel="noreferrer noopener" href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/">recent memo</a>, Zuckerberg draws a parallel to the early days of high-performance computing, when major tech companies poured resources into proprietary Unix systems. At the time, few imagined that open-source alternatives could win. Yet Linux ultimately prevailed—not because it was better from the start, but because it allowed developers to modify the code freely, run it more securely and affordably, and build a broader ecosystem that enabled more capabilities than any closed system.</p>



<p>Meta is betting AI will follow a similar path. While Llama 2 could only match older, closed models, Llama 3 has reached parity with the frontier. Their <a target="_blank" rel="noreferrer noopener" href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3.1 405B</a> model can reportedly run at roughly half the cost of GPT-4, while giving enterprises something potentially more valuable than raw performance: complete control over their data and freedom from vendor lock-in.&nbsp;</p>



<p>Meanwhile, Meta’s consumer distribution is unmatched. LLaMA 3 currently powers AI features across Facebook, Instagram, and WhatsApp, reaching 1.1B users in 14 countries—and they’re only a third through their rollout. While ChatGPT has reached <a target="_blank" rel="noreferrer noopener" href="https://www.pewresearch.org/short-reads/2024/03/26/americans-use-of-chatgpt-is-ticking-up-but-few-trust-its-election-information/">23% of adults in the U.S.</a>, Meta is deploying its AI to billions of users globally. LLaMA 3 might not match GPT-4 in every research benchmark, but it doesn’t need to. Meta has optimized for what most users want: quick, reliable responses on mobile devices, especially in emerging markets where simpler queries dominate.</p>



<p>This creates brutal economics for OpenAI. While they reportedly plan to <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html">raise ChatGPT’s subscription price</a> to $44/month over the next five years, Meta can give away their AI for free. As <a target="_blank" rel="noreferrer noopener" href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/">Zuckerberg notes</a>: “Selling access to AI models isn’t our business model. Openly releasing Llama doesn’t undercut our revenue or ability to invest in research like it does for closed providers.”</p>



<p>Other factors in Meta’s favor: it’s unencumbered by the legacy issues slowing down competitors like Google, whose search-based advertising business faces an existential threat from generative AI. It also has a founder-CEO willing to prioritize long-term market dominance over short-term profits.</p>



<h3><strong>⚓ The Microsoft dilemma</strong></h3>



<p><a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2024/10/17/technology/microsoft-openai-partnership-deal.html">OpenAI’s relationship with Microsoft</a> introduces another layer of complexity. What started as a lifeline—providing essential capital and computing resources—now risks becoming a constraint. Microsoft receives 75% of OpenAI’s profits until its $13B investment is recouped, followed by 49% until it hits $92B.</p>



<p>Their partnership shows growing signs of strain. Microsoft’s $650M acquisition of Inflection AI’s team looks less like opportunistic talent acquisition and more like a hedge against overreliance on OpenAI. Meanwhile, OpenAI’s $10B computing contract with Oracle (while structured through Microsoft to maintain exclusivity agreements) suggests a push for independence from Microsoft’s infrastructure.</p>



<p>Adding to the tensions is <a target="_blank" rel="noreferrer noopener" href="https://www.wsj.com/tech/ai/the-14-billion-question-dividing-openai-and-microsoft-71cf7d37">OpenAI’s pending change</a> from a nonprofit to a for-profit entity—one of the most controversial corporate restructurings in recent history. Both companies have engaged investment banks to negotiate Microsoft’s future equity position. OpenAI needs to complete this conversion within two years or risk investors from the latest round demanding their money back. Any major changes will also need approval from the FTC, which has been less than friendly of late to big tech.</p>



<h2><strong>Where value in AI will accrue</strong></h2>



<p>So where will the most promising opportunities in AI for investors and startups lie? Analyzing the AI ecosystem layer by layer reveals where the most durable value is likely to emerge.</p>



<h3><strong>🏗️ Physical and cloud infrastructure</strong></h3>



<p>At the bottom of the AI stack is hardware: vast arrays of GPUs, specialized processors, networking equipment, and massive data centers. Hyperscalers are investing billions here—not for immediate returns, but for strategic control. Each is pursuing vertical integration, developing proprietary silicon to reduce reliance on suppliers like NVIDIA, which is also <a target="_blank" rel="noreferrer noopener" href="https://venturebeat.com/ai/nvidia-just-dropped-a-bombshell-its-new-ai-model-is-open-massive-and-ready-to-rival-gpt-4/">moving up the stack</a> to compete in models. Several AI chip startups are also competing at this layer.</p>



<h3><strong>🧬 Foundation models</strong></h3>



<p>This layer seems exciting until you look at the economics. Building and improving foundation models requires massive ongoing investment, yet their value erodes faster than perhaps any other technology to date. Hyperscalers can absorb these costs by using models to drive demand for their cloud services, but independent startups face a steeper climb. The future of general-purpose models increasingly resembles a race to the bottom.&nbsp;</p>



<p>Still, there may be a handful of opportunities for model startups to carve out niches, whether by leveraging proprietary data or developing new architectures like state-space models. We have a few stealth investments in this category.</p>



<h3><strong>🛠️ Software infrastructure and developer tools</strong></h3>



<p>The next layer up includes companies that help developers build AI-powered software. Startups like Anyscale, Arize, Skyflow, and Turing are players here.</p>



<p>The cloud era saw <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">about 15 companies</a> scale to $1B+ revenue at this layer. I expect a similar pattern to play out in AI, with a few dozen startups building valuable franchises to serve AI developers. Our current investments align with this view, and we’re particularly excited about tooling to develop agentic systems.&nbsp;</p>



<p>At the same time, this category is challenging due to the rapid pace of innovation, competition from open source, and bundling by both hyperscalers and proprietary model providers. As one example, vector databases like Pinecone were highly sought after just a year ago, but their appeal has since waned.</p>



<h3><strong>🤖 AI Applications</strong></h3>



<p>The top of the stack is where I see the most promise. AI is not just adding features to existing software; it’s transforming entire service industries into software products. This shift expands the addressable market from the $350B software sector to the multi-trillion dollar services industry.</p>



<p>While OpenAI has focused on building general-purpose models, a new wave of specialized startups is addressing specific industry needs with precision. The early dismissal of these companies as “GPT wrappers”—basic interfaces layered over foundation models—now feels outdated. We’re seeing the rise of <a target="_blank" rel="noreferrer noopener" href="https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/">compound AI systems</a> that combine multiple AI models with retrieval mechanisms, external tools, and diverse data sources, orchestrated by advanced control logic.</p>



<p>History suggests this layer will produce the most winners. The cloud era created <a target="_blank" rel="noreferrer noopener" href="https://www.sequoiacap.com/article/generative-ais-act-o1/">over 20 application companies with $1B+ revenue</a>. In AI, we believe this number could exceed 100. These new companies will redefine how industries operate and potentially replace legacy systems of record like Salesforce and SAP.</p>



<h2>Where I land</h2>



<p>New technologies, no matter how revolutionary, don’t automatically translate into sustainable businesses. OpenAI’s $157B valuation suggests we might be forgetting this lesson.</p>



<p>This isn’t to diminish what OpenAI has achieved. They’ve shown us that AI can do things many thought impossible just a few years ago. They’ve forced enterprises to rethink how they operate and changed how humans interact with computers. But starting a revolution isn’t the same as profiting from it. Today’s headline-grabbing AI companies are creating tremendous value, but that doesn’t guarantee they’ll be the ones to capture it in the long run.</p>



<div><p>I’d argue that the most valuable companies of the AI era don’t exist yet. They’ll be the startups that harness AI’s potential to solve specific, costly problems across our economy—from engineering and finance to healthcare, logistics, legal, marketing, sales, and more.</p><p><em>For more, I also publish to <a href="https://ashugarg.substack.com/">Substack</a>, along with more frequent updates on <a href="https://www.linkedin.com/in/ashugargvc">LinkedIn</a> and <a href="https://x.com/ashugarg?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">X</a> where I share perspectives on enterprise software, AI, and the technical founder’s journey.</em></p></div>
</div></div>]]></description>
        </item>
    </channel>
</rss>