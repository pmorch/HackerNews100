<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 05 Aug 2025 15:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[AI is not Making Engineers 10x as Productive (198 pts)]]></title>
            <link>https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/</link>
            <guid>44798189</guid>
            <pubDate>Tue, 05 Aug 2025 14:10:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/">https://colton.dev/blog/curing-your-ai-10x-engineer-imposter-syndrome/</a>, See on <a href="https://news.ycombinator.com/item?id=44798189">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
			<heading-anchors>
				



<p>Curing Your AI 10x Engineer Imposter Syndrome</p>

<ul>
	<li><time datetime="2025-08-05">05 August 2025</time></li>
	<li><a href="https://colton.dev/tags/ai/">AI</a></li>
</ul>

<p>A few months ago I went through a bit of a mental slump. I've always been confident of my abilities as an engineer, but I couldn't help but feel like my skills were falling hopelessly behind as I scrolled places like LinkedIn and Twitter. If these sources were to be believed, engineering had moved on from the medieval practice of typing code into an editor. <em>Real</em> engineers were now 10-100x more productive than I was. I'm writing this hoping to help others who are feeling similar anxieties.</p>
<p>I'm a skeptical person so I don't usually fall over myself immediately when I hear a claim like that. I usually roll my eyes in the same way I do when someone tells me a simple herbal remedy cures all disease. But the sheer volume these 10x engineer claims are reaching right now started to hit a nerve. What if I'm <em>wrong</em>? Will I miss the bus and become unemployable if I don't learn to use AI right now? After all, there are a lot of fancy words going around that distance the "AI" these people are talking about with the "AI" I was familiar with.</p>
<p>These people were using <em>✨agentic✨</em> AI. They were using <em>✨thinking✨</em> models that surfed the internet, ran tests, and corrected their own mistakes. Sure I popped into a chat window here and there and asked it to write some code, then promptly discarded most of the output once I got the idea that I needed. But these engineers were letting Claude fully take the wheel and had agents ripping 5 PRs for them while they made morning coffee. Was I becoming a dinosaur, an old man yelling at cloud?</p>
<p>Part of what made me feel so anxious was that it was entirely possible AI changed without me knowing it because I didn't use AI very much. Because I didn't <em>like</em> using AI that much. Reviewing code is vastly less enjoyable process than writing it. Had my stubborn desire to <em>enjoy coding</em> set me up to be left behind?</p>
<h2 id="diving-in">Diving In</h2>
<p>Eventually I hit a breaking point and decided I simply had to dive in head first to AI coding. I tried Claude Code, Cursor, Roo Code, and Zed for their agentic coding promises. I started asking AI to write all sorts of code in all sorts of projects. I tried the different models and compared them. I even vibe coded a few things, not editing the code manually once.</p>
<p>And it was... Fine. Despite claims that AI today is improving at a fever pitch, it felt largely the same as before. It's good at writing boilerplate, especially in Javascript, and particularly in React. It's not good at keeping up with the standards and utilities of your codebase. It tends to struggle with languages like Terraform. It still hallucinates libraries leading to significant <a href="https://en.wikipedia.org/wiki/Slopsquatting">security vulnerabilities</a>.</p>
<p>AIs still struggle to absorb the context of a larger codebase, even with a great prompt and <code>CLAUDE.md</code> file. If you use a library that isn't StackOverflow's favorite it will butcher it even after an agentic lookup of the documentation. Agents occasionally do something neat like fix the tests they broke. Often they just waste time and tokens, going back and forth with themselves not seeming to gain any deeper knowledge each time they fail. Thus, AI's best use case for me remains writing one-off scripts. Especially when I have no interest in learning deeper fundamentals for a single script, like when writing a custom ESLint rule.</p>
<p>Dark warnings that if I didn't start using AI now I'd be hopelessly behind proved unfounded. Using AI to code is not hard to learn. Obviously? Well, the AI coding community seems split on whether AI makes coding so easy a caveman can do it and that it requires an advanced, dedicated prompt engineer skillset. There are a few things you need to learn but they come quickly. You learn how to split up tasks into smaller pieces so the AI doesn't lose its mind late in the context window. Tools like Claude Code can do a bit of this themselves, even, though not always reliably. And you learn to identify when the AI is too far off and it's time to take the wheel.</p>
<p>A competent engineer will figure this stuff out in less than a week of moderate AI usage. Further, if AI is about to get 2x, 10x, or 100x better at any minute (as everyone keeps saying it will), then any lessons about how to use it now are moot for the future.</p>
<p>Every time I encountered AI working "just okay", it strangely made me more anxious, not less. It meant I couldn't find the spicy secret sauce that made everyone else so productive. I just didn't have what it takes: dinosaur, meet asteroid, thy name is AI. Eventually, a few things shook me out of this slump. One of those was <a href="https://ludic.mataroa.blog/blog/contra-ptaceks-terrible-article-on-ai/">this article</a> from Ludicity, directly countering the claims of the AI pumpers. I write this article to share more things that helped me get out of the AI 10x engineer imposter syndrome.</p>
<h2 id="the-math">The Math</h2>
<p>Let's start by looking at the simple math of 10-100x productivity. 10x productivity means ten times the outcomes, not ten times the lines of code. This means what you used to ship in a quarter you now ship in a week and a half. These numbers should make even the truest AI believer pause. The amount of product ideation, story point negotiation, bugfixing, code review, waiting for deployments, testing, and QA in that go into what was traditionally 3 months of work is now getting done in 7 work days? For that to happen each and every one of these bottlenecks has to also seen have 10x productivity gains.</p>
<p>Any software engineer who has worked on actual code in an actual company knows this isn't possible. You can't compress the back and forth of 3 months of code review into 1.5 weeks. When you code review you:</p>
<ol>
<li>Tag your reviewer</li>
<li>Hope they will get to it sooner rather than later (which will be tough because they are apparently code reviewing 10x as much code as before)</li>
<li>Context switch to something else while you wait</li>
<li>See a notification (perhaps immediately, perhaps 2 hours after your reviewer went offline for the day)</li>
<li>Context switch back to the review</li>
<li>Read their comments</li>
<li>Respond accordingly</li>
<li>Rinse and repeat.</li>
</ol>
<p>This process can be made fairly efficient at a competent company with good standards and communication practices. But you're telling me you made this process <strong>10 times</strong> as efficient to handle 10x the work? This simply can not be done.</p>
<p>The human processes involved in actual corporate software engineering have not changed significantly. Product managers might use ChatGPT to do "research" but they aren't suddenly pumping out ten times as many well vetted, well justified, well estimated stories as they did before. They can not do 10 user interviews all at once. The same goes for Designers and QA testers. Hiring 10x the number of PMs to keep up isn't feasible. Each hire has diminishing returns as network effects and bureaucracy take hold.</p>
<p>Even if we assume people mean only the actual code writing process is now 10-100x faster, we should still be skeptical of how this maths out. When you write code, how much of your time do you truly spend pushing buttons on the keyboard? It's probably less than you think. Much of your prime coding time is actually reading and thinking, often while waiting for compiling, a page refresh, or for tests to run. LLMs do not make <code>rustc</code> go faster.</p>
<p>What LLMs produce is often broken, hallucinated, or below codebase standards. The frequency of these errors go up with the size of the codebase. When that happens you have to re-prompt, which could instantly fix the problem or could be a huge waste of time. Or you can go in and fix the code yourself. But then you're back to measly 1x engineer status, perhaps worse if you've gotten so used to vibe coding you <a href="https://nmn.gl/blog/ai-and-learning">forgot how to code</a>. If you're "embracing the vibes" and not even looking at the code produced, you're simply going to hit a productivity wall once the codebase gets large enough. And once you do you'll have to reckon with the complete lack of standards and proper abstractions.</p>
<p>I think sometimes people lose the scale of just how big a 10x improvement is. 10x is the difference between your mini-van and a record setting <a href="https://en.wikipedia.org/wiki/ThrustSSC">supersonic land jet</a>. Imagine trying to drive your 10 minute commute down your city streets in a car that goes 600mph. Will you get to the other side of town in one tenth the time? No, because even a single 60 second stoplight will eat up your entire time budget. F1 cars slow down to mini-van speeds in basic turns. It turns out that most of any activity is not spent going at top speed.</p>
<p>100x productivity means you now do what used to be one year of work in two days. I shouldn't even need to touch the ludicrousness of numbers at that scale.</p>
<h2 id="do-10x-engineers-exist">Do 10x Engineers Exist?</h2>
<p>This debate isn't something I want to weigh in on but I might have to. My answer is sometimes, kinda. When I have had engineers who were 10x as valuable as others it was primarily due to their ability to <em>prevent unnecessary work</em>. Talking a PM down from a task that was never feasible. Getting another engineer to not build that unnecessary microservice. Making developer experience investments that save everyone just a bit of time on every task. Documenting your work so that every future engineer can jump in faster. These things can add up over time to one engineer saving 10x the time company wide than what they took to build it.</p>
<p>Work of this nature is not always available, so great engineers will only find themselves being 10x as productive in certain situations. At a certain point every engineer just needs to build features, which a great engineer might do twice as fast as a junior engineer, but they'll still hit the same bottlenecks as before. Flawed as story points are, I've never seen an engineer actually complete ten times as many as an average engineer consistently.</p>
<p>Notably, AI coding assistants do very little to prevent unnecessary work. On the contrary, AI often seems to encourage hastiness and over-building. When I ask architectural questions, it often recommends something that I realize is not necessary after a good night's sleep or a talk with a great engineer. All other things held the same, is a faster coder a better engineer? Yes, but it's not the 10x difference maker and it's hard to hold everything else constant. The more you focus on pumping out tasks as fast as possible the easier is to miss the important time savers that reduce total work.</p>
<h2 id="so-are-the-ai-posters-lying-or-what">So are the AI-posters lying or what?</h2>
<p>I think the AI-posters are a mix of the following, in order of least to most malevolent:</p>
<ul>
<li>Good-natured folks who are mismeasuring themselves and others</li>
<li>People heavily invested, personally or financially, in the success of AI (AI startup founders, investors, etc.)</li>
<li>Bosses outright trying to make their engineers feel precarious so they don't quit, look for other jobs, or ask for raises</li>
</ul>
<h3 id="the-good-natured-engineer-with-bad-math-skills">The good-natured engineer with bad math skills</h3>
<p>In my experience, AI delivers rare, short bursts of 10-100x productivity. When I have AI write me a custom ESLint rule in a few minutes, which would have taken hours of documentation surfing and tutorials otherwise, that's a genuine order of magnitude time and effort improvement. Moments like this do happen with AI. Many career non-coders have felt the magic in the first few days after spinning an app up with Lovable.</p>
<p>The problem is that productivity does not scale. I don't write more than one ESLint rule per year. This burst of productivity was enabled solely by the fact that I didn't care about this code and wasn't going to work to make it readable for the next engineer. If constantly writing ESLint rules became a core job requirement I'd sink the one-time cost to learn how ESLint internals work. After that, there simply wouldn't be a big difference in the time it takes to vibe code a rule vs. write it myself, especially when you add in the extra time to make my code human readable for when I come back to this file in 6 months.</p>
<p>Eventually every vibe coder reaches the point where the returns start heavily diminishing. Their <a href="https://pivot-to-ai.com/2025/03/18/guys-im-under-attack-ai-vibe-coding-in-the-wild/">site gets hacked</a> and they need to actually sink the time to learn how security works. The app gets too big for context windows and things start looking and functioning inconsistently. Real frontend engineers who know what they are doing are hired to implement a consistent design system and UX.</p>
<p>There's also a lot of simple biases and blind spots that can cause a productivity illusion. If you leave the depths of big corporate for a startup you will genuinely be shocked at how much more productive each engineer is. It's easy to credit this to AI. Some people really enjoy the technological novelty of AI coding and when you are working in something new you often feel like you're doing more than you ever did. I know the first time I used Python I felt like I was "sipping rocket fuel", but, as with all other technologies, it always comes back down to earth.</p>
<p>I think a lot of the more genuine 10x AI hype is coming from people who are simply in the honeymoon phase or haven't sat down to actually consider what 10x improvement means mathematically. I wouldn't be surprised to learn AI helps many engineers do certain tasks 20-50% faster, but the nature of software bottlenecks mean this doesn't translate to a 20% productivity increase and certainly not a 10x increase.</p>
<h3 id="incentives-matter">Incentives matter</h3>
<p>Look, I'm not an AI startup hater. If you want to plug OpenAI's API into your healthcare startup I might raise an eyebrow of concern over the risks, but I'd do the same for any startup desiring to move fast and break things in the medical field. My goal here isn't to say AI startup founders or investors are evil or even dishonest. My point is to say in the droll voice of your high school Econ 101 professor, "Incentives Matter".</p>
<p>If you are running an AI startup and every other AI startup is telling investors they are seeing 10x more productivity thanks to AI, the incentives are plain and simple: you should say the same publicly and privately. If your company is built on the back of AI, you are incentivized to sell AI as a miracle solution in every part of life. If you are an engineer at an AI startup and your boss asks you:</p>
<blockquote>
<p>Hey, you're getting 10x the productivity thanks to AI, just like all the other engineers, right?</p>
</blockquote>
<p>You are strongly incentivized to say yes. And when every other engineer also says yes for the same reason, that CEO <a href="https://www.youtube.com/watch?v=vn_PSJsl0LQ">isn't lying</a>, they are just relaying what they heard.</p>
<p>What I'd like to stress to those feeling anxiety like me is that this is nothing new. CEOs are not unbiased sources. Executives have been claiming that everything from Agile to Meyers-Briggs have unlocked limitless productivity. There will always be a new synergistic buzzword on LinkedIn, don't let it get you down. In fact, stop scrolling LinkedIn at all. It's a silly place.</p>
<h3 id="outright-malice">Outright Malice</h3>
<p>When something is said that makes people feel anxious, at least some of the time you should conclude it's because that's what the speaker wanted to happen. Bosses trying to make their engineers feel like their position is precarious is also nothing new. We all remember the narrative that a 3 month coding bootcamp could churn out 4-year-degree quality engineers, so you'd best not get too uppity or you'll be replaced with a bachelor of arts doing a career pivot. Then a few years went by and people realized that bootcamp grads were usually <a href="https://www.sandofsky.com/lambda-school/">woefully underprepared</a> for actual software engineering since they were not given the proper foundation.</p>
<p>Bootcamps and AI are just examples in a long series of poorly born out threats to commoditize the highly expensive, highly professionalized field of software engineering. They are rhetorical devices designed to imply precarity. Your boss can't actually fire you and replace you with AI, but he can make you <em>feel</em> like he <em>could</em>, and maybe not ask for that raise.</p>
<p>Some amount of the 10x AI engineer story is likely being told by people who simply want you to feel bad for this purpose. How much of it, I don't know. Despite how highly distrustful we've become of each other in these times, I still believe most people are fundamentally decent, so I'm not inclined to believe it's a high percentage.</p>
<h2 id="degrees-of-separation">Degrees of separation</h2>
<p>One thing I've noticed about all these characters in AI coding hype pieces is there is almost always a degree of separation from the writer to the actual productivity benefits. The poster is a founder, or a manager, or an investor, making grandiose claims about someone else's productivity. There's nothing wrong with secondary sources but if you can't find a primary source, you might start questioning the reliability of the information.</p>
<p>Presentations from actual engineers demonstrating how they achieve more productivity with AI are much more varied and much more muted in their praise. These demos show largely AI as the same technology you and I were familiar with before we got so anxious: a neat text generator that sometimes does magic but often requires you to take the wheel.</p>
<p>AI usage on open source projects, where the productive process can be publicly witnessed, has famously been a <a href="https://old.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/">hilarious failure</a>. I have learned things about how to use AI better from a few youtube videos. <a href="https://www.youtube.com/watch?v=sQYXZCUvpIc">Here's</a> a good one referenced in that Ludicity article above. I'll spoil it for you though, this engineer has not found the fountain of coding productivity.</p>
<h2 id="its-okay-to-be-less-productive">It's okay to be less productive</h2>
<p>Even after I got over the idea that there was a secret clade of engineer who was now ten times as productive and strong and tall and sexy as I was, I still felt some anxiety over the fact that I still didn't enjoy using AI very much. Vibe coding is a complete bore once the magic wears off. Reading LLM generated code sucks. Asking it politely to use a not hallucinated library is painful. But what if I was, despite all that, 20% more productive vibe coding than regular coding? Would it be wrong for me to do "normal" coding if a higher output path is available?</p>
<p>No. It's okay to sacrifice some productivity to make work enjoyable. More than okay, it's <em>essential</em> in our field. If you force yourself to work in a way you hate, you're just going to burn out. Only so much of coding is writing code, the rest is solving problems, doing system design, reasoning about abstractions, and interfacing with other humans. You are better at all those things when you feel good. It's okay to feel pride in your work and appreciate the craft. Over the long term your codebase will benefit from it.</p>
<p>It doesn't matter if digital music sounds objectively better than vinyl. It doesn't matter if flipping the record is less "productive" than letting the streaming service automatically roll over to the next song in 100x less time. If listening to a 70 year old disk makes you happier, just do it. You'll listen to more music if you do that than you would by forcing yourself to use the more "productive" streaming service. You will spend more time writing code and you'll write better code if you do it the way you like to.</p>
<p>Oh, and this exact argument works in reverse. If you feel good doing AI coding, just do it. If you feel so excited that you code more than ever before, that's awesome. I want everyone to feel that way, regardless of how they get there.</p>
<h2 id="how-to-be-a-good-ai-leader">How to be a good AI leader</h2>
<p>Making all your engineers feel constantly anxious about their performance is <em>bad for your company</em>. It will make your engineers not want to work for you. This is a recipe for short term thinking that will encourage engineers to max out bad metrics, like lines of code. Code review will get neglected, tech debt will compound, and in the long term the whole company will be footing the bill of those errors.</p>
<p>Unrealistic 10x expectations will result in rushed and thus subpar work without fail. Engineers need to have room to breathe. Room to take a little bit more time to do the thing right. Good codebases and good companies are built on a healthy balance of thinking for today and tomorrow. I'm thankful to work at one of these companies right now, but many aren't so fortunate.</p>
<p>Do not scold engineers for not using enough tokens. Your engineers are highly educated professionals in an extremely competitive field. Software engineers are already infamous for an over-eager cycle of embracing and abandoning new languages and tools. If you are paying these people this much, you should have the trust in them that if a super amazing productivity boost becomes available, they'll <em>come to you</em> asking for the pro plan. If you're worried about missing out on all the AI coding gains everyone else seems to be getting, sign up for a LLM team plan, host a training session, and see what comes out of it. That's all you need to do.</p>
<h2 id="conclusion">Conclusion</h2>
<p>There is no secret herbal medicine that prevents all disease sitting out in the open if you just follow the right Facebook groups. There is no AI coding revolution available if you just start vibing. You are not missing anything. Trust yourself. You are enough.</p>
<p>Oh, and don't scroll LinkedIn. Or Twitter. Ever.</p>

<ul><li>← Previous<br> <a href="https://colton.dev/blog/tailwind-is-the-worst-of-all-worlds/">Tailwind is the Worst of All Worlds</a></li>
</ul>

			</heading-anchors>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Genie 3: A new frontier for world models (249 pts)]]></title>
            <link>https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/</link>
            <guid>44798166</guid>
            <pubDate>Tue, 05 Aug 2025 14:08:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/">https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/</a>, See on <a href="https://news.ycombinator.com/item?id=44798166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="capabilities">
              
                
                
                  
                  <div>
  <h2 data-block-key="wbu8i">Genie 3’s capabilities include:</h2><p data-block-key="7oibn">The following are recordings of real time interactions from Genie 3.</p><h3 data-block-key="4b33n">Modelling physical properties of the world</h3><p data-block-key="5alru">Experience natural phenomena like water and lighting, and complex environmental interactions.</p>
</div>
                
              
                
                
                  
                  


<gdm-carousel id="block-2c894174-1e81-45ce-9d7e-f6eed0466be8">
  
  <div>
<div aria-label="Item 2" id="block-4d2e1943-df2a-474d-916b-748e4b2e48b3">
  <figure aria-labelledby="caption-4d2e1943-df2a-474d-916b-748e4b2e48b3">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_physical_properties_1_MNInndd.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> The video shows a first person perspective of someone navigating difficult terrain in the middle of a volcanic area. This is a real world video shot from the perspective of a wheeled robot that needs to traverse across a terrain. The vehicle has chunky offroad tires that crunch under the blackened rock. The camera is an egocentric camera mounted to the vehicle, and you can see the front tires just on the bottom of the camera along with the body of the robot. In the distance you can see smoke and lava flowing from the volcano. There are no other visible signs of life. There are lava pools that the agent is trying to avoid and random rock formations. The sky is a vivid blue.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 2" id="block-29587f73-7711-41b8-a95d-ceb42d5f598a">
  <figure aria-labelledby="caption-29587f73-7711-41b8-a95d-ceb42d5f598a">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_physical_properties_2_Qvyu3BP.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Jetski during the festival of lights</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 2" id="block-6fc97f07-124d-46bb-a8fb-39d74c3eaf92">
  <figure aria-labelledby="caption-6fc97f07-124d-46bb-a8fb-39d74c3eaf92">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_physical_properties_3_I8KO3kl.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Walking on a pavement in Florida next to a two-lane road from one side and the sea on the other, during an approaching hurricane, with strong wind and waves splashing over the road. There is a railing on the left of the agent, separating them from the sea. The road goes along the coast, with a short bridge visible in front of the agent. Waves are splashing over the railing and onto the road one after another. Palm trees are bending in the wind. There is heavy rain, and the agent is wearing a rain coat. Real world, first-person.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 2" id="block-245112c4-0e91-4311-844a-f341a4be5398">
  <figure aria-labelledby="caption-245112c4-0e91-4311-844a-f341a4be5398">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_physical_properties_4_6dT1rge.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Fast tracking real world video following a jellyfish swimming at high speed through the darkness of the deep sea between canyons covered in densely packed vent mussels with tiny white crabs crawling on them. Blurry hydrothermal vents in the distance spew thick, billowing plumes of vibrant blue, mineral-rich smoke from glowing rocky structures. Very dark, dim deep sea lighting, particles float in the cloudy ocean.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 2" id="block-030795d7-6f37-4866-beb3-9784132bf143">
  <figure aria-labelledby="caption-030795d7-6f37-4866-beb3-9784132bf143">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_physical_properties_5_1sm8u8o.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> A helicopter pilot carefully maneuvering over a coastal cliff with a small waterfall.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div></div>
  
</gdm-carousel>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="wbu8i">Simulating the natural world</h3><p data-block-key="dr8gd">Generate vibrant ecosystems, from animal behaviors to intricate plant life.</p>
</div>
                
              
                
                
                  
                  


<gdm-carousel id="block-bc05a02a-a545-41b3-8795-62d152b86618">
  
  <div>
<div aria-label="Item 5" id="block-7ce58041-66f0-4125-ae19-3f529874688a">
  <figure aria-labelledby="caption-7ce58041-66f0-4125-ae19-3f529874688a">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_simulating_natural_world_1_KkDJNGE.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Running by the shores of a glacial lake, exploring branching paths through the forest, crossing flowing mountain streams. Set amidst beautiful snow capped mountains and pine forest. Plentiful wildlife makes the journey a delight.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 5" id="block-5ed99f1c-ffb5-4e78-ace4-b51c8edefe5f">
  <figure aria-labelledby="caption-5ed99f1c-ffb5-4e78-ace4-b51c8edefe5f">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_simulating_natural_world_2_BJ3YgL8.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Real world tracking shot swimming through deep dimly lit ocean between deep ocean canyons, densely packed vast school of jellyfish swimming, bioluminescent lighting.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 5" id="block-0ac1f2a3-6136-49e7-93df-a38677c5b5dc">
  <figure aria-labelledby="caption-0ac1f2a3-6136-49e7-93df-a38677c5b5dc">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_simulating_natural_world_3_gwzGBLr.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> This is a natural, real-world landscape designed as a Japanese zen garden. The scene is set in the early morning under a clear sky. Soft, warm sunlight illuminates the garden, casting long, gentle shadows. The ground is covered in fine, white sand that is raked into meticulous swirling patterns. A small, still pond is present, with pink water lilies floating on its surface. Smooth, grey rocks of various sizes are placed throughout the garden, some with green moss on their surfaces. Key structures include a stacked stone cairn and a traditional Japanese stone lantern. The entire area is enclosed by a tall bamboo fence in the background. The visual style is photorealistic, with high detail in the textures of the sand, stone, and lush green vegetation.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 5" id="block-5e81a48a-68a4-4cc6-8b5c-88a4fe9e1189">
  <figure aria-labelledby="caption-5e81a48a-68a4-4cc6-8b5c-88a4fe9e1189">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_simulating_natural_world_4_KmMnEoZ.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> The environment is a natural, real-world landscape, specifically a dense arrangement of lush, vibrant foliage. The leaves are broad and deeply textured, displaying an array of green hues from emerald to lime, interspersed with hints of yellow and red, suggesting a rich, healthy ecosystem. Abstract dappled light filters down from above, creating shifting patterns of illumination and shadow across the leaves, highlighting their intricate veins and varied surfaces. The atmosphere is serene and deeply immersive, evoking a sense of being within a vibrant, living natural world. Small water droplets are visible on some leaf surfaces, reflecting the ambient light. The background is a soft blur of similar foliage, emphasizing the foreground elements. The air appears humid and still.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div></div>
  
</gdm-carousel>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="wbu8i">Modelling animation and fiction</h3><p data-block-key="dcth0">Tap into imagination, creating fantastical scenarios and expressive animated characters.</p>
</div>
                
              
                
                
                  
                  


<gdm-carousel id="block-0b6f1d8c-1b87-4c40-86fe-d5faf67c0988">
  
  <div>
<div aria-label="Item 8" id="block-64b19250-9187-4eb7-b5b1-24ddc6f45064">
  <figure aria-labelledby="caption-64b19250-9187-4eb7-b5b1-24ddc6f45064">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_animation_fiction_1_cTvKl3P.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> A vibrant 3D style, an adorable, fluffy creature bounding across a vibrant rainbow bridge in a fantastical landscape. The creature is small and compact, with fur that mimics the warm hues of a sunrise – oranges, yellows, and pinks blending seamlessly together. Its most striking feature is a pair of large, perked ears, shaped like those of a German Shepherd, adding a touch of playful contrast to its otherwise rounded form. As it runs on four short legs across the rainbow, its fur appears to ripple and flow, adding to its sense of dynamism and energy. The rainbow bridge arches gracefully through a whimsical landscape, perhaps filled with floating islands, glowing flora, and swirling clouds. The lighting is bright and cheerful, casting a warm glow on the creature and its surroundings. The overall impression is one of joy, wonder, and boundless energy, capturing the creature's playful spirit and the magical nature of the world it inhabits. This image evokes a sense of childlike whimsy and invites the viewer to imagine the adventures that await this charming creature in its fantastical realm.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 8" id="block-334ea4df-0e0f-4d4d-8446-4cb390db797d">
  <figure aria-labelledby="caption-334ea4df-0e0f-4d4d-8446-4cb390db797d">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_animation_fiction_2_RNg1ibH.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Being a lizard, origami style</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 8" id="block-03e8584c-a8a4-41bf-aceb-2e29096871cc">
  <figure aria-labelledby="caption-03e8584c-a8a4-41bf-aceb-2e29096871cc">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_animation_fiction_3_ZXctgpq.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> A fantastical, wide-angle shot captures a lush, enchanted forest bathed in the soft glow of twilight. The player controls a large firefly flying through towering trees with vibrant foliage creating a dense canopy overhead, filtering the sunlight and casting dappled shadows on the forest floor. Nestled among the branches are a handful of charming tree houses, each glowing with a warm, inviting light. The tree houses vary in size and design, some resembling whimsical castles, others cozy cabins. Tiny details, like glowing windows and miniature balconies, add to their charm. A winding path, barely visible beneath the undergrowth, leads the viewer's eye deeper into the enchanted forest. The overall scene evokes a sense of wonder, tranquility, and the magic of childhood dreams.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 8" id="block-99192087-b0e2-4be4-9596-a81f5b1f5777">
  <figure aria-labelledby="caption-99192087-b0e2-4be4-9596-a81f5b1f5777">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_modelling_animation_fiction_4_DuLFEfx.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> A serene Irish landscape, with rolling emerald-green hills, misty lakes, and rugged mountains, suddenly trembles violently—as if the earth itself is being torn apart. In a moment of surreal chaos, entire sections of land rip free, rising into the sky in jagged, brutalist formations, their rocky undersides exposed like raw, fractured earth. The lakes are wrenched upward, now suspended in the sky, their waters spilling downward in colossal waterfalls, creating an apocalyptic storm of mist and rain over the land below. The camera pulls back, revealing a new impossible geography—mountains floating, cliffs inverted, rivers twisting mid-air—as gravity itself bends, turning the once-peaceful countryside into a brutalist, surreal monument to nature’s violent transformation.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div></div>
  
</gdm-carousel>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="wbu8i">Exploring locations and historical settings</h3><p data-block-key="bkrrk">Transcend geographical and temporal boundaries to explore places and past eras.</p>
</div>
                
              
                
                
                  
                  


<gdm-carousel id="block-4cab3e39-d22b-47fb-92a9-846bb4186784">
  
  <div>
<div aria-label="Item 11" id="block-9b140acb-78a9-447f-8d4c-20ab136ef61c">
  <figure aria-labelledby="caption-9b140acb-78a9-447f-8d4c-20ab136ef61c">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_exploring_locations_1_OUxxXpK.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> A real world mountainous environment in the Alps. The landscape features steep, rocky cliffs and narrow gorges filled with loose scree and debris. The rock is predominantly grey and white, with patches of green vegetation clinging to the cliff faces. The top of the gorge opens up to a vista of dense evergreen forests and meadows. The overall theme is one of rugged, natural beauty and extreme terrain.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 11" id="block-b4bb6eb9-eed1-4ee2-96aa-3633951d10ee">
  <figure aria-labelledby="caption-b4bb6eb9-eed1-4ee2-96aa-3633951d10ee">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_exploring_locations_2_ryC3jcr.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Venice by Vaporetto. The canals of Venice are recreated with painstaking detail. The water has realistic reflections and wakes. The buildings show crumbling plaster and centuries of weathering. The scene is populated with other gondolas, water taxis, and barges.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 11" id="block-a7c32395-2772-4d7c-8689-42572cda3ff0">
  <figure aria-labelledby="caption-a7c32395-2772-4d7c-8689-42572cda3ff0">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_exploring_locations_3_dlHXqb9.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Exploring the palace of Knossos on Crete as it would have stood in its glorious heyday.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 11" id="block-9f40e5ba-079f-45b6-a26a-1442c28d7693">
  <figure aria-labelledby="caption-9f40e5ba-079f-45b6-a26a-1442c28d7693">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_exploring_locations_4_oZjNQYu.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Walking around on a beautiful day out in Hinsdale, Illinois. Real world. There are cars parked. The person filming is standing on the sidewalk, there are flocks of birds flying overhead.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 11" id="block-070f8fe9-52f4-4f85-85ba-4c7b549e32cd">
  <figure aria-labelledby="caption-070f8fe9-52f4-4f85-85ba-4c7b549e32cd">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_exploring_locations_5_LKqlgO0.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> A biking enthusiast driving on a narrow road on an edge of a cliff in India, the Killar-Kishtwar Road. Real-world, first-person, only hands on handles visible.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div></div>
  
</gdm-carousel>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="wbu8i">Pushing the frontier of real-time capabilities</h3><p data-block-key="1001j">Achieving a high degree of controllability and real-time interactivity in Genie 3 required significant technical breakthroughs. During the auto-regressive generation of each frame, the model has to take into account the previously generated trajectory that grows with time. For example, if the user is revisiting a location after a minute, the model has to refer back to the relevant information from a minute ago. To achieve real-time interactivity, this computation must happen multiple times per second in response to new user inputs as they arrive.</p>
</div>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="apn2o">Environmental consistency over a long horizon</h3><p data-block-key="a0tgg">In order for AI generated worlds to be immersive, they have to stay physically consistent over long horizons. However, generating an environment auto-regressively is generally a harder technical problem than generating an entire video, since inaccuracies tend to accumulate over time. Despite the challenge, Genie 3 environments remain largely consistent for several minutes, with visual memory extending as far back as one minute ago.</p>
</div>
                
              
                
                
                  
                  


<gdm-carousel id="block-2338c9da-d003-40eb-9541-65621a0a7bb1">
  
  <div>
<div aria-label="Item 15" id="block-6e097311-816e-4176-8695-9117c3496c0b">
  <figure aria-labelledby="caption-6e097311-816e-4176-8695-9117c3496c0b">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_environmental_consistency_1_iNVUBuv.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> POV action camera of a tan house being painted by a first person agent with a paint roller</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 15" id="block-ce662459-2030-4c14-a575-df35b8a57f56">
  <figure aria-labelledby="caption-ce662459-2030-4c14-a575-df35b8a57f56">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_environmental_consistency_2_zS0EAgg.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> A Victorian street with a grey house. The grey house has a portal ringed by magical sparks. The portal leads to a vast desert filled with dunes, and that desert is visible from the outside. The agent can walk into the portal and is teleported to the desert.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 15" id="block-3dbaa10d-fbfe-4bb9-9aa7-d85048fb3732">
  <figure aria-labelledby="caption-3dbaa10d-fbfe-4bb9-9aa7-d85048fb3732">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_environmental_consistency_3_ccj4cHU.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> A classroom where on the blackboard at the front of the room it says GENIE-3 MEMORY TEST and underneath is a beautiful chalk picture of an apple, a mug of coffee, and a tree. The classroom is empty except for this. Outside the window are trees and a few cars driving past.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 15" id="block-5555c342-6995-4d49-ae78-2dc7971a53e8">
  <figure aria-labelledby="caption-5555c342-6995-4d49-ae78-2dc7971a53e8">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_environmental_consistency_4_gIb3IbS.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> This is a fantastical, whimsical forest environment. The lighting is bright and cheerful, suggesting a sunny day with dappled light filtering through a dense canopy of lush, oversized leaves. The air is clear and still. The ground is a soft, verdant carpet of moss and unusually large, brightly coloured mushrooms in shades of red and blue, their caps dotted with white. Winding dirt paths, well-trodden and narrow, weave between towering, ancient trees with smooth, grey bark. Interspersed throughout the forest are charming, mushroom-shaped houses, with intricate wooden doors and tiny, circular windows, each one unique in its design and colour palette, ranging from vibrant reds to gentle blues and greens. Various small, friendly forest creatures, such as colourful butterflies and tiny singing birds, flit amongst the foliage, adding to the lively atmosphere. There is an abundance of peculiar, oversized flowers blooming in an array of pastel and bright hues, releasing a gentle glow.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 15" id="block-4fa74576-2ab3-4a55-a4d2-c969a31984bb">
  <figure aria-labelledby="caption-4fa74576-2ab3-4a55-a4d2-c969a31984bb">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_environmental_consistency_5_OCdIbx8.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> An extremely enormous, realistic gorilla, draped in a flamboyant, emerald red vest with ornate brass buttons and an elaborate, feathered bicorne hat, brandishing only a vintage silk parasol, navigates a series of outrageously extravagant, moss-laden McMansions where grand marble structures are subtly embraced by sprawling, ancient rose bushes and creeping ivy.</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div>
<div aria-label="Item 15" id="block-34d3776a-7837-4d86-9192-64684008bd3b">
  <figure aria-labelledby="caption-34d3776a-7837-4d86-9192-64684008bd3b">
    <gdm-video-embed><video muted="" playsinline="" loop="" width="1280" height="704"><source src="https://deepmind.google/api/blob/website/media/genie_environmental_consistency_6_96KPmd3.mp4" type="video/mp4">
  </video></gdm-video-embed>
    <figcaption>
      <gdm-caption>
    <p data-block-key="qmqby"><strong>Prompt:</strong> Walking around ancient Athens, Greek architecture, marble</p>
    
    
  </gdm-caption>
    </figcaption>
  </figure>
</div></div>
  
</gdm-carousel>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-a1bfba40-e2ee-436c-b3d3-7272a93d8666">
  

  <figcaption>
      <p data-block-key="w07ur"><i>The trees to the left of the building remain consistent throughout the interaction, even as they go in and out of view.</i></p>
    </figcaption>
</figure>
                
              
                
                
                  
                  <p data-block-key="wbu8i">Genie 3’s consistency is an emergent capability. Other methods such as NeRFs and Gaussian Splatting also allow consistent navigable 3D environments, but depend on the provision of an explicit 3D representation. By contrast, worlds generated by Genie 3 are far more dynamic and rich because they’re created frame by frame based on the world description and actions by the user.</p>
                
              
                
                
                  
                  





<figure aria-labelledby="caption-ddec9ea1-cb6f-4e51-967b-dffdcad93111">
  

  <figcaption>
      <gdm-caption>
    <p data-block-key="3r9m9"><strong>Prompt:</strong> First-person view drone video. High speed flight into and along a narrow canyon in Iceland with a river at the bottom and moss on the rocks, golden hour, realworld</p>
    
    
  </gdm-caption>
    </figcaption>
</figure>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
  <h3 data-block-key="wbu8i">Promptable world events</h3><p data-block-key="40ru4">In addition to navigational inputs, Genie 3 also enables a more expressive form of text-based interaction, which we refer to as <i>promptable world events</i>.</p><p data-block-key="7abeu">Promptable world events make it possible to change the generated world, like altering weather conditions or introducing new objects and characters, enhancing the experience from navigation controls.</p><p data-block-key="cirvr">This ability also increases the breadth of counterfactual, or “what if” scenarios, that can be used by agents learning from experience to handle unexpected situations.</p>
</div>
                
              
                
                
                  
                  <p data-block-key="du66g"><strong>Choose a world setting. Then, pick an event, and see Genie 3 create it.</strong></p>
                
              
                
                
                  
                  
                
              
                
                
                  
                  <hr>
                
              
            </div><div id="embodied-agent-research">
              
                
                
                  
                  <div>
  <h3 data-block-key="apn2o">Fueling embodied agent research</h3><p data-block-key="d857">To test the compatibility of Genie 3 created worlds for future agent training, we generated worlds for a recent version of our <a href="https://deepmind.google/discover/blog/sima-generalist-ai-agent-for-3d-virtual-environments/" rel="noopener" target="_blank">SIMA agent</a>, our generalist agent for 3D virtual settings. In each world we instructed the agent to pursue a set of distinct goals, which it aims to achieve by sending navigation actions to Genie 3. Like any other environment, Genie 3 is not aware of the agent’s goal, instead it simulates the future based on the agent's actions.</p>
</div>
                
              
                
                
                  
                  <p data-block-key="du66g"><strong>Choose a world setting. Then, pick a goal you'd like an agent to achieve and watch how it accomplishes it.</strong></p>
                
              
                
                
                  
                  
                
              
                
                
                  
                  <p data-block-key="wbu8i">Since Genie 3 is able to maintain consistency, it is now possible to execute a longer sequence of actions, achieving more complex goals. We expect this technology to play a critical role as we push toward AGI, and agents play a greater role in the world.</p>
                
              
                
                
                  
                  


<gdm-carousel id="block-e703a0c8-b4b3-4935-bf81-e0806049082b">
  
  
  
</gdm-carousel>
                
              
                
                
                  
                  <hr>
                
              
            </div><div id="limitations">
  <h2 data-block-key="wbu8i">Limitations</h2><p data-block-key="ggqs">While Genie 3 pushes the boundaries of what world models can accomplish, it's important to acknowledge its current limitations:</p><ul><li data-block-key="61sd2"><strong>Limited action space</strong>. Although promptable world events allow for a wide range of environmental interventions, they are not necessarily performed by the agent itself. The range of actions agents can perform directly is currently constrained.</li><li data-block-key="bfsot"><strong>Interaction and simulation of other agents</strong>. Accurately modeling complex interactions between multiple independent agents in shared environments is still an ongoing research challenge.</li><li data-block-key="55slt"><strong>Accurate representation of real-world locations</strong>. Genie 3 is currently unable to simulate real-world locations with perfect geographic accuracy.</li><li data-block-key="4irvj"><strong>Text rendering.</strong> Clear and legible text is often only generated when provided in the input world description.</li><li data-block-key="1nt0m"><strong>Limited interaction duration.</strong> The model can currently support a few minutes of continuous interaction, rather than extended hours.</li></ul>
</div><div id="responsibility">
  <h2 data-block-key="swoy7">Responsibility</h2><p data-block-key="fit92">We believe foundational technologies require a deep commitment to responsibility from the very beginning. The technical innovations in Genie 3, particularly its open-ended and real-time capabilities, introduce new challenges for safety and responsibility. To address these unique risks while aiming to maximize the benefits, we have worked closely with our Responsible Development &amp; Innovation Team.</p><p data-block-key="29qra">At Google DeepMind, we're dedicated to developing our best-in-class models in a way that amplifies human creativity, while limiting unintended impacts. As we continue to explore the potential applications for Genie, we are announcing Genie 3 as a limited research preview, providing early access to a small cohort of academics and creators. This approach allows us to gather crucial feedback and interdisciplinary perspectives as we explore this new frontier and continue to build our understanding of risks and their appropriate mitigations. We look forward to working further with the community to develop this technology in a responsible way.</p>
</div><div id="next-steps">
              
                
                
                  
                  <div>
  <h2 data-block-key="swoy7">Next steps</h2><p data-block-key="5i4jt">We believe Genie 3 is a significant moment for world models, where they will begin to have an impact on many areas of both AI research and generative media. To that end, we're exploring how we can make Genie 3 available to additional testers in the future.</p><p data-block-key="2us9t">Genie 3 could create new opportunities for education and training, helping students learn and experts gain experience. Not only can it provide a vast space to train agents like robots and autonomous systems, Genie 3 can also make it possible to evaluate agents’ performance, and explore their weaknesses.</p><p data-block-key="b7jov">At every step, we’re exploring the implications of our work and developing it for the benefit of humanity, safely and responsibly.</p>
</div>
                
              
                
                
                  
                  

<section>
  
    <h2>Please cite using the following BibTex</h2>
  

  <ul>
    
      <li>
            <gemini-button data-in-view="">
              <a data-gtm-tag="cta-selection" href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/genie-3/genie3worldmodel2025.bib" rel="noopener" target="_blank">
      <span>Download BibTeX</span>
      
    </a>
            </gemini-button>
        </li>
        
    
  </ul>
</section>
                
              
                
                
                  
                  <div>
      <h2 data-block-key="vmhlu">Acknowledgments</h2><p data-block-key="dpcrj">Genie 3 was made possible due to key research and engineering contributions from Phil Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleks Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang and Jessica Yung.</p><p data-block-key="1qkkl">We thank Andrew Audibert, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Nilesh Ray, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young and Vadim Zubov for their invaluable partnership in developing and refining key components of this project.</p><p data-block-key="rr7r">Thanks to Tim Rocktäschel, Satinder Singh, Adrian Bolton, Inbar Mosseri, Aäron van den Oord, Douglas Eck, Dumitru Erhan, Raia Hadsell, Zoubin Gharamani, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process.</p><p data-block-key="uevg">Feature video was produced by Suz Chambers, Matthew Carey, Alex Chen, Andrew Rhee, JR Schmidt, Scotch Johnson, Heysu Oh, Kaloyan Kolev, Arden Schager, Sam Lawton, Hana Tanimura, Zach Velasco, Ben Wiley, and Dev Valladares. Including samples generated by Signe Norly, Eleni Shaw, Andeep Toor, Gregory Shaw, and Irina Blok.</p><p data-block-key="7r9de">Finally, we extend our gratitude to Mohammad Babaeizadeh, Gabe Barth-Maron, Parker Beak, Jenny Brennan, Tim Brooks, Max Cant, Harris Chan, Jeff Clune, Kaspar Daugaard, Dumitru Erhan, Ashley Feden, Simon Green, Nik Hemmings, Michael Huber, Jony Hudson, Dirichi Ike-Njoku, Bonnie Li, Simon Osindero, Georg Ostrovski, Ryan Poplin, Alex Rizkowsky, Giles Ruscoe, Ana Salazar, Guy Simmons, Jeff Stanway, Metin Toksoz-Exley, Petko Yotov, Mingda Zhang and Martin Zlocha for their insights and support.</p>
    </div>
                
              
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TSMC says employees tried to steal trade secrets on iPhone 18 chip process (150 pts)]]></title>
            <link>https://9to5mac.com/2025/08/05/tsmc-says-employees-tried-to-steal-trade-secrets-on-iphone-18-chip-process/</link>
            <guid>44797408</guid>
            <pubDate>Tue, 05 Aug 2025 12:53:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5mac.com/2025/08/05/tsmc-says-employees-tried-to-steal-trade-secrets-on-iphone-18-chip-process/">https://9to5mac.com/2025/08/05/tsmc-says-employees-tried-to-steal-trade-secrets-on-iphone-18-chip-process/</a>, See on <a href="https://news.ycombinator.com/item?id=44797408">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="800" src="https://9to5mac.com/wp-content/uploads/sites/6/2025/08/TSMC-says-employees-tried-to-steal-trade-secrets-on-iPhone-18-chip-process.jpg?quality=82&amp;strip=all&amp;w=1600" alt="TSMC says employees tried to steal trade secrets on iPhone 18 chip process | Photo shows the inside of a hard drive" srcset="https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/08/TSMC-says-employees-tried-to-steal-trade-secrets-on-iPhone-18-chip-process.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/08/TSMC-says-employees-tried-to-steal-trade-secrets-on-iPhone-18-chip-process.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/08/TSMC-says-employees-tried-to-steal-trade-secrets-on-iPhone-18-chip-process.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2025/08/TSMC-says-employees-tried-to-steal-trade-secrets-on-iPhone-18-chip-process.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p><a href="https://9to5mac.com/guides/aapl/" target="_blank" rel="noreferrer noopener">Apple</a> chipmaker <a href="https://9to5mac.com/guides/tsmc/" target="_blank" rel="noreferrer noopener">TSMC</a> has said that several then-employees tried to steal trade secrets relating to the company’s most advanced chip process. TSMC fired the individuals concerned and is now taking legal action against them. The former employees may also face criminal prosecution.</p>



<p>The report relates to the company’s 2-nanometer chip process, which is expected to be used for the A20 chips across next year’s <a href="https://9to5mac.com/guides/iphone-18/" target="_blank" rel="noreferrer noopener">iPhone 18</a> lineup …</p>



<h2 id="h-tsmc-2nm-process-expected-to-debut-in-iphone-18">TSMC 2nm process expected to debut in iPhone 18</h2>



<p>TSMC leads the world in the most advanced chip processes, and is next year expected to use its 2nm technology <a href="https://9to5mac.com/2025/06/03/apples-a20-chip-packaging-breakthrough/" target="_blank" rel="noreferrer noopener">for the A20 chips</a> used in the iPhone 18 range. Apple typically gets access to TSMC’s most advanced chip processes ahead of the company’s other customers.</p>



<p>Apple analyst Ming-Chi Kuo has suggested that the new chip will be <a href="https://9to5mac.com/2025/03/22/apple-iphone-18-2nm-a20-chip-kuo/" target="_blank" rel="noreferrer noopener">used for all iPhone 18 models</a>, not just the two Pro ones.</p>



<h2 id="h-tsmc-says-employees-tried-to-steal-trade-secrets">TSMC says employees tried to steal trade secrets</h2>



<p><em><a href="https://asia.nikkei.com/business/technology/tsmc-fires-workers-for-breaching-data-rules-on-cutting-edge-chip-tech" target="_blank" rel="noreferrer noopener">Nikkei Asia</a></em> reports that TSMC accused several former employees of attempting to obtain secret information about its 2nm chip development and production process.</p>



<blockquote>
<p>Several former employees of TSMC are suspected of attempting to obtain critical proprietary information on 2-nanometer chip development and production while working at the company, according to multiple sources familiar with the matter.</p>



<p>In response to Nikkei Asia’s questions, TSMC said that it recently “detected unauthorized activities during routine monitoring, leading to the discovery of potential trade secret leaks.”</p>



<p>The world’s top chipmaker said on Monday it took “strict disciplinary actions against the personnel involved and has initiated legal proceedings.”</p>
</blockquote>



<p>The attempt was detected by spotting “unusual access patterns” on the part of one of the employees.</p>



<p>The report says there could even be national security implications, as the Taiwanese government takes extremely seriously the protection of advanced technology developed within the country. Prosecutors have confirmed that they are investigating, and TSMC says that it will seek prosecution to the fullest extent of the law.</p>



<p>No details have been shared on the nature of the information obtained. It is likely that it relates to the 2nm process in general rather than anything specific to Apple’s A20 chip.</p>



<h4 id="h-highlighted-accessories">Highlighted accessories</h4>



<ul>
<li><a href="https://amzn.to/3UaJjDn" target="_blank" rel="noreferrer noopener">Official Apple Store on Amazon</a></li>



<li><a href="https://www.amazon.com/Anker-Charger-Compact-Technology-Included/dp/B0CP7NWH6L?tag=blovejoy-20" target="_blank" rel="noreferrer noopener">Anker 511 Nano Pro ultra-compact iPhone charger</a></li>



<li><a href="https://www.amazon.com/Spigen-Compatible-Accessories-Anti-Yellowing-Military-Grade/dp/B0DKGBTVHW?tag=blovejoy-20" target="_blank" rel="noreferrer noopener">Spigen MagFit case for iPhone 16e – adds MagSafe support</a></li>



<li><a href="https://www.amazon.com/Apple-MagSafe-Charger-Capability-Compatible/dp/B0DGJ4QQ5W?tag=blovejoy-20" target="_blank" rel="noreferrer noopener">Apple MagSafe Charger with 25w power for iPhone 16 models</a></li>



<li><a href="https://www.amazon.com/Apple-30W-USB-C-Power-Adapter/dp/B0CX23PHFD?tag=blovejoy-20" target="_blank" rel="noreferrer noopener">Apple 30W charger for above</a></li>



<li><a href="https://www.amazon.co.uk/dp/B0C4FDJ8F7?tag=blovejoy-20" target="_blank" rel="noreferrer noopener">Anker 240W braided USB-C to USB-C cable</a></li>
</ul>



<p><em>Photo by&nbsp;<a href="https://unsplash.com/@heapdump?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Patrick Lindenberg</a>&nbsp;on&nbsp;<a href="https://unsplash.com/photos/photo-of-optical-disc-drive-1iVKwElWrPA?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a></em></p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Mac to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Build Your Own Lisp (125 pts)]]></title>
            <link>https://www.buildyourownlisp.com/</link>
            <guid>44796953</guid>
            <pubDate>Tue, 05 Aug 2025 11:55:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.buildyourownlisp.com/">https://www.buildyourownlisp.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44796953">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<h2>
  Build Your Own Lisp<br>
  <small>Learn C and build your own programming language in 1000 lines of code!</small>
</h2>

<p><img src="https://www.buildyourownlisp.com/static/img/lovelace.png" alt="lovelace" width="275px" height="395px">
</p>

<hr>

<p>If you're looking to learn C, or you've ever wondered how to build your own programming language, this is the book for you.</p>

<p>In just a few lines of code, I'll teach you how to use C, and together, we'll start building your very own language.</p>

<p>Along the way we'll learn about the weird and wonderful nature of Lisps, how to develop a real-world project, concisely solve problems, and write beautiful code!</p>

<p>This book is free to read online, so you can get started right away! But for those who want to show their support, or who want the best reading experience, this book is also available for purchase in print format, or for cheap in all major e-book formats.</p>






<!--
<p>If you want to show support, I also accept donations in the following formats...</p>

<table cellpadding='10'>
  <tr><td><em>Bitcoin</em></td>    <td><code><small>17WQM2WYt28j6pNYZvUcRsozG87wCwPhmp</small></code></td></tr>
  <tr><td><em>Dogecoin</em></td>   <td><code><small>D5Tozp7epcLZCfN3zU9x51cM7uZD72PbwU</small></code></td></tr>
  <tr><td><em>Real Money &trade;</em></td>
    <td style='text-align:center;'>
      <form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
      <input type="hidden" name="cmd" value="_s-xclick">
      <input type="hidden" name="hosted_button_id" value="AJYFXLPVF8S72">
      <button type="submit" name="submit" class="btn btn-large btn-primary" >Donate</button>
      </form>
    </td>
  </tr>
</table>
-->

<table>

  <tbody><tr>
    <td><em>"I finally feel complete as a C programmer, having implemented my own Lisp."</em></td>
    <td><em>"Every programmer should do something like this, at least once."</em></td>
    <td><em>"One of the greatest things I've ever found on the internet..."</em></td>
  </tr>
  
  <tr>
    <td><a href="https://twitter.com/hirojin">@hirojin</a></td>
    <td><a href="https://twitter.com/mattcaldwell">@mattcaldwell</a></td>
    <td><a href="https://twitter.com/euryadam">@euryadam</a></td>
  </tr>
  
</tbody></table>




         </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[uBlock Origin Lite now available for Safari (536 pts)]]></title>
            <link>https://apps.apple.com/cn/app/ublock-origin-lite/id6745342698</link>
            <guid>44795825</guid>
            <pubDate>Tue, 05 Aug 2025 09:01:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apps.apple.com/cn/app/ublock-origin-lite/id6745342698">https://apps.apple.com/cn/app/ublock-origin-lite/id6745342698</a>, See on <a href="https://news.ycombinator.com/item?id=44795825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="" data-test-bidi=""><p>uBO Lite (uBOL) 是一个基于最新浏览器扩展接口（Manifest Version 3）打造的的内容屏蔽工具。</p><p>该扩展预设的规则列表对应 uBlock Origin 的预设过滤规则列表：</p><p>- uBlock Origin 内置过滤规则列表<br>- EasyList<br>- EasyPrivacy<br>- Peter Lowe 的广告和跟踪服务器列表</p><p>访问选项页面，点击弹出面板中的 _齿轮_ 图标，即可启用更多规则集。</p><p>uBOL 的过滤规则是完全声明式的，并不需要固定保留一个 uBOL 扩展进程，基于 CSS/JS 注入的内容过滤更是交由浏览器进行调度，比起扩展本身更为可靠。 这也即是说当内容被过滤时 uBOL 自身并不占用额外 CPU 和内存资源，_只有_在您打开弹出面板或是设置页面时才会生成 uBOL 扩展进程。</p></div><section>
  <div>
    <h2>
      App 隐私
    </h2>

    


  </div>

  <p>
    开发者“<span>Raymond Hill</span>”已表明该 App 的隐私规范可能包括了下述的数据处理方式。有关更多信息，请参阅<a href="https://github.com/uBlockOrigin/uBOL-home/wiki/Privacy-policy">开发者隐私政策</a>。
  </p>

  <div>
        
        <h3>未收集数据</h3>
        <p>开发者不会从此 App 中收集任何数据。</p>
<!---->      </div>

    <p>隐私处理规范可能基于你使用的功能或你的年龄等因素而有所不同。<a href="https://apps.apple.com/story/id1538632801">了解更多</a></p>
</section><section>
  <div>
    <h2>信息</h2>
    <dl>
        <p>
          <dt>提供者</dt>
          <dd>
              Raymond Hill
          </dd>
        </p>
        <p>
          <dt>大小</dt>
          <dd aria-label="5.8 MB">5.8 MB</dd>
        </p>
        <p>
          <dt>类別</dt>
          <dd>
              <a href="https://itunes.apple.com/cn/genre/id6002" data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;actionUrl&quot;:&quot;https://itunes.apple.com/cn/genre/id6002&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;GenrePage&quot;}">
                工具
              </a>
          </dd>
        </p>
      <div>
        <dt>兼容性</dt>
          <dd>
              <dl>
                <dt>
                  iPhone
                </dt>
                <dd>设备需装有 iOS 18.0 或更高版本。
                </dd>
              </dl>
              <dl>
                <dt>
                  iPad
                </dt>
                <dd>设备需装有 iPadOS 18.0 或更高版本。
                </dd>
              </dl>
              <dl>
                <dt>
                  Mac
                </dt>
                <dd>设备需装有 macOS 15.0 或更高版本。
                </dd>
              </dl>
              <dl>
                <dt>
                  Apple Vision
                </dt>
                <dd>设备需装有 visionOS 2.0 或更高版本。
                </dd>
              </dl>
          </dd>
      </div>
<!---->      
      
<!---->      <p>
        <dt>Copyright</dt>
        <dd>© Raymond Hill 2022</dd>
      </p>
        <p>
          <dt>价格</dt>
          <dd>免费</dd>
        </p>
<!---->
    </dl>
  </div>
  <div>
    <ul>
<!---->        <li>
          <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToAppSupport&quot;}" href="https://github.com/uBlockOrigin/uBOL-home">
            App 支持
          </a>
        </li>
        <li>
          <a data-metrics-click="{&quot;actionType&quot;:&quot;navigate&quot;,&quot;targetType&quot;:&quot;link&quot;,&quot;targetId&quot;:&quot;LinkToPrivacyPolicy&quot;}" href="https://github.com/uBlockOrigin/uBOL-home/wiki/Privacy-policy">
            隐私政策
          </a>
        </li>
<!----><!---->    </ul>
  </div>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apache ECharts 6 (142 pts)]]></title>
            <link>https://echarts.apache.org/handbook/en/basics/release-note/v6-feature/</link>
            <guid>44794926</guid>
            <pubDate>Tue, 05 Aug 2025 06:33:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://echarts.apache.org/handbook/en/basics/release-note/v6-feature/">https://echarts.apache.org/handbook/en/basics/release-note/v6-feature/</a>, See on <a href="https://news.ycombinator.com/item?id=44794926">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article> <p>Twelve years ago, ECharts was first released on GitHub, planting the seed for an open-source journey.</p> <p>From a simple charting tool to a visualization powerhouse supporting millions of developers; from a single front-end charting library to a comprehensive technology system covering mobile, large screens, and server-side rendering—over these 12 years, we've witnessed ECharts' continuous technical breakthroughs and have been delighted to see developers worldwide create countless stunning data stories with ECharts.</p> <p>Now, Apache ECharts 6.0 is officially released, bringing 12 major upgrades to take your data visualization to the next level.</p> <h2 id="feature-overview" tabindex="-1">Feature Overview</h2> <p>Twelve years of accumulation, all for more ultimate visual expression. The core strength of Apache ECharts has always come from a deep understanding of developers' real challenges. When defining the direction for 6.0, the question was clear: <strong>How can we make complex data presentation both powerful and elegant?</strong></p> <p>This drove us to evolve deeply around three core dimensions:</p> <ul><li><strong>More professional visual presentation</strong>: From a meticulously crafted default theme to intelligent dark mode switching, ensuring charts have a professional look and seamlessly integrate into modern applications.</li> <li><strong>Expanding the boundaries of data expression</strong>: New chart types and features to handle complex scenarios and enable intuitive expression of deep data.</li> <li><strong>Unleashing freedom of composition</strong>: From the revolutionary matrix coordinate system to reusable custom series and optimized axis labels—empowering developers to freely compose and turn creativity into unconstrained visual works.</li></ul> <p>We have made 12 upgrades across these three core dimensions. These are not just simple feature additions, but a solid foundation for building the next generation of data-driven applications. They all point to one goal: <strong>to make ECharts powerful, reliable, and stable in the background, leaving the stage and spotlight for your creative expression.</strong></p> <p>Below, we introduce these twelve upgrades in detail:</p> <ul><li><strong>More professional visual presentation</strong> <ul><li><strong>1. Brand New Default Theme</strong>: Modern design language for professional data expression</li> <li><strong>2. Dynamic Theme Switching</strong>: Seamless runtime theme switching for multi-theme scenarios</li> <li><strong>3. Dark Mode Support</strong>: Automatically adapts to system dark/light mode for better UX</li></ul></li> <li><strong>Expanding the boundaries of data expression</strong> <ul><li><strong>4. New Chord Chart</strong>: Visualize complex relationships and distributions</li> <li><strong>5. New Beeswarm Chart</strong>: Smartly expand overlapping data points into a honeycomb layout</li> <li><strong>6. New Scatter Jittering</strong>: Add jitter to scatter plots for better readability of dense data</li> <li><strong>7. New Broken Axis</strong>: Easily present data with large magnitude differences</li> <li><strong>8. Enhanced Stock Trading Charts</strong>: Improved label capabilities and more out-of-the-box trading charts</li></ul></li> <li><strong>Unleashing freedom of composition</strong> <ul><li><strong>9. New Matrix Coordinate System</strong>: Freely combine chart types and components like a table</li> <li><strong>10. Enhanced Custom Series</strong>: Support npm publishing and dynamic registration for code reuse</li> <li><strong>11. New Custom Charts</strong>: Violin, contour, stage, bar range, and line range charts</li> <li><strong>12. Axis Label Optimization</strong>: Smarter default axis label layout to prevent overflow and overlap</li></ul></li></ul> <p>With these upgrades, Apache ECharts 6.0 helps users create more charts more flexibly and conveniently, truly achieving "unlimited possibilities in charting"!</p> <h2 id="feature-details" tabindex="-1">Feature Details</h2> <h3 id="1.-brand-new-default-theme" tabindex="-1">1. Brand New Default Theme</h3> <p>During the development of ECharts 6.0, we analyzed real user scenarios and found that over 70% of developers use the default theme. This made us realize: an excellent default theme should not only be aesthetically pleasing but also meet the general needs of various business scenarios.</p> <p>The new theme system uses design tokens to reconstruct colors, spacing, and other design elements, <strong>making different chart types and components more harmonious and consistent</strong>.</p> <img data-src="images/feature-v6/1-default-theme.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/1-default-theme.png"> <p>Although the 6.0 theme has significant changes from 5.x, we provide a <a href="https://github.com/apache/echarts/blob/master/theme/v5.js">v5.js</a> theme file for developers who want to use new features but keep the old style for quick migration.</p> <h3 id="2.-dynamic-theme-switching" tabindex="-1">2. Dynamic Theme Switching</h3> <p>In previous versions, changing a chart's theme required disposing of the chart instance and re-initializing, which could negatively impact user experience due to repeated animations. In the new version, we implemented <strong>dynamic theme switching</strong> (see documentation), significantly improving the user experience.</p> <img data-src="images/feature-v6/2-switch-themes.gif" width="600px"> <h3 id="3.-dark-mode-support" tabindex="-1">3. Dark Mode Support</h3> <p>With dynamic theme registration and switching, a typical scenario is <strong>listening to the system's dark mode and dynamically adjusting the chart's theme</strong>.</p> <img data-src="images/feature-v6/3-responsive-themes.gif" width="600px"> <p>This is crucial for business scenarios supporting dark mode, ensuring the application interface matches the system theme and greatly enhancing user experience.</p> <p>Here's how to listen for system dark mode and change the chart theme:</p> <div><pre><code><span>const</span> darkModeMediaQuery <span>=</span> window<span>.</span><span>matchMedia</span><span>(</span><span>'(prefers-color-scheme: dark)'</span><span>)</span><span>;</span>
<span>function</span> <span>updateDarkMode</span><span>(</span><span>)</span> <span>{</span>
    <span>const</span> isDarkMode <span>=</span> darkModeMediaQuery<span>.</span>matches<span>;</span>
    <span>for</span> <span>(</span><span>const</span> chart <span>of</span> charts<span>)</span> <span>{</span>
        chart<span>.</span><span>setTheme</span><span>(</span>isDarkMode <span>?</span> <span>'dark'</span> <span>:</span> <span>'default'</span><span>)</span><span>;</span>
    <span>}</span>
<span>}</span>
darkModeMediaQuery<span>.</span><span>addEventListener</span><span>(</span><span>'change'</span><span>,</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>updateDarkMode</span><span>(</span><span>)</span><span>;</span>
<span>}</span><span>)</span><span>;</span></code></pre></div> <h3 id="4.-new-chord-chart" tabindex="-1">4. New Chord Chart</h3> <p>Chord charts intuitively show flows and weights in complex relationship networks, ideal for scenarios like financial transactions and social networks. ECharts innovatively supports <strong>using gradient colors from source and target nodes for edges, creating unique visual effects</strong>. See <a href="https://echarts.apache.org/option.html#series-chord">series-chord</a>.</p> <img data-src="images/feature-v6/4-chord.gif" width="600px"> <h3 id="5.-new-beeswarm-chart" tabindex="-1">5. New Beeswarm Chart</h3> <p>Traditional scatter plots can become overcrowded on category axes. Beeswarm charts use non-numeric axis offsets to <strong>distribute points without overlap while preserving value axis accuracy</strong>. Set <a href="https://echarts.apache.org/option.html#xAxis.jitter">jitter</a> to a non-zero value and <a href="https://echarts.apache.org/option.html#xAxis.jitterOverlap">jitterOverlap</a> to <code>false</code> to enable beeswarm charts.</p> <img data-src="images/feature-v6/5-beeswarm.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/5-beeswarm.png"> <h3 id="6.-new-scatter-jittering" tabindex="-1">6. New Scatter Jittering</h3> <p>Scatter jittering adds random offsets to non-data dimensions, <strong>solving the problem of overly dense data points</strong>.</p> <p>Without jittering, it's hard to see the distribution when data is dense:</p> <img data-src="images/feature-v6/6-jittering-off.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/6-jittering-off.png"> <p>With jittering enabled, the densest range (6-8) becomes clear. Compared to beeswarm, scatter jittering offers better performance.</p> <img data-src="images/feature-v6/6-jittering-on.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/6-jittering-on.png"> <p>Set <a href="https://echarts.apache.org/option.html#xAxis.jitter">jitter</a> to a non-zero value and <a href="https://echarts.apache.org/option.html#xAxis.jitterOverlap">jitterOverlap</a> to <code>true</code> to enable scatter jittering.</p> <h3 id="7.-new-broken-axis" tabindex="-1">7. New Broken Axis</h3> <p>Broken axis is a visualization technique for showing data with large magnitude differences. In ECharts 6.0, we innovatively implemented a <strong>torn-paper effect for broken axes</strong>, making the meaning more intuitive, and supporting click-to-expand to restore the real data ratio.</p> <img data-src="images/feature-v6/7-break-axis.gif" width="600px"> <h3 id="8.-enhanced-stock-trading-charts" tabindex="-1">8. Enhanced Stock Trading Charts</h3> <p>ECharts 6.0 deeply optimizes for financial trading scenarios, enhancing label positioning relative to coordinate systems to help developers quickly build professional-grade trading analysis tools.</p> <p>Below is a comprehensive stock trading chart using ECharts, combining <strong>time-sharing, MACD, volume, order book, and depth chart</strong>:</p> <img data-src="images/feature-v6/8-stock.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/8-stock.png"> <p>These examples help developers quickly meet financial trading needs. For example, displaying numbers in the four corners of the chart can be achieved with <a href="https://echarts.apache.org/handbook/$%7BoptionPathseries-line.markPoint.data.relativeTo">relativeTo</a>.</p> <h3 id="9.-new-matrix-coordinate-system" tabindex="-1">9. New Matrix Coordinate System</h3> <p>The above example also uses the new matrix coordinate system in ECharts 6.0, which is very powerful. It can be used for covariance matrix charts:</p>  <p>Periodic table:</p>  <p>As a layout, it also allows developers to combine various chart types and components to create flexible and complex visualizations:</p>  <h3 id="10.-enhanced-custom-series" tabindex="-1">10. Enhanced Custom Series</h3> <p>Previously, using ECharts custom series meant developers had to write complex <code>renderItem</code> logic from scratch, and code reuse was limited to copy-pasting. Now, ECharts 6.0 brings a standardized, reusable solution:</p> <ul><li><strong>Custom series registration</strong>: Like theme registration, custom series can be dynamically registered and used as easily as built-in series. See <a href="https://echarts.apache.org/option.html#series-custom.renderItem">series-custom.renderItem</a></li> <li><strong>Official custom series project</strong>: The official project at <a href="https://github.com/apache/echarts-custom-series">https://github.com/apache/echarts-custom-series</a> provides multiple custom series, available via npm after the official release</li> <li><strong>Publish your own custom series</strong>: Submit a pull request to the above project or publish to your own repo for code reuse</li></ul> <h3 id="11.-new-custom-charts" tabindex="-1">11. New Custom Charts</h3> <p>This release provides 6 practical custom charts in the custom series project. See <a href="https://github.com/apache/echarts-custom-series">echarts-custom-series</a> for usage and documentation. Including <strong>violin chart</strong>:</p> <img data-src="images/feature-v6/11-violin.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/11-violin.png"> <p><strong>Contour chart</strong>:</p> <img data-src="images/feature-v6/11-contour.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/11-contour.png"> <p><strong>Sleep stage chart</strong>:</p> <img data-src="images/feature-v6/11-stage.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/11-stage.png"> <p><strong>Segmented doughnut chart</strong>:</p> <img data-src="images/feature-v6/11-segmentedDoughnut.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/11-segmentedDoughnut.png"> <p><strong>Bar range chart</strong>:</p> <img data-src="images/feature-v6/11-barRange.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/11-barRange.png"> <p><strong>Line range chart</strong>:</p> <img data-src="images/feature-v6/11-lineRange.png" width="600px" src="https://echarts.apache.org/handbook/images/feature-v6/11-lineRange.png"> <p>Unleash your creativity and join us in creating more custom charts!</p> <h3 id="12.-axis-label-optimization" tabindex="-1">12. Axis Label Optimization</h3> <p>In previous versions, axis labels and names in rectangular coordinate systems could easily overflow or overlap when data was long. Users couldn't always predict space needs as data changed. In this version, we've optimized the default strategies to prevent overflow and overlap.</p> <h2 id="upgrade-guide" tabindex="-1">Upgrade Guide</h2> <p>See the full <a href="https://echarts.apache.org/changelog.html#v6-0-0">changelog</a> and <a href="https://echarts.apache.org/handbook/en/basics/release-note/v6-upgrade-guide">upgrade guide</a>.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Monitor your security cameras with locally processed AI (380 pts)]]></title>
            <link>https://frigate.video/</link>
            <guid>44794508</guid>
            <pubDate>Tue, 05 Aug 2025 05:05:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frigate.video/">https://frigate.video/</a>, See on <a href="https://news.ycombinator.com/item?id=44794508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><main id="content"><div id="hero"><p><img src="https://frigate.video/images/live.webp" alt="App preview"></p><div><h2>Monitor your security cameras with locally processed AI</h2><div><p>Frigate is an open source NVR built around real-time AI object detection. All processing is performed locally on your own hardware, and your camera feeds never leave your home.</p><p>Get access to custom models designed specifically for Frigate with Frigate+.</p></div></div></div><div id="features"><div><p><img src="https://frigate.video/images/detection.jpg" alt="App preview on a phone and tablet"></p><div><h3>Reduce false positives with local object detection</h3><p>Traditional NVRs can require hours of fine tuning to reduce false positive rates because they rely on simple motion detection. By offloading object detection to a supported AI accelerator, even modest hardware can run advanced analysis to determine if the motion is actually a person, car, or other object of interest. With Frigate's local processing, there is no need to pay for your personal camera footage to be sent to the cloud for analysis.</p></div></div><div><p><img src="https://frigate.video/images/review.webp" alt="App users welcoming a new member"></p><div><h3>Stop reviewing shadows and wind and start reviewing detections that matter</h3><p>Let Frigate's AI scrub your video feeds for you. With a supported AI accelerator, Frigate can run 100+ object detections per second so it doesn't miss a single frame.</p></div></div><div><p><img src="https://frigate.video/images/driveway_zones-min.png" alt="App users welcoming a new member"></p><div><h3>Fine tune your events and alerts with zones</h3><p>Frigate tracks objects in real-time and can determine the exact moment a person starts walking up your front steps or when a car enters your driveway. Refine your notifications based on precise locations.</p></div></div><div><p><img src="https://frigate.video/images/hass-opt.png" alt="App user profile preview"></p><div><h3>Integrate with Home Assistant and other automation platforms</h3><p>Give your home eyes by integrating object detection into Home Assistant, OpenHab, NodeRed, or anything with MQTT support. Frigate integrates directly into Home Assistant's media browser, provides low latency camera entities, and exposes real-time sensors and switches to power automations and notifications to your heart's content.</p></div></div></div><div id="reviews"><blockquote><div><p>Frigate's high level of customizability, fast object detection and tight integration with  Home Assistant creates the perfect open source, locally controlled, security camera system.</p></div></blockquote><blockquote><div><p>Frigate has helped me reduce hours of false detections from my hard drive and saved me maybe as much time scouring through said, uneventful, footage. Ok maybe not that much, but seriously,  zero false detections.</p></div></blockquote><blockquote><div><p>Frigate has allowed me to remove all cloud dependencies from my security cameras, without losing  any sort of object detection features or recording history. Support is second to none. Highly recommended.</p></div></blockquote></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PHP 8.5 adds pipe operator (324 pts)]]></title>
            <link>https://thephp.foundation/blog/2025/07/11/php-85-adds-pipe-operator/</link>
            <guid>44794271</guid>
            <pubDate>Tue, 05 Aug 2025 04:13:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thephp.foundation/blog/2025/07/11/php-85-adds-pipe-operator/">https://thephp.foundation/blog/2025/07/11/php-85-adds-pipe-operator/</a>, See on <a href="https://news.ycombinator.com/item?id=44794271">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
            <p>PHP 8.5, due out November of this year, will bring with it another long-sought-after feature: the <a href="https://wiki.php.net/rfc/pipe-operator-v3">pipe operator</a> (<code>|&gt;</code>).  It's a small feature with huge potential, yet it still took years to happen.</p>

<h2 id="what-is-a-pipe-operator%3F">What is a pipe operator?</h2>

<p>The pipe operator, spelled <code>|&gt;</code>, is deceptively simple.  It takes the value on its left side and passes it as the single argument to a function (or in PHP's case, <code>callable</code>) on its right side:</p>

<pre><code>$result = "Hello World" |&gt; strlen(...)

// Is equivalent to
$result = strlen("Hello World");
</code></pre>

<p>On its own, that is not all that interesting.  Where it becomes interesting is when it is repeated, or chained, to form a "pipeline."  For example, here's real code from a real project I've worked on, recast to use pipes:</p>

<pre><code>$arr = [
  new Widget(tags: ['a', 'b', 'c']),
  new Widget(tags: ['c', 'd', 'e']),
  new Widget(tags: ['x', 'y', 'a']),
];

$result = $arr
    |&gt; fn($x) =&gt; array_column($x, 'tags') // Gets an array of arrays
    |&gt; fn($x) =&gt; array_merge(...$x)       // Flatten into one big array
    |&gt; array_unique(...)                  // Remove duplicates
    |&gt; array_values(...)                  // Reindex the array.
;

// $result is ['a', 'b', 'c', 'd', 'e', 'x', 'y']
</code></pre>

<p>The same code without pipes would require either this horribly ugly nest of evil:</p>

<pre><code>array_values(array_unique(array_merge(...array_column($arr, 'tags'))));
</code></pre>

<p>Or manually creating a temporary variable for each step.  While temp variables are not the worst thing in the world, they are extra mental overhead, and mean that a chain like that cannot be used in a single-expression context, like a <code>match()</code> block.  A pipe chain can.</p>

<p>Anyone who has worked on the Unix/Linux command line will likely recognize the similarity to the shell pipe, <code>|</code>.  That's very deliberate, as it is effectively the same thing: Use the output from the left side as the input on the right side.</p>

<h2 id="where-did-it-come-from%3F">Where did it come from?</h2>

<p>The <code>|&gt;</code> operator appears in many languages, mostly in the functional world.  F# has essentially the exact same operator, as does OCaml.  Elixir has a slightly fancier version (which we considered but ultimately decided against for now).  Numerous PHP libraries exist in the wild that offer similar capability with many extra expensive steps, including my own <a href="https://github.com/Crell/fp/">Crell/fp</a>.</p>

<p>The story for PHP pipes, though, begins with Hack/HHVM, Facebook's PHP fork née competitive implementation.  Hack included many features beyond what PHP 5 of the day offered; many of them eventually ended up in later PHP versions.  One of its features was a unique spin on a pipe operator.</p>

<p>In 2016, Sara Golemon, long-time PHP contributor and former Open Source lead on the HHVM project, proposed porting <a href="https://wiki.php.net/rfc/pipe-operator">Hack's pipes</a> to PHP directly.  In that RFC, the right side of a pipe wasn't a <code>callable</code> but an expression, and used a magic <code>$$</code> token (lovingly called <code>T_BLING</code>, at least according to yours truly) to inject the left-side result into it.  In that case, the example above would look like this:</p>

<pre><code>$result = $arr
    |&gt; array_column($$, 'tags')
    |&gt; array_merge(...$$)
    |&gt; array_unique($$)
    |&gt; array_values($$)
;
</code></pre>

<p>While powerful, it was also somewhat limiting.  It was very non-standard, unlike any other language.  It also meant a weird, one-off syntax for partially-calling functions that worked only when paired with pipes.</p>

<p>That RFC didn't go as far as a vote.  Nothing much happened for several years, until 2020/2021.  That's when I, fresh off of writing a book on functional programming in PHP that talked about function composition, decided to take a swing at it.  In particular, I partnered with a team to work on <a href="https://wiki.php.net/rfc/partial_function_application">Partial Function Application</a> (PFA) as a separate RFC from a more <a href="https://wiki.php.net/rfc/pipe-operator-v2">traditional pipe</a>.  The idea was that turning a multi-parameter function (like <code>array_column()</code> above) into the single-parameter function that <code>|&gt;</code> needed was a useful feature on its own, and should be usable elsewhere.  The syntax was a bit different than the Hack version, in order to make it more flexible: <code>some_function(?, 5, ?, 3, ...)</code>, which would take a 5-or-more parameter function and turn it into a 3 parameter function.</p>

<p>Sadly, PFA didn't pass due to some engine complexity issues, and that largely undermined the v2 Pipe RFC, too.  However, we did get a consolation prize out of it: <a href="https://wiki.php.net/rfc/first_class_callable_syntax">First Class Callables</a> (the <code>array_values(...)</code> syntax), courtesy Nikita Popov, were by design a "junior", degenerate version of partial function application.</p>

<p>Fast-forward to 2025, and I was sufficiently bored to take another swing at pipes.  This time with a better implementation with lots of hand-holding from Ilija Tovilo and Arnaud Le Blanc, both part of the PHP Foundation dev team, I was able to get it through.</p>

<p>Third time's the charm.</p>

<h2 id="more-than-the-sum-of-its-parts">More than the sum of its parts</h2>

<p>Above, we described pipes as "deceptively simple."  The implementation itself is almost trivial; it's just syntax sugar for the temp variable version, effectively.  However, the best features are the ones that can combine with others or be used in novel ways to punch above their weight.</p>

<p>We saw above how a long array manipulation process could now be condensed into a single chained expression.  Now imagine using that in places where only a single expression is allowed, such as a <code>match()</code>:</p>

<pre><code>$string = 'something GoesHERE';

$newString = match ($format) {
    'snake_case' =&gt; $string
        |&gt; splitString(...)
        |&gt; fn($x) =&gt; implode('_', $x)
        |&gt; strtolower(...),
    'lowerCamel' =&gt; $string
        |&gt; splitString(...),
        |&gt; fn($x) =&gt; array_map(ucfirst(...), $x)
        |&gt; fn($x) =&gt; implode('', $x)
        |&gt; lcfirst(...),
    // Other case options here.
};
</code></pre>

<p>Or, consider that the right-side can also be a function call that returns a <code>Closure</code>.  That means with a few functions that return functions:</p>

<pre><code>$profit = [1, 4, 5] 
    |&gt; loadSeveral(...)
    |&gt; filter(isOnSale(...))
    |&gt; map(sellWidget(...))
    |&gt; array_sum(...);
</code></pre>

<p>Which... gives us mostly the same thing as the long-discussed scalar methods!  Only pipes are more flexible as you can use any function on the right-side, not just those that have been blessed by the language designers as methods.</p>

<p>At this point, pipe comes very close to being "extension functions", a feature of Kotlin and C# that allows writing functions that look like methods on an object, but are actually just stand-alone functions.  It's spelled a bit differently (<code>|</code> instead of <code>-</code>), but it's 75% of the way there, for free.</p>

<p>Or take it a step further.  What if some steps in the pipe may return <code>null</code>?  We can, with a single function, "lift" the elements of our chain to handle <code>null</code> values in the same fashion as null-safe methods.</p>

<pre><code>function maybe(\Closure $c): \Closure
{
    return fn(mixed $arg) =&gt; $arg === null ? null : $c($arg);
}

$profit = [1, 4, 5] 
    |&gt; maybe(loadSeveral(...))
    |&gt; maybe(filter(isOnSale(...)))
    |&gt; maybe(map(sellWidget(...)))
    |&gt; maybe(array_sum(...));
</code></pre>

<p>That's right, we just implemented a Maybe Monad with a pipe and a single-line function.</p>

<p>Now, think about that for streams...</p>

<pre><code>fopen('pipes.md', 'rb') // No variable, so it will close automatically when GCed.
    |&gt; decode_rot13(...)
    |&gt; lines_from_charstream(...)
    |&gt; map(str_getcsv(...))
    |&gt; map(Product::create(...))
    |&gt; map($repo-&gt;save(...))
;
</code></pre>

<p>The potential is absolutely huge.  I don't think it's immodest to say that the pipe operator has one of the highest "bangs for the buck" of any feature in recent memory, alongside such niceties as constructor property promotion.  And all thanks to a little syntax sugar.</p>

<h2 id="what-comes-next%3F">What comes next?</h2>

<p>Although pipes are a major milestone, we're not done.  There is active work on not one but two follow-up RFCs.</p>

<p>The first is a second attempt at <a href="https://wiki.php.net/rfc/partial_function_application_v2">Partial Function Application</a>.  This is a larger feature, but with first-class callables already bringing in much of the necessary plumbing, which simplifies the implementation.  With pipes now providing a natural use case, as well as easy optimization points, it's worth a second attempt.  Whether it makes it into PHP 8.5, is delayed to 8.6, or is again rejected is still an open question as of this writing, though I am hopeful.  Major thanks to Arnaud Le Blanc from the PHP Foundation team for picking it up to update the implementation.</p>

<p>The second is a <a href="https://wiki.php.net/rfc/function-composition">function composition operator</a>.  Where pipe executes immediately, function composition creates a new function by sticking two functions end-to-end.  That would mean the streams example above could be further optimized by combining the <code>map()</code> calls:</p>

<pre><code>fopen('pipes.md', 'rb')
    |&gt; decode_rot13(...)
    |&gt; lines_from_charstream(...)
    |&gt; map(str_getcsv(...) + Product::create(...) + $repo-&gt;save(...))
;
</code></pre>

<p>This one is definitely not going to make it into PHP 8.5, but I am hopeful that we'll be able to get it into 8.6.  Stay tuned.</p>

<blockquote>
  <p>Special thanks to Ilija Tovilo and Arnaud Le Blanc from the PHP Foundation team for their assistance with the pipe implementation.  If you’d like to help push PHP forward, consider <a href="https://thephp.foundation/sponsor/">becoming a sponsor</a>.</p>
</blockquote>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[3D Line Drawings (260 pts)]]></title>
            <link>https://amritkwatra.com/experiments/3d-line-drawings</link>
            <guid>44792441</guid>
            <pubDate>Mon, 04 Aug 2025 23:16:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://amritkwatra.com/experiments/3d-line-drawings">https://amritkwatra.com/experiments/3d-line-drawings</a>, See on <a href="https://news.ycombinator.com/item?id=44792441">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                    <p>
                    This is an experiment examing how to create a 3D line drawing of a scene. In this post, I will describe how this can be done by augmenting the process of generating <span>3D Gaussian Splats</span>
                    <label for="mn-35be8a"></label>
                    
                    <span><a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" target="_blank" rel="noopener noreferrer">3D Gaussian Splatting for Real-Time Radiance Field Rendering</a>, by Kerbl et al.</span> and leveraging a process to transform photographs into <span>Informative Line Drawings</span>
                    <label for="mn-32f931"></label>
                    
                    <span><a href="https://carolineec.github.io/informative_drawings/" target="_blank" rel="noopener noreferrer">Learning to Generate Line Drawings that Convey Geometry and Semantics</a>, by Chan, Isola &amp; Durand</span>.
                </p>
                
                <figure>
                    
                </figure>
                <p>
                The majority of scenes shown above are generated using a <em>contour</em> style. <strong>You can switch the active scene using the menu in the top-right corner of the iframe</strong>. Each scene is trained for 21,000 iterations on an Nvidia RTX 4080S, using <a href="https://github.com/MrNeRF/gaussian-splatting-cuda" target="_blank" rel="noopener noreferrer">
<code>
gaussian-splatting-cuda
                </code>
            </a> from MrNerf with default settings. Examples here were generated using scenes from the <a href="https://www.tanksandtemples.org/" target="_blank" rel="noopener noreferrer">Tanks &amp; Temples Benchmark</a>. The scene is interactive, and rendered using <a href="https://github.com/mkkellogg/GaussianSplats3D" target="_blank" rel="noopener noreferrer">Mark Kellogg's web-based renderer</a>.
            To explore these scenes in fullscreen, <a href="https://splat-serve.pages.dev/?id=caterpillar-contour" target="_blank" rel="noopener noreferrer">click here.</a>
        </p>
        
        <h3>
            Creating Line Drawings from Images
        </h3>
        <figure>
            <label for="mn-figure-60d5a5">⊕</label>
            
            <span>
                <em>Figure 1.</em> <em>(a)</em> A source image from the Caterpillar scene in <a href="https://www.tanksandtemples.org/">Tanks &amp; Temples</a>. <em>(b)</em> A generated line drawing in the <em>contour</em> style. <em>(c)</em> A generated line drawing in the <em>anime</em> style.
            </span>
            <img src="https://amritkwatra.com/assets/3d_line_drawing__fig1-ShImBJCe.png" alt="Figure 1">
        </figure>
        <p>
        Images are transformed into line drawings using the approach introduced by Chan et al. in <a href="https://carolineec.github.io/informative_drawings/" target="_blank" rel="noopener noreferrer">Learning to Generate Line Drawings that Convey Geometry and Semantics</a>. They describe a process for transforming photographs into line drawings that preserve the semantics and geometry captured in the photograph while rendering the image in an artistic style. They do this by training a generative adversarial network (GAN) that minimizes <span>geometry</span>
        <label for="mn-5aa174"></label>
        
        <span>The geometry loss is computed using monocular depth estimation.</span>, <span>semantics</span>
        <label for="mn-7dcc1a"></label>
        
        <span>The semantics loss is computed using CLIP embeddings.</span>, and <span>appearance</span>
        <label for="mn-16f242"></label>
        
        <span>The appearance loss is based on a collection of unpaired style references.</span> losses. Their work is fantastic and I recommend reading their paper if you're interested in the details. Figure 1 depicts the input photograph and output line drawing in two styles, generated using Chan et al.'s <a href="https://github.com/carolineec/informative-drawings" target="_blank" rel="noopener noreferrer">code and model weights</a>.
    </p>
    <h3>
        3D Gaussian Splatting
    </h3>
    <p>
    3D Gaussian Splatting is a technique that transforms collections of <span>posed images</span>
    <label for="mn-5402d3"></label>
    
    <span>Images for which we estimate the relative position and rotation of the camera in the scene. Usually, this is computed using Structure-from-Motion (SfM).</span> into a volumetric representation called a radiance field. Generally, scenes can be created from collections of images captured from multiple overlapping viewpoints, either by taking multiple photographs from different angles or by sampling an input video while moving through the scene. For more complete details on 3D Gaussian Splatting, I encourage you to read <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/" target="_blank" rel="noopener noreferrer">the paper</a> or watch this <a href="https://www.youtube.com/watch?v=VkIJbpdTujE" target="_blank" rel="noopener noreferrer">explanation video</a> from Computerphile.
</p>
<p>
The scenes that are produced using 3D Gaussian Splatting are photorealistic and can be rendered at real-time rates using existing tools such as WebGL. However, I noticed that if the images used to train the 3D Gaussian Splat were swapped out with the <span>line-drawing counterparts</span>
<label for="mn-459d32"></label>

<span>While I transformed the images into line drawings, this transformation could be carried out using a number of other stylistic effects.</span> then the resulting scene would depict a kind of 3D line drawing. Similar to <span>regular sketching</span>
<label for="mn-ea08a3"></label>

<span>Illustrating a 3D scene as a 2D drawing from a specific perspective.</span>, the lines that are rendered are view-dependant and change based on your perspective in the scene.
</p>
<h3>
    Swapping Source Images
</h3>
<figure>
    <label for="mn-figure-5dc885">⊕</label>
    
    <span>
        <em>Figure 2.</em> The transformed images can be used to create 3D line drawings by swapping them for the original images in one of two places, shown here in green. Either before using SfM to generate the sparse model and camera poses or prior to training the 3D Gaussian Splat. The former computes SfM points based on the transformed images, while the latter keeps the points from the original images and computes the rendering loss using the transformed images. Figure adapted from the <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">original 3D Gaussian Splatting paper</a>.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__swap-xknqvGhJ.png" alt="Figure 2">
</figure>
<p>
Generating these 3D line drawings requires only a slight modification to the conventional process for generating 3D Gaussian Splats. At a high level, we simply swap the original images with the ones generated using Chan et al.'s method for transforming images into informative line drawings. This swap can happen in two places: prior to training the 3D Gaussian Splat or prior to estimating camera poses and sparse points using structure-from-motion (SfM).
</p>
<p>
When images are transformed prior to training the 3D Gaussian Splat, the camera poses and initial 3D Gaussians are derived from the original images but the loss is computed by comparing the rasterization output to the informative line drawing. As a result, the final scene also captures slight amounts of color that is not present in the transformed images. I believe this is a side-effect of intializing the 3D Gaussians using the original images. If you transform the images prior to applying SfM, the initialization and camera poses are based on the transformed images and the color artifacts are removed.
</p>
<figure>
    <label for="mn-figure-35efdb">⊕</label>
    
    <span>
        <em>Figure 3.</em> <em>(a)</em> A 3D line drawing created by swapping the transformed images for the original ones prior to using SfM. <em>(b)</em> A 3D line drawing generated by swapping the transformed images for the original ones prior to training the 3D Gaussian Splat.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__pre-sfm-pre-train-DdbSCa4M.png" alt="Figure 3">
</figure>
<p>
If you replace the images prior to applying SfM, there is less information to use when generating a sparse model and estimating camera poses. As a result, the process is more reliable when images are replaced prior to training the 3D Gaussian Splat. Additionally, if you're iterating on different styles and tweaking the transformation process, swapping prior to training the 3D Gaussian Splat lets you avoid re-running COLMAP (or other SfM tools) for each iteration. For the majority of my examples, I swapped the images prior to training the 3D Gaussian Splat. Figure 3 shows a comparison between scenes generated using these two methods.
</p>

<figure>
    <label for="mn-figure-2b9fff">⊕</label>
    
    <span>
        <em>Figure 4.</em> <em>(a)</em> A 3D line drawing created by swapping the original images with informative line drawings prior to training the 3D Gaussian Splat. Note that the slight amount of color is an artifact that is likely the result of initializing the 3D Gaussians from the original set of images. <em>(b)</em> A 3D line drawing that blends low-frequency color information from the original scene to produce a watercolor-like effect.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__color_comp-Ck96zCOw.png" alt="Figure 4">
</figure>
<p>
I wanted to see how we could add some color information back into the generated line drawings and ultimately into the generated scenes. To do this, I generated a slightly modified <span>hybrid image</span>
<label for="mn-a23a0a"></label>

<span>"A hybrid image is a picture that combines the low-spatial frequencies of one picture with the high spatial frequencies of another picture" - <a href="https://web.stanford.edu/class/ee367/reading/OlivaTorralb_Hybrid_Siggraph06.pdf" target="_blank" rel="noopener noreferrer">Oliva, Torralba &amp; Schyns</a> </span> which blends color information from the original image into the line drawing to create a watercolor effect in the final image. Figure 4 shows the contour image and the blended hybrid image. There are a few scenes in the interactive example above that use this method of adding color, such as the <a href="https://splat-serve.pages.dev/?id=caterpillar-color" target="_blank" rel="noopener noreferrer">Blended Caterpillar Scene</a>, and <a href="https://splat-serve.pages.dev/?id=lighthouse-color" target="_blank" rel="noopener noreferrer">Blended Lighthouse Scene</a>.
</p>
<figure>
    <label for="mn-figure-03a07b">⊕</label>
    
    <span>
        <em>Figure 5.</em> A video orbiting around the spliced Caterpillar scene. In this scene, viewpoints from one hemisphere of the scene render the Blended Color Contour Caterpillar Scene, and viewpoints from the other hemisphere render the source photorealistic Caterpillar Scene. Orbitting around the scene demonstrates how style can gradually change as you move through the scene.
    </span>
    <video width="100%" controls="" autoplay="" loop="" muted="" playsinline="">
        <source src="https://splats.amritkwatra.com/real-hybrid-blend-orbit-web-2x.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </figure>
    <p>
    A subtle, but interesting effect can be achieved by splicing together the source scene and color scene. By training a 3D Gaussian Splat to reconstruct photorealistic images from one set of perspectives and colored, contour images from another distinct set of perspectives, you can gradually transition styles within the same scene based on the viewing perspective. Figure 5 demonstrates this by orbiting around a scene in the <a href="https://splat-serve.pages.dev/?id=caterpillar-real-hybrid-blend" target="_blank" rel="noopener noreferrer">Spliced Caterpillar Scene</a>.
</p>

<figure>
    <label for="mn-figure-ce6432">⊕</label>
    
    <span>
        <em>Figure 6.</em> A <em>collage</em> image that segments out an object (in this example the tractor) in a scene and replaces it with the corresponding object from the stylized scene. This replacement, or collage, happens in image space prior to training the Gaussian Splat. <em>(a)</em> Inserts the <em>contour</em> style into the original scene. <em>(b)</em> Inserts the original scene into the stylized <em>contour</em> scene.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__collage-D6cuP6A0.png" alt="Figure 6">
</figure>
<p>
Another experiment I tried was to create a <em>collage</em> scene that renders only the subject of the scene as a line drawing and leaves the background unaltered. To do this I used a Meta's <a href="https://github.com/facebookresearch/segment-anything/tree/main?tab=readme-ov-file#model-checkpoints" target="_blank" rel="noopener noreferrer">Segment-Anything Model</a> <span>(SAM)</span>
<label for="mn-f88186"></label>

<span>To avoid manually specifying areas in the image to segment using SAM, I used <a href="https://github.com/luca-medeiros/lang-segment-anything" target="_blank" rel="noopener noreferrer">LangSAM</a> to automatically segment images based on a text prompt.</span>  to mask out the subject in each input photograph and replace it with the corresponding area from the generated line drawing image. The resulting scene is depicted in Figure 6 and can be explored here: <a href="https://splat-serve.pages.dev/?id=caterpillar-collage" target="_blank" rel="noopener noreferrer">Caterpillar Collage Scene</a>.
</p>
<!--
<figure>
    <label for="mn-figure-9039c0" class="margin-toggle">&#8853;</label>
    <input type="checkbox" id="mn-figure-9039c0" class="margin-toggle"/>
    <span class="marginnote">
        <em>Figure 6.</em> A <em>collage</em> image that segments out multiple objects (in this example, benches) in a scene and replaces it with the corresponding object from the stylized scene.
    </span>
    <img src="../imgs/3d_line_drawing__collage_multiple.png" alt="Figure 6" />
</figure>
This method can also render multiple subjects in the scene in different styles. Figure 6 shows multiple objects (benches) being rendered as line drawings within a scene. View the full scene here: [Family Collage Scene](https://splat-serve.pages.dev/?id=family-collage). It is also possible to mix and match different styles applied to different objects in the same scene. -->

<figure>
    <label for="mn-figure-fb84bd">⊕</label>
    
    <span>
        <em>Figure 7.</em> The same scene rendered by line drawings at two output resolutions. In each scene the inset image is one of the transformed images used to generate the scene. <em>(a)</em> <a href="https://splat-serve.pages.dev/?id=caterpillar-contour-256p">460x256 pixels</a>. <em>(b)</em> <a href="https://splat-serve.pages.dev/?id=caterpillar-contour">1940x1080 pixels</a>.
    </span>
    <img src="https://amritkwatra.com/assets/3d_line_drawing__resolution__inset-BSGYWx8Y.png" alt="Figure 7">
</figure>
<p>
The scenes so far have been trained on high resolution images (~1080p). This not only effects how long it takes to train the 3D Gaussian Splat, but also the fidelity of details captured in the line drawing. The lower resolution outputs capture the major lines that define the shape of the subject in the scene, while higher resolution outputs can pick up on more minor details in the scene. Figure 7 shows the same scene rendered at a variety of resolutions.
</p>
<p>
The following table describes the training time and number of splats generated for line drawings generated in a range of resolutions. Each scene is trained using the same parameters for 21,000 iterations. The final row describes the original scene using images from the Caterpillar scene in the Tanks and Temples Benchmark.
</p>
<div>
    <table>
        <thead>
            <tr>
                <th>Resolution</th>
                <th>Training Time</th>
                <th>Number of Splats</th>
                <th>Uncompressed File Size (.ply)</th>
                <th>Compressed File Size (.ksplat)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-contour-128p">232x128</a></td>
                <td>3m 38s</td>
                <td>662,428</td>
                <td>164 MB</td>
                <td>14.1 MB</td>
            </tr>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-contour-256p">460x256</a></td>
                <td>4m 31s</td>
                <td>1,105,383</td>
                <td>274 MB</td>
                <td>24.5 MB</td>
            </tr>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-contour-512p">920x512</a></td>
                <td>6m 48s</td>
                <td>1,611,563</td>
                <td>400 MB</td>
                <td>36.4 MB</td>
            </tr>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-contour">1940x1080</a></td>
                <td>15m 27s</td>
                <td>2,046,676</td>
                <td>507 MB</td>
                <td>46 MB</td>
            </tr>
            <tr>
                <td><a href="https://splat-serve.pages.dev/?id=caterpillar-original">Original (1957x1090)</a></td>
                <td>15m 9s</td>
                <td>900,798</td>
                <td>223 MB</td>
                <td>21 MB</td>
            </tr>
        </tbody></table>
    </div>
    <p>
    It is notable that a line drawing scene is roughly double the size of it's source scene in both number of splats and file size. I hypothesize that this is because splats are better suited at modelling large areas and textures than they are at strokes. As a result, a scene of a 3D line drawing must use more individual gaussians to render long, thin lines in the scene.
</p>

<p>
The code to generate these is a mashup of scripts to orchestrate between the different libraraies referenced in this post. Contact me if you're curious about running this yourself.
</p>

<p>
If you have thoughts about this work or would like to collaborate, I would love to hear from you. You can contact me at:
<code>
tansh at amritkwatra dot com
</code>
</p>

<p>
Special thanks to Ritik Batra, Ilan Mandel &amp; Thijs Roumen for their feedback and suggestions. This page was created using the open-source <a href="https://github.com/tansh-kwa/tufte-project-template" target="_blank" rel="noopener noreferrer">Tufte Project Pages</a> template.
</p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I've been building an ERP for manufacturing for the last 3 years (270 pts)]]></title>
            <link>https://github.com/crbnos/carbon</link>
            <guid>44792005</guid>
            <pubDate>Mon, 04 Aug 2025 22:24:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/crbnos/carbon">https://github.com/crbnos/carbon</a>, See on <a href="https://news.ycombinator.com/item?id=44792005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
   <a href="https://carbon.ms/" rel="nofollow">
      <img width="auto" height="100" alt="Carbon Logo" src="https://private-user-images.githubusercontent.com/64510427/465005356-86a5e583-adac-4bf9-8192-508a0adf2308.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDUzNTYtODZhNWU1ODMtYWRhYy00YmY5LTgxOTItNTA4YTBhZGYyMzA4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWFiNjZkMTZlYTBjMmE1Y2Y2ODM1ZGMxNjU0ODRlMzEwN2QyOGZkZmM5NWM2Zjk3YTk5YTc2ZGQ4Y2NlNzI1YWYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.qGm2vI_fgIS_CX3858gOiYnZevpLmnJQaPk-14G2y50" secured-asset-link="">
   </a>
</p>


<p dir="auto">
  <a href="https://go.midday.ai/K7GwMoQ" rel="nofollow">
    <img src="https://camo.githubusercontent.com/0d69bd5f3e56aabad0d1a0869a2bbf559648e8fde5478d9fa6331f424a097618/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53757061626173652d3345434638453f7374796c653d666f722d7468652d6261646765266c6f676f3d7375706162617365266c6f676f436f6c6f723d7768697465" alt="Supabase" data-canonical-src="https://img.shields.io/badge/Supabase-3ECF8E?style=for-the-badge&amp;logo=supabase&amp;logoColor=white">
  </a>
</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/64510427/465004791-2e09b891-d5e2-4f68-b924-a1c8ea42d24d.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDQ3OTEtMmUwOWI4OTEtZDVlMi00ZjY4LWI5MjQtYTFjOGVhNDJkMjRkLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWUxODI0ZDhiOGQxZGFjNGJlYzlhYTY2MThlZWU0YTM4YmIwNzU4NGY5ZDQ5MGFjNDIyZTg3N2NjM2EwMzk5NmQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iXA9k7MJN4NDQx4-7ujQxmuXYLommM6cNM59cW9kcjo"><img src="https://private-user-images.githubusercontent.com/64510427/465004791-2e09b891-d5e2-4f68-b924-a1c8ea42d24d.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDQ3OTEtMmUwOWI4OTEtZDVlMi00ZjY4LWI5MjQtYTFjOGVhNDJkMjRkLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWUxODI0ZDhiOGQxZGFjNGJlYzlhYTY2MThlZWU0YTM4YmIwNzU4NGY5ZDQ5MGFjNDIyZTg3N2NjM2EwMzk5NmQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.iXA9k7MJN4NDQx4-7ujQxmuXYLommM6cNM59cW9kcjo" alt="ERP Screenshot"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/64510427/465004825-b04f3644-91aa-4f74-af8d-6f3e12116a6b.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDQ4MjUtYjA0ZjM2NDQtOTFhYS00Zjc0LWFmOGQtNmYzZTEyMTE2YTZiLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ1NmVhYzg5MmY2ODhkYTUwZDQ1YzJhNGExNWQ2N2Q2NGJlNDIzYTdhN2Y4NzA1ZWUzZDgxMDYxZDQ2ZTAxMzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.9v_tr9MxKAgyv7W8sNm5RmA7LO3JlQ_9-la1xIVVyoo"><img src="https://private-user-images.githubusercontent.com/64510427/465004825-b04f3644-91aa-4f74-af8d-6f3e12116a6b.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NjUwMDQ4MjUtYjA0ZjM2NDQtOTFhYS00Zjc0LWFmOGQtNmYzZTEyMTE2YTZiLmpwZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ1NmVhYzg5MmY2ODhkYTUwZDQ1YzJhNGExNWQ2N2Q2NGJlNDIzYTdhN2Y4NzA1ZWUzZDgxMDYxZDQ2ZTAxMzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.9v_tr9MxKAgyv7W8sNm5RmA7LO3JlQ_9-la1xIVVyoo" alt="MES Screenshot"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does the world need another ERP?</h2><a id="user-content-does-the-world-need-another-erp" aria-label="Permalink: Does the world need another ERP?" href="#does-the-world-need-another-erp"></a></p>
<p dir="auto">We built Carbon after years of building end-to-end manufacturing systems with off-the-shelf solutions. We realized that:</p>
<ul dir="auto">
<li>Modern, API-first tooling didn't exist</li>
<li>Vendor lock-in bordered on extortion</li>
<li>There is no "perfect ERP" because each company is unique</li>
</ul>
<p dir="auto">We built Carbon to solve these problems ☝️.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">Carbon is designed to make it easy for you to extend the platform by building your own apps through our API. We provide some examples to get you started in the <a href="https://github.com/crbnos/carbon/blob/main/examples">examples</a> folder.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/64510427/445149654-ed6dc66b-e9cb-435e-b5a9-9daf933f4a1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NDUxNDk2NTQtZWQ2ZGM2NmItZTljYi00MzVlLWI1YTktOWRhZjkzM2Y0YTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyMjYzOTE5OTg4Yjc0MThkMjI4Yzg0ZDNkY2ExOWVlYTM3ZTAxMjk3NTc4MDU3YmM2M2JkZGIwZTY3NjU1MmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.7r7Af605IQ1B4mUp-fSqs-VojI1soLKXUZD3FYkaZ2c"><img src="https://private-user-images.githubusercontent.com/64510427/445149654-ed6dc66b-e9cb-435e-b5a9-9daf933f4a1d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQzNjEzMDIsIm5iZiI6MTc1NDM2MTAwMiwicGF0aCI6Ii82NDUxMDQyNy80NDUxNDk2NTQtZWQ2ZGM2NmItZTljYi00MzVlLWI1YTktOWRhZjkzM2Y0YTFkLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA4MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwODA1VDAyMzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgyMjYzOTE5OTg4Yjc0MThkMjI4Yzg0ZDNkY2ExOWVlYTM3ZTAxMjk3NTc4MDU3YmM2M2JkZGIwZTY3NjU1MmYmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.7r7Af605IQ1B4mUp-fSqs-VojI1soLKXUZD3FYkaZ2c" alt="Carbon Architecture"></a></p>
<p dir="auto">Features:</p>
<ul>
<li> ERP</li>
<li> MES</li>
<li> QMS</li>
<li> Custom Fields</li>
<li> Nested BoM</li>
<li> Traceability</li>
<li> MRP</li>
<li> Configurator</li>
<li> MCP Client/Server</li>
<li> API</li>
<li> Webhooks</li>
<li> Accounting</li>
<li> Capacity Planning</li>
<li> Simulation</li>
</ul>
<p dir="auto">Technical highlights:</p>
<ul>
<li> Unified auth and permissions across apps</li>
<li> Full-stack type safety (Database → UI)</li>
<li> Realtime database subscriptions</li>
<li> Attribute-based access control (ABAC)</li>
<li> Role-based access control (Customer, Supplier, Employee)</li>
<li> Row-level security (RLS)</li>
<li> Composable user groups</li>
<li> Dependency graph for operations</li>
<li> Third-party integrations</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Techstack</h2><a id="user-content-techstack" aria-label="Permalink: Techstack" href="#techstack"></a></p>
<ul dir="auto">
<li><a href="https://remix.run/" rel="nofollow">Remix</a> – framework</li>
<li><a href="https://www.typescriptlang.org/" rel="nofollow">Typescript</a> – language</li>
<li><a href="https://tailwindcss.com/" rel="nofollow">Tailwind</a> – styling</li>
<li><a href="https://radix-ui.com/" rel="nofollow">Radix UI</a> - behavior</li>
<li><a href="https://supabase.com/" rel="nofollow">Supabase</a> - database</li>
<li><a href="https://supabase.com/" rel="nofollow">Supabase</a> – auth</li>
<li><a href="https://upstash.com/" rel="nofollow">Upstash</a> - cache</li>
<li><a href="https://trigger.dev/" rel="nofollow">Trigger</a> - jobs</li>
<li><a href="https://resend.com/" rel="nofollow">Resend</a> – email</li>
<li><a href="https://novu.co/" rel="nofollow">Novu</a> – notifications</li>
<li><a href="https://vercel.com/" rel="nofollow">Vercel</a> – hosting</li>
<li><a href="https://stripe.com/" rel="nofollow">Stripe</a> - billing</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Codebase</h2><a id="user-content-codebase" aria-label="Permalink: Codebase" href="#codebase"></a></p>
<p dir="auto">The monorepo follows the Turborepo convention of grouping packages into one of two folders.</p>
<ol dir="auto">
<li><code>/apps</code> for applications</li>
<li><code>/packages</code> for shared code</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>/apps</code></h3><a id="user-content-apps" aria-label="Permalink: /apps" href="#apps"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Package Name</th>
<th>Description</th>
<th>Local Command</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>erp</code></td>
<td>ERP Application</td>
<td><code>npm run dev</code></td>
</tr>
<tr>
<td><code>mes</code></td>
<td>MES</td>
<td><code>npm run dev:mes</code></td>
</tr>
<tr>
<td><code>academy</code></td>
<td>Academy</td>
<td><code>npm run dev:academy</code></td>
</tr>
<tr>
<td><code>starter</code></td>
<td>Starter</td>
<td><code>npm run dev:starter</code></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto"><code>/packages</code></h3><a id="user-content-packages" aria-label="Permalink: /packages" href="#packages"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Package Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>eslint-config-carbon</code></td>
<td>Shared, extendable eslint configuration for apps and packages</td>
</tr>
<tr>
<td><code>@carbon/database</code></td>
<td>Database schema, migrations and types</td>
</tr>
<tr>
<td><code>@carbon/documents</code></td>
<td>Transactional PDFs and email templates</td>
</tr>
<tr>
<td><code>@carbon/integrations</code></td>
<td>Integration definitions and configurations</td>
</tr>
<tr>
<td><code>@carbon/jest</code></td>
<td>Jest preset configuration shared across apps and packages</td>
</tr>
<tr>
<td><code>@carbon/jobs</code></td>
<td>Background jobs and workers</td>
</tr>
<tr>
<td><code>@carbon/logger</code></td>
<td>Shared logger used across apps</td>
</tr>
<tr>
<td><code>@carbon/react</code></td>
<td>Shared web-based UI components</td>
</tr>
<tr>
<td><code>@carbon/kv</code></td>
<td>Redis cache client</td>
</tr>
<tr>
<td><code>@carbon/lib</code></td>
<td>Third-party client libraries (slack, resend)</td>
</tr>
<tr>
<td><code>@carbon/stripe</code></td>
<td>Stripe integration</td>
</tr>
<tr>
<td><code>@carbon/tsconfig</code></td>
<td>Shared, extendable tsconfig configuration used across apps and packages</td>
</tr>
<tr>
<td><code>@carbon/utils</code></td>
<td>Shared utility functions used across apps and packages</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setup</h3><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repo into a public GitHub repository (or fork <a href="https://github.com/crbnos/carbon/fork">https://github.com/crbnos/carbon/fork</a>). If you plan to distribute the code, keep the source code public to comply with <a href="https://github.com/crbnos/carbon/blob/main/LICENSE">AGPLv3</a>. To clone in a private repository, <a href="https://carbon.ms/sales" rel="nofollow">acquire a commercial license</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/crbnos/carbon.git"><pre>git clone https://github.com/crbnos/carbon.git</pre></div>
</li>
<li>
<p dir="auto">Go to the project folder</p>

</li>
</ol>
<p dir="auto">Make sure that you have <a href="https://docs.docker.com/desktop/install/mac-install/" rel="nofollow">Docker installed</a> on your system since this monorepo uses the Docker for local development.</p>
<p dir="auto">In addition you must configure the following external services:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Service</th>
<th>Purpose</th>
<th>URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>Upstash</td>
<td>Serverless Redis</td>
<td><a href="https://console.upstash.com/login" rel="nofollow">https://console.upstash.com/login</a></td>
</tr>
<tr>
<td>Trigger.dev</td>
<td>Job runner</td>
<td><a href="https://cloud.trigger.dev/login" rel="nofollow">https://cloud.trigger.dev/login</a></td>
</tr>
<tr>
<td>Posthog</td>
<td>Product analytics platform</td>
<td><a href="https://us.posthog.com/signup" rel="nofollow">https://us.posthog.com/signup</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Each of these services has a free tier which should be plenty to support local development. If you're self hosting, and you don't want to use Upstash or Posthog, it's pretty easy to replace upstash with a redis container in <code>@carbon/kv</code> and remove the Posthog analytics.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">First download and initialize the repository dependencies.</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ nvm use           # use node v20
$ npm install       # install dependencies
$ npm run db:start  # pull and run the containers"><pre>$ nvm use           <span><span>#</span> use node v20</span>
$ npm install       <span><span>#</span> install dependencies</span>
$ npm run db:start  <span><span>#</span> pull and run the containers</span></pre></div>
<p dir="auto">Create an <code>.env</code> file and copy the contents of <code>.env.example</code> file into it</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ cp ./.env.example ./.env"><pre>$ cp ./.env.example ./.env</pre></div>
<ol dir="auto">
<li>Use the output of <code>npm run db:start</code> to set the supabase entries:</li>
</ol>
<ul dir="auto">
<li><code>SUPABASE_SERVICE_ROLE_KEY=[service_role key]</code></li>
<li><code>SUPABASE_ANON_KEY=[anon key]</code></li>
</ul>
<ol start="2" dir="auto">
<li><a href="https://console.upstash.com/redis" rel="nofollow">Create a Redis database in upstash</a> and copy the following from the <code>REST API</code> section:</li>
</ol>
<ul dir="auto">
<li><code>UPSTASH_REDIS_REST_URL=[UPSTASH_REDIS_REST_URL]</code></li>
<li><code>UPSTASH_REDIS_REST_TOKEN=[UPSTASH_REDIS_REST_TOKEN]</code></li>
</ul>
<ol start="3" dir="auto">
<li>Navigate to the project you created in <a href="https://github.com/crbnos/carbon/blob/main/Trigger.dev">https://cloud.trigger.dev/</a> and copy the following from the <code>Environments &amp; API Keys</code> section:</li>
</ol>
<ul dir="auto">
<li><code>TRIGGER_PUBLIC_API_KEY=[Public 'dev' API Key, starting 'pk_dev*']</code></li>
<li><code>TRIGGER_API_KEY=[Server 'dev' API Key, starting 'tr_dev*']</code></li>
</ul>
<ol start="4" dir="auto">
<li>In Posthog go to <a href="https://%5Bregion%5D.posthog.com/project/%5Bproject-id%5D/settings/project-details" rel="nofollow">https://[region].posthog.com/project/[project-id]/settings/project-details</a> to find your Project ID and Project API key:</li>
</ol>
<ul dir="auto">
<li><code>POSTHOG_API_HOST=[https://[region].posthog.com]</code></li>
<li><code>POSTHOG_PROJECT_PUBLIC_KEY=[Project API Key starting 'phc*']</code></li>
</ul>
<ol start="5" dir="auto">
<li>Add a <code>STRIPE_SECRET_KEY</code> from the Stripe admin interface, and then run <code>npm run -w @carbon/stripe register:stripe</code> to get a <code>STRIP_WEBHOOK_SECRET</code></li>
</ol>
<ul dir="auto">
<li><code>STRIPE_SECRET_KEY="sk_test_*************"</code></li>
<li><code>STRIP_WEBHOOK_SECRET="whsec_************"</code></li>
</ul>
<p dir="auto">Then you can run the following:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm run db:build     # run db migrations and seed script
$ npm run build        # build the packages"><pre>$ npm run db:build     <span><span>#</span> run db migrations and seed script</span>
$ npm run build        <span><span>#</span> build the packages</span></pre></div>
<p dir="auto">Finally, start the apps and packages:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm run dev
$ npm run dev:mes        # npm run dev in all apps &amp; packages"><pre>$ npm run dev
$ npm run dev:mes        <span><span>#</span> npm run dev in all apps &amp; packages</span></pre></div>
<p dir="auto">You can now sign in with:</p>
<p dir="auto">username: <a href="mailto:your-email@address.com">your-email@address.com</a>
password: carbon</p>
<p dir="auto">After installation you should be able run the apps locally.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Application</th>
<th>URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>ERP</td>
<td><a href="http://localhost:3000/" rel="nofollow">http://localhost:3000</a></td>
</tr>
<tr>
<td>MES</td>
<td><a href="http://localhost:3001/" rel="nofollow">http://localhost:3001</a></td>
</tr>
<tr>
<td>Academy</td>
<td><a href="http://localhost:4111/" rel="nofollow">http://localhost:4111</a></td>
</tr>
<tr>
<td>Starter</td>
<td><a href="http://localhost:4000/" rel="nofollow">http://localhost:4000</a></td>
</tr>
<tr>
<td>Postgres</td>
<td>postgresql://postgres:postgres@localhost:54322/postgres</td>
</tr>
<tr>
<td>Supabase Studio</td>
<td><a href="http://localhost:54323/project/default" rel="nofollow">http://localhost:54323/project/default</a></td>
</tr>
<tr>
<td>Mailpit</td>
<td><a href="http://localhost:54324/" rel="nofollow">http://localhost:54324</a></td>
</tr>
<tr>
<td>Edge Functions</td>
<td><a href="http://localhost:54321/functions/v1/%3Cfunction-name%3E" rel="nofollow">http://localhost:54321/functions/v1/</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Notes</h3><a id="user-content-notes" aria-label="Permalink: Notes" href="#notes"></a></p>
<p dir="auto">To kill the database containers in a non-recoverable way, you can run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm run db:kill   # stop and delete all database containers"><pre>$ npm run db:kill   <span><span>#</span> stop and delete all database containers</span></pre></div>
<p dir="auto">To restart and reseed the database, you can run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm run db:build # runs db:kill, db:start, and setup"><pre>$ npm run db:build <span><span>#</span> runs db:kill, db:start, and setup</span></pre></div>
<p dir="auto">To run a particular application, use the <code>-w workspace</code> flag.</p>
<p dir="auto">For example, to run test command in the <code>@carbon/react</code> package you can run:</p>
<div data-snippet-clipboard-copy-content="$ npm run test -w @carbon/react"><pre><code>$ npm run test -w @carbon/react
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">API</h2><a id="user-content-api" aria-label="Permalink: API" href="#api"></a></p>
<p dir="auto">The API documentation is located in the ERP app at <code>${ERP}/x/api/js/intro</code>. It is auto-generated based on changes to the database.</p>
<p dir="auto">There are two ways to use the API:</p>
<ol dir="auto">
<li>From another codebase using a supabase client library:</li>
</ol>
<ul dir="auto">
<li><a href="https://supabase.com/docs/reference/javascript/introduction" rel="nofollow">Javascript</a></li>
<li><a href="https://supabase.com/docs/reference/dart/introduction" rel="nofollow">Flutter</a></li>
<li><a href="https://supabase.com/docs/reference/python/introduction" rel="nofollow">Python</a></li>
<li><a href="https://supabase.com/docs/reference/csharp/introduction" rel="nofollow">C#</a></li>
<li><a href="https://supabase.com/docs/reference/swift/introduction" rel="nofollow">Swift</a></li>
<li><a href="https://supabase.com/docs/reference/kotlin/introduction" rel="nofollow">Kotlin</a></li>
</ul>
<ol start="2" dir="auto">
<li>From within the codebase using our packages.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">From another Codebase</h3><a id="user-content-from-another-codebase" aria-label="Permalink: From another Codebase" href="#from-another-codebase"></a></p>
<p dir="auto">Navigate to settings in the ERP to generate an API key. If you're self-hosting you can also use the supabase service key instead of the public key for root access. In that case you don't needto include the <code>carbon-key</code> header.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import { Database } from &quot;@carbon/database&quot;;
import { createClient } from &quot;@supabase/supabase-js&quot;;

const apiKey = process.env.CARBON_API_KEY;
const apiUrl = process.env.CARBON_API_URL;
const publicKey = process.env.CARBON_PUBLIC_KEY;

const carbon = createClient<Database>(apiUrl, publicKey, {
  global: {
    headers: {
      &quot;carbon-key&quot;: apiKey,
    },
  },
});

// returns items from the company associated with the api key
const { data, error } = await carbon.from(&quot;item&quot;).select(&quot;*&quot;);"><pre><span>import</span> <span>{</span> <span>Database</span> <span>}</span> <span>from</span> <span>"@carbon/database"</span><span>;</span>
<span>import</span> <span>{</span> <span>createClient</span> <span>}</span> <span>from</span> <span>"@supabase/supabase-js"</span><span>;</span>

<span>const</span> <span>apiKey</span> <span>=</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>CARBON_API_KEY</span><span>;</span>
<span>const</span> <span>apiUrl</span> <span>=</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>CARBON_API_URL</span><span>;</span>
<span>const</span> <span>publicKey</span> <span>=</span> <span>process</span><span>.</span><span>env</span><span>.</span><span>CARBON_PUBLIC_KEY</span><span>;</span>

<span>const</span> <span>carbon</span> <span>=</span> <span>createClient</span><span>&lt;</span><span>Database</span><span>&gt;</span><span>(</span><span>apiUrl</span><span>,</span> <span>publicKey</span><span>,</span> <span>{</span>
  <span>global</span>: <span>{</span>
    <span>headers</span>: <span>{</span>
      <span>"carbon-key"</span>: <span>apiKey</span><span>,</span>
    <span>}</span><span>,</span>
  <span>}</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// returns items from the company associated with the api key</span>
<span>const</span> <span>{</span> data<span>,</span> error <span>}</span> <span>=</span> <span>await</span> <span>carbon</span><span>.</span><span>from</span><span>(</span><span>"item"</span><span>)</span><span>.</span><span>select</span><span>(</span><span>"*"</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">From the Monorepo</h3><a id="user-content-from-the-monorepo" aria-label="Permalink: From the Monorepo" href="#from-the-monorepo"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { getCarbonServiceRole } from &quot;@carbon/auth&quot;;
const carbon = getCarbonServiceRole();

// returns all items across companies
const { data, error } = await carbon.from(&quot;item&quot;).select(&quot;*&quot;);

// returns items from a specific company
const companyId = &quot;xyz&quot;;
const { data, error } = await carbon
  .from(&quot;item&quot;)
  .select(&quot;*&quot;)
  .eq(&quot;companyId&quot;, companyId);"><pre><span>import</span> <span>{</span> <span>getCarbonServiceRole</span> <span>}</span> <span>from</span> <span>"@carbon/auth"</span><span>;</span>
<span>const</span> <span>carbon</span> <span>=</span> <span>getCarbonServiceRole</span><span>(</span><span>)</span><span>;</span>

<span>// returns all items across companies</span>
<span>const</span> <span>{</span> data<span>,</span> error <span>}</span> <span>=</span> <span>await</span> <span>carbon</span><span>.</span><span>from</span><span>(</span><span>"item"</span><span>)</span><span>.</span><span>select</span><span>(</span><span>"*"</span><span>)</span><span>;</span>

<span>// returns items from a specific company</span>
<span>const</span> <span>companyId</span> <span>=</span> <span>"xyz"</span><span>;</span>
<span>const</span> <span>{</span> data<span>,</span> error <span>}</span> <span>=</span> <span>await</span> <span>carbon</span>
  <span>.</span><span>from</span><span>(</span><span>"item"</span><span>)</span>
  <span>.</span><span>select</span><span>(</span><span>"*"</span><span>)</span>
  <span>.</span><span>eq</span><span>(</span><span>"companyId"</span><span>,</span> <span>companyId</span><span>)</span><span>;</span></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thingino: Open-Source Firmware for IP Cameras (236 pts)]]></title>
            <link>https://thingino.com/</link>
            <guid>44791984</guid>
            <pubDate>Mon, 04 Aug 2025 22:22:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thingino.com/">https://thingino.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44791984">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      

      <h2>Supported Hardware</h2>

      <p>Please note that we list not only the camera model, but also its SoC, image sensor, Wi-Fi module, and flash chip size.
        These must match to be supported by the firmware. We have found that some manufacturers change the hardware in different
        batches of the same module without notice.</p>

      <h3>Indoor IP Cameras</h3>

      <div>
        <dl>
          <dt>360 AP1PA3</dt>
          <dd><img src="https://thingino.com/a/cam/360-ap1pa3.webp" alt="360 AP1PA3"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-360_ap1pa3_t31x_gc4653_atbm6031.bin">T31X, GC4653, ATBM6031, 16MB</a></dd>
        </dl>
        <dl>
          <dt>AJCloud T-CP2011-W32A
          </dt><dd><img src="https://thingino.com/a/cam/ajcloud-cp2011.webp" alt="AJCloud CP2011"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-ajcloud_cp2011_t23n_cs2336_atbm6132bu.bin">T23N, SC2336, ATBM6132BU, 8MB</a></dd>
        </dl>
        <dl>
          <dt>AJCloud T-CP8010TF-W3M
          </dt><dd><img src="https://thingino.com/a/cam/ajcloud-cp8010.webp" alt="AJCloud CP8010"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-ajcloud_cp8010_t23n_cs2336p_atbm6132bu.bin">T23N, SC2336P, ATBM6132BU, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Aobocam A12</dt>
          <dd><img src="https://thingino.com/a/cam/aobocam-a12.webp" alt="Aobocam A12"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-aobocam_a12_t23dl_jxh63p_txw901u.bin">T23DL, JHX63P, TXW901U, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Aoqee C1</dt>
          <dd><img src="https://thingino.com/a/cam/aoqee-c1.webp" alt="Aoqee C1"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-aoqee_c1_t23n_sc2336_atbm6062.bin">T23N, SC2336, ATBM6062, 8MB</a></dd>
        </dl>
        <dl>
          <dt>ATOM Cam 1</dt>
          <dd><img src="https://thingino.com/a/cam/atom-cam-1.webp" alt="ATOM Cam 1"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam2_t20x_jxf22_rtl8189ftv.bin">T20X, JXF22, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam2_t20x_jxf23_rtl8189ftv.bin">T20X, JXF23, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>ATOM Cam 2</dt>
          <dd><img src="https://thingino.com/a/cam/atom-cam-2.webp" alt="ATOM Cam 2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-atom_cam2_t31x_gc2053_atbm6031.bin">T31X, GC2053, ATBM6031, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Cinnado D1</dt>
          <dd><img src="https://thingino.com/a/cam/cinnado-d1.webp" alt="Cinnado D1"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-cinnado_d1_t23n_sc2336_atbm6012bx.bin">T23N, SC2336, ATBM6012BX, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-cinnado_d1_t31l_sc2336_atbm6031.bin">T31L, SC2336, ATBM6031, 8MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Eufy C120 (T8400X)</dt>
          <dd><img src="https://thingino.com/a/cam/eufy-c120.webp" alt="Eufy C120 (T8400X)"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8400x_t31x_sc3235_syn4343.bin">T31X, SC3235, SYN4343, 32MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8400x_t31x_sc3338_syn4343.bin">T31X, SC3338, SYN4343, 32MB</a></dd>
        </dl>
        <dl>
          <dt>Eufy E220 (T8410C/X)</dt>
          <dd><img src="https://thingino.com/a/cam/eufy-t8410x.webp" alt="Eufy E220 (T8410X)"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8410c_t31x_sc3338_syn4343.bin">T31X, SC3338, ATBM6031X, 32MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8410x_t31x_sc3235_syn4343.bin">T31X, SC3235, SYN4343, 32MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8410x_t31x_sc3335_syn4343.bin">T31X, SC3335, SYN4343, 32MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8410x_t31x_sc3336_syn4343.bin">T31X, SC3336, SYN4343, 32MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8410x_t31x_sc3338_syn4343.bin">T31X, SC3338, SYN4343, 32MB</a></dd>
        </dl>
        <dl>
          <dt>eLife ET-N3431H-DW</dt>
          <dd><img src="https://thingino.com/a/cam/elife-et-n4341h-dw.webp" alt="eLife ET-N3431H-DW"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-elife_etn3431hdw_t31x_os03b10_ssv6155.bin">T31X, OS03B10, SSV6155, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Galayou G2</dt>
          <dd><img src="https://thingino.com/a/cam/galayou-g2.webp" alt="Galayou G2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-galayou_g2_t23n_sc2336_atbm6012bx.bin">T23N, SC2336, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Galayou G7</dt>
          <dd><img src="https://thingino.com/a/cam/galayou-g7.webp" alt="Galayou G7"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-galayou_g7_t23n_sc2336_atbm6012bx.bin">T23N, SC2336, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>GNCC GC2</dt>
          <dd><img src="https://thingino.com/a/cam/gncc-gc2.webp" alt="GNCC GC2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-gncc_gc2_t23n_sc2336_atbm6012bx.bin">T23N, SC2336, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Hualai HL-JDPAN01</dt>
          <dd><img src="https://thingino.com/a/cam/hualai-hl-jdpan01.webp" alt="Hualai HL-JDPAN01"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-hl_jdpan01_t31l_gc2053_atbm6031.bin">T31L, GC2053, ATBM6031, 16MB</a></dd>
        </dl>
        <dl>
          <dt>iFlytek XFP301-M</dt>
          <dd><img src="https://thingino.com/a/cam/iflytek-xfp301-m.webp" alt="iFlytek XFP301-M"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-iflytek_xfp301m_t31x_jxq03_rtl8188ftv.bin">T31ZX, JXQ03, RTL8188FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-iflytek_xfp301m_t31x_jxq03_ssv6155.bin">T31ZX, JXQ03, SSV6155, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Imou Ranger 2</dt>
          <dd><img src="https://thingino.com/a/cam/imou-ranger-2.webp" alt="Imou Ranger 2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-imou_ranger2_t31n_gc2053_ssv6155.bin">T31N, GC2053, SSV6155, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Jooan A6M</dt>
          <dd><img src="https://thingino.com/a/cam/jooan-a6m.webp" alt="Jooan A6M"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_a6m_t23n_sc1a4t_atbm6012bx.bin">T23N, SC1A4T, ATBM6012BX, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_a6m_t23n_sc1a4t_ssv6355.bin">T23N, SC1A4T, SSV6355, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/wiki/Camera:-Jooan-A6M">Installation</a></dd>
        </dl>
        <dl>
          <dt>Jooan C9TS</dt>
          <dd><img src="https://thingino.com/a/cam/jooan-c9ts.webp" alt="Jooan C9TS"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_c9ts_t23n_sc2336p_atbm6132u.bin">T23N, SC2336P, ATBM6132U, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Jooan Q3H</dt>
          <dd><img src="https://thingino.com/a/cam/jooan-q3h.webp" alt="Jooan Q3H"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_q3h_t30l_sc1235_rtl8189ftv.bin">T30L, SC1235, RTL8189FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>LongPlus X07</dt>
          <dd><img src="https://thingino.com/a/cam/longplus-x07.webp" alt="LongPlus X07"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-longplus_x07_t31n_jxf23_rtl8189ftv.bin">T31N, JXF23, RTL8189FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>LSC 3215672</dt>
          <dd><img src="https://thingino.com/a/cam/lsc-3215672.webp" alt="LSC 3215672"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-lsc_3215672_t23n_sc2331_atbm6012bx.bin">T23N, SC2331, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Neos SmartCam 2</dt>
          <dd><img src="https://thingino.com/a/cam/neos-smartcam2.webp" alt="Neos SmartCam 2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam2_t20x_jxf22_rtl8189ftv.bin">T20X, JXF22, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam2_t20x_jxf23_rtl8189ftv.bin">T20X, JXF23, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>NexHT 86336</dt>
          <dd><img src="https://thingino.com/a/cam/nexht-86336.webp" alt="NexHT 86336"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-nexht_86336_t21z_jxf37_rtl8188ftv.bin">T21Z, JXF37, RTL8188FTV, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Personal Cam Pan</dt>
          <dd><img src="https://thingino.com/a/cam/personal-campan.webp" alt="Personal Cam Pan"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-personal_campan_t31x_gc2053_atbm6031.bin">T31X, GC2053, ATBM6031, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Personal Cam 2</dt>
          <dd><img src="https://thingino.com/a/cam/personal-cam-2.webp" alt="Personal Cam 2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-personal_cam2_t31x_gc2053_atbm6031.bin">T31X, GC2053, ATBM6031, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Pesita X09</dt>
          <dd><img src="https://thingino.com/a/cam/pesita-x09.webp" alt="Pesita X09"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-pesita_x09_t31n_jxf23_rtl8189ftv.bin">T31N, JXF23, RTL8189FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>PrimeCables 08360</dt>
          <dd><img src="https://thingino.com/a/cam/primecables-08360.webp" alt="PrimeCables 08360"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-primecables_08360_t21z_sc2300_rtl8188ftv.bin">T21Z, SC2300, RTL8188FTV, 8MB</a></dd>
        </dl>
        <dl>
          <dt>PrimeCables 08361</dt>
          <dd><img src="https://thingino.com/a/cam/primecables-08361.webp" alt="PrimeCables 08361"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-primecables_08361_t21n_sc2300_rtl8188ftv.bin">T21N, SC2300, RTL8188FTV, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Sonoff Slim Gen2</dt>
          <dd><img src="https://thingino.com/a/cam/sonoff-s2.webp" alt="Sonoff Cam Slim Gen2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-sonoff_s2_t23n_sc2336_atbm6012bx.bin">T23N, SC2336, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>TP-Link Tapo C100</dt>
          <dd><img src="https://thingino.com/a/cam/tapo-c100.webp" alt="Tapo C100"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-tplink_tapo_c100_t23n_sc2336p_wq9001.bin">T23N, SC2336P, WQ9001, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-tplink_tapo_c100_t31l_sc2336_rtl8188ftv.bin">T31L, SC2336, RTL8188FTV, 8MB</a></dd>
        </dl>
        <dl>
          <dt>TP-Link Tapo C110</dt>
          <dd><img src="https://thingino.com/a/cam/tapo-c100.webp" alt="Tapo C110"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-tplink_tapo_c110_t23n_sc2336p_wq9001.bin">T23N, SC2336P, WQ9001, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Vanhua DJZ</dt>
          <dd><img src="https://thingino.com/a/cam/vanhua-djz.webp" alt="Vanhua DJZ"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-vanhua_djz_t31n_gc2083_eth.bin">T31N, GC2083, ETH, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Vanhua FJZ</dt>
          <dd><img src="https://thingino.com/a/cam/vanhua-fjz.webp" alt="Vanhua FJZ"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-vanhua_fjz_t31x_gc4653_eth.bin">T31X, GC4653, ETH, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Victure PC420</dt>
          <dd><img src="https://thingino.com/a/cam/victure-pc420.webp" alt="Victure PC420"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-victure_pc420_t21n_jxf23_eth+rtl8188ftv.bin">T21N, JXF23, RTL8188FTV, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Wanjiaan G7</dt>
          <dd><img src="https://thingino.com/a/cam/!photo.webp" alt="Wanjiaan G7"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wanjiaan_g7_t31x_jxf37_eth.bin">T31X, JXF37, ETH, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Wanjiaan HDC-51</dt>
          <dd><img src="https://thingino.com/a/cam/wanjiaan-hdc51.webp" alt="Wanjiaan HDC-51"></dd>
          <dd><span>China Mobile</span></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wanjiaan_hdc51_t21n_sc2235_rtl8189ftv.bin">T21N, SC2235, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wanjiaan_hdc51_t31l_jxf37_rtl8188ftv.bin">T31L, JXF37, RTL8188FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wanjiaan_hdc51_t31l_sc2332_rtl8188ftv.bin">T31L, SC2332, RTL8188FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Wanjiaan HDC-55</dt>
          <dd><img src="https://thingino.com/a/cam/wanjiaan-hdc55.webp" alt="Wanjiaan HDC-55"></dd>
          <dd><span>China Mobile</span></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wanjiaan_hdc55_t31n_sc2332_rtl8188ftv.bin">T31N, SC2332, RTL8188FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Wansview K5</dt>
          <dd><img src="https://thingino.com/a/cam/wansview-k5.webp" alt="Wansview K5"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wansview_k5_t21n_ov2735b_mt7601sta.bin">T21N, OV2735b, MT7601STA, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wansview_k5_t21n_os02b10_mt7601sta.bin">T21N, OS02B10, MT7601STA, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Wansview Q5</dt>
          <dd><img src="https://thingino.com/a/cam/wansview-q5.webp" alt="Wansview Q5"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wansview_q5_t23n_sc2336_atbm6012bx.bin">T23N, SC2336, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>WUUK Y0310</dt>
          <dd><img src="https://thingino.com/a/cam/wuuk-y0310.webp" alt="WUUK Y0310"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wuuk_y0310_t31x_sc401ai_ssv6158.bin">T31X, SC401AI, SSV6158, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>WUUK Y0510</dt>
          <dd><img src="https://thingino.com/a/cam/wuuk-y0510.webp" alt="WUUK Y0510"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wuuk_y0510_t31x_sc401ai_ssv6158.bin">T31X, SC401AI, SSV6158, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wuuk_y0510_t31x_sc4336p_ssv6158.bin">T31X, SC4336p, SSV6158, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Wyze Cam Pan 1</dt>
          <dd><img src="https://thingino.com/a/cam/wyze-cp1.webp" alt="Wyze Cam Pan 1"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_campan1_t20x_jxf22_rtl8189etv.bin">T20X, JXF22, RTL8189ETV, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Wyze Cam 2</dt>
          <dd><img src="https://thingino.com/a/cam/wyze-c2.webp" alt="Wyze Cam 2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam2_t20x_jxf22_rtl8189ftv.bin">T20X, JXF22, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam2_t20x_jxf23_rtl8189ftv.bin">T20X, JXF23, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Wyze Cam Pan 2</dt>
          <dd><img src="https://thingino.com/a/cam/wyze-cp2.webp" alt="Wyze Cam Pan 2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_campan2_t31x_gc2053_atbm6031.bin">T31X, GC2053, ATBM6031, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Wyze Cam 3</dt>
          <dd><img src="https://thingino.com/a/cam/wyze-c3.webp" alt="Wyze Cam 3"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam3_t31al_gc2053_atbm6031.bin">T31AL, GC2053, ATBM6031, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam3_t31x_gc2053_atbm6031.bin">T31X, GC2053, ATBM6031, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_cam3_t31x_gc2053_rtl8189ftv.bin">T31X, GC2053, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Xiaomi HL-CAM04</dt>
          <dd><img src="https://thingino.com/a/cam/xiaomi-hl-cam04.webp" alt="Xiaomi HL-CAM04"></dd>
          <dd>
            <a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xiaomi_hlcam04_t31n_sc3335_atbm6031.bin">T31N, SC3335, ATBM6031, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Xiaomi iSC5</dt>
          <dd><img src="https://thingino.com/a/cam/xiaomi-isc5.webp" alt="Xiaomi iSC5"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xiaomi_xiaofang_t20l_jxf22_rtl8189ftv.bin">T20L, JXF22, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xiaomi_xiaofang_t20l_jxf23_rtl8189ftv.bin">T20L, JXF23, RTL8189FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Xiaomi SXJ02ZM</dt>
          <dd><img src="https://thingino.com/a/cam/xiaomi-sxj02zm.webp" alt="Xiaomi iSC5"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xiaomi_sxj02zm_t20l_ps5220_rtl8189ftv.bin">T20L, PS5250, RTL8189FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Xiaomi MJSXJ03HL</dt>
          <dd><img src="https://thingino.com/a/cam/xiaomi-mjsxj03hl.webp" alt="Xiaomi MJSXJ03HL"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xiaomi_mjsxj03hl_t31l_jxq03p_rtl8189ftv.bin">T31L, JXQ03p, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xiaomi_mjsxj03hl_t31n_jxq03_rtl8189ftv.bin">T31N, JXQ03, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xiaomi_mjsxj03hl_t31n_jxq03p_rtl8189ftv.bin">T31N, JXQ03p, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/wiki/Camera:-Xiaomi-MJSXJ03HL">Installation</a></dd>
        </dl>
        <dl>
          <dt>ZTE K540</dt>
          <dd><img src="https://thingino.com/a/cam/zte-k540.webp" alt="ZTE K540"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-zte_k540_t31x_sc4336_eth+atbm6032.bin">T31X, SC4336, ETH. ATBM6032, 16MB</a></dd>
        </dl>
      </div>

      <h3>Bulb IP Cameras</h3>

      <div>
        <dl>
          <dt>AJCloud T-CP8040LF-W3M
          </dt><dd><img src="https://thingino.com/a/cam/ajcloud-cp8040.webp" alt="AJCloud CP8010"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-ajcloud_cp8040_t23n_cs2336p_atbm6132bu.bin">T23N, SC2336P, ATBM6132BU, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Jooan T2R</dt>
          <dd><img src="https://thingino.com/a/cam/jooan-t2r.webp" alt="Jooan T2R"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_t2r_t23n_sc1a4t_atbm6012bx.bin">T23N, SC1A4T, ATBM6012BX, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_t2r_t23n_sc1a4t_ssv6355.bin">T23N, SC1A4T, SSV6355, 8MB</a></dd>
        </dl>
        <dl>
          <dt>LaView L2</dt>
          <dd><img src="https://thingino.com/a/cam/laview-l2.webp" alt="LaView L2"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-laview_l2_t31l_sc3338_atbm6012b.bin">T31L, SC3338, ATBM6012B, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-laview_l2_t31l_sc3338_ssv6256p.bin">T31L, SC3338, SSV6256P, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Wansview G6</dt>
          <dd><img src="https://thingino.com/a/cam/wansview-g6.webp" alt="Wansview G6"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wansview_g6_t31l_sc2336_atbm6012b.bin">T31L, SC2336, ATBM6012B, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wansview_g6_t31l_sc2336_ssv6256p.bin">T31L, SC2336, SSV6256P, 8MB</a></dd>
        </dl>
      </div>

      <h3>Outdoor IP Cameras</h3>

      <div>
        <dl>
          <dt>AOSU C5L</dt>
          <dd><img src="https://thingino.com/a/cam/aosu-c5l.webp" alt="AOSU C5L"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-aosu_c5l_t31l_sc3336_rtl8188ftv.bin">T31L, SC3336, RTL8188FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Dekco DC5L</dt>
          <dd><img src="https://thingino.com/a/cam/dekco-dc5l.webp" alt="Dekco DC5L"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-dekco_dc5l_t31l_sc3336_rtl8188ftv.bin">T31L, SC3336, RTL8188FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Eufy E220 (T8441X)</dt>
          <dd><img src="https://thingino.com/a/cam/eufy-t8441x.webp" alt="Eufy Outdoor (T8441X)"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8441x_t31x_sc3335_syn4343.bin">T31X, SC3335, SYN4343, 32MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8441x_t31x_sc3338_syn4343.bin">T31X, SC3338, SYN4343, 32MB</a></dd>
        </dl>
        <dl>
          <dt>Eufy E210 (T8442X)</dt>
          <dd><img src="https://thingino.com/a/cam/eufy-t8442x.webp" alt="Eufy E210 Outdoor (T8442X)"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8442x_t31x_sc3335_syn4343.bin">T31X, SC3335, SYN4343, 32MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-eufy_t8442x_t31x_sc3338_syn4343.bin">T31X, SC3338, SYN4343, 32MB</a></dd>
        </dl>
        <dl>
          <dt>Feisda WF-HD620</dt>
          <dd><img src="https://thingino.com/a/cam/feisda-wf-hd620.webp" alt="Fiesda WF-HD620"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-feisda_hd620_t31x_jxq03_eth+rtl8188ftv.bin">T31X, JXQ03, ETH, RTL8188FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Galayou Y4</dt>
          <dd><img src="https://thingino.com/a/cam/galayou-y4.webp" alt="Galayou Y4"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-galayou_y4_t23n_sc2336_atbm6062.bin">T23N, SC2336, ATBM6062, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-galayou_y4_t23n_sc2336p_atbm6062.bin">T23N, SC2336P, ATBM6062, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-galayou_y4_t31l_sc2336_atbm6012bx.bin">T31L, SC2336, ATBM6012BX, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-galayou_y4_t31l_sc2336_atbm6032.bin">T31L, SC2336, ATBM6032, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-galayou_y4_t31n_sc2336_atbm6012b.bin">T31N, SC2336, ATBM6012B, 8MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Ginzzu OP-200</dt>
          <dd><img src="https://thingino.com/a/cam/ginzzu-op200.webp" alt="Ginzzu OP-200"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-ginzzu_op200_t31l_gc2083_ssv6155.bin">T31L, GC2083, SSV6155, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Howell QJ05T</dt>
          <dd><img src="https://thingino.com/a/cam/howell-qj05t.webp" alt="Howell QJ05T"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-howell_qj05t_t20l_sc2235_eth+rtl8188ftv.bin">T20L, SC2235, ETH, RTL8188FTV, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Jienuo JN-107-AR-E-WIFI</dt>
          <dd><img src="https://thingino.com/a/cam/jienuo-jn107arewifi.webp" alt="Jienuo JN-107-AR-E-WIFI"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jienuo_jn107arewifi_t31x_sc5235_eth+rtl8731bu.bin">T31X, SC5235, ETH, RTL8131BU, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Jooan A2R</dt>
          <dd><img src="https://thingino.com/a/cam/jooan-a2r.webp" alt="Jooan A2R"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_a2r_t23n_sc1a4t_atbm6012bx.bin">T23N, SC1A4T, ATBM6012BX, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_a2r_t23n_sc1a4t_ssv6355.bin">T23N, SC1A4T, SSV6355, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Jooan F2T</dt>
          <dd><img src="https://thingino.com/a/cam/jooan-f2t.webp" alt="Jooan F2T"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_f2t_t30x_sc4236_eth.bin">T30X, SC4236, ETH, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Jooan Q3R</dt>
          <dd><img src="https://thingino.com/a/cam/jooan-q3r.webp" alt="Jooan Q3R"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-jooan_q3r_t23n_sc1a4t_atbm6012bx.bin">T23N, SC1A4T, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Litokam M1</dt>
          <dd><img src="https://thingino.com/a/cam/litokam-m1.webp" alt="Litokam M1"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-litokam_m1_t31l_sc3336_atbm6012b.bin">T31L, SC3336, ATBM6012B, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-litokam_m1_t31l_sc3336_atbm6012bx.bin">T31L, SC3336, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Overtech OV-59WB</dt>
          <dd><img src="https://thingino.com/a/cam/overtech-ov59wb.webp" alt="Overtech OV-59WB"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-overtech_ov59wb_t31n_sc223a_rtl8188ftv.bin">T31N, SC223A, RTL8188FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Wansview W6</dt>
          <dd><img src="https://thingino.com/a/cam/wansview-w6.webp" alt="Wansview W6"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wansview_w6_t21n_ov2735b_rtl8188ftv.bin">T21N, OV2735b, RTL8188FTV, ETH, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wansview_w6_t21n_os02g10_rtl8188ftv.bin">T21N, OS02G10, RTL8188FTV, ETH, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Wansview W7</dt>
          <dd><img src="https://thingino.com/a/cam/wansview-w7.webp" alt="Wansview W7"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-galayou_y4_t23n_sc2336_atbm6062.bin">T23N, SC2336, ATBM6062, 8MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wansview_w7_t31l_sc2336_atbm6012b.bin">T31L, SC2336, ATBM6012B, 8MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Wyze Cam Floodlight 1</dt>
          <dd><img src="https://thingino.com/a/cam/wyze-cfl1.webp" alt="Wyze CFL1"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_camfl1_t31al_gc2053_atbm6031.bin">T31AL, GC2053, ATBM6031, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_camfl1_t31x_gc2053_atbm6031.bin">T31X, GC2053, ATBM6031, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_camfl1_t31x_gc2053_rtl8189ftv.bin">T31X, GC2053, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/wltechblog/thingino-installers">Installation</a></dd>
        </dl>
        <dl>
          <dt>Wyze Video Doorbell 1</dt>
          <dd><img src="https://thingino.com/a/cam/wyze-vdb1.webp" alt="Wyze VDB1"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_vdb1_t30x_sc4236_rtl8189ftv.bin">T30X, SC4236, RTL8189FTV, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-wyze_vdb1_t31x_sc4236_rtl8189ftv.bin">T31ZX, SC4236, RTL8189FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Xiaomi MJSXJ05HL</dt>
          <dd><img src="https://thingino.com/a/cam/xiaomi-mjsxj05hl.webp" alt="Xiaomi MJSXJ05HL"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xiaomi_mjsxj05hl_t31l_gc2053_atbm6031.bin">T31L, GC2053, ATBM6031, 16MB</a></dd>
        </dl>
        <dl>
          <dt>XVIM IPCAM-100</dt>
          <dd><img src="https://thingino.com/a/cam/xvim-ipcam-100.webp" alt="XVIM IPCAM-100"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-xvim_ipcam100_t21n_jxf37_eth+rtl8188ftv.bin">T21N, JXF37, ETH, RTL8188FTV, 8MB</a></dd>
        </dl>
      </div>

      <h3>IPC Modules</h3>

      <div>
        <dl>
          <dt>Enzhi / Vanhua AK54</dt>
          <dd><img src="https://thingino.com/a/cam/enz-ak54.webp" alt="Enzhi AK54"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-vanhua_ak54_t31n_gc2053_eth.bin">T31N, GC2053, ETH, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Enzhi / Vanhua H33</dt>
          <dd><img src="https://thingino.com/a/cam/enz-h33.webp" alt="Enzhi H33"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-vanhua_h33_t31l_gc2083_eth.bin">T31L, GC2083, ETH, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Enzhi / Vanhua L34</dt>
          <dd><img src="https://thingino.com/a/cam/enz-l34.webp" alt="Enzhi L34"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-vanhua_l34_t31l_gc2083_eth.bin">T31L, GC2083, ETH, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Enzhi / Vanhua S37i</dt>
          <dd><img src="https://thingino.com/a/cam/enz-s37i.webp" alt="Enzhi S37i"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-vanhua_s37i_t31l_imx307_eth.bin">T31L, IMX307, ETH, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Enzhi / Vanhua Z55</dt>
          <dd><img src="https://thingino.com/a/cam/enz-z55.webp" alt="Enzhi Z55"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-vanhua_z55_t31x_gc4653_eth.bin">T31X, GC4653, ETH, 16MB</a></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/wiki/Camera:-AliExpress-LTIA%E2%80%9037FJZ-(Vanhua-Z55-module)">Installation</a></dd>
        </dl>
        <dl>
          <dt>Enzhi / Vanhua Z55I</dt>
          <dd><img src="https://thingino.com/a/cam/enz-z55i.webp" alt="Enzhi Z55I"></dd>
          <dd><a href="https://github.com/themactep/thingino-firmware/releases/latest/download/thingino-vanhua_z55i_t31x_gc4653_eth.bin">T31X, GC4653, ETH, 16MB</a></dd>
        </dl>
      </div>

      <h3>Web Cameras</h3>

      

      <h3>Development Boards</h3>

      

      <h2>Conditionally Supported Hardware</h2>

      <p>Some brands protect their cameras by writing a secret key into the
        <abbr title="One-Time Programmable">OTP</abbr> area of the <abbr title="System-on-Chip">SoC</abbr>.
        They digitally sign the installed firmware with a matching key, so that replacing the pre-installed
        firmware with one that is not signed with the same key will render the camera unusable, unless you
        replace the SoC with a new, undamaged one.</p>

      

      <p>Color code: <span>Some tested units had Secure Boot.</span>
      <span>All tested units had Secure Boot.</span></p>

      <h2>Mystery Box Hardware</h2>

      <p>These cameras are not necessarily based on Ingenic SoC. Some units purchased on AliExpress were marketed as the same model, but they came with unsupported ARM processors.</p>

      

      <h2>Potentially Supported Cameras</h2>

      <p>We've heard that these cameras use Ingenic SoC, so we think they could be added to the supported list. Unfortunately, we don't have a sample of this camera to work on, so we'd be happy to accept it as a donation to the project. Just a heads-up, this is a non-commercial project, and the people working on it contribute their free time for nothing more than a sense of accomplishment. If you want things to develop faster, offer your help.</p>

      <div>
        <dl>
          <dt>Ezviz CS-MY3-3WHY</dt>
          <dd><img src="https://thingino.com/a/cam/!photo.webp" alt="Galayou G7"></dd>
          <dd><a href="https://thingino.com/donate-hardware">T31X, JXQ03, RTL8189FTV, 16MB</a></dd>
        </dl>
        <dl>
          <dt>Galayou G2</dt>
          <dd><img src="https://thingino.com/a/cam/galayou-g2.webp" alt="Galayou G2"></dd>
          <dd><a href="https://thingino.com/donate-hardware">T31LC, SC2336, ATBM6012BX, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Galayou G7</dt>
          <dd><img src="https://thingino.com/a/cam/galayou-g7.webp" alt="Galayou G7"></dd>
          <dd><a href="https://thingino.com/donate-hardware">T31LC, SC2336, ATBM6012B, 8MB</a></dd>
        </dl>
        <dl>
          <dt>Winees M3 PRO</dt>
          <dd><img src="https://thingino.com/a/cam/winees-m3pro.webp" alt="Winees M3 PRO"></dd>
          <dd><a href="https://thingino.com/donate-hardware">T31X, SC3336, RTL8192FC, 16MB</a></dd>
        </dl>
      </div>

      <h2>Unsupported Hardware</h2>

      <p>Battery powered cameras using the Zeratul platform are not supported, at least for now.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Projects evaluated to see if they're as free and open source as advertised (155 pts)]]></title>
            <link>https://isitreallyfoss.com/</link>
            <guid>44791554</guid>
            <pubDate>Mon, 04 Aug 2025 21:26:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://isitreallyfoss.com/">https://isitreallyfoss.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44791554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<header id="top">
				<a href="https://isitreallyfoss.com/">is <strong>it</strong> <em>really</em> <strong>FOSS</strong>?</a>
				
			</header>
			<main>
	<div>

		<h2>
			Where Projects are Evaluated <br>
			<span>To see if they're as free and open source as advertised</span>
		</h2>

		<p>
			The software rights of users are continously (and often opaquely) being eroded by the desire of growth.
			<br>
			This website aims to push back against that by bringing transparency to <a href="https://isitreallyfoss.com/about/foss">FOSS</a> software users.
		</p>

		

		<div>
			<h3>Latest Projects Added</h3>
			
		</div>

		

	</div>
</main>
			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Once a death sentence, cardiac amyloidosis is finally treatable (143 pts)]]></title>
            <link>https://www.nytimes.com/2025/08/04/well/cardiac-amyloidosis.html</link>
            <guid>44790944</guid>
            <pubDate>Mon, 04 Aug 2025 20:23:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/08/04/well/cardiac-amyloidosis.html">https://www.nytimes.com/2025/08/04/well/cardiac-amyloidosis.html</a>, See on <a href="https://news.ycombinator.com/item?id=44790944">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/08/04/well/cardiac-amyloidosis.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Content-Aware Spaced Repetition (166 pts)]]></title>
            <link>https://www.giacomoran.com/blog/content-aware-sr/</link>
            <guid>44790422</guid>
            <pubDate>Mon, 04 Aug 2025 19:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.giacomoran.com/blog/content-aware-sr/">https://www.giacomoran.com/blog/content-aware-sr/</a>, See on <a href="https://news.ycombinator.com/item?id=44790422">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>  <p>Spaced repetition systems are powerful, but they have a fundamental blind spot: they don’t understand what your flashcards are&nbsp;<em>about</em>.</p>
<p>To your SRS, a card asking “what’s the capital of Italy?” and another asking “what country is Rome the capital of?” are treated independently, each with its own isolated review history. It has no concept that reviewing related material should reinforce your memory of the whole topic.</p>
<p>At the heart of every SRS is a&nbsp;<strong>memory model</strong> which predicts how long you’ll remember each card based on your past performance. Most of today’s models ignore the <em>content</em> of the cards entirely. This is where <strong>content-aware memory models</strong> come in: they account for the semantic meaning of your cards, not just your review ratings.</p>
<p>This is more than a minor tweak for scheduling accuracy. It’s a foundational change that makes it practical to build the fluid, intelligent learning tools many have envisioned: from&nbsp;<a href="https://notes.andymatuschak.org/z7wCFe7MP9VeCVApcBLC7SN">idea-centric memory systems</a>&nbsp;that test understanding from multiple angles to truly&nbsp;<a href="https://davidbieber.com/snippets/2024-03-04-conversational-spaced-repetition/">conversational spaced repetition</a>&nbsp;with a voice-enabled AI agent as tutor.</p>
<p>This post explores what content-aware memory models are, and the new kinds of learning experiences they make possible.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#schedulers-and-memory-models">Schedulers and memory models</a></li>
<li><a href="#content-aware-memory-models">Content-aware memory models</a>
<ul>
<li><a href="#karl">KARL</a></li>
<li><a href="#small-experiments-on-rember-data">Small experiments on Rember data</a></li>
<li><a href="#other-considerations">Other considerations</a></li>
</ul>
</li>
<li><a href="#ux-unlocks">UX unlocks</a></li>
<li><a href="#data-problem">Data problem</a></li>
</ul>
<h2 id="schedulers-and-memory-models">Schedulers and memory models</h2>
<p>I find it useful to distinguish between schedulers and memory models. This distinction wasn’t immediately obvious to me when I first approached the topic, but I’ve found it essential for thinking clearly about spaced repetition systems (SRS). Here, I’ll introduce both concept and I’ll make the case that separating schedulers from memory models enables independent innovation and simplifies the development of each component by isolating user experience (UX) concerns within the scheduler. In the literature, the scheduler is sometimes called the “teacher model” since it decides what to teach, while the memory model is the “student model” since it represents what the student knows.</p>
<p>In Anki and other spaced repetition systems, the <strong>scheduler</strong> is an algorithm that picks the next card to review today, answering the question <em>“Given the review history of every card in the student’s collection, which cards should the student review in this session?”</em>. In practice, when building a spaced repetition system, this is the question you actually care about. The scheduler’s core job is deciding&nbsp;<em>which cards to show today</em>, not just&nbsp;<em>when each card should ideally be reviewed</em>. A card might be “due” for review, but the scheduler might skip it due to daily review limits, prioritize other overdue cards, or defer it based on other goals.</p>
<p>For a long time, the only scheduler available in Anki was <a href="https://faqs.ankiweb.net/what-spaced-repetition-algorithm">a variant of SuperMemo’s SM-2 scheduler</a>, which dates back to <a href="https://www.supermemo.com/en/blog/the-true-history-of-spaced-repetition#1987">1987</a>. It’s remarkably simple yet effective. SuperMemo has since advanced its scheduler, now at version <a href="https://supermemo.guru/wiki/Algorithm_SM-18">SM-18</a>. The latest iterations of the algorithm achieve the same retention levels with fewer reviews, and are more robust when reviews deviate from optimal intervals, for example when a student returns from a break of several weeks. While the SuperMemo schedulers are closed-source, an explanation of how they work is <a href="https://supermemo.guru/wiki/Algorithm_SM-17">available</a>. <a href="https://github.com/open-spaced-repetition/fsrs4anki">FSRS</a> is an open-source scheduler by Jarrett Ye built on similar principles to modern SuperMemo algorithms and can be used in Anki.</p>
<p>A <strong>memory model</strong> predicts forgetting curves, answering the question <em>“Given the review history of every card in the student’s collection, what are the chances the student remembers a specific card at any given moment?”</em></p>
<p>We define <strong>retrievability</strong> as the probability that a student remembers a card at a particular time. In practice, we often model this as a binary outcome where a student either remembers or forgets a card. Some spaced repetition systems allow for more nuanced grading. For example, Anki has four options: “Again”, “Hard”, “Good”, “Easy”. This additional information from the review history can be used to make memory models more accurate.</p>
<p>The forgetting curve plots this retrievability over time. A memory model’s job is to compute these curves based on a student’s review history. Retrievability generally decreases over time, as it becomes more likely that the student will forget the card. For example, here are the forgetting curves estimated by different memory models from the same review history (the figure comes from my <a href="https://www.politesi.polimi.it/handle/10589/186407">master’s thesis</a>):</p>
<p><img alt="Plot of forgetting curves estimated by different memory models from the same review history" loading="lazy" decoding="async" fetchpriority="auto" width="1500" height="900" src="https://www.giacomoran.com/_astro/fc_comparison.DEARgvNj_2owL0b.webp"></p>
<p>The relationship between schedulers and memory models varies across different systems. A scheduler may or may not use a memory model. For example, the <a href="https://en.wikipedia.org/wiki/Leitner_system">Leitner system</a> and SM-2 do not rely on a memory model to schedule reviews, they are based on simple mechanical rules. Modern schedulers like SM-18 and FSRS instead include a memory model (stability and difficulty are used to compute retrievability). I recommend taking a look at Fernando Borretti’s articles on the implementing <a href="https://borretti.me/article/implementing-sm2-in-rust">SM-2</a> and <a href="https://borretti.me/article/implementing-fsrs-in-100-lines">FSRS</a>. Note that FSRS ties together the scheduler and the memory model at the implementation level, but they can be separated quite easily to unlock more flexibility in designing the system. There are also <a href="https://siddharth.io/files/deep-tutor.pdf">schedulers based on model-free reinforcement-learning</a>, that learn a scheduling policy directly from user interactions, without building an explicit, human-interpretable model of forgetting like the ones we’re discussing.</p>
<p>Once you have a memory model that estimates how likely the student is to remember each card, you can build different schedulers with various strategies:</p>
<ul>
<li>You can schedule a card for review when retrievability drops below 90%. This is the most common strategy in SRS as far as I can tell. This is more or less what FSRS does, you can also adjust the <em>desired retention</em> to other values.</li>
<li>You can randomly select cards for review, with probability proportional to how likely they have been forgotten (one minus retrievability). <a href="https://www.nature.com/articles/s41539-021-00105-8">Example</a>.</li>
<li>You can override the default scheduler behavior for new cards or after a failed review by implementing custom&nbsp;<em>learning</em> and&nbsp;<em>relearning steps</em>. This is also included in most FSRS implementations.</li>
<li>You can fuzz the intervals (adjust them randomly by small amounts) or apply load balancing to smooth out the amount of reviews scheduled for the student over a few days.</li>
<li>You can implement different strategies for when the student takes a break of weeks or months. For example, in Rember we mark cards overdue for a week as stale and the student can limit the amount of stale cards in each review session. This is an idea we took from <a href="https://www.remnote.com/">RemNote</a>.</li>
<li>You can build an exam feature, where the student sets the date for the exam and you schedule reviews in order to achieve high retrievability on that date for cards related to the exam. <a href="https://arxiv.org/abs/1805.08322">Example</a>.</li>
<li>You can account for goals complementary to retention, for example the student workload, the amount of new cards the student is adding, or the estimated answering time for each card. <a href="https://www.nature.com/articles/s41539-020-00074-4">Example</a>.</li>
<li>You can optimize for long-term outcomes, not just today’s retrievability. For example, if a student fails a card today, reviewing it again tomorrow might boost its future stability more than waiting a week. This forward-looking approach considers how today’s review outcome affects the card’s entire future learning trajectory. Some of the examples above account for similar ramifications. Nate Meyvis has explored <a href="https://www.natemeyvis.com/notes-on-spaced-repetition-scheduling.html">similar ideas</a>.</li>
</ul>
<p>As these examples illustrate, a single, accurate memory model can act as a foundation for a diverse ecosystem of schedulers, each optimized for different goals and user experiences.</p>
<p>The distinction between memory models and schedulers offers two key advantages:</p>
<ol>
<li><strong>Independent innovation cycles.</strong> We can innovate on schedulers independently of memory models, and vice versa. Scheduler research can treat memory models as a black box to obtain retrievability predictions.</li>
<li><strong>Separation from UX concerns.</strong> We can focus on building better memory models independently of product or UX considerations. The design of schedulers is deeply intertwined with product and UX considerations. For example, a scheduler might ensure a student re-attempts a failed card before the session ends. The primary benefit here might be psychological, providing the student with the assurance of getting it right, rather than a decision optimized purely for long-term retention. Another example is load balancing, the goal of which is primarily improving the student’s experience.</li>
</ol>
<p>This architectural separation isn’t just theoretical. It enables practical benefits: you can A/B test different scheduling strategies using the same underlying memory model, swap in improved memory models without rebuilding your entire system, and allow users to choose scheduling approaches that fit their learning style while maintaining consistent forgetting predictions underneath.</p>
<p>Most current SRS implementations conflate these concerns. The next generation of systems will likely benefit from treating them as distinct, composable components.</p>
<h2 id="content-aware-memory-models">Content-aware memory models</h2>
<p>This section explores how leveraging the textual content and semantic relationships between cards can improve memory models.</p>
<p>To the best of my knowledge, most memory models in real-world spaced repetition systems treat each card in isolation. SM-2, SM-18, and FSRS rely solely on each card’s individual review history to predict its forgetting curve. Ignoring the following factors means we are leaving useful information on the table:</p>
<ol>
<li><strong>The review histories of related cards</strong>. Card semantics allow us to identify related cards. This enables memory models to account for the review histories of&nbsp;<em>all</em> relevant cards when estimating a specific card’s retrievability.</li>
<li><strong>The card’s textual content</strong>. Beyond identifying related cards, the semantic content itself can directly inform the memory model. A model could, for example, estimate the inherent difficulty of a card based on its question or answer text, even before any reviews have taken place.</li>
</ol>
<p>It’s important to note that “card semantics” encompasses more than simple textual similarity. We’re looking to capture the card’s quality: how effectively the question can activate the memory pathways that lead the student to remember the concept. For example, we need to detect slight ambiguities in the question, which would make it difficult to answer. Moreover, we will likely need to examine the semantics of groups of cards, not just cards in isolation. For example, to memorize a list of three items, you might want a card for each item, plus an integrative card for the entire list. The card for the entire list in isolation would be quite poor in terms of card quality and would be very difficult to remember without the other three cards supporting it.</p>
<p>Informally, this direction proposes shifting the memory model’s focus from:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>retrievability(t) = f(t; single_card_history)</span></span></code></pre>
<p>To leveraging a richer context:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>retrievability(t) = f(t; all_cards_history, all_cards_content)</span></span></code></pre>
<p>Where <code>t</code> is the time since the last review, and each history (whether <code>single_card_history</code> or <code>all_cards_history</code>) is a chronological sequence of review events, typically comprising a timestamp and the student’s rating for that review.</p>
<p>Note that current memory models treat all new cards (cards with no reviews) as the same. Considering the textual content would allow us to obtain more informed initial estimates of a card’s inherent difficulty, leading to better scheduling for cards with no or few reviews compared to uniform defaults.</p>
<p>For example, consider the following collection of cards:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>Q: In reinforcement learning, broadly, what is the Bellman equation?</span></span>
<span><span>A: An equation that describes the fundamental relationship between the value of a state and the values of its successor states</span></span>
<span><span></span></span>
<span><span>Q: In reinforcement learning, what equation describes the fundamental relationship between the value of a state and the values of its successor states?</span></span>
<span><span>A: The Bellman equation</span></span>
<span><span></span></span>
<span><span>Q: In reinforcement learning, what's the formula of the Bellman equation?</span></span>
<span><span>A: $$ v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] $$</span></span></code></pre>
<p>All three cards are semantically related, but the first two, being conceptual inversions of each other, share a much stronger connection than with the third, which focuses on the formula.
Reviewing either of the first two cards would likely make recalling the other significantly easier. The third card might have a similar effect, but considerably weaker. (Alternatively, if a student successfully reviews either of the first two cards, we should increase our confidence that they’ll recall the other). The third card is also inherently more challenging due to its “less atomic” nature: a student must recall multiple components of the formula, increasing the likelihood of forgetting a single element and marking the card as forgotten.</p>
<h3 id="karl">KARL</h3>
<p>This idea has been explored in the literature by <a href="https://arxiv.org/abs/2402.12291">Shu et al., 2024 - KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students</a>, where they introduce the term <strong>content-aware scheduling</strong>. The memory model in KARL encodes the textual content of the cards with BERT embeddings.</p>
<p>These embeddings play a dual role:</p>
<ol>
<li>They facilitate the&nbsp;retrieval of the top-k semantically similar cards&nbsp;from the user’s study history, whose review histories are then passed to the memory model</li>
<li>The&nbsp;embeddings of the current card and these top-k similar cards&nbsp;are themselves fed into the memory model</li>
</ol>
<p>The KARL scheduler was evaluated on a dataset consisting of 123,143 study logs on diverse trivia questions, collected from 543 users within a custom flashcard app. It slightly outperformed FSRSv4, which has improved since then, being now at version 6. This result is remarkable because KARL does not model the memory dynamics explicitly, like FSRS does by estimating difficulty and stability, and using a power-law forgetting curve. I wonder how the performance would be for a model that is good at both capturing the memory dynamics, like FSRSv6, and at accounting for card semantics, like KARL.</p>
<h3 id="small-experiments-on-rember-data">Small experiments on Rember data</h3>
<p>I ran a few small experiments myself that provide additional evidence that this direction is promising. In <a href="https://rember.com/">Rember</a> we group cards around small notes; at the time of the experiment, my account had 4,447 reviews for 940 cards, grouped in 317 notes. You can think of notes as grouping together semantically similar cards.</p>
<p>I ran a couple of experiments on top of FSRS:</p>
<ul>
<li><code>exp_1</code>: when the card has no reviews, set the initial stability to the average stability of other cards in the note.</li>
<li><code>exp_2</code>: multiply stability by a constant factor when other cards from the note have been reviewed between now and the card’s last review (the constant factor is optimized using grid search). This simulates a “priming” effect where recent exposure to related concepts reinforces the current card</li>
</ul>
<p>Stability represents how long the card will last in memory, it’s defined as the interval at which the card’s retrievability drops to 90%.</p>
<p>I compared the following memory models:</p>
<ul>
<li><code>random</code>: random retrievability predictions in <code>[0,1]</code></li>
<li><code>fsrs</code>: FSRSv5 with default parameters</li>
<li><code>fsrs_optimized</code>: FSRSv5 with parameters optimized on the collection</li>
<li><code>fsrs_exp_1</code>: <code>fsrs</code> + <code>exp_1</code></li>
<li><code>fsrs_exp_2</code>: <code>fsrs</code> + <code>exp_2</code></li>
<li><code>fsrs_optimized_exp_1</code>: <code>fsrs_optimized</code> + <code>exp_1</code></li>
<li><code>fsrs_optimized_exp_2</code>: <code>fsrs_optimized</code> + <code>exp_2</code></li>
</ul>
<p>I performed 4-fold cross-validation splitting by note IDs, and compared the models on the same evaluation metrics from my <a href="https://www.politesi.polimi.it/handle/10589/186407">master’s thesis</a>: <em>AUC</em> (measuring discrimination, the highest the better), <em>ICI</em> (measuring the average calibration error, the lower the better), <em>E_max</em> (measuring the maximum calibration error, the lower the better).</p>
<p>The results are summarized in the following table:</p>
<div>




















































<table><thead><tr><th>Model</th><th>AUC</th><th>ICI</th><th>E_max</th></tr></thead><tbody><tr><td><code>random</code></td><td>0.4887±0.0156</td><td>0.4235±0.0055</td><td>0.9193±0.0099</td></tr><tr><td><code>fsrs</code></td><td>0.5708±0.0172</td><td>0.0364±0.0120</td><td>0.2720±0.0912</td></tr><tr><td><code>fsrs_optimized</code></td><td>0.6294±0.0115</td><td>0.0108±0.0041</td><td>0.1904±0.0689</td></tr><tr><td><code>fsrs_exp_1</code></td><td>0.5883±0.0248</td><td>0.0281±0.0086</td><td>0.2204±0.0640</td></tr><tr><td><code>fsrs_exp_2</code></td><td>0.5716±0.0159</td><td>0.0307±0.0079</td><td>0.2355±0.0867</td></tr><tr><td><code>fsrs_optimized_exp_1</code></td><td>0.6148±0.0128</td><td><strong>0.0070±0.0021</strong></td><td>0.1383±0.0356</td></tr><tr><td><code>fsrs_optimized_exp_2</code></td><td><strong>0.6386±0.0185</strong></td><td>0.0075±0.0031</td><td><strong>0.1285±0.0522</strong></td></tr></tbody></table></div>
<p>The experimental results, though based on a small dataset, indicate a clear trend:</p>
<ul>
<li>Incorporating note-level information into FSRS, even with default parameters (<code>fsrs_exp_1</code> and <code>fsrs_exp_2</code>), generally outperforms <code>fsrs</code> across all metrics.</li>
<li>The optimal performance is achieved when FSRS is first optimized on the collection and then combined with note-level information.</li>
</ul>
<p>These results, while preliminary given the dataset size, support the hypothesis that integrating semantic context enhances memory models.</p>
<h3 id="other-considerations">Other considerations</h3>
<p><a href="https://www.mathacademy.com/">MathAcademy</a> includes an interesting spaced repetition feature, which accounts for the card semantics. They manually create a tree of concepts, with dependencies between them. The spaced repetition scheduler accounts for reviews of prerequisite concepts when scheduling a card. You can read more about how their scheduler works <a href="https://www.mathacademy.com/how-our-ai-works">here</a>. Here’s a quote:</p>
<blockquote>
<p>Existing spaced repetition algorithms are limited to the context of independent flashcards - but this is not appropriate for a hierarchical body of knowledge like mathematics. For instance, if a student practices adding two-digit numbers, then they are effectively practicing adding one-digit numbers as well! In general, repetitions on advanced topics should “trickle down” the knowledge graph to update the repetition schedules of simpler topics that are implicitly practiced.</p>
</blockquote>
<p>This is amazing work, but the scheduler is limited to MathAcademy’s tree of concepts; we need memory models that account for card semantics and apply more generally.</p>
<p>A general caveat might be the computational cost of the scheduler. For example, FSRS can schedule thousands of reviews in a few milliseconds on my M2 MacBook Pro, and therefore it can easily run on-device in a SRS. A memory model that accounts for the review history of all cards and the textual content of the cards will likely be more computationally intensive and might not be able to run on-device without excessive delays or battery drain. KARL addresses this computational challenge by considering only the top-k most semantically similar cards rather than all cards in the collection.</p>
<p>This kind of models will likely be trained on data from many spaced repetition students, which has a few implications:</p>
<ul>
<li>Even if a student has no reviews in the system, the model will still be able to estimate the initial difficulty for their cards.</li>
<li>We are making the implicit assumption that card semantics influence reviews in the same way across students, this might not always be true. For example, a card difficult for one person might be easy for another due to differing familiarity with the topic. However, if a card is hard to remember for most students, it is reasonable to assume it will be hard for others. Potential solutions include: focusing on how the question relates to the underlying topic, rather than the topic itself, or somehow modeling the student’s ability.</li>
</ul>
<h2 id="ux-unlocks">UX unlocks</h2>
<p>While accuracy improvements are valuable, the bigger impact comes from the UX opportunities unlocked by card semantics. By removing the rigid coupling between cards and their review histories, content-aware memory models give designers of spaced repetition systems much more freedom.</p>
<p>Here’s a concrete example of this constraint in action. When we started working on Rember (in the pre-LLM era), we considered building a fully markdown-based SRS. The main reason we dropped the idea is that you need an ID for each card, to link a card to its review history. Keeping IDs in the markdown quickly gets messy. You end up with something like:</p>
<pre tabindex="0" data-language="plaintext"><code><span><span>[card:abcxyz]</span></span>
<span><span>Q: What's the capital of Italy?</span></span>
<span><span>A: Rome</span></span></code></pre>
<p>Which is far from ideal, it’s fragile since the user might accidentally edit the ID, or end up with duplicate IDs by copy-pasting cards.</p>
<p>Solutions that get rid of the IDs were all dead-ends. You cannot rely on exact textual matches because the user might edit a card, and you don’t want to reset the review history if the user fixes a typo. You cannot rely on the relative position of the card in the document, as the user might move cards around. Forcing a GUI on top of the markdown that takes care of the IDs is viable, but kinda defeats the point of markdown.</p>
<p>The key insight is decoupling cards from their specific review histories. Instead of linking reviews to card IDs, the memory model considers only a universal history of content-based reviews, as triples: <code>(rating, timestamp, card's textual content)</code>. This eliminates the need for card consistency over time. There’s no “card’s review history” anymore, a single “review history” spans the entire student’s collection.</p>
<p>The scheduler can then assess the retrievability of all current cards in a student’s collection, including newly added ones, without relying on past review IDs. Students can edit cards freely without disrupting the scheduler. The memory model, using semantic understanding, differentiates between minor stylistic edits (like typos, which maintain the core meaning) and substantive changes to the card’s content (like replacing an answer, which would alter its semantic meaning sufficiently to be treated as a distinct card).</p>
<p>This decoupling also simplifies systems that dynamically generate prompts. Andy Matuschak explored bringing <em>ideas</em> rather than cards into the system. Prompts are dynamically generated during each review session to test the ideas from multiple angles, and evolve over time as you get more familiar with the ideas (see his Patreon post&nbsp;<a href="https://www.patreon.com/posts/fluid-practice-83882597">Fluid practice for fluid understanding</a>&nbsp;or his public&nbsp;<a href="https://notes.andymatuschak.org/z7wCFe7MP9VeCVApcBLC7SN">notes</a>). Content-aware memory models make this approach much more tractable.&nbsp;Current schedulers assume a review history per card. For dynamically generated prompts, this forces an awkward choice: either treat each unique prompt variation as a new card, losing its connection to the core idea, or group them coarsely at the idea level, which likely leads to under-reviewing the individual prompts.&nbsp;Content-aware memory models, however, naturally handle this gray area.</p>
<p>Taking this further, we could design a SRS where the flashcard-based review session is replaced with a conversation with a voice-enabled AI agent that asks questions or engages in open-ended discussion. This is part of the idea of <a href="https://davidbieber.com/snippets/2024-03-04-conversational-spaced-repetition/">conversational spaced repetition</a> explored by David Bieber. The AI agent could track the key ideas or concept the student goes over during the conversation, somehow judging whether the user could remember them or not. The content-aware memory model should be able account for those less structured reviews, even if they don’t directly map to Q&amp;A cards.</p>
<p>Additional benefits include:</p>
<ul>
<li>Having duplicates cards in the system is less disruptive to the review practice, since the scheduler will consider them as one and the same (even though you might still want to implement ways to detect and remove them from the system).</li>
<li>Reduced migration costs between SRS. Current importers must meticulously map both cards and review histories. With content-aware memory models, importing reviews could leverage the prior system’s textual card representation, enabling the new model to “understand” review histories from&nbsp;<em>any</em>&nbsp;system, regardless of perfect content mapping.</li>
</ul>
<p>While this approach reduces some direct user control over individual card histories, such as manually resetting a specific card’s schedule, I believe we can mitigate the problem and that the benefits of the approach far outweigh the costs.</p>
<p>In summary, I predict that content-aware memory models will make it much easier to design and build new interfaces for memory systems. They remove annoying hurdles that occupy the minds of SRS developers.</p>
<h2 id="data-problem">Data problem</h2>
<p>The main challenge in building content-aware memory models is lack of data. To my knowledge, no publicly available dataset exists that contains real-world usage data with both card textual content and review histories.</p>
<p><a href="https://arxiv.org/abs/2402.12291">KARL</a>, mentioned above, is trained on a dataset collected by paying users to review trivia flashcards on a custom app. I would hesitate to rely on a memory model trained solely on artificial data for my own spaced repetition practice. FSRS is trained on <a href="https://huggingface.co/datasets/open-spaced-repetition/anki-revlogs-10k">anki-revlogs-10k</a>, a large dataset consisting of more than 200M reviews from 10k Anki collections. The dataset includes only card, note, and deck IDs, omitting their textual content due to Anki’s <a href="https://ankiweb.net/account/privacy">Privacy Policy</a>, which states:</p>
<blockquote>
<p>In the interests of research and product development, we may use your review history and options (but not the text or media on cards) for statistical purposes, such as calculating the average pass rate across all users.</p>
</blockquote>
<p>While other review datasets exist, a crucial missing piece is a large dataset that:</p>
<ol>
<li>Is non-commercial and can be used for research purposes</li>
<li>Includes review histories</li>
<li>Includes cards’ textual content</li>
<li>Covers a wide range of topics (e.g. not just language learning data)</li>
</ol>
<p>Building on <a href="https://www.natemeyvis.com/notes-on-spaced-repetition-scheduling.html">Nate Meyvis’s insights</a>, I’ll add another requirement (I’ll discuss below why this is important):</p>
<ol start="5">
<li>A small fraction of the reviews are scheduled at random to provide unbiased data points</li>
</ol>
<p>Some of the challenges with data coming from spaced repetition systems:</p>
<ul>
<li><strong>The data is sparse.</strong> Limited to a time-stamped binary sequence of review ratings, spaced repetition data offers only a faint and insufficient signal to fully reconstruct the complex, dynamic state of a student’s memory.</li>
<li><strong>The data is incomplete.</strong> The limited data captured by spaced repetition systems fails to account for crucial out-of-system interactions that significantly shape a student’s memory. Students interact with material outside the SRS: through reading, conversation, or practical application. These interactions, important for memory, are not captured by the system. Furthermore, each review significantly alters the card’s future schedule. Consequently, unrecorded external recall events can have a substantial but uncaptured effect on the memory state assumed by the system.</li>
<li><strong>The data is biased.</strong> Spaced repetition data is inherently biased by the memory model that schedules reviews, creating a “chicken or the egg” problem where the data used to train the model is influenced by the model itself, potentially hindering further optimization (see <a href="https://ceur-ws.org/Vol-1432/sl_pap3.pdf">this paper</a>). This is why scheduling a small fraction of reviews at random in a real-world SRS could significantly improve the accuracy of memory models.</li>
<li><strong>The data is self-reported.</strong>&nbsp;We assume students provide ratings that truly reflect their inner memory state, but they may mark cards as “remembered” when they’ve actually forgotten them, perhaps to avoid the discomfort of perceived failure.</li>
</ul>
<p>The strong results achieved by current-generation schedulers like FSRS and SM-18 provide compelling evidence that a valuable signal indeed exists and can be separated from noise, despite the challenges described above.</p>
<p>One potential path forward is an open-source, community-contributed dataset where users voluntarily share their Anki and other SRS data, complete with tools to filter out sensitive content and eventually standardized benchmarks for evaluating memory models. If you’re interested in contributing data, have experience building community-driven datasets, or have thoughts on this approach, I’d love to <a href="https://www.giacomoran.com/cdn-cgi/l/email-protection#31565850525e5c5e43505f71565c50585d1f525e5c">hear from you</a>.</p>
<hr>
<p><em>We’re looking for testers for a new workflow for generating flashcards with AI, if you might be interested sign up for the waitlist at <a href="https://rember.com/">rember.com</a>. The best way to get updates on my work is <a href="https://x.com/giacomo_ran">x dot com</a>.</em></p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Passkeys are just passwords that require a password manager (149 pts)]]></title>
            <link>https://danfabulich.medium.com/passkeys-are-just-passwords-that-require-a-password-manager-ebb7f2fdcadf</link>
            <guid>44790385</guid>
            <pubDate>Mon, 04 Aug 2025 19:29:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danfabulich.medium.com/passkeys-are-just-passwords-that-require-a-password-manager-ebb7f2fdcadf">https://danfabulich.medium.com/passkeys-are-just-passwords-that-require-a-password-manager-ebb7f2fdcadf</a>, See on <a href="https://news.ycombinator.com/item?id=44790385">Hacker News</a></p>
Couldn't get https://danfabulich.medium.com/passkeys-are-just-passwords-that-require-a-password-manager-ebb7f2fdcadf: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[NASA's Curiosity picks up new skills (147 pts)]]></title>
            <link>https://www.jpl.nasa.gov/news/marking-13-years-on-mars-nasas-curiosity-picks-up-new-skills/</link>
            <guid>44790271</guid>
            <pubDate>Mon, 04 Aug 2025 19:20:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jpl.nasa.gov/news/marking-13-years-on-mars-nasas-curiosity-picks-up-new-skills/">https://www.jpl.nasa.gov/news/marking-13-years-on-mars-nasas-curiosity-picks-up-new-skills/</a>, See on <a href="https://news.ycombinator.com/item?id=44790271">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p data-block-key="k29rj">Thirteen years since Curiosity landed on Mars, engineers are finding ways to make the NASA rover even more productive. The six-wheeled robot has been given more autonomy and the ability to multitask — improvements designed to make the most of Curiosity’s energy source, a multi-mission radioisotope thermoelectric generator (MMRTG). Increased efficiency means the rover has ample power as it continues to decipher how the ancient Martian climate changed, transforming a world of lakes and rivers into the chilly desert it is today.</p><p data-block-key="7hhum">Curiosity recently rolled into a region filled with <a href="https://www.nasa.gov/missions/mars-science-laboratory/curiosity-rover/nasas-curiosity-mars-rover-starts-unpacking-boxwork-formations/">boxwork formations</a>. These hardened ridges are believed to have been created by underground water billions of years ago. Stretching for miles on this part of <a href="https://science.nasa.gov/resource/curiositys-proposed-path-up-mount-sharp/">Mount Sharp</a>, a 3-mile-tall (5-kilometer-tall) mountain, the formations might reveal whether microbial life could have survived in the Martian subsurface eons ago, extending the period of habitability farther into when the planet was drying out.</p></div><div><p data-block-key="3uu6m">Instead, Curiosity and its younger sibling <a href="https://science.nasa.gov/mission/mars-2020-perseverance/">Perseverance</a> each use their <a href="https://www.jpl.nasa.gov/news/press_kits/mars_2020/launch/mission/spacecraft/power/">MMRTG</a> nuclear power source, which relies on decaying plutonium pellets to create energy and recharge the rover’s batteries. Providing ample power for the rovers’ many science instruments, MMRTGs are known for their longevity (the twin <a href="https://science.nasa.gov/mission/voyager/">Voyager</a> spacecraft have relied on <a href="https://science.nasa.gov/mission/voyager/spacecraft/#h-radioisotope-power-system-rps">RTGs</a> since 1977). But as the plutonium decays over time, it takes longer to recharge Curiosity’s batteries, leaving less energy for science each day.</p><p data-block-key="fq71o">The team carefully manages the rover’s daily power budget, factoring in every device that draws on the batteries. While these components were all tested extensively before launch, they are part of complex systems that reveal their quirks only after years in the extreme Martian environment. Dust, radiation, and sharp temperature swings bring out edge cases that engineers couldn’t have expected.</p><p data-block-key="1nfva">“We were more like cautious parents earlier in the mission,” said Reidar Larsen of NASA’s Jet Propulsion Laboratory in Southern California, which built and operates the rover. Larsen led a group of engineers who developed the new capabilities. “It’s as if our teenage rover is maturing, and we’re trusting it to take on more responsibility. As a kid, you might do one thing at a time, but as you become an adult, you learn to multitask.”</p><h3 data-block-key="e6csg"><b>More Efficient Science</b></h3><p data-block-key="2ho0n">Generally, JPL engineers send Curiosity a list of tasks to complete one by one before the rover ends its day with a nap to recharge. In 2021, the team began studying whether two or three rover tasks could be safely combined, reducing the amount of time Curiosity is active.</p><p data-block-key="cfof9">For example, Curiosity’s radio regularly sends data and images to a passing orbiter, which <a href="https://www.nasa.gov/centers-and-facilities/jpl/the-mars-relay-network-connects-us-to-nasas-martian-explorers/">relays them to Earth</a>. Could the rover talk to an orbiter while driving, moving its robotic arm, or snapping images? Consolidating tasks could shorten each day’s plan, requiring less time with heaters on and instruments in a ready-to-use state, reducing the energy used. Testing showed Curiosity safely could, and all of these have now been successfully demonstrated on Mars.</p></div><div><p data-block-key="436fb">Another trick involves letting Curiosity decide to nap if it finishes its tasks early. Engineers always pad their estimates for how long a day’s activity will take just in case hiccups arise. Now, if Curiosity completes those activities ahead of the time allotted, it will go to sleep early.</p><p data-block-key="b26s7">By letting the rover manage when it naps, there is less recharging to do before the next day’s plan. Even actions that trim just 10 or 20 minutes from a single activity add up over the long haul, maximizing the life of the MMRTG for more science and exploration down the road.</p><h3 data-block-key="f5j4r"><b>Miles to Go</b></h3><p data-block-key="ai5jd">In fact, the team has been implementing other new capabilities on Curiosity for years. Several mechanical issues required a rework of how the robotic arm’s <a href="https://www.jpl.nasa.gov/news/drilling-success-curiosity-is-collecting-mars-rocks/">rock-pulverizing drill</a> collects samples, and <a href="https://www.nasa.gov/missions/mars-science-laboratory/curiosity-rover/nasas-curiosity-mars-rover-gets-a-major-software-upgrade/">driving capabilities</a> have been enhanced with software updates. When a color filter wheel stopped turning on one of the two cameras mounted on Mastcam, Curiosity’s swiveling “head,” the team <a href="https://www.nasa.gov/missions/mars-science-laboratory/curiosity-rover/nasas-curiosity-rover-clocks-4000-days-on-mars/">developed a workaround</a> allowing them to capture the same beautiful panoramas.</p><p data-block-key="5sjdo">JPL also developed an <a href="https://www.jpl.nasa.gov/news/an-algorithm-helps-protect-mars-curiositys-wheels/">algorithm to reduce wear</a> on Curiosity’s rock-battered wheels. And while engineers closely monitor any new damage, they aren’t worried: After 22 miles (35 kilometers) and extensive research, it’s clear that, despite some punctures, the wheels have years’ worth of travel in them. (And in a worst-case scenario, Curiosity could remove the damaged part of the wheel’s “tread” and still drive on the remaining part.)</p><p data-block-key="fi860">Together, these measures are doing their job to keep Curiosity as busy as ever.</p><h3 data-block-key="5hk3l"><b>More About Curiosity</b></h3><p data-block-key="c18ov">Curiosity was built by NASA’s Jet Propulsion Laboratory, which is managed by Caltech in Pasadena, California. JPL leads the mission on behalf of NASA’s Science Mission Directorate in Washington as part of NASA’s Mars Exploration Program portfolio. Malin Space Science Systems in San Diego built and operates Mastcam.</p><p data-block-key="2s4lj">For more about Curiosity, visit:</p><p data-block-key="fmd7f"><a href="https://science.nasa.gov/mission/msl-curiosity"><b>science.nasa.gov/mission/msl-curiosity</b></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Europe is breaking its reliance on American science (101 pts)]]></title>
            <link>https://www.reuters.com/sustainability/climate-energy/europe-is-breaking-its-reliance-american-science-2025-08-01/</link>
            <guid>44789903</guid>
            <pubDate>Mon, 04 Aug 2025 18:48:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/sustainability/climate-energy/europe-is-breaking-its-reliance-american-science-2025-08-01/">https://www.reuters.com/sustainability/climate-energy/europe-is-breaking-its-reliance-american-science-2025-08-01/</a>, See on <a href="https://news.ycombinator.com/item?id=44789903">Hacker News</a></p>
Couldn't get https://www.reuters.com/sustainability/climate-energy/europe-is-breaking-its-reliance-american-science-2025-08-01/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Offline.kids – Screen-free activities for kids (131 pts)]]></title>
            <link>https://offline.kids/</link>
            <guid>44789192</guid>
            <pubDate>Mon, 04 Aug 2025 17:50:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://offline.kids/">https://offline.kids/</a>, See on <a href="https://news.ycombinator.com/item?id=44789192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><ul><li>
<figure><a href="https://offline.kids/teaching-kids-to-lose-gracefully/" target="_self"><img loading="lazy" decoding="async" width="1536" height="1024" src="https://offline.kids/wp-content/uploads/2025/06/Teaching-Kids-to-Lose-Gracefully.png" alt="Teaching Kids to Lose Gracefully" srcset="https://offline.kids/wp-content/uploads/2025/06/Teaching-Kids-to-Lose-Gracefully.png 1536w, https://offline.kids/wp-content/uploads/2025/06/Teaching-Kids-to-Lose-Gracefully-300x200.png 300w" sizes="auto, (max-width: 1536px) 100vw, 1536px"></a></figure>

<p><time datetime="2025-06-05T08:26:49+00:00">June 5, 2025</time></p>

<h2><a href="https://offline.kids/teaching-kids-to-lose-gracefully/" target="_self">Teaching Kids to Lose Gracefully</a></h2>

<div><p>Losing a game can feel devastating for kids — but it’s also an important opportunity to help them build resilience and empathy. In this gentle guide, we share practical tips for parents and carers to support children in learning how to handle setbacks with confidence and grace.</p><p><a href="https://offline.kids/teaching-kids-to-lose-gracefully/">View post</a></p></div>
</li><li>
<figure><a href="https://offline.kids/solo-play-ideas-for-kids-when-you-need-a-moment/" target="_self"><img loading="lazy" decoding="async" width="1536" height="1024" src="https://offline.kids/wp-content/uploads/2025/06/Solo-Play-Activities.png" alt="Solo Play Ideas for Kids (When You Need a Moment)" srcset="https://offline.kids/wp-content/uploads/2025/06/Solo-Play-Activities.png 1536w, https://offline.kids/wp-content/uploads/2025/06/Solo-Play-Activities-300x200.png 300w" sizes="auto, (max-width: 1536px) 100vw, 1536px"></a></figure>

<p><time datetime="2025-05-29T11:40:34+00:00">May 29, 2025</time></p>

<h2><a href="https://offline.kids/solo-play-ideas-for-kids-when-you-need-a-moment/" target="_self">Solo Play Ideas for Kids (When You Need a Moment)</a></h2>

<div><p>Sometimes kids need something they can do on their own — while you’re on a work call, cooking dinner, or just taking a breath. Here’s a list of solo activities you can try with your child, grouped by age. Some are linked to full activity guides on Offline.Kids, and others are simple ideas you can…</p><p><a href="https://offline.kids/solo-play-ideas-for-kids-when-you-need-a-moment/">View post</a></p></div>
</li><li>
<figure><a href="https://offline.kids/wed-love-your-feedback-help-shape-offline-kids/" target="_self"><img loading="lazy" decoding="async" width="1536" height="1024" src="https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration.png" alt="We’d Love Your Feedback – Help Shape Offline.Kids" srcset="https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration.png 1536w, https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration-300x200.png 300w, https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration-1024x683.png 1024w, https://offline.kids/wp-content/uploads/2025/05/Feedback-Invitation-Illustration-768x512.png 768w" sizes="auto, (max-width: 1536px) 100vw, 1536px"></a></figure>

<p><time datetime="2025-05-22T13:36:59+00:00">May 22, 2025</time></p>

<h2><a href="https://offline.kids/wed-love-your-feedback-help-shape-offline-kids/" target="_self">We’d Love Your Feedback – Help Shape Offline.Kids</a></h2>

<div><p>Hi there 👋 Offline.Kids is a passion project — started by a tired parent (me!) looking for simple, low-effort ways to spend meaningful time with my daughter. If you’ve ever found yourself Googling “easy activities for kids” with one eye on the clock and the other on your coffee, you’re in the right place. Now…</p><p><a href="https://offline.kids/wed-love-your-feedback-help-shape-offline-kids/">View post</a></p></div>
</li><li>
<figure><a href="https://offline.kids/smartphone-free-childhood-create-powerful-new-video/" target="_self"><img loading="lazy" decoding="async" width="1280" height="720" src="https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb.jpg" alt="Smartphone Free Childhood create powerful new video" srcset="https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb.jpg 1280w, https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb-300x169.jpg 300w, https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb-1024x576.jpg 1024w, https://offline.kids/wp-content/uploads/2025/05/SFC-video-thumb-768x432.jpg 768w" sizes="auto, (max-width: 1280px) 100vw, 1280px"></a></figure>

<p><time datetime="2025-05-22T10:53:34+00:00">May 22, 2025</time></p>

<h2><a href="https://offline.kids/smartphone-free-childhood-create-powerful-new-video/" target="_self">Smartphone Free Childhood create powerful new video</a></h2>

<div><p>Discover how the Smartphone Free Childhood movement is empowering parents and carers to protect kids from smartphone harm — with community, courage, and hope.</p><p><a href="https://offline.kids/smartphone-free-childhood-create-powerful-new-video/">View post</a></p></div>
</li></ul>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: What trick of the trade took you too long to learn? (277 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44789068</link>
            <guid>44789068</guid>
            <pubDate>Mon, 04 Aug 2025 17:39:59 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44789068">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><tbody><tr id="44794030"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794030" href="https://news.ycombinator.com/vote?id=44794030&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Excellence in anything is a byproduct of having fun. Fun is a byproduct of understanding. Understanding is a byproduct of going slow. Going slow is a byproduct of curiosity. Curiosity is a byproduct of saying "I don’t know," of shunning beliefs and attending to what is in front, with zero baggage or impositions of your own—shunning the ego in the moment, moment by moment. Excellence comes when each piece is as equal as any other, when preference is shunned, when space is created to allow what is in the moment, without resistance, without insistence.</p></div></td></tr></tbody></table></td></tr><tr id="44794160"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44794160" href="https://news.ycombinator.com/vote?id=44794160&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is relatable. Once one gets over the frustration of failing and making mistakes (thousands of times in some cases), it becomes fun and easier to stay curious.</p></div></td></tr></tbody></table></td></tr><tr id="44790699"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790699" href="https://news.ycombinator.com/vote?id=44790699&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Start as early as possible in investing (in index funds) and otherwise being financially savvy. It is very beneficial to realise early on that growing your hard earned money and spending it wisely is way more important as it will in the future lead to some unexpected benefits. Freedom of thought and action!</p></div></td></tr></tbody></table></td></tr><tr id="44793713"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793713" href="https://news.ycombinator.com/vote?id=44793713&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>My mistake was not starting early, because the numbers were small and it didn’t seem worth the time. The habit and systems are important to build, so that when the numbers do get bigger it goes to the right place.</p></div></td></tr></tbody></table></td></tr><tr id="44793768"><td></td></tr><tr id="44793855"><td></td></tr><tr id="44794155"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44794155" href="https://news.ycombinator.com/vote?id=44794155&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I also graduated in 2008.</p><p>Starting my adult life with the markets in freefall made it hard to get in the habit of investing.  All through school, people told me "most people don't start investing until their 30s, and end up in a worse than they ought to because the time-value of money compounds a bunch if you add a few more years."  I thought I knew better than to be one of those people.</p><p>Instead, I ended up keeping a lot of savings in cash during years when the interest rates were approximately 0.  I've tried to get better at putting money into the markets over the past few years, but my financials look very different than they might have.</p><p>&gt; In other words, 2008 had no meaningful impact on you then.</p><p>People who graduated in the late 00s might not have accrued big financial losses, but it had a very meaningful impact on my comfort investing.</p></div></td></tr></tbody></table></td></tr><tr id="44794254"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44794254" href="https://news.ycombinator.com/vote?id=44794254&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I don't follow. The markets were low, great time to invest. Great time to buy a home. 2008 was bad for people who owned homes or were already investing.</p></div></td></tr></tbody></table></td></tr><tr id="44793501"><td></td></tr><tr id="44794177"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794177" href="https://news.ycombinator.com/vote?id=44794177&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Excellent resource. The videos link to a YouTube channel with a treasure trove of content in multiple disciplines. Thanks for sharing!</p></div></td></tr></tbody></table></td></tr><tr id="44790860"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790860" href="https://news.ycombinator.com/vote?id=44790860&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Call Vanguard: 877-662-7447</p><p>An investing prof at Chicago puts this on the whiteboard at the start of semester, saying this is really all most people need to know and this class is unlikely to learn anything in his or any class that will let them, personally, do better.</p></div></td></tr></tbody></table></td></tr><tr id="44791587"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44791587" href="https://news.ycombinator.com/vote?id=44791587&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>That’s good advice for a layman but most high earners can do much better if they care and are motivated. Most are neither though lol.</p><p>Mostly on the tax side. Some specific examples:</p><p>- after maxing out your 401k what should you do next? IRA? Mega backdoor roth? Something else?</p><p>- If you have kids, how to best save for future education expenses? Hint: consider 529 plan.</p><p>- HSA is technically the best tax advantaged account, most high earners don’t realize it and “waste” the HSA funds to reimburse typical medical bills. HSA has triple tax benefits: contributions are tax-free, growth is tax-free, and withdrawals are also tax-free after age 65 for any reason, not just medical expenses. So basically investing without any tax obligation. You can also withdraw tax free before 65, but for medical expenses only.</p><p>i could go on…investing is great, but reducing your tax obligation is an even more powerful technique if you want to grow your net worth.</p></div></td></tr></tbody></table></td></tr><tr id="44793631"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793631" href="https://news.ycombinator.com/vote?id=44793631&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Looked into HSA recently at work but legally you need a high deductible health care plan to be eligible. Looking at the options, just nothing looked good compared to my current $0 deductible/$0 co-pay plan. Hard to know for sure but just seemed like I would be paying a lot more out of pocket every year.</p></div></td></tr></tbody></table></td></tr><tr id="44793910"><td></td></tr><tr id="44793345"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793345" href="https://news.ycombinator.com/vote?id=44793345&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>The HSA thing intrigued me and so I did some digging. It appears that post-65, you still have to pay income tax on non-medical withdrawals from an HSA? That is, besides the tax-free-for-medical-expenses part it reverts to a traditional IRA?</p><p>One additional trick though is that it looks like you can pay for any HSA-eligible medical expenses (incurred after you created the HSA) out-of-pocket now, and reimburse your bills at any point in the future? Thus you can still earn interest on the cash before withdrawing it at any point in the future (treating it as tax-free liquidity).</p><p>(I don't fully understand this so these are questions not statements, but hopefully I'm correct!)</p></div></td></tr></tbody></table></td></tr><tr id="44793621"><td></td></tr><tr id="44793544"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44793544" href="https://news.ycombinator.com/vote?id=44793544&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p><i>&gt; The HSA thing intrigued me and so I did some digging. It appears that post-65, you still have to pay income tax on non-medical withdrawals from an HSA?</i></p><p>That's what I understood too. That claim that you can completely skip taxes looks wrong.</p></div></td></tr></tbody></table></td></tr><tr id="44793565"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44793565" href="https://news.ycombinator.com/vote?id=44793565&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You can completely escape taxes for any amounts that you have receipts for medical expenses incurred after the HSA was opened.</p><p>You can pay cash for a qualified medical expense in 2025 and take out that
amount of money from the HSA decades later.</p></div></td></tr></tbody></table></td></tr><tr id="44793591"><td></td></tr><tr id="44793725"><td><table><tbody><tr><td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td><center><a id="up_44793725" href="https://news.ycombinator.com/vote?id=44793725&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>One thing to note!!!! Make sure to also keep records on the HSA start date and any transfers you may engage in, e.g., after leaving an employer or just changing providers. It is worth having that on record in case 20, 30, 40 years later your claims are rejected because the original opening date was lost in some transfer at some point.</p></div></td></tr></tbody></table></td></tr><tr id="44793595"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44793595" href="https://news.ycombinator.com/vote?id=44793595&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>i’m the parent. You are correct, my mistake! It’s too late to edit at this point.  :(</p><p>What I should have said is that after 65 you can spend it on non medical stuff without penalty. BUT if you do so you’ll owe tax that year (withdrawal).</p><p>So to summarize, you can avoid all tax if it’s spent on medical stuff. For non medical (post 65) it’s still good, but not as good.</p><p>Still an amazing deal because old people tend to spend a lot more on healthcare.</p></div></td></tr></tbody></table></td></tr><tr id="44793516"><td></td></tr><tr id="44791842"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44791842" href="https://news.ycombinator.com/vote?id=44791842&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>What you have stated is almost the same as call Vanguard, all those options are the same in the sense that they all involve investing, leaving it alone for a long time. Its just the vehicle thats slightly different and tax advantages.</p><p>I wouldn’t consider those options needing much motivation or research. The key with all of them is investing early and leaving it alone.</p></div></td></tr></tbody></table></td></tr><tr id="44792069"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44792069" href="https://news.ycombinator.com/vote?id=44792069&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>If one is clueless with investing and taxes in general, and they call vanguard, their eyes will glaze over. It would be like me explaining software development to my 85 year old father.</p><p>I do agree people should call vanguard. But just blindly following steps they give you is unlikely to be productive if you don’t understand why you’re doing those steps. Furthermore, those people who don’t understand _why_ will freak out every time there’s a huge market correction. They get scared - because they don’t understand any of it.</p><p>I’m also curious, do they offer financial advice for your accounts outside vanguard? Genuinely curious since i’m unsure.</p></div></td></tr></tbody></table></td></tr><tr id="44792388"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44792388" href="https://news.ycombinator.com/vote?id=44792388&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I’ve think the “call” part is a bit from the past, should be a setup account online with Vanguard, put it in VOO or VTSAX or the equivalent low fee market index fund and leave it alone.</p><p>Replace Vanguard with any other firm but the key is picking low fee ETFs and leaving them alone. Vanguard tends to have a reputation for the lowest fees.</p></div></td></tr></tbody></table></td></tr><tr id="44793037"><td></td></tr><tr id="44791702"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44791702" href="https://news.ycombinator.com/vote?id=44791702&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>At that point get someone to do it for you for x% of what they're getting you back and live blissfully unaware of arcane tax code specifics.</p></div></td></tr></tbody></table></td></tr><tr id="44791769"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44791769" href="https://news.ycombinator.com/vote?id=44791769&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>At least the 401k, IRA, and HSA don't require knowing anything particularly arcane. Money goes in, don't touch until 59 1/2 (401k, IRA) or 65 (HSA).</p><p>529 plans can get a bit more complicated because you'll  want one from your state (if your state has an income tax) and they may offer several, but then it's less about knowing tax code specifics than about what the differences are between their offerings.</p></div></td></tr></tbody></table></td></tr><tr id="44792135"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44792135" href="https://news.ycombinator.com/vote?id=44792135&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>right, it’s not that arcane at all. I discovered all this over a couple weekends in my 20s. Started on reddit, then moved on to more official guides and books. I spent maybe 5 weekends in total doing this learning.</p><p>It’s really not that hard and i don’t understand why more people aren’t interested. Let’s reframe for a minute…if i said a high earner could retire a year earlier, or maybe even a few years earlier just by learning some semi-advanced tax strategies. Should they do so? Yeah. They’d be crazy not to lol.</p></div></td></tr></tbody></table></td></tr><tr id="44793575"><td></td></tr><tr id="44792477"><td></td></tr><tr id="44793648"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793648" href="https://news.ycombinator.com/vote?id=44793648&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>i tried that hack one year. Cigna just emptied my account on the first try with a provider charging 2 emergency room visits when I was there only for a xray.</p><p>Cigna refused to lift a finger unless i sued them both.</p><p>yeah, i wouldn't recommend that.</p></div></td></tr></tbody></table></td></tr><tr id="44793967"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793967" href="https://news.ycombinator.com/vote?id=44793967&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>An important part that could have been beneficial would have been to add, "today".</p><p>I had a class where the teacher did something similar, but she showed that if you started a ROTH today and contributed only the 4 years you were in college and then stopped forever. You would have nearly the same amount of money as someone who started 1 year after they completed college and invested every year until retirement.</p><p>Ultimately she was encouraging us to take out student loans and invest it or use any excess scholarship money to max out a ROTH IRA. She even advocated for investing all student loan money and opening credit cards tp actually pay for college, making minimum payments until graduation. Then moving away to a LCOL country and learn the language for 8-9 years while remaining in school taking 1 online class a year and travelling the world on student loans and not to worry about starting a career until 30 and start paying once you are back and start a job.</p></div></td></tr></tbody></table></td></tr><tr id="44794132"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794132" href="https://news.ycombinator.com/vote?id=44794132&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>ROTH IRA contributions have to be from earned income, though. The rest of the advice is of the same quality, imho. Beware!</p></div></td></tr></tbody></table></td></tr><tr id="44793511"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793511" href="https://news.ycombinator.com/vote?id=44793511&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is generally not applicable to most people.</p><p>1. Invest enough to get the company match in an S&amp;P 500.  It probably isn’t Vanguard that your company uses</p><p>2. Pay off all of your debt except your house (and maybe your car)</p><p>3. Max out your HSA - if you are married it’s - $8550</p><p>4.  Max out your 401K - again that’s probably not through Vanguard - $23500</p><p>5. Step 5 - then call Vanguard and depending on your income just do a Roth up to $8000 (?).</p><p>(Unless you are over 50 then do catch  up contributions as 4.5)</p><p>If you are under 50, you can do $40,500 tax advantaged and over 50 $48050</p></div></td></tr></tbody></table></td></tr><tr id="44793884"><td></td></tr><tr id="44794124"><td></td></tr><tr id="44793965"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793965" href="https://news.ycombinator.com/vote?id=44793965&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Step 6 is the mega-backdoor Roth. Do after tax contributions to your 401k and have your 401k servicer do an in-plan conversion to Roth 401k.</p><p>Also, on step 4, you may need to do a backdoor Roth IRA if you’re over the Roth IRA income limits.</p></div></td></tr></tbody></table></td></tr><tr id="44794057"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44794057" href="https://news.ycombinator.com/vote?id=44794057&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I’ve only had one employer that allowed after tax contributions (which for other people reading this is not the same as Roth 401K).</p><p>The issue is that most companies don’t allow it because of compliance reasons and rules regarding highly compensated employees.  Of course the one company that did allow it was BigTech.</p><p>Not that I’m missing much.  I doubt I will be in a higher tax bracket at retirement than I am now and I live in a state tax free state.</p></div></td></tr></tbody></table></td></tr><tr id="44794109"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44794109" href="https://news.ycombinator.com/vote?id=44794109&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>To me the primary benefit is when it comes to RMDs. Having a mix of traditional and Roth allows me to draw down the traditional in lower tax years to avoid RMD and have the Roth to fill in those later years or pass on to children. Plus, if you’ve maxed the $23,500ish of the traditional 401k and still have extra to save, it’s more tax advantaged in a Roth vs a taxable brokerage account.</p><p>These are obviously champagne problems but if you’re a high earning W2 it’s worth considering.</p></div></td></tr></tbody></table></td></tr><tr id="44794228"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794228" href="https://news.ycombinator.com/vote?id=44794228&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>For those of us very late to the investing game, what is the best strategy to try to catch up at least somewhat?</p></div></td></tr></tbody></table></td></tr><tr id="44792340"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44792340" href="https://news.ycombinator.com/vote?id=44792340&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>My millionaire, step-father-in-law, gave this advice to my brother when graduated.</p><p>I was lucky, my physics department administrator told me the same thing when I was graduating.</p><p>The 2ND best piece of advice is to rollover your 401k when you move to a new company -&gt; this cost me at least 500k because they effectively stagnate when your company isn't paying the maintenance cost (AIUI).</p></div></td></tr></tbody></table></td></tr><tr id="44792607"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44792607" href="https://news.ycombinator.com/vote?id=44792607&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>&gt; this cost me at least 500k because they effectively stagnate when your company isn't paying the maintenance cost</p><p>Is this true? My understanding is that the fees come out of the account itself. There's other good reasons to roll over (primarily investment flexibility) but I have not heard of something like this.</p></div></td></tr></tbody></table></td></tr><tr id="44793312"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793312" href="https://news.ycombinator.com/vote?id=44793312&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I don't think the point about 401k stagnation is true. At most fee structures and optionality of funds change. How did that cost you 500k exactly?</p></div></td></tr></tbody></table></td></tr><tr id="44793307"><td></td></tr><tr id="44793420"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793420" href="https://news.ycombinator.com/vote?id=44793420&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Vanguard is one of the cooler-structured brokerages. Vanguard (the management firm) is owned by the mutual funds themselves, of which you are an investor of. So their shareholder obligation is genuinely towards "clients" of the individual mutual funds. As far as I'm aware, this is the only mutually-owned mutual fund firm.</p><p>It's definitely got a solid track record and good fees, but these are things I'd feel weird about advertising it on HN for.</p></div></td></tr></tbody></table></td></tr><tr id="44793333"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793333" href="https://news.ycombinator.com/vote?id=44793333&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>vanguards very large and they're a mutual fund, so they have a lower profit motive than most brokers for it. they're basically the standard retirement fund company now</p></div></td></tr></tbody></table></td></tr><tr id="44791219"><td></td></tr><tr id="44790852"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790852" href="https://news.ycombinator.com/vote?id=44790852&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Most problems (including analytically intractible ones) can be modeled with a relatively simple monte-carlo simulation. Most simple monte-carlo simulations can be fully implemented in a spreadsheet.</p><p>Using timing coincidences in particle physics experiments is incredibly powerful. If multiple products from the same reaction can be measured at once, it's usually worth looking into.</p><p>Circular saws using wood cutting blades with carbide teeth can cut aluminum plates.</p><p>You can handle and attach atomically thin metal foils to things by floating them on water.</p><p>Use library search tools and academic databases. They are entirely superior to web search and AI.</p></div></td></tr></tbody></table></td></tr><tr id="44790911"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790911" href="https://news.ycombinator.com/vote?id=44790911&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I feel like the Monte-Carlo simulation modeling trick is one I picked up intuitively but only recently heard formalized. Do you (or anyone else) have a list of example problems that are solved in this way? Like a compendium of case studies on how to apply this trick to real world problems?</p></div></td></tr></tbody></table></td></tr><tr id="44791753"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44791753" href="https://news.ycombinator.com/vote?id=44791753&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>_How to Measure Anything_ by Douglas Hubbard includes a chapter on Monte Carlo simulations and comes with downloadable Excel examples: <a href="https://www.howtomeasureanything.com/3rd-edition/" rel="nofollow">https://www.howtomeasureanything.com/3rd-edition/</a> (scroll down to Ch. 6)</p><p>The main example is, you're considering leasing new equipment that might save you money. What's the risk that it will actually cost more, considering various ranges of potential numbers (and distributions)?</p><p>I think it's harder to apply to software since there are more unknowns (or the unknowns are fatter-tailed) but I still liked the book just for the philosophical framing at the beginning: you want to the measure things because they help you make decisions; you don't need perfect measurements since reducing the range of uncertainty is often enough to make the decision.</p></div></td></tr></tbody></table></td></tr><tr id="44791827"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44791827" href="https://news.ycombinator.com/vote?id=44791827&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>A simple example that highlights the strength of this method: a 137Cs point source is at the origin. A detector consisting of a right cylinder at arbitrary distance and orientation is nearby. What is the solid angle?</p><p>There may exist an analytical solution for this, but I wouldn't trust myself to derive it correctly. It would certainly be a huge mess.</p><p>If we add that the source is also a right cylinder instead of point source, and we want to add first order attenuation of emitted gammas by the source itself, the spreadsheet becomes only a bit more complex, but there will not be a pen and paper equation solution.</p><p>In this example every row of the spreadsheet would represent a hypothetical ray. One could randomly choose a location in the source, a random trajectory, and check if the photon intersects the detector. An alternative approach would be randomly choosing points in both target and detector, then doing additional math.</p><p>The results are recovered by making histograms and computing stats on the outputs of all the rows. You probably need a few thousand for most things at least. Remember roughly speaking 10k hits gets you ~1% statistics.</p></div></td></tr></tbody></table></td></tr><tr id="44793863"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793863" href="https://news.ycombinator.com/vote?id=44793863&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>What annoys me with Monte Carlo methods is trying to get self-consistent statistics at multiple parameter values. E.g., in your example, what is the derivative of the solid angle as the detector is moved? Or more generally, if we have some 'net goodness' measure for a system depending on parameters, how can we efficiently maximize it, when simulations are noisy and basins are shallow?</p><p>My understanding is that these sorts of questions come up in ML, and there are ways of dealing with it, but they can't converge nearly as fast as simple iterations like Newton's method. Even if I have to take a series approximation instead of a simple formula, I'll be able to use autodiff (or at worst, symbolic differentiation) to get quick and precise answers to these questions.</p></div></td></tr></tbody></table></td></tr><tr id="44793432"><td></td></tr><tr id="44791231"><td></td></tr><tr id="44793142"><td></td></tr><tr id="44793567"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793567" href="https://news.ycombinator.com/vote?id=44793567&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I've used various commercial databases over the years. Some popular commercial databases relevant to HN readers include Web of Science, Scopus, and Engineering Village. When I worked at the USPTO, I used the less popular database Dialog, which I preferred. To my knowledge, none of these are available direct to consumers. I've only been able to get access from places with subscriptions. Some university libraries allow visitors where you can use these databases for free on-site.</p><p>I would call these databases complementary, not "entirely superior". There are two main advantages. One is that these databases will contain many things that you can't find on Google. The second advantage of these databases is that they are designed for advanced searchers and have more powerful query languages. Google on the other hand is dumbed down and will try to guess what you want, often doing a poor job. You can get very specific on these databases in ways that you can't with Google.</p><p>Related: I'm somewhat fascinated by more specialized bibliographic databases because they often contain things that can't be found on Google or the major commercial databases I listed above. I started keeping a list of them. <a href="https://github.com/btrettel/specialized-bibs" rel="nofollow">https://github.com/btrettel/specialized-bibs</a></p></div></td></tr></tbody></table></td></tr><tr id="44794329"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794329" href="https://news.ycombinator.com/vote?id=44794329&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>On a related note, if you studied and got a degree at a university, check if they have an alumni program. I pay a small yearly fee that lets me access the university's academic databases and their VPN, so I get some other perks as it looks like I'm connected through eduroam.</p></div></td></tr></tbody></table></td></tr><tr id="44793656"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793656" href="https://news.ycombinator.com/vote?id=44793656&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>even wood tipped circular saw can cut Al plate...</p><p>and academic/library search is indeed so underrated!</p></div></td></tr></tbody></table></td></tr><tr id="44793964"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793964" href="https://news.ycombinator.com/vote?id=44793964&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Most people really cannot tell you what they want in any reasonable way. So expecting good specs for software without a very laborious interview and review process is pure wishful thinking. People "know what they like when they see it", so spend time rapid prototyping.</p><p>Smaller and more recent: iTerm has deep tmux support. Just do `tmux -CC` to start your session or `tmux -CC a` to attach to it and you don't have to memorise all the tmux commands.</p></div></td></tr></tbody></table></td></tr><tr id="44793892"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793892" href="https://news.ycombinator.com/vote?id=44793892&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Understanding the concept of Opportunity Cost and how it applies to everything in life.</p><p>- Buying a house: Is this the best return I can get on a downpayment. (Spoiler: It is not).</p><p>- Accepting a specific job offer: Is this the best way to spend 8 hours a day?</p><p>- Not making a successful trade, is as much as a loss than losing money explicitely on a trade.</p><p>- If you own something, you should consider what could you do with it's cash value if you sold it instead.</p><p>- If you have a paid off home, could you sell it and get a better ROI with the cash equivalent and rent instead? (The answer is yes and you should do it).</p></div></td></tr></tbody></table></td></tr><tr id="44794201"><td></td></tr><tr id="44794235"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794235" href="https://news.ycombinator.com/vote?id=44794235&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>As usual, I'd like to bring up that it is until you have enough profit for it to no longer be only about that.</p><p>Also regardless of the quality of the examples, it may be the case that the point (opportunity cost is important to consider) is still valid.</p></div></td></tr></tbody></table></td></tr><tr id="44794251"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794251" href="https://news.ycombinator.com/vote?id=44794251&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Thanks for clarifying. Being mindful of opportunity cost allows people to retire early, to spend more time with their loved ones, do better financial decisions.</p><p>I have seen too many of my loved ones work until their 70s because they didn't think about those concepts enough. That is what is sad.</p></div></td></tr></tbody></table></td></tr><tr id="44794236"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794236" href="https://news.ycombinator.com/vote?id=44794236&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>So It's really sad to think about the value of time or things? Don't you implicitly do that every time you chose an activity over another? When you chose a job over another? when you chose to spend your time with a friend or with your wife?</p></div></td></tr></tbody></table></td></tr><tr id="44791033"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44791033" href="https://news.ycombinator.com/vote?id=44791033&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>The big one, for keeping my focus on the power of repeated, consistent action, and prioritizing my "future selves":</p><p><i>What is likely to happen if I do (or don't do) this thing one thousand days (or times) in a row?</i></p><p>Examples:</p><p>- exercising 2h per day and eating right --&gt; I'm going to look and feel great and my health will be far better than that of my peers</p><p>- Should I buy these cookies along with the rest of my groceries? If I do that 1,000 grocery trips in a row …</p><p>- spending 30+ minutes per day reading the highest quality material I can find; taking notes; and figuring out ways to implement the knowledge and ideas I gain --&gt; …</p></div></td></tr></tbody></table></td></tr><tr id="44793663"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793663" href="https://news.ycombinator.com/vote?id=44793663&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is a good one. What's gonna happen if I respond to comments on HN for 1000 days in a row? Uh oh.</p></div></td></tr></tbody></table></td></tr><tr id="44791987"><td></td></tr><tr id="44791609"><td></td></tr><tr id="44793610"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793610" href="https://news.ycombinator.com/vote?id=44793610&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Whenever people try to argue that small and meaningful commits do not matter, I introduce them to git bisect. I’ve yet to meet someone familiar with it that does not agree.</p></div></td></tr></tbody></table></td></tr><tr id="44793447"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793447" href="https://news.ycombinator.com/vote?id=44793447&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Similar feelings about git worktree. Being able to check out multiple branches at once without having to deal with stash is a game changer.</p></div></td></tr></tbody></table></td></tr><tr id="44793335"><td></td></tr><tr id="44793086"><td></td></tr><tr id="44794262"><td></td></tr><tr id="44793347"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793347" href="https://news.ycombinator.com/vote?id=44793347&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Agreed with interactive rebase, but what of reflog? I've checked that for commit hashes when things have gone very wrong but I don't know many commands.</p></div></td></tr></tbody></table></td></tr><tr id="44793792"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793792" href="https://news.ycombinator.com/vote?id=44793792&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>It allows you to observe and interact with the history of refs like HEAD, so superficially destructive operations like amending a commit can be recovered, reverted, etc. It's kind of like meta-git, allowing version control operations on the version control system. It's a lifesaver when you botch a merge or something and allows you to do "destructive" operations fearlessly (up to a point, you can munge a git repo pretty bad if you try hard enough).</p></div></td></tr></tbody></table></td></tr><tr id="44794139"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794139" href="https://news.ycombinator.com/vote?id=44794139&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>duplicate code is not that bad. reduce duplication over time as you find the common patterns/abstractions, instead of trying to build abstractions that don't fit the use cases</p></div></td></tr></tbody></table></td></tr><tr id="44794214"><td></td></tr><tr id="44789137"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44789137" href="https://news.ycombinator.com/vote?id=44789137&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I only learned this in the last five years: do less, automate less, do more by hand, and use the limited capability of the manual method to really choose projects that are worthwile, rather than aim for maximum efficiency.</p></div></td></tr></tbody></table></td></tr><tr id="44790847"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790847" href="https://news.ycombinator.com/vote?id=44790847&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Similar to this: if you want to optimize your productivity*, do so on a timescale of at least weeks if not months or years.</p><p>Simple example: Can you get more done working 12 hours a day than 8? Sure, for the first day. Second day maybe. But after weeks, you're worse off in one way or another.</p><p>It's easy to chase imaginary gains like automating repetitive tasks that don't actually materialize, but some basics like sleep, nutrition, happiness, etc are 100% going to affect you going forward.</p><p>* I actually hate that word, and prefer saying "effectiveness". Productivity implies the only objective is more, more, more, endlessly. Effectiveness opens up the possibility that you achieve better results with less.</p></div></td></tr></tbody></table></td></tr><tr id="44794014"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794014" href="https://news.ycombinator.com/vote?id=44794014&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Realizing that buying a house is absolutely not a good investment and that the whole society is on a narrative to convince more and more people to blindly buy (realtors, lenders, other homeowners, the governments, parents,...)</p><p>Doing the real math is the trick of the trade. The math for owning has been made so that it looks like a good deal while in reality it is not at all. Most people will literally compare mortgage to their rent, or "I sold my house I bought for 500k for 1M$, therefore I made 500k$".</p><p>Treat owning as a luxury item. The same way you would own a sport car or travel on a private jet. Do the (real) math and realize Owning is costing you money.</p><p>Also don't let yourself get emotional about buying a house. Society has made it look as if buying a house was a proof of success. A lot of research shows that once people buy they lose flexibility, feel more stuck, cannot access higher paying job in a different places etc. Renting has a ton of advantages.</p><p>This calculator get most things right. As an exercise, you can try to retroactively put the numbers for the house you bought and the rent equivalent. The results might surprise you:
www.nytimes.com/interactive/2024/upshot/buy-rent-calculator.html</p></div></td></tr></tbody></table></td></tr><tr id="44794118"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44794118" href="https://news.ycombinator.com/vote?id=44794118&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>As I grew older (and hopefully wiser), I have internalized investment returns is a lot more than sanitized numbers. I did the numbers and as a result did not buy much real estate when I was yonger. It was pretty clear, if I put the same cash outflow into stocks, I would come out ahead... Way ahead!</p><p>The things is, I didn't put the same cash outflow in stocks. For various reasons. But mainly, it was just not psychologically palatable to max out my investments on equity by streching my finances. With hindsight, given the buffer in most real estate investments (left with tangible assets, banks shouldering some of the risk etc.) it would have been a lot more psychologically palatable to investment my max in real estate.</p><p>My point is not that real estate is "best". But merely, there is a lot more to returns than cold numbers. If you think you already understand it, and are under 30, you are likely underestimating the significance of that. I know I did.</p></div></td></tr></tbody></table></td></tr><tr id="44794161"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794161" href="https://news.ycombinator.com/vote?id=44794161&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is the argument that buying a house serves as a subpar, forced investment for most people.</p><p>But I don't know if I agree with you, I understood in my twenties that housing was generally a bad investment. We are now 10 years later and I invested everything in the stock market instead and came way ... way ahead than if I bough one or more houses.</p><p>I personally find it way way more scary to buy an average 2M$ house that could crash and lose half of its value overnight than the whole American economy that is relatively diversified.
What happens if there is a fire in your neighborhood? Or if your city is the next Detroit? 
We have been made to believe a house is a safe investment. I personally think it's the scariest least diversified investment you could ever make.</p></div></td></tr></tbody></table></td></tr><tr id="44794250"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794250" href="https://news.ycombinator.com/vote?id=44794250&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You know I hear this all the time but it never quite makes sense to me. REA is inherently a better asset class than the vast majority of assets, simply because you can leverage up without a risk of being stopped out.</p><p>Doing the numbers on (most) developed economies, buying freehold housing is typically a worse investment than stocks before leverage, but after, RE almost always comes ahead. Nobody is going to let you mortgage a basket of stocks, but almost any bank will let you do that with a house.</p><p>That said - I hate this idea because I think this kind of thinking is what has lead to many of societies problems in the developed world at the moment. But, from a rational point of view the numbers make sense.</p></div></td></tr></tbody></table></td></tr><tr id="44794268"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794268" href="https://news.ycombinator.com/vote?id=44794268&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>It's worse than that. Buying a house without leverage is a terrible investment.</p><p>The leverage is absolutely required because all the fees that come with real estate (Realtors, Interest, HOA, Taxes, Insurances, Closing costs, Downpayment opportunity cost,...). The leverage makes it an OK investment, but still historically not on the level of a diversified basket of stocks.</p><p>The leverage also works in both ways and essentially makes your house an even more risky asset. It supposes that it only ever goes up which has not always been the case historically.</p></div></td></tr></tbody></table></td></tr><tr id="44794094"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44794094" href="https://news.ycombinator.com/vote?id=44794094&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>*&gt; Most people will literally compare mortgage to their rent, or "I sold my house I bought for 500k for 1M$, therefore I made 500k$".</p><p>I’ll bite: what’s the problem? If anything mortgage is even more favorable than rent because part of it goes to paying off the principal. Which is still your money, just in a different asset. But I guess you include this and still reach the opposite result?</p><p>Are you talking about taxes or maintenance or something ? Which country / locality? What time span? How many places are there where you would’ve been better off renting than buying in recent memory… not where I’ve lived but you never know!</p></div></td></tr></tbody></table></td></tr><tr id="44794131"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794131" href="https://news.ycombinator.com/vote?id=44794131&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Because the mortgage and the rent are completely different numbers.</p><p>The rent is the maximum you will ever pay for that period.
The mortgage is the minimum you will pay and doesn't include the downpayment, repairs, HOA, Insurance and tax increases. It also doesn't include the closing costs, realtor costs, all the fees.</p><p>This is why the math is so complex. People easily forget all those fees and costs to make the math look simple.</p><p>This also takes into account that you invest into the US market and equity with all the cash that you would have had to spend on your house.</p><p>I'm talking about the US, but this holds true in multiple countries.</p><p>To answer your last question: In almost all HCOL (SF, Seattle, ...) you would have been way better renting than buying, except if you time it perfectly.
If you look at today's market you would almost definitely be better renting than buying (But nobody knows the future...)</p></div></td></tr></tbody></table></td></tr><tr id="44794299"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44794299" href="https://news.ycombinator.com/vote?id=44794299&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is just plain wrong. Let's break it down:</p><p>* Insurance and tax increases: real and you have to deal with them, but they don't go up as fast as rent does. Advantage: owning</p><p>* HOA fees: don't buy an HOA home, problem solved. You should do that anyway just because HOAs suck ass.</p><p>* Down payment: not a real cost, that money is yours in the form of equity</p><p>* Closing costs and other fees: real but completely negligible when amortized over the life of the home.</p><p>* Repairs: real, and they suck. This is the only place renting can stack up superior, but you can do a lot to mitigate this by doing your due diligence up front and not buying a dud house. Overall, unless you get unlucky you should come out ahead, but you can lose the die roll.</p><p>* Realtor costs: greatly depends. When we bought our house we paid zero, because the seller pays the buyer's costs. This can be a serious cost if you're changing houses a lot, but... just don't do that (for multiple reasons).</p><p>Overall, after buying my house 7 years ago I'm coming out ahead month over month (thanks to rent going up like clockwork every year, while my mortgage has stayed the same). By the time I die, I will be <i>significantly</i> ahead, and that's not even taking into account unrealized gains in the value of the property. Which I don't count for much, because my home is a place to live and not an investment vehicle. But maybe someday it'll do me good, or my heirs.</p><p>Owning a home <i>really is</i> a great deal for the <i>vast</i> majority of people in the US. It's not some society wide conspiracy, it's not an ad to get others into the market so you can benefit, it's the truth.</p></div></td></tr></tbody></table></td></tr><tr id="44794073"><td></td></tr><tr id="44794087"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794087" href="https://news.ycombinator.com/vote?id=44794087&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You are exactly right and I will add this to my post.</p><p>This calculator get most things right. As an exercise, you can try to retroactively put the numbers for the house you bought and the rent equivalent. The results might surprise you:
www.nytimes.com/interactive/2024/upshot/buy-rent-calculator.html</p></div></td></tr></tbody></table></td></tr><tr id="44794121"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44794121" href="https://news.ycombinator.com/vote?id=44794121&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is of course true if your house is not income producing. If you rent out rooms of your house, the calculus changes considerably.</p><p>…and now having viewed their calculator, I find it disingenuous that they haven’t added this as an additional configurable step, given how many bells and whistles their tool has. The takeaway from the tool is that you should never buy in HCOL areas, but renting out rooms in your unit is potentially a way to make it work out financially.</p></div></td></tr></tbody></table></td></tr><tr id="44794190"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44794190" href="https://news.ycombinator.com/vote?id=44794190&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I don't know if I agree. On average renting a room out of your place is the same as buying a place as a rental investment with the difference you can claim you live there (which gives you some small tax advantage).</p><p>In general, real estate rental investment are terrible subpar investment. How would this be different?</p></div></td></tr></tbody></table></td></tr><tr id="44792099"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44792099" href="https://news.ycombinator.com/vote?id=44792099&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>&gt; What Trick of the Trade took you too long to learn?</p><p>"Everything worth doing is worth doing badly"</p><p>And as a corollary, every complex system that works came from a simple system that works.</p><p>I learned this in programming, but now I apply it on everything from motorcycle maintenance, home appliance repair to parenting.</p><p>--</p><p>Often the easier way to fix a complex system is to pretend that it could be simpler and then reintroduce the complexity-inducing requirements.</p><p>I had a professor who taught debugging as a whole another skill from programming and used to say "Most of programming is starting from an empty editor and debugging until your code works".</p><p>The debugging "lab" in Java course (in the year 2000) was one of my transformational after-school classes - where I got a java program which fits within 2-3 pages of print code with a bug and was told to go find it in print for ~20 minutes, then given 40 minutes with a debugger instead.</p></div></td></tr></tbody></table></td></tr><tr id="44792563"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44792563" href="https://news.ycombinator.com/vote?id=44792563&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>&gt; And as a corollary, every complex system that works came from a simple system that works.</p><p>"Do the simplest thing that works" is one of the few core architecture principles I stick my neck out for time and time again. Why write a simple function when you can spend a week accounting for every imagined corner case and implement modular expansion capabilities! Please... stop...</p><p>I empathize with the debugger story. If you're super deep in a language it makes sense to know the debugger inside and out. But stdout is universal and I've never been a specific language developer rather than being able to jump into whatever is needed.</p></div></td></tr></tbody></table></td></tr><tr id="44793050"><td></td></tr><tr id="44790171"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790171" href="https://news.ycombinator.com/vote?id=44790171&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Writing tests first is a good way to end up with testable code. If you skip that, retrofitting tests is incredibly difficult.</p></div></td></tr></tbody></table></td></tr><tr id="44790762"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790762" href="https://news.ycombinator.com/vote?id=44790762&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>While it can be a useful forcing function, I find it also just fits into a higher velocity workflow in general, one I would describe like so:</p><p>1. Make PRs small, but not as small as you can possibly make them.</p><p>2. Intend to write at least one nice-ish test suite that tests like 50-80% of the LOC in the PR. Don't try to unit test the entire thing, that's not a unit test. And if something is intrinsically hard to test - requires extensive mocking etc - let it go instead of writing a really brittle test.</p><p>3. Tests only do two things: help you make the code correct right now, or help the company keep the code right long term. If your test doesn't do either, don't write it.</p><p>4. Ok - now code. And just keep in mind you're the poor sod who's gonna have to test it, so try to factor most of the interesting stuff to not require extensive mocking or shit tons of state.</p><p>I've found this workflow works almost anywhere and on almost any project or code. It doesn't require any dogmatic beliefs in PR sizes or test coverages. And it helps prevent the evils that dogmatic beliefs often lead you into. It just asks you to keep your eyes open and don't paint yourself into a corner, short term or long term.</p></div></td></tr></tbody></table></td></tr><tr id="44790930"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44790930" href="https://news.ycombinator.com/vote?id=44790930&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>One more piece - if the test would be hard to write, use that to drive you to clean up the architecture of that piece of code.</p></div></td></tr></tbody></table></td></tr><tr id="44790728"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44790728" href="https://news.ycombinator.com/vote?id=44790728&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Even if you aren't going to do the complete test code just writing down the expected checks for each test makes things so much easier</p></div></td></tr></tbody></table></td></tr><tr id="44790915"><td></td></tr><tr id="44793555"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793555" href="https://news.ycombinator.com/vote?id=44793555&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>When I stopped trying to be right and I started trying to be friends my career finally took off.</p></div></td></tr></tbody></table></td></tr><tr id="44793643"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793643" href="https://news.ycombinator.com/vote?id=44793643&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>When you buy a house and get a mortgage, you are going to be paying MUCH more in interest (than expected). Over the course of the mortgage, you are going to be paying MUCH more than the sticker price. Between closing costs and taxes and fees maintenance, you will need more cash than you think.</p><p>My advice is look at the numbers very carefully and choose something that is (below) or fits your budget. Sudden financial issues like the loss of a job or new vehicle purchase can put a big strain on all this.</p></div></td></tr></tbody></table></td></tr><tr id="44793748"><td></td></tr><tr id="44793875"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793875" href="https://news.ycombinator.com/vote?id=44793875&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>it's called renting.</p><p>Americans are obsessed with owning a home at all cost. This means that you are effectively bidding against people that do not even do the math. They are ready to spend Millions of dollar on something that is comparatively cheap to rent.</p><p>The fact that absolutely everyone wants to buy pushed prices through the roof. The good news is that you can take the other side of this bet. it's called renting.</p><p>Currently in most places in the US you will save literally millions over the 30 year mortgage by renting and investing in the market instead.</p><p>Reminder that renting and owning is functionally almost exactly the same thing.</p><p>Never trust your realtor and never trust other homeowner that most of the time never did the math (We all know those people: "I bought my home for 500k 15 years ago. It is now worth 1M$, therefore I made 500k$").</p><p>In other words, let other people take the irrational side of this bet and take the rational side by renting. It's an arbitrage opportunity.</p></div></td></tr></tbody></table></td></tr><tr id="44793997"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44793997" href="https://news.ycombinator.com/vote?id=44793997&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>An important fact that this doesn't account for is that, in the United States, living in housing that you own is highly tax-advantaged, at least if you can get a mortgage on it. For example, mortgage interest is tax-deductible for owner-occupied housing (whereas landlords usually can't deduct interest on their mortgages and so those taxes are passed on to renters), and mortgage rates for owner-occupied housing are far below market due to government subsidies and guarantees (whereas landlords have to pay higher rates that, again, they pass on to renters). This isn't good policy, but as long as it's the case, buying a single-family home is a smarter financial choice for most Americans than renting one.</p></div></td></tr></tbody></table></td></tr><tr id="44794047"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44794047" href="https://news.ycombinator.com/vote?id=44794047&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I live in the US and I'm aware of this. Those tax deductible interest should be calculated in the complex buy vs rent equation.</p><p>In fact, even when taking them into account, it still doesn't make sense for most Americans to own. (In today's market).</p><p>Renting and investing is still the way to go for at least 75% of Americans (this is slightly more nuanced for low cost of living areas, but hold true for any MCOL or HCOL areas).</p><p>Eventually the math could make sense again, but right now owning is a huge luxury that will cost you millions in the long run.</p><p>I invite you to play with this calculator:
www.nytimes.com/interactive/2024/upshot/buy-rent-calculator.html</p></div></td></tr></tbody></table></td></tr><tr id="44794178"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44794178" href="https://news.ycombinator.com/vote?id=44794178&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I am a renter so I don't have a horse in this race, but renting is many times the financially worse choice even in HCOL.</p><p>Why? Because rent inflates like crazy over here! In the Bay Area 7%+ a year is completely expected, and 10% is not unusual. I have been all over the Bay Area for more than a decade (San Francisco proper, East Bay, South Bay) and know this well. It's been nuts.</p><p>Random example: the 1 bedroom apartment that I lived in 2012 and was then going for $1,500 a month, is now going for $3,800 in the exact same building (with no/minimal renovations it seems, I just looked it up). An ~8% YoY increase. That will do it to any buy vs rent calculator, very easy to break even in under 5 years, and that's excluding the speculative ability to refinance if interest rates go down from the current 7%, in which case it becomes a huge boost.</p><p>Renting as a long term choice just works in European countries where normal people can lock in 5+ year leases with no or minimal rent increases. America is too profit-seeking and greedy for that.</p><p>I still rent for flexibility reasons, but I definitely see it as a luxury lifestyle choice, the most financially responsible thing would be to buy, even in HCOL.</p><p>All this in my opinion and personal experience, totally fine if people see it differently.</p></div></td></tr></tbody></table></td></tr><tr id="44794199"><td><table><tbody><tr><td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td><center><a id="up_44794199" href="https://news.ycombinator.com/vote?id=44794199&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>SF is the poster child of a HCOL where buying makes absolutely no sense.</p><p>Even if rent increases a lot, the buy to rent ratio is so horrible that it could continue to increase for MANY more years before buying could make sense.</p><p>I invite you to use the NYT Rent or buy calculator, It is clear as day: 
www.nytimes.com/interactive/2024/upshot/buy-rent-calculator.html</p></div></td></tr></tbody></table></td></tr><tr id="44794226"><td><table><tbody><tr><td indent="7"><img src="https://news.ycombinator.com/s.gif" height="1" width="280"></td><td><center><a id="up_44794226" href="https://news.ycombinator.com/vote?id=44794226&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>I just did, picturing exactly the situation I'm in right now:</p><p>- Rent: $3,500</p><p>- Home price: $700,000 (a similar unit just sold for this price a few months ago in my building)</p><p>- Rent increase: 8%</p><p>- All other parameters left as default, which seem reasonable (and as I said, there might be chances of refinancing over the next 10 years, which would drastically skew the picture, but I'm leaving that assumption out)</p><p>The ratio of 0.5% monthly rent/price is common for non-luxury "dated" condos all over the city, so I think my situation reflects well the typical renter.</p><p>Once again, in my personal experience, guided by a decade+ of living here, what people miss is the crazy rate of rent inflation. There is always a massive rent increase right around the corner, and God forbid if you are forced to move (because the landlord wants you to, it happened a couple times), then you take a gigantic hit at market rate. Once you factor in these occasional resets and the standard yearly increase, you get very close to 10% rent increase.</p></div></td></tr></tbody></table></td></tr><tr id="44794167"><td><table><tbody><tr><td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td><center><a id="up_44794167" href="https://news.ycombinator.com/vote?id=44794167&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>It's not all numbers, though.  Both have a lot of intangibles that can and should affect your decision.</p><p>Owning can feel suffocating at times, and like a ball and chain at others. You can't just decide you don't like it or the area anymore and go.  Maintenance is also no joke.</p><p>Renting feels ephemeral.  Getting kicked out at lease end sucks, it's hard to uproot everything and start over.  Having inane rules and a landlord constantly drive by can make you feel both infantile and spied on.</p><p>I've done both off and on and those are my own thoughts on the two.</p><p>Financially only it's easy to pick a winner.  But for some, one of these factors may be worth the extra however much money the difference is.</p></div></td></tr></tbody></table></td></tr><tr id="44794210"><td><table><tbody><tr><td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td><center><a id="up_44794210" href="https://news.ycombinator.com/vote?id=44794210&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>The best way to calculate those intangible is to associate a value to them, Most people love to say that owning is so good because they can decide on their own house improvement.</p><p>Ok, but how much do you really value this over? Is it worth 2M$ over 30 years? Because in a lot of cases this is what you leave on the table by deciding to own.</p></div></td></tr></tbody></table></td></tr><tr id="44794205"><td><table><tbody><tr><td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td><center><a id="up_44794205" href="https://news.ycombinator.com/vote?id=44794205&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>But mortgage interest is an itemized deduction, which means it only becomes a tax benefit when your interest + other deductions exceed the standard deduction. And if you do take the deduction, only the delta between it the standard deduction is a benefit.</p></div></td></tr></tbody></table></td></tr><tr id="44793718"><td></td></tr><tr id="44794023"><td></td></tr><tr id="44793708"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793708" href="https://news.ycombinator.com/vote?id=44793708&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You can memorize the correct opening moves in chess. For maybe my first year playing chess, I just YOLO'd the opening moves. My judgement there was probably not much worse than the rest of my play, but with other players playing engine moves in the opening, I was probably in a losing position early on in most of my games. I gained about, I think, 100 ELO after learning some 3 or 4 move opening combinations.</p></div></td></tr></tbody></table></td></tr><tr id="44794039"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794039" href="https://news.ycombinator.com/vote?id=44794039&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Claude Code recently showcased how powerful it can be when you don’t have to memorize commands. My AI agent works similarly. It finds the right CLI commands instead of relying on Playwright or an MCP server to perform tasks. What’s interesting is that even the agent doesn’t know many commands upfront; it simply uses the help option to discover what’s available.</p></div></td></tr></tbody></table></td></tr><tr id="44793618"><td></td></tr><tr id="44793962"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793962" href="https://news.ycombinator.com/vote?id=44793962&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Similarly, it's `XXX=123`, and cannot be `XXX = 123` or `$XXX=123`.</p><p>Shellcheck (and Wooledge) are crucial!</p></div></td></tr></tbody></table></td></tr><tr id="44793933"><td></td></tr><tr id="44793993"><td></td></tr><tr id="44789428"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44789428" href="https://news.ycombinator.com/vote?id=44789428&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>How do you maintain tests, in order for LLM edits to not keep breaking things?</p><pre><code>  - As a formal test suite in the program's own language?
  - Or using a .md natural language "tests" collection that must pass, which an LLM can understand?

</code></pre><p>
To answer the OP, I learned use different models for reasoning vs. coding.</p></div></td></tr></tbody></table></td></tr><tr id="44793743"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793743" href="https://news.ycombinator.com/vote?id=44793743&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Always fix errors in reverse order, from the bottom of the file to the top. That way the line numbers don't change as you go.</p></div></td></tr></tbody></table></td></tr><tr id="44793047"><td></td></tr><tr id="44790844"><td></td></tr><tr id="44793093"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793093" href="https://news.ycombinator.com/vote?id=44793093&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>But don’t fall into the trap of that’s all you do. This can lead to procrastination of the thing you’re trying to do.</p></div></td></tr></tbody></table></td></tr><tr id="44790879"><td></td></tr><tr id="44793448"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793448" href="https://news.ycombinator.com/vote?id=44793448&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Easy transfers between different people's iThings with Airdrop. I got my CompSci degree over 30 years ago and yet my 74yo aunt taught me this - the shame!</p></div></td></tr></tbody></table></td></tr><tr id="44793506"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44793506" href="https://news.ycombinator.com/vote?id=44793506&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>One other benefit, if it happens to matter to you, is that Airdropped files like photos or videos retain their original quality as opposed to taking a slight hit to quality when being transferred via text or email.</p></div></td></tr></tbody></table></td></tr><tr id="44793534"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793534" href="https://news.ycombinator.com/vote?id=44793534&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Also good to know is that if you crop a picture in your iOS photos app and then airdrop it to someone, they can undo the crop in their photos app. It is a non-obviously non-destructive operation.</p></div></td></tr></tbody></table></td></tr><tr id="44790848"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790848" href="https://news.ycombinator.com/vote?id=44790848&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>That macros ruin everything they touch. I used them extensively for maybe 15 years, stopped adding them, and then a bit later removed them all from my code. My C/C++ code was a lot nicer without them.</p></div></td></tr></tbody></table></td></tr><tr id="44791011"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44791011" href="https://news.ycombinator.com/vote?id=44791011&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>My early C code would have been awfully slow without them.  We needed enums and good inlining before we could ditch (most) macros.  When did Zorland/Zortech C become good enough?</p><p>There are still a few special cases where macros are useful, such as the multiple #include trick where a macro #defined before the #include determines what the macro invocations in the include file does -- really helpful for building certain kinds of tables.</p></div></td></tr></tbody></table></td></tr><tr id="44791313"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44791313" href="https://news.ycombinator.com/vote?id=44791313&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Zortech had an inliner even before it was Zortech.</p><p>The #include trick is called the "X Macro". I used it extensively, and eventually just removed it.</p></div></td></tr></tbody></table></td></tr><tr id="44791757"><td><table><tbody><tr><td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td><center><a id="up_44791757" href="https://news.ycombinator.com/vote?id=44791757&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Was it reliable enough?</p><p>(I have never played with it -- I saw the ads in Byte but I never met anybody who had tried it.  It seemed so ridiculously cheap that I felt it had to be a scam ;) )</p></div></td></tr></tbody></table></td></tr><tr id="44792862"><td></td></tr><tr id="44794172"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794172" href="https://news.ycombinator.com/vote?id=44794172&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Learn how to stab your colleagues in the back and play petty mean spirited office politics games. That's most of what being employed in this industry actually is, and if you accept that now and optimize yourself around it you can save decades wasted trying to actually get good at doing tangible (albeit economically useless) things.</p></div></td></tr></tbody></table></td></tr><tr id="44793384"><td></td></tr><tr id="44793825"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793825" href="https://news.ycombinator.com/vote?id=44793825&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>If you're selling something physical, always have free shipping. Include the price of shipping in the price of the product.</p><p>I guess people now expect free shipping via Amazon and boy does this make things sell faster.</p></div></td></tr></tbody></table></td></tr><tr id="44793158"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793158" href="https://news.ycombinator.com/vote?id=44793158&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>read something new every day before going to bed</p><p>journal before you start your day</p><p>buy some sort of electric kettle</p></div></td></tr></tbody></table></td></tr><tr id="44793112"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793112" href="https://news.ycombinator.com/vote?id=44793112&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Environment variables can be expanded at the command line, just like files.  Believe I started before it was a thing and never thought to check until twenty years later(?), when I hit the tab key by mistake. :-D</p></div></td></tr></tbody></table></td></tr><tr id="44794037"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44794037" href="https://news.ycombinator.com/vote?id=44794037&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>You should never look at code and say "this should work". If it "should" work it would work. If it doesn't work, you definitely made a mistake. Poke every assumption one by one. Preferably with an interactive debugger.</p><p>This may seem obvious but when I was younger I used to spin out in frustration at bugs.</p></div></td></tr></tbody></table></td></tr><tr id="44790820"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44790820" href="https://news.ycombinator.com/vote?id=44790820&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>When I realized that a) `screen` exists and b) what it does, I felt like an utter fool for having gone for years—<i>YEARS</i>—without benefiting from it.</p></div></td></tr></tbody></table></td></tr><tr id="44793398"><td></td></tr><tr id="44793550"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793550" href="https://news.ycombinator.com/vote?id=44793550&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>It is a terminal multiplexer. You will be able to find youtube videos. The gp is talking about a tool called gnu screen. If you need a more distinct token to search on try “tmux”.</p></div></td></tr></tbody></table></td></tr><tr id="44793883"><td></td></tr><tr id="44793551"><td><table><tbody><tr><td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td><center><a id="up_44793551" href="https://news.ycombinator.com/vote?id=44793551&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Pretty sure they mean GNU screen, a terminal multiplexer. Similar in functionality to tmux or zellj (a newer alternative) if you have heard of those.</p></div></td></tr></tbody></table></td></tr><tr id="44793355"><td></td></tr><tr id="44793336"><td></td></tr><tr id="44791973"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44791973" href="https://news.ycombinator.com/vote?id=44791973&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Measure everything. There are two benefits to this.</p><p>1. Discarding the bullshit. A consistent practice of weighting assumptions and conclusions on evidence/numbers helps identify biases and motives from other people.</p><p>2. Measures allow for value identification and performance. Most people just guess at this. Guessing is wrong more than 80% of the time and often wrong by multiple orders of magnitude.</p><p>Most people don’t think like this and find this line of thinking completely foreign, so I often just keep my conclusions to myself. To see a movie about this watch Money Ball.</p></div></td></tr></tbody></table></td></tr><tr id="44792060"><td><table><tbody><tr><td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td><center><a id="up_44792060" href="https://news.ycombinator.com/vote?id=44792060&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>This is interesting. I’ve often thought about building a dashboard for my personal life, comparable to the dashboards I build for growth teams.</p><p>And then iterating on it over time, and I find what’s valuable and what’s not.</p></div></td></tr></tbody></table></td></tr><tr id="44793439"><td><table><tbody><tr><td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td><center><a id="up_44793439" href="https://news.ycombinator.com/vote?id=44793439&amp;how=up&amp;goto=item%3Fid%3D44789068"></a></center></td><td><br>
<div><p>Learning that it is more important to know how to communicate, how to promote myself and the art of persuasion to get ahead than any technical skills I could know.</p></div></td></tr></tbody></table></td></tr><tr id="44793105"><td></td></tr><tr id="44790861"><td></td></tr><tr id="44793481"><td></td></tr><tr id="44793857"><td></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I asked four former friends why we stopped speaking (2023) (152 pts)]]></title>
            <link>https://www.vogue.com/article/reconnecting-with-ex-friends</link>
            <guid>44788783</guid>
            <pubDate>Mon, 04 Aug 2025 17:18:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vogue.com/article/reconnecting-with-ex-friends">https://www.vogue.com/article/reconnecting-with-ex-friends</a>, See on <a href="https://news.ycombinator.com/item?id=44788783">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content" tabindex="-1"><article lang="en-US"><div><header><div data-testid="ContentHeaderContainer"><figure><div><p><span><div data-testid="aspect-ratio-container"><picture><source media="(max-width: 767px)" srcset="https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_120,c_limit/00-story%20(2).jpg 120w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_240,c_limit/00-story%20(2).jpg 240w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_320,c_limit/00-story%20(2).jpg 320w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_640,c_limit/00-story%20(2).jpg 640w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_960,c_limit/00-story%20(2).jpg 960w" sizes="100vw"><source media="(min-width: 768px)" srcset="https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_120,c_limit/00-story%20(2).jpg 120w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_240,c_limit/00-story%20(2).jpg 240w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_320,c_limit/00-story%20(2).jpg 320w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_640,c_limit/00-story%20(2).jpg 640w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_960,c_limit/00-story%20(2).jpg 960w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_1280,c_limit/00-story%20(2).jpg 1280w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_1600,c_limit/00-story%20(2).jpg 1600w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_1920,c_limit/00-story%20(2).jpg 1920w, https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_2240,c_limit/00-story%20(2).jpg 2240w" sizes="100vw"><img alt="Women at beach with tear running through it" src="https://assets.vogue.com/photos/64e9155ea6d093ac3c19bb62/master/w_2560%2Cc_limit/00-story%2520(2).jpg"></picture></div></span></p><p><span>Photo: Getty Images</span></p></div></figure></div></header></div><div data-testid="ArticlePageChunks" data-attribute-verso-pattern="article-body"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>On a warm July evening, I dove into bed and grabbed my phone, giddy and anxious. As I scrolled through TikTok, attempting to calm my nerves, a Google Calendar notification flashed on the screen: “VIDEO CALL WITH SIMONE.”</p><p>Before I could swipe the reminder away, Simone FaceTimed me. I attempted to rehearse my greeting as the call buffered: <em>Should I keep it cool with a, “Hey, what’s good?” No, that sounds cold. What about a Keke Palmer-esque, “Girl!” No, that’s doing too much. “Good evening?” No, it’s not evening her time, that doesn’t even make sen—</em></p><p>“Girl!” Simone said with a chuckle.</p><p>I couldn’t help but crack a smile. As I’d learned over the course of our six-year friendship, her warmth never failed to replace my anxiety with joy.</p><p>“Damn, it’s been a <em>minute</em>.” she added.</p><p>She was right. Though Simone is my closest friend, we don’t see or talk to each other often. Both are my fault. In 2020, after months holed up in my tiny Washington, D.C. apartment, I decided to wait out the winter at my mother’s cottage in Kenya. It was just what the doctor ordered, and a few months later, I decided to move to Nairobi permanently.</p><p>My move changed our friendship—it changed <em>all</em> of my friendships, actually. I tried to stay in touch with my friends stateside for a while, but as time went on, FaceTime dates became harder to plan, and fewer voice notes were exchanged via WhatsApp. Now, I don’t know if I can call any of them friends anymore—and my relationship with Simone felt like it was hanging by a thread.</p><p>Things in Kenya aren’t much better. Though I’m Kenyan by ethnicity, I grew up abroad, in the US and UK, and I’ve found that my foreign accent and perspective other me, even within my family. These days, my social life tends to begin and end with nights on the couch, re-watching <em>Shameless</em> with my boyfriend. I’m ashamed and terrified about that reality; it feels dangerous to rely on only him for human connection.</p><p>After all, friends are witnesses to your life. They enrich the living experience. Not having that makes me feel like that tree that falls in the forest alone: Can anybody hear me? Do I matter?</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Marriage and family therapist Shontel Cargill promises me that these feelings are normal. She says that friendship loss in one’s mid-to-late 20s is common for several reasons: life transitions, romantic relationships, evolution of priorities, and more. And while it doesn’t happen to everyone, for some, friendship loss “can lead to psychological distress,” sparking issues with anxiety, depression, trust, and self-esteem. <em>Check, check, check, and check.</em></p><p>Cargill says that talking about your struggles with others can help the healing process, but I’ll be honest—that hasn’t worked for me. Most people I’ve spoken to about my predicament don’t get it, which only makes me feel worse. I tried to bring it up on my aforementioned call with Simone, but her empathetic smile and pitying eyes said it all: She couldn’t relate. Lucky her.</p><p>I needed answers. Concrete ones—not those generic suggestions that I “put myself out there” or “just give it time.” Everyone around me had managed to hold on to friends throughout their lives; everyone seemed to be on girls’ trips and boozy brunches; seemed to have a tribe of confidants ready to drop everything for them. And here I was, a lonely, overworked 28-year-old who spent way too much time in her apartment, wondering why she didn’t have any of that.</p><p>So, like a good journalist, I decided to investigate. After speaking to Simone, I determined that I’d reach out to some of my former friends directly, and see if we could have a conversation about why we “broke up.” Many declined, and understandably so. But to my surprise, a few agreed to participate in my crazy scheme.&nbsp;</p><p>Here are those conversations—and their revelations. Their names have been changed.</p><p>Celine</p><p>Circumstance brought Celine and me together. We were both new freshmen at an international school in Nairobi, and our shared fear proved the perfect BFF elixir. Celine was sweet and reserved, with a quiet confidence that I admired—even more so when I got older. But she wanted to do her own thing and I, a not-so-confident 14-year-old, wanted to fit in. I had a hunger for popularity, and when I realized that Celine didn’t share that, I neglected the friendship. Soon, it evaporated.</p><p>Celine remembered things similarly.</p><p>“Once school started, we new kids were initially welcomed into the group of ‘misfits’ that every high school has,” she wrote to me via Facebook. “But eventually, we broke away—you, to join the funny kids, a group of hilarious and friendly people who could match your unparalleled wit and high-octane energy and me, to join the kids at the back of the bus, literally and figuratively.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I thought that by the end of high school, we were strangers. But Celine reminded me that we had our special moments, and that there was always love between the two of us. “By senior year, we were moving in completely different circles, even in our somewhat tiny school. But, to my mind, there wasn’t an acrimonious end to our friendship, and we could always share a funny moment here and there. We just evolved in different directions,” she wrote.</p><p>Celine’s kindness surprised me—the pain of friendship breakups past had colored the way I thought about <em>all</em> of my former chums. I forgot that people can grow apart and still love each other from afar. Celine lives in Europe, and the chances of us revitalizing our former bond are slim. But I feel a sense of peace, knowing that we’ll always be rooting for each other.</p><p>Steve</p><p>My cousin Steve and I have always had a love-hate relationship. I despised him when we were young—during one squabble, I was so consumed with fury that I took our shared Game Boy Advance and threw it down the stairs, destroying it—but, peppered throughout my memories of us going toe-to-toe are flashes of roaring laughter. The more joyful side of our relationship really developed when I started at the international school. He had already been a student there for a few years, and to my surprise, he took me under his wing. Those were some of the best years of my life—we partied (arguably too much), we cried, we learned. We were free. And when my boyfriend died in a tragic accident during our senior year, Steve became my fiercest protector. He allowed me to grieve, however I chose to. When he held my hand it felt like he’d never let go.</p><p>But he did, because he had to. It was time to go to college, and for us to have our own adventures. I tried to stay connected to him, but it didn’t seem like he was interested in pursuing an adult friendship. Texts would go unanswered, calls missed, and after a while, my bond with Steve felt as lost as my youth.</p><p>When I first floated the idea of this article to Steve, he didn’t think our relationship qualified. “We’re family,” he explained on WhatsApp. “And the thing about family is, relationships can wax and wane and friends drift apart, but, you know, families still have to come back together.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>That is true. Steve and I are bonded by blood, so he’ll always be a part of my life in some capacity. But being family is not synonymous with being friends, and I think the understanding that we’re irrevocably tied may be part of why we aren’t close anymore. Why try, when I know I’ll see you at some cousin’s wedding or brother’s baby shower?</p><p>So, we had another conversation. Steve was hesitant—it took weeks for him to get back to me—and he didn’t say much, but he did change the way I looked at our estrangement.</p><p>He described what he considered a defining moment in our friendship’s collapse. In 2017, Steve and I both found ourselves living at home in Kenya, depressed and unemployed. Our college years were tough, and we needed a break to figure things out. Looking back, I remember my own pain being my top priority. I barely noticed that Steve was struggling too, and it goes without saying that I wasn’t there for him. We were in Kenya for months, but only spent one, disastrous night together, when we barely spoke. (I, for one, was too busy making out with my cousin’s friend.)</p><p>“I don’t know if you remember that night, but I remember I took you home and spent all night talking to you and consoling you,” Steve told me. He went on to explain that at the time, we were in exactly the same place emotionally, but we weren’t there for each other. “We were both Kenyan-Americans who had this lovely upbringing, and we both faced trials and adversities when it comes to the United States,” he recounted. “We almost had to come back home to recuperate, and to find some sort of moral guide. We were going through something so similar and there were so many anecdotes and so much support that we could have given to one another, but we really didn't.”</p><p>If anything, Steve found our relationship one-sided, feeling that the support he showed me was never reciprocated. But that, I argued, wasn’t <em>entirely</em> fair. I’ve tried to be there for Steve over the years, but he’s evasive and holds his emotions very close to his chest. How can I show up for someone who doesn’t let me in?</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>This, he could understand. “I’m usually the one who’s more distant,” he admitted.</p><p>Yet Steve asked an interesting question toward the end of the conversation—one that I can’t stop thinking about. “Can we boil down our lack of a relationship to a series of instances?” He asked. “Or is it more that at some point, neither of us felt lonely enough to put in the work to maintain the relationship?”</p><p>Our conversation ended with Steve suggesting that closeness comes solely from in-person interactions. Because he lives stateside and I’m in Kenya, the likelihood of us having that time is slim to none. I don’t know where my relationship with Steve stands now, but I do know that I feel defeated and misunderstood by him. Maybe that will change one day, but if not, I’ll just have to be content with the friendship we had—or, the one <em>I</em> thought we had, anyway.</p><p>Matt</p><p>Matt, I met in college. He was a year older, and worked behind the front desk of my dorm. It wasn’t long before his polite smiles as I entered the building graduated to conversations about classes, crushes, and Greek life. Soon after that, we became proper friends.</p><p>Matt was the first person who made college feel like home to me. He made the <em>US</em> feel like home. I hadn’t lived stateside in years, and to my surprise, I was out of the loop with American culture. I often felt out of place—except, that is, when I was with Matt. A white Texan-Californian whose family runs a 5K every Thanksgiving, he was, to my surprise, made of everything I was made of.</p><p>It’s possible that even then, our connection wasn’t the healthiest. I remember being jealous of his other relationships, particularly with our mutual friend Madison. As they grew closer, I felt left out, and like I had to fight for his love and attention. I sensed that Matt knew what was happening, and that he didn’t like what he saw.</p><p>Years passed, and Matt and I remained close, even after we both graduated from college. But then, he decided to move back to Texas.</p><p>I don’t know why Matt and I didn’t try harder to stay in touch. I wanted to visit him, but my minimum-wage salary was not going to cover travel costs. He seemed to have little interest in texts or FaceTimes, but I would still try to reach out every now and again to see how he was doing. He was nice enough during those virtual interactions, but it was clear he had moved on. I found myself wondering if our relationship meant more to me than it did to him.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Those insecurities came to a head a few years ago. My boyfriend and I were in a really bad place, and I found myself alone and devastated. I needed a friend, so I reached out to Matt—and boy, did I trauma dump on him. He was kind and listened patiently, but I didn’t hear from him for months after that. Then, when he finally resurfaced, he leveled with me, explaining that what I had gone through was a lot to be confronted with, especially after we hadn’t spoken in a while. As a somewhat overly emotional person with deep abandonment issues, that was all I needed to hear. I got it, but I was crushed. I’m still crushed.</p><p>“I think you’re really hard on yourself,” Matt said in a voice message recently, when I rehashed all of this. It wasn’t that I had driven him away, he urged, but that he (like Steve) had a different communication style, and was more reticent. “I’m hyper-focused on whatever environment I’m in at the moment, and I know that seems really annoying to say, but that hyper-awareness stops me from reaching out to people,” he continued. “l think about you every single day—like, you are one of my best friends in my entire life—but I’m so bad at reminding you and other people I love of that.”</p><p>As I wiped away tears, Matt went on to open up about how he’s changed over the years, and how it’s shifted the way he looks at the Boyfriend Incident. He explained that back then, he’d thought that relationships were simple: If you and your beau weren’t getting along, you should leave him. That mentality affected the way that he responded to my woes. Besides, he’d been going through troubles of his own. “I don’t think either of us were in a good place,” he confessed.</p><p>Matt said a lot of wonderful things about me and our friendship during our conversation, but one thing meant the most. “In my mid 20s, I was really selfish,” he said. “But I’m currently at a point where I don’t really care about things for myself. Now that I’m almost 30, my loved ones and my friendship are all that really matter.”</p><p>I was so inspired by Matt’s introspection. Not only did it give me hope for our future as friends, but it also felt like proof that these conversations, however hard and emotional, were worth it.</p><p>Dominique</p><p>When I first met Dominique, I was sure we would be friends forever. It was sorority rush, and amid the sea of women I spoke to that hellish week, Dominique stood out. That wasn’t only because we were two of the handful of Black women participating in the Greek process; Dominique was also fabulous and accessible, she was effortlessly warm and hilarious, and she had a glow that reminded you not to take life too seriously.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I learned to love everything about Dominique—not just her star quality, but also her vulnerability, her darkness. She quickly became my most treasured friend. There wasn’t anything we couldn’t do or talk about. We could party together, we could drink wine at home together, we could cry together, we could gently call each other out. We could save each other.</p><p>I didn’t want to do anything without her by my side.</p><p>I can’t pinpoint when things changed in our friendship. I had to leave college for a semester due to medical issues, and during that period I was disengaged from everyone close to me. It cost me a lot of friends, including Dominique, to an extent. When I returned, she was distant. She had graduated and was moving on from our college life, yes, but the rift felt deeper than that. She wasn’t there when I needed her most, but I also wasn’t telling her what I needed.</p><p>I held onto that resentment, and Dominique and I continued to grow apart. She found herself in a dangerously toxic relationship, and instead of helping her, I just worried from afar.</p><p>Out of all of these daunting conversations with my former friends, I was most nervous to talk to Dominique. I knew I’d failed her as a friend, and I wasn’t sure if I was prepared for her to not-so-gently call me out on it.</p><p>Yet she did the opposite. She couldn’t have been kinder or more gracious about what happened between us. “I am in my maturity now, [and] I have come to an understanding and a realization that I am not a person that’s good at maintaining friendships,” she confessed to me. I was shocked. Beautiful, brilliant, lovely Dominique, not good with friendships? 2015 me wouldn’t have believed it.</p><p>It turns out that Dominique felt the same way I did. She’d thought I’d shut her out, and instead of talking to me about it, she’d taken a step back. <em>Maybe we’re not as close as I think we are</em>, she’d mused, adding that she felt “out of the loop” when I was struggling with my health. She’d become “comfortable with the idea [that] there were other people that were closer to you than me.” All the while, I’d thought that <em>I</em> wasn’t as important to her as she was to me. We both agreed that nothing concrete had happened; neglected feelings had just led us to stop communicating.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>I can’t believe Dominique and I waited so many years to have this conversation. I’d harbored so much guilt, confusion, and pain over our friendship. It had haunted me, and played a big role in the way I saw myself as a friend. And all this time, Dominique had thought the same of herself.</p><p>I don’t know if Dominique and I will ever be friends like we used to be, but the olive branch has been extended. And, for the first time, I feel hopeful.</p><p>At first, my motivation for talking to my former friends about why we fell out of touch was a little masochistic. I thought I was a bad friend, and my loneliness was a product of my own self-centeredness, my stubbornness, my tendency to either vent or withhold. I thought I deserved to be punished by the people I’d wronged.</p><p>I’m not walking away from these conversations with the conviction that I’m a <em>good</em> friend, or even a good person. However, talking with my ex-friends did remind me that loving people—even platonically—isn’t easy. Sometimes you hurt your friends, sometimes they hurt you, and sometimes there’s no hurt at all, but they still fade away like a memory. Life is short, but it’s long, too. If you’re lucky, people will come in and out of your life and, for however long they’re there, you’ll feel loved.</p><p>So, should I tackle my ex-boyfriends next?</p></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I spent 6 years building a ridiculous wooden pixel display (1031 pts)]]></title>
            <link>https://benholmen.com/blog/kilopixel/</link>
            <guid>44787902</guid>
            <pubDate>Mon, 04 Aug 2025 16:16:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benholmen.com/blog/kilopixel/">https://benholmen.com/blog/kilopixel/</a>, See on <a href="https://news.ycombinator.com/item?id=44787902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2>TL,DR: I built the world's most impractical 1000-pixel display and anyone in the world can draw on it</h2>

<iframe width="560" height="315" src="https://www.youtube.com/embed/4OUF7sfAuHA?si=f_ypc5pkT7tevWDZ&amp;controls=0&amp;modestbranding=1&amp;rel=0&amp;showinfo=0&amp;autoplay=1&amp;mute=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>If you just want to play with it, goto <a href="https://kilopx.com/?ref=blog">kilopx.com</a>.</p>

<h3>The backstory</h3>

<p>Six years ago I had an idea to build a large, inefficient display with a web interface that anyone could interact with. I've enjoyed <a href="https://en.wikipedia.org/wiki/Danny_Rozin">Danny Rozin's unconvenional mirrors</a> over the years and was inspired by an <a href="https://github.com/TomWhitwell/SlowMovie">eInk movie player that played at 24 frames per <em>hour</em></a> that got me thinking about a laborious display that could slowly assemble an image.</p>

<p>I landed on the idea of a 40×25 pixel grid of pixels, turned one by one by a single mechanism. Compared to our modern displays with millions of pixels changing 60 times a second, a wooden display that changes a single pixel 10 times a <em>minute</em> is an incredibly inefficient way to create an image. Conveniently, 40×25 = 1,000 pixels, leading to the name <em>Kilopixel</em> and the six-letter domain name <a href="https://kilopx.com/?ref=blog">kilopx.com</a>. How do you back down from that? That's the best domain name I've ever owned.</p>

<p>So I got to work. This project has everything: a web app, a physical controller, a custom CNC build, generated gcode, tons of fabrication, 3d modeling, 3d printing, material sourcing - so much to get lost in. It's the most ambitious project I've ever built.</p>

<h3>The first prototype: 21×3 pixels</h3>

<p>My first thought was to use a wooden gantry that would ride on some sort of track. Since I'm most comfortable working with wood, it's my default prototyping medium. However, I quickly pivoted to extruded aluminum and the excellent <a href="https://openbuildspartstore.com/v-slot-linear-rail-1/">hardware kits from Openbuilds</a> that include pulleys, gantry parts, extruded aluminum, and timing belts. It's very similar to the materials used in 3D printer frames, and connects very easily with off the shelf stepper motors. This allowed me to build a gantry with X and Y, essentially a wall-mounted XY plotter. I built the first prototype with two stepper motors, a Raspberry Pi, a CNC controller, and a beefy power supply. It allowed me to generating and sending instructions to the CNC controller to move to a particular pixel, turning that pixel, and reading values from sensors. It also revealed quite a few problems with my pixel choices and pixel manipulation mechanisms.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/prototype-1-1.jpg" alt="The first prototype" width="1440" height="1440" loading="lazy"></p>

<h3>1,000 of anything is <em>expensive</em></h3>

<p>Picking pixels was a real adventure. I've tried ping pong balls, styrofoam balls, bouncy balls, wooden balls, 3d printed balls, golf balls, foam balls...anything approximately spherical and about 1-1.5in in diameter. The problems I encountered were largely cost (even a 50¢ ball is $500 of balls), weight (again, a thousand of these things), and availability. For a long time I thought ping pong balls were my best bet, so I purchased a few hundred of them, 3d printed painting jigs, and spray painted them. I used a hot nail to melt two opposite holes on each ball so they could be strung up on the display.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/ping-pong-pixels-1.jpg" alt="Painting ping pong pixels" width="1080" height="1080" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/ping-pong-pixels-2.jpg" alt="Painting ping pong pixels" width="1080" height="1080" loading="lazy">
</p>

<h3>Ping pong balls are basically soda cans</h3>

<p>You can stand on a soda can, <em>as long as it's not open</em>. Open the can, and it crushes easily. Ping pong balls are the same way. They're relatively strong until you melt two holes in them. Then they can be deformed, which is fatal to any spray paint you've put on them. And not only are they fragile, but the cheap ones are inconsistently sized, and a half millimeter here and there adds up when you have a row of 40 balls. Ping pong balls were a no-go.</p>

<h3>Nerfed</h3>

<p>My next attempt at a cheap, spherical pixel was foam Nerf balls - much smaller than ping pong balls, and only available in bright colors. They accepted spray paint OK but the paint deteriorated over time.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/prototype-2-pixel-1.jpg" alt="Nerf balls covered in black paint" width="1440" height="1920" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/prototype-2-pixel-2.jpg" alt="Nerf balls half painted in black paint" width="1440" height="1920" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/prototype-2.jpg" alt="The second prototype" width="1440" height="1080" loading="lazy">
</p>

<p>It was difficult to consistently bore a hole through the nerf balls, and they really liked to grab the wire and were hard to turn. I struggled to consistently turn them and I wasn't thrilled with the bright colors.</p>

<p>I also tried bouncy balls (hard to drill a hole, hard to paint, inconsistent sizes, heavy), wooden balls (not very round, hard to paint a crisp line, heavy), and styrofoam balls (hard to paint with acrylic paint, and they melt with spray paint).</p>

<h3>Turning balls</h3>

<p>I had the idea to use a small, slow motor to rotate a LEGO wheel against the ping pong ball. I'd use a reflectivity sensor to detect if it was showing black or white, and stop once the pixel was rotated properly. I modeled and printed a custom hub for a LEGO wheel, a few different mechanisms to move the wheel in and out of contact of the sphere, and an interface for the gantry. I tried using a solenoid to push the motor into the ball, which was underpowered, and a servo. Neither approach worked great and ultimately I decided this ball turning approach was a dead end.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/gantry-with-servo.png" alt="Gantry mechanism with servo" width="906" height="970" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/lego-wheel-hub.png" alt="LEGO wheel hub" width="906" height="970" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/prototype-2-wheel.jpg" alt="Wheel turning mechanism" width="1440" height="1080" loading="lazy">
</p>

<h3>Pivoting to non-spherical pixels</h3>

<p>In 2024 I had a couple of productive conversations with <a href="https://sideprojectpodcast.com/episodes/kilopixel-with-ben-holmen">Joe Tannenbaum on the Side Project podcast</a> and <a href="https://overengineered.fm/episodes/the-art-of-pairing-with-strangers-w-ben-holmen">Chris Morrell on the Over Engineered podcast</a>. Those conversations helped me consider that maybe balls were not the way to go - I thought about flaps and illuminated buttons, then settled on a cubic wooden pixel. I also decided to manufacture the pixels myself because I'm very comfortable in my wood shop. This decision cost me a huge amount of time because doing things one thousand times takes <em>forever</em>, but I was really pleased with how it operated and looked.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/painting-timelapse.gif" alt="Painting a thousand pixels timelapse" width="480" height="270" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/finished-pixel.jpg" alt="Finished pixel" width="1440" height="1080" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/many-pixels.jpg" alt="Hundreds of pixels" width="1440" height="1080" loading="lazy">
</p>

<h3>Building the grid</h3>

<p>I'd learned from earlier prototypes that I needed to strictly define a grid and not depend on the pixels themselves for spacing. That 40mm pixel might be 39.5mm, or 41mm. And that variation adds up across 40 pixels - you might be 10mm off by the end of the row. So for my (hopefully final) build I created 25 thin shelves, drilled 40 holes in each one using a jig to enforce consistent spacing, and threaded pixels on 40 metal wires. This was painstaking and time consuming - I broke it down into multiple sessions over several weeks. But it did create a very predictable grid of pixels and guaranteed that each pixel moved completely independently of the surrounding pixels.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/pixel-assembly.jpg" alt="Dozens of pixels being assembled" width="1440" height="1080" loading="lazy">
<img src="https://benholmen.com/assets/images/kilopixel/assembling-timelapse.gif" alt="Assembling the pixels timelapse" width="480" height="270" loading="lazy">
</p>

<p>Finally, I had my first thousand-pixel display and it seemed promising! I could stop here and have some interesting wall art - and it feels amazing to swipe across with your hand. But we're not stopping! In Wisconsin, we say <em>Forward!</em></p>

<h3>A CNC machine in my office</h3>

<p>I've used a hobby CNC machine in my wood shop for many years, so I was familiar with the basics of CNC and the possibilities for this project. Generally speaking, a CNC machine is something that takes very specific movement instructions written in a language called gcode, and uses those instructions to move to a certain position and do something like drill a hole, cut a groove, or burn with a laser. Stepper motors are typically used because they move very predictably when they receive electrical signals from a CNC controller. Common hobby CNC machines include laser engravers (Glowforge), milling machines (X-Carve), and 3D printers. They all use movement instructions to move X, Y, and Z axes very precisely and do things at those coordinates.</p>

<p>It's easy to find a basic CNC controller that can be used for a CNC mill, a laser engraver, or plotter. These CNC controllers accept gcode over USB/serial, and turn stepper motors to put the machine in the correct position. They typically run <a href="https://github.com/gnea/grbl">grbl</a>, an open source gcode parser that runs on Arduinos.</p>

<p>The Kilopixel is essentially a 2-axis machine that uses the third axis for the pixel poking mechanism.</p>

<p>I connected a Raspberry Pi to the CNC controller and use it for two purposes: querying my API to get the next pixel, writing the appropriate gcode to get there, activating the pixel poker, and then reading a light sensor to determine the physical state of the pixel. It then returns that state to the API and continues the loop. This is run with a Python script and depends on <a href="https://github.com/joan2937/pigpio">pigpio</a> to read the light sensor over GPIO pins.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/completed-display.jpg" alt="Completed Kilopixel display" width="1440" height="1117" loading="lazy"></p>

<h3>Poking pixels</h3>

<p>The pixels rotate and have a notch that registers every 90° to encourage them to align properly. To turn them, I created a reciprocating poking mechanism that uses a flexible glue stick to push on the edge of the pixel. As the pixel turns, the poker moves to the right and lifts up slightly, then moves out of the way and retracts. This is all controlled by gcode and is a rather finicky part of the whole machine.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/poker-1.gif" alt="Prototype stepper motor poker" loading="lazy" width="480" height="270">
<img src="https://benholmen.com/assets/images/kilopixel/poker-2.gif" alt="Pixel poker poking the pixels" loading="lazy" width="480" height="480">
</p>

<h3>We're changing pixels. What should we draw?</h3>

<p>At this point, I have a thousand pixel display that listens to an API and changes pixels one-by-one. So what does the API say?</p>

<p>The API is controlled by a <a href="https://kilopx.com/?ref=blog">web app</a> that is the source of truth for what should be on the display. It has a few modes:</p>

<ul>
<li>User-submitted: anyone can submit a 40×25 image to be drawn, and the most popular submission will be drawn next. Loop forever.</li>
<li>Real-time collaboration: there's a single picture being drawn, and anyone can change any pixel in real time. This doesn't work great with many participants, but is a solid choice if I install the Kilopixel in a coffee shop or something.</li>
<li>Idle modes: I wrote a few algorithms to generate shapes and patterns, but my favorite mode is a clock that can barely keep up with drawing itself.</li>
</ul>

<p>For the public launch of the Kilopixel I chose the user-submitted mode, and you can <a href="https://kilopx.com/draw?ref=blog">submit your own right now</a>. Or <a href="https://kilopx.com/submissions?ref=blog">vote for the submissions</a> you want to see drawn next.</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/kilopx.com-submissions.png" alt="Screenshot of kilopx.com showing user submissions" loading="lazy" width="1496" height="1340">
<img src="https://benholmen.com/assets/images/kilopixel/kilopx.com-draw.png" alt="Screenshot of kilopx.com showing user submissions" loading="lazy" width="1496" height="1340">
</p>

<p>I tinkered with a few stacks for the web app over the years, using it as an excuse to try new things. At first it was a node/Socket.IO app, then a Laravel + Livewire app, and finally a Laravel + InertiaJS + VueJS app. It's hosted on a modest DigitalOcean VPS. It also runs locally on my laptop to record and upload video.</p>

<h3>Putting it out there</h3>

<p>Since the inception of the project, I really wanted this to be something that I shared with at least a few people. It's neat to have in my office, but if it was just for my own enjoyment, it wouldn't be worth all this effort.</p>

<p>I originally planned to hang this in my friend's coffee shop and let a few people at a time interact with it. I still love this idea! And I might do it.</p>

<p>But what I'm really excited about it putting this on the internet for <em>everyone</em> and that means recording and streaming the display in my office. Here's the setup:</p>

<p><img src="https://benholmen.com/assets/images/kilopixel/streaming-setup.jpg" alt="" width="1440" height="930" loading="lazy"></p>

<p>There are two webcams involved: one mounted directly on the pixel poker for a closeup, and one wide shot. The two cameras are combined in OBS where I can stream to YouTube, and the wide shot is also recorded continuously using ffmpeg. Streaming to YouTube provides a live view of the physical device alongside the digital queue of submissions. The camera, USB hub, and light are hung from the ceiling with a respectful amount of jank for the streaming phase of this project.</p>

<p>Besides streaming, the laptop is running a scheduled job that queries the API to see if a submission has recently finished drawing. If it does, it generates a rather complex ffmpeg command to generate a one minute timelapse of the submission being drawn. The timelapse is uploaded to kilopx.com and posted to Bluesky where it can be shared by the creator of the artwork - <a href="https://bsky.app/profile/kilopx.com/post/3lutmwu7kls2v">for example, pixel art by Matt Stauffer</a></p>

<h3>Something physical, in my office, controlled by the internet. What could go wrong?</h3>

<p>I've built some defensive features into the web app so I can mitigate common abuse patterns if they become a problem. I've decided to not lock it down prematurely - I think it might be fun to see what people can do with this thing! Voting is open to anyone with a few basic session checks, submission of artwork requires a Bluesky OAuth login, and I have a mechanism to quickly delete problem submissions.</p>

<p>I'll see what the internet does and adapt accordingly!</p>

<h3>What next?</h3>

<p>I'm sincerely hoping the internet has fun with this project for a bit! Once it winds down, I've considered turning control of the display over to an internet friend - after all, it just hits an API, why not yours? If you're interested, <a href="https://benholmen.com/cdn-cgi/l/email-protection#ceacaba08eacaba0a6a1a2a3aba0e0ada1a3">email me</a>.</p>

<p>And then, the final destination will be behind me on my webcam - I'll let anyone on a video call monkey with my background to their heart's content. What could go wrong?</p>

<p>In the meantime, please <a href="https://kilopx.com/draw?ref=blog">submit something</a> or just <a href="https://kilopx.com/?ref=blog">follow along</a>!</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla withheld data, lied, misdirected police to avoid blame in Autopilot crash (476 pts)]]></title>
            <link>https://electrek.co/2025/08/04/tesla-withheld-data-lied-misdirected-police-plaintiffs-avoid-blame-autopilot-crash/</link>
            <guid>44787780</guid>
            <pubDate>Mon, 04 Aug 2025 16:07:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/08/04/tesla-withheld-data-lied-misdirected-police-plaintiffs-avoid-blame-autopilot-crash/">https://electrek.co/2025/08/04/tesla-withheld-data-lied-misdirected-police-plaintiffs-avoid-blame-autopilot-crash/</a>, See on <a href="https://news.ycombinator.com/item?id=44787780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="841" src="https://electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=1600" alt="" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2025/08/Tesla-Autopilot-crash-data-lies-Elon-Musk.png?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Tesla was caught withholding data, lying about it, and misdirecting authorities in the wrongful death case involving Autopilot that it lost this week.</p>



<p>The automaker was undeniably covering up for Autopilot.</p>



<p>Last week, a jury found Tesla partially liable for a wrongful death involving a crash on Autopilot. I explained the case in the verdict in <a href="https://electrek.co/2025/08/01/tesla-tsla-is-found-liable-in-fatal-autopilot-crash-has-to-pay-329-million/" target="_blank" rel="noreferrer noopener">this article </a>and <a href="https://www.youtube.com/watch?v=fzV1i7LeRrw">video</a>.</p>



<p>But we now have access to the trial transcripts, which confirm that Tesla was extremely misleading in its attempt to place all the blame on the driver.</p>	
	



<p>The company went as far as to actively withhold critical evidence that explained Autopilot’s performance around the crash.</p>



<h3 id="h-tesla-withheld-the-crash-snapshot-data-that-its-own-server-received-within-minutes-of-the-collision">Tesla <strong>withheld the crash‑snapshot data that its own server received within minutes of the collision</strong></h3>



<p>Within about three minutes of the crash, the Model S uploaded a “collision snapshot”—video, CAN‑bus streams, EDR data, etc.—to Tesla’s servers, the “Mothership”, and received an acknowledgement. The vehicle then deleted its local copy, resulting in Tesla being the only entity having access.</p>



<p>What ensued were years of battle to get Tesla to acknowledge that this collision snapshot exists and is relevant to the case.</p>



<p>The police repeatedly attempted to obtain the data from the collision snapshot, but Tesla led the authorities and the plaintiffs on a lengthy journey of deception and misdirection that spanned years.</p>



<p>Here, in chronological order, is what happened based on all the evidence in the trial transcript:</p>



<h3><strong>1 | 25 Apr 2019 – The crash and an instant upload Tesla pretended never happened</strong></h3>



<p><span>Within ~3 minutes of the cras</span>h, the Model S packaged sensor video, CAN‑bus, EDR, and other streams into a single&nbsp;“snapshot_collision_airbag-deployment.tar”&nbsp;file and pushed it to Tesla’s server, then deleted its local copy.</p>



<p>We know that now, thanks to forensic evidence extracted from the onboard computer.</p>



<p>The plaintiffs hired Alan Moore, a mechanical engineer who specializes in accident reconstruction, to forensically recover data from the Autopilot ECU (computer).</p>



<p>Based on the data, Moore was able to confirm that Tesla had this “collision snapshot” all along, but “unlinked” it from the vehicle:</p>



<blockquote>
<p><em>“That tells me within minutes of this crash Tesla had all of this data … the car received an acknowledgement … then said ‘OK, I’m done, I’m going to unlink it.’”</em></p>
</blockquote>



<p>The plaintiffs tried to obtain this data, but Tesla told them that it didn’t exist.</p>



<p>Tesla’s written discovery responses were shown during the trial to prove that the company acted as if this data were not available.</p>



<hr>



<h3><strong>2 | 23 May 2019 – Tesla’s lawyer scripts the homicide investigator’s evidence request</strong></h3>



<p>Corporal Riso, a homicide investigator with the Florida Highway Patrol (FHP), sought Tesla’s help in retrieving telemetry data to aid in reconstructing the crash.</p>



<p>He was put in contact with Tesla attorney Ryan McCarthy and asked if he needed to subpoena Tesla to get the crash data.</p>



<p>Riso said of McCarthy during the trial:</p>



<blockquote>
<p><em>“He said it’s not necessary. <strong>‘Write me a letter and I’ll tell you what to put in the letter.’</strong>”</em></p>
</blockquote>



<p>At the time, he didn’t see Tesla as an adversary in this case and thought that McCarthy would facilitate the retrieval of the data without having to go through a formal process. However, the lawyer crafted the letter to avoid sending the police the full crash data.</p>



<p>Riso followed the instructions verbatim. He said during the trial:</p>



<blockquote>
<p>“I specifically wrote down what the attorney at Tesla told me to write down in the letter.”</p>
</blockquote>



<p>But McCarthy specifically crafted the letter to ommit sharing the colllision snapshot, which includes bundled video, EDR, CAN bus, and Autopilot data.</p>



<p>Instead, Tesla provided the police with infotainment data with call logs, a copy of the Owner’s Manual, but not the actual crash telemetry from the Autopilot ECU.</p>



<p>Tesla never said that it already had this data for more than a month by now.</p>



<hr>



<h3><strong>3 | June 2019 – A staged “co‑operation” that corrupts evidence</strong></h3>



<p>Tesla got even more deceptice when the police specifically tried to collect the data directly from the Autopilot computer.</p>



<p>On June 19, 2019, Riso physically removed the MCU and Autopilot ECU from the Tesla.</p>



<p>Again, the investigator thought that Tesla was being collaborative with the investigation at the time so he asked the company how to get the data out of the computer. He said at the trial:</p>



<blockquote>
<p>I had contacted Mr. McCarthy and asked him how I can get this data off of the computer components. He said that he would coordinate me meeting with a technician at their service center, the Tesla service center in Coral Gables.</p>
</blockquote>



<p>Tesla arranged for Riso to meet Michael Calafell, a Tesla technician, at the local service center in in Coral Gables with the Autopilot ECU and the Model S’ MCU, the two main onboard computers.</p>



<p>To be clear, Tesla already had all this data in its servers and could have just sent it to Riso, but instead, they lured him into its service center with the piece of evidence in his custody.</p>



<p>What ensued was pure cinema.</p>



<p>Michael Calafell, who testified never having been tasked with extracting data from an Autopilot ECU before, connected both computers to a Model S in the shop to be able to access them, but he then claimed that the data was “corrupted” and couldn’t be access.</p>



<p>Riso said during his testimony:</p>



<blockquote>
<p><strong>I brought the center tablet [MCU] and the flat silver box [Autopilot ECU] with multicolored connectors to the Tesla service center.”</strong></p>
</blockquote>



<blockquote>
<p><strong>“I watched Mr. Calafell the whole time. The evidence was in my custody. I did not let it out of my sight.”</strong></p>
</blockquote>



<p>However, the situation got a lot more confusing as Calafell swore in an affidavit that he didn’t actually power the ECU, only the MCU, on that day, June 19.</p>



<p>Only years later, when Alan Moore, the forensic engineer hired by the plaintiff, managed to get access to the Autopilot ECU, we learned that Tesla undeniably powered up the computer on June 19 and the data was accessible.</p>



<hr>



<h3><strong>4 | 2019 – 2024 – Repeated denials and discovery stonewalling</strong></h3>



<p>Through years of communications with the police, the plaintiffs and the court through the investigation and later the discovery process for the lawsuit, Tesla never mentioned that it had all the data that explained how Autopilot saw the crash, which everyone was seeking, sitting on its servers for years.</p>



<p>The facts are:</p>



<ul>
<li>Tesla had the data on its servers within minutes of the crash</li>



<li>When the police sought the data, Tesla redirected them toward other data</li>



<li>When the police sought Tesla’s help in extracting it from the computer, Tesla falsely claimed it was “corrupted”</li>



<li>Tesla invented an “auto-delete” feature that didn’t exist to try explain why it couldn’t originally find the data in the computer</li>



<li>When the plaintiffs asked for the data, Tesla said that it didn’t exist</li>



<li>Tesla only admitted to the existence of the data once presented with forensic evidence that it was created and transfered to its servers.</li>
</ul>



<hr>



<h3 id="h-5-late-2024-court-orders-a-bit-for-bit-nand-flash-image"><strong>5 | Late 2024 – Court orders a bit‑for‑bit NAND‑flash image</strong></h3>



<p>By late 2024, the court allowed the plantiffs to have a third-party expert access the Autopilot ECU to try to acccess the data that Tesla claimed was now corrupted.</p>



<p>The court allowed the forensic engineers to do a bit-for-bit NAND flash image, which consists of a complete, sector-by-sector copy of the data stored on a NAND flash memory chip, including all data, metadata, and error correction code (ECC) information.</p>



<p>The engineers quickly found that all the data was there despite Tesla’s previous claims.</p>



<p>Moore, the forensic engineer hired by the plaintiffs, said:</p>



<blockquote>
<p>“Tesla engineers said this couldn’t be done… yet it was done by people outside Tesla.”</p>
</blockquote>



<p>Now, the plaintiffs had access to everything.</p>



<hr>



<h3><strong>6 | Feb‑Mar 2025 – The forensic “treasure‑trove” reveals the file name &amp; checksum</strong></h3>



<p>Moore was astonished by all the data found through cloning the Autopilot ECU:</p>



<blockquote>
<p>“For an engineer like me, the data out of those computers was a treasure‑trove of how this crash happened.”</p>
</blockquote>



<p>The data that Tesla had provided was not as easily searchable, the videos were grainy, and it was missing key alerts and timestamps about Autopilot and its decision-making leading up to the crash.</p>



<p>On top of all the data being so much more helpful, Moore found unallocated space and metadata for ‘snapshot_collision_airbag‑deployment.tar’, including its SHA‑1 checksum and the exact server path.</p>



<hr>



<h3><strong>7 | May 2025 – Subpoenaed server logs corner Tesla</strong></h3>



<p>Armed with the the newly found metadata, plaintiffs were able to subpoenaed Tesla’s AWS logs. </p>



<p>Tesla still fought them, but facing a sanctions hearing, Tesla finally produced the untouched TAR file plus access logs showing it had been stored <strong>since 18:16 PDT on 25 Apr 2019</strong>—the same three‑minute timestamp Moore had highlighted.</p>



<p>The automaker had to admit to have the data all along.</p>



<p>During the trial, Mr. Schreiber, attorney for the plaintiffs, claimed that Tesla used the data for its own internal analysis of the crash:</p>



<blockquote>
<p>They not only had the snapshot — they used it in their own analysis. It shows Autopilot was engaged. It shows the acceleration and speed. It shows McGhee’s hands off the wheel.</p>
</blockquote>



<p>Yet, it didn’t give access to the police nor the family of the victim who have been trying to understand what happened to their daughter.</p>



<hr>



<h3><strong>8 | July 2025 Trial – The puzzle laid bare for the jury</strong></h3>



<p>Finally, this entire situation was laid bare in front of the jury last month and certainly influenced the jury in their verdict.</p>



<p>The jury was confronted with clear evidence of Tesla trying to hide data about the crash, and then, they were shown what that data revealed.</p>



<p>The data recovered made a few things clear:</p>



<ul>
<li>Autopilot was active</li>



<li>Autosteer was controlling the vehicle</li>



<li>No manual braking or steering override was detected from the driver</li>



<li>There was <strong>no record of a “Take Over Immediately” alert</strong>, despite approaching a T-intersection with a stationary vehicle in its path.</li>



<li>Moore found logs showing <strong>Tesla systems were capable of issuing such warnings</strong>, but <strong>did not</strong> in this case.</li>



<li>Map and vision data from the ECU revealed:
<ul>
<li>Map data from the Autopilot ECU included a flag that the area was a <strong>“restricted Autosteer zone.”</strong></li>



<li>Despite this, the system <strong>allowed Autopilot to remain engaged</strong> at full speed.</li>
</ul>
</li>
</ul>



<p>Moore commented on the last point:</p>



<blockquote>
<p>“Tesla had the map flag. The car knew it was in a restricted zone, yet Autopilot did not disengage or issue a warning.”</p>
</blockquote>



<p>This was critical to the case as one of the arguments was that Tesla dangerously let owners use Autopilot on roads it was not designed to operate on as it was specifically trained for highways.</p>



<p>The National Transportation Safety Board (NTSB) had even worn Tesla about it and the automaker didn’t geofenced the system:.</p>



<p>The NTSB had wrote Tesla:</p>



<blockquote>
<p>“Incorporate system safeguards that limit the use of automated vehicle control systems to those conditions for which they were designed (the vehicle’s operational design domain).”</p>
</blockquote>



<p>The driver was responsible for the crash and he admitted as such. He admitted to not using Autopilot properly and not paying attention during the crash.</p>



<p>However, the main goal of the plaintiffs in this case was to assign part of the blame for the crash to Tesla for not preventing such abuse of the system despite the clear risk.</p>



<p>The logic is that if Tesla had implemted geofencing and better driver monitoring, the driver, McGee, would have never been able to use Autopilot in this case, which could have potentially avoidded putting himself in the situation that led to the crash.</p>



<p>That’s on top of Autopilot failing at what Tesla has repeatedly claim it could do: stop those crashes from happening in the first place.</p>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>



<p>Tesla fans need to do a quick exercise in empathy right now. The way they are discussing this case, such as claiming the plaintiffs are just looking for a payout, is truly appalling.</p>



<p>You should put yourself in the family’s shoes. If your daughter died in a car crash, you’d want to know exactly what happened, identify all contributing factors, and try to eliminate them  to give some meaning to this tragic loss and prevent it from happening to someone else.</p>



<p>It’s an entirely normal human reaction. And to make this happen in the US, you must go through the courts.</p>



<p>Secondly, Tesla fans need to do a quick exercise in humbleness. They act like they know exactly what this case is about and assume that it will “just be thrown out in appeal.”</p>



<p>The truth is that unless you read the entire transcripts and saw all the evidence, you don’t know more about it than the 12 jurors who unanimously decided to assign 33% of the blame for the crash to Tesla.</p>



<p>And that’s the core of the issue here. They want to put all the blame on the driver, and what the plaintiffs were trying to do was just assign part of the blame on Tesla, and the jurors agreed.</p>



<p>The two sides are not that far off from each other. They both agreed that most of the blame goes to the driver, and even the driver appears to agree with that. He admitted to being distracted and he quickly settled with the plaintiffs.</p>




	<p>This case was only meant to explore how Tesla’s marketing and deployment of Autopilot might have contributed to the crash, and after looking at all the evidence, the jury agreed that it did.</p>



<p>There’s no doubt that the driver should bare most of the responsability and there’s no doubt that he didn’t use Autopilot properly.</p>



<p>However, there’s also no doubt that Autopilot was active, didn’t prevent the crash despite Tesla claiming it is safer than humans, and Tesla was warned to use better geo-fencing and driver monitoring to prevent abuse of the system like that.</p>



<p>I think a 33% blame in this case is more than fair.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/450t9Bz"><img src="https://electrek.co/wp-content/uploads/sites/3/2025/07/NativeBanner_ElecktrekDisplayAds_BeaumontRev2.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen-Image: Crafting with native text rendering (500 pts)]]></title>
            <link>https://qwenlm.github.io/blog/qwen-image/</link>
            <guid>44787631</guid>
            <pubDate>Mon, 04 Aug 2025 15:56:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwenlm.github.io/blog/qwen-image/">https://qwenlm.github.io/blog/qwen-image/</a>, See on <a href="https://news.ycombinator.com/item?id=44787631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/merge3.jpg#center" width="100%"></figure><p><a href="https://github.com/QwenLM/Qwen-Image" target="_blank">GITHUB</a>
<a href="https://huggingface.co/Qwen/Qwen-Image" target="_blank">HUGGING FACE</a>
<a href="https://modelscope.cn/models/Qwen/Qwen-Image" target="_blank">MODELSCOPE</a>
<a href="https://modelscope.cn/aigc/imageGeneration?tab=advanced" target="_blank">DEMO</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><p>We are thrilled to release <strong>Qwen-Image</strong>, a 20B MMDiT image foundation model that achieves significant advances in complex text rendering and precise image editing. To try the latest model, feel free to visit <a href="https://chat.qwenlm.ai/">Qwen Chat</a> and choose “Image Generation”.</p><p>The key features include:</p><ul><li><strong>Superior Text Rendering</strong>: Qwen-Image excels at complex text rendering, including multi-line layouts, paragraph-level semantics, and fine-grained details. It supports both alphabetic languages (e.g., English) and logographic languages (e.g., Chinese) with high fidelity.</li><li><strong>Consistent Image Editing</strong>: Through our enhanced multi-task training paradigm, Qwen-Image achieves exceptional performance in preserving both semantic meaning and visual realism during editing operations.</li><li><strong>Strong Cross-Benchmark Performance</strong>: Evaluated on multiple public benchmarks, Qwen-Image consistently outperforms existing models across diverse generation and editing tasks, establishing a strong foundation model for image generation.</li></ul><h2 id="performance">Performance</h2><p>We present a comprehensive evaluation of Qwen-Image across multiple public benchmarks, including GenEval, DPG, and OneIG-Bench for general image generation, as well as GEdit, ImgEdit, and GSO for image editing. Qwen-Image achieves state-of-the-art performance on all benchmarks, demonstrating its strong capabilities in both image generation and editing. Furthermore, results on LongText-Bench, ChineseWord, and TextCraft show that it excels in text rendering—particularly in Chinese text generation—outperforming existing state-of-the-art models by a significant margin. This highlights Qwen-Image’s unique position as a leading image generation model that combines broad general capability with exceptional text rendering precision.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center" width="100%"></figure><h2 id="demo">Demo</h2><p>One of Qwen-Image’s outstanding capabilities is its ability to achieve high-fidelity text rendering in different scenarios. Let’s take a look at the following Chinese rendering case:</p><blockquote><p>宫崎骏的动漫风格。平视角拍摄，阳光下的古街热闹非凡。一个穿着青衫、手里拿着写着“阿里云”卡片的逍遥派弟子站在中间。旁边两个小孩惊讶的看着他。左边有一家店铺挂着“云存储”的牌子，里面摆放着发光的服务器机箱，门口两个侍卫守护者。右边有两家店铺，其中一家挂着“云计算”的牌子，一个穿着旗袍的美丽女子正看着里面闪闪发光的电脑屏幕；另一家店铺挂着“云模型”的牌子，门口放着一个大酒缸，上面写着“千问”，一位老板娘正在往里面倒发光的代码溶液。</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/aliyun.png#center%20" width="100%"></figure><p>The model not only accurately captures Miyazaki’s anime style, but also features shop signs like “云存储” “云计算” and “云模型” as well as the “千问” on the wine jars, all rendered realistically and accurately with the depth of field. The poses and expressions of the characters are also perfectly preserved.</p><p>Let’s look at another example of Chinese rendering:</p><blockquote><p>一副典雅庄重的对联悬挂于厅堂之中，房间是个安静古典的中式布置，桌子上放着一些青花瓷，对联上左书“义本生知人机同道善思新”，右书“通云赋智乾坤启数高志远”， 横批“智启通义”，字体飘逸，中间挂在一着一副中国风的画作，内容是岳阳楼。</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/tongyi.png#center" width="100%"></figure><p>The model accurately drew the left and right couplets and the horizontal scroll, applied calligraphy effects, and accurately generated the Yueyang Tower in the middle. The blue and white porcelain on the table also looked very realistic.</p><p>So, how does the model perform on English?
Let’s look at an English rendering example:</p><blockquote><p>Bookstore window display. A sign displays “New Arrivals This Week”. Below, a shelf tag with the text “Best-Selling Novels Here”. To the side, a colorful poster advertises “Author Meet And Greet on Saturday” with a central portrait of the author. There are four books on the bookshelf, namely “The light between worlds” “When stars are scattered” “The slient patient” “The night circus”</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/book.png#center%20" width="100%"></figure><p>In this example, the model not only accurately outputs “New Arrivals This Week”, but also accurately generates the cover text of four books: “The light between worlds”, “When stars are scattered”, “The slient patient”, and “The night circus”.</p><p>Let’s look at a more complex case of English rendering:</p><blockquote><p>A slide featuring artistic, decorative shapes framing neatly arranged textual information styled as an elegant infographic. At the very center, the title “Habits for Emotional Wellbeing” appears clearly, surrounded by a symmetrical floral pattern. On the left upper section, “Practice Mindfulness” appears next to a minimalist lotus flower icon, with the short sentence, “Be present, observe without judging, accept without resisting”. Next, moving downward, “Cultivate Gratitude” is written near an open hand illustration, along with the line, “Appreciate simple joys and acknowledge positivity daily”. Further down, towards bottom-left, “Stay Connected” accompanied by a minimalistic chat bubble icon reads “Build and maintain meaningful relationships to sustain emotional energy”. At bottom right corner, “Prioritize Sleep” is depicted next to a crescent moon illustration, accompanied by the text “Quality sleep benefits both body and mind”. Moving upward along the right side, “Regular Physical Activity” is near a jogging runner icon, stating: “Exercise boosts mood and relieves anxiety”. Finally, at the top right side, appears “Continuous Learning” paired with a book icon, stating “Engage in new skill and knowledge for growth”. The slide layout beautifully balances clarity and artistry, guiding the viewers naturally along each text segment.</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/six.png#center%20" width="100%"></figure><p>In this case, the model needs to generate 6 submodules, each with its own icon, title, and corresponding introductory text. Qwen-Image has completed the layout.</p><p>What about smaller text? Let us test it:</p><blockquote><p>A man in a suit is standing in front of the window, looking at the bright moon outside the window. The man is holding a yellowed paper with handwritten words on it: “A lantern moon climbs through the silver night, Unfurling quiet dreams across the sky, Each star a whispered promise wrapped in light, That dawn will bloom, though darkness wanders by.” There is a cute cat on the windowsill.</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/small.png#center%20" width="100%"></figure><p>In this case, the paper is less than one-tenth of the entire image, and the paragraph of text is relatively long, but the model still accurately generates the text on the paper.</p><p>What if there are more words? Let’s try a harder case:</p><blockquote><p>一个穿着"QWEN"标志的T恤的中国美女正拿着黑色的马克笔面相镜头微笑。她身后的玻璃板上手写体写着 “一、Qwen-Image的技术路线： 探索视觉生成基础模型的极限，开创理解与生成一体化的未来。二、Qwen-Image的模型特色：1、复杂文字渲染。支持中英渲染、自动布局； 2、精准图像编辑。支持文字编辑、物体增减、风格变换。三、Qwen-Image的未来愿景：赋能专业内容创作、助力生成式AI发展。”</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/long.png#center%20" width="100%"></figure><p>You can see that the model has completely generated a complete handwritten paragraph on the glass plate.</p><p>What if it’s bilingual? For the same scenario, let’s try this prompt:</p><blockquote><p>一个穿着"QWEN"标志的T恤的中国美女正拿着黑色的马克笔面相镜头微笑。她身后的玻璃板上手写体写着 “Meet Qwen-Image – a powerful image foundation model capable of complex text rendering and precise image editing. 欢迎了解Qwen-Image, 一款强大的图像基础模型，擅长复杂文本渲染与精准图像编辑”</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bi.png#center%20" width="50%"></figure><p>As you can see, the model can switch between two languages at any time when rendering text.</p><p>Qwen-Image’s text capabilities make it easy to create posters, such as:</p><blockquote><p>A movie poster. The first row is the movie title, which reads “Imagination Unleashed”. The second row is the movie subtitle, which reads “Enter a world beyond your imagination”. The third row reads “Cast: Qwen-Image”. The fourth row reads “Director: The Collective Imagination of Humanity”. The central visual features a sleek, futuristic computer from which radiant colors, whimsical creatures, and dynamic, swirling patterns explosively emerge, filling the composition with energy, motion, and surreal creativity. The background transitions from dark, cosmic tones into a luminous, dreamlike expanse, evoking a digital fantasy realm. At the bottom edge, the text “Launching in the Cloud, August 2025” appears in bold, modern sans-serif font with a glowing, slightly transparent effect, evoking a high-tech, cinematic aesthetic. The overall style blends sci-fi surrealism with graphic design flair—sharp contrasts, vivid color grading, and layered visual depth—reminiscent of visionary concept art and digital matte painting, 32K resolution, ultra-detailed.</p></blockquote><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/poster.png#center%20" width="50%"></figure><p>Since we can make posters, of course we can also make PPTs directly. Let’s look at a case of making PPTs in Chinese:</p><blockquote><p>一张企业级高质量PPT页面图像，整体采用科技感十足的星空蓝为主色调，背景融合流动的发光科技线条与微光粒子特效，营造出专业、现代且富有信任感的品牌氛围；页面顶部左侧清晰展示橘红色Alibaba标志，色彩鲜明、辨识度高。主标题位于画面中央偏上位置，使用大号加粗白色或浅蓝色字体写着“通义千问视觉基础模型”，字体现代简洁，突出技术感；主标题下方紧接一行楷体中文文字：“原生中文·复杂场景·自动布局”，字体柔和优雅，形成科技与人文的融合。下方居中排布展示了四张与图片，分别是：一幅写实与水墨风格结合的梅花特写，枝干苍劲、花瓣清雅，背景融入淡墨晕染与飘雪效果，体现坚韧不拔的精神气质；上方写着黑色的楷体"梅傲"。一株生长于山涧石缝中的兰花，叶片修长、花朵素净，搭配晨雾缭绕的自然环境，展现清逸脱俗的文人风骨；上方写着黑色的楷体"兰幽"。一组迎风而立的翠竹，竹叶随风摇曳，光影交错，背景为青灰色山岩与流水，呈现刚柔并济、虚怀若谷的文化意象；上方写着黑色的楷体"竹清"。一片盛开于秋日庭院的菊花丛，花色丰富、层次分明，配以落叶与古亭剪影，传递恬然自适的生活哲学；上方写着黑色的楷体"菊淡"。所有图片采用统一尺寸与边框样式，呈横向排列。页面底部中央用楷体小字写明“2025年8月，敬请期待”，排版工整、结构清晰，整体风格统一且细节丰富，极具视觉冲击力与品牌调性。</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/ppt.png#center%20" width="100%"></figure></blockquote><p>In fact, beyond text processing, Qwen-Image also excels at general image generation, supporting a wide range of artistic styles. From photorealistic scenes to impressionistic paintings, from anime styles to minimalist designs, the model flexibly responds to a wide range of creative prompts, becoming a versatile tool for artists, designers, and storytellers. We will describe these in detail in our technical report.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s2.jpg#center%20" width="100%"></figure><p>In terms of image editing, Qwen-Image supports a variety of operations, including style transfer, additions, deletions, detail enhancement, text editing, and character pose adjustment. This allows even ordinary users to easily achieve professional-level image editing. We will describe these in detail in our technical report.</p><figure><img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/s3.jpg#center%20" width="100%"></figure><p>In summary, we hope that Qwen-Image can further promote the development of image generation, lower the technical barriers to visual content creation, and inspire more innovative applications. At the same time, we also look forward to the active participation and feedback of the community to jointly build an open, transparent, and sustainable generative AI ecosystem.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Customizing tmux (160 pts)]]></title>
            <link>https://evgeniipendragon.com/posts/customizing-tmux-and-making-it-less-dreadful/</link>
            <guid>44787374</guid>
            <pubDate>Mon, 04 Aug 2025 15:41:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://evgeniipendragon.com/posts/customizing-tmux-and-making-it-less-dreadful/">https://evgeniipendragon.com/posts/customizing-tmux-and-making-it-less-dreadful/</a>, See on <a href="https://news.ycombinator.com/item?id=44787374">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><p>I have been exploring some new tools here and there. When I started watching <a href="https://www.youtube.com/c/theprimeagen">Primeagen</a>, I took a note of several tools that he was using and advocating for. One of them was tmux.</p><h2 id="what-is-tmux">What is tmux?</h2><p><a href="https://github.com/tmux/tmux/wiki">tmux</a> is a terminal multiplexer. What that means is you can have many terminals in one. According to tmux wiki:</p><blockquote><p>tmux is a program which runs in a terminal and allows multiple other terminal programs to be run inside it. Each program inside tmux gets its own terminal managed by tmux, which can be accessed from the single terminal where tmux is running - this called multiplexing and tmux is a terminal multiplexer.</p></blockquote><p>tmux goes further by allowing you to have a lot of control over how the terminals are displayed, how they function, and how are they styled. You can have many separate windows as tabs. You can have many panes within one window. Are you working on several different projects? Then why don’t you have multiple sessions - each of them containing sets of windows and panes - all giving you access to multiple terminals. With a tool like this you no longer need to arrange multiple terminal instance windows on your desktop or using multiple workspaces for those instances - you can have it all in one place.</p><p>One additional feature that tmux boasts in is the ability to retain the terminal session upon its closure. Imagine that you have a long running process like a local server or a persistent connection like ssh session. If you were to close that window, you would lose that session and the process that was in it. Tmux has the ability to detach and attach to multiple sessions making them easier to manage and not to worry about losing it.</p><p>tmux wiki goes into detail talking about several other features of tmux:</p><blockquote><p>The main uses of tmux are to:</p><ul><li><p>Protect running programs on a remote server from connection drops by running them inside tmux.</p></li><li><p>Allow programs running on a remote server to be accessed from multiple different local computers.</p></li><li><p>Work with multiple programs and shells together in one terminal, a bit like a window manager.</p></li></ul><p>For example:</p><ul><li><p>A user connects to a remote server using ssh(1) from an xterm(1) on their work computer and run several programs. perhaps an editor, a compiler and a few shells.</p></li><li><p>They work with these programs interactively, perhaps start compiling, then close the xterm(1) with tmux and go home for the day.</p></li><li><p>They are then able to connect to the same remote server from home, attach to tmux, and continue from where they were previously.</p></li></ul></blockquote><h2 id="my-first-tmux-experience">My first tmux experience</h2><figure><img loading="lazy" src="https://github.com/tmux/tmux/wiki/images/tmux_default.png" alt="The first impression of tmux didn’t leave me with much excitement of exploring what is truly possible :/ Source: https://github.com/tmux/tmux/wiki/Getting-Started"><figcaption><p>The first impression of tmux didn’t leave me with much excitement of exploring what is truly possible :/ Source: <a href="https://github.com/tmux/tmux/wiki/Getting-Started">https://github.com/tmux/tmux/wiki/Getting-Started</a></p></figcaption></figure><p>When I first started using tmux, I felt overwhelmed by the very poor default UI that it offers and the amount of options and shortcuts that I needed to know to operate it well and be productive. It took me several tries before I started to feel more or less comfortable with it.</p><p>One of the things that stood out to me right away was how dreadful UI looked. Not only it felt uninviting, but it almost felt “gatekeepy” - if you don’t know how to use it in this configuration, then might as well forget about it. But one thing that kept me going was the promise of a highly customizable terminal that I could make my own at some point.</p><figure><img loading="lazy" src="https://github.com/tmux/tmux/wiki/images/tmux_with_panes.png" alt="While promising all of these amazing features, the dreadful tmux UI didn’t necessarily inspire :( Source: https://github.com/tmux/tmux/wiki/Getting-Started"><figcaption><p>While promising all of these amazing features, the dreadful tmux UI didn’t necessarily inspire :( Source: <a href="https://github.com/tmux/tmux/wiki/Getting-Started">https://github.com/tmux/tmux/wiki/Getting-Started</a></p></figcaption></figure><h2 id="customization-journey">Customization journey</h2><p>To preface, I must mention that I am using <a href="https://iterm2.com/">iterm2</a> with <a href="https://github.com/romkatv/powerlevel10k">Powerline10k</a> and <a href="https://github.com/ryanoasis/nerd-fonts">NerdFonts</a>. They heavily affect the final look of the tmux due to colors and icons being inherited from the iterm2 configuration. With that said, let’s get into it.</p><p>The file that is responsible for customizing tmux - its function and its look - is <code>.tmux.conf</code>. This file should be located in user’s Home folder a.k.a. <code>~/</code>. That is where tmux will be looking for it.</p><p>There are quite a few options when it comes to customizing tmux. One of the first things that I wanted to address was the <strong>prefix</strong> key binding. This prefix is a key combination that you would use to enter the tmux shortcut or command mode. By default, the prefix is <code>C-b</code> a.k.a. <code>Ctrl-b</code>. One of the common remaps that most people go with is to switch it to <code>C-a</code>, which is a lot easier to use. To do that, we need to add the following lines to the configuration file:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># remap prefix from 'C-b' to 'C-a'</span>
</span></span><span><span>unbind C-b
</span></span><span><span>set-option -g prefix C-a
</span></span><span><span>bind-key C-a send-prefix
</span></span></code></pre></div><p>While researching ways to customize tmux, I stumbled upon several more useful key remaps, which I decided to use:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># split panes using | and -</span>
</span></span><span><span><span>bind</span> <span>|</span> split-window -h
</span></span><span><span><span>bind</span> - split-window -v
</span></span><span><span>unbind <span>'"'</span>
</span></span><span><span>unbind %
</span></span><span><span>
</span></span><span><span><span># reload config file (change file location to your the tmux.conf you want to use)</span>
</span></span><span><span><span>bind</span> r source-file ~/.tmux.conf<span>;</span> display-message <span>"~/.tmux.conf reloaded."</span>
</span></span><span><span>
</span></span><span><span><span># switch panes using Alt-arrow without prefix</span>
</span></span><span><span><span>bind</span> -n M-Left <span>select</span>-pane -L
</span></span><span><span><span>bind</span> -n M-Right <span>select</span>-pane -R
</span></span><span><span><span>bind</span> -n M-Up <span>select</span>-pane -U
</span></span><span><span><span>bind</span> -n M-Down <span>select</span>-pane -D
</span></span><span><span>
</span></span><span><span><span># switch windows using Shift-arrow without prefix</span>
</span></span><span><span><span>bind</span> -n S-Left previous-window
</span></span><span><span><span>bind</span> -n S-Right next-window
</span></span></code></pre></div><p>Here we are changing function to split window from <code>C-a "</code> to <code>C-a |</code> for vertical splits and from <code>C-a %</code> to <code>C-a -</code> for horizontal splits, which is a lot more intuitive. Next we are creating several useful keybinds: <code>C-a r</code> to source the configuration file, <code>M-Left</code>, <code>M-Right</code>, <code>M-Up</code>, <code>M-Down</code> for faster navigation between the panes (<code>M</code> being either <code>Alt</code> or <code>Option</code>), and <code>S-Left</code> and <code>S-Right</code> for better navigation between the windows (<code>S</code> being <code>Shift</code>). These changes alone have already made my tmux experience 10x better than before.</p><p>Next, I wanted to improve my experience when scrolling the terminal output. By default, if you were to attempt to scroll it up or down, tmux would iterate over the history of the commands that you have executed. I would like to be able to look over the previous output and be able to operate on it. By default it is possible by entering the copy mode using <code>C-a [</code>. Once you’re there, it gives you very little to work with - the default terminal output retention is quite small. To fix all of these issues, I used the following configuration:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># Increase the scrollback buffer to a higher limit than the default 2000 lines</span>
</span></span><span><span><span>set</span> -g history-limit <span>9999999</span>
</span></span><span><span>
</span></span><span><span><span># Enable mouse control (clickable windows, panes, resizable panes)</span>
</span></span><span><span><span>set</span> -g mouse on
</span></span><span><span>
</span></span><span><span><span># Enable vim keybinds to be used in copy mode</span>
</span></span><span><span>set-window-option -g mode-keys vi
</span></span></code></pre></div><p>These settings enable tmux retain 9999999 lines of terminal output (more than enough for me), enable using mouse, which removes the necessity to enter the copy mode to browse terminal output, and lastly enable Vim key bindings when in copy mode to quickly search and navigate through it.</p><p>And that brings us to styling tmux. As I mentioned, I didn’t like the original UI much, so I felt strongly about changing that. To do that, I used the following configuration:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># clock mode</span>
</span></span><span><span>setw -g clock-mode-colour yellow
</span></span><span><span>
</span></span><span><span><span># copy mode</span>
</span></span><span><span>setw -g mode-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># panes</span>
</span></span><span><span><span>set</span> -g pane-border-style <span>'fg=yellow'</span>
</span></span><span><span><span>set</span> -g pane-active-border-style <span>'fg=green'</span>
</span></span><span><span>
</span></span><span><span><span># statusbar</span>
</span></span><span><span><span>set</span> -g status-position top
</span></span><span><span><span>set</span> -g status-justify left
</span></span><span><span><span>set</span> -g status-style <span>'fg=green'</span>
</span></span><span><span>
</span></span><span><span><span>set</span> -g status-left <span>''</span>
</span></span><span><span><span>set</span> -g status-left-length <span>10</span>
</span></span><span><span>
</span></span><span><span><span>set</span> -g status-right <span>'#[fg=green,bg=default,bright]#(tmux-mem-cpu-load) #[fg=red,dim,bg=default]#(uptime | cut -f 4-5 -d " " | cut -f 1 -d ",") #[fg=white,bg=default]%a%l:%M:%S %p#[default] #[fg=blue]%Y-%m-%d'</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-current-style <span>'fg=black bg=green'</span>
</span></span><span><span>setw -g window-status-current-format <span>' #I #W #F '</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-style <span>'fg=green bg=black'</span>
</span></span><span><span>setw -g window-status-format <span>' #I #[fg=white]#W #[fg=yellow]#F '</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-bell-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># messages</span>
</span></span><span><span><span>set</span> -g message-style <span>'fg=black bg=yellow bold'</span>
</span></span></code></pre></div><p>These lines attempt to create a (subjectively) better looking UI that is easier on the eyes and more intuitive. It uses standard color names that would fit the color scheme that your terminal is already using so that it doesn’t stick out too much. Although it is very much possible to use whatever colors you want here - tmux is your oyster. Here we are also calibrating the positioning of the elements like status bar, look and function of parts of the status bar, and lastly colors for messages and copy mode.</p><p>Finally, let’s add tmux plugin manager. That’s right - tmux has a whole library of community maintained plugins. If you want to you can find entire plugins that will change the look and feel or your tmux in an instant. The reason I decided to go with my own configuration is so that I could learn it better and own it. Any new theme plugin would require some learning to know how it functions and where everything is. Regardless, configuring tmux plugin manager can be done in just a few lines:</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># List of plugins</span>
</span></span><span><span><span>set</span> -g @plugin <span>'tmux-plugins/tpm'</span>
</span></span><span><span><span>set</span> -g @plugin <span>'tmux-plugins/tmux-sensible'</span>
</span></span><span><span>
</span></span><span><span><span># Other examples:</span>
</span></span><span><span><span># set -g @plugin 'github_username/plugin_name'</span>
</span></span><span><span><span># set -g @plugin 'github_username/plugin_name#branch'</span>
</span></span><span><span><span># set -g @plugin 'git@github.com:user/plugin'</span>
</span></span><span><span><span># set -g @plugin 'git@bitbucket.com:user/plugin'</span>
</span></span><span><span>
</span></span><span><span><span># Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf)</span>
</span></span><span><span>run <span>'~/.tmux/plugins/tpm/tpm'</span>
</span></span></code></pre></div><p>One note to make here is that you would need to clone the repository for tmux plugin manager into <code>~/.tmux/plugins/tpm</code> for it to work. You can do that by following instructions on the <a href="https://github.com/tmux-plugins/tpm">tmux plugin manager</a> GitHub repository.</p><p>I haven’t installed many plugins just yet because I am still exploring which plugins I would want to have installed. I have several that I’m interested in exploring and giving a go:</p><ul><li><a href="https://github.com/tmux-plugins/tmux-battery">tmux-battery</a></li><li><a href="https://github.com/b0o/tmux-autoreload">tmux-autoreload</a></li><li><a href="https://github.com/ofirgall/tmux-browser">tmux-browser</a></li><li><a href="https://github.com/lost-melody/tmux-command-palette">tmux-command-palette</a></li><li><a href="https://github.com/wfxr/tmux-fzf-url">tmux-fzf-url</a></li><li><a href="https://github.com/jaclu/tmux-menus">tmux-menus</a></li><li><a href="https://github.com/tmux-plugins/tmux-resurrect">tmux-ressurect</a></li></ul><h2 id="full-configuration">Full configuration</h2><p>Here is a full configuration file. It includes a couple of other features that I haven’t mentioned because they are less significant than the ones that I have highlighted.</p><div><pre tabindex="0"><code data-lang="shell"><span><span><span># remap prefix from 'C-b' to 'C-a'</span>
</span></span><span><span>unbind C-b
</span></span><span><span>set-option -g prefix C-a
</span></span><span><span>bind-key C-a send-prefix
</span></span><span><span>
</span></span><span><span><span># split panes using | and -</span>
</span></span><span><span><span>bind</span> <span>|</span> split-window -h
</span></span><span><span><span>bind</span> - split-window -v
</span></span><span><span>unbind <span>'"'</span>
</span></span><span><span>unbind %
</span></span><span><span>
</span></span><span><span><span># reload config file (change file location to your the tmux.conf you want to use)</span>
</span></span><span><span><span>bind</span> r source-file ~/.tmux.conf<span>;</span> display-message <span>"~/.tmux.conf reloaded."</span>
</span></span><span><span>
</span></span><span><span><span># switch panes using Alt-arrow without prefix</span>
</span></span><span><span><span>bind</span> -n M-Left <span>select</span>-pane -L
</span></span><span><span><span>bind</span> -n M-Right <span>select</span>-pane -R
</span></span><span><span><span>bind</span> -n M-Up <span>select</span>-pane -U
</span></span><span><span><span>bind</span> -n M-Down <span>select</span>-pane -D
</span></span><span><span>
</span></span><span><span><span># switch windows using Shift-arrow without prefix</span>
</span></span><span><span><span>bind</span> -n S-Left previous-window
</span></span><span><span><span>bind</span> -n S-Right next-window
</span></span><span><span>
</span></span><span><span><span># Increase the scrollback buffer to a higher limit than the default 2000 lines</span>
</span></span><span><span><span>set</span> -g history-limit <span>9999999</span>
</span></span><span><span>
</span></span><span><span><span># Enable mouse control (clickable windows, panes, resizable panes)</span>
</span></span><span><span><span>set</span> -g mouse on
</span></span><span><span>
</span></span><span><span><span># Enable vim keybinds to be used in copy mode</span>
</span></span><span><span>set-window-option -g mode-keys vi
</span></span><span><span>
</span></span><span><span><span># don't rename windows automatically</span>
</span></span><span><span><span># set-option -g allow-rename off</span>
</span></span><span><span>
</span></span><span><span><span># rename window to reflect current program</span>
</span></span><span><span>setw -g automatic-rename on
</span></span><span><span><span># renumber windows when a window is closed</span>
</span></span><span><span><span>set</span> -g renumber-windows on
</span></span><span><span>
</span></span><span><span><span># don't do anything when a 'bell' rings</span>
</span></span><span><span><span>set</span> -g visual-activity off
</span></span><span><span><span>set</span> -g visual-bell off
</span></span><span><span><span>set</span> -g visual-silence off
</span></span><span><span>setw -g monitor-activity off
</span></span><span><span><span>set</span> -g bell-action none
</span></span><span><span>
</span></span><span><span><span># clock mode</span>
</span></span><span><span>setw -g clock-mode-colour yellow
</span></span><span><span>
</span></span><span><span><span># copy mode</span>
</span></span><span><span>setw -g mode-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># panes</span>
</span></span><span><span><span>set</span> -g pane-border-style <span>'fg=yellow'</span>
</span></span><span><span><span>set</span> -g pane-active-border-style <span>'fg=green'</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span># Allows for faster key repetition</span>
</span></span><span><span><span># set -s escape-time 50</span>
</span></span><span><span>
</span></span><span><span><span># statusbar</span>
</span></span><span><span><span>set</span> -g status-position top
</span></span><span><span><span>set</span> -g status-justify left
</span></span><span><span><span>set</span> -g status-style <span>'fg=green'</span>
</span></span><span><span>
</span></span><span><span><span>set</span> -g status-left <span>''</span>
</span></span><span><span><span>set</span> -g status-left-length <span>10</span>
</span></span><span><span>
</span></span><span><span><span>set</span> -g status-right <span>'#[fg=green,bg=default,bright]#(tmux-mem-cpu-load) #[fg=red,dim,bg=default]#(uptime | cut -f 4-5 -d " " | cut -f 1 -d ",") #[fg=white,bg=default]%a%l:%M:%S %p#[default] #[fg=blue]%Y-%m-%d'</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-current-style <span>'fg=black bg=green'</span>
</span></span><span><span>setw -g window-status-current-format <span>' #I #W #F '</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-style <span>'fg=green bg=black'</span>
</span></span><span><span>setw -g window-status-format <span>' #I #[fg=white]#W #[fg=yellow]#F '</span>
</span></span><span><span>
</span></span><span><span>setw -g window-status-bell-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># messages</span>
</span></span><span><span><span>set</span> -g message-style <span>'fg=black bg=yellow bold'</span>
</span></span><span><span>
</span></span><span><span><span># List of plugins</span>
</span></span><span><span><span>set</span> -g @plugin <span>'tmux-plugins/tpm'</span>
</span></span><span><span><span>set</span> -g @plugin <span>'tmux-plugins/tmux-sensible'</span>
</span></span><span><span>
</span></span><span><span><span># Other examples:</span>
</span></span><span><span><span># set -g @plugin 'github_username/plugin_name'</span>
</span></span><span><span><span># set -g @plugin 'github_username/plugin_name#branch'</span>
</span></span><span><span><span># set -g @plugin 'git@github.com:user/plugin'</span>
</span></span><span><span><span># set -g @plugin 'git@bitbucket.com:user/plugin'</span>
</span></span><span><span>
</span></span><span><span><span># Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf)</span>
</span></span><span><span>run <span>'~/.tmux/plugins/tpm/tpm'</span>
</span></span></code></pre></div><p>This file could also be found in my GitHub repo: <a href="https://github.com/EvgeniiKlepilin/config-files/blob/main/.tmux.conf">https://github.com/EvgeniiKlepilin/config-files/blob/main/.tmux.conf</a> . I will keep updating it as I continue my customization efforts, so stay tuned.</p><h2 id="final-look">Final look</h2><p>In the end, here is what my tmux looks like now:</p><figure><img loading="lazy" src="https://evgeniipendragon.com/posts/customizing-tmux-and-making-it-less-dreadful/tmux_is_awesome.png" alt="Much better! 😎"><figcaption><p>Much better! 😎</p></figcaption></figure><p>I hope you will find this useful and maybe even inspiring to create something custom of your own!</p></div></article></div></div>]]></description>
        </item>
    </channel>
</rss>