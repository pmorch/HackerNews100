<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 23 Mar 2025 20:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A USB Interface to the "Mother of All Demos" Keyset (153 pts)]]></title>
            <link>https://www.righto.com/2025/03/mother-of-all-demos-usb-keyset-interface.html</link>
            <guid>43453582</guid>
            <pubDate>Sun, 23 Mar 2025 15:31:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.righto.com/2025/03/mother-of-all-demos-usb-keyset-interface.html">https://www.righto.com/2025/03/mother-of-all-demos-usb-keyset-interface.html</a>, See on <a href="https://news.ycombinator.com/item?id=43453582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-4116959493954575947" itemprop="description articleBody">
<p>In the early 1960s, Douglas Engelbart started investigating how computers could augment human intelligence: <!-- https://youtu.be/yJDv-zdhzMY?si=m8GpQSIqnYfNnFsf&t=130)-->
"If, in your office, you as an intellectual worker
were supplied with a computer display backed up by a computer that was alive for you all day and was instantly responsive to every
action you had, how much value could you derive from that?"
Engelbart developed many features of modern computing that we now take for granted: the mouse,<span id="fnref:mouse"><a href="#fn:mouse">1</a></span> hypertext, shared documents, windows,
and a graphical user interface.
At the 1968 Joint Computer Conference, Engelbart demonstrated these innovations in a groundbreaking presentation, now known as
"The Mother of All Demos."</p>
<!-- [Engelbart using the keyset to edit text. Note that the display doesn't support lower case text; instead, upper case is indicated by a line above the character. Adapted from <a href="https://youtu.be/UhpTiWyVa6k?si=cqfTbRsOxTy8eE01">The Mother of All Demos</a>.](keyset-video2.jpg "w500")  -->

<p><a href="https://static.righto.com/images/engelbart/interface.jpg"><img alt="The keyset with my prototype USB interface." height="364" src="https://static.righto.com/images/engelbart/interface-w500.jpg" title="The keyset with my prototype USB interface." width="500"></a></p><p>The keyset with my prototype USB interface.</p>
<p>Engelbart's demo also featured an input device known as the keyset, but unlike his other innovations, the keyset failed to catch on.
The 5-finger keyset lets you type without moving your hand, entering characters by pressing multiple keys simultaneously as a chord.
Christina Englebart, his daughter, loaned one of Engelbart's keysets to me.
I constructed an interface to connect the keyset to USB, so that it can be used with a modern computer.
The video below shows me typing with the keyset, using the mouse buttons to select upper case and special characters.<span id="fnref:keys"><a href="#fn:keys">2</a></span></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/DpshKBKt_os?si=gzyYjd-2_ltR9oeI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>

<p>I wrote this blog post to describe my USB keyset interface.
Along the way, however, I got sidetracked by the history of The Mother of All Demos and how it obtained that name.
It turns out that Engelbart's demo isn't the first demo to be called "The Mother of All Demos".</p>
<h2>Engelbart and The Mother of All Demos</h2>
<!--
As SRI put it, Doug Engelbart envisioned harnessing the power of computers as tools for collaboration and the augmentation of our collective
intelligence to work on humanity's most important problems.
-->

<p>Engelbart's work has its roots in
Vannevar Bush's 1945 visionary essay, "<a href="https://worrydream.com/refs/Bush%20-%20As%20We%20May%20Think%20(Life%20Magazine%209-10-1945).pdf">As We May Think</a>."
Bush envisioned thinking machines, along with the "memex", a compact machine holding a library of collective knowledge with hypertext-style links: "The Encyclopedia Britannica could be reduced to the volume of a matchbox."
The memex could search out information based on associative search, building up a hypertext-like trail of connections.</p>
<p>In the early 1960s, Engelbart was inspired by Bush's essay and set out
to develop means to augment human intellect: "increasing the capability of a man to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems."<span id="fnref:1962"><a href="#fn:1962">3</a></span>
Engelbart founded the Augmentation Research Center at the Stanford Research Institute (now SRI), where
he and his team created a system called NLS (oN-Line System).</p>
<p><a href="https://static.righto.com/images/engelbart/shopping-list.jpg"><img alt="Engelbart editing a hierarchical shopping list." height="351" src="https://static.righto.com/images/engelbart/shopping-list-w500.jpg" title="Engelbart editing a hierarchical shopping list." width="500"></a></p><p>Engelbart editing a hierarchical shopping list.</p>
<p>In 1968, Engelbart demonstrated NLS to a crowd of two thousand people
at the Fall Joint Computer Conference.
Engelbart gave the demo from the stage, wearing a crisp shirt and tie and a headset microphone.
Engelbart created hierarchical documents, such as the shopping list above, and moved around them with hyperlinks.
He demonstrated how text could be created, moved, and edited with the keyset and mouse.
Other documents included graphics, crude line drawing by today's standards but cutting-edge for the time.
The computer's output was projected onto a giant screen, along with video of Engelbart.</p>
<p><a href="https://static.righto.com/images/engelbart/keyset-video.jpg"><img alt="Engelbart using the keyset to edit text. Note that the display doesn't support lowercase text; instead, uppercase is indicated by a line above the character. Adapted from The Mother of All Demos." height="354" src="https://static.righto.com/images/engelbart/keyset-video-w500.jpg" title="Engelbart using the keyset to edit text. Note that the display doesn't support lowercase text; instead, uppercase is indicated by a line above the character. Adapted from The Mother of All Demos." width="500"></a></p><p>Engelbart using the keyset to edit text. Note that the display doesn't support lowercase text; instead, uppercase is indicated by a line above the character. Adapted from <a href="https://youtu.be/UhpTiWyVa6k?si=cqfTbRsOxTy8eE01">The Mother of All Demos</a>.</p>
<p>Engelbart sat at a specially-designed Herman Miller desk<span id="fnref:herman-miller"><a href="#fn:herman-miller">6</a></span> that held the
keyset, keyboard, and mouse, shown above.
While Engelbart was on stage in San Francisco,
the SDS 940<span id="fnref:sds940"><a href="#fn:sds940">4</a></span> computer that ran the NLS software was 30 miles to the south in Menlo Park.<span id="fnref:moad-video"><a href="#fn:moad-video">5</a></span></p>
<p>To the modern eye, the demo resembles a PowerPoint presentation over Zoom, as
Engelbart collaborated with
Jeff Rulifson and Bill Paxton, miles away in Menlo Park.
(Just like a modern Zoom call, the remote connection started with "We're not hearing you. How about now?")
Jeff Rulifson browsed the NLS code, jumping between code files with hyperlinks and expanding subroutines by clicking on them.
NLS was written in custom <a href="https://bitsavers.org/pdf/sri/arc/NLS_Programmers_Guide_Jan76.pdf">high-level languages</a>, which they developed
with a "compiler compiler" called <a href="https://en.wikipedia.org/wiki/TREE-META">TREE-META</a>.
The NLS system held interactive documentation as well as tracking bugs and changes.
Bill Paxton interactively drew a diagram and then demonstrated how NLS could be used as a database, retrieving information by searching on keywords.
(Although Engelbart was stressed by the live demo, Paxton told me that he was "too young and inexperienced to be concerned.")</p>
<p><a href="https://static.righto.com/images/engelbart/demo-english.jpg"><img alt="Bill Paxton, in Menlo Park, communicating with the conference in San Francisco." height="326" src="https://static.righto.com/images/engelbart/demo-english-w500.jpg" title="Bill Paxton, in Menlo Park, communicating with the conference in San Francisco." width="500"></a></p><p>Bill Paxton, in Menlo Park, communicating with the conference in San Francisco.</p>
<p>Bill English, an electrical engineer, not only built the first mouse for Engelbart but was also the hardware mastermind behind the demo.
In San Francisco, the screen images were projected on a 20-foot screen by a Volkswagen-sized
Eiodophor projector, bouncing light off a modulated oil film.
Numerous cameras, video switchers and mixers created the video image.
Two leased microwave links and half a dozen antennas connected SRI in Menlo Park to the demo in San Francisco.
High-speed modems send the mouse, keyset, and keyboard signals from the demo back to SRI.
Bill English spent months assembling the hardware and network for the demo and then managed the demo behind the scenes, assisted by a team of about 17 people.</p>
<p>Another participant was the famed counterculturist Stewart Brand, known for the <a href="https://en.wikipedia.org/wiki/Whole_Earth_Catalog">Whole Earth Catalog</a>
and the WELL, one of the oldest online virtual communities.
Brand advised Engelbart on the presentation, as well as running a camera. He'd often point the camera at a monitor to generate swirling psychedelic
feedback patterns, reminiscent of the LSD that he and Engelbart had experimented with.</p>
<p>The demo received press attention such as
a San Francisco Chronicle article titled "Fantastic World of Tomorrow's Computer".
It stated, "The most fantastic glimpse into the computer future was taking place in a windowless room on the third floor of the Civic Auditorium"
where Engelbart "made a computer in Menlo Park do secretarial work for him that ten efficient secretaries couldn't do in twice the time."
His goal: "We hope to help man do better what he does—perhaps by as much as 50 per cent."
However, the demo received little attention in the following decades.<span id="fnref:attention"><a href="#fn:attention">7</a></span></p>
<p>Engelbart continued his work at SRI for almost a decade, but as Engelbart commented with frustration,
“There was a slightly less than universal perception of our value at SRI”.<span id="fnref:levy"><a href="#fn:levy">8</a></span>
In 1977, SRI sold the Augmentation Research Center to Tymshare, a time-sharing computing company.
(Timesharing was the cloud computing of the 1970s and 1980s,
where companies would use time on a centralized computer.)
At Tymshare, Engelbart's system was renamed AUGMENT and marketed as an office automation service, but Engelbart himself was sidelined from development,
a situation that he <a href="https://stanford.edu/dept/SUL/sites/engelbart/engfmst3-ntb.html">described</a> as
sitting in a corner and becoming invisible.</p>
<p>Meanwhile, Bill English and some other SRI researchers<span id="fnref:researchers"><a href="#fn:researchers">9</a></span> migrated four miles south to Xerox PARC and worked on the Xerox Alto computer.
The Xerox Alto incorporated many ideas from the Augmentation Research Center including the graphical user interface, the mouse, and the keyset.
The Alto's keyset 
was almost identical to the Engelbart keyset, as can be seen in the photo below.
The Alto's keyset was most popular for the networked 3D shooter game "<a href="https://www.digibarn.com/collections/games/xerox-maze-war/index.html">Maze War</a>", with the clicking of keysets echoing through the hallways of Xerox PARC.</p>
<p><a href="https://static.righto.com/images/engelbart/alto.jpg"><img alt="A Xerox Alto with a keyset on the left." height="359" src="https://static.righto.com/images/engelbart/alto-w500.jpg" title="A Xerox Alto with a keyset on the left." width="500"></a></p><p>A Xerox Alto with a keyset on the left.</p>
<p>Xerox famously failed to commercialize the ideas from the Xerox Alto, but Steve Jobs recognized the importance of interactivity, the graphical user interface, and the mouse
when he visited Xerox PARC in 1979.
Steve Jobs provided the Apple Lisa and Macintosh ended up with a graphical user interface and the mouse (streamlined to one button instead of three), but he left the keyset behind.<span id="fnref:parc"><a href="#fn:parc">10</a></span></p>
<p>When McDonnell Douglas acquired Tymshare in 1984, Engelbart and his software—now called Augment—had a new home.<span id="fnref:augment"><a href="#fn:augment">11</a></span>
In 1987, McDonnell Douglas released a text editor and outline processor for the IBM PC called
<a href="https://archive.org/details/1987-augment-mini-base-users-guide_202503">MiniBASE</a>, 
one of the few PC applications that supported a keyset.
The functionality of MiniBASE was almost identical to Engelbart's 1968 demo, but in 1987, MiniBASE
was competing against GUI-based word processors such as MacWrite and Microsoft Word, so MiniBASE had little impact.
Engelbart left McDonnell Douglas in 1988, forming a research foundation called the <a href="https://www.nytimes.com/1988/09/05/business/business-people-computer-scientist-forming-a-foundation.html">Bootstrap Institute</a> to continue his research independently.</p>

<p>The name "The Mother of All Demos" has its roots in the Gulf War.
In August 1990, Iraq invaded Kuwait, leading to war between Iraq and a coalition of the United States and 41 other countries.
During the months of buildup prior to active conflict, Iraq's leader, Saddam Hussein,
exhorted the Iraqi people to prepare for "<a href="https://www.nytimes.com/1990/09/22/world/confrontation-in-the-gulf-leaders-bluntly-prime-iraq-for-mother-of-all-battles.html">the mother of all battles</a>",<span id="fnref:mother"><a href="#fn:mother">12</a></span> a phrase that caught the attention of the media.
The battle didn't proceed as Hussein hoped: during <a href="https://www.nytimes.com/1991/02/28/world/war-gulf-president-bush-halts-offensive-combat-kuwait-freed-iraqis-crushed.html">exactly 100 hours</a> of ground combat, the US-led coalition liberated Kuwait, pushed into Iraq, crushed the Iraqi forces,
and declared a ceasefire.<span id="fnref:gulf-war"><a href="#fn:gulf-war">13</a></span>
Hussein's mother of all battles became the <a href="https://www.nytimes.com/1991/02/27/arts/critic-s-notebook-human-images-help-add-drama-to-war-coverage.html">mother of all surrenders</a>.</p>
<p>The phrase "mother of all ..." became the 1990s equivalent of a meme, used as a slightly-ironic superlative.
It was applied to everything
from <a href="https://www.nytimes.com/1993/06/18/sports/us-open-golf-notebook-fore-the-mother-of-all-traffic-jams.html">The Mother of All Traffic Jams</a> to <a href="https://amzn.to/4bzQ7Tc">The Mother of All Windows Books</a>, from <a href="https://cooking.nytimes.com/recipes/1132-the-mother-of-all-butter-cookies">The Mother of All Butter Cookies</a> to Apple calling mobile devices
<a href="https://www.nytimes.com/1992/07/19/business/the-executive-computer-mother-of-all-markets-or-a-pipe-dream-driven-by-greed.html">The Mother of All Markets</a>.<span id="fnref:mobile"><a href="#fn:mobile">14</a></span></p>
<p>In 1991, this superlative was applied to a computer demo, but it wasn't Engelbart's demo.
Andy Grove, Intel's president, gave a keynote speech at Comdex 1991 entitled <a href="https://www.youtube.com/watch?v=CwvOeKqXv18">The Second Decade: Computer-Supported Collaboration</a>,
a live demonstration of his vision for PC-based video conferencing and wireless communication in the PC's second decade.
This complex hour-long demo required almost six months to prepare, with 15 companies collaborating.
Intel called this demo "The Mother of All Demos", a name repeated in the New York Times, San Francisco Chronicle, Fortune, and PC Week.<span id="fnref:intel"><a href="#fn:intel">15</a></span>
Andy Grove's demo was a hit, with over 20,000 people requesting a video tape, but the demo was soon forgotten.</p>
<p><a href="https://static.righto.com/images/engelbart/nytimes-moad.jpg"><img alt="On the eve of Comdex, the New York Times wrote about Intel's &quot;Mother of All Demos&quot;. Oct 21, 1991, D1-D2." height="357" src="https://static.righto.com/images/engelbart/nytimes-moad-w350.jpg" title="On the eve of Comdex, the New York Times wrote about Intel's &quot;Mother of All Demos&quot;. Oct 21, 1991, D1-D2." width="350"></a></p><p>On the eve of Comdex, the New York Times <a href="https://www.nytimes.com/1991/10/21/business/computer-industry-gathers-amid-chaos.html">wrote</a> about Intel's "Mother of All Demos". Oct 21, 1991, D1-D2.</p>
<p>In 1994, <em>Wired</em> writer Steven Levy wrote <a href="https://amzn.to/4kCE63A">Insanely Great: The Life and Times of Macintosh, the Computer that Changed Everything</a>.<span id="fnref2:levy"><a href="#fn:levy">8</a></span>
In the second chapter of this comprehensive book, Levy explained how Vannevar Bush and Doug Engelbart "sparked a chain reaction" that led to the Macintosh.
The chapter described Engelbart's 1968 demo in detail including a throwaway line saying, "<a href="https://archive.org/details/insanely_great_levy_hard_cover_1994_pdf__mlib/page/42/mode/1up">It was the mother of all demos.</a>"<span id="fnref:vandam"><a href="#fn:vandam">16</a></span>
Based on my research, I think this is the source of the name "The Mother of All Demos" for Engelbart's demo.</p>
<p>By the end of the century, multiple publications echoed Levy's catchy phrase.
In February 1999, the San Jose Mercury News had a <a href="https://web.archive.org/web/19991003082606/http://www.mercurycenter.com/svtech/news/special/engelbart/part4.htm">special article</a> on Engelbart, saying that the demonstration was "still called 'the mother of all demos'", a description echoed by
the industry publication <a href="https://archive.org/details/sim_computerworld_1999-05-10_33_19/page/n83/mode/1up">Computerworld</a>.<span id="fnref:still"><a href="#fn:still">17</a></span>
The book <a href="https://archive.org/details/nerds20100step/page/124/mode/2up">Nerds: A Brief History of the Internet</a> stated that the demo "has entered legend as 'the mother of all demos'".
By this point, Engelbart's fame for the "mother of all demos" was cemented and the phrase became near-obligatory when writing about him.
The classic Silicon Valley history <a href="https://archive.org/details/fireinvalleymaki0000frei">Fire in the Valley</a> (1984), for example,
didn't even mention Engelbart but in the <a href="https://archive.org/details/fireinvalleymaki00frei_0/page/303">second edition</a> (2000),
"The Mother of All Demos" had its own chapter.</p>
<h2>Interfacing the keyset to USB</h2>
<p>Getting back to the keyset interface,
the keyset consists of five microswitches, triggered by the five levers.
The switches are wired to a standard DB-25 connector.
I used a <a href="https://www.pjrc.com/store/teensy36.html">Teensy 3.6</a> microcontroller board for the interface, since this board can act both as a USB device
and as a USB host.
As a USB device, the Teensy can emulate a standard USB keyboard.
As a USB host, the Teensy can receive input from a standard USB mouse.</p>
<p>Connecting the keyset to the Teensy is (almost) straightforward, wiring the switches to five data inputs on the Teensy and the common line connected to ground.
The Teensy's input lines can be configured with pullup resistors inside the microcontroller. The result is that a data line shows <code>1</code> by default and
<code>0</code> when the corresponding key is pressed.
One complication is that the keyset apparently has a 1.5 KΩ between the leftmost button and ground, maybe to indicate that the device is plugged in.
This resistor caused that line to always appear low to the Teensy.
To counteract this and allow the Teensy to read the pin, I connected a 1 KΩ pullup resistor to that one line.</p>
<h3>The interface code</h3>
<p>Reading the keyset and sending characters over USB is mostly straightforward, but there are a few complications.
First, it's unlikely that the user will press multiple keyset buttons at exactly the same time. Moreover, the button contacts may bounce.
To deal with this, I wait until the buttons have a stable value for 100 ms (a semi-arbitrary delay) before sending a key over USB.</p>
<p>The second complication is that with five keys, the keyset only supports 32 characters. To obtain upper case, numbers, special characters, and control
characters, the keyset is designed to be used in conjunction with mouse buttons.
Thus, the interface needs to act as a USB host, so I can plug in a USB mouse to the interface.
If I want the mouse to be usable as a mouse, not just buttons in conjunction with the keyset, the interface mus forward mouse events over USB.
But it's not that easy, since mouse clicks in conjunction with the keyset shouldn't be forwarded. Otherwise, unwanted clicks will happen while
using the keyset.</p>
<p>To emulate a keyboard, the code uses the <a href="https://docs.arduino.cc/language-reference/en/functions/usb/Keyboard/">Keyboard</a> library. This library provides
an API to send characters to the destination computer.
Inconveniently, the simplest method, <code>print()</code>, supports only regular characters, not special characters like <code>ENTER</code> or <code>BACKSPACE</code>. For those, I needed to
use the lower-level <code>press()</code> and <code>release()</code> methods.
To read the mouse buttons, 
the code uses the <a href="https://github.com/PaulStoffregen/USBHost_t36">USBHost_t36</a> library, the Teensy version of the <a href="https://docs.arduino.cc/libraries/usb-host-shield-library-2.0/">USB Host</a> library.
Finally, to pass mouse motion through to the destination computer, I use the <a href="https://docs.arduino.cc/language-reference/en/functions/usb/Mouse/">Mouse</a> library.</p>
<h2>Conclusions</h2>
<p>Engelbart claimed <!-- https://web.stanford.edu/class/history34q/readings/Engelbart/Engelbart_AugmentWorkshop.html --> that learning a keyset wasn't
difficult—a six-year-old kid could learn it in less than a week—but I'm not willing to invest much time into learning it. In my brief use of the keyset, I found it very difficult to use physically.
Pressing four keys at once is difficult, with the worst being all fingers except the ring finger. Combining this with a mouse button or two at the same time
gave me the feeling that I was sight-reading a difficult piano piece.
Maybe it becomes easier with use, but I noticed that Alto programs tended to treat the keyset as function keys, rather than a mechanism for typing with chords.<span id="fnref:alto"><a href="#fn:alto">18</a></span>
David Liddle of Xerox PARC <a href="https://archive.computerhistory.org/resources/access/text/2020/06/102792010-05-01-acc.pdf#page=9">said</a>, "We found that [the keyset] was tending to slow people down, once you got away from really hot [stuff] system programmers.
It wasn't quite so good if you were giving it to other engineers, let alone clerical people and so on."</p>
<p>If anyone else has a keyset that they want to connect via USB (unlikely as it may be), my code is on
<a href="https://github.com/shirriff/keyset-to-usb-interface">github</a>.<span id="fnref:hackaday"><a href="#fn:hackaday">19</a></span>  Thanks to Christina Engelbart for loaning me the keyset. Thanks to Bill Paxton for answering my questions.
Follow me on Bluesky (<a href="https://bsky.app/profile/righto.com">@righto.com</a>) or <a href="https://www.righto.com/feeds/posts/default">RSS</a> for updates.</p>
<h2>Footnotes and references</h2>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Technicalities of Homeworld 2 Backgrounds (109 pts)]]></title>
            <link>https://simonschreibt.de/gat/homeworld-2-backgrounds/</link>
            <guid>43452688</guid>
            <pubDate>Sun, 23 Mar 2025 13:14:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonschreibt.de/gat/homeworld-2-backgrounds/">https://simonschreibt.de/gat/homeworld-2-backgrounds/</a>, See on <a href="https://news.ycombinator.com/item?id=43452688">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<div>

<p><img decoding="async" src="https://data.simonschreibt.de/assets/flag_ru.png"></p>


</div>
<p><span><span>What you see here</span></span><br>
<img fetchpriority="high" decoding="async" alt="" src="https://data.simonschreibt.de/gat026/background_panorama01.jpg" width="500" height="172"></p>
<p><span><span>is the stunning background art</span></span><br>
<img decoding="async" alt="" src="https://data.simonschreibt.de/gat026/background_panorama03.jpg" width="500" height="182"></p>
<p><span><span>of one of the most beautiful sci-fi games.</span></span><br>
<img decoding="async" alt="" src="https://data.simonschreibt.de/gat026/background_panorama02.jpg" width="500" height="162"></p>
<p><span> H o m e w o r l d <span>2</span></span></p>
<p>Thanks for reading.</p>
<p>Just kidding. Of course i have something to say about this. In the company we look at the art of <a href="http://en.wikipedia.org/wiki/Homeworld">Homeworld</a> from time to time and bow to the creators of this masterpiece. Once we talked about how great the background look and how interesting this sketched style is. There is something…some details seem…special to us.</p>
<p><img loading="lazy" decoding="async" alt="" src="https://data.simonschreibt.de/gat026/vertexcolor_gradient.jpg" width="500" height="145"></p>
<p>I mentioned, that this looks a bit like… a vertex color gradient. But they wouldn’t paint the background on geometry, right? I mean…that would has to be a highly tessellated sphere.</p>
<p>The discussion was over but I wasn’t satisfied and wanted at least see the textures. So i used some <a href="http://www.moddb.com/games/homeworld-2/downloads/homeworld-universe-mod-tools">mod tools</a> to extract the Homeworld 2 Demo data but there were no textures. Only some .HOD files. I used Google and found a thread how to generate these .HOD files from a .TGA. It was said:</p>
<blockquote>
<p><span>“…scans every pixel of the image then based on contrast<br>
it decides whether or not to add a new vertex and color…”</span></p>
</blockquote>
<p><span><span>What?</span></span></p>
<p>Could it really be, that this is vertex color? Luckily you can watch at .HOD file with CFHodEdit. And another tool can force a wireframe mode. And now look what this brought to light:</p>
<p><span><span>This is one</span></span><br>
<img loading="lazy" decoding="async" alt="" src="https://data.simonschreibt.de/gat026/wireframe01.gif" width="500" height="203"></p>
<p><span><span>of the most brave</span></span><br>
<img loading="lazy" decoding="async" alt="" src="https://data.simonschreibt.de/gat026/wireframe03.gif" width="500" height="203"></p>
<p><span><span>solution<span>s for game art <span>i ever saw.</span></span></span></span><br>
<img loading="lazy" decoding="async" alt="" src="https://data.simonschreibt.de/gat026/wireframe02.gif" width="500" height="203"></p>
<p>And here you can see how this influences the sky sphere geometry of the game. Do you see how low the resolution is in the low contrast areas? And how round the sphere is where details were painted?</p>
<p><img loading="lazy" decoding="async" alt="" src="https://data.simonschreibt.de/gat026/rotating_sphere01.gif" width="500" height="500"></p>
<p>I never ever had thought, that this can produce such good results. Oh and don’t forget that this technique solves two major problems.</p>
<p><span><b>#1</b></span> You don’t have any problems with DDS texture compression artifacts.<br>
<span><b>#2</b></span> More important from composition perspective: since you can’t get too fine detail (it was said in the tutorial that the base TGA shouldn’t contain too sharp details), the background stays were it should:</p>
<p><span>In </span><span><span>the</span> </span><b><span><span>background</span></span></b>.</p>
<p>Too often i see games where the background contains so much noise and details, that you can’t really separate fore-/midground from background.<br>
The last time i saw this perfect combination of tech &amp; composition was in Diablo 3. <a href="http://simonschreibt.de/gat/diablo-3-trees">I talk about the 2.5D tree article</a>.</p>
<p>If you want know more about how these spheres are generated, read <a href="http://simonschreibt.de/gat/homeworld-2-backgrounds-tech">my next article about this topic</a>.</p>
<p>Thanks for reading.</p>
<section id="update1">
<p><img decoding="async" src="https://data.simonschreibt.de/assets/icon_update_01.png">Update 1</p>
<div id="update-content">
<p><a href="http://oskarstalberg.tumblr.com/" target="_blank">Oskar Stålberg</a> used the Homeworld-Background-Idea in <a href="http://oskarstalberg.com/game/planet/planet.html" target="_blank">his personal project</a> which looks soooo gorgeous! :,)</p>

</div>
</section>
<section id="update2">
<p><img decoding="async" src="https://data.simonschreibt.de/assets/icon_update_01.png">Update 2</p>
<div id="update-content">
<p><a href="https://twitter.com/ChrizCorr" target="_blank">Chris Correia</a> works on a space game and asked me about the stars in the Homeworld-Backgrounds because they are super-sharp. I remembered having seen a thread like this a while ago and <a href="https://forums.gearboxsoftware.com/t/background-star-fields/544377/12" rel="noopener" target="_blank">here it is</a>! </p>
<p>In fact, the stars are single textures/billboards:  </p>

</div>
</section>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Worst Programmer I Know (2023) (258 pts)]]></title>
            <link>https://dannorth.net/the-worst-programmer/</link>
            <guid>43452649</guid>
            <pubDate>Sun, 23 Mar 2025 13:08:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dannorth.net/the-worst-programmer/">https://dannorth.net/the-worst-programmer/</a>, See on <a href="https://news.ycombinator.com/item?id=43452649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
  
  


  <p>The great thing about measuring developer productivity is that you can quickly identify the bad programmers. I want to tell you about the worst programmer I know, and why I fought to keep him in the team.</p>
<p>A few years ago I wrote a Twitter/X thread about <a href="https://twitter.com/tastapod/status/1010461873270153216?s=20">the best programmer I know</a>, which I should write up as a blog post. It seems only fair to tell you about the worst one too. His name is <a href="https://www.linkedin.com/in/timmackinnon/">Tim Mackinnon</a> and I want you to know how <em>measurably unproductive</em> he is.</p>
<p>We were working for a well-known software consultancy at a Big Bank that decided to introduce individual performance metrics, “for appraisal and personal development purposes”. This was cascaded through the organisation, and landed in our team in terms of story points delivered. This was after some considered discussion from the department manager, who knew you shouldn’t measure things like lines of code or bugs found, because people can easily game these.</p>
<figure><img src="https://dannorth.net/the-worst-programmer/dilbert-bug-free-software-1024x311.gif" alt="Source: http://dilbert.com/strip/1995-11-13"><figcaption>
      <p><em>Source: <a href="http://dilbert.com/strip/1995-11-13">http://dilbert.com/strip/1995-11-13</a></em></p>
    </figcaption>
</figure>

<p>Instead we would measure stories delivered, or it may have been story points (it turns out it <a href="https://www.researchgate.net/publication/4106463_The_Slacker's_Guide_to_Project_Tracking_or_spending_time_on_more_important_things">doesn’t matter</a>), because these represented business value. We were using something like Jira, and people would put their name against stories, which made it super easy to generate these productivity metrics.</p>
<p>Which brings me to Tim. Tim’s score was consistently zero. Zero! Not just low, or trending downwards, but literally zero. Week after week, iteration after iteration. Zero points for Tim.</p>
<p>Well Tim clearly had to go. This was the manager’s conclusion, and he asked me to make the necessary arrangements to have Tim removed and replaced by someone who actually delivered, you know, stories.</p>
<p>And I flatly refused. It wasn’t even a hard decision for me, I just said no.</p>
<p>You see, the reason that Tim’s productivity score was zero, was that <em>he never signed up for any stories</em>. Instead he would spend his day pairing with different teammates. With less experienced developers he would patiently let them drive whilst nudging them towards a solution. He would not crowd them or railroad them, but let them take the time to learn whilst carefully crafting moments of insight and learning, often as <a href="https://en.wikipedia.org/wiki/Socratic_questioning">Socratic questions</a>, what ifs, how elses.</p>
<p>With seniors it was more like co-creating or sparring; bringing different worldviews to bear on a problem, to produce something better than either of us would have thought of on our own. Tim is a heck of a programmer, and you always learn something pairing with him.</p>
<p>Tim wasn’t delivering software; Tim was delivering a team that was delivering software. The entire team became more effective, more productive, more aligned, more idiomatic, more <em>fun</em>, because Tim was in the team.</p>
<p>I explained all this to the manager and invited him to come by and observe us working from time to time. Whenever he popped by, he would see Tim sitting with someone different, working on “their” thing, and you could be sure that the quality of that thing would be significantly better, and the time to value significantly lower—yes, you can have better and faster and cheaper, it just takes discipline—than when Tim wasn’t pairing with people.</p>
<p>In the end we kept Tim, and we quietly dropped the individual productivity metrics in favour of team accountability, where we tracked—and celebrated—the business impact we were delivering to the organisation as a high-performing unit.</p>
<h2 id="tldr">tl;dr
  <a href="#tldr"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path d="M0 256C0 167.6 71.6 96 160 96h80c8.8 0 16 7.2 16 16s-7.2 16-16 16H160C89.3 128 32 185.3 32 256s57.3 128 128 128h80c8.8 0 16 7.2 16 16s-7.2 16-16 16H160C71.6 416 0 344.4 0 256zm576 0c0 88.4-71.6 160-160 160H336c-8.8 0-16-7.2-16-16s7.2-16 16-16h80c70.7 0 128-57.3 128-128s-57.3-128-128-128H336c-8.8 0-16-7.2-16-16s7.2-16 16-16h80c88.4 0 160 71.6 160 160zM152 240H424c8.8 0 16 7.2 16 16s-7.2 16-16 16H152c-8.8 0-16-7.2-16-16s7.2-16 16-16z"></path></svg></a></h2>
<p>Measure productivity by all means—I’m all for accountability—ideally as tangible business impact expressed in dollars saved, generated, or protected. This is usually hard, so proxy business metrics are fine too.</p>
<p>Just don’t try to measure the individual contribution of a unit in a complex adaptive system, because the premise of the question is flawed.</p>
<p>DORA metrics, for example, are about how the system of work works, whether as Westrum culture indicators or flow of technical change into production. They measure the engine, not the contribution of individual pistons, because that <a href="https://en.wikipedia.org/wiki/Chewbacca_defense">makes no sense</a>.</p>
<p>Also, if you ever get the chance to work with Tim Mackinnon, you should do that.</p>

  <section><em>We can help <strong>your</strong> organisation to go faster — <a href="https://dannorth.net/contact">ask us how</a></em>
</section>

  <section>
    
    <hyvor-talk-comments website-id="4330" page-id="/the-worst-programmer/" loading="lazy"></hyvor-talk-comments>
</section>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built website for sharing Drum Patterns (127 pts)]]></title>
            <link>http://drumpatterns.onether.com</link>
            <guid>43452629</guid>
            <pubDate>Sun, 23 Mar 2025 13:05:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://drumpatterns.onether.com">http://drumpatterns.onether.com</a>, See on <a href="https://news.ycombinator.com/item?id=43452629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
			<p><a href="#content">Skip to content</a></p>
    <div><div><h6><a href="http://drumpatterns.onether.com/maraca-on-3/" title="Maraca on 3">Maraca on 3</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span></span></p><p><span>MA</span><span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p></div>
<p><span data-post-id="373" data-nonce="16837db759">Play</span> <span><span>120</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/cymbal-funk/" title="Cymbal Funk">Cymbal Funk</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MA</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p></div>
<p><span data-post-id="372" data-nonce="16837db759">Play</span> <span><span>120</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/cloudy-day/" title="Cloudy Day">Cloudy Day</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span></span></p><p><span>CL</span><span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MA</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span></span></p><p><span>CH</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>X</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p></div>
<p><span data-post-id="371" data-nonce="16837db759">Play</span> <span><span>119</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/adonis-no-way-back/" title="Adonis - No Way Back">Adonis - No Way Back</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span></span></p><p><span>RS</span><span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CP</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span></span></p></div>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>RS</span><span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span></span></p><p><span>CP</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span></span></p></div>
<p><span data-post-id="370" data-nonce="16837db759">Play</span> <span><span>125</span>BPM</span> <span><span>1</span> <span>2</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/ashleys-roachclip/" title="Ashley's Roachclip">Ashley's Roachclip</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HT</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>RS</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MA</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span></span></p></div>
<p><span data-post-id="369" data-nonce="16837db759">Play</span> <span><span>95</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/the-big-beat/" title="The Big Beat">The Big Beat</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CP</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p></div>
<p><span data-post-id="368" data-nonce="16837db759">Play</span> <span><span>102</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/papa-was-too/" title="Papa Was Too">Papa Was Too</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CP</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>X</span></span></p></div>
<p><span data-post-id="367" data-nonce="16837db759">Play</span> <span><span>92</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/its-a-new-day/" title="It's a New Day">It's a New Day</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MA</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span></span></p></div>
<p><span data-post-id="365" data-nonce="16837db759">Play</span> <span><span>96</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/walk-this-way/" title="Walk This Way">Walk This Way</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MA</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span></span></p></div>
<p><span data-post-id="364" data-nonce="16837db759">Play</span> <span><span>109</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/when-the-levee-breaks/" title="When The Levee Breaks">When The Levee Breaks</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MA</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span></span></p></div>
<p><span data-post-id="363" data-nonce="16837db759">Play</span> <span><span>112</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/impeach-the-president/" title="Impeach The President">Impeach The President</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MA</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>X</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span></span></p></div>
<p><span data-post-id="362" data-nonce="16837db759">Play</span> <span><span>112</span>BPM</span> <span><span>1</span></span></p></div><div><h6><a href="http://drumpatterns.onether.com/the-funky-drummer/" title="The Funky Drummer">The Funky Drummer</a></h6>
<div>
<p><span>AC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>BD</span><span><span>X</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span></span></p><p><span>SD</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>X</span><span>-</span><span>X</span><span>-</span><span>X</span><span>X</span><span>-</span><span>-</span><span>X</span></span></p><p><span>LC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>HC</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CL</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>MA</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CB</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>CY</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span></span></p><p><span>OH</span><span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span><span>-</span><span>-</span><span>-</span><span>X</span><span>-</span><span>-</span></span></p><p><span>CH</span><span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>-</span><span>X</span><span>X</span><span>X</span><span>X</span><span>X</span><span>-</span><span>X</span><span>X</span></span></p></div>
<p><span data-post-id="361" data-nonce="16837db759">Play</span> <span><span>96</span>BPM</span> <span><span>1</span></span></p></div></div>			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[argp: GNU-style command line argument parser for Go (116 pts)]]></title>
            <link>https://github.com/tdewolff/argp</link>
            <guid>43452525</guid>
            <pubDate>Sun, 23 Mar 2025 12:45:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tdewolff/argp">https://github.com/tdewolff/argp</a>, See on <a href="https://news.ycombinator.com/item?id=43452525">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">GNU command line argument parser</h2><a id="user-content-gnu-command-line-argument-parser" aria-label="Permalink: GNU command line argument parser" href="#gnu-command-line-argument-parser"></a></p>
<p dir="auto">Command line argument parser following the GNU standard.</p>
<div data-snippet-clipboard-copy-content="./test -vo out.png --size 256 input.txt"><pre><code>./test -vo out.png --size 256 input.txt
</code></pre></div>
<p dir="auto">with the following features:</p>
<ul dir="auto">
<li>build-in help (<code>-h</code> and <code>--help</code>) message</li>
<li>scan arguments into struct fields with configuration in tags</li>
<li>scan into composite field types (arrays, slices, structs)</li>
<li>allow for nested sub commands</li>
</ul>
<p dir="auto">GNU command line argument rules:</p>
<ul dir="auto">
<li>arguments are options when they begin with a hyphen <code>-</code></li>
<li>multiple options can be combined: <code>-abc</code> is the same as <code>-a -b -c</code></li>
<li>long options start with two hyphens: <code>--abc</code> is one option</li>
<li>option names are alphanumeric characters</li>
<li>options can have a value: <code>-a 1</code> means that <code>a</code> has value <code>1</code></li>
<li>option values can be separated by a space, equal sign, or nothing: <code>-a1 -a=1 -a 1</code> are all equal</li>
<li>options and non-options can be interleaved</li>
<li>the argument <code>--</code> terminates all options so that all following arguments are treated as non-options</li>
<li>a single <code>-</code> argument is a non-option usually used to mean standard in or out streams</li>
<li>options may be specified multiple times, only the last one determines its value</li>
<li>options can have multiple values: <code>-a 1 2 3</code> means that <code>a</code> is an array/slice/struct of three numbers of value <code>[1,2,3]</code></li>
</ul>
<p dir="auto"><em>See also <a href="https://github.com/tdewolff/prompt">github.com/tdewolff/prompt</a> for a command line prompter.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Make sure you have <a href="https://git-scm.com/" rel="nofollow">Git</a> and <a href="https://golang.org/dl/" rel="nofollow">Go</a> (1.22 or higher) installed, run</p>
<div data-snippet-clipboard-copy-content="mkdir Project
cd Project
go mod init
go get -u github.com/tdewolff/argp"><pre><code>mkdir Project
cd Project
go mod init
go get -u github.com/tdewolff/argp
</code></pre></div>
<p dir="auto">Then add the following import</p>
<div dir="auto" data-snippet-clipboard-copy-content="import (
    &quot;github.com/tdewolff/argp&quot;
)"><pre><span>import</span> (
    <span>"github.com/tdewolff/argp"</span>
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Default usage</h3><a id="user-content-default-usage" aria-label="Permalink: Default usage" href="#default-usage"></a></p>
<p dir="auto">A regular command with short and long options.
See <a href="https://github.com/tdewolff/argp/blob/master/cmd/test/main.go"><code>cmd/test/main.go</code></a>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="package main

import &quot;github.com/tdewolff/argp&quot;

func main() {
    var verbose int
    var input string
    var output string
    var files []string
    size := 512 // default value

    cmd := argp.New(&quot;CLI tool description&quot;)
    cmd.AddOpt(argp.Count{&amp;verbose}, &quot;v&quot;, &quot;verbose&quot;, &quot;Increase verbosity, eg. -vvv&quot;)
    cmd.AddOpt(&amp;output, &quot;o&quot;, &quot;output&quot;, &quot;Output file name&quot;)
    cmd.AddOpt(&amp;size, &quot;&quot;, &quot;size&quot;, &quot;Image size&quot;)
    cmd.AddArg(&amp;input, &quot;input&quot;, &quot;Input file name&quot;)
    cmd.AddRest(&amp;files, &quot;files&quot;, &quot;Additional files&quot;)
    cmd.Parse()

    // ...
}"><pre><span>package</span> main

<span>import</span> <span>"github.com/tdewolff/argp"</span>

<span>func</span> <span>main</span>() {
    <span>var</span> <span>verbose</span> <span>int</span>
    <span>var</span> <span>input</span> <span>string</span>
    <span>var</span> <span>output</span> <span>string</span>
    <span>var</span> <span>files</span> []<span>string</span>
    <span>size</span> <span>:=</span> <span>512</span> <span>// default value</span>

    <span>cmd</span> <span>:=</span> <span>argp</span>.<span>New</span>(<span>"CLI tool description"</span>)
    <span>cmd</span>.<span>AddOpt</span>(argp.<span>Count</span>{<span>&amp;</span><span>verbose</span>}, <span>"v"</span>, <span>"verbose"</span>, <span>"Increase verbosity, eg. -vvv"</span>)
    <span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>output</span>, <span>"o"</span>, <span>"output"</span>, <span>"Output file name"</span>)
    <span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>size</span>, <span>""</span>, <span>"size"</span>, <span>"Image size"</span>)
    <span>cmd</span>.<span>AddArg</span>(<span>&amp;</span><span>input</span>, <span>"input"</span>, <span>"Input file name"</span>)
    <span>cmd</span>.<span>AddRest</span>(<span>&amp;</span><span>files</span>, <span>"files"</span>, <span>"Additional files"</span>)
    <span>cmd</span>.<span>Parse</span>()

    <span>// ...</span>
}</pre></div>
<p dir="auto">with help output</p>
<div data-snippet-clipboard-copy-content="Usage: test [options] input files...

Options:
  -h, --help          Help
  -o, --output string Output file name
      --size=512 int  Image size
  -v, --verbose int   Increase verbosity, eg. -vvv

Arguments:
  input     Input file name
  files     Additional files"><pre><code>Usage: test [options] input files...

Options:
  -h, --help          Help
  -o, --output string Output file name
      --size=512 int  Image size
  -v, --verbose int   Increase verbosity, eg. -vvv

Arguments:
  input     Input file name
  files     Additional files
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Sub commands</h3><a id="user-content-sub-commands" aria-label="Permalink: Sub commands" href="#sub-commands"></a></p>
<p dir="auto">Example with sub commands using a main command for when no sub command is used, and a sub command named "cmd". For the main command we can also use <code>New</code> and <code>AddOpt</code> instead and process the command after <code>argp.Parse()</code>.</p>
<div dir="auto" data-snippet-clipboard-copy-content="package main

import &quot;github.com/tdewolff/argp&quot;

func main() {
    cmd := argp.NewCmd(&amp;Main{}, &quot;CLI tool description&quot;)
    cmd.AddCmd(&amp;Command{}, &quot;cmd&quot;, &quot;Sub command&quot;)
    cmd.Parse()
}

type Main struct {
    Version bool `short:&quot;v&quot;`
}

func (cmd *Main) Run() error {
    // ...
}

type Command struct {
    Verbose bool `short:&quot;v&quot; name:&quot;&quot;`
    Output string `short:&quot;o&quot; desc:&quot;Output file name&quot;`
    Size int `default:&quot;512&quot; desc:&quot;Image size&quot;`
}

func (cmd *Command) Run() error {
    // ...
}"><pre><span>package</span> main

<span>import</span> <span>"github.com/tdewolff/argp"</span>

<span>func</span> <span>main</span>() {
    <span>cmd</span> <span>:=</span> <span>argp</span>.<span>NewCmd</span>(<span>&amp;</span><span>Main</span>{}, <span>"CLI tool description"</span>)
    <span>cmd</span>.<span>AddCmd</span>(<span>&amp;</span><span>Command</span>{}, <span>"cmd"</span>, <span>"Sub command"</span>)
    <span>cmd</span>.<span>Parse</span>()
}

<span>type</span> <span>Main</span> <span>struct</span> {
    <span>Version</span> <span>bool</span> <span>`short:"v"`</span>
}

<span>func</span> (<span>cmd</span> <span>*</span><span>Main</span>) <span>Run</span>() <span>error</span> {
    <span>// ...</span>
}

<span>type</span> <span>Command</span> <span>struct</span> {
    <span>Verbose</span> <span>bool</span> <span>`short:"v" name:""`</span>
    <span>Output</span> <span>string</span> <span>`short:"o" desc:"Output file name"`</span>
    <span>Size</span> <span>int</span> <span>`default:"512" desc:"Image size"`</span>
}

<span>func</span> (<span>cmd</span> <span>*</span><span>Command</span>) <span>Run</span>() <span>error</span> {
    <span>// ...</span>
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Arguments</h3><a id="user-content-arguments" aria-label="Permalink: Arguments" href="#arguments"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="var input string
cmd.AddArg(&amp;input, &quot;input&quot;, &quot;Input file name&quot;)

var files []string
cmd.AddRest(&amp;files, &quot;files&quot;, &quot;Additional input files&quot;)"><pre><span>var</span> <span>input</span> <span>string</span>
<span>cmd</span>.<span>AddArg</span>(<span>&amp;</span><span>input</span>, <span>"input"</span>, <span>"Input file name"</span>)

<span>var</span> <span>files</span> []<span>string</span>
<span>cmd</span>.<span>AddRest</span>(<span>&amp;</span><span>files</span>, <span>"files"</span>, <span>"Additional input files"</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Options</h3><a id="user-content-options" aria-label="Permalink: Options" href="#options"></a></p>
<p dir="auto">Basic types</p>
<div dir="auto" data-snippet-clipboard-copy-content="var v string = &quot;default&quot;
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)

var v bool = true
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)

var v int = 42 // also: int8, int16, int32, int64
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)

var v uint = 42 // also: uint8, uint16, uint32, uint64
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)

var v float64 = 4.2 // also: float32
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)"><pre><span>var</span> <span>v</span> <span>string</span> <span>=</span> <span>"default"</span>
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)

<span>var</span> <span>v</span> <span>bool</span> <span>=</span> <span>true</span>
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)

<span>var</span> <span>v</span> <span>int</span> <span>=</span> <span>42</span> <span>// also: int8, int16, int32, int64</span>
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)

<span>var</span> <span>v</span> <span>uint</span> <span>=</span> <span>42</span> <span>// also: uint8, uint16, uint32, uint64</span>
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)

<span>var</span> <span>v</span> <span>float64</span> <span>=</span> <span>4.2</span> <span>// also: float32</span>
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)</pre></div>
<p dir="auto">Composite types</p>
<div dir="auto" data-snippet-clipboard-copy-content="v := [2]int{4, 2} // element can be any valid basic or composite type
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)
// --var [4 2]  =>  [2]int{4, 2}
// or: --var 4,2  =>  [2]int{4, 2}

v := []int{4, 2, 1} // element can be any valid basic or composite type
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)
// --var [4 2 1]  =>  []int{4, 2, 1}
// or: --var 4,2,1  =>  []int{4, 2, 1}

v := map[int]string{1:&quot;one&quot;, 2:&quot;two&quot;} // key and value can be any valid basic or composite type
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)
// --var {1:one 2:two}  =>  map[int]string{1:&quot;one&quot;, 2:&quot;two&quot;}

v := struct { // fields can be any valid basic or composite type
    S string
    I int
    B [2]bool
}{&quot;string&quot;, 42, [2]bool{0, 1}}
cmd.AddOpt(&amp;v, &quot;v&quot;, &quot;var&quot;, &quot;description&quot;)
// --var {string 42 [0 1]}  =>  struct{S string, I int, B [2]bool}{&quot;string&quot;, 42, false, true}"><pre><span>v</span> <span>:=</span> [<span>2</span>]<span>int</span>{<span>4</span>, <span>2</span>} <span>// element can be any valid basic or composite type</span>
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)
<span>// --var [4 2]  =&gt;  [2]int{4, 2}</span>
<span>// or: --var 4,2  =&gt;  [2]int{4, 2}</span>

<span>v</span> <span>:=</span> []<span>int</span>{<span>4</span>, <span>2</span>, <span>1</span>} <span>// element can be any valid basic or composite type</span>
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)
<span>// --var [4 2 1]  =&gt;  []int{4, 2, 1}</span>
<span>// or: --var 4,2,1  =&gt;  []int{4, 2, 1}</span>

<span>v</span> <span>:=</span> <span>map</span>[<span>int</span>]<span>string</span>{<span>1</span>:<span>"one"</span>, <span>2</span>:<span>"two"</span>} <span>// key and value can be any valid basic or composite type</span>
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)
<span>// --var {1:one 2:two}  =&gt;  map[int]string{1:"one", 2:"two"}</span>

<span>v</span> <span>:=</span> <span>struct</span> { <span>// fields can be any valid basic or composite type</span>
    <span>S</span> <span>string</span>
    <span>I</span> <span>int</span>
    <span>B</span> [<span>2</span>]<span>bool</span>
}{<span>"string"</span>, <span>42</span>, [<span>2</span>]<span>bool</span>{<span>0</span>, <span>1</span>}}
<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>v</span>, <span>"v"</span>, <span>"var"</span>, <span>"description"</span>)
<span>// --var {string 42 [0 1]}  =&gt;  struct{S string, I int, B [2]bool}{"string", 42, false, true}</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Count</h4><a id="user-content-count" aria-label="Permalink: Count" href="#count"></a></p>
<p dir="auto">Count the number of time a flag has been passed.</p>
<div dir="auto" data-snippet-clipboard-copy-content="var c int
cmd.AddOpt(argp.Count{&amp;c}, &quot;c&quot;, &quot;count&quot;, &quot;Count&quot;)
// Count the number of times flag is present
// -c -c / -cc / --count --count  =>  2
// or: -c 5  =>  5"><pre><span>var</span> <span>c</span> <span>int</span>
<span>cmd</span>.<span>AddOpt</span>(argp.<span>Count</span>{<span>&amp;</span><span>c</span>}, <span>"c"</span>, <span>"count"</span>, <span>"Count"</span>)
<span>// Count the number of times flag is present</span>
<span>// -c -c / -cc / --count --count  =&gt;  2</span>
<span>// or: -c 5  =&gt;  5</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Append</h4><a id="user-content-append" aria-label="Permalink: Append" href="#append"></a></p>
<p dir="auto">Append each flag to a list.</p>
<div dir="auto" data-snippet-clipboard-copy-content="var v []int
cmd.AddOpt(argp.Append{&amp;v}, &quot;v&quot;, &quot;value&quot;, &quot;Values&quot;)
// Append values for each flag
// -v 1 -v 2  =>  [1 2]"><pre><span>var</span> <span>v</span> []<span>int</span>
<span>cmd</span>.<span>AddOpt</span>(argp.<span>Append</span>{<span>&amp;</span><span>v</span>}, <span>"v"</span>, <span>"value"</span>, <span>"Values"</span>)
<span>// Append values for each flag</span>
<span>// -v 1 -v 2  =&gt;  [1 2]</span></pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Config</h4><a id="user-content-config" aria-label="Permalink: Config" href="#config"></a></p>
<p dir="auto">Load all arguments from a configuration file. Currently only TOML is supported.</p>
<div dir="auto" data-snippet-clipboard-copy-content="cmd.AddOpt(&amp;argp.Config{cmd, &quot;config.toml&quot;}, &quot;&quot;, &quot;config&quot;, &quot;Configuration file&quot;)"><pre><span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span>argp.<span>Config</span>{<span>cmd</span>, <span>"config.toml"</span>}, <span>""</span>, <span>"config"</span>, <span>"Configuration file"</span>)</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">List</h4><a id="user-content-list" aria-label="Permalink: List" href="#list"></a></p>
<p dir="auto">Use a list source specified as type:list. Default supported types are: inline.</p>
<ul dir="auto">
<li>Inline takes a []string, e.g. <code>inline:[foo bar]</code></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="list := argp.NewList(il)
defer list.Close()

cmd.AddOpt(&amp;list, &quot;&quot;, &quot;list&quot;, &quot;List&quot;)"><pre><span>list</span> <span>:=</span> <span>argp</span>.<span>NewList</span>(<span>il</span>)
<span>defer</span> <span>list</span>.<span>Close</span>()

<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>list</span>, <span>""</span>, <span>"list"</span>, <span>"List"</span>)</pre></div>
<p dir="auto">You can add a MySQL source:</p>
<div data-snippet-clipboard-copy-content="type mysqlList struct {
	Hosts    string
	User     string
	Password string
	Dbname   string
	Query    string
}

func newMySQLList(s []string) (argp.ListSource, error) {
	if len(s) != 1 {
		return nil, fmt.Errorf(&quot;invalid path&quot;)
	}

	t := mysqlList{}
	if err := argp.LoadConfigFile(&amp;t, s[0]); err != nil {
		return nil, err
	}

	uri := fmt.Sprintf(&quot;%s:%s@%s/%s&quot;, t.User, t.Password, t.Hosts, t.Dbname)
	db, err := sqlx.Open(&quot;mysql&quot;, uri)
	if err != nil {
		return nil, err
	}
	db.SetConnMaxLifetime(time.Minute)
	db.SetConnMaxIdleTime(time.Minute)
	db.SetMaxOpenConns(10)
	db.SetMaxIdleConns(10)
	return argp.NewSQLList(db, t.Query, &quot;&quot;)
}

// ...
list.AddSource(&quot;mysql&quot;, newMySQLList)
// ..."><pre><code>type mysqlList struct {
	Hosts    string
	User     string
	Password string
	Dbname   string
	Query    string
}

func newMySQLList(s []string) (argp.ListSource, error) {
	if len(s) != 1 {
		return nil, fmt.Errorf("invalid path")
	}

	t := mysqlList{}
	if err := argp.LoadConfigFile(&amp;t, s[0]); err != nil {
		return nil, err
	}

	uri := fmt.Sprintf("%s:%s@%s/%s", t.User, t.Password, t.Hosts, t.Dbname)
	db, err := sqlx.Open("mysql", uri)
	if err != nil {
		return nil, err
	}
	db.SetConnMaxLifetime(time.Minute)
	db.SetConnMaxIdleTime(time.Minute)
	db.SetMaxOpenConns(10)
	db.SetMaxIdleConns(10)
	return argp.NewSQLList(db, t.Query, "")
}

// ...
list.AddSource("mysql", newMySQLList)
// ...
</code></pre></div>
<p dir="auto">Use as <code>./bin -list mysql:list-config.toml</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Dict</h4><a id="user-content-dict" aria-label="Permalink: Dict" href="#dict"></a></p>
<p dir="auto">Use a dict source specified as type:dict. Default supported types are: static and inline.</p>
<ul dir="auto">
<li>Static takes a string and will return that as a value for all keys, e.g. <code>static:foobar</code></li>
<li>Inline takes a map[string]string, e.g. <code>inline:{foo:1 bar:2}</code></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="dict := argp.NewDict([]string{&quot;static:value&quot;})
defer dict.Close()

cmd.AddOpt(&amp;dict, &quot;&quot;, &quot;dict&quot;, &quot;Dict&quot;)"><pre><span>dict</span> <span>:=</span> <span>argp</span>.<span>NewDict</span>([]<span>string</span>{<span>"static:value"</span>})
<span>defer</span> <span>dict</span>.<span>Close</span>()

<span>cmd</span>.<span>AddOpt</span>(<span>&amp;</span><span>dict</span>, <span>""</span>, <span>"dict"</span>, <span>"Dict"</span>)</pre></div>
<p dir="auto">You can add custom sources must like the mysqlList example above.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Option tags</h3><a id="user-content-option-tags" aria-label="Permalink: Option tags" href="#option-tags"></a></p>
<p dir="auto">The following struct will accept the following options and arguments:</p>
<ul dir="auto">
<li><code>-v</code> or <code>--var</code> with a default value of 42</li>
<li>The first argument called <code>first</code> with a default value of 4.2</li>
<li>The other arguments called <code>rest</code></li>
</ul>
<div dir="auto" data-snippet-clipboard-copy-content="type Command struct {
    Var1 int `short:&quot;v&quot; name:&quot;var&quot; default:&quot;42&quot; desc:&quot;Description&quot;`
    Var2 float64 `name:&quot;first&quot; index:&quot;0&quot; default:&quot;4.2&quot;`
    Var3 []string `name:&quot;rest&quot; index:&quot;*&quot;`
}

func (cmd *Command) Run() error {
    // run command
    return nil
}"><pre><span>type</span> <span>Command</span> <span>struct</span> {
    <span>Var1</span> <span>int</span> <span>`short:"v" name:"var" default:"42" desc:"Description"`</span>
    <span>Var2</span> <span>float64</span> <span>`name:"first" index:"0" default:"4.2"`</span>
    <span>Var3</span> []<span>string</span> <span>`name:"rest" index:"*"`</span>
}

<span>func</span> (<span>cmd</span> <span>*</span><span>Command</span>) <span>Run</span>() <span>error</span> {
    <span>// run command</span>
    <span>return</span> <span>nil</span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Released under the <a href="https://github.com/tdewolff/argp/blob/master/LICENSE.md">MIT license</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is it safe to travel to the United States with your phone? (143 pts)]]></title>
            <link>https://www.theverge.com/policy/634264/customs-border-protection-search-phone-airport-rights</link>
            <guid>43452474</guid>
            <pubDate>Sun, 23 Mar 2025 12:31:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/policy/634264/customs-border-protection-search-phone-airport-rights">https://www.theverge.com/policy/634264/customs-border-protection-search-phone-airport-rights</a>, See on <a href="https://news.ycombinator.com/item?id=43452474">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p>In recent weeks, airport Customs and Border Protection (CBP) agents have drawn public outcry for denying travelers US entry based on searches of their phones. A doctor on an H-1B visa was<a href="https://www.theverge.com/policy/632843/cbp-phone-search-airport-arrest-mass-deportations"> deported to Lebanon</a> after CBP found “sympathetic photos and videos” of Hezbollah leaders. A French scientist was turned away after a device search unearthed messages criticizing the Trump administration’s cuts to research programs, which officers said “conveyed hatred of Trump” and “could be qualified as terrorism.” As the administration ratchets up pressure to turn away even legal immigrants, its justifications are becoming thinner and thinner — but travelers can still benefit from knowing what are supposed to be their legal rights. </p><p>Your ability to decline a search depends on your immigration status — and, in some cases, on where and how you’re entering the country. Courts across the country have issued different rulings on device searches at ports of entry. But no matter your situation, there are precautions you can take to safeguard your digital privacy.</p><p>CBP device searches have historically been relatively rare. During the 2024 fiscal year, less than 0.01 percent of arriving international travelers had their phones, computers, or other electronic devices searched by CBP,<a href="https://www.cbp.gov/sites/default/files/2024-11/Border%20Search%20of%20Electronics%20at%20Ports%20of%20Entry%20FY%2024%20Statistics%20%28508%29.pdf"> according to the agency</a>. That year, CBP officers conducted 47,047 device searches. But even before this recent wave of incidents, inspections were on the rise: eight years earlier, during the 2016 fiscal year, CBP searched only 19,051 devices.</p><div><p id="the-border-search-exception"><h2>The “border search” exception</h2></p></div><p>The Supreme Court<a href="https://www.oyez.org/cases/2013/13-132"> ruled</a> in 2014 that warrantless searches of people’s cell phones violated the Fourth Amendment. But there’s one exception to that rule: searches that happen at the border. The courts have held that border searches “are reasonable simply because they occur at the border,” meaning in most cases, CBP and Border Patrol don’t need a warrant to look through travelers’ belongings — including their phones. That exception applies far beyond the US’s literal borders, since airports are considered border zones, too.</p><p>“Traditionally, the border search exception to the Fourth Amendment allowed customs officers to search things like luggage. The idea was whatever you’re taking with you is pertinent to your travel,” Saira Hussain, a senior staff attorney at the Electronic Frontier Foundation, told <em>The Verge</em>. The point was to look for people or things that were inadmissible into the country.</p><div><p>“It can show every facet of your life.”</p></div><p>These days, most travelers are carrying a lot more in their pockets — not only information stored on a phone’s hardware, but anything that’s accessible on it with a data connection. “When you look at devices, the data that you carry with you isn’t just pertinent to your travel. This data can precede your travel by over a decade because of how much information is stored on the cloud,” Hussain said. “It can show every facet of your life. It can show your financial history, your medical history, your communications with your doctor and your attorney. It can reveal so much information that is not analogous at all to the notion of a customs officer looking through your luggage.” Privacy advocates have warned of this issue for years, but in an environment where officers are seeking any pretext to turn someone away, it’s an even bigger problem.</p><p>If you’re a US citizen, “you have the right to say no” to a search, “and they are not allowed to bar you from the country,” Hussain said. But if you refuse, CBP can still take your phone, laptop, or other devices and hold onto them.</p><p>Permanent residents can similarly refuse a search, but with complicating factors. If someone with a green card leaves the US for more than 180 days, they’re screened for “inadmissibility” — reasons they may be barred from entry — upon returning to the country. Green card holders who have certain offenses on their record may also be deemed inadmissible. That appears to have been the case with Fabian Schmidt, a<a href="https://www.wgbh.org/news/local/2025-03-14/green-card-holder-from-new-hampshire-interrogated-at-logan-airport-detained"> permanent resident whose family said he was “violently interrogated”</a> by CBP agents at Boston Logan Airport after returning from a trip to Europe. Because of these factors, permanent residents may not feel comfortable refusing a search, even if doing so wouldn’t bar them from entering the country. </p><p>Visa holders have fewer rights at ports of entry, and refusing a search could lead to them being denied entry to the country. </p><div><p id="how-deep-is-the-search"><h2>How deep is the search?</h2></p></div><p>There are two types of device searches CBP officers can conduct: basic and forensic, or advanced. “There’s a distinction that the government draws between searching your phone and just looking at whatever is on it, versus connecting your phone to external equipment to search it using advanced algorithms or to copy the contents of your phone,” Hussain said.</p><p>The government maintains that it doesn’t need a warrant to conduct “basic” searches of the contents of a person’s phone. During these searches, Hussain explained, agents are supposed to put your phone on airplane mode and can only look at what is accessible offline — but that can still be a lot of information, including any cloud data that’s currently synced.</p><p>“While forensic inspections are powerful, a lot of mischief can happen through the physical, ‘thumbing-through’ inspections that law enforcement can engage in,” Tom McBrien, counsel at the Electronic Privacy Information Center, also told <em>The Verge</em>.</p><div><p>“A lot of mischief can happen through the physical, ‘thumbing-through’ inspections that law enforcement can engage in”</p></div><p>For the most part, courts have avoided the question of whether CBP can conduct warrantless basic searches of a person’s phone or laptop, effectively allowing the agency to do so. But there’s one geographic exception to this rule. Last year, a federal judge in New York’s Eastern District<a href="https://www.theverge.com/2024/7/29/24209130/customs-border-protection-unlock-phone-warrant-new-york-jfk"> ruled that CBP can’t conduct any warrantless searches</a> of travelers’ devices. That ruling doesn’t apply anywhere else in the country, but the district includes John F. Kennedy Airport in Queens — the sixth-busiest airport in the US. That ruling applies to both basic and forensic inspections.</p><p>Elsewhere in the country, judges have imposed some limitations on advanced searches. Warrantless forensic searches are allowed in some places and prohibited in others, depending on how different federal circuit courts rule. The Supreme Court could clear this up with a ruling that applies nationwide, but it’s avoided the question for years. </p><p>“Your rights will be different depending on whether you’re on a flight landing in Boston Logan in the First Circuit or Reagan/Dulles in the Fourth Circuit,” McBrien said. “Similarly, your rights would be different if you’re crossing the border in Arizona (Ninth Circuit) or New Mexico (Tenth Circuit). This does not make a lot of sense, but the Supreme Court has consistently declined to address these disparities by consistently denying petitions for certiorari in cases that have teed the question up.”</p><p>Some courts have been more permissive than others. The Ninth Circuit — which includes Alaska, Arizona, California, Hawaii, Idaho, Montana, Nevada, Oregon, and Washington — prohibits warrantless forensic searches unless officers are looking for “digital contraband,” such as child sexual abuse material. The Fourth Circuit — covering Maryland, North Carolina, South Carolina, Virginia, and West Virginia — prohibits warrantless forensic searches unless officers are looking for information related to ongoing border violations, such as human smuggling or drug trafficking. </p><p>In 2023, a federal judge in the Southern District of New York<a href="https://www.techdirt.com/2023/05/23/federal-judge-says-riley-applies-at-border-warrants-are-needed-for-some-cell-phone-searches/"> ruled</a> that the border search exception doesn’t extend to forensic searches, for which warrants are needed. (Oddly, the case in question involved a phone search at Newark Liberty Airport in New Jersey, a state that is in a different federal circuit from New York.) These searches, judge Jed Rakoff wrote, “extend the Government’s reach far beyond the person and luggage of the border-crosser — as if the fact of a border crossing somehow entitled the Government to search that traveler’s home, car, and office.”</p><div><p>Malik’s phone was taken even though he’s enrolled in Global Entry</p></div><p>Not all judges agree. In 2021, Adam Malik, an immigration lawyer, <a href="https://www.techdirt.com/2021/02/02/texas-immigration-lawyer-sues-dhs-cbp-over-seizure-search-his-work-phone/">sued CBP</a> after agents at Dallas Fort Worth International Airport seized his phone and searched the contents without a warrant. According to the lawsuit, Malik’s phone was taken even though he’s enrolled in Global Entry, CBP’s trusted traveler program. Because the agents couldn’t bypass Malik’s password, they sent the phone to a forensics lab, which extracted all the phone’s data.</p><p>A federal court ruled in favor of DHS, saying the warrantless search hadn’t violated Malik’s rights. When Malik appealed to the Fifth Circuit — which covers Louisiana, Mississippi, and Texas — the judges held that the search didn’t require a warrant. But the court also expressed “no view on how the border-search exemption may develop or be clarified in future cases.” </p><p>In other words, the constitutionality of these searches is still an open question — and CBP won’t stop conducting them until and unless it’s expressly forbidden from doing so.</p><p>These distinctions matter because they determine a person’s basis for challenging device inspections in court. But given the Trump administration’s recent track record of ignoring the law and flouting judicial orders, limiting what can be found on your phone is a safer bet than suing the government over an unlawful search after the fact.</p><div><p id="safeguarding-your-data"><h2>Safeguarding your data </h2></p></div><p>Instead of trying to game out what rights you have depending on your immigration status and what airport you’re flying into (or what land border you’re crossing), the best way to keep your devices safe from CBP is to limit what’s on them.</p><p>“We always encourage data minimization when crossing the border; you want to travel with the least amount of data possible,” Hussain said. </p><p>Before traveling, you should encrypt your devices and make sure you’re using secure passwords. Travelers should disable biometric logins like Face ID, since some courts have ruled that <a href="https://www.theverge.com/2024/9/24/24252235/police-unlock-phone-password-face-id-apple-wallet-id">police can’t compel you to tell them your password</a> but they <em>can</em> use biometrics to unlock your phone. </p><div><p>Travelers should disable biometric logins like Face ID</p></div><p>The EFF<a href="https://ssd.eff.org/module/things-consider-when-crossing-us-border"> recommends</a> that travelers limit what can be found during basic phone or laptop searches by uploading their data onto the cloud and deleting it off their device — and ensuring that it’s <em>fully</em> been removed, since agents can also look through your phone’s “recently deleted” files during basic searches. Customs agents are supposed to keep your phone on airplane mode while they conduct a basic search, but that still lets them see any cached emails, text messages, and other communications. The best way to safeguard this information is to back it up onto the cloud and then wipe your phone or laptop entirely.</p><p>Backing up sensitive or personal data doesn’t just prevent others from accessing your device; it also ensures you don’t lose that data if CBP seizes your phone or computer. McBrien also suggests that people turn their phones off when they’re crossing the border or at the airport. “Turning the phone off means that when you turn it back on, it requires a passcode whether or not you use FaceID or other biometric measures,” McBrien said.</p><p>In a better legal environment, these precautions wouldn’t be the only meaningful shield between you and a border search. “Without strong constitutional and statutory protections, personal choices about how to configure one’s device and apps can only mitigate — not eliminate — the dangers that border device searches pose to their privacy and speech rights,” McBrien said. For now, if CBP really wants to look through your phone, they’ll likely find a way. But you can still protect yourself as much as possible. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The SeL4 Microkernel: An Introduction [pdf] (147 pts)]]></title>
            <link>https://sel4.systems/About/seL4-whitepaper.pdf</link>
            <guid>43452185</guid>
            <pubDate>Sun, 23 Mar 2025 11:09:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sel4.systems/About/seL4-whitepaper.pdf">https://sel4.systems/About/seL4-whitepaper.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=43452185">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Do Viruses Trigger Alzheimer's? (163 pts)]]></title>
            <link>https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers</link>
            <guid>43451397</guid>
            <pubDate>Sun, 23 Mar 2025 07:29:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers">https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers</a>, See on <a href="https://news.ycombinator.com/item?id=43451397">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><div><p><span><a href="https://www.economist.com/science-and-technology" data-analytics="sidebar:section"><span>Science &amp; technology</span></a></span><span> | <!-- -->Going viral</span></p></div><h2>A growing group of scientists think so, and are asking whether antivirals could treat the disease</h2></section><div><div><p><time datetime="2025-03-17T13:56:11.552Z"> <!-- -->Mar 17th 2025</time></p></div><section><p data-component="paragraph"><span data-caps="initial">I</span><small>n the summer</small> of 2024 several groups of scientists published a curious finding: people vaccinated against shingles were less likely to develop dementia than their unvaccinated peers. Two of the papers came from the lab of Pascal Geldsetzer at Stanford University. Analysing medical records from Britain and Australia, the researchers concluded that around a fifth of dementia diagnoses could be averted through the original shingles vaccine, which contains live varicella-zoster virus. Two other studies, one by <small>GSK</small>, a pharmaceutical company, and another by a group of academics in Britain, also reported that a newer “recombinant” vaccine, which is more effective at preventing shingles than the live version, appeared to confer even greater protection against dementia.</p></section><p><h3 id="article-tags">Explore more</h3><nav aria-labelledby="article-tags"><a href="https://www.economist.com/topics/science-and-technology" data-analytics="tags:science_and_technology"><span>Science &amp; technology</span></a><a href="https://www.economist.com/topics/health-care" data-analytics="tags:health_care"><span>Health care</span></a><a href="https://www.economist.com/topics/science" data-analytics="tags:science"><span>Science</span></a></nav></p><p>This article appeared in the Science &amp; technology section of the print edition under the headline “A viral hypothesis”</p><div data-test-id="chapterlist" data-tracking-id="content-well-chapter-list"><div><hr data-testid="rule-accent"><div><h3><a href="https://www.economist.com/science-and-technology" text="Science &amp; technology" data-analytics="chapter_list_header:Science &amp; technology">Science &amp; technology</a></h3><p><span>March 22nd 2025</span></p></div></div><ul><li><a href="https://www.economist.com/science-and-technology/2025/03/17/do-viruses-trigger-alzheimers" id="f535323c-b434-437a-a67e-515be93d1279" data-analytics="article:reports_headline:1" data-test-id="chapterlist-link-0"><span data-testid="right-economist-red-false"><span>→</span></span><span>Do viruses trigger Alzheimer’s?</span></a></li><li><a href="https://www.economist.com/science-and-technology/2025/03/20/why-dont-seals-drown" id="d9852625-3b65-404b-b2d7-c4b88bb8ed4a" data-analytics="article:reports_headline:2" data-test-id="chapterlist-link-1"><span data-testid="right-london-5-false"><span>→</span></span><span>Why don’t seals drown?</span></a></li><li><a href="https://www.economist.com/science-and-technology/2025/03/19/rumours-on-social-media-could-cause-sick-people-to-feel-worse" id="4d9108ca-aba1-4f83-a18e-d965447ea258" data-analytics="article:reports_headline:3" data-test-id="chapterlist-link-2"><span data-testid="right-london-5-false"><span>→</span></span><span>Rumours on social media could cause sick people to feel worse</span></a></li><li><a href="https://www.economist.com/science-and-technology/2025/03/19/can-people-be-persuaded-not-to-believe-disinformation" id="86eba9e8-d2f2-4c81-b71c-0704e4f6dbe8" data-analytics="article:reports_headline:4" data-test-id="chapterlist-link-3"><span data-testid="right-london-5-false"><span>→</span></span><span>Can people be persuaded not to believe disinformation? </span></a></li><li><a href="https://www.economist.com/science-and-technology/2025/03/14/what-is-the-best-way-to-keep-your-teeth-healthy" id="dae17e3a-d717-4c4f-83b6-1fa9da5c7bf8" data-analytics="article:reports_headline:5" data-test-id="chapterlist-link-4"><span data-testid="right-london-5-false"><span>→</span></span><span>What is the best way to keep your teeth healthy?</span></a></li></ul></div><div orientation="vertical" data-test-id="vertical"><div orientation="vertical"><figure><img loading="lazy" width="1280" height="1709" decoding="async" data-nimg="1" sizes="300px" srcset="https://www.economist.com/cdn-cgi/image/width=16,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 16w, https://www.economist.com/cdn-cgi/image/width=32,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 32w, https://www.economist.com/cdn-cgi/image/width=48,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 48w, https://www.economist.com/cdn-cgi/image/width=64,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 64w, https://www.economist.com/cdn-cgi/image/width=96,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 96w, https://www.economist.com/cdn-cgi/image/width=128,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 128w, https://www.economist.com/cdn-cgi/image/width=256,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 256w, https://www.economist.com/cdn-cgi/image/width=360,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 360w, https://www.economist.com/cdn-cgi/image/width=384,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 384w, https://www.economist.com/cdn-cgi/image/width=480,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 480w, https://www.economist.com/cdn-cgi/image/width=600,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 600w, https://www.economist.com/cdn-cgi/image/width=834,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 834w, https://www.economist.com/cdn-cgi/image/width=960,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 960w, https://www.economist.com/cdn-cgi/image/width=1096,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 1096w, https://www.economist.com/cdn-cgi/image/width=1280,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 1280w, https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg 1424w" src="https://www.economist.com/cdn-cgi/image/width=1424,quality=80,format=auto/content-assets/images/20250322_DE_EU.jpg"></figure></div><div orientation="vertical"><h3 orientation="vertical">From the March 22nd 2025 edition</h3><p orientation="vertical">Discover stories from this section and more in the list of contents</p><p><a href="https://www.economist.com/weeklyedition/2025-03-22" data-analytics="sidebar:weekly_edition"><span data-testid="right-economist-red-true"><span>⇒</span></span><span>Explore the edition</span></a></p></div></div><div><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=economist&amp;publication=economist&amp;title=Do%20viruses%20trigger%20Alzheimer%E2%80%99s%3F&amp;publicationDate=2025-03-17&amp;contentID=%2Fcontent%2Fubbrfahbhj8pvu7grmt7e88egdiohic7&amp;type=A&amp;orderBeanReset=TRUE" target="_blank" rel="noreferrer" data-analytics="end_of_article:reuse_this_content"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" data-testid="renew-outline"><path fill="var(--mb-colour-base-chicago-45)" d="M5.1 16.05a8.25 8.25 0 0 1-.825-1.95A7.696 7.696 0 0 1 4 12.05c0-2.233.775-4.133 2.325-5.7C7.875 4.783 9.767 4 12 4h.175l-1.6-1.6 1.4-1.4 4 4-4 4-1.4-1.4 1.6-1.6H12c-1.667 0-3.083.588-4.25 1.763C6.583 8.938 6 10.367 6 12.05c0 .433.05.858.15 1.275.1.417.25.825.45 1.225l-1.5 1.5ZM12.025 23l-4-4 4-4 1.4 1.4-1.6 1.6H12c1.667 0 3.083-.587 4.25-1.762C17.417 15.063 18 13.633 18 11.95c0-.433-.05-.858-.15-1.275-.1-.417-.25-.825-.45-1.225l1.5-1.5c.367.633.642 1.283.825 1.95.183.667.275 1.35.275 2.05 0 2.233-.775 4.133-2.325 5.7C16.125 19.217 14.233 20 12 20h-.175l1.6 1.6-1.4 1.4Z"></path></svg><span>Reuse this content</span></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Improving recommendation systems and search in the age of LLMs (297 pts)]]></title>
            <link>https://eugeneyan.com/writing/recsys-llm/</link>
            <guid>43450732</guid>
            <pubDate>Sun, 23 Mar 2025 03:40:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eugeneyan.com/writing/recsys-llm/">https://eugeneyan.com/writing/recsys-llm/</a>, See on <a href="https://news.ycombinator.com/item?id=43450732">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            



<!--https://docs.mathjax.org/en/latest/input/tex/delimiters.html-->

<p>Recommendation systems and search have historically drawn inspiration from language modeling. For example, the adoption of <a href="https://arxiv.org/abs/2009.12192" target="_blank">Word2vec</a> to learn item embeddings (for embedding-based retrieval), and using <a href="https://arxiv.org/abs/1511.06939" target="_blank">GRUs</a>, <a href="https://arxiv.org/abs/1905.06874" target="_blank">Transformer</a>, and <a href="https://arxiv.org/abs/1904.06690" target="_blank">BERT</a> to predict the next best item (for ranking). The current paradigm of large language models is no different.</p>

<p>Here, we’ll discuss how industrial search and recommendation systems have evolved over the past year or so and cover model architectures, data generation, training paradigms, and unified frameworks:</p>
<ul>
  <li><a href="#llmmultimodality-augmented-model-architecture">LLM/multimodality-augmented model architecture</a></li>
  <li><a href="#llm-assisted-data-generation-and-analysis">LLM-assisted data generation and analysis</a></li>
  <li><a href="#scaling-laws-transfer-learning-distillation-loras">Scaling Laws, transfer learning, distillation, LoRAs, etc.</a></li>
  <li><a href="#unified-architectures-for-search-and-recommendations">Unified architectures for search and recommendations</a></li>
</ul>

<h2 id="llmmultimodality-augmented-model-architecture">LLM/multimodality-augmented model architecture</h2>

<p>Recommendation models are increasingly adopting language models and multimodal content to overcome traditional limitations of ID-based approaches. These hybrid architectures include content understanding alongside the strengths of behavioral modeling, addressing the common challenges of cold-start and long-tail item recommendations.</p>

<p><strong><a href="https://arxiv.org/abs/2306.08121" target="_blank">Semantic IDs (YouTube)</a> explores content-derived features as substitutes for traditional hash-based IDs.</strong> This approach targets difficulties in predicting user preferences for new and infrequently interacted items. Their solution involves a two-stage framework.</p>

<p>In the first stage, a transformer-based video encoder (similar to Video-BERT) generates dense content embeddings. These embeddings are then compressed into discrete Semantic IDs through a Residual Quantization Variational AutoEncoder (RQ-VAE). Representing user histories with these compact semantic IDs—a few integers rather than high-dimensional embeddings—significantly improves efficiency. Once trained, the RQ-VAE is frozen and used to generate Semantic IDs for the second stage to train a production-scale ranking model.</p>

<p>The RQ-VAE itself is a single-layer encoder-decoder structure with a 256-dimensional latent space. It has eight quantization levels with a codebook of 2048 entries per level. The encoder maps content embeddings to a latent vector, while a residual quantizer discretizes this vector, and the decoder reconstructs the original embedding. The initial embeddings originate from a transformer with a VideoBERT backbone, producing detailed, 2048-dimensional representations that capture the topical content in video.</p>

<p><img src="https://eugeneyan.com/assets/semantic-ids-fig1.jpg" loading="lazy" title="Semantic IDs" alt="Semantic IDs"></p>

<p>To integrate Semantic IDs into ranking models, the authors propose two techniques: an N-gram-based approach, which groups fixed-length sequences, and a SentencePiece Model (SPM)-based method that adaptively learns variable-length subwords. The ranking model is a multi-task production ranking model that recommends the next video to watch given the current video and user history.</p>

<p><strong>Results:</strong> Directly using the dense content embeddings performed worse than using random hash IDs. The authors hypothesize that ranking models heavily rely on memorization from the ID-based embedding tables—replacing these with <em>fixed</em> dense content embeddings led to poorer CTR. However, both N-gram and SPM methods did better than random hashing, especially in cold-start scenarios. Ablation tests revealed that while N-gram approaches had a slight advantage when embedding table sizes were limited (e.g., $8 \times K$ or $4 \times K^2$), SPM methods offered superior generalization and efficiency with larger embedding tables.</p>

<p><img src="https://eugeneyan.com/assets/semantic-ids-fig2.jpg" loading="lazy" title="Semantic IDs" alt="Semantic IDs"></p>
<p>Dense content embeddings (dashed lines) perform worse than random hashing (solid orange).</p>

<p>Similarly,&nbsp;<strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688098" target="_blank">M3CSR (Kuaishou)</a> introduces multimodal content embeddings (visual, textual, audio) clustered via K-means into trainable category IDs.</strong> This transforms static content embeddings into adaptable, behavior-aligned representations.</p>

<p>The M3CSR framework has a dual-tower architecture, splitting user-side and item-side towers to optimize for online inference efficiency where user and item embeddings can be pre-computed and indexed via approximate nearest neighbor indices. Item embeddings are derived from multimodal pretrained models—ResNet for visual, Sentence-BERT for text, and VGGish for audio—and concatenated into a single embedding vector. These vectors are then clustered using K-means (with approximately 1,000 clusters from over 10 million videos).</p>

<p><img src="https://eugeneyan.com/assets/m3csr-fig2.jpg" loading="lazy" title="M3CSR" alt="M3CSR"></p>

<p>Next, cluster IDs are embedded through a Modal Encoder, a dense network translating content features into behaviorally aligned spaces and assigning trainable embeddings. The Modal Encoder uses a dense network to learn the mapping from content-space to behavior space and a cluster ID lookup to assign a trainable cluster ID embedding.</p>

<p><img src="https://eugeneyan.com/assets/m3csr-fig3.jpg" loading="lazy" title="M3CSR" alt="M3CSR"></p>

<p>On the user side, M3CSR learns on user behavior sequences to train sequential models that capture user preferences. In addition, to accurately model user modality preferences, the framework concatenates general behavioral interests with modality-specific interests. These modality-specific interests are derived by converting item IDs back into their multimodal embeddings using the same Modal Encoder.</p>

<p><strong>Results:</strong> M3CSR outperformed several multimodal baselines such as VBPR, MMGCN, and LATTICE. Ablation studies highlighted the importance of modeling modality-specific user interests and demonstrated consistent superiority of multimodal features over single-modal features across datasets (Amazon, TikTok, Allrecipes). A/B testing measured that clicks increased by 3.4%, likes by 3.0%, and follows by 3.1%. In cold-start scenarios, M3CSR also showed improved performance, achieving a 1.2% boost in cold-start velocity and a 3.6% increase in cold-start video coverage.</p>

<p><strong><a href="https://arxiv.org/abs/2310.19453" target="_blank">FLIP (Huawei)</a> shows how to align ID-based recommendation models with LLMs by jointly learning from masked tabular and language data.</strong> The core idea is to reconstruct masked features from one modality (user and item IDs) using information from another modality (text tokens), ensuring tight cross-modal alignment.</p>

<p>FLIP operates in three stages: modality transformation, modality alignment pretraining, and adaptive finetuning. First, tabular data is translated into text using structured prompt templates. Then, joint masked language/tabular modeling is conducted to achieve fine-grained alignment between modalities. During pretraining, textual data undergoes field-level masking (replacing entire fields with <code>[MASK]</code> tokens), while corresponding tabular features are masked by substituting feature IDs with <code>[MASK]</code>.</p>

<p>FLIP trains two parallel models with three objectives: (i) Masked Language Modeling (MLM) predicts masked text tokens using complete tabular context; (ii) Masked Tabular Modeling (MTM) predicts masked feature IDs leveraging textual data; and (iii) Instance-level Contrastive Learning (ICL) aligns global representations across modalities.</p>

<p><img src="https://eugeneyan.com/assets/flip-fig1.jpg" loading="lazy" title="FLIP" alt="FLIP"></p>

<p>Finally, the aligned models—TinyBERT as the LLM and DCNv2 as the ID-based model—are finetuned on the downstream click-through rate (CTR) prediction task. To do this, FLIP adds randomly initialized output layers on both models to estimate click probabilities. The final prediction is a weighted sum of both models’ outputs, where the weights are learned adaptively during training.</p>

<p><img src="https://eugeneyan.com/assets/flip-fig2.jpg" loading="lazy" title="FLIP" alt="FLIP"></p>

<p><strong>Results:</strong> FLIP outperforms the baselines of ID-only, LLM-only, and ID+LLM models. Ablation studies show that (i) both MLM and MTM objectives improve performance, (ii) field-level masking is more effective than random token masking, and (iii) joint reconstruction between modalities is key.</p>

<p>Similarly, <strong><a href="https://dl.acm.org/doi/10.1145/3523227.3551482" target="_blank">beeFormer</a> demonstrates how to train language-only Transformers on user-item interaction data enriched with textual information.</strong> The goal is to bridge the gap between semantic similarity (from textual data) and interaction-based similarity (from user behavior).</p>

<p>beeFormer combines a sentence Transformer encoder for item embeddings with an <a href="https://dl.acm.org/doi/10.1145/3523227.3551482" target="_blank">ELSA (scalabl<strong>E</strong> <strong>L</strong>inear <strong>S</strong>hallow <strong>A</strong>utoencoder)</a>-based decoder that captures patterns from user-item interactions. First, item embeddings are generated through a Transformer trained on textual data. These embeddings are then used to compute user recommendations via ELSA’s low-rank approximation of item-to-item weight. The key here is to backpropagate the gradients from the recommendation loss through the Transformer model. As a result, weight updates capture interaction patterns rather than just semantic similarity.</p>

<p><img src="https://eugeneyan.com/assets/beeformer-fig1.jpg" loading="lazy" title="beeFormer" alt="beeFormer"></p>

<p>To make training computationally feasible on large catalogs, beeFormer applies gradient checkpointing to manage memory usage, gradient accumulation for larger effective batch sizes, and negative sampling to focus training efficiently on relevant items.</p>

<p><strong>Results:</strong> Offline evaluations show that beeFormer surpasses baseline models like mpnet-base-v2 and bge-m3. However, the comparison is limited (and IMHO unfair) since the baselines weren’t finetuned on the training dataset. Interestingly, models trained across multiple domains (movies + books) performed better than domain-specific ones, suggesting that there was transfer learning across domains.</p>

<p><strong><a href="https://arxiv.org/abs/2405.02429" target="_blank">CALRec (Google)</a> introduces a two-stage framework that finetunes a pretrained LLM (PaLM-2 XXS) for sequential recommendations.</strong> Both user interactions and model predictions are represented entirely through text.</p>

<p>First, all input (e.g., user-item interactions) is converted into text sequences by concatenating meaningful attributes (title, category, brand, price) into structured textual prompts. Attributes are formatted in the style of “Attribute name: Attribute description” and concatenated. At the end of the user history sequence, they append the item prefix, thus prompting the LLM to predict the user’s next purchase as a sentence completion task.</p>

<p><img src="https://eugeneyan.com/assets/calrec-fig2.jpg" loading="lazy" title="CALRec" alt="CALRec"></p>

<p>CALRec has a two-stage finetuning approach. The first stage involves multi-category training to adapt the model to sequential recommendation patterns in a category-agnostic way. The second stage refines the model within specific item categories. The training objective combines next-item generation tasks (predicting textual descriptions of items) with auxiliary contrastive alignment. The former aims to generate the text description of the target item given the user’s history; the latter applies contrastive loss on the output of the separate user and item towers to align user history to target item representations.</p>

<p><img src="https://eugeneyan.com/assets/calrec-fig1.jpg" loading="lazy" title="CALRec" alt="CALRec"></p>

<p>During inference, the model is prompted to generate multiple candidates via temperature sampling. They remove duplicates, sort by the output’s log probabilities in descending order, and keep the top k candidates. Then, these textual predictions are matched to catalog items via BM25 and sorted by the matching scores.</p>

<p><strong>Results:</strong> On the Amazon Review Dataset 2018, CALRec outperforms ID-based and text-based baselines (e.g., SASRec, BERT4Rec, FDSA, UniSRec). While the evaluation dataset is limited, CalRec beating the baselines is promising. Ablations demonstrate the necessity of both training stages, especially highlighting transfer learning benefits from multi-category training and incremental gains (0.8 - 1.7%) from contrastive alignment.</p>

<p><strong><a href="https://arxiv.org/abs/2405.11441" target="_blank">EmbSum (Meta)</a> presents a content-based recommendation approach using precomputed textual summaries of user interests and candidate items</strong> to capture interactions within the user engagement history.</p>

<p>EmbSum uses T5-small (61M parameters) to encode user interactions and candidate content, managing long user histories by partitioning them into sessions for encoding. Then, Mixtral-8x22B-Instruct generates the interpretable user interest summaries from user histories. These summaries are then fed into the T5’s encoder to derive final embeddings.</p>

<p><img src="https://eugeneyan.com/assets/embsum-fig1.jpg" loading="lazy" title="EmbSum" alt="EmbSum"></p>

<p>Key to this architecture are User Poly-Embeddings (UPE) and Content Poly-Embeddings (CPE). To get a global representation for UPE, they take the last token of the decoder output (<code>[EOS]</code>) and concatenate it with the representation vectors from the session encoder. This combined representation passes through a poly-attention layer which distills nuanced user interests into multiple embeddings. EmbSum training combines noisy contrastive estimation loss and summarization loss, ensuring high-quality user embeddings.</p>

<p><strong>Results:</strong> EmbSum beats several state-of-the-art content-based recommenders. <em>Nonetheless, direct comparisons with behavioral recommenders were glaringly absent.</em> Ablation studies show that CPE contributes most to performance, followed by session-based grouping and encoding, user poly-embeddings, and summarization losses. Additionally, GPT-4 evaluations indicate strong interpretability and quality of generated user interest summaries.</p>

<p>• • •</p>

<h2 id="llm-assisted-data-generation-and-analysis">LLM-assisted data generation and analysis</h2>

<p>Another common theme is using LLMs to enrich data. Several papers share about using LLMs to tackle data scarcity and enhance the quality of search and recommendations. Examples include generating webpage metadata at Bing, creating synthetic training data to identify poor job matches at Indeed, adding semantic labels for query understanding at Yelp, crafting exploratory search queries at Spotify, and enriching music playlist metadata at Amazon.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688062" target="_blank">Recommendation Quality Improvement (Bing)</a> shares how Bing improved webpage recommendations by using LLMs to generate high-quality metadata</strong> and training an LLM to predict clicks and quality.</p>

<p>Previously, Bing’s webpage representations relied on extractive summaries, which often caused query classification failures. To address this, they used GPT-4 to generate high-quality titles and snippets from full webpage content for two million pages. Then, for efficient large-scale deployment, they finetuned a Mistral-7B model using this GPT-4-generated data.</p>

<p>To improve webpage-to-webpage recommendation rankings, they finetuned a multitask MiniLM-based cross-encoder on both pairwise click predictions <em>and</em> quality classification tasks. The resulting quality scores were then linearly combined with click predictions from an existing LightGBM ranker.</p>

<p><img src="https://eugeneyan.com/assets/bing-fig2.jpg" loading="lazy" title="Recommendation Quality Improvement" alt="Recommendation Quality Improvement"></p>
<p>The MiniLM (right) is ensembled with the LightGBM ranker (left).</p>

<p>To better understand user preferences, they defined 16 distinct recommendation scenarios reflecting common user patterns. Using high-precision prompts, they classified each webpage-to-webpage recommendation, incorporating the enhanced title and snippets from Mistral-7B, into these scenarios. Then, by monitoring the distribution changes of each scenario, they quantified the improvements in webpage recommendation quality.</p>

<p><img src="https://eugeneyan.com/assets/bing-table4.jpg" loading="lazy" title="Recommendation Quality Improvement" alt="Recommendation Quality Improvement"></p>

<p><strong>Results:</strong> The enhanced system reduced clickbait by 31%, low-authority content by 35%, and duplicate content by 76%. At the same time, higher authority content increased by 18%, cross-medium recommendations rose by 48%, and recommendations with greater specificity improved by 20%. This is despite lower-quality content (e.g., clickbait) historically showing higher CTR, demonstrating the effectiveness of the quality-focused cross-encoder.</p>

<p>(👉 Recommended read) <strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688043" target="_blank">Expected Bad Match (Indeed)</a> shares how they used LLM-generated labels to filter poor job matches.</strong> Specifically, they finetuned LLMs to evaluate recommendation quality and generate labels for a post-processing classifier.</p>

<p>They started with building an evaluation set by cross-reviewing 250 matches, narrowing it down to 147 high confidence labeled examples. Then, they prompted various LLMs, such as Llama2 and Mistral-7B, using expert recruitment guidelines to evaluate match quality across dimensions like job descriptions, resumes, and user interactions. However, these models struggled with detailed prompts, producing generalized assessments that didn’t consider detailed job and job seeker information. On the other hand, GPT-4 performed better but was prohibitively expensive.</p>

<p>To balance cost and effectiveness, the team finetuned GPT-3.5 on a curated dataset of over 200 human-reviewed GPT-4 responses. This finetuned GPT-3.5 matched GPT-4’s performance at just a quarter of the cost and latency. But despite the improvements, its inference latency of 6.7 seconds remained too high for online use. Thus, they trained a lightweight classifier, eBadMatch, using LLM-generated labels and categorical features from job descriptions, resumes, and user activity. In production, a daily pipeline samples job matches, engineers features, anonymizes data, generates LLM labels, and retrains the model. This classifier acts as a post-processing filter to remove low-quality matches.</p>

<p><strong>Results:</strong> The eBadMatch classifier achieved an AUC-ROC of 0.86 against LLM labels, with latency suitable for real-time filtering. Online experiments demonstrated that applying a 20% threshold filter on invitation-to-apply emails reduced batch matches by 17.68%, lowered unsubscribe rates by 4.97%, and increased application rates by 4.13%. Similar improvements were observed in homepage recommendation feeds.</p>

<p><img src="https://eugeneyan.com/assets/ebadmatch-table2.jpg" loading="lazy" title="Expected Bad Match" alt="Expected Bad Match"></p>

<p>(👉 Recommended read) <strong><a href="https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html" target="_blank">Query Understanding (Yelp)</a> shows how they integrated LLMs into their query understanding pipeline</strong> to improve query segmentation and review highlights.</p>

<p>Query segmentation identifies meaningful parts of user queries—such as topic, name, time, location, and question—and tags them accordingly. Along the way, they learned that spelling correction and segmentation could be done together and thus added a meta tag to mark spell-corrected sections and combined both tasks into a single prompt. Retrieval-augmented generation (RAG) further improved segmentation accuracy by incorporating business names and categories as context that disambiguated user intent. For evaluation, they compared LLM-identified segments against human-labeled datasets of name match and location intent.</p>

<p>Review highlights selects key snippets from reviews to highlight in search results. They used LLMs to generate synonymous phrases suitable for highlights. Curated examples prompted LLMs to replicate human reasoning in phrase expansion. RAG further enhanced relevance by augmenting the input with relevant business categories to guide phrase generation. Offline evaluation was done via human annotators before online A/B testing of the new highlight phrases. To scale efficiently and cover 95% of traffic, Yelp pre-computed snippet expansions using batch calls to OpenAI and stored them in key-value stores to reduce latency.</p>

<p><img src="https://eugeneyan.com/assets/yelp-fig3.jpg" loading="lazy" title="Review highlights" alt="Review highlights"></p>

<p>The team shared their approach—from initial formulation and proof of concept (POC) to scaling up. Initially, they assessed LLM suitability and defined the project’s scope. During POC, they leveraged the power-law distribution of queries, caching pre-computed LLM responses for common queries covering most traffic. To scale, they created golden datasets using GPT-4 outputs and finetuned smaller, cost-effective models like GPT-4o-mini. Additionally, real-time models like BERT and T5 addressed less frequent, long-tail queries.</p>

<p><strong>Results:</strong> Yelp’s query segmentation significantly improved location intent detection, while enhanced review highlights increased both session and search click-through rates (CTR), especially benefiting long-tail queries.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688035" target="_blank">Query Recommendations (Spotify)</a> details how they built a hybrid query recommendation system to suggest exploratory search queries</strong> alongside direct results. This approach was necessary to support Spotify’s expansion beyond music to podcasts, audiobooks, and diverse content types by helping users explore those content.</p>

<p><img src="https://eugeneyan.com/assets/query-recs-fig1.jpg" loading="lazy" title="Query Recommendations" alt="Query Recommendations"></p>

<p>Spotify generated query suggestions by (i) extracting from catalog titles, playlist names, and podcasts, (ii) mining suggestions from search logs, (iii) leveraging users’ recent searches, (iv) applying metadata and expansion rules (e.g., “artist name” + “covers”), and (v) generating synthetic natural language queries via LLMs. To generate synthetic queries, techniques such as Doc2query and InPars were used to broaden query variations, enhancing exploratory searches and mitigating retrievability bias.</p>

<p>The query suggestions were then combined with regular results and ranked by a point-wise ranker optimized for downstream user actions like streaming or adding content to playlists. The ranker use features such as lexical matching, query statistics, retrieval scores, and user consumption patterns. For personalization, they relied on vector representations of users and query suggestion candidates.</p>

<p><strong>Results:</strong> Spotify saw a 9% increase in exploratory intent queries, a 30% rise in maximum query length per user, and a 10% increase in average query length—this suggests the query recommendation updates helped users express more complex intents. An online ablation showed the ranker’s removal caused a 20% decline in clicks on recommendations, underscoring its importance.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688047" target="_blank">Playlist Search (Amazon)</a> discusses Amazon’s integration of LLMs into playlist search pipelines to tackle challenges</strong> like data scarcity, metadata enrichment, and scalable evaluation while reducing reliance on manual annotation.</p>

<p>To enrich metadata, they used LLMs (LLM curator) to create detailed descriptions for community playlists based on their initial 15 tracks, capturing themes, genres, activities, and artists. (These community playlists typically only had a playlist title.) This addressed data scarcity in community-generated content. Then, Flan-T5-XL was finetuned to scale this inference process.</p>

<p>They also applied LLMs to generate synthetic queries paired with playlists (and associated metadata) to create training data for bi-encoder models. These pairs were generated and scored by an LLM (LLM labeler) to maintain balanced positive and negative examples. Lastly, they used an LLM (LLM judge), guided by human annotations and careful prompting to ensure alignment, to streamline evaluations.</p>

<p><img src="https://eugeneyan.com/assets/playlist-fig1.jpg" loading="lazy" title="Playlist Search" alt="Playlist Search"></p>

<p><strong>Results:</strong> Integrating LLMs led to substantial double-digit recall improvements across benchmarks, SEO, and paraphrasing datasets. Overall, the use of LLMs helped overcome the challenges of data scarcity and evaluation scalability without extensive manual effort.</p>

<p>• • •</p>

<h2 id="scaling-laws-transfer-learning-distillation-loras">Scaling Laws, transfer learning, distillation, LoRAs</h2>

<p>Another trend is the adoption of training approaches from large language models (LLMs) and computer vision into recommender systems. This includes exploring scaling laws (how model size and data quantity affect performance), using knowledge distillation to transfer insights from large models to smaller, efficient ones, applying cross-domain transfer learning to handle limited data, and parameter-efficient fine-tuning techniques such as LoRAs.</p>

<p>(👉 Recommended read)&nbsp;<strong><a href="https://arxiv.org/abs/2311.11351" target="_blank">Scaling Laws</a> investigates how the performance of ID-based sequential recommender models improve as their model size and data scale increase.</strong> The authors uncovered a predictable power-law relationship where performance consistently improves as the size of both models and datasets expands.</p>

<p>They adopt a decoder-only transformer architecture, experimenting with models ranging from 98.3K to 0.8B parameters. They evaluated these models on the MovieLens-20M and Amazon-2018 datasets. For the Amazon dataset, interaction records from 29 domains were combined, sorted chronologically, and simplified to include only item IDs without additional metadata. The datasets were then formatted into fixed-length sequences of 50 items each; shorter sequences were padded and longer ones were truncated. The model is then optimized to predict the next item at time step $t + 1$ conditioned on the previous $t$ items.</p>

<p><img src="https://eugeneyan.com/assets/scaling-fig1.jpg" loading="lazy" title="Scaling Laws" alt="Scaling Laws"></p>

<p>To tackle instability in training larger models, the authors introduced two key improvements. First, they implemented layer-wise adaptive dropout, applying higher dropout rates in lower layers and lower dropout rates in upper layers. The intuition is that lower layers process direct input from data and are more prone to overfitting. Conversely, higher layers build more abstract representations and thus benefit from less dropout to reduce information loss that could lead to underfitting.</p>

<p>The second improvement was dynamically switching optimizers during training—starting with Adam before switching to stochastic gradient descent (SGD) at a predefined point. This approach is motivated by the observation that Adam quickly reduces loss in early training phases but ultimately SGD achieves better convergence.</p>

<p><strong>Results:</strong> Unsurprisingly, increased model capacity (excluding embedding parameters) consistently reduced cross-entropy loss. They modeled this with a power-law curve and accurately predicted performance for larger models (75.5M and 0.8B params). Similarly, they observed that larger models could achieve lower losses even with smaller datasets, whereas smaller models needed more data to reach comparable performance. For example, a smaller 98.3K-parameter model required twice the data (18.5M interactions) compared to a larger 75.5M-parameter model (9.2M interactions) to attain similar performance.</p>

<p><img src="https://eugeneyan.com/assets/scaling-fig2.jpg" loading="lazy" title="Scaling Laws" alt="Scaling Laws"></p>

<p>Regarding data repetition, models of sizes 75.5M and 98.3K parameters continued improving beyond a single training epoch, with notable gains observed from two to five epochs. Surprisingly, changing model shape had minimal impact on performance. Ablation studies showed that layer-wise adaptive dropout and optimizer switching substantially enhanced performance in larger models (24 layers), though smaller models (2 layers) remained largely unaffected. Further ablations on five challenging recommendation tasks highlighted the advantage of larger models, particularly for long-tail items and cold-start users.</p>

<p><strong><a href="https://arxiv.org/abs/2401.01497" target="_blank">PrepRec</a> shows how pretraining can be adapted to recommender systems, enabling cross-domain, zero-shot recommendations.</strong> The key innovation is leveraging item popularity dynamics derived solely from user interactions, without relying on item metadata.</p>

<p>PrepRec uses popularity statistics calculated over coarse (monthly) and fine (weekly) timescales. These popularity metrics are converted into percentiles and then encoded into vector representations. In addition, the model incorporates relative time intervals between user interactions and uses a fixed positional encoding for each interaction in a user’s sequence. (IMHO, while the approach is effective, it relies on several specialized techniques—coarse vs. fine-grained periods, relative time intervals, and positional encodings—which might limit its generalizability.)</p>

<p><img src="https://eugeneyan.com/assets/preprec-fig2.jpg" loading="lazy" title="PrepRec" alt="PrepRec"></p>

<p>For training, PrepRec has binary cross-entropy as the objective and uses Adam for optimization. The model and baselines have consistent settings: embedding dimension of 50, max sequence length of 200, and batch size of 128. During inference, PrepRec calculates item popularity dynamics from the target domain before generating recommendations via inference on the pretrained model.</p>

<p><strong>Results:</strong> PrepRec achieves promising zero-shot performance, with only a minor reduction (2-6% recall@10) compared to models like SasREC and BERT4Rec which were specifically trained on the target domains. When trained from scratch on the target domains, PrepRec matches or slightly surpasses these models in regular sequential recommendations despite using just 1-5% of their parameters, thanks to not having item-specific embeddings. Ablations showed that modeling relative time intervals significantly boosted performance, and capturing both coarse and fine-grained popularity trends was essential for tracking evolving user interests.</p>

<p><strong><a href="https://arxiv.org/abs/2408.16238" target="_blank">E-CDCTR (Meituan)</a> demonstrates the potential of transfer learning by using organic item data to improve click-through rate (CTR) predictions in advertising</strong>, tackling the challenge of sparse ad data.</p>

<p>E-CDCTR has three components: the tiny pretraining model (TPM), complete pretraining model (CPM), and advertising CTR model (A-CTR). The TPM, a lightweight model with just embedding and MLP layers, trains monthly on six months of organic impressions and clicks. It captures long-term collaborative filtering signals via historical user and item embeddings. Features include user and item IDs, category IDs, etc.</p>

<p><img src="https://eugeneyan.com/assets/e-cdctr-fig2.jpg" loading="lazy" title="E-CDCTR" alt="E-CDCTR"></p>

<p>Next, the CPM pretrains a CTR model weekly using the most recent month’s organic data and using the user and item embeddings learned by TPM. Finally, the A-CTR model is initialized from the CPM and finetuned daily on advertising-specific data. A-CTR also uses user and item embeddings from the TPM. A-CTR also uses richer features such as user behavior sequences, user context, item metadata, and feature interactions, resulting in a more sophisticated model architecture that includes sequential input, feature crosses, and a larger MLP layer.</p>

<p>For online inference, E-CDCTR employs user and item embeddings generated by TPM from the past three months. The A-CTR model then uses these embeddings to predict the advertising CTR. (The authors mention using self-attention to combine embeddings but provide limited details on training it.)</p>

<p><strong>Results:</strong> E-CDCTR outperforms cross-domain baselines such as KEEP, CoNet, DARec, and MMoE. Ablation studies confirm the value of both TPM and CPM, with CPM having a more substantial impact. In addition, extending historical embeddings from one to three months further enhanced performance, whereas simply merging advertising data with organic data did not yield improvements.</p>

<p><strong><a href="https://arxiv.org/abs/2408.14678" target="_blank">Bridging the Gap (YouTube)</a> shares insights on applying knowledge distillation in large-scale personalized video recommendations at YouTube.</strong></p>

<p>Their recommenders are multi-objective pointwise models for ranking videos. These models simultaneously optimizing short-term objectives like video CTR and long-term objectives like the estimated long-value value of a user. Their models typically feature a teacher-student setup, with the teacher and student models sharing similar architectures though the teacher model is 2 - 4x larger than the student model.</p>

<p>However, distribution shifts between teacher and student can cause biases. To address this, the authors propose an auxiliary distillation strategy—instead of directly using the teacher’s predictions (soft labels), they decouple the hard labels from the soft teacher predictions via separate task logits. This enables the student model to effectively learn from the teacher without inheriting unwanted biases.</p>

<p><img src="https://eugeneyan.com/assets/bridge-fig2.jpg" loading="lazy" title="Bridging the gap" alt="Bridging the gap"></p>

<p>To amortize the cost of training the large teacher model, they have a single teacher improve multiple student models. As a result, a single teacher model can provide distilled knowledge to various specialized recommendation tasks, reducing redundancy and computational overhead. Teacher labels are stored in a columnar database that prioritizes read performance for the students during training.</p>

<p><img src="https://eugeneyan.com/assets/bridge-fig3.jpg" loading="lazy" title="Bridging the gap" alt="Bridging the gap"></p>

<p><strong>Results:</strong> The auxiliary distillation strategy delivered a 0.4% improvement in E(LTV) prediction compared to direct distillation methods, which performed similarly to models without distillation. This confirms the auxiliary distillation approach’s effectiveness in reducing teacher noise. In ablation studies on teacher size, even a modest teacher (2x the student’s size) led to meaningful improvements (+0.42% engagement, +0.34% satisfaction) while a 4x teacher led to +0.43% engagement and +0.46% satisfaction.</p>

<p>Similarly, <strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688041" target="_blank">Self-Auxiliary Distillation (Google)</a> introduces a distillation framework aimed at improving sample efficiency for large-scale recommendation models.</strong></p>

<p>The core idea is to prioritize training on high-quality labels while improving the resolution of lower-quality labels. The intuition is that positive labels provide more signal than negative labels when predicting CTR, thus it makes sense to prioritize them. On the other hand, negative labels are closer to weak positives than an absolute zero—thus, representing them with an estimated CTR value offers better training signal.</p>

<p>The model has a shared bottom tower with two heads: the main head (teacher) is trained directly on ground-truth labels, serving as the primary inference model and generating calibrated soft labels. Calibration is maintained by ensuring the mean prediction matches the mean of actual labels. The auxiliary head (student) learns from a mixture of these soft teacher labels and original labels, helping stabilize the training process. Specifically, the auxiliary head has a bilateral branch where one branch distills knowledge from the teacher’s soft labels and the other learns from the hard ground-truth label. A selector merges the labels from both branches using functions such as $max(y, y’)$.</p>

<p><img src="https://eugeneyan.com/assets/selfaux-fig1.jpg" loading="lazy" title="Self-auxiliary distillation" alt="Self-auxiliary distillation"></p>

<p><strong>Results:</strong> Self-attention distillation consistently improved recommendation quality across multiple domains including apps, commerce, and video recommendations. Ablations show that training on original ground-truth labels primarily drives performance gains, while the distillation component significantly stabilizes and aligns the model’s predictions. Training exclusively on ground-truth labels showed inconsistent results while training on the distillation labels only didn’t lead to improvements.</p>

<p><strong><a href="https://arxiv.org/abs/2405.00338" target="_blank">DLLM2Rec</a> shows how to distill recommendation knowledge from LLMs into lightweight, conventional sequential recommendation models</strong>, making deployment more practical. The paper identifies three main challenges: (i) unreliable teacher knowledge/labels, (ii) the capability gap between teacher and student models, and (iii) semantic divergence between the teacher’s and student’s embedding spaces.</p>

<p>To tackle these issues, DLLM2Rec adopts two key strategies: importance-aware ranking distillation and collaborative embedding distillation. Importance-aware ranking distillation focuses on selecting reliable instances for training via importance weights. These weights consider factors like ranking position (prioritizing items ranked higher by the teacher), teacher confidence (evaluated through content similarity between generated descriptions and actual items), and the consistency between the teacher’s and student’s recommendations. Meanwhile, collaborative embedding distillation involves using a learnable MLP to effectively translate embeddings from the teacher’s semantic space into the student’s space.</p>

<p><img src="https://eugeneyan.com/assets/dllm2rec-fig1.jpg" loading="lazy" title="DLLM2Rec" alt="DLLM2Rec"></p>

<p>In their experiments, they use BIGRec (built on Llama2-7B) as the teacher and three popular sequential models (GRU4Rec, SASRec, and DROS) as students.</p>

<p><strong>Results:</strong> DLLM2Rec boosts the performance of student models, showing an average improvement of 47.97% across three datasets (Amazon Video Games, MovieLens-10M, and Amazon Toys and Games) when evaluating hit rate@k and NDCG@k (see Table 5 in the paper). Additionally, inference time dropped significantly, from 3-6 hours with the teacher model down to just 1.6-1.8 seconds with DLLM2Rec.</p>

<p><strong><a href="https://arxiv.org/abs/2408.08913" target="_blank">MLoRA (Alibaba)</a> describes using domain-specific LoRAs (low-rank adapters) to enhance multi-domain CTR prediction models.</strong> It addresses two common problems: data sparsity (limited data per domain) and domain diversity (variations across domains) that typically arise when training either separate models or a single combined model respectively.</p>

<p>They adopt a two-step training process. First, they pretrained a shared backbone network on extensive, multi-domain data to learn generalizable patterns across domains. Then, they freeze the backbone and finetune domain-specific LoRAs on each domain’s unique data. A key challenge was adapting LoRA ranks layer-by-layer due to varying dimensions in CTR model layers. (Recommender models have different dimentions per layer unlike language models which typically have uniform dimensions.) In their experiments, all models had hidden layers of 256, 128, and 64 dimensions.</p>

<p><img src="https://eugeneyan.com/assets/mlora-fig3.jpg" loading="lazy" title="MLoRA" alt="MLoRA"></p>

<p>To get a sense of data distribution differences between pretraining and finetuning: During their A/B test, the pretrained backbone used 13 billion samples spanning 90 days from 10 domains, whereas finetuning involved 3.2 billion samples from just 21 days.</p>

<p><strong>Results:</strong> MLoRA increased AUC by 0.5% across datasets such as Taobao-10, Amazon-6, and MovieLens. Ablation studies showed that domains with smaller datasets and higher inter-domain differences benefited more. They also found that simpler models (like MLP) performed best with lower LoRA ranks (32), while more complex models (like DeepFM) benefited from higher ranks (64 - 128). A/B testing showed substantial business gains—a 1.49% lift in CTR, a 3.37% boost in conversions, and a 2.71% increase in paid buyers—with only a modest 1.76% rise in model complexity due to the use of LoRAs.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688053" target="_blank">Taming One-Epoch (Pinterest)</a> highlights the challenge of models overfitting after just one training epoch</strong>, primarily due to the long-tail nature of recommendation data. (Perhaps the Scaling Laws paper above, which showed gains beyond one epoch, used datasets (i.e., Amazon and MovieLens) that had the long-tail filtered out.) This overfitting arises because tail entries have far more degrees of freedom compared to the limited training examples available.</p>

<p>Here’s more context on the “one-epoch problem”: In online experiments, they saw that deep CTR models without ID embeddings typically require multiple epochs to converge. However, introducing ID embeddings often causes performance to peak after just one epoch, leading to worse results compared to multi-epoch training without ID embeddings.</p>

<p>Their solution involves two distinct stages. In the first stage, they pretrain foundational ID embeddings using a minimal dot-product model combined with contrastive loss, utilizing in-batch and uniformly random negatives. This contrastive approach reduces the effective dimensionality of tail entries, minimizing overfitting. Moreover, because the pretraining step is relatively lightweight, they can use a much larger dataset—around ten times the engagement data compared to the downstream recommendation model.</p>

<p>In the second stage, the pretrained embeddings are finetuned in task-specific models for multiple epochs. By separating embedding pretraining from downstream finetuning, they mitigate overfitting and get better results compared to merely freezing the embeddings.</p>

<p><img src="https://eugeneyan.com/assets/one-epoch-fig1.jpg" loading="lazy" title="Taming One-Epoch" alt="Taming One-Epoch"></p>

<p><strong>Results:</strong> In Figure 2 above, the typical binary cross-entropy (BCE) loss tends to overfit after the first epoch, whereas the contrastive loss remains stable. Ablation studies revealed that a single-stage training method underperformed relative to baseline models due to severe overfitting (−3.347% for Homefeed and −1.907% for Related Pins). Conversely, the two-stage training consistently yielded superior results (+1.323% Homefeed, +2.187% Related Pins), and in online A/B tests, led to a significant overall engagement lift of 2.2%.</p>

<p><strong><a href="https://arxiv.org/abs/2409.14517" target="_blank">Sliding Window Training (Netflix)</a> describes their method for efficiently training on long user history sequences</strong> without incurring the memory and latency costs associated with large input sizes. One workaround is to truncate user historical interactions—however, this comes at the cost of not using valuable information from the entire user journey.</p>

<p>Their solution is elegantly simple. Assuming a baseline model that only handles sequences of up to 100 items, they introduce a sliding window sampler during training. This sampler selects different segments of user history in each training epoch, allowing the model to learn on long-term user patterns. Additionally, they experimented with mixing epochs—some focused exclusively on sliding windows, while others emphasized only the latest 100 interactions—to balance between recent user behavior and historical preferences.</p>

<p><img src="https://eugeneyan.com/assets/sliding-window-fig2.jpg" loading="lazy" title="Sliding Window Training" alt="Sliding Window Training"></p>

<p><strong>Results:</strong> Offline evaluations showed the sliding window method consistently outperformed models trained solely on the most recent 100 interactions. Specifically, a pure sliding window variant slightly reduced Mean Reciprocal Rank (MRR) by 1.2%, but improved Mean Average Precision (MAP) by 1.5% and recall significantly by 7.01%. Hybrid approaches combining sliding windows with recent interactions, and extending input sequence lengths to 500 or even 1000 items, delivered the best overall performance. However, these extended approaches had slightly worse perplexity, indicating a trade-off between predictive confidence and actual recommendation performance.</p>

<p>• • •</p>

<h2 id="unified-architectures-for-search-and-recommendations">Unified architectures for search and recommendations</h2>

<p>The final theme highlights a growing shift toward unified system architectures that blend search and recommendations, drawing inspiration from foundation models. Instead of deploying multiple single-task models, recent papers present unified frameworks capable of handling diverse retrieval and ranking tasks within a shared infrastructure. For example, LinkedIn’s 360Brew and Netflix’s UniCoRn show how unified models trained on multiple tasks can outperform specialized, single-task counterparts.</p>

<p><strong><a href="https://arxiv.org/abs/2410.16823" target="_blank">Bridging Search &amp; Recommendations (Spotify)</a> demonstrates the advantages of training a unified generative retrieval model</strong> on both search and recommendation data, rather than separately, and how it can outperform task-specific models.</p>

<p>In their approach, a generative recommender predicts item IDs based on a user’s past interactions, while a generative search retriever predicts item IDs from tokenized search queries. The underlying model builds upon Flan-T5-base, extending the vocabulary to include all item IDs with one additional token per item. These models are trained auto-regressively using teacher forcing and cross-entropy loss, aiming to accurately predict the next relevant item ID. During inference, item IDs are generated directly from either a user’s interaction history (for recommendations) or a text query (for search).</p>

<p><img src="https://eugeneyan.com/assets/bridging-spotify-table1.jpg" loading="lazy" title="Bridging search and recsys" alt="Bridging search aand recsys"></p>

<p>Evaluation is done via standard recall metrics (recall@10 for simulated datasets, recall@30 for real-world datasets) against common baselines like BM25, SASRec, and BERT4Rec.</p>

<p><strong>Results:</strong> Jointly trained multi-task models outperformed their single-task counterparts, achieving an average increase of 16% in recall@30. On the Podcasts dataset, the unified model significantly improved performance by +33% across both tasks, especially for torso items (those outside the top 1%), showing gains of 262% for recommendations and 855% for search.</p>

<p>While the research wasn’t focused on replacing conventional models, the comparisons against behavioral baselines were insightful. Across three datasets, generative models consistently lagged behind specialized recommendation baselines (SASRec, BERT4Rec) significantly (green below). Similarly, for search, traditional baselines (BM25, Bi-encoder) were still superior (green below). This indicates that generative retrieval models are still far from fully replacing conventional methods.</p>

<p><img src="https://eugeneyan.com/assets/bridging-spotify-table5.jpg" loading="lazy" title="Bridging search and recsys" alt="Bridging search aand recsys"></p>

<p>(👉 Recommended read)  <strong><a href="https://arxiv.org/abs/2501.16450" target="_blank">360Brew (LinkedIn)</a> consolidates several ID-based ranking models into a single large 150B decoder-only model</strong> equipped with a natural language interface, effectively replacing traditional feature engineering with prompt engineering.</p>

<p><img src="https://eugeneyan.com/assets/360brew-table1.jpg" loading="lazy" title="360Brew" alt="360Brew"></p>

<p>360Brew builds upon the Mixtral-8x22B pretrained Mixture-of-Experts model. Its fine-tuning dataset includes 3-6 months of interactions from roughly 45 million monthly active users in the US, encompassing member profiles, job descriptions, posts, and various interaction logs—all transformed into a text-based format.</p>

<p>Training involves three key stages. First, continuous pretraining (CPT) is done with a maximum context length of 16K tokens with packing techniques. Next, instruction fine-tuning (IFT) is performed using a mix of open-source datasets (such as UltraChat) and internally generated instruction-following data. Finally, supervised fine-tuning (SFT) applies multi-turn chat templates designed to enhance the model’s understanding of member-entity interactions, improving its predictive capabilities across specific user interfaces.</p>

<p>The model was trained on 256-512 H100 GPUs using FSDP, and production deployment adopts vLLM and inference-time RoPE scaling. 360Brew focuses on binary prediction tasks, such as whether a user will like a posts, and uses token logits to assign scores.</p>

<p><strong>Results:</strong> The unified model supports over 30 different ranking tasks across LinkedIn’s platforms, matching or surpassing specialized production models while reducing complexity and maintenance overhead. The researchers also found that the unified model improved substantially with more data—while initial iterations performed poorly, tripling the dataset resulted in performance exceeding specialized models (Figure 2 below). Additionally, larger models consistently outperformed smaller versions (8x22B &gt; 8x7B &gt; 7B). Also, 360Brew delivered strong performance for cold-start users, outperforming traditional models by a wider margin when user interaction data was limited.</p>

<p><img src="https://eugeneyan.com/assets/360brew-fig2.jpg" loading="lazy" title="360Brew" alt="360Brew"></p>

<p>Similarly, <strong><a href="https://arxiv.org/abs/2408.10394" target="_blank">UniCoRn (Netflix)</a> introduces a unified contextual ranker designed to serve both search and recommendation tasks</strong> through a shared contextual framework. This unified model achieves comparable or better performance than multiple specialized models, thus reducing operational complexity.</p>

<p>The UniCoRn model uses contextual information such as user ID, search queries, country, source entity ID, and task type, predicting the probability of positive engagement with a target entity (e.g., a movie). Since not all contexts are always available, heuristics are used to impute missing data. For example, missing source entity IDs in search tasks are imputed as null, and missing query contexts in recommendation tasks use the entity’s display names.</p>

<p>UniCoRn incorporates two broad feature categories: context-specific features (like query length and source entity embeddings) and combined context-target features (such as click counts for a target entity in response to a query). The architecture includes embedding layers for categorical features, enhanced with residual connections and feature crossing.</p>

<p><img src="https://eugeneyan.com/assets/unicorn-fig1.jpg" loading="lazy" title="Unicorn" alt="Unicorn"></p>

<p>Training uses binary cross-entropy loss and the Adam optimizer. Netflix incrementally increased personalization: starting from a semi-personalized model using user clusters, progressing to including outputs from other recommendation models, and finally incorporating pretrained and fine-tuned user and item embeddings.</p>

<p><strong>Results:</strong> UniCoRn consistently matched or exceeded specialized models. Personalization boosted outcomes, delivering a 10% improvement in recommendations and a 7% lift in search. Ablation studies showed the importance of explicitly including the task type as context, imputing missing features to maximize feature coverage, and applying feature crossing to enhance multi-task learning effectiveness.</p>

<p>(👉 Recommended read) <strong><a href="https://arxiv.org/abs/2306.04833" target="_blank">Unified Embeddings (Etsy)</a> shares how they unified transformer-based, term-based, and graph-based embeddings within a two-tower model</strong> architecture. This goal was to address common gaps such as mismatches between search queries and product vocabulary (lexical matching) and the poor performance of neural embeddings due to limited user context.</p>

<p><img src="https://eugeneyan.com/assets/unifiedemb-fig2.jpg" loading="lazy" title="Unified Embeddings" alt="Unified Embeddings"></p>

<p>Their model adopts a classic two-tower structure, consisting of a product encoder and a joint query-user encoder. The product encoder combines transformer-based embeddings, bipartite graph embeddings (trained using a full year of query-product interaction data), product title embeddings, and location information. Interestingly, direct finetuning of transformer-based models like distilBERT and T5 did not yield significant offline metric improvements. Instead, inspired by docT5query, they pretrained a T5-small model specifically designed to predict historically purchased queries based on product descriptions. The query-user encoder combines query text embeddings, location, and historical engagement data. Both query/title and location embeddings are shared across the two towers for consistency.</p>

<p>They emphasize the effectiveness of negative sampling, sharing multiple approaches such as hard in-batch negatives (positives from other queries within the batch), uniform negatives (randomly selected from the entire product corpus), and dynamic hard negatives (random samples narrowed down by the model to identify the most challenging examples). The goal here is to find the most similar negatives to help the model learn on the hardest samples.</p>

<p>To balance relevance with product quality, they integrated quality boosting into their embeddings via an approximate nearest neighbor (ANN) index. Product embeddings are augmented with query-independent quality scores reflecting attributes such as product ratings, freshness, and conversion rates—factors proven to increase engagement independently from query relevance. Given the original product embeddings, they concatenate it with the quality score vectors; the respective query embedding is concatenated with a constant vector. The final score of the product, for a query, is the dot product of the updated product and query embedding.</p>

<p><img src="https://eugeneyan.com/assets/unifiedemb-fig3.jpg" loading="lazy" title="Unified Embeddings" alt="Unified Embeddings"></p>

<p>The system operates through two main stages: offline indexing and online serving. Offline, embeddings and quality scores are generated and pre-indexed into an ANN system (using FAISS with a 4-bit product quantizer). This approach, combined with a re-ranking step, achieves a recall loss below 4% while keeping latency under 20ms@p99. At the online stage, incoming queries are embedded in real time to retrieve products from the ANN index. They also shared how they applied caching while handling in-session personalization features.</p>

<p><strong>Results:</strong> In A/B testing, the unified embedding model drove a site-wide conversion lift of 2.63% and boosted organic search purchases by 5.58%. Offline tests showed that Unified Embeddings consistently outperformed traditional baselines for both head and tail queries. Ablation studies revealed the strongest contributions came from graph embeddings (+15% recall@100), followed by description embeddings (+6.3%) and attributes (+3.9%). Additionally, location embeddings significantly improved purchase recall@100 (+8%) for US users by minimizing geographic mismatches. Removing hard negatives resulted in a noticeable 7% drop in performance, underscoring their importance.</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688039" target="_blank">Embedding Long Tail (Best Buy)</a> shared how they optimize semantic product search to better address long-tail queries</strong> which typically suffer from sparse user interaction data.</p>

<p>To create a high-quality dataset, they collected user engagement data from product pages and applied a two-stage filtering process, reducing data volume (by 10x) while maintaining quality and balanced coverage across product categories. First, they retained interactions observed from at least two unique visitors, then performed stratified sampling across categories to mitigate popularity bias. To further augment this data, they prompted a Llama-13B model to generate ten synthetic search queries per product using the product’s title, category, description, and specifications, thus ensuring comprehensive catalog coverage.</p>

<p>Their model follows a two-tower architecture based on Best Buy’s internally developed BERT variant, an adaptation of RoBERTa finetuned through masked language modeling on search queries and product information. They used the first five layers of this BERT model to initialize both the search and product encoders. Training involved using in-batch negatives with multi-class cross-entropy loss. For deployment, Solr functions as both the inverted index and vector database, with a caching layer added to minimize redundant requests to the embedding service.</p>

<p><strong>Results:</strong> Adding semantic retrieval to the existing lexical search improved conversion rates by 3% in online A/B tests. Offline experiments demonstrated incremental improvements through various enhancements: two-stage data filtering (+0.24% recall@200), synthetic positive queries (+0.7%), additional product features (+1.15%), query-to-query followed by query-to-product fine-tuning (+2.44%), and model weight merging (+4.67%). Notably, their final model outperformed the baseline (all-mpnet-base-v2) while using only half the parameters at 50M vs 110M. (Nonetheless, it may not have been a fair comparison given the baseline was not finetuned.)</p>

<p><strong><a href="https://dl.acm.org/doi/10.1145/3640457.3688030" target="_blank">User Behavioral Service (YouTube)</a> presented an innovative approach for serving large user sequence models efficiently while sidestepping latency challenges.</strong></p>

<p><img src="https://eugeneyan.com/assets/ubs-fig1.jpg" loading="lazy" title="User Behavioral Service" alt="User Behavioral Service"></p>

<p>The intuition behind User Behavior Service (UBS) is decoupling the serving of the user sequence model from the main recommendation model. This design allows independent control over user embedding computation. Although both models are co-trained, they are exported and served separately. The user model computes embeddings asynchronously, storing them in a high-speed key-value cache that’s regularly updated. If a requested embedding isn’t available, an empty embedding is returned while an asynchronous refresh is triggered. This setup enables experimentation with significantly larger models without latency constraints—a concept similar to what I described as “Just-in-time infrastructure” in my <a href="https://eugeneyan.com/speaking/recsys2022-keynote/" target="_blank">RecSys 2022 keynote</a>.</p>

<p><strong>Results:</strong> In A/B tests, UBS improved performance across six different ranking tasks while limiting the increase in cost. For example, a User Model with a sequence length of 1,000 showed a 0.38% improvement in online metrics compared to a baseline model using a sequence length of 20, with offline accuracy gains ranging from 0.01% to 0.40% across multiple tasks. Directly serving a large user sequence model would have increased costs by 28.7% but the UBS approach limited this increase to just 2.8%.</p>

<p>(👉 Recommended read) <strong><a href="https://arxiv.org/abs/2409.02856" target="_blank">Modern Ranking Platform (Zalando)</a> details their real-time platform designed for both search and browsing scenarios.</strong> The paper discusses their system design, candidate generation, retrieval methods, and ranking policies.</p>

<p><img src="https://eugeneyan.com/assets/zalando-fig2.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>Their platform is built around a few key principles:</p>

<ul>
  <li><strong>Composability:</strong> Models can be combined vertically (layered ranking) or horizontally by integrating outputs from various models or candidate generators.</li>
  <li><strong>Scalability:</strong> To manage computational costs, the platform first uses efficient but less precise candidate generators. These initial candidates are then refined by more accurate but computationally intensive rankers, a <a href="https://eugeneyan.com/writing/system-design-for-discovery/" target="_blank">standard design for recsys</a>.</li>
  <li><strong>Shared Infrastructure:</strong> Whenever possible, training datasets, embeddings, feature stores, and serving infrastructure are reused to simplify operations.</li>
  <li><strong>Steerable Ranking:</strong> The platform allows external adjustments through a policy layer, making it easy to align rankings with business objectives.</li>
</ul>

<p><img src="https://eugeneyan.com/assets/zalando-fig3.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>Their candidate generator uses a classic two-tower model. The customer tower updates embeddings based on a customer’s recent actions and current context whenever the customer visits the site, ensuring embeddings remain fresh. The item tower precomputes item embeddings and stores them in a vector database for rapid retrieval. These embeddings are matched via dot product. To create customer embeddings, a Transformer encoder is trained on historical customer behavior and contextual data, predicting the next likely interaction.</p>

<p><img src="https://eugeneyan.com/assets/zalando-fig4.jpg" loading="lazy" title="Modern Ranking Platform" alt="Modern Ranking Platform"></p>

<p>The ranker is a multi-task model that predicts the likelihood of different customer actions, such as clicks, adding items to wishlist or cart, and purchases. Each action has its own prediction head, with all contributing equally to training loss. During serving, each action type’s importance can be dynamically adjusted. Overall, the ranker outputs personalized scores for each candidate item across multiple potential customer interactions.</p>

<p>Finally, the policy layer ensures the system aligns with broader business goals. For instance, it can encourage exploration by promoting new products through heuristics like epsilon-greedy strategies. It also applies other business rules, such as reducing the visibility of previously purchased items and ensuring item diversity by preventing items from the same brand from appearing back-to-back.</p>

<p><strong>Results:</strong> The unified architecture demonstrated strong performance across four A/B tests, achieving a combined engagement increase of +15% and a revenue uplift of +2.2%. Iterative improvements further illustrate the effectiveness of each system component: introducing trainable embeddings in candidate generation boosted engagement by +4.48% and revenue by +0.18%; adding advanced ranking and policy layers delivered an additional +4.04% engagement and +0.86% revenue; and using contextual data provided a further lift of +2.40% in engagement and +0.60% in revenue.</p>

<p>• • •</p>

<p>Although early research in 2023—that applied LLMs to recommendations and search—often fell short, these recent efforts show more promise, especially since they’re backed by industry results. It suggests that there are tangible benefits from exploring the augmentation of recsys and search systems with LLMs, increasing performance while reducing cost and effort.</p>

<h2 id="references">References</h2>

<p>Chamberlain, Benjamin P., et al. “Tuning Word2vec for Large Scale Recommendation Systems.” <em>Fourteenth ACM Conference on Recommender Systems</em>, 2020, pp. 732–37. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3383313.3418486">https://doi.org/10.1145/3383313.3418486</a>.</p>

<p>Hidasi, Balázs, et al. <em>Session-Based Recommendations with Recurrent Neural Networks</em>. arXiv:1511.06939, arXiv, 29 Mar. 2016. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1511.06939">https://doi.org/10.48550/arXiv.1511.06939</a>.</p>

<p>Chen, Qiwei, et al. <em>Behavior Sequence Transformer for E-Commerce Recommendation in Alibaba</em>. arXiv:1905.06874, arXiv, 15 May 2019. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1905.06874">https://doi.org/10.48550/arXiv.1905.06874</a>.</p>

<p>Sun, Fei, et al. <em>BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</em>. arXiv:1904.06690, arXiv, 21 Aug. 2019. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.1904.06690">https://doi.org/10.48550/arXiv.1904.06690</a>.</p>

<p>Singh, Anima, et al. <em>Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations</em>. arXiv:2306.08121, arXiv, 30 May 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2306.08121">https://doi.org/10.48550/arXiv.2306.08121</a>.</p>

<p>Chen, Gaode, et al. “A Multi-Modal Modeling Framework for Cold-Start Short-Video Recommendation.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 391–400. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688098">https://doi.org/10.1145/3640457.3688098</a>.</p>

<p>Wang, Hangyu, et al. <em>FLIP: Fine-Grained Alignment between ID-Based Models and Pretrained Language Models for CTR Prediction</em>. arXiv:2310.19453, arXiv, 30 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2310.19453">https://doi.org/10.48550/arXiv.2310.19453</a>.</p>

<p>Vančura, Vojtěch, et al. “beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 1102–07. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3691707">https://doi.org/10.1145/3640457.3691707</a>.</p>

<p>Li, Yaoyiran, et al. <em>CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation</em>. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2405.02429">https://doi.org/10.48550/arXiv.2405.02429</a>.</p>

<p>Zhang, Chiyu, et al. <em>EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations</em>. arXiv:2405.11441, arXiv, 19 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2405.11441">https://doi.org/10.48550/arXiv.2405.11441</a>.</p>

<p>Shah, Jaidev, et al. “Analyzing User Preferences and Quality Improvement on Bing’s WebPage Recommendation Experience with Large Language Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 751–54. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688062">https://doi.org/10.1145/3640457.3688062</a>.</p>

<p>Pei, Yingchi, et al. “Leveraging LLM Generated Labels to Reduce Bad Matches in Job Recommendations.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 796–99. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688043">https://doi.org/10.1145/3640457.3688043</a>.</p>

<p><em>Search Query Understanding with LLMs: From Ideation to Production</em>. <a href="https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html">https://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html</a>. Accessed 5 Mar. 2025.</p>

<p>Lindstrom, Henrik, et al. “Encouraging Exploration in Spotify Search through Query Recommendations.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 775–77. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688035">https://doi.org/10.1145/3640457.3688035</a>.</p>

<p>Aluri, Geetha Sai, et al. “Playlist Search Reinvented: LLMs Behind the Curtain.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 813–15. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688047">https://doi.org/10.1145/3640457.3688047</a>.</p>

<p>Zhang, Gaowei, et al. <em>Scaling Law of Large Sequential Recommendation Models</em>. arXiv:2311.11351, arXiv, 19 Nov. 2023. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2311.11351">https://doi.org/10.48550/arXiv.2311.11351</a>.</p>

<p>Wang, Junting, et al. “A Pre-Trained Sequential Recommendation Framework: Popularity Dynamics for Zero-Shot Transfer.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 433–43. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3688145">https://doi.org/10.1145/3640457.3688145</a>.</p>

<p>Liu, Qi, et al. <em>Efficient Transfer Learning Framework for Cross-Domain Click-Through Rate Prediction</em>. arXiv:2408.16238, arXiv, 29 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.16238">https://doi.org/10.48550/arXiv.2408.16238</a>.</p>

<p>Khani, Nikhil, et al. <em>Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems</em>. arXiv:2408.14678, arXiv, 26 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.14678">https://doi.org/10.48550/arXiv.2408.14678</a>.</p>

<p>Zhang, Yin, et al. “Self-Auxiliary Distillation for Sample Efficient Learning in Google-Scale Recommenders.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 829–31. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688041">https://doi.org/10.1145/3640457.3688041</a>.</p>

<p>Cui, Yu, et al. “Distillation Matters: Empowering Sequential Recommenders to Match the Performance of Large Language Model.” <em>18th ACM Conference on Recommender Systems</em>, 2024, pp. 507–17. <em>arXiv.org</em>, <a href="https://doi.org/10.1145/3640457.3688118">https://doi.org/10.1145/3640457.3688118</a>.</p>

<p>Yang, Zhiming, et al. <em>MLoRA: Multi-Domain Low-Rank Adaptive Network for CTR Prediction</em>. arXiv:2408.08913, arXiv, 14 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.08913">https://doi.org/10.48550/arXiv.2408.08913</a>.</p>

<p>Hsu, Yi-Ping, et al. “Taming the One-Epoch Phenomenon in Online Recommendation System by Two-Stage Contrastive ID Pre-Training.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 838–40. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688053">https://doi.org/10.1145/3640457.3688053</a>.</p>

<p>Joshi, Swanand, et al. “Sliding Window Training - Utilizing Historical Recommender Systems Data for Foundation Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 835–37. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688051">https://doi.org/10.1145/3640457.3688051</a>.</p>

<p>Penha, Gustavo, et al. <em>Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other?</em> arXiv:2410.16823, arXiv, 22 Oct. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2410.16823">https://doi.org/10.48550/arXiv.2410.16823</a>.</p>

<p>Firooz, Hamed, et al. <em>360Brew: A Decoder-Only Foundation Model for Personalized Ranking and Recommendation</em>. arXiv:2501.16450, arXiv, 27 Jan. 2025. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2501.16450">https://doi.org/10.48550/arXiv.2501.16450</a>.</p>

<p>Bhattacharya, Moumita, et al. <em>Joint Modeling of Search and Recommendations Via an Unified Contextual Recommender (UniCoRn)</em>. arXiv:2408.10394, arXiv, 19 Aug. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2408.10394">https://doi.org/10.48550/arXiv.2408.10394</a>.</p>

<p>Jha, Rishikesh, et al. <em>Unified Embedding Based Personalized Retrieval in Etsy Search</em>. arXiv:2306.04833, arXiv, 25 Sept. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2306.04833">https://doi.org/10.48550/arXiv.2306.04833</a>.</p>

<p>Kekuda, Akshay, Yuyang Zhang, and Arun Udayashankar. “Embedding based retrieval for long tail search queries in ecommerce.” Proceedings of the 18th ACM Conference on Recommender Systems. 2024. <a href="https://dl.acm.org/doi/10.1145/3640457.3688039">https://dl.acm.org/doi/10.1145/3640457.3688039</a>.</p>

<p>Li, Yuening, et al. “Short-Form Video Needs Long-Term Interests: An Industrial Solution for Serving Large User Sequence Models.” <em>Proceedings of the 18th ACM Conference on Recommender Systems</em>, Association for Computing Machinery, 2024, pp. 832–34. <em>ACM Digital Library</em>, <a href="https://doi.org/10.1145/3640457.3688030">https://doi.org/10.1145/3640457.3688030</a>.</p>

<p>Celikik, Marjan, et al. <em>Building a Scalable, Effective, and Steerable Search and Ranking Platform</em>. 1, arXiv:2409.02856, arXiv, 4 Sept. 2024. <em>arXiv.org</em>, <a href="https://doi.org/10.48550/arXiv.2409.02856">https://doi.org/10.48550/arXiv.2409.02856</a>.</p>


            
            
<p>If you found this useful, please cite this write-up as:</p>

<blockquote>
    <p>Yan, Ziyou. (Mar 2025). Improving Recommendation Systems &amp; Search in the Age of LLMs. eugeneyan.com.
        https://eugeneyan.com/writing/recsys-llm/.</p>
</blockquote>

<p>or</p>

<div><pre><code>@article{yan2025recsys-llm,
  title   = {Improving Recommendation Systems &amp; Search in the Age of LLMs},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2025},
  month   = {Mar},
  url     = {https://eugeneyan.com/writing/recsys-llm/}
}</code></pre>
</div>

            
            
            



<p><span>Share on:  </span></p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Next.js version 15.2.3 has been released to address a security vulnerability (202 pts)]]></title>
            <link>https://nextjs.org/blog/cve-2025-29927</link>
            <guid>43448723</guid>
            <pubDate>Sat, 22 Mar 2025 21:19:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextjs.org/blog/cve-2025-29927">https://nextjs.org/blog/cve-2025-29927</a>, See on <a href="https://news.ycombinator.com/item?id=43448723">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Next.js version 15.2.3 has been released to address a security vulnerability (<a href="https://github.com/advisories/GHSA-f82v-jwr5-mffw" rel="noopener noreferrer nofollow" target="_blank">CVE-2025-29927<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a>). Additionally, backported patches are available.</p>
<p>We recommend that all self-hosted Next.js deployments using <code>next start</code> and <code>output: 'standalone'</code> should <a href="https://nextjs.org/docs/app/building-your-application/upgrading">update</a> immediately.</p>
<p>Continue reading for more details on the CVE.</p>
<h2 id="timeline" data-docs-heading=""><a href="#timeline">Timeline<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<ul>
<li><code>2025-02-27T06:03Z</code>: Disclosure to Next.js team via GitHub private vulnerability reporting</li>
<li><code>2025-03-14T17:13Z</code>: Next.js team started triaging the report</li>
<li><code>2025-03-14T19:08Z</code>: Patch pushed for Next.js 15.x</li>
<li><code>2025-03-14T19:26Z</code>: Patch pushed for Next.js 14.x</li>
<li><code>2025-03-17T22:44Z</code>: Next.js 14.2.25 released</li>
<li><code>2025-03-18T00:23Z</code>: Next.js 15.2.3 released</li>
<li><code>2025-03-18T18:03Z</code>: CVE-2025-29927 issued by GitHub</li>
<li><code>2025-03-21T10:17Z</code>: Security Advisory published</li>
<li><code>2025-03-22T21:21Z</code>: Next.js 13.5.9 released</li>
</ul>
<p>We are also publishing a backport for v12. We will update this post as they are released.</p>
<h2 id="vulnerability-details" data-docs-heading=""><a href="#vulnerability-details">Vulnerability details<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<p>Next.js uses an internal header <code>x-middleware-subrequest</code> to prevent recursive requests from triggering infinite loops. The security report showed it was possible to skip running Middleware, which could allow requests to skip critical checks—such as authorization cookie validation—before reaching routes.</p>
<h2 id="impact-scope" data-docs-heading=""><a href="#impact-scope">Impact scope<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<h3 id="affected" data-docs-heading=""><a href="#affected">Affected<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h3>
<ul>
<li>Self-hosted Next.js applications using Middleware (<code>next start</code> with output: <code>standalone</code>)</li>
<li>This affects you if you rely on Middleware for auth or security checks, which are not then validated later in your application.</li>
<li>Applications using Cloudflare can turn on a <a href="https://developers.cloudflare.com/changelog/2025-03-22-next-js-vulnerability-waf/" rel="noopener noreferrer nofollow" target="_blank">Managed WAF rule<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a></li>
</ul>
<h3 id="not-affected" data-docs-heading=""><a href="#not-affected">Not affected<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h3>
<ul>
<li>Applications hosted on Vercel</li>
<li>Applications hosted on Netlify</li>
<li>Applications deployed as static exports (Middleware not executed)</li>
</ul>
<h2 id="patched-versions" data-docs-heading=""><a href="#patched-versions">Patched versions<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<ul>
<li>For Next.js 15.x, this issue is fixed in <code>15.2.3</code></li>
<li>For Next.js 14.x, this issue is fixed in <code>14.2.25</code></li>
<li>For Next.js 13.x, this issue is fixed in <code>13.5.9</code></li>
</ul>
<p>If patching to a safe version is infeasible, it is recommended that you prevent external user requests which contain the <code>x-middleware-subrequest</code> header from reaching your Next.js application.</p>
<p>We are also publishing a backport for v12. We will update this post as they are released.</p>
<h2 id="our-security-responsibility" data-docs-heading=""><a href="#our-security-responsibility">Our security responsibility<span><svg viewBox="0 0 16 16" height="0.7em" width="0.7em">
  <g stroke-width="1.2" fill="none" stroke="currentColor">
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M8.995,7.005 L8.995,7.005c1.374,1.374,1.374,3.601,0,4.975l-1.99,1.99c-1.374,1.374-3.601,1.374-4.975,0l0,0c-1.374-1.374-1.374-3.601,0-4.975 l1.748-1.698"></path>
    <path fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-miterlimit="10" d="M7.005,8.995 L7.005,8.995c-1.374-1.374-1.374-3.601,0-4.975l1.99-1.99c1.374-1.374,3.601-1.374,4.975,0l0,0c1.374,1.374,1.374,3.601,0,4.975 l-1.748,1.698"></path>
  </g>
</svg></span></a></h2>
<p>Next.js has published <a href="https://github.com/vercel/next.js/security/advisories?state=published" rel="noopener noreferrer" target="_blank">16 security advisories<span><svg data-testid="geist-icon" height="16" stroke-linejoin="round" style="color:currentColor" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M6.75011 4H6.00011V5.5H6.75011H9.43945L5.46978 9.46967L4.93945 10L6.00011 11.0607L6.53044 10.5303L10.499 6.56182V9.25V10H11.999V9.25V5C11.999 4.44772 11.5512 4 10.999 4H6.75011Z" fill="currentColor"></path></svg></span></a> since 2016. Over time, we've continued to improve how we gather, patch, and disclose vulnerabilities.</p>
<p>GitHub Security Advisories and CVEs are industry-standard approaches to notifying users, vendors, and companies of vulnerabilities in software. While we have published a CVE, <strong>we missed the mark</strong> on partner communications.</p>
<p>To help us more proactively work with partners depending on Next.js, and other infrastructure providers, we are opening a partner mailing list. Please reach out to <a href="mailto:partners@nextjs.org"><code>partners@nextjs.org</code></a> to be included.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CEO of Kubient sentenced for fraud (164 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/</link>
            <guid>43448606</guid>
            <pubDate>Sat, 22 Mar 2025 21:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/">https://arstechnica.com/gadgets/2025/03/ceo-of-ai-ad-tech-firm-pledging-world-free-of-fraud-sentenced-for-fraud/</a>, See on <a href="https://news.ycombinator.com/item?id=43448606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                      
                      
          <p>In <a href="https://web.archive.org/web/20240511122402/https://kubient.com/">May 2024</a>, the website of ad-tech firm Kubient touted that the company was "a perfect blend" of ad veterans and developers, "committed to solving the growing problem of fraud" in digital ads. Like many corporate sites, it also linked old blog posts from its home page, including <a href="https://web.archive.org/web/20240315021936/https://kubient.com/ad-fraud/how-to-create-a-world-free-of-fraud-kubients-secret-sauce/">a May 2022 post</a> on "How to create a world free of fraud: Kubient's secret sauce."</p>
<p>These days, Kubient's website cannot be reached, the team is no more, and CEO Paul Roberts is <a href="https://www.justice.gov/usao-sdny/pr/former-ceo-kubient-inc-sentenced-prison-connection-accounting-fraud-scheme">due to serve one year and one day in prison</a>, having pled guilty Thursday to creating his own small world of fraud. Roberts, according to federal prosecutors, schemed to create $1.3 million in fraudulent revenue statements to bolster Kubient's initial public offering (IPO) and significantly oversold "KAI," Kubient's artificial intelligence tool.</p>
<p>The core of the case is an I-pay-you, you-pay-me gambit that Roberts initiated with an unnamed "Company-1," according to prosecutors. Kubient and this firm would each bill the other for nearly identical amounts, with Kubient purportedly deploying KAI to find instances of ad fraud in the other company's ad spend.</p>
<p>Roberts, prosecutors said, "directed Kubient employees to generate fake KAI reports based on made-up metrics and no underlying data at all." These fake reports helped sell the story to independent auditors and book the synthetic revenue in financial statements, according to Roberts' indictment.</p>

          
                      
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Quitting an Intel x86 Hypervisor (106 pts)]]></title>
            <link>https://halobates.de/blog/p/446</link>
            <guid>43448457</guid>
            <pubDate>Sat, 22 Mar 2025 20:42:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://halobates.de/blog/p/446">https://halobates.de/blog/p/446</a>, See on <a href="https://news.ycombinator.com/item?id=43448457">Hacker News</a></p>
Couldn't get https://halobates.de/blog/p/446: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA["Vibe Coding" vs. Reality (210 pts)]]></title>
            <link>https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</link>
            <guid>43448432</guid>
            <pubDate>Sat, 22 Mar 2025 20:35:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html">https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.html</a>, See on <a href="https://news.ycombinator.com/item?id=43448432">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body"><article><time datetime="2025-03-19T15:00:00.000Z" title="3/19/2025, 10:00:00 AM">Published Mar 19, 2025</time> - 11 min read - <a href="https://cendyne.dev/posts/2025-03-19-vibe-coding-vs-reality.txt">Text Only</a><div id="table-of-contents-container"><p>Table of contents</p><ul><li><a href="#title" data-id="title">"Vibe Coding" vs Reality</a></li><li><a href="#working-around-the-problem" data-id="working-around-the-problem">Working around the problem</a></li><li><a href="#conclusion" data-id="conclusion">Conclusion</a></li></ul></div><p>There's a trend on social media where many repeat <a href="https://x.com/karpathy/status/1886192184808149383">Andrej Karpathy's words</a> (<a href="https://archive.is/yNSTA">archived</a>): "give in to the vibes, embrace exponentials, and forget that the code even exists." This belief — like many flawed takes humanity holds — comes from laziness, inexperience, and self-deluding imagination. It is called "Vibe Coding."</p><div><p><img width="128" height="128" alt="head-empty" src="https://cendyne.dev/s/128/cendyne/head-empty" sizes="128px"></p><div><p>"Embrace the exponentials" sounds like it came from an NFT junkie.</p></div></div><div><p><img width="128" height="128" alt="shinji-cup" src="https://cendyne.dev/s/128/cendyne/shinji-cup" sizes="128px"></p><div><p>Like the NFT crowd, there is a bubble of unreality they cling to justifying their perception of the world.</p></div></div><p>Producing software is now more accessible as newer tools allow people to describe what they want in a natural language to a large language model (LLM). This idea is catching on because LLM agents are now accessible to anyone willing to subscribe to vendors like <a href="https://www.cursor.com/en">Cursor</a>, <a href="https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode">GitHub</a>, Windsurf, and others. These editors have an "agent" option where users can request something and in response changes are made to the appropriate files, rather than only the file currently in focus. Over time, the agent will request to run commands to run tests or even run scripts it previously wrote to the file system, much as you would if you were solving the problem.</p><p>In 2022, folks could copy code into <a href="https://en.wikipedia.org/wiki/ChatGPT">ChatGPT</a> and ask questions or for rewrites.</p><p>In 2023, folks could ask it to review and edit a single file with an IDE integration like Copilot.</p><p>In 2024 and 2025, folks could ask it to solve a specific problem in the project and have it find out what files to edit, edit them, then verify its own work, and correct any mistakes it made with feedback from linting errors and unit tests.</p><p>With LLM agents having so much capability, people can delegate the idea of refining their imprecise ideas to a precise implementation elaborated by an LLM through "Vibe Coding."</p><div><p><a href="https://twitter.com/a16z">@a16z</a> <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a> First - what is vibe coding?</p><p>A concise definition from <a href="https://twitter.com/stuffyokodraws">@stuffyokodraws</a>, and then an exploration of how technical vs. non-technical users approach these tools.</p></div><p>If you open a blank folder and tell it to set up an initial project, it can do a lot at once. With no rules, no patterns to mimic, and no constraints, it can produce something that feels more tailored for you in minutes than <code>npx create-react-app</code> ever could.</p><p>With a simple instruction like "I want to create a website for my ski resort" and about ten minutes of having it massage errors of its own making, I can have just that.</p><p><img data-blurhash="MSQc#U~WxtIW%LM|t6t7WCt7^*9Zt7%LR*" data-width="645" data-height="423" data-ratio="true" src="https://cendyne.dev/c/XH7rgh3H?width=645" alt="A generated website about a ski resort with a phrase like 'Easy to Reach, Hard to Leave'" width="645" height="423"></p><p>These leaps of progress are what fuels the "Vibe Coding" idea. To go from nothing to something shareable and personal sounds incredible.</p><div><p><img width="128" height="128" alt="beat-saber" src="https://cendyne.dev/s/128/cendyne/beat-saber" sizes="128px"></p><div><p>This moment provided a thrill I hadn't experienced in a long time when coding. However, this excitement drained quickly the further I got from a blank canvas.</p></div></div><p>Agents, as a concept, aren’t new. <a href="https://www.youtube.com/watch?v=ijeXop674Dg">Google IO made up buzzwords</a> like <a href="https://www.theverge.com/2024/12/11/24317436/google-deepmind-project-astra-mariner-ai-agent">"agentic era"</a> (<a href="https://archive.is/dw8XE">archived</a>) to describe this concept. It has been realized through open technologies like <a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>, <a href="https://github.com/OpenBMB/XAgent">XAgent</a>, and <a href="https://www.anthropic.com/news/model-context-protocol">more recently by Anthropic</a> with the <a href="https://modelcontextprotocol.io/introduction">Model Context Protocol</a> (MCP).</p><p>When the model can interact with more than just a person who proxies their outputs into different domains, it is autonomous. If it can perform searches on the web or in a codebase, it can enrich its own context with the information it needs to fulfill the current request. Further, when it can commit outputs and then gain immediate and automatic feedback on those outputs, it can refine its solution without a person intervening.</p><p>There are actions that do prompt the user for consent before proceeding, such as running commands in the console or deleting files. This consent can be pre approved with a mode called "YOLO."</p><p><img data-blurhash="E04ec*~qofxuoft7D%Rj?b-;xut7" data-width="645" data-height="149" data-ratio="true" src="https://cendyne.dev/c/E8IJPCCd?width=645" alt="Cursor settings YOLO mode, allows running commands automatically" width="645" height="149"></p><div><p><img width="128" height="127" alt="we-live-in-a-society" src="https://cendyne.dev/s/128/cendyne/we-live-in-a-society" sizes="128px"></p><div><p>A mode for "You Only Live Once"!? Really?</p></div></div><p>You can witness this autonomy for yourself today in Cursor.</p><p>The agent concept has merit and today can deliver proofs of concept that <a href="https://youtu.be/IACHfKmZMr8">VC firms like Y-Combinator</a> will invest in — proofs of concept that are trash by unskilled founders hoping to win the lottery while living the life of leisure.</p><div><p>I’ve cracked vibe coding, TrendFeed has almost hit its first 10k month, and Ai built the entire thing</p><p>Im just sitting here sipping coffee, coding with Ai + MCP</p><p>Also more time to shitpost on X haha</p></div><div><p><img width="128" height="128" alt="cheers" src="https://cendyne.dev/s/128/cendyne/cheers" sizes="128px" loading="lazy"></p><div><p>The optimal technical founder for a VC is not the 10x engineer. It is someone who'll deliver <em>enough</em> of a product to test its fitness in the market and then succeed in raising more investment money. Their execution on their vision and hiring prowess is more important than their technical skillset.</p></div></div><p>The execution of agents today is over-hyped and does not hold up to the needs of any functioning businesses which need experts to develop and maintain their technical capabilities instead of single points of failure on the internet.</p><div><p>babe, come to bed</p><p>i can't, i'm vibe coding</p></div><p>These models are trained on average sloppy code, wrong answers on Stack Overflow, and the junk that ends up on Quora. Despite the power and capability Claude 3.7 Sonnet has in small contexts, when faced with even a small codebase it makes constant silly mistakes that no normal developer would repeat and continue to repeat every hour of its operation.</p><div><p><span></span><span>Specific details on the mistakes, feel free to skip</span><span></span></p><div><ul><li>Regularly clones TypeScript interfaces instead of exporting the original and importing it.</li><li>Reinvents components all the time with the same structure without searching the code base for an existing copy of that component.</li><li>Writes trusted server side logic on the client side, using RPC calls to update the database.</li><li>As a feature develops, it prioritizes maintaining previous mistakes instead of re-evaluating its design, even when told to do so. You have to say the previous implementation is outright unusable for it to replace its design.</li><li>Cursor has some sort of <a href="https://www.reddit.com/r/ClaudeAI/comments/1i8n3wq/does_claude_have_stupid_mode_enabled_tonight/">"concise mode"</a> (<a href="https://archive.is/iU8gx">archived</a>) that they'll turn on when there is high load where the model will still be rated at the normal price but behaves in a useless manner. This mode will omit details, drop important findings, and corrupt the output that is being produced.</li><li>Cannot be trusted to produce unit tests with decent coverage.</li><li>Will often break the project's code to fit a unit test rather than fix the unit test when told to do so.</li><li>When told to fix styles with precise details, it will alter the wrong component entirely.</li><li>When told specifically where there are many duplicated components and instructed to refactor, will only refactor the first instance of that component in the file instead of all instances in all files.</li><li>When told to refactor code, fails to search for the breaks it caused even when told to do so.</li><li>Will merrily produce files over 1000 lines which exceed its context window over time, even when told to refactor early on.</li><li>Will regularly erase entire route handlers if not bound to the file hierarchy.</li></ul></div></div><p>As currently designed, these models cannot learn new information. They cannot do better than the dataset they were created with. Instead their capability is realized by how effective they can process tokens entering their context window.</p><p>If you ask Claude 3.7 Sonnet to develop a runtime schema for validating some domain specific language and then ask it to refactor the file — because it is too large for its context window to continue — it will degrade and output incoherent nonsense before finishing its work.</p><p><img data-blurhash="E042PB?bM{kCWBof%M%MRjRjtRay" data-width="645" data-height="244" data-ratio="true" src="https://cendyne.dev/c/l6YxDTlA?width=645" alt="Now that we've created all the schemado that: ... I'v schema files for each schema schemaschema schemaactored code?" width="645" height="244" loading="lazy"></p><div><p><img width="128" height="110" alt="wat" src="https://cendyne.dev/s/128/cendyne/wat" sizes="128px" loading="lazy"></p><div><p>It did not type "I've" correctly and conjoined the words "schema" and "refactored" into one.</p></div></div><div><p>my saas was built with Cursor, zero hand written code</p><p>AI is no longer just an assistant, it’s also the builder</p><p>Now, you can continue to whine about it or start building.</p><p>P.S. Yes, people pay for it</p></div><p>You cannot ask these tools today to develop a performant React application. You cannot ask these tools to implement a secure user registration flow. It will choose to execute functions like is user registered on the client instead of the server.</p><div><p><img width="128" height="128" alt="trash" src="https://cendyne.dev/s/128/cendyne/trash" sizes="128px" loading="lazy"></p><div><p>Others are learning this the hard way too.</p></div></div><div><p>guys, i'm under attack</p><p>ever since I started to share how I built my SaaS using Cursor</p><p>random thing are happening, maxed out usage on api keys, people bypassing the subscription, creating random shit on db</p><p>as you know, I'm not technical so this is taking me longer that usual to figure out</p><p>for now, I will stop sharing what I do publicly on X</p><p>there are just some weird ppl out there</p></div><p>Without expert intervention, the best these tools can do today is produce a somewhat functional mockup, where every future change beyond that risks destroying existing functionality.</p><p>I cannot — and would not — trust a team member who vibe codes in a production application. The constant negligence I observe when "Vibe Coding" is atrocious and unacceptable to a customer base of any size.</p><p>No available model demonstrates consistent and necessary attention to detail needed for a production environment. They are not yet equipped or designed to transform information involving multiple contexts inherent to producing a digital product.</p><p>These tools are optimized to produce solutions that fit in a single screen of markdown and are now being asked to do far more than they were trained for. As the context window overflows and the model degrades, it will fail to even format MCP calls correctly and upon reaching this point of no return, produces a log that comes across as being tortured. Like a robot losing a limb, it will try and try again to walk only to fall down until the editor pauses the conversation to save on resources.</p><p><img data-blurhash="L03+Dt~q%2ofM{WURjWBx]t7WUj[" data-width="645" data-height="628" data-ratio="true" src="https://cendyne.dev/c/xNtV9Ji3?width=645" alt="Let me try a different approach. Error calling tool." width="645" height="628" loading="lazy"></p><h2 id="working-around-the-problem">Working around the problem</h2><p>A modern <a href="https://en.wikipedia.org/wiki/Twitch_Plays_Pok%C3%A9mon">"Twitch plays Pokémon"</a> is going on right now: <a href="https://www.twitch.tv/claudeplayspokemon">Claude Plays Pokémon</a>. It mitigates this context window problem by starting a new context with seeded information provided by its previous incarnation in the form of many Markdown files, which it can then read as if new and search via MCP during its playthrough.</p><div><div><p>So, what makes this possible? Claude was given a knowledge base to store notes, vision to see the screen, and function calls which allow it to simulate button presses and navigate the game.</p><p>Together, they allow Claude to sustain gameplay with tens of thousands of interactions.</p></div><p><img data-blurhash="L04epF.7M-a1.6t6WDofI2VtxstR" data-width="645" data-height="645" data-ratio="true" src="https://cendyne.dev/c/a9W5MgiW?width=645" alt="Photo included with tweet" width="645" height="645" loading="lazy"></p></div><p>Even so, it can make bad assumptions and spend 43 hours intentionally blacking out over and over in Mt. Moon (an in-game route between story locations) making no effective progress towards achieving its next goal because by the time it could second guess itself, its context window is no longer fit to continue.</p><p><video poster="https://cendyne.dev/c/-PEsNE-L" preload="metadata" playsinline="" controls="" autoplay="" loop="" muted="" width="644" height="362"><source src="https://cendyne.dev/c-no-index/3RrkLRXm" type="video/mp4"><img data-blurhash="L98zorE19a~C^+IoIo-p9t%2xaE1" data-width="644" data-height="362" data-ratio="true" src="https://cendyne.dev/c/-PEsNE-L?width=645" alt="Claude plays pokemon going through a context clean up and restart" width="644" height="362" loading="lazy"></video></p><div><p><img width="128" height="128" alt="galaxy-brain2" src="https://cendyne.dev/s/128/cendyne/galaxy-brain2" sizes="128px" loading="lazy"></p><div><p>It did escape and progress, but only after the critic instance of the model suggested its assumption was incorrect.</p></div></div><p>After a context cleanup completes, which takes about five minutes (the video above is edited to the meaningful moments), the model proceeds to make the same mistakes its prior incarnation did. The notes it wrote are not meaningfully interpreted in context, I find the same happens too with the Cursor rules I write.</p><p>While increasing the length of the context window will improve some immediate experiences, this is a problem of scale that needs a different solution for agents to be more effective and, perhaps, move "Vibe Coding" closer to reality.</p><div><p><img width="107" height="128" alt="thinker" src="https://cendyne.dev/s/128/cendyne/thinker" sizes="128px" loading="lazy"></p><div><p>Would a formalized <a href="https://bulletjournal.com/">bullet journal</a> over MCP help a model be more complete in delivering more reliable results?</p></div></div><div><div><p>As long as the model correctly checks it before concluding its work is complete!</p></div><p><img width="128" height="123" alt="point-left" src="https://cendyne.dev/s/128/jacobi/point-left" sizes="128px" loading="lazy"></p></div><p><img data-blurhash="K5SF;MIV~q?vt7%Mxuxuj[" data-width="645" data-height="669" data-ratio="true" src="https://cendyne.dev/c/FD-gkvrg?width=645" alt="Bullet journal with ski examples" width="645" height="669" loading="lazy"></p><p>A bullet journal may be one of many tools that improve the reliability of the models we have today.</p><p>The next issue is that these models cannot ingest information from multiple concurrent real-time sources. In one terminal we may be running the server and in another some end-to-end tests. Both of these terminals were created at the agent's request. It either ignores or is not fed the stack trace logged by the server in the first terminal as it watches the output of the end-to-end tests fail and retry, fail and retry.</p><p>For agents to have the impact promised by the hype, LLMs need a robust mechanism to mimic the development of short and long term memory without fine-tuning the memories into the model.</p><p>Furthermore, for agents to contribute to a team, there must be a way to develop long-term memories bound to the organization and its products that seamlessly merge with and reconcile with memories personal to each team member.</p><p>And lastly, these memories have to be portable. As models improve and are integrated into our tools, domain specific memories must be usable by the next generation of large language models.</p><h2 id="conclusion">Conclusion</h2><p>"Vibe Coding" might get you 80% the way to a functioning concept. But to produce something reliable, secure, and worth spending money on, you’ll need experienced humans to do the hard work not possible with today’s models.</p><p>Agents do demonstrate enough capability that LinkedIn CEO influencers confidently spread the unreality that we can replace jobs with "agentic AI."</p><p>Agents do enable skilled people to create more independently than they ever have. For the time being, it will not replace those that can solve the hard problems that only experience and intuition can identify. Like other no-code solutions, agents do give the less skilled more capability than they had the day before. Until they develop their own competent skill set, "Vibe Coders" will not be able to release production quality software in this world, no matter how <em>exponential</em> the agent is over their own inferior skill set.</p><p>Keep an eye on how LLM agents develop and improve. For now, they are worth evaluating and discussing, but are not ready for us to delegate the precise task of creating reliable, secure, and scalable software that powers our society. "Vibe Coding" will not create the next big thing in 2025.</p></article></div></div>]]></description>
        </item>
    </channel>
</rss>