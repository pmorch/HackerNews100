<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 12 Sep 2024 05:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[My business card runs Linux (and Ultrix), yours can too (171 pts)]]></title>
            <link>http://dmitry.gr/?r=05.Projects&amp;proj=33.%20LinuxCard</link>
            <guid>41516476</guid>
            <pubDate>Thu, 12 Sep 2024 00:21:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://dmitry.gr/?r=05.Projects&#x26;proj=33.%20LinuxCard">http://dmitry.gr/?r=05.Projects&#x26;proj=33.%20LinuxCard</a>, See on <a href="https://news.ycombinator.com/item?id=41516476">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<p><span>My business card runs Linux (and Ultrix), yours can too</span>

<b>UPDATES:</b>: See "<a href="#fwv2">Version 2</a>"</p><h2>Table of Contents</h2>
<p><img src="http://dmitry.gr/images/linuxCardPromo.jpg" alt="Linux card project cover image"></p><ol type="1"><li><a href="#_TOC_2673ce8234cc65bf5d5e81743172700b">Why?</a></li><li><a href="#_TOC_544a3f85f27429ab93dcd5ab2a63de0f">Parts selection</a></li><li><a href="#_TOC_7c1edc4d9290405fda54122244529141">What to emulate</a><ol type="a"><li><a href="#_TOC_d30f50757568b8cfaf8978a26d616b30">A MIPS primer</a></li><li><a href="#_TOC_1f0bd75c3792469488787400675c6973">What system?</a></li></ol></li><li><a href="#_TOC_7e76ca59b9efa6e1895f99d810e88da7">Let's emulate!</a><ol type="a"><li><a href="#_TOC_2e782d97f878671469dbb579dab9819e">The CPU</a></li><li><a href="#_TOC_d26852db28343b84596911de435caac9">The FPU</a></li><li><a href="#_TOC_5568f04140f1e9c2bde99c4e9c3572c9">The MMU</a><ol type="I"><li><a href="#_TOC_f05af7a1cabb44e206600aa7d7074fbc">MMU basics</a></li><li><a href="#_TOC_68fe0e8a8517a9a939dc8ad830f3ed3f">The MIPS MMU</a></li><li><a href="#_TOC_3ec65a91b377c508e2a27543e95858ca">Emulating the MMU efficiently</a></li></ol></li><li><a href="#_TOC_07a3dd247ad6bec7fe401aa11619959f">Communication</a></li><li><a href="#_TOC_3a173782d010897c6136c3b4ca51cd47">Hypercalls</a></li></ol></li><li><a href="#_TOC_d2ce354603c45f5e016fd47b9b5c8524">Bring on the hardware!</a><ol type="a"><li><a href="#_TOC_30aace8835696930f69a264dd89cbc60">The honeymoon period</a></li><li><a href="#_TOC_da93b232e827178270bfa561bdf6569c">How not to design a DMA unit</a></li><li><a href="#_TOC_e8ee65a238a03dd3a641faaff13a9c8a">Clocks again</a></li><li><a href="#_TOC_f53cca91f548c50486dcd52bac3efe7c">SD card support</a></li><li><a href="#_TOC_03751c187a665141b52e489852323619">Coolness enhancement</a></li></ol></li><li><a href="#_TOC_9ffdb95250e26c7a6b468126ac7c75b0">How it works</a><ol type="a"><li><a href="#_TOC_ddd6939c0f0cff6ceb8b36172e02b459">How a normal DECstation boots</a></li><li><a href="#_TOC_f9ff15b5ee1683a37691730e23cbd714">How uMIPS boots</a></li><li><a href="#_TOC_55a0a5396aa1bd6b11c6eb5f8ce8d62a">How uMIPS runs</a></li><li><a href="#_TOC_d70d07edc2b683000e09c709f5921fe9">Linux changes</a></li></ol></li><li><a href="#_TOC_9a98bd63857ec302fcec693aae109dde">Improving performance</a><ol type="a"><li><a href="#_TOC_0f63c0f6caf6daba30629abfa403d51a">Instruction cache</a></li><li><a href="#_TOC_fc5278bbec5501f7f5fe590fb59653f2">Improving CPU speed</a></li><li><a href="#_TOC_aee3d0e166f8b8358baf265cb949a756">Improving RAM bandwidth</a></li><li><a href="#_TOC_73dc18b03073c31c5a2bbf1264aff327">Dirty hacks specifically for Linux</a></li></ol></li><li><a href="#_TOC_23b5439c727c2e8557d28f8495da9977">How to build and use one</a><ol type="a"><li><a href="#_TOC_c39b56d4489fb2507289e7ae19567b80">Building</a></li><li><a href="#_TOC_b3abf203d70a08b6d9725f0000f27122">Building from source</a></li><li><a href="#_TOC_7b797cff6f4cf0ca82225d6125fe1861">If you are lazy</a></li><li><a href="#_TOC_3f05d6f38862a5b18b2eb4e867a61fb1">Using</a></li></ol></li><li><a href="#_TOC_425d8e1e777a03c3e220dfaac38dbf1f">Version 2</a><ol type="a"><li><a href="#_TOC_2b50d6628ed817de809605854d478f68">Booting Ultrix</a><ol type="I"><li><a href="#_TOC_2375a25fb86a26c24006ed1d6e2c1c47">About Ultrix</a></li><li><a href="#_TOC_c7bce03d32236f53d1a3d5c04e680838">First time booting Ultrix</a></li><li><a href="#_TOC_87e05c1a936fe8f08f1621e5cd10c534">SCSI</a></li><li><a href="#_TOC_938bce276b64c8ccdaeb079ff7a0bd84">LANCE</a></li><li><a href="#_TOC_2b0d0fb2ee0246234fe0fd9845b87021">ESAR</a></li><li><a href="#_TOC_73d22765ace36f09f4bab935a1608d3c">Memory probing &amp; proper PROM API</a></li><li><a href="#_TOC_c6323dca5b2dcaf05756b569ad13f4b9">Ultrix Loader</a></li></ol></li><li><a href="#_TOC_fd2d1e73f2cd71a4c375685b4abed537">Making Ultrix work</a><ol type="I"><li><a href="#_TOC_42badd9e49002a3cefeaaf28867add83">Framebuffer</a></li><li><a href="#_TOC_347b8386b145ced18ea02f2654e00883">Mouse, Keyboard, ... and Tablet</a></li><li><a href="#_TOC_c4d283323af70979073f5cb6145f3a4b">Patches</a></li></ol></li><li><a href="#_TOC_9f744f0f0ac29e818b7b95da8ad8ee40">Improvements in the emulator</a><ol type="I"><li><a href="#_TOC_7c775bb4800b345425796dce2acef3a8">USB improvements</a></li><li><a href="#_TOC_c5f0dc32adc4e66649115eef7e76f94f">More perf improvements</a></li><li><a href="#_TOC_819a79dcd016b5a99e05c56515825abb">Removing the TLB refill fast path</a></li><li><a href="#_TOC_dda4cbe5c1a77c2c9bf66b0adc6fff4b">Cache geometry changes</a></li><li><a href="#_TOC_b3c8fd852368b78e09e2fc8b39a6ea5c">Serial improvements</a></li></ol></li><li><a href="#_TOC_cb1862398dbf0b5f7d8a7dbe73bee626">More Floating Point Unit work</a></li><li><a href="#_TOC_244cd79fc0c2e4a295228e647cf88dbc">A bootloader</a></li><li><a href="#_TOC_7fa430e699655fa918b84f63d1c5fae5">Hardware improvements</a><ol type="I"><li><a href="#_TOC_9183d42e96c2104c27070b535e3793d9">v1.3 hardware</a></li><li><a href="#_TOC_8971392389323f2057db749c94dcba05">And old hardware too</a></li></ol></li><li><a href="#_TOC_1a574cfae32687e675d081616b06e0a0">Building from source (updated)</a><ol type="I"><li><a href="#_TOC_3d2780b0eeabee178926de36f72658b8">The emulator</a></li><li><a href="#_TOC_7213a2cd3fa80d577b3092f614b7fdb3">The loader</a></li></ol></li><li><a href="#_TOC_d8926db6c437f738792454ba22ec2755">Further Updates</a><ol type="I"><li><a href="#_TOC_52cbfe3bfc8a13d03124dd2283d83807">Firmware v2.1.1</a></li><li><a href="#_TOC_6b3ae1897005585fd36708ff39e693cf">Firmware v2.2.0</a></li></ol></li></ol></li><li><a href="#_TOC_3cd960e7edc378fd94d8777b595ea515">In conclusion</a><ol type="a"><li><a href="#_TOC_0407c27180c9b019e644e8ad4c6a9324">Acknowledgements</a></li><li><a href="#_TOC_c20c35ef53bf1b70789ce94e66800147">Downloads</a></li></ol></li><li><a href="#_TOC_7e1e75c32bc9b275daf70df8cba8efb5">Comments...</a></li></ol>







<h2>Why?</h2>
<p><a href="http://dmitry.gr/images/linuxCardWhole.jpg"><img src="http://dmitry.gr/images/linuxCardWholeSmall.jpg" alt="Linux card in action"></a></p><p>A long long time ago (in 2012) I <a href="http://dmitry.gr/?r=05.Projects&amp;proj=07.%20Linux%20on%208bit">ran Linux on an 8-bit AVR</a>. It was kind of a cool record at the time. I do not think anyone has beaten it - nobody's managed to run Linux on a lower-end device than that 8-bit AVR. The main problem was that is was too slow to be practical. The effective speed was 10KHz, the boot time was 6 hours. Cool, but I doubt that any one of those people who built one of those devices based on my design ever waited for the device to boot more than once. It was time to improve it!
</p>
<p>So what could I improve? A number of things. First, I wanted the new design to be speedy enough to boot in a few minutes and reply to commands in seconds. This would make using the device practical and not a test of patience. Second, I wanted it to be easy to assemble for anyone. This meant no components with tight spacing, no components with too many pins, and no components with contacts hidden underneath them. A part of this wish was also that someone could <em>actually</em> assemble one, meaning that I had to select components that are <em>actually</em> buyable in the middle of the current ongoing shortage of, well, everything. Additionally, I wanted the device to be easy to interface with. The original project required a USB-to-serial adapter. This would not do. And, finally, I wanted the whole thing to be cheap and compact enough to serve as my business card.
</p>

<h2>Parts selection</h2>
<p><a href="http://dmitry.gr/images/linuxCardSchem.png"><img src="http://dmitry.gr/images/linuxCardSchemSmall.jpg" alt="Linux card schematics"></a></p><p>Some things were pretty easy to decide on. For storage, for example, microSD is perfect - easy to interface with, widely available, cheap. I picked a simple microSD slot that is easy to solder and easy to buy: <a href="https://octopart.com/1140084168-amphenol-25513977?r=sp">Amphenol 1140084168</a>.
</p>
<p>Some choices were a litle harder, but not too much so. For example, I was surely not going to use DRAM again. It requires too many pins, necessitating more soldering than I would consider acceptable, given that I wanted this device to be easy to assemble. SRAM in megabyte sizes does not really exist. But there is a cool thing called PSRAM. It is basically DRAM, but in easy mode. It itself takes care of all the refreshing and externally acts just like SRAM. Ok, cool, but still that would usually be a lot of pins. Right? Enter "AP Memory" and "ISSI". They make QSPI PSRAM chips in nice SOIC-8 packages. AP Memory has models with <a href="https://octopart.com/search?q=APS1604M-3&amp;currency=USD&amp;specs=0">2MB</a> and <a href="https://octopart.com/search?q=APS6404L-3&amp;currency=USD&amp;specs=0">8MB</a> of RAM per chip, ISSI has them in <a href="https://octopart.com/search?q=IS66WVS1M8BLL&amp;currency=USD&amp;specs=0">1MB</a>, <a href="https://octopart.com/search?q=IS66WVS2M8BLL&amp;currency=USD&amp;specs=0">2MB</a>, and <a href="https://octopart.com/search?q=IS66WVS4M8BLL&amp;currency=USD&amp;specs=0">4MB</a> sizes. I decided to use these. They are available and my code supports them all!
</p>
<p>There were some miscellaneous choices, like which regulator to use. I chose <a href="https://octopart.com/search?q=MIC5317-3.3YM5TR&amp;currency=USD&amp;specs=0">MIC5317-3.3YM5TR</a> due to having worked with it before and it being available in my "random chips" box. It is also easily available to buy.
</p>
<p>The USB connector was also a fun choice. I settled on: none. With the proper PCB thickness, one can lay out the board edge to fit into the end of a USB-C cable. I've seen this done before for micro-USB and figured it could be done for USB-C as well. At the end, though, I did not even need to do it, since <a href="https://github.com/Pinuct/Eagle_PCB_USB_connectors">someone else</a> already saved me the 30 minutes it would have taken. I just had to remember that the board thickness needs to be 0.8mm for this to work. 
</p>
<p>The last choice was the hardest - which microcontroller to use. The criteria were: built-in USB, no more than 32 pins with at least 0.65mm spacing, no pin-less packages, actually available to buy, QSPI support, as fast as possible. I did not get my last two wishes. After much searching and filtering for "in stock", I was forced to settle for an ATSAMD21 series chip, specifically the <a href="https://octopart.com/search?q=ATSAMDA1E16b-a&amp;currency=USD&amp;specs=0">ATSAMDA1E16</a>. It is not fast (specced to 48MHz, I clock it at 90MHz), it has many bugs (especially in its DMA engine), but it can be bought, it is easy to solder, and it'll have to do... <b>UPDATE</b>: another chip is now supported too, see later in this article.
</p>

<h2>What to emulate</h2>
<p><a href="http://dmitry.gr/images/linuxCardWholeBoot.png"><img src="http://dmitry.gr/images/linuxCardWholeBootSmall.png" alt="Linux card boot log"></a></p><p>I could have just taken my old ARM emulator (uARM) and used that. But what's the fun there? I decided to pick a new target. The ideal emulation target will: (1) be a RISC chip so that I have to spend fewer cycles on decoding instructions, (2) have no condition codes (like MIPS) or only set them on demand (like ARM), so that I am not wasting time calculating them every virtual cycle, (3) be 32-bit since 16-bit machines are all funky and 64 bit is a pain to emulate, (4) be known, and (5) have a workable set of GNU tools and Linux userspaces available. This set of requirements actually only leaves a few candidates: PowerPC, ARM, MIPS. I've done ARM, and I had no desire to mess with an endian-switchable CPU, so MIPS it was! This gives rise to the internal name of the project: uMIPS.
</p>
<h3>A MIPS primer</h3>
<p>MIPS is old - one of the original RISC designs. If you are a RISC-V fanboy(/girl/being), MIPS will look familiar - it is where 99.9994% of the initial RISC-V spec was copied from. The original MIPS was a 32-bit design, optimized for ease-of-designing-it. It has (and does not hide) a delay slot, has a lot of registers, including a hard-wired zero register, and does not use condition codes. The original design was R2000, back in 1986, followed soon by the improved R3000 in 1988. These were the last chips implementing the MIPS-I instruction set. MIPS-II was short lived and only included the R6000, which barely saw the light of day. The real successors were the MIPS-III R4000-series chips, released in 1991. These were 64-bit already in 1991! Clearly, the easiest target would be the R2000/R3000 chips with their simple MIPS-I instruction set.
</p>
<p>MIPS-I is a <a href="https://vhouten.home.xs4all.nl/mipsel/r3000-isa.html">rather simple instruction set</a>. So much so that a complete emulator of just the instructions can be written in under 1000 lines of C code without any dirty tricks. The floating point unit is optional, so it can be skipped (for now). The MMU is weird. It is just a TLB that the software must fill manually. This may seem like a rather unusual choice, but in reality it is a clever one, if you're in 1986 and tring to minimize the number of transistors in your chip. Why have a hardware pagetable walker, when you can make the software do it? You may ask how it handles the situation where the code that would do the walking is itself not mapped? Well, a part of physical memory is always hard-mapped at a certain address, and all exception handlers live there. Even if this were not the case, since the software manages the TLB, it would not be hard to reserve an entry for this purpose. The hardware even has support for some "wired" entries that are meant to be permanent. More on all of this later.
</p>
<h3>What system?</h3>
<p>MIPS R2000/R3000 is a processor. A processor does not a complete system make. What system to emulate? I searched around for a cool system and settled on DECstation2100 (or its big brother - DECstation3100). Why bother? It seemed like a simple system that Linux does support. Initially I was not planning to emulate the whole thing. Why? I had no plans to emulate the LANCE network adapter or the SII SCSI adapter. The last part might surprise you, since we will need a disk to use as our root fs. I did later add emulation of both of these parts, to make Ultrix happy.
</p>

<h2>Let's emulate!</h2>
<h3>The CPU</h3>
<p>MIPS is a rather old instruction set, which shows in a few places. The main one is that it attempts to prevent signed overflow. The normal instructions used for addition and subtraction will cause an actual exception if they cause an overflow. This does not map to how CPUs are used today, so nobody cares, but I still had to emulate it. There are "unsigned" versions of the instructions for addition and subtraction that do not do this, which is what all modern compilers will emit on MIPS.
</p>
<p>I wrote an emulator for the CPU in C first, to allow easy testing on my desktop, while the PCBs were being manufactured. It was not fast, nor meant to be, but it did allow for testing. You can see this emulator in <span>cpu.c</span>. Along the way here, I implemented some features of the R4000 CPU optionally. It turned out that to boot Linux compiled with modern compilers, this is necessary, as the compilers assume these instructions exist. Technically this is a bug. Realistically, I am likely the only person to ever notice. So, which features did I need to add? Likely branches (<span>BxxL</span> instructions), conditional traps (<span>Tcc/TccI</span> instructions), and atomics (<span>LL/CC</span> instructions).
</p>
<p>Of course, C is not the language one uses when one wants to go fast. I wrote an emulator in assembly too, targetting ARMv6-M (for the Cortex-M0 MCU I chose). I later added a sprinking of enhancements for ARMv7-M (in case I ever upgrade the project to a fancier CPU). This was tested on a Cortex-M7 and worked well too. The assembly emulator core is contained in <span>cpuAsm.S</span> and the ARMv6-M specific parts are in <span>cpuM0.inc</span>
</p>
<p>I mentioned delay slots earlier. What is a delay slot? Well, back in the day it was considered cool to expose your CPU's pipeline to the world. Just kiding, it was just a way to save some more transistors. Basically, the instruction after a jump will be executed even if the jump happens. This is called the delay slot. A naive way to avoid dealing with this is to place a <span>NOP</span> after each jump instruction. But with a good compiler, the delay slot can be put to a good use in almost all cases. Obviously one cannot place a jump instruction in the delay slot, since the CPU is already jumping somewhere. Doing this is illegal and undefined. An issue arises, however, if the instruction in the delay slot causes an exception of any sort. The CPU will record that the instruction was in the delay slot, and point the exception handler to the <em>jump</em> whose delay slot we're in. There is no way to return to this "in delay slot" state, so the exception handler is expected to take steps to somehow execute the delay-slot instruction and then complete the jump.
</p>

<h3>The FPU</h3>
<p>The DECstation came with an FPU, so that floating point operations would be fast. Back then this was a separate chip, which was optional in a MIPS R2000/R3000 system. Linux, in fact, will more-or-less corectly emulate the FPU if it is not present, but this is slow. I used this mode initially, and even fixed a few bugs in Linux's emulation, but, in the end, I implemented an FPU emulator. This was necessary since it seems like a lot of MIPS binaries I could find all assume the FPU is available and use it freely. I never reimplemented the FPU emulator in assembly, instead calling out to the C FPU emulator when needed. I figure that squeezing a few cycles out of each instruction is meaningless when the actual FPU operation takes hundreds. The code for this is in <span>fpu.c</span>. I include Linux patches to remove FPU emulation support from the kernel. This saves some RAM. Later, I also added support for a "minimal" FPU - it supports the registers but no operations. This is allowed by the spec, since the FPU may refuse to execute any operation it is "not sure it can do perfectly correctly", so any compliant OS must implement a full FPU fallback anyways. Why? This saves 16K of code size in the binary, opening the possibility of running uMIPS on smaller devices yet.
</p>
<h3>The MMU</h3>
<h4>MMU basics</h4>
<p>(this is a <em>very</em> oversimplified summary, feel free to skip if you know this, and do not complain to me that it is not perfectly accurate!)
</p>
<p>Most CPUs access memory using virtual addresses (<span>VA</span>). The hardware works in terms of physical addresses (<span>PA</span>). Ability to map one to the other is the underpinning of memory safety in modern operating systems. The purpose of an <span>MMU</span> (Memory Management Unit) is to translate virtual addresses to physical addresses, to allow for this mapping. Normally this is done using a tree-like structure in RAM, called a <span>pagetable</span>. Most CPUs have a component whose job it is to walk that structure to resolve what physical address a given virtual address maps to. This component is a <span>pagetable walker</span>. In most cases the <span>pagetable</span> has 3 or 4 levels, which means that resolving a <span>VA</span> to a <span>PA</span> requires reading 3 or 4 words from main memory. Clearly you do not want to do 3 useless memory accesses for every useful one. So usually another component is included in an <span>MMU</span> - a <span>TLB</span> (Translation Lookaside Buffer). Basically you can think of a <span>TLB</span> as a cache of some of the current <span>pagetable</span>'s contents. The idea is that before you go off doing those 3-4 memory reads into the <span>pagetables</span>, you can check and see if the <span>TLB</span> has a matching entry. If so, you can skip the <span>pagetable walk</span>.
</p>
<p>Clearly, like any cache, the <span>TLB</span> needs to stay in sync with the things it caches (the current <span>pagetables</span>). So, if the OS changes the <span>pagetables</span>, it needs to flush the <span>TLB</span>, since it might have stale entries. Usually, <span>TLB</span>s expose very little interface to the CPU, so there isn't a way to go read all the entries and remove only the newly-invalid ones. Additionally, this would be slow, so this is not usually done. However, invalidating the entire <span>TLB</span> also has costs - it needs to be re-filled, at the cost of 3-4 memory accesses per entry. This could hurt performance. A solution commonly used is called an <span>ASID</span>.
</p>
<p>What are the four main cases when <span>pagetables</span> might be modified? (1) Adding a new mapping over a virtual address that previously was not mapped to anything, (2) changing permissions on on existing mapping, (3) removing a mapping, and (4) entirely changing the memory map (for example to switch to a completely different process). In case 1, no <span>TLB</span> flush is necessary, since no stale <span>TLB</span> entry can exist. Cases 2 and 3 do indeed require flushing the <span>TLB</span>, but they aren't that common. Case 4 is quite common, though. It is done at every context switch. One might point out that since we're changing the entire memory map, the entire <span>TLB</span> would be invalid, and thus flushing it isn't a problem. This is wrong. Besides mapping userspace things, the <span>MMU</span> also maps various kernel structures, and there is no point penalizing them.
</p>
<p>If we could somehow tag which entries in the <span>TLB</span> go with which process, and temporarily disable them when another process runs, we could avoid a lot of context-swich flushing and the performance costs imposed by it. It would also be cool if we could tag entires that belong to the kernel and are valid in every process. Well, this exact technology exists in many <span>MMU</span>s. The idea here is that each <span>pagetable</span> entry will have a bit marking it as "global" (valid in all memory maps) or not. There should also be a register in the CPU setting the current <span>ASID</span> (Address Space ID). When a <span>TLB</span> entry is populated from the <span>pagetables</span>, the current <span>ASID</span> is recorded in it. When a lookup in the <span>TLB</span> is done, only entries matching the current <span>ASID</span> or those marked "global" will match. Cool!
</p>
<h4>The MIPS MMU</h4>
<p>The idea at the time was to save transistors. Which of the above could be cut? Well, cutting out the <span>TLB</span> guarantees terrible performance in all cases. But that <span>pagetable walker</span>, do we really need it? What if we make the sotware do it? We can add a little bit of assistance, like ability to manage the <span>TLB</span> efficiently, but skip on the <span>pagetable walker</span> hardware. This is what MIPS did. Here is the MIPS virtual address space:
</p>
<p>Addresses               Name   Mapping
0x00000000..0x7fffffff  kuseg  mapped via MMU
0x80000000..0x9fffffff  kseg0  mapped to physical 0x00000000..0x1fffffff, cached if there is a cache, only accessible in priviledged mode
0xa0000000..0xbfffffff  kseg1  mapped to physical 0x00000000..0x1fffffff, not cached, only accessible in priviledged mode
0xc0000000..0xffffffff  kseg2  mapped via MMU, only accessible in priviledged mode
</p>
<p>So, as you can see, some <span>VA</span>s do not map via the <span>MMU</span> at all. This means that code living there is able to run no matter the state of the <span>MMU</span>. Linux and Ultrix, predictably, put the kernel in <span>kseg0</span>. The kernel does, however, need to be able to dynamically map things in as well. <span>kseg2</span> is one gigabyte of address space that is mappable via the <span>MMU</span> that the kernel can use. Memory-mapped devices will usually be accessed via <span>kseg1</span>. The 2 gigabytes at the bottom of the address range(<span>kuseg</span>) are for userspace tasks.
</p>
<p>What entry in a <span>TLB</span> should one replace when one needs to insert a new entry? An obvious answer might be "the one least recently used", but that would require tracking use, which costs transistors too. A simplification is "the one least recently added". This is easy, but it hides a fatal flaw. Imagine your <span>TLB</span> has N entries, and your workload sequentially uses N + 1 addresses, such that each would need a <span>TLB</span> entry. Now you'll always be replacing the entry you're about to need, guaranteeing that you <em>NEVER</em> hit the <span>TLB</span> and do a lot of pointless <span>pagetable</span> walks. How do we avoid this? The simplest method is replace a random entry. Sure, it might be the entry you're about to need, but for an N-entry <span>TLB</span> the chances are 1/N.
</p>
<p>Generating random numbers is slow in software, so MIPS R2000/R3000 provide some help. The CPU has a register called, literally <span>RANDOM</span> which is supposed to be constantly incrementing, every cycle. Since the "when" of "when will you next need a new <span>TLB</span> entry" is not predictable, this is as good as random, and requires very few transistors. The idea is that whenever you need to replace a <span>TLB</span> entry, you use a special instruction <span>TLBWR</span> to write to a random entry. I did not tell you about <span>ASID</span>s by accident either. The MIPS R3000 <span>MMU</span> implements a 6-bit <span>ASID</span>.
</p>
<h4>Emulating the MMU efficiently</h4>
<p>Emulating the R3000 <span>MMU</span> is a bit of a pain. Since any entry can be in any location, the proper way to do a lookup is to check each one. Doing a 64-cycle loop for every memory access is a non-starter speed-wise, of course. I use a hashtable indexed by the virtual address to keep all the TLB entries in buckets for faster checking. Using 128 buckets virtually guarantees that most buckets have zero or one entry in them, permitting much faster lookups. Initially this was a simple table of pointers, but this used too much RAM, so now it is a table of indices.
</p>
<h3>Communication</h3>
<p>The DECstation had a few ways to communicate with the outside world. It had a built-in network card<s>, which I do not emulate</s>. It was optional<s>, and I haven't found a use for it yet</s>. Maybe I will later - it does not look complex. It also had a SCSI controller which one could attach hard disks and other SCSI peripherals to. Emulating this would be a fun challenge, and I'll probably get to it later, but I did not do it now - it was not necessary - I wrote a paravirtualized disk driver for Linux using hypercalls, more on this later. There was also an optional framebuffer card one could install that added support for a monochrome or a color display. Emulating these would also not be too hard, but my business card lacks a display, <s>so I did not do it either</s> - plus I am not even sure that Linux can make a use of it.
</p>
<p>The last method of communications that the DECstation had was <span>DC7085</span> - a serial port controller that is basically a clone of a PDP11-era <a href="https://gunkies.org/wiki/DZ11_asynchronous_serial_line_interface"><span>DZ-11</span></a>. It supports four serial ports at a blistering 9,600bps speed (or any integer division thereof). Each serial port was allocated a purpose, and they were wired to different connectors indicating this purpose. #0 was for the keyboard, #1 for the mouse, #2 for modem, and #3 for printer. To the machine they are all the same, this was just the purpose DEC assigned to them. The stock <span>PROM</span> would use #3 as serial console instead if it did not detect a keyboard at #0, thus it is customary to use #3 as serial console for Linux on the DECstation. My <span>PROM</span> surrogate does not bother looking for or supporting external keyboard, and just defaults to serial console on #3. That being said, since it is cool to allow multiple login sessions, I also export <s>#0</s> #2 as a second virtual serial port, so that you may login from two serial consoles at once, and do two things at once. How cool is that?
</p>
<p>So, how do I export these serial ports? When you connect the card to a computer, it'll show up as a USB composite device comprised of two CDC-ACM virtual serial ports. One of them is port #3, another is port <s>#0</s> #2 on the virtual <span>DZ-11</span>. How will you know which is which? #3 has the boot console printing and will have the initial <span>sh</span> prompt. If you do not see this, try the other one, computers do not always number them in the order I export them.
</p>
<h3>Hypercalls</h3>
<p>In the real world the <span>PROM</span> had to probe the real hardware to detect what was present where. As my <span>PROM</span> is running in an emulator, there is no need for such mess. We can simply request things from the emulator in an agreed-upon way. That way is a <span>hypercall</span> - a special invalid instruction that, if encounted in supervisor mode, the emulator will treat as a request for some kind of service. The instruction I chose is <span>0x4f646776</span>, which is in the <span>COP3</span> (coprocessor 3) decode space that was not allocated to any real purpose in these chips. The calling convention is close to the normal C calling convention on MIPS: parameters are passed in <span>$a0</span>, <span>$a1</span>, <span>$a2</span>, and <span>$a3</span>, return values are in <span>$v0</span> and <span>$v1</span>. The <span>$at</span> register gets the "hypercall number" - the specific service we're requesting.
</p>
<p>A few hypercalls are implemented. #0 is used to get the memory map. The parameter is word index of the memory map to read. Word 0 is "how many bits the memory map bitmap contains", word 1 is "how many bytes of RAM each bit represents", words 2 and on are the bits of the map, up to the total specified in word 0. This can be used to build a memory map that the <span>PROM</span> can furnish to the running OS and allows me to have discontinuous RAM. Linux supports this and I tried it, but did not end up needing it. It is here in case I change my mind and need it again.
</p>
<p>Hypercall #1 outputs a single byte to the debug console (which is the same as <span>DZ-11</span> port 3). This is used by the <span>PROM</span> and <span>mbrboot</span> to output debug strings without needing to have a complete <span>DZ-11</span> driver in there. Hypercall #5 will terminate emulation. This can be used on the PC version of the emulator to quit peacefully.
</p>
<p>Hypercalls #2, #3, and #4 are used for SD card access. #2 will return card size in sectors, #3 will request a read of a given sector to a given physical RAM address and reply with a nonzero value if that worked. #4 will do the same for a card write.
</p>

<h2>Bring on the hardware!</h2>
<h3>The honeymoon period</h3>
<p><a href="http://dmitry.gr/images/linuxCardBoard.png"><img src="http://dmitry.gr/images/linuxCardBoardSmall.jpg" alt="Linux card board layout"></a></p><p>The first revision of this board came up well initially, after I sorted out the mess that is ATSAMD21's clocking system. I appreciate flexibility as much as the next guy, but this thing is <em>TOO</em> flexible. It took a lot longer than I'd care to admit to get this thing running at a sane speed and to enable some peripherals. The docs were too sparse to be of much use, too. Atmel, what happened to you? You used to have the best docs!
</p>
<p>The first revision of the board had two memory chips, each on their own SPI bus, an SD card on an SPI bus, and USB with the proper resistors. The USB was perfect. Unlike everyone and their grandmother (STMicro, I am glaring at you), Atmel did not license annoying Synopsis USB IP. They made their own. It is easy to use, elegant, and works well. Seriously, it just worked. In two days I got the hardware to work and wrote a USB device stack. I tip my hat to the team that worked on the USB controller. That being said, I have concerns. My main issue: USB descriptors aren't small. They are constant. I'd prefer to keep them in flash. I'd prefer to, but cannot. The USB unit uses a built-in DMA unit to read the data to send. This DMA unit <em>CAN</em> access flash, but if you have any flash wait states enabled, it sends garbage. I suspect that Atmel only tested it for reading from RAM, forgot that some memories have wait states, and did not account for that. Keeping all my descriptors in RAM is a colossal waste of RAM, which there is only 8KB of. Remember that tiped hat? I rescind it, Atmel. I had to work around the issue by sending the descriptors one piece at a time (rather than letting the hardware DMA it all automatically) just to save the valuable RAM.
</p>
<p>Using the SPI units directly worked well enough, until I tried to speed them up. Past about 18MHz The received data was garbled (missing a bit or two, all the following bits shifted). No amount of searching found an issue in my code, and all sample code did more or less the same things. My bus analyzer showed no issues. What gives? <a href="https://microchipsupport.force.com/s/article/SPI-max-clock-frequency-in-SAMD-SAMR-devices">THIS GIVES</a> (<a href="https://archive.ph/IJTHU">archived</a>)! I was beyond furious when I found this forum post. Here I was, trying to build a fast device, and my SPI bus was going to be limited to the speed of a tired snail calmly strolling through peanut butter! With some more testing I found that the SPI units will work fine to about 16MHz, which I'll have to live with.
</p>
<p>The SPI units have no FIFOs, so code must manually feed them one byte at a time and read one byte at a time. This means that there is space between bytes on the bus as code wrangles bytes in and out of registers and memory. This is a waste of potential speed. The solution is DMA. Luckily this chip has DMA. Unluckily, it is fucked beyond belief, to a point where I am beginning to suspect that it was designed by a sleep-deprived stark raving lunatic.
</p>
<h3>How not to design a DMA unit</h3>
<p>A normal garden-variety DMA unit has some minimal global configuration, and a few channels, each independent from the rest. Each channel will usually have a source address, a destination address, a length, and some configuration, to store things like transfer chunk size, trigger, interrupt enable bits, etc. Thus it is common in ARM MCUs to have each channel have precisely these 4 32-bit configuration registeres: SRC, DST, LEN, CFG. This is 16 bytes of SRAM per channel. ATSAMD21 has 12 DMA channels, so that would be 192 bytes of config data for the DMA unit as a whole. Not that much. Well, Atmel was having none of this! Instead, the unit itself only has a <em>POINTER</em> to where in the user RAM all this config data lives. For every transfer, the DMA unit will load its internal state for the active channel from this structure in RAM, and then operate on the channel. If another channel's data was already loaded, it will be written out to RAM first. Depending on your experience level, you may already be on your third or fourth "oh, hell fucking no" as you read this...
</p>
<p>Why is this bad? Let's imagine two SPI units being fed by DMA. Each one will have two DMA channels, one for receive, one for transmit. Four channels are active in total. Now what happens as both the SPI units are enabled? Two DMA channels (the transmit ones) will go active and attempt to send a byte. One will go first, then the second. This will generate <em>14</em>(!!) bus transactions to the RAM! Four to read config data for one channel, one to read the byte to send, four to write back this config data, four to read the config data for the next channel, and one more to read the byte to send. So in order to send 2 bytes, the DMA unit did 14 RAM accesses. Not great. But wait...there's more. Let's take a look what happens next, as the SPI units finish sending this byte and clocking in the received byte, but are also ready for the next byte to send! At this point in time, logically only four bytes need to be moved (two from the units into the receive buffers, two from the transmit buffers into the units). Let's see how this plays out. Remember the DMA unit's internal config data is currently loaded to the second transmit channel's. First, it'll have to do 4 writes to write that data out, then 4 reads to load the first receive channel's structures, one write to memory to write the received byte to RAM, 4 writes to write out this channel's structures out, 4 reads to load structures for receive channel number 2, one write of the received byte to RAM, 4 bytes to write out the config structure for this channel back out to RAM, and then the 14 we already discussed to send the next two bytes. That adds up to 36 RAM accesses to simply read two bytes and write two bytes. All this pain, simply to save the transistors on the 192 bytes of SRAM it would have taken for the DMA unit to store all the config data internally.
</p>
<p>So, why is this bad? Let's say our MCU is running at its designed speed of 48MHz, its SPI units running at their designed max speed of 12MHz. At the point the second bytes need to be sent and first received bytes need to be received, we'll need to perform 36 accesses to RAM, but also 4 accesses to the SPI unit. The SPI unit is on an APB bus, which means that any access to it takes at least 4 cycles. This means that in between each sent and received byte we'll need 36 + 4 * 4 = 52 cycles. If the SPI unit runs at 1/4 the CPU speed, then it will send/receive a byte every 8 * 4 = 32 cycles. So every 32 cycles we'll need to do 52 cycles' worth of work. When they do not get enough cycles, the DMA channels give up and stop working... Oops... 
</p>
<p>So, what can be done? I worked out a hybrid method where I send data using CPU writes and receive using DMA. This worked for two channels, but would not work for more. Once I got rev2 boards that had 4 RAM chips, even this failed, as just the 4 receive DMA units starved each other of bandwidth and got cancelled. Why was Atmel so damn stingy with internal SRAM? We'll probably never know. But they could have solved this exact issue simpler than with 192 bytes of SRAM in the DMA unit. Just adding a 4-byte FIFOs into the SPI units would do as well, then each DMA transaction could transfer more than a single byte, alleviating this traffic jam. Sadly, apparently nobody at Atmel has even tried to actually use their chip for anything. Atmel, what happened to you?
</p>
<h3>Clocks again</h3>
<p>My clocking woes were not over yet. This chip has a number of internal oscillators, one of which is supposed to be a rather precise 32KHz oscillator called <span>OSC32K</span>. I wanted to use that as a source clock for a timer to implement my virtual real time clock. Well, despite much pain and many tears, the damn clock would not start... ever. The code should be simple: <span>SYSCTRL-&gt;OSC32K.bit.EN32K = 1; SYSCTRL-&gt;OSC32K.bit.ENABLE = 1; while (!SYSCTRL-&gt;PCLKSR.bit.OSC32KRDY); </span> Yeah... that did not happen. At the end, I decided that I can use a less-precise <span>OSC32KULP</span> to clock my timer. That one did start and I was able to use it. By this point in the project I was worn out, desensitized to this chip's many faults, and completely out of WTFs, so I resigned myself to a slightly imprecise real-time clock and trudged on.
</p>
<h3>SD card support</h3>
<p>Not really much to say about SD card support. Been there, done that, got the t-shirt. My initial code for the prototype used multi-block reads and writes for better card access speed, but in the final prototype I was forced to abandon it since one of the RAM chips on the b2 boards shared the SPI bus with the SD card, so leaving the card selected was not an option. This was not that big a deal since SD access is rarely, if ever, a bottleneck here. Any card up to 2TB is supported.
</p>
<p>In the v2 revision of the board I wired up card detect pin to the MCU. It was not used, but I thought that I might find a use for it. I did not, so in v3 boards it was removed. I also added a card "activity" LED which lights up when card is accessed. It is simply a LED between the card's chip select line and Vcc. Whenever the card is selected, it is on. This LED also surves a second purpose. If at boot time the SD card or SPI SRAMs fail to initialize, it'll blink out an error code to help identify the problem.
</p>
<h3>Coolness enhancement</h3>
<p>Now that the prototype worked and I was doing the layout for the final version, I decided to do some things to make it look cool. I buried all the traces in layers 2 and 3, leaving layers 1 and 4 uninterrupted copper. It loooks super cool! Of course the top layer copper is interrupted for the actual SMT pads, but other than that, it is all perfectly smooth and looks amazing!
</p>

<h2>How it works</h2>
<h3>How a normal DECstation boots</h3>
<p>Normally there is a built in 256KB ROM (called <span>PROM</span> by DEC) at physical address <span>0x1fc00000</span> that contains enough code to show messages onscreen and accept keyboard input, talk to SCSI devices, load files from disk to RAM and jump to them. This <span>PROM</span> also provides a lot of services to the loaded operating system via an array of callbacks. This includes things like console logging, EEPROM-backed environment variables, memory mapping info, etc. This is rather similar to UEFI. Normally this <span>PROM</span> would read the environment variables from EEPROM that would tell it which device to boot, and then load a kernel and boot from that device if all goes well. This emulator does not boot this way
</p>
<h3>How uMIPS boots</h3>
<p>I had no desire to include a large ROM in the emulator, as the flash space in the microcontroller is limited. I also do not have a graphical console or a keyboard per se. That being said, I had to implement a sizeable subset of the <span>PROM</span> somehow, since MIPS Linux uses it. What to do? I decided to come up with my own boot process, which can still work just as well. There is indeed a ROM at <span>0x1fc00000</span>. This is necessary for rebooting to work from Linux. <s>That rom is tiny - 32 bytes</s>. Its source code is found in the "romboot" directory. It <s>merely</s> loads the first sector of the SD card to the start of RAM at <span>0x80000000</span> and jumps to it. The first sector of the SD card contains a standard MBR partition table and up to 446 bytes of code. The code that lives here can found in the "mbrboot" directory. It is also rather simple. It looks through the partition table for a partition with type byte of <span>0xBB</span>. If not found, an error is shown. Else, the partition in its entirety is read into RAM at <span>0x80001000</span>, and then jumped to. This partition can be arbitrarily large, and this is where my implementation of the "PROM" lives. The actual size limit on it is placed by the fact that MIPS Linux expects to be loaded at <span>0x80040000</span>. This is no accident - the first 192K of RAM is reserved for the <span>PROM</span> to use as long as the operating system expects to use <span>PROM</span>'s services. Thus the limit on the loader's size is 188K. 
</p>
<p>My <span>PROM</span> implementation's code can be found in the "loader" directory. It will search the SD card for a partition marked as active, attempt to mount it as FAT12/16/32, and look for a file called "VMLINUX" in the root directory. If found, it will be parsed as an ELF file, properly loaded, and run. Else an error will be shown. As this code has no serious size limits, it implements a proper ability to log to console, printf, and all sort of such creature comforts. As far as <span>PROM</span> services go, it provides console logging, memory mapping info, and reading environment variables, at least enough to make Linux happy. I have not tried to boot other operating systems on uMIPS (yet?).
</p>
<p>The kernel commandline I pass is rather simple: <span>earlyprintk=prom0 console=ttyS3 root=/dev/pvd3 rootfstype=ext4 rw init=/bin/sh</span>. The first parameter provides for early boot logging via the <span>PROM</span> console, which is useful to see. After the kernel is up, it'll use the third serial port for console. Originally for the DECstation that was the printer serial port, but Linux users on DECstation use that for serial console due to that being the easiest port to convert into a simple serial port. The rest just tells the kernel how to boot. I prefer to boot into sh, and then issue <span>exec init</span> myself, thus the <span>init=/bin/sh</span>
</p>
<h3>How uMIPS runs</h3>
<p>After all the optimizations (which I'll detail in a bit) the effective speed of my virtual MIPS R2000/R3000 on this infernal ATSAMD21 chip is around <s>900KHz</s> 1.2MHz. The CPU spends around 8% of its time handling timer interrupts, and thus around <s>0.83MIPS</s> 1.06MIPS of CPU cycles are left for useful work. With this, the kernel takes around 2 minutes to boot and run <span>sh</span>. Executing busybox's <span>init</span> and getting to the login prompt takes another minute. Overall not too bad. Commands reply instantly, or in a few seconds. It takes gcc around 2 minutes to compile a hello world C program, and I estimate that in a few days' time, one could rebuild the kernel on the device itself, copy it to <span>/boot</span>, and reboot into it. Yes, I <s>do intend to try this and time it</s> did do this and it worked!
</p>
<p>The emulated real time clock is actually real time, plus or minus the inaccuracies of ATSAMD21's ultra-low-power 32KHz timer. It is ok enough that you will not notice. Try the <span>uptime</span> command.
</p>
<p>There is just one thing I did not yet address concerning running Linux on uMIPS. The storage. I said that it is an SD card, but surely DECstation had no SD card slot. However, Linux is open source. I simply created my own very simple paravirtualized disk driver which uses a hypercall to talk directly to the emulator and request sectors to be read or written directly into the virtual RAM. To Linux, this looks just like DMA, except instant. The whole implementation of the driver is under 200 lines of code and can be seen in <span>pvd.patch</span>
</p>

<h3>Linux changes</h3>
<p>I made some changes to the kernel to make life easier. They are provided as patches against the 4.4.292 kernel, and as is a working kernel image. Why that version? Because when I started that project, it was an LTS version of the kernel, and since RAM is short, I wanted the smallest possible kernel, so this was preferable to a later version. The config I am using is available in <span>kernel_4.4.292.config</span>. A config for an even smaller kernel (that requires uMIPS to emulate the full FPU) is available in <span>kernel_4.4.292.config_nofpu</span>
</p>
<p>I did a lot of work making the kernel as small as possible. Since Linux does not support paging out pieces of the kernel, every byte of kernel code is one byte fewer available to use for user space. I ruthlessly removed options that were not needed. In the end I got the kernel down to just under 4MB, which is pretty damn good, considering that MIPS instructions are not very dense.
</p>
<p>As part of this work, I made a few code patches. For various reasons (cough..delay slots..cough) the kernel can find itself needing to interpret userspace code, or parse userspace instructions. No matter what kernel configs I gave, the code to handle microMIPS (a future MIPS expansion not known in the days of R2000/R3000) was present. It was wasting space and time trying to handle things that would never happen. The patch <span>useless_exc_code.patch</span> removes this code if the target CPU does not support microMIPS</p>
<p>Before I implemented my FPU emulator, I was using the kernel's FPU emulation code that traps and executes FPU instructions. It had a bug. If compiled for a 32-bit MIPS processor it did not properly emulate some FPU instructions that operate on the double type. I believe this is wrong. It was causing crashes in code compiled for R3000. The patch <span>fpu.patch</span> modifies the kernel's MIPS FPU emulator by adding a config option to enable the full FPU emulation even on MIPS-I chips.
</p>
<p>Due to the differences between the R2000/R3000 and the R4000 the kernel needs to know at build time which CPU it is being built for. If you attempt to run the wrong kind of kernel on the wrong kind of CPU, it only gets far enough to panic about it. Fine, OK, but then why does this flag not affect a lot of TLB-handling code. Both kinds are always compiled in, despite us knowing at build time with 100% certainty that at least half of it will not ever be of any use? The patch <span>tlbex_shrinkify.patch</span> wraps the useless code in checks for the compile-time-selected CPU type and thus removes some kernel code, saving valuable bytes.
</p>
<p>As uMIPS runs with a real real-time clock, I did not want Linux to spend too much time handling timer interrupts. Normally, a 128Hz timer is used on DECstations by Linux. I added options for 64Hz, 32Hz, and 16Hz timer ticks as well. This reduces effective timer resolution, but effectively unloads the virtual CPU from having to spend most of its time handling timer interrupts. The patch <span>clocksrc.patch</span> does this, and the one called <span>kill_clocksrc_warning.patch</span> silences a pointless warning about timer resolution.
</p>
<p>If you do build uMIPS with full FPU emulation, there is aso a patch to remove all of the FPU emulation code from the kernel to save a few KB of RAM: <span>fpu.patch</span>. 
</p>

<h2>Improving performance</h2>
<h3>Instruction cache</h3>
<p><a href="http://dmitry.gr/images/linuxCardCompileTime.png"><img src="http://dmitry.gr/images/linuxCardCompileTimeSmall.jpg" alt="Linux card board layout"></a></p><p>One thing the processor will surely do every cycle is fetch an instruction. This means that every cycle begins with a memory access. For us that is a painful subject thanks to Atmel's errata-ridden SPI unit. And not just that, memory translation also needs to happen, and that also takes time. A good way to avoid both of these problems is a VIVT instruction cache. It'll read instructions 32 bytes at a time, and allow us to hopefully often not need to translate addresses or reach for main memory. I allocated 2KB of RAM to this cache. It is 32 sets of 2 ways of 32 byte lines. Whenever memory mappings change, it needs to be invalidated. I do this automatically and thus the running code on the virtual MIPS CPU does not need to know about it. The measured hit rate while booting Linux is around 95%, which is pretty nice for such a small cache. The geometry was determined experimentally by profiling how long a boot takes with various cache geometries. This one was found to be the best.
</p>
<h3>Improving CPU speed</h3>
<p>ATSAMD21 series is specified to run at 48MHz. In my testing they run perfectly well up to 96MHz, with some specific chips able to hit 110MHz. I found no chip unstable at 96MHz, so I decided to just run at 90MHz, for some safety margin. This immediately got me a pretty serious performance uplift. No, it is not really 100%, since (1) SPI RAM is still limited by the SPI speed limit, and (2) flash memory has wait states which had to increase for the larger speed. But this did give me an honest 65% improvement. Still a good start. Now RAM SPI runs at CPU / 6 = 15MHz.
</p>
<h3>Improving RAM bandwidth</h3>
<p>Since I could not make the RAM SPI units go faster due to Atmel's incompetence, I decided to go wider! I can drive four units at once. Given, there is overhead to each read and write command, but still this is faster than one or two. <s>My code initially supported one, two, or four RAM chips, but for simplification I dropped that support and now only support four-channel RAM.</s> Quite the statement eh? This microcontroller has four-channel RAM! The emulator accesses RAM in increments of 32 bytes. The RAM read/write commands themselves are 4 bytes each. This means that for a single-RAM chip situation, reading 32 bytes takes (4 + 32) * 8 = 288 SPI bits. In dual-channel configuration it'll take (4 + 16) * 8 = 160 SPI bits, since the command is still 4 bytes long, but we only read 16 bytes from each RAM , for a total of 32. For quad-channel RAM, we thus have (4 + 8) * 8 = 96 SPI bits to read 32 bytes. This is a 66% improvement from the single-channel case! In reality the improvement is less, since quad-channel mode cannot use DMA at all, so it is a bit slower. Real-life measurement shows that quad-channel mode is a 50% improvement over the single-channel case. But still, given this damn chip, any improvement is an improvement I'll take.
</p>
<p>But, why are all the RAM acceses 32 bytes in size? Well, as you see RAM accesses are slow. A typical 32-byte access takes 140-ish SPI cycles, which is around 12 microseconds. If every access took that long, my emulated CPU would be limited to no more than 85,000 memory accesses per second. That is too slow to be practical. Something had to be done. I decided on a cache. Sadly, my microcontroller has a very limited amount of RAM, so the cache had to be small. I evaluated various cache geometries, and found that a 20-set 2-way cache with 32-byte lines produced the best performance uplift for the emulator. It gets a 91% hit rate while booting the kernel, which is a pretty good payoff for 1.25KB of RAM. With a hit taking around half a microsecond and a miss taking around 12 microseconds, adding this cache improved the average memory access by 87%! Yes, this is effectively an L2 cache. Now, how many emulators do you know that have an L2 cache to paper over the terrible performance of their chosen host hardware, eh? The cache allocates on reads and writes, except for reads and writes of precisely 32 bytes in size. Those are passed through directly because they are either SD card access DMA or icache fetches that do not need to also be cached in this cache.
</p>
<p>After some more profiling, I rewrote the "hot" part of the memory access code in assembly for some more speed gain. GCC may have come a long way since a decade ago, but it still does not hold a candle up to hand-written assembly. I removed support I had for one and two-channel RAM to simplify the hot path as well. So now you need to populate all four RAM slots for the card to boot. If you populate different RAM sizes, the smallest one will dictate the final usable RAM size. The usable RAM size will always be four times the size of the smallest RAM chip. This isn't a big deal, the DECstation came with 4MB of RAM, and could be outfitted with a maximum of 24MB. This card can be outfitted with 32MB, so you'll be living like a king! That being said, due to the size of the Linux kernel, you're not going to get a successfull Linux boot unless you have at least 6MB of RAM<s>, and uMIPS will refuse to boot if that is the case (eg: if you populate 4x 1MB chip)</s>.
</p>
<h3>Dirty hacks specifically for Linux</h3>
<p><s>Remember how on MIPS the operating system must do its own pagetable walking and filling of the TLB? As you can imagine this happens often. Very often. How could I speed this up without causing any correctness issues? On taking the <span>TLB refill</span> exception, I verify the handler has not changed and matches the expected bytes, if so, I do what it would have done, but in native code, not emulated MIPS. This helps this particular code run quite a bit faster. Correctness is not compromised since this is only done if the handler matches what is expected, byte for byte.</s>
</p>
<p>I also mentioned that due to how delay slots work, if a CPU takes an exception on an instruction in the delay slot, the kernel must be able to completely emulate that instruction, or in other way execute it and then jump to the right place? Linux uses the fact that MIPS has no PC-relative instructions, except jumps, and it is illegal to place a jump in the delay slot. How? Instead of emulating the delay-slot instruction, Linux copies it out to a special page in memory, where it is followed by a trap. Linux then jumps there in user mode to let it execute, catches the trap, and then re-directs execution where it should go. Now, if this sounds like a giant hassle to you, you are right. What can we do? Well, if an instruction in a delay slot causes an actual exception (like an illegal access, or a TLB refill exception, or some such thing), not much can be done. But what we <em>CAN</em> do is not make things worse. uMIPS will not deliver IRQs before executing an instruction in the delay slot of a branch. At worst, this will delay an IRQ by a cycle, which makes no difference to correctness. The benefit is that this sort of instruction copying and juggling can be done less.
</p>

<h2>How to build and use one</h2>
<h3>Building</h3>

<p><a href="http://dmitry.gr/images/linuxCardPcb.jpg"><img src="http://dmitry.gr/images/linuxCardPcb.jpg" alt="Linux card PCBs"></a></p><p>Now, why you really came here. How do you get one? Well, you could try knowing me personally and asking for my business card, I have a few to give out, but other than that, here is how to do it.
</p>
<p>You'll need to order the board from a board fabrication place. I am a fan of <a href="https://jlcpcb.com/">JLPCB</a> and recommend them. The gerber files I provide come in two flavours. One as you see my card exactly, and one without my name and contact info :). This is a four-layer board, the board house will ask you for layer order, it is: GTL, G1, G2, GBL. At least JLPCB has options to also cover the edge connector in gold for better contact, called "gold fingers" and to grind the board edge to 45° for easier insertion. I suggest selecting both of these options - they are free. Remember to set the board thickness to 0.8mm.
</p>
<p>While you wait for the board to arrive, you'll want to order the parts. You'll need four of the same memory chip (I have the links above), an ATSAMDA1E16, an AMPHENOL 11400841 SD card slot, and a MIC5317-3.3YM5TR regulator. You'll also want to (optionally) order an 0603 sized blue or white LED for SD activity light. If you choose to have that LED, you'll also need a 430 ohm resistor in 0603 or 0805 size. Besides that, you'll need in 0603 or 0805 sizes: 2x 5.1Kohm resistor, 1x 1Kohm resistor, 3x 0.1uF capacitor, and 7x 1.0uF capacitor. You will also need an SD card and any SWD programmer capable of programming the ATSAMD chip. There are many out there. Pick your favourite.
</p>
<p>You'll need an SD card as well. 128MB is the bare minimum here if you want to fit the busybox-based rootfs in. To fit the debian or hybrid image I am providing, you'll want at least 512MB. You can write the image to the card using your favourite tool for that. On Linux and MacOS that is probably <span>dd</span>, on windows, <span>Win32DiskImager</span>.
</p>
<p>Once you've assembled the board, program the MCU with the provided binary <span>software/emu/uMIPS.bin</span> and you're done!
</p>
<h3>Building from source</h3>
<p>You'll want to build a few things. You'll need both an ARM (CodeSourcery) and a MIPS GCC toolchain (I used mips-mti-linux I found online). First, build "romboot", "mbrboot", and "loader". Then, build the kernel. I provided the config, patches, etc. Then you'll want to build the emulator. To build for the MCU, use <s><span>make CPU=atsamd21</span></s>(<b>UPDATE</b>: proper target name changed, see updates later in the article). To build for PC, try <span>make CPU=pc</span>. Then you can build the SD card image. You'll want to copy the MBR from one of mine and modify it, then use <span>mkdisk.sh</span> to embed your kernel, mbrboot, and loader. Use a loopback mount to copy in your rootfs.
</p>
<p>If you want to run the emulator on PC, there are a few things to note. First of all, Ctrl^C will kill it :). Second, unlike the MCU version, the PC version does not incorporate the rom loader in the binary, so you'll need to provide a pointer to it on the command line. A typical command line is <span>./uMIPS ../romboot/loader.bin ../disk.wheezy</span>
</p>

<h3>If you are lazy</h3>
<p>For the lazy ones I am trialing selling all the parts and the board together as a kit on tindie. I'll see how this goes. My suspicion is that it'll end up being a giant pain in my ass and not worth the time, but I am giving it a fair shot. <b>EDIT:</b> Apparently not, and not even with a good reason. I quote: <em>Please resubmit for admin approval once you have addressed: Other Reason.</em>. LOL, how about <em>NO</em>? As a sidenote, if anyone knows companies that do this sort of thing for me (sell a kit I designed), please drop me a line <a href="mailto:tips@dmitry.gr">by email</a>. If you are <em>really really</em> lazy, I might consider having a batch of these factory-assembled by JLPCB as well. If you are interested, click <a href="mailto:assembled_requests_linuxcard@dmitry.gr">here</a> and let me know. No promises yet.
</p>
<h3>Using</h3>
<p>I provide a few disk images. The smallest is the busybox-based one (disk.busybox) - it is small, fast, and cool. I built the busybox from source for MIPS-I with as many applets enabled as I could imagine being needed. The second image is a full debian wheezy (last version to support MIPS-I) rootfs. I should warn you that debian's "init" starts around 3000 processes while it boots, so that takes a long time. If you are using the debian disk image (disk.wheezy), I strongly suggest to just mount proc and sys, and do your things in "sh" without running "init", but it will work if you do ... eventually. I also provide a hybrid image (disk.hybrid). It has a busybox shell and init, but has all of the debian binaries, so things not provided by busybox are still there and work, like gcc and vim. This is the "hybrid" image.
</p>
<p>Using the LinuxCard is easy, insert the SD card, connect USB-C to a computer, and open your favourite serial console app (minicom, PuTTY, etc), if you do not see the boot log, try the other virtual serial port (two exist). In case of a boot error, the SD card LED will blink in an infinite pattern, you can see the code for details on what various numbers of blinks mean.
</p>
<p>Once you see the shell prompt, you can play around, or continue boot to login by typing <span>exec init</span>. After this you'll be able to login as "root" with the password of <s>"mips"</s> "mipsmips". There will also be a login prompt on the second serial port as well. So cool!
</p>


<h2>Version 2</h2>


<h3>Booting Ultrix</h3>
<h4>About Ultrix</h4>
<p>Ultrix is the period-correct UNIX for the DECstation2100/3100. The latest version is 4.5 and with some google-fu you can find ISOs of the install media. It supports the DECstation2100/3100 perfectly, and even has an X11-based UI! The goal of the v2 firmware was to properly run Ultrix on the card. This ended up requiring a lot of work. I had to improve emulation accuracy and implement more hardware. But it did work!
</p>
<h4>First time booting Ultrix</h4>
<p>My first attempts were simple - copy the kernel to my "boot" partition and attempt to load it. It would, of course, not find its root filesystem and panic, but I wanted to see how far I would get at all. The first roadblock was an obvious one - the kernel is not in the <span>ELF</span> format that the linux kernel uses and my loader expects. It is in an older format called <span>COFF</span>. I dug up docs and started working on a parser for <span>COFF</span>. After a little work, I was able to load the kernel and let it run, just to see how far it would go. To my susprise, it got far enough to log some messages to the console! It crashed soon after, when it asked my PROM code for an env variable that I did not know about "scsiid0". Not a bad start. At this point I figured that in a week or so I would have Ultrix booting. It took a little longer...
</p>
<p>Ultrix was designed for this machine, and it was designed to support all parts of it. It does not probe for hardware since it knows that a DECstation2100/3100 should have. It assumes that the requisite hardware is there and starts initializing it. This was a problem for me - I still was not emulating the graphics, SCSI, or the network card. Linux has no support for them so I had not bothered.
</p>
<h4>SCSI</h4>
<p>As this was my first time attempting to emulate SCSI, it took a while. SCSI is so over-engineered, the very word "overengineered" does not do justice to just how much so it it. There are messages, commands, statusses, selects and reselects, and oh so very much more. The SCSI chip in the DECstation2100/3100 is a very strange one that DEC designed just for this device. It is called SII or SMII and I found no docs for it other than the official summary in the <a href="http://www.bitsavers.org/pdf/dec/mips/DS3100_Functional_Specification_Rev3.1_Aug1990.pdf">DECstation3100 specification</a>. It was helpful, as it listed the register bits and values. It was a start. Watching the Ultrix kernel try to access it before it gave up and paniced provided some more help, and reading the SCSI-I and SCSI-II specs filled in the rest. After much work it seemed like the kernel was happy enough to try to enumerate the bus. It would try to select each device in order. Progress!
</p>
<p>From there, the next step was to write a virtual SCSI disk. If you haven't dealt with SCSI before, it is rather unlike most sane designs. A sane design would have a host controller be a heavy/expensive/complex machine that talks to cheap simple devices. This makes sense because typically one would have more devices than host controllers. Not here. A SCSI device drives the bus and determines what it does and when. The only thing the host can do is reqest attention from the device. This took a little while to wrap my head around as it is rather backwards. It is actually even more complex since the target device can disconnect from the bus to do things and later reconnect and continue a transaction. It <em>really</em> is quite complex. Luckily, some of that is optional. A device can also reply without disconnecting, and my virtual disk does that. With a lot of work, I was able to figure out the proper state machinery to make Ultrix indeed identify and talk to my virtual SCSI disk. I split the code into two layers. The bottom handles the basics of just being a SCSI device and the top handles actual disk-specific things.</p>
<p>The code later got expanded to support emulating a CDROM too, to allow me to do an Ultrix install from a virtual CDROM. While working on this, I noticed that the bus enumeration is slowing down the boot a lot. The issue is that there is no way to detect that "no device with this ID exists on the bus". One must attempt a select, and then wait for a timeout. This was taking a while since Ultrix implemented a timeout using a loop with a counter (not using the RTC), and at my virtal CPU speed it was taking seconds. The solution was a dummy SCSI device that does reply to some commands enough to be identified and tell the host that it has no media and is of an unknown type. This device is the "SCSI nothing".
</p>
<p>The SII controller has 128KB of SRAM for DMA-ing data to/from devices. The idea is that one schedules the transfer and it goes on at its pace, when done, an interrupt occurs and data can be copied in/out of this memory. On the PC, this is simple - i can allocate 128KB of RAM and be done with it. On the microcontroller, I do not have that much SRAM, so I steal some memory from my external memory for this, and present less than the full amount to the virtual OS. This works fine for Ultrix as it probes the memory amount page-by-page. Linux probes in 4MB increments, but I have a patch <span>allow_64K_memory_multiples.patch</span> that changes it to probe in smaller increments so that this memory stealing does not cost 4MB of usable RAM.
</p>
<p>Linux has no support for SII SCSI controller, so it continues to use the <span>pvd</span> device.
</p>
<h4>LANCE</h4>
<p>The network card in the DECstation2100/3100 is LANCE. It is somewhat documented in the DECstation2100/3100 specification sheet and I implemented it enought to please Ultrix. It never sends or receives any packets (I can add that later), but it does initialize and interrupt as needed. LANCE has a 64KB SRAM buffer for packets. The PC build of uMIPS fully supports this, the "micro" build of uMIPS will just ignore writes and produce zero reads of this area to avoid wasting 64KB of memory. This works well enough to please Ultrix. Linux has no support for LANCE, so I have no idea if it would be ok with this setup.
</p>
<h4>ESAR</h4>
<p>The MAC address for the network card is stored in a on-board EPROM called the "ESAR" (Ethernet Station AddRess). It lives at the same address as the real time clock, except it is wired to the upper byte of every word, while the DS1287 is wired to the bottom byte. This is a weird thing to do but it works. It does mean that some weird things are possible, like reading both the ESAR and the real time clock registers at once with one read. Luckily this is not usualy done. The ESAR data has some checksums and redundancy (so that its correctness is easy to verify). I implemented an ESAR for uMIPS, assigned the ethernet address <span>66:44:22:44:66:22</span> to the device, and provided for all the required redundancy and checksums. Ultrix is satisfied with this.
</p>
<h4>Memory probing &amp; proper PROM API</h4>
<p>While booting Ultrix I notied that it directly probed the amount of RAM in the system. This is strange since Linux simply queried the memory amount from a PROM API that conveniently exists for this. This was actually my mistake since I was emulating a much newer PROM iterface than the real DECstation2100/3100 had, and Linux was happy to use it. The newer standard (called REX) provides the OS a function pointer table with a lot of API. To signal REX support, a magic value is also passed. DECstation2100/3100 predate the REX API and used a different method of providing API to the OS - a table of jumps is placed at known offsets from the start of the PROM in the <span>0xbfcXXXXX</span> address space. This API is also more primitive, and lacks, for example, the ability to tell the OS how much RAM there is. The pieces now fall into place... My only problem is that I do not have an ability to have a huge PROM, as I wrote earlier. I needed another method to offer this API. I decided to indeed have this jump table, but redirect all the jumps to an address in the RAM area reserved for the PROM <span>0x80001000..0x8002ffff</span>. You'll recall that my OS loader loads there. Now it can provide this PROM API, just like it did the REX API. Cool! Testing Linux also shows that it happily uses this API properly as well. It is, of course, now also forced to <em>probe</em> the RAM amount. No big deal. I did find a bona fide bug in the kernel here! While it means (as per comments) to probe for a maximum of 480MB of RAM, but actually only probes for up to 30. The fix is in <span>fix_mem_limit.patch</span>.
</p>
<h4>Ultrix Loader</h4>
<p>At this point, the kernel was loading far enough to panic about not finding the root filesystem, so it was time to figure out a good way to make this work. The problem is that Ultrix uses a completely different partitioning system than the well-familiar MBR I had been using. The Ultrix "disklabel" allows for 8 "partitions" but with some assumptions, like that the first (caled "a") is always the rootfs, the second (called "b") is always swap, the third ("c") always covers the entire disk (yes it does and is expected to overlap others), and another one ("g") is <span>/usr</span>. Now, if this was not fun enough yet, the partition table itself is expected to be <em>inside</em> the rootfs partition, and a whole lot of tools (including the installer) assume that this all starts at sector zero. Fun, eh?
</p>
<p>I spent a lot of time trying to figure out how to make the installer be happy to not start the rootfs at the 0th sector, but this was a lost cause. A large number of scripts involved assume that both the "a" and the "c" partition start at zero. The kernel also has similar assumptions. With some patching, I got it to work with an offset, but this was not a good approach. I decided to see if I could live with how Ultrix does things, instead of trying to force it to do things my way. Even though the rootfs and the partition table both start at the 0th sector, they both reserve some space up front for "boot code". Specifically, the first 16 sectors (8KB) are always free. I decided to simply place my loader there and teach it how to understand the Ultrix disklabel. As part of this work, I refactored the loader into a few pieces. One part was a partition table handler. There is an option for MBR, one for Ultrix, and one for NetBSD disk labels. One of these (build-time determined) is linked in to the loader, as needed. Another module was a binary loader. Two exist: ELF for Linux and NetBSD, and COFF for Ultrix. Same as before, only one is linked into the loader, as needed. The third modue is the filesystem driver. There is one for FAT12/16/32 (used for my Linux boot sequence), one for old UFS (for Ultrix), and one for modern UFS (for NetBSD). Again, just one is linked in, as needed.
</p>
<p>The cool part now is that I can mix and match these pieces as needed to create a loader for the OS I want to boot. The Linux loader is thus <span>FAT + ELF + MBR</span>, for Ultrix, the loader is <span>UFS.old + COFF + Ultrix disklabel</span>, and for NetBSD, it is <span>UFS.new + ELF + NetBSD disklabel</span>. I was too lazy to implement proper CD-booting, so installing Ultrix is a bit weird. I make a disk image with just the installer kernel (extracted from the CD), in a FAT partition, attach the CDROM to the emulator, and then boot. The installer will then re-partition the disk. For this, yet another loader combination is used: <span>FAT + COFF + MBR</span>. The modularity pays for itself!
</p>
<h3>Making Ultrix work</h3>
<p><a href="http://dmitry.gr/images/linuxCardUltrixUiBig.png"><img src="http://dmitry.gr/images/linuxCardUltrixUiSmall.jpg" alt="Ultrix UI fully booted with a graphical paint program and a terminal open"></a>
<a name="_TOC_42badd9e49002a3cefeaaf28867add83"></a></p><h4>Framebuffer</h4>
<p>Once I had the Ultrix kernel booting properly, at least in the PC build of uMIPS, I <em>really</em> wanted to get the GUI working. Who wouldn't‽ The framebuffer came in two varieties for this machine. There was a monochrome one and a 8-bit color one. They both supported hardware cursor as well. I implemented most of the normally-used modes in the cursor hardware, but not any test modes. I emulated both the framebuffer types and they both work! The 8-bit framebuffer can display up to 259 colors onscreen at a time, out of a 24-bit palette. That is not a typo. The display itself can display 256 colors, and the cursor has its own 3-entry palette, which need not use any of the same colors. The resolution is 1024x1024 in memory, and 1024x864 onscreen. The remaining memory is free for the OS to use however it wishes. I steal memory from the main RAM, same as for the SII buffer. 128KB is used for the mono framebuffer, and a whole megabyte for the color one. The palette is also stored in stolen ram (just about a kilobyte).
</p>
<h4>Mouse, Keyboard, ... and Tablet</h4>
<p>Of course, to make this work, I also had to make the keyboard and mouse work. They talk to the DECstation via serial, and the protocol is somewhat known, from various shreds available online. I was able to put together a passable keyboard emulator rather quickly. It is <em>not</em> a dumb keyboard. It has regions of keys, a bell, some lights, and can support differing autorepeat settings per key group. It is actually pretty cool. The mouse is a pretty basic one, with three buttons. I got that working rather quickly. The problem with emulating mice is a well known one - they are relative device, and most OSs apply acceleration to the mouse as you keep moving it to allow for better reach. Now, if you are running another OS, and passing these accelerated movements to it, it will re-accelerate them even more. This ends up being a mess. This is why most virtualization solutions prefer to load an absolute-pointing-device driver into the guest. I was not prepared to hack up Ultrix or find a way to load a different mouse driver in it. But then I noticed that DEC wrote about a "graphical tablet" that they were selling, that hooked up to the mouse port. Could it be that Ultrix supports this? Yup... Ultrix does. I wrote an emulator for the tablet and it worked wondefully - no more over-accelerated mouse for me! Sweet!
</p>
<h4>Patches</h4>
<p>Ultrix assumes that it is booting on a real DECstation2100/3100, and that includes expecting the CPU to have caches. My virtual CPU does not expose caches to the guest OS, and while Linux handles that fine, Ultrix does not. It correctly probes the cache and finds its size as zero. But there is a logic bug in <span>r3_kn01flush_cache</span>, where if the cache size is zero, it gets into an almost-infinite loop. As uMIPS exposes no cache, it makes sense to patch the function away into just a return. There is another function of interest: <span>kn01delay</span>. It is used for short busy-wait delays when dealing with hardware. All of our virtual hardware is instant-fast, and thus no delays are needed. As long as I am patching a kernel, might as well make it faster. There is also a third area of interest - the periodic timer. In Linux, I was able to change the tick to 16Hz, but I cannot build Ultrix from source, so I cannot modify it easily. Ultrix uses a 256Hz tick. At that rate, on uMIPS hardware we'd never get any useful work done while only handling interrupts. I attempted to patch Ultrix to use a 16Hz timer and account for it correctly. This does not work - there are mathematical errors that happen. 64Hz works, but that is still too freqent for the uMIPS hardware to be usefully fast. I ended up patching the init code to set the timer to 16Hz, but accounting code to act like it is 64Hz. This means that "realtime" in Ultrix runs 4x slower than actual real time, but this is not really a big deal. Just keep in mind that a <span>sleep 1</span> will delay 4 seconds and not 1.
</p>
<p>So how does one even apply such patches? How does one find the proper places to patch? I spent a <em>LOT</em> of time learning about the barely-documented symbol format used in the Ultrix kernel. It worked! I made a working parser for it and was able to properly identify the symbols I needed and to patch the places that needed patching. This was good until I realized that while the installer kernel does ship with symbols, the kernel installed for first boot does not (after first boot, the kernel is recompiled again, with options you choose, and that version DOES have symbols). No symbols means that I cannot use them to find the proper locations to patch. I decided on a different method - binary matching. Look for the proper set of bytes in a row, it should be unique in the kernel. If you find just one case - it's the right one. To save space in the loader (as it is limited to 8KB), I compress the "pattern to look for" cleverly. Cool. This is the final approach I used and you can see it in <span>loadUltrix.c</span>.
</p>
<h3>Improvements in the emulator</h3>
<h4>USB improvements</h4>
<p>After a lot of googling, I learned about interface association descriptors. Turns out that without them, windows will not load the USB CDC-ACM drivers for a device. After adding them, Windows would properly load the driver and it would show up as a COM port. I also learned about the peculiar ways that Windows enumerates devices. Sometimes it'll ask for a descriptor, stating that it'll accept 64 bytes, but after receiving just one 8-byte packet it will reset the bus. This was breaking my USB code, and this is now fixed. Windows now properly supports uMIPS and shows it as two COM ports. Sweet!
</p>
<h4>More perf improvements</h4>
<p>At the end of emulating every instruction, the emulator jumps "to the top", fetching a new instruction to execute. In most cases before this a check is done for whether there is an interrupt pending. This jump was done using a <span>BL</span> - the only long-distance branch available on the Cortex-M0. It takes 3 cycles. The check involved loading a byte from memory (2 cycles), checking if it is zero (1 cycle), and jumping to the interrupt exception creation code if so (1 cycle if not - the common case). That means that the entire "jump and begin handling the next instruction" step took 6 cycles. I wanted to make it faster somehow. I decided that if I could free up a register, I could. Some reworking freed <span>r11</span>. There is a parameter you can pass to gcc to tell it to not use a given register in any C code it compiles: <span>--ffixed-r11</span>. Now that this register is not being used by anyone ever, we can do the clever thing. We keep the address of the "load next instruction and execute it" label in it. Now we can jump to it using just <span>bx r11</span>. This takes just 2 cycles - 4 cycles saved per virtual instruction - a significant speed up. But what if we do have a virtual interrupt to report? Whenever we have one to deliver, we just set <span>r11</span> to point to the "report a virtual interrupt" label, and whenever the current virtual instruction is done being emulated, the interrupt will be reported and <span>r11</span> will be reset. There is a bit more machinery needed to make this work, but this is it in general terms, and it does work!
</p>
<p>I also changed how the TLB hash works (from a table of 32-bit pointers to a table of 8-bit indices) to make the table and each entry smaller (from 24 bytes to 16). This saved a bit under a kilobyte of RAM, which I was able to allocate to the L2 cache. It has now grown from 1.25KB to a full 2KB for a measurable perf improvement!
</p>
<h4>Removing the TLB refill fast path</h4>
<p>For Linux, I had implemented a fast-path for the TLB refill code - it executed in native code what he TLB refill handler would do. In my measurements it slightly improved performance. With all the other performance improvements I had implemented, it no longer offered a measurable improvement. Plus, it did not help Ultrix at all, by definition. Removing it saved flash space and removed complexity. Less complexity is always better. It is gone.
</p>
<h4>Cache geometry changes</h4>
<p>Previously, when profiling to find the best L1i geometry, I used the Linux boot process. I decided to try harder. Now I profiled that, gcc compiling some code, a few other Linux binaries, Ultrix boot, and some Ultrix userspace utilities. The result of this investigation was that a direct-mapped L1i is slightly faster than a 2-way L1i cache. The hit rate goes down slightly, but checking only one cache line instead of two speeds up the checking enough to make up for it. I thus reconfigured the cache as a direct-mapped cache.
</p>
<h4>Serial improvements</h4>
<p>Previously, the emulator would wait a fixed 20ms to send a character to the PC before giving up. I changed this to a permanent wait for the main console. This allows the user to not miss any output if they close their terminal. The emulator also shows its version up front, since it will definitely not be missed now. As of firmware v2.1.1, uMIPS also shows the RAM configuration in terms of the number of chips, each chip's size, and the bit width of the per-chip interface.
</p>
<h3>More Floating Point Unit work</h3>
<p>I had already implemented a full virtual FPU, but now I wanted to see how necessary it really was. I knew that Linux would run if I emulated no FPU at all and would emulate it. I wanted to see if Ultrix would. It did not - it crashed with an invalid instruction trap in the kernel. This was not all that surprising. Once again, it was compiled for a particular machine - a machine that had an FPU. Its assumption that an FPU exists was sane. But there was still more to investigate. The MIPS spec says that the FPU may refuse to execute any instruction if it is not sure that it can perform it perfectly accurately. Since the spec is not clear on what that really means, basically any OS running on such a MIPS chip must implement a complete FPU fallback, capable of emulating any FPU instruction. But then why am I hitting an exception?
</p>
<p>The trick is that the FPU must still exist, it must refuse to do math. This is strictly different from not existing at all. I thus implemented a "minimal" FPU. It implements the instructions to identify itself, move data in and out of the floating point registers, and load and store floating point registers to memory. Any attempts to do actual floating point math report a "coprocessor usage exception" which is the proper way for the FPU to refuse to do math. This worked correctly for Ultrix - it now will not crash at boot, all applications that do floating point math still run, with the kernel emulating the math. I checked and Linux also supports this setup. Thus uMIPS now has three FPU configs that it can be built with: <span>full</span>, <span>minimal</span>, and <span>none</span>.
</p>
<h3>A bootloader</h3>
<p>As I handed out more and more of these cards, the update story needed to be improved. Not everyone has a <a href="https://cortexprog.com/">CortexProg</a> lying around to reflash the firmare. I decided to make it simple and require as little user interaction as possible. The bootloader is just under 3K, I allocated 4K of flash to it, and relocated the main firmware to start 4K into the flash. So, how does it work? At boot, the bootloader will minimally initialize the SD card, attempt to find a FAT16 partition on it, see if it contains a properly-sized file called <span>FIRMWARE.BIN</span> on it, and if so, the firmware will be flashed from this file. On error, the error number will be blinked out on the LED, repeatedly. On success, a varying-frequency pattern of the LED will be repeated forever.
</p>
<p>If the card fails to be initialized, if it fails to mount, if the update file does not exist, or if it is not correctly sized, the bootloader will continue to boot the existing firmware, if any exists (some sanity checking is peformed). This means that when you insert a card with my Linux image or the Ultrix image, all will work as expected. Only FAT16 is supported, so some partitioning may be required on larger cards. I can live with that.
</p>

<h3>Hardware improvements</h3>
<h4>v1.3 hardware</h4>
<p><a href="http://dmitry.gr/images/linuxCardSchem2.png"><img src="http://dmitry.gr/images/linuxCardSchemSmall2.jpg" alt="Linux card schematics"></a>
<a href="http://dmitry.gr/images/linuxCardBoard2.png"><img src="http://dmitry.gr/images/linuxCardBoardSmall2.jpg" alt="Linux card board layout"></a></p><p>After reading my original article, a few people wrote in (including in the comments section here, on twitter, and in email) to suggest that maybe I should entirely abandon the shitty SPI units in this chip. Initially I was worried that the SPI unit speed issue was really an IO port speed issue, but a quick test showed that I could toggle a pin at half my CPU clock reliably and get nice square edges. I prototyped bit-banging SPI on the existing board to see what speeds I could attain and it was promising. I then laid out a new board, with different wiring, to allow me to actually use QSPI mode. The images for the new schematics and the layouts are the ones you see here!
</p>
<p>The ATSAMD21 series features a single-cycle IO port. This optional Cortex-M0+ feature is pretty useful for bit-banging. It really is single-cycle-fast. Normal loads and stores take two cycles minimum on a Cortex-M0+, but ones targetting this kind of a unit take just one. That is <em>how</em> I could toggle a pin at half the cpu speed for my test that I had just mentioned.
</p>
<p>With big-banging, the trick is to do as few operations per cycle as possible. Given this, it would be ideal to do minimal bit-twiddling. It would be super-awesome if I could wire up the four QSPI chips to GPIOS numbered 0..15, allowing me to just read/write the bottom 16 bits of the GPIO port for simple access. Alas, this was not meant to be. This chip has no contiguous 16 GPIO pins wired to the physical pins, so I settled for wiring RAM0 to GPIO0..3, RAM1 to GPIO4..7, RAM2 to GPIO8..11, and RAM3 to GPIO14..17. Since I will be driving them all together, the clock and chip select lines are all wired together. After all was said and done, after the assembly was coded, and the dust settled, I was able to get around a 9MHz clock speed on average. Since the command and address are also sent 4-bits-wide, the speed increase is nice. Previously (using hardware SPI) it took around 8 microseconds to read/write 32 bytes, now it took just under 4 microseconds. A nice speedup.
</p>
<p>An astute reader might notice that the first three RAMS <em>ARE</em> on consecutive GPIO pins. Three is not of much use to us, as it is not a power of two, but <em>two</em>... Yes indeed using only two RAMs i can attain faster speeds (but at half the width). The actual time to read/write 32 bytes is around 5 microseconds. Given this, I decided to re-add the previously-removed support for using less than 4 RAMs on the board. And I did. The newest firmware now supports 1, 2, or 4 RAMs populated on the new boards. I then went futher, and re-added this support for the old boards. That is not as well optimized - it is in C, not ASM, but good enough to play with. This will allow assembling these boards cheaper. Plus, Ultrix happily boots and runs in 4MB (it does need 5MB to start the GUI though).
</p>
<h4>And old hardware too</h4>
<p>I did not want to maintain two separate-but-almost-equal branches of code for the older v1.2 hardware and the new v1.3 hardware. There was also no easy way to tell them apart in software from first glance. But a bit more investigation does provide an idea. The wiring for the RAMs is different enough that we can try each way and see if we detect a plausible RAM chip. It helps that not having RAM0 populated is never supported. This is precisely what I did, in fact. I tried both configs and see which produces a valid-looking ID from RAM0. From there, all four RAMs are probed, identified, and a configuration is picked.
</p>
<p>Support for less than 4 populated RAMs raises a few interesting questions. For speed, all RAMs are treated as if they are the same size, so the size of the smallest RAM determines the total amount of available RAM. This is because I stripe the data across them, of course. So, what if RAM0 is populated with 8MB, and RAM1 with 2MB? We could use just RAM0 and get 8MB of RAM or we could use both and get just 4MB, but faster, since more RAMs in parallel is always faster. I decided that more RAM is better than faster RAM, so in case of such conflicts, more RAM is always chosen. When there is a tie, the faster configuration is used, eg: 4MB, 1MB, 1MB, 1MB RAMs populated add up to 4MB in both the x1 and x4 configs. In this case the x4 config will be chosen and all the RAMs will be used.
</p>
<h3>Building from source (updated)</h3>
<h4>The emulator</h4>
<p>A new parameter called <span>FPU</span> is now passed to uMIPS build to specify the FPU type desired. Options are: <span>none</span> - no FPU at all, Ultrix will not like this but it makes the smallest image; <span>minimal</span> - an FPU that can store values but refuses to do math - Ultrix and Linux will support this, it is slightly larger; and <span>full</span> - a full FPU that does all the math - the fastest option that bloats the Cortex-M0 image by 17KB or so.
</p>
<h4>The loader</h4>
<p>To build the proper loader, pass the <span>BUILD</span> parameter to <span>make</span>. The options are <span>linux</span>, <span>ultrix</span>, <span>ultrix_install</span>, or <span>netbsd</span>. The install loader is just for clean installs, which you have no reason to do since I already did it for you. The netbsd one is to attempt boots of NetBSD on this machine, as it is supported by NetBSD. The proper loader needs to be built and integrated into a disk image for a working system.
</p>
<p>The integration step also changed, <span>mkdisk.sh</span> is gone, replaced by a number of different tools, depending on the intended system. They are: <span>mkdisk-linux.sh</span>, <span>mkdisk-netbsd.sh</span>, <span>mkdisk-unix.sh</span>, and <span>mkdisk-unixinstall.sh</span>. Unix here refers to Ultrix, of course. The scripts are small and self explanatory. Open them for more details. They all operate on a disk image called "disk"
</p>
<p>To enable GUI in Ultrix, the env variable "console" needs to be properly set. In <span>loader.c</span>, find it and set it to "0,0" for text mode or "1,0" for console mode.
</p>
<h3>Further Updates</h3>
<h4>Firmware v2.1.1</h4>
<p>In this version, BBQSPI memory access sped up by 11% for 4-chip case, 6% for others. Ram config shown on boot.
</p>
<h4>Firmware v2.2.0</h4>
<p>In this version, the bootloader was updated to better support other ATSAMD21 parts, including those with more flash &amp; RAM. It now also exposes a version byte at offset <span>0x08</span>. The previous bootloader was version <span>0x10</span>, making this one version <span>0x11</span>. The version will be shows on the serial console at boot now.
</p>
<p>Also, as ATSAMDA1E16 is now apparently out of stock everywhere, I added support for <a href="https://octopart.com/atsamd21e17a-au-microchip-77761547?r=sp">ATSAMD21E17A-AU</a>/<a href="https://octopart.com/atsamd21e17a-aut-microchip-77761548?r=sp">ATSAMD21E17A-AUT</a>. The sad news is that this non-automotive part does not overclock nearly as well. It gets unstable much past 76MHz, so I decided to clock it at 72MHz. It does have more RAM (16KB), which allowed me to allocate a lot more memory to L1i and L2 caches. In most measurements, the performance loss due to lower speed is papered over by the gains of larger caches.
</p>
<p>On the topic of performance, I also rewrote the L2 cache code in assembly for speed and size gains. The speed gains are significant. For extra speed, there is now an option to move the actual access functions to RAM (which is faster than flash). This gains an extra 8% speed, but at the cost of using RAM. On the old 8KB-of-RAM parts this is not always worth it, since it necessitates shrinking the L2 from 2KB to 1.625KB to make space. On the new 16KB-of-RAM parts, though, it is we;ll worth it. It should be noted that there are 6 variants of RAM access low level functions as there are 2 possible access types (SERCOM or bit-banged) and 3 possible chip counts (1, 2, or 4). Only the ones you plan to use need to be moved to RAM. Others will still work from flash, if you want to make a universal firmware. The firmware I provide now moves the 4-chip bit-banged functions to RAM for ATSAMD21E17. See <span>RAM_FUNCS_IN_RAM</span> in the Makefile and the contents of <span>spiRamAtsamd21.c</span>
</p>
<p>While moving functions to RAM, it is easy to accidentally use too much RAM and end up with random crashes as stack collides with data. These are a pain to debug, so I decided to improve this experience. As an option in the Makefile, there is now ability to enable <span>STACKGUARD</span>. What does this do? As the very last word in the pre-allocated RAM (and thus the very first one that stack would overflow over) the code will keep a magic cookie, whose value depends on the current <span>ticks.hi</span> value. This value is checked and updated in the <span>SysTick</span> interrupt which happens every 16 million cycles. If the check fails, an error will be blinked out the LED and the execution will be halted. 
</p>
<p>Starting with this version, the proper make incantations are now <span>make CPU=atsamda1e16</span> and <span>make CPU=atsamd21e17</span>
</p>
<p>The downloads have been updated with the new code and binaries for both chip types. They can be updated using the bootloader and an SD card
</p>





<h2>In conclusion</h2>
<h3>Acknowledgements</h3>
<p>I send a great many thanks to my cats for cutely lying under my table to keep me company during the many hours spent on this project. I send a giant, Mount Rushmore-sized middle finger to Atmel for this sorry excuse of a chip. <b>UPDATE:</b> I even gave up on using the SPI units here, so at this point, I send <em>two</em> of those fingers. Thanks for nothing, Atmel.
</p>
<h3>Downloads</h3>
<p>The source code, gerbers, schematics, and all else <em>except</em> the disk images can be downloaded [<a href="http://dmitry.gr/images/LinuxCard2.7z?v=220">here</a>]. The license on my code is simple: free for all non-commercial use, including using as your own business card. For commercial use (like if you wish to sell kits of this project), <a href="mailto:licensing@dmitry.gr">contact me</a>.
</p>
<p>The disk images (<b>updated</b>) are large so I have no desire to host them on my site, so: [<a href="https://drive.google.com/file/d/14fhdW4Vdz4-ZKucB-iIP4MLTk8OLB7dI/view?usp=sharing">Google Drive</a>] or [<a href="https://mega.nz/file/p9ZWzLrK#saRKVlgBthOFE4Cp-6sb2fMTM7JXtuXMlsYQaaWrEAI">MEGA</a>].
</p>


<!--- We do not show this to the user, but ToC system will index this and we'll get a link to comments in the ToC -->






					
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Be a Thermostat, Not a Thermometer (107 pts)]]></title>
            <link>https://larahogan.me/blog/be-a-thermostat-not-a-thermometer/</link>
            <guid>41516327</guid>
            <pubDate>Wed, 11 Sep 2024 23:50:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://larahogan.me/blog/be-a-thermostat-not-a-thermometer/">https://larahogan.me/blog/be-a-thermostat-not-a-thermometer/</a>, See on <a href="https://news.ycombinator.com/item?id=41516327">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
		
		<p>Originally posted Apr 4, 2023  
			
				• More resources on <a href="https://larahogan.me/resources/communication-team/">communication &amp; team dynamics</a>
			
		</p>

		
			<p>This post originally appeared in my <a href="https://mailchi.mp/wherewithall/be-a-thermostat-not-a-thermometer">newsletter</a>. <a href="http://eepurl.com/dxUybL">Subscribe</a> to receive it!</p>
		

		<p>As I’ve learned more about how humans interact with one another at work, I’ve been repeatedly reminded that we are very easily influenced by the mood of those around us. It’s usually not even something we do consciously; we just see someone using a different tone of voice or shifting their body language, and something deep in our brain notices it.</p>

<p>If you’ve ever attended a meeting where there were some “weird vibes,” you know what I’m talking about. You couldn’t quite put your finger on it, but something about the energy of the room was <em>off</em>—and that feeling affected you, even if it was super subtle.</p>

<p>We’re wired to spidey sense this stuff; this gut instinct is part of what’s helped us stay safe for millenia. Our amygdalas are constantly on the lookout for threats in our environment that could be bad news. Plus, we tend to infer meaning from those weird vibes. Our brain is trying to make sense of the shift in behavior, so we’ll make some (often subconscious) guesses about what’s truly going on. We often even jump to the assumption that those vibes are about us.</p>

<h2 id="humans-mirror-each-other">Humans mirror each other</h2>

<p>If I’m distracted in our one-on-one because I’ve got some stuff happening out of work that you don’t know about, it’s a recipe for misunderstanding. What you might observe is that I’m not making eye contact, I’m suddenly changing the subject, and my arms are crossed. How does your brain make sense of this? It decides that I’m upset with you—without any other information, it’s the most likely reason, of course. :)</p>

<p>Plus, humans, like most other mammals, mirror each other. When I change my tone or my body language, there’s some likelihood that your tone and body language will change in response. So now we’ve got a compounding situation—I’m having a bad day, so I’m giving off strange vibes, then <em>you’re</em> giving off strange vibes because you’re picking up on my bad day. We leave the one-on-one and go meet with other people, and now <em>they’re</em> picking up on our strange vibes.</p>

<p>This cycle is far more noticeable when someone is <a href="https://en.wikipedia.org/wiki/Amygdala_hijack">amygdala-hijacked</a>. It’s tremendously easy to be caught off guard by someone who is overcome with a surprising emotion, and feel triggered by it ourselves. Again, this is just a normal defense mechanism—there is no judgment here.</p>

<h3 id="noticing-a-change-in-someones-behavior">Noticing a change in someone’s behavior</h3>

<p>It takes a lot of practice to recognize when this pattern of shifting and influencing behavior is happening! But once you start paying attention to people’s patterns of behavior (what words do they use when they’re feeling upset? How does their body language change? Do they get louder or quieter? In what situations are they cracking jokes, and in what situations are they more quiet?) you can develop a stronger spidey sense when someone’s “vibes” are different than usual.</p>

<p>In my video course on <a href="https://courses.wherewithall.com/courses/surprising-emotions">Dealing with Surprising Human Emotions</a>, I talk about how to recognize when someone’s behavior seems off, it’s just a signal—just data—that one of their core needs might be being messed with. (Or maybe they simply didn’t get enough sleep last night, or haven’t had coffee yet today!) You can try to see it as a weather vane that something has gone awry for this person. Because once you can transform these signals into data—and not simply mirror the weird vibes back—you have an opportunity to positively affect what happens next.</p>

<h3 id="thermometer-vs-thermostat">Thermometer vs thermostat</h3>
<p>I like to use the metaphor of a thermometer and a thermostat for this idea. If you’re looking for signals about how someone is feeling, it’s kind of like you’re trying to take their emotional temperature. You’re being a thermometer. When they’re subtly giving off weird vibes—they’re frowning, answering your questions with fewer words than normal, etc.—you’ve noticed that their temperature is different. When their amygdala is hijacked, you might see large changes in their behavior (they’re picking a fight with you, going completely silent, skipping your meeting, etc.)—in the thermometer metaphor, they’re running a fever, and you’re picking up on it.</p>

<p>And since we know that one person’s behavior change can cause <em>others</em> to change their behavior in response, we can think of it like they’re being a thermostat: they’re setting the whole temperature for the room. Even if it’s unintentional on both sides. It’s just how we’re wired: to mirror the “vibes” that someone else is giving off.</p>

<p>Rather than let that cycle play out subconsciously, you have an opportunity to become the thermostat as soon as you notice that another person’s temperature has changed. <strong>You</strong> get to set the new temperature of the room, in a positive and healthy way.</p>

<h2 id="being-the-thermostat">Being the thermostat</h2>

<p>Once you’re able to start noticing when someone’s amygdala-hijacked, or simply that the vibes are <em>off</em>, you can reframe and use “be the thermostat, not the thermometer” for good. Since humans tend to mirror each other, you can <strong>intentionally</strong> change the energy in the room, setting the thermostat to a more comfortable temperature.</p>

<h3 id="naming-whats-happening">Naming what’s happening</h3>

<p>One way to reset the temperature is to say out loud, with your mouthwords, that you’ve noticed that the energy has shifted. Here’s a how-to blog post on <a href="https://larahogan.me/blog/skill-naming-whats-happening/">naming what’s happening in the room</a>.</p>

<p>As I mention in that post, there are a few risks to doing this, so you should use your best judgment on whether or not naming what’s happening in the room would be helpful in the moment. You won’t always get it right! Avoid projecting your feelings onto others, or putting them on the defensive, that would make the temperature of the room even more uncomfortable!</p>

<p>If you’re noticing a major shift in someone’s demeanor, instead of guessing what’s going on for them (like “you seem upset”) ask an open question about what they need or how they’re feeling. This way you’ll know if you need to get your thermostat hat on.</p>

<h3 id="choose-your-tone-and-body-language">Choose your tone and body language</h3>

<p>When naming what’s happening or asking open questions, keep what you say short and sweet, and remember to use a calm tone and open body language. I’ve <a href="https://larahogan.me/blog/when-coaching-questions-dont-work/#check-your-tone-and-body-language">written about this before</a>, but it’s definitely worth recapping here, because this is a huge component of being an effective thermostat!</p>

<ol>
  <li>
    <p>Gently <strong>nod</strong> at the pace they’re talking at, or slightly slower. It shows you’re following and tracking what they’re saying.</p>
  </li>
  <li>
    <p>Make <strong>soft eye contact</strong>. Hard eye contact is intense, eyes wide—it’s a little creepy. Soft eye contact is more like a Tyra Banks “smize”—a subtle relaxing of your facial muscles that shows you’re not ready to pounce as soon as they’re done talking. Don’t worry about keeping constant eye contact. Research shows you can break eye contact every 3 seconds naturally, then connect again, and this still feels attentive and affirming to the other person.</p>
  </li>
  <li>
    <p><strong>Lean in</strong>, but not too much. When we’re uncomfortable, we sometimes unconsciously tip away from the person in whatever way we can. This can send a signal that you’re uncomfortable or trying to get out of this conversation ASAP, or even that you are asserting dominance. Make sure you’re squarely facing the person—or if you’re on video, squarely face the camera—and lean in slightly. Even as little as 1” will do the trick! If I’m on Zoom and sitting at my desk, I like to make sure my elbows or wrists are evenly resting on it.</p>
  </li>
  <li>
    <p>Be intentional about the <strong>tone</strong> that you’re using. You’re responsible for communicating that you want to hear what they have to say, and that you’re here to support them. This intentional choice, in combination with your body language cues, will communicate to the other person that you are actively listening. I’ve found that even a subtle change in my tone—like going a little quieter if the other person has gotten a little louder, or adding a little bit of joy to my voice if they seem unsure or a little bit stressed—can reset the temperature in the room.</p>
  </li>
</ol>

<p>There’s a lot more to say about active listening; you can read more in <a href="https://larahogan.me/blog/actively-listening/">this blog post</a>! Your whole goal here is to set or reset the temperature of the room by modeling it with your tone, body language, and word choice.</p>

<p>This skill of intentionally choosing your body language, tone, and words can help the other person move out of whatever “weird vibes” they were giving off earlier, as they can now start mirroring yours. But if it’s a more drastic scenario, like this person is in an amygdala-hijack mode, this approach can also help them feel more heard, understood, and confident that you are decidedly not mad at them.</p>

<p>Usually, this skill does the trick. You smiled a bit, told a little joke that made them chuckle, nodded at the pace that they spoke to indicate you’re listening, and their mood started to change. You’ve just acted as the thermostat in a healthy, intentional way. But in case this doesn’t work, or if this person is in a more lizard-brain state, read on for some additional tools you can try.</p>

<h3 id="offer-a-break">Offer a break</h3>

<p>If it feels like the other person has been amygdala-hijacked, or if they are decidedly stressed or distracted and you sense that there’s no way that the rational, logical part of their brain will be able to return in the next few minutes, use a <a href="https://larahogan.me/blog/get-feedback-from-colleagues/#what-if-you-need-some-processing-time">back-pocket script</a> to offer a pause in the conversation and a plan to return to it later. Some of my favorites to use are:</p>

<ul>
  <li>“I’m not sure how y’all are feeling, but I think I could use some more processing time on this. Could we reconvene again tomorrow at 2pm?”</li>
  <li>“I know how much we want to come to an agreement on this decision today, but my spidey sense is that we might need some more time to think on it. How about we sleep on it and check in again tomorrow?”</li>
  <li>“I really want to support you on this and make sure you feel good about our next steps. How would you feel about us taking a break now to spend some more time thinking it through, and chat again at 4pm?”</li>
</ul>

<p>You’ll notice that the phrasing is intentionally trying to avoid putting someone else on the spot, or make them feel attacked. Your gentleness can help set the new temperature in the room.</p>

<h3 id="what-i-learnedwhat-ill-do">What I learned/What I’ll do</h3>

<p>If you’ve contributed to a big shift in the temperature by creating or escalating an awkward or tense situation, you have an opportunity to own your role as the thermostat here. Because if you never acknowledge it, you’re going to risk developing a forever-antagonistic relationship with them.</p>

<p>Sure, they should just be a grownup and get over it, right? But this does not happen in practice. (When was the last time you, yourself, actually did that?) People hold on to this stuff! Your life is going to be SO MUCH HARDER if you don’t clear the air after you amygdala-hijack someone.</p>

<p>In the <a href="https://courses.wherewithall.com/courses/surprising-emotions">Dealing With Surprising Human Emotions</a> video course, I talk with Jason Wong about this template that we both learned from Paloma Medina :)</p>

<blockquote>
<p>“What I <strong>learned</strong>…”<br>“What I’ll <strong>do</strong>…”</p>
</blockquote>

<p>For example, “What I learned is that that last email didn’t do a good job explaining the changes, so what I plan to do is start a forum for folks to post their questions and our CEO will answer them every Tuesday.”</p>

<p>When said with heartfelt authenticity, this phrase tells people that their needs, feelings, and concerns are not irrelevant. It allows people’s bellies to relax because their needs have been acknowledged. You can begin the work of recovering from the amygdala hijack, because you’ve reset the temperature in the room.</p>

<p>My parents actually taught me to “be the thermostat, not the thermometer”. It’s not always easy, that’s for sure. But by being aware of these cycles, we’re more likely to remember to use our thermostat power for good (not just give up or bail out when you notice someone else is running hot).</p>

<p>Next time you find yourself in a conversation that’s exuding some <em>off</em> vibes, or even an intense one, if you use a combination of these tools, you’ll be giving others the opportunity to mirror the temperature you set right back to you.</p>

	</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Android now allows apps to block sideloading (113 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/09/android-now-allows-apps-to-block-sideloading-and-push-a-google-play-version/</link>
            <guid>41515588</guid>
            <pubDate>Wed, 11 Sep 2024 21:42:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/09/android-now-allows-apps-to-block-sideloading-and-push-a-google-play-version/">https://arstechnica.com/gadgets/2024/09/android-now-allows-apps-to-block-sideloading-and-push-a-google-play-version/</a>, See on <a href="https://news.ycombinator.com/item?id=41515588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">



<article itemscope="" itemtype="http://schema.org/NewsArticle" id="">
      <div>
        <header>
            <h4>
      Only way in now is through the roof    —
</h4>
            
            <h2 itemprop="description">"Select Play Partners" can block unofficial installation of their apps.</h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/get_this_app_play.png" alt="Image from an Android phone, suggesting user " get="" this="" app="" from="" play="" and="" showing="" disjointed="" pieces="" of="" an="" including="" a="" frowning="" emoji-like="" face.="">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2024/09/get_this_app_play.png" data-height="635" data-width="753">Enlarge</a> <span>/</span> It's never explained what this collection of app icons quite represents. A disorganized app you tossed together by sideloading? A face that's frowning because it's rolling down a bar held up by app icons? It's weird, but not quite evocative.</p></figcaption>  </figure>

  




<!-- cache hit 5:single/related:0bed4900a3187d6e128db7810b46a4b0 --><!-- empty -->
<p>You might sideload an Android app, or manually install its APK package, if you're using a custom version of Android that doesn't include Google's Play Store. Alternately, the app might be experimental, under development, or perhaps no longer maintained and offered by its developer. Until now, the existence of sideload-ready APKs on the web was something that seemed to be tolerated, if warned against, by Google.</p>
<p>This quiet standstill is being shaken up by a new feature in Google's Play Integrity API. As <a href="https://www.androidauthority.com/play-integrity-sideloading-detection-3480639/">reported by Android Authority</a>, developer tools to push "remediation" dialogs during sideloading <a href="https://io.google/2024/explore/f757438a-844f-4c59-8dd4-9a5580a5e23d/">debuted</a> at Google's I/O conference in May, have begun showing up on users' phones. Sideloaders of apps from the British shop <a href="https://forum.fairphone.com/t/cannot-install-the-new-tesco-clubcard-app-fp4/93824/10">Tesco</a>, fandom app <a href="https://www.reddit.com/r/Beyblade/comments/16bi9hh/getting_this_message_when_i_open_the_bbx_app/">BeyBlade X</a>, and <a href="https://x.com/AssembleDebug/status/1833700730349515062?t=BYKpcVNNC-8z7Jems7j37Q&amp;s=19">ChatGPT</a> have reported "Get this app from Play" prompts, which cannot be worked around. An Android gaming handheld user encountered <a href="https://www.reddit.com/r/retroid/comments/1dihb9d/how_to_bypass_get_this_app_from_play_on_rp2s/">a similarly worded prompt from <em>Diablo Immortal</em></a> on their device three months ago.</p>
<p>Google's Play Integrity API is how apps have previously blocked access when loaded onto phones that are in some way modified from a stock OS with all Google Play integrations intact. Recently, a popular two-factor authentication app <a href="https://arstechnica.com/gadgets/2024/07/loss-of-popular-2fa-tool-puts-security-minded-grapheneos-in-a-paradox/">blocked access on rooted phones</a>, including the security-minded GrapheneOS. Apps can call the Play Integrity API and get back an "integrity verdict," relaying if the phone has a "trustworthy" software environment, has Google Play Protect enabled, and passes other software checks.</p>                                                                        
                                                                                
<p>Graphene has questioned <a href="https://grapheneos.org/articles/attestation-compatibility-guide">the veracity of Google's Integrity API and SafetyNet Attestation systems</a>, recommending instead <a href="https://developer.android.com/training/articles/security-key-attestation">standard Android hardware attestation</a>. Rahman notes that apps do not have to take an all-or-nothing approach to integrity checking. Rather than block installation entirely, apps could call on the API only during sensitive actions, issuing a warning there. But not having a Play Store connection can also deprive developers of metrics, allow for installation on incompatible devices (and resulting bad reviews), and, of course, open the door to paid app piracy.</p>
<figure><img src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/Screenshot-2024-09-11-at-4.38.04%E2%80%AFPM.png" width="1764" height="864"><figcaption><p>Google</p></figcaption></figure>
<h2>“Unknown distribution channels” blocked</h2>
<p>Google's developer video about "Automatic integrity protection" (at the <a href="https://youtu.be/RccJYep2v5I?si=XMuw3sBdM64cC2BH&amp;t=745">12-minute, 24-second mark on YouTube</a>) notes that "select" apps have access to automatic protection. This adds an automatic checking tool to your app and the "strongest version of Google Play's anti-tamper protection." "If users get your protected app from an unknown distribution channel," a slide in the presentation reads, "they'll be prompted to get it from Google Play," available to "select Play Partners."</p>
<p>Last year, Google introduced <a href="https://arstechnica.com/gadgets/2023/10/android-will-now-scan-sideloaded-apps-for-malware-at-install-time/">malware scanning of sideloaded apps</a> at install time. Google and Apple have come out against legislation that would <a href="https://arstechnica.com/tech-policy/2021/08/apple-and-google-seem-spooked-by-bill-requiring-more-app-stores-and-sideloading/">broaden sideloading rights for smartphone owners</a>, citing security and reliability concerns. <a href="https://arstechnica.com/apple/2024/05/apple-must-open-ipados-to-sideloading-within-6-months-eu-says/">European regulators forced Apple</a> earlier this year to allow for sideloading apps and app stores, though with fees and geographical restrictions in place.</p>

                                                </div>
    </div>

<section>
          
    
    <p>
      <section>
        <a href="https://arstechnica.com/author/kevinpurdy">Kevin Purdy</a>
        Kevin is a senior technology reporter at Ars Technica, covering open-source software, PC gaming, home automation, repairability, e-bikes, and tech history. He has previously worked at Lifehacker, Wirecutter, iFixit, and Carbon Switch.      </section>
    </p>

  </section>

  </article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The first release candidate of FreeCAD 1.0 is out (202 pts)]]></title>
            <link>https://blog.freecad.org/2024/09/10/the-first-release-candidate-of-freecad-1-0-is-out/comment-page-1/#comments</link>
            <guid>41515101</guid>
            <pubDate>Wed, 11 Sep 2024 20:29:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.freecad.org/2024/09/10/the-first-release-candidate-of-freecad-1-0-is-out/comment-page-1/#comments">https://blog.freecad.org/2024/09/10/the-first-release-candidate-of-freecad-1-0-is-out/comment-page-1/#comments</a>, See on <a href="https://news.ycombinator.com/item?id=41515101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>We’ve just pub­lished builds of the first release can­di­date for FreeCAD 1.0. You can down­load them <a href="https://github.com/FreeCAD/FreeCAD-Bundle/releases/tag/1.0rc1">here</a>.</p>



<p>So far, we’ve enjoyed the con­tri­bu­tions of users who are hap­py to be liv­ing on the edge with our week­ly builds and report­ing what­ev­er bugs they run into. That real­ly helped us make the prover­bial edge less&nbsp;edgy.</p>



<p>The inten­tion behind mak­ing release can­di­dates is to give them into the hands of a dif­fer­ent demo­graph­ic — users who usu­al­ly stay away from unsta­ble soft­ware yet are hap­py enough to try very near­ly com­plete soft­ware and report issues they come across.</p>



<p>We are cur­rent­ly down to just 7 release block­ers, but we expect that the release can­di­dates will bump that num­ber up a tad, and that’s a good thing. While we des­per­ate­ly want 1.0 out, deliv­er­ing a real­ly sta­ble release is a big deal for&nbsp;us.</p>



<p>So please down­load the <span>RC1</span>, try it on a real project if you can, give it some real­ly hard time if you must, and report any bugs you see to our <a href="https://github.com/FreeCAD/FreeCAD/issues">issue track­er</a>.</p>



<p>If you are a devel­op­er who is inter­est­ed in con­tribut­ing to the effort, come look at the list of reports, pick an issue that seems triv­ial to resolve, write a fix, and sub­mit a pull request. We have week­ly <a href="https://www.freecad.org/events.php">merge meet­ings</a> on Mon­days to go through the queue of cur­rent­ly open pull requests. You are wel­come to&nbsp;join!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A MiniGolf game for Palm OS (215 pts)]]></title>
            <link>https://ctrl-c.club/~captain/posts/2024-08-29-holy-smokes-I-Just-released-a-minigolf-game-for-palmos-in-2024.html</link>
            <guid>41514944</guid>
            <pubDate>Wed, 11 Sep 2024 20:10:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ctrl-c.club/~captain/posts/2024-08-29-holy-smokes-I-Just-released-a-minigolf-game-for-palmos-in-2024.html">https://ctrl-c.club/~captain/posts/2024-08-29-holy-smokes-I-Just-released-a-minigolf-game-for-palmos-in-2024.html</a>, See on <a href="https://news.ycombinator.com/item?id=41514944">Hacker News</a></p>
<div id="readability-page-1" class="page"><a href="https://ctrl-c.club/~captain/">Back to overview</a>


<p>This is a follow up to my previous post:</p>
<a href="https://ctrl-c.club/~captain/posts/2024-07-18-retro-coding-journey-into-creating-a-palmos-minigolf-game.html">Retro Coding Like It's 1999: My Journey into creating a Palm OS MiniGolf Game</a>

<p>This summer, I embarked on a side project to create a brand-new Palm OS game, and after less than two months of intermittent coding, I'm excited to announce that it's ready to be released to the public!</p>

<p>Let me present to you "Captain's MiniGolf (v0.6)":</p>
<video controls="" src="https://ctrl-c.club/~captain/posts/images/Captains_MiniGolf_Play_Preview.mp4"><a src="images/Captains_MiniGolf_Play_Preview.mp4">Gameplay video</a></video>

<p>Besides hoping to have created a fun little MiniGolf game, the strong point of the game is that you can create your own levels:</p>
<video controls="" src="https://ctrl-c.club/~captain/posts/images/Captains_MiniGolf_LevelEdit_Preview.mp4"><a src="images/Captains_MiniGolf_LevelEdit_Preview.mp4">Level Creation video</a></video>

<p>The game allows you to create your own levelpack databases. Those can be exported and shared with other users, not sure if anyone is going to create these, but it would be fun to see what courses other users can come up with.</p>

<h2>Play it!</h2>
<p>Don't have a Palm OS device? No problem, thanks to the cloudpilot emulator, you can directly play it from the browser (Get a Palm though, you won't regret it) </p>
<a href="https://quarters.captaintouch.com/captainsminigolf.html">Game download and in-browser emulator</a>

<h2>Coding for Palm is sometimes harder than I remember</h2>
<p>Some things I realized while coding this in C:</p>
<p>- You can really mess things up without the hand-holding that you get in modern programming languages</p>
<p>- Memory leaks happen more often than you think</p>
<p>- Debugging polygon shapes and trajectories can be hard, so having a debug build that visualizes some behind-the-scenes logic is a big help</p>

<p>Programming for an old platform like Palm OS can be difficult because of the lack of documentation, but I used the following 2 reference guides to help me out:</p>
<a href="https://stuff.mit.edu/afs/sipb/user/yonah/docs/Palm%20OS%20Companion.pdf">The Palm OS Programmer Companion (part of the Palm OS SDK) </a>
<a href="https://archive.org/details/palmosprogrammin0000fost">Palm OS Programming Bible</a>
<p>There are also some projects up on GitHub of developers that shared the code for their old Palm OS games.</p>

<h2>Why bother?!</h2>
<p>Palm OS devices have always been close to my heart, from the moment I got my Palm M100 to this very day. The simplicity and elegance of devices that can achieve so much with so little is something we've lost over time. When programming for these devices, you inevitably encounter their limitations, but these constraints encourage you to think creatively and find alternative solutions. Is a function too slow or using too much memory? You have to find another way to do it! </p>
<p>With the excessively performant phones we have today, nobody is going to give it a second look to see if a function could be optimized... cpu makes up for coding mediocrity/laziness. </p>

<h2>Known bugs</h2>
<p>- The ball can get stuck in a wall</p>
<p>- If you create a level that has a closed polygon of walls within the main playing field walls, the game can't color the background/course correctly.</p>

<h2>Improvements</h2>
<p>- Add a delete/move level option</p>
<p>- Add a delete and share levelpack option (can be done now using an external application like FileZ)</p>
<p>- Resolution is now fixed to 160x160 (or 320x320 on Palm OS 5 hi-res devices), this should be made dynamic based on the available screen size.</p>

<h2>Sharing is caring</h2>
<p>I am releasing the full source code (GPL3 license) for this game as well, in the hope that this can inspire or help others to create more games for Palm OS.</p>
<a href="https://github.com/captaintouch/Captains_MiniGolf_PalmOS">Captain's MiniGolf source code</a>

<p>The draft for this article was written on my Palm Zire 72.</p>


<p>You can get in touch through Mastodon:  </p>
<a href="https://social.linux.pizza/@rxpz">@rxpz@social.linux.pizza</a>
<p>Holy smokes, I just released a MiniGolf game for Palm OS in 2024 was published on 2024-08-29</p>

<a href="https://ctrl-c.club/~captain/">Back to the overview</a>

<a href="https://ctrl-c.club/~captain/posts.xml" target="_blank" rel="noopener noreferrer">📰 Subscribe to RSS feed 📰</a><br>
<img src="https://ctrl-c.club/~captain/posts/images/mozilla.gif"><img src="https://ctrl-c.club/~captain/posts/images/alienow.gif"><img src="https://ctrl-c.club/~captain/posts/images/cassette.gif"><img src="https://ctrl-c.club/~captain/posts/images/linux_now.gif"><img src="https://ctrl-c.club/~captain/posts/images/ns-best.gif">
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral releases Pixtral 12B, its first multimodal model (148 pts)]]></title>
            <link>https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/</link>
            <guid>41514727</guid>
            <pubDate>Wed, 11 Sep 2024 19:47:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/">https://techcrunch.com/2024/09/11/mistral-releases-pixtral-its-first-multimodal-model/</a>, See on <a href="https://news.ycombinator.com/item?id=41514727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">French AI startup <a href="https://mistral.ai/" target="_blank" rel="noreferrer noopener nofollow">Mistral</a> has released its first model that can process images as well as text.</p>

<p>Called Pixtral 12B, the 12-billion-parameter model is about 24GB in size. Parameters&nbsp;roughly correspond to a model’s problem-solving skills, and models with more&nbsp;parameters&nbsp;generally perform better than those with fewer parameters.</p>

	
	


<p>Built on one of Mistral’s text models, Nemo 12B, the new model can answer questions about an arbitrary number of images of an arbitrary size given either URLs or images encoded using base64, the binary-to-text encoding scheme. Similar to other multimodal models such as Anthropic’s Claude family and OpenAI’s GPT-4o, Pixtral 12B should — at least in theory — be able to perform tasks like captioning images and counting the number of objects in a photo.</p>

<p>Available via a torrent link on <a href="https://github.com/mistralai/mistral-common/releases/tag/v1.4.0" target="_blank" rel="noreferrer noopener nofollow">GitHub</a> and AI and machine learning development platform <a href="https://huggingface.co/mistral-community/pixtral-12b-240910" target="_blank" rel="noreferrer noopener nofollow">Hugging Face</a>, Pixtral 12B can be downloaded, fine-tuned and used under an Apache 2.0 license without restrictions. (A Mistral spokesperson confirmed the license being applied to Pixtral 12B via email.)</p>

<p>This writer wasn’t able to take Pixtral 12B for a spin, unfortunately — there weren’t any working web demos at the time of publication. In a <a href="https://x.com/sophiamyang/status/1833825577398354077" target="_blank" rel="noreferrer noopener nofollow">post on X</a>, Sophia Yang, head of Mistral developer relations, said Pixtral 12B will be available for testing on Mistral’s chatbot and API-serving platforms, Le Chat and Le Plateforme, soon.</p>

<p>It’s unclear which image data Mistral might have used to develop Pixtral 12B.</p>

<p>Most generative AI models, <a href="https://www.cnbc.com/2024/03/06/gpt-4-researchers-tested-leading-ai-models-for-copyright-infringement.html" target="_blank" rel="noreferrer noopener nofollow">including Mistral’s other models</a>, are trained on vast quantities of public data from around the web, which is often copyrighted. Some model vendors argue that “fair use” rights entitle them to scrape <em>any</em> public data, but many copyright holders disagree, and have filed lawsuits against larger vendors like OpenAI and Midjourney to put a stop to the practice.</p>


<p>Pixtral 12B comes in the wake of Mistral closing a $645 million funding round led by General Catalyst that <a href="https://techcrunch.com/2024/05/09/sources-mistral-ai-raising-at-a-6b-valuation-softbank-not-in-but-dst-is/">valued</a> the company at $6 billion. Just over a year old, Mistral — minority owned by Microsoft — is seen by many in the AI community as Europe’s answer to OpenAI. The younger company’s strategy thus far has involved releasing free “open” models, charging for managed versions of those models, and providing consulting services to corporate customers.</p>

	
	


<p><em>Updated 9/11 at 8:11 a.m. Pacific: Clarified that Pixtral 12B is being made available under an Apache 2.0 license, not Mistral’s standard dev license that carries with it certain restrictions on commercial usage.</em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Noisy neighbor detection with eBPF (169 pts)]]></title>
            <link>https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd</link>
            <guid>41513860</guid>
            <pubDate>Wed, 11 Sep 2024 18:11:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd">https://netflixtechblog.com/noisy-neighbor-detection-with-ebpf-64b1f4b3bbdd</a>, See on <a href="https://news.ycombinator.com/item?id=41513860">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a href="https://netflixtechblog.medium.com/?source=post_page-----64b1f4b3bbdd--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Netflix Technology Blog" src="https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://netflixtechblog.com/?source=post_page-----64b1f4b3bbdd--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Netflix TechBlog" src="https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div><p id="3108"><em>By </em><a href="https://www.linkedin.com/in/josefernandezmn/" rel="noopener ugc nofollow" target="_blank"><em>Jose Fernandez</em></a><em>, </em><a href="https://www.linkedin.com/in/sebastien-dabdoub-2a5a0958/" rel="noopener ugc nofollow" target="_blank"><em>Sebastien Dabdoub</em></a><em>, </em><a href="https://www.linkedin.com/in/jason-koch-5692172/" rel="noopener ugc nofollow" target="_blank"><em>Jason Koch</em></a><em>, </em><a href="https://www.linkedin.com/in/artemtkachuk/" rel="noopener ugc nofollow" target="_blank"><em>Artem Tkachuk</em></a></p><p id="8709">The Compute and Performance Engineering teams at Netflix regularly investigate performance issues in our multi-tenant environment. The first step is determining whether the problem originates from the application or the underlying infrastructure. One issue that often complicates this process is the "noisy neighbor" problem. On <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436">Titus</a>, our multi-tenant compute platform, a "noisy neighbor" refers to a container or system service that heavily utilizes the server's resources, causing performance degradation in adjacent containers. We usually focus on CPU utilization because it is our workloads’ most frequent source of noisy neighbor issues.</p><p id="f191">Detecting the effects of noisy neighbors is complex. Traditional performance analysis tools such as <a href="https://www.brendangregg.com/perf.html" rel="noopener ugc nofollow" target="_blank">perf</a> can introduce significant overhead, risking further performance degradation. Additionally, these tools are typically deployed after the fact, which is too late for effective investigation.<em> </em>Another challenge is that debugging noisy neighbor issues requires significant low-level expertise and specialized tooling<em>. </em>In this blog post, we'll reveal how we leveraged <a href="https://ebpf.io/" rel="noopener ugc nofollow" target="_blank">eBPF</a> to achieve continuous, low-overhead instrumentation of the Linux scheduler, enabling effective self-serve monitoring of noisy neighbor issues. You’ll learn how Linux kernel instrumentation can improve your infrastructure observability with deeper insights and enhanced monitoring.</p><h2 id="1e46">Continuous Instrumentation of the Linux Scheduler</h2><p id="792f">To ensure the reliability of our workloads that depend on low latency responses, we instrumented the <a href="https://en.wikipedia.org/wiki/Run_queue" rel="noopener ugc nofollow" target="_blank">run queue</a> latency for each container, which measures the time processes spend in the scheduling queue before being dispatched to the CPU. Extended waiting in this queue can be a telltale of performance issues, especially when containers are not utilizing their total CPU allocation. Continuous instrumentation is critical to catching such matters as they emerge, and eBPF, with its hooks into the Linux scheduler with minimal overhead, enabled us to monitor run queue latency efficiently.</p><p id="048b">To emit a run queue latency metric, we leveraged three eBPF hooks: <code>sched_wakeup</code><strong>, </strong><code>sched_wakeup_new</code><strong>,</strong> and <code>sched_switch</code>.</p></div><div><p id="184b">The <code>sched_wakeup</code><strong> </strong>and <code>sched_wakeup_new</code> hooks are invoked when a process changes state from 'sleeping' to 'runnable.' They let us identify when a process is ready to run and is waiting for CPU time. During this event, we generate a timestamp and store it in an eBPF hash map using the process ID as the key.</p><pre><span id="07d3">struct {<br>    __uint(type, BPF_MAP_TYPE_HASH);<br>    __uint(max_entries, MAX_TASK_ENTRIES);<br>    __uint(key_size, sizeof(u32));<br>    __uint(value_size, sizeof(u64));<br>} runq_enqueued SEC(".maps");<p>SEC("tp_btf/sched_wakeup")<br>int tp_sched_wakeup(u64 *ctx)<br>{<br>    struct task_struct *task = (void *)ctx[0];<br>    u32 pid = task-&gt;pid;<br>    u64 ts = bpf_ktime_get_ns();</p><p>    bpf_map_update_elem(&amp;runq_enqueued, &amp;pid, &amp;ts, BPF_NOEXIST);<br>    return 0;<br>}</p></span></pre><p id="5852">Conversely, the <code>sched_switch</code> hook is triggered when the CPU switches between processes. This hook provides pointers to the process currently utilizing the CPU and the process about to take over. We use the upcoming task's process ID (PID) to fetch the timestamp from the eBPF map. This timestamp represents when the process entered the queue, which we had previously stored. We then calculate the run queue latency by simply subtracting the timestamps.</p><pre><span id="012a">SEC("tp_btf/sched_switch")<br>int tp_sched_switch(u64 *ctx)<br>{<br>    struct task_struct *prev = (struct task_struct *)ctx[1];<br>    struct task_struct *next = (struct task_struct *)ctx[2];<br>    u32 prev_pid = prev-&gt;pid;<br>    u32 next_pid = next-&gt;pid;<p>     // fetch timestamp of when the next task was enqueued<br>    u64 *tsp = bpf_map_lookup_elem(&amp;runq_enqueued, &amp;next_pid);<br>    if (tsp == NULL) {<br>        return 0; // missed enqueue<br>    }</p><p>    // calculate runq latency before deleting the stored timestamp<br>    u64 now = bpf_ktime_get_ns();<br>    u64 runq_lat = now - *tsp;</p><p>    // delete pid from enqueued map<br>    bpf_map_delete_elem(&amp;runq_enqueued, &amp;next_pid);<br>    ....</p></span></pre><p id="58f0">One of the advantages of eBPF is its ability to provide pointers to the actual kernel data structures representing processes or threads, also known as tasks in kernel terminology. This feature enables access to a wealth of information stored about a process. We required the process's cgroup ID to associate it with a container for our specific use case. However, the cgroup information in the process struct is safeguarded by an<a href="https://elixir.bootlin.com/linux/v6.6.16/source/include/linux/sched.h#L1225" rel="noopener ugc nofollow" target="_blank"> RCU (Read Copy Update) lock</a>.</p><p id="0a15">To safely access this RCU-protected information, we can leverage <a href="https://docs.kernel.org/bpf/kfuncs.html" rel="noopener ugc nofollow" target="_blank">kfuncs</a> in eBPF. kfuncs are kernel functions that can be called from eBPF programs. There are kfuncs available to lock and unlock RCU read-side critical sections. These functions ensure that our eBPF program remains safe and efficient while retrieving the cgroup ID from the task struct.</p><pre><span id="769f">void bpf_rcu_read_lock(void) __ksym;<br>void bpf_rcu_read_unlock(void) __ksym;<p>u64 get_task_cgroup_id(struct task_struct *task)<br>{<br>    struct css_set *cgroups;<br>    u64 cgroup_id;<br>    bpf_rcu_read_lock();<br>    cgroups = task-&gt;cgroups;<br>    cgroup_id = cgroups-&gt;dfl_cgrp-&gt;kn-&gt;id;<br>    bpf_rcu_read_unlock();<br>    return cgroup_id;<br>}</p></span></pre><p id="ba4a">Once the data is ready, we must package it and send it to userspace. For this purpose, we chose the eBPF <a href="https://nakryiko.com/posts/bpf-ringbuf/" rel="noopener ugc nofollow" target="_blank">ring buffer</a>. It is efficient, high-performing, and user-friendly. It can handle variable-length data records and allows data reading without necessitating extra memory copying or syscalls. However, the sheer number of data points was causing the userspace program to use too much CPU, so we implemented a rate limiter in eBPF to sample the data.</p><pre><span id="5a42">struct {<br>    __uint(type, BPF_MAP_TYPE_RINGBUF);<br>    __uint(max_entries, RINGBUF_SIZE_BYTES);<br>} events SEC(".maps");<p>struct {<br>    __uint(type, BPF_MAP_TYPE_PERCPU_HASH);<br>    __uint(max_entries, MAX_TASK_ENTRIES);<br>    __uint(key_size, sizeof(u64));<br>    __uint(value_size, sizeof(u64));<br>} cgroup_id_to_last_event_ts SEC(".maps");</p><p>struct runq_event {<br>    u64 prev_cgroup_id;<br>    u64 cgroup_id;<br>    u64 runq_lat;<br>    u64 ts;<br>};</p><p>SEC("tp_btf/sched_switch")<br>int tp_sched_switch(u64 *ctx)<br>{<br>    // ....<br>    // The previous code<br>    // ....</p><p>     u64 prev_cgroup_id = get_task_cgroup_id(prev);<br>    u64 cgroup_id = get_task_cgroup_id(next);</p><p>     // per-cgroup-id-per-CPU rate-limiting <br>    // to balance observability with performance overhead<br>    u64 *last_ts = <br>        bpf_map_lookup_elem(&amp;cgroup_id_to_last_event_ts, &amp;cgroup_id);<br>    u64 last_ts_val = last_ts == NULL ? 0 : *last_ts;</p><p>    // check the rate limit for the cgroup_id in consideration<br>    // before doing more work<br>    if (now - last_ts_val &lt; RATE_LIMIT_NS) {<br>        // Rate limit exceeded, drop the event<br>        return 0;<br>    }</p><p>    struct runq_event *event;<br>    event = bpf_ringbuf_reserve(&amp;events, sizeof(*event), 0);</p><p>      if (event) {<br>        event-&gt;prev_cgroup_id = prev_cgroup_id;<br>        event-&gt;cgroup_id = cgroup_id;<br>        event-&gt;runq_lat = runq_lat;<br>        event-&gt;ts = now;<br>        bpf_ringbuf_submit(event, 0);<br>        // Update the last event timestamp for the current cgroup_id<br>        bpf_map_update_elem(&amp;cgroup_id_to_last_event_ts, &amp;cgroup_id,<br>            &amp;now, BPF_ANY);</p><p>    }</p><p>    return 0;<br>}</p></span></pre><p id="cd03">Our userspace application, developed in Go, processes events from the ring buffer to emit metrics to our metrics backend, <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a">Atlas</a>. Each event includes a run queue latency sample with a cgroup ID, which we associate with containers running on the host. We categorize it as a system service if no such association is found. When a cgroup ID is associated with a container, we emit a percentile timer Atlas metric (<code>runq.latency</code>) for that container. We also increment a counter metric (<code>sched.switch.out</code>) to monitor preemptions occurring for the container's processes. Access to the <code>prev_cgroup_id</code> of the preempted process allows us to tag the metric with the cause of the preemption, whether it's due to a process within the same container (or cgroup), a process in another container, or a system service.</p><p id="1bb4">It's important to highlight that both the <code>runq.latency</code> metric and the <code>sched.switch.out</code> metrics are needed to determine if a container is affected by noisy neighbors, which is the goal we aim to achieve — relying solely on the <code>runq.latency </code>metric can lead to misconceptions. For example, if a container is at or over its cgroup CPU limit, the scheduler will throttle it, resulting in an apparent spike in run queue latency due to delays in the queue. If we were only to consider this metric, we might incorrectly attribute the performance degradation to noisy neighbors when it's actually because the container is hitting its CPU quota. However, simultaneous spikes in both metrics, mainly when the cause is a different container or system process, clearly indicate a noisy neighbor issue.</p><h2 id="b5da">A Noisy Neighbor Story</h2><p id="8be9">Below is the <code>runq.latency</code> metric for a server running a single container with ample CPU capacity. The 99th percentile averages 83.4µs (microseconds), serving as our baseline. Although there are some spikes reaching 400µs, the latency remains within acceptable parameters.</p></div><div><p id="d6c6">At 10:35, launching <code>container2</code>, which fully utilized all CPUs on the host, caused a significant 131-millisecond spike (131,000 microseconds) in <code>container1</code>'s P99 run queue latency. This spike would be noticeable in the userspace application if it were serving HTTP traffic. If userspace app owners reported an unexplained latency spike, we could quickly identify the noisy neighbor issue through run queue latency metrics.</p></div><div><p id="1ce4">The <code>sched.switch.out</code> metric indicates that the spike was due to increased preemptions by system processes, highlighting a noisy neighbor issue where system services compete with containers for CPU time. Our metrics show that the noisy neighbors were actually system processes, likely triggered by <code>container2</code> consuming all available CPU capacity.</p><h2 id="37af">Optimizing eBPF Code</h2><p id="5964">We developed an open-source eBPF process monitoring tool called <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5">bpftop</a> to measure the overhead of eBPF code in this kernel hot path. Our profiling with <code>bpftop</code> shows that the instrumentation adds less than 600 nanoseconds to each <code>sched_*</code> hook. We conducted a performance analysis on a Java service running in a container, and the instrumentation did not introduce significant overhead. The performance variance with the run queue profiling code active versus inactive was not measurable in milliseconds.</p><p id="2b6f">During our research on how eBPF statistics are measured in the kernel, we identified an opportunity to improve the calculation. We submitted this <a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=ce09cbdd988887662546a1175bcfdfc6c8fdd150" rel="noopener ugc nofollow" target="_blank">patch</a>, which was included in the Linux kernel 6.10 release.</p></div><div><p id="3e54">Through trial and error and using <code>bpftop</code>, we identified several optimizations that helped maintain low overhead for our eBPF code:</p><ul><li id="bdd6">We found that <code>BPF_MAP_TYPE_HASH</code> was the most performant for storing enqueued timestamps. Using <code>BPF_MAP_TYPE_TASK_STORAGE</code> resulted in nearly a twofold performance decline. <code>BPF_MAP_TYPE_PERCPU_HASH</code> was slightly less performant than <code>BPF_MAP_TYPE_HASH</code>, which was unexpected and requires further investigation.</li><li id="6517"><code>BPF_MAP_TYPE_LRU_HASH</code> maps are 40–50 nanoseconds slower per operation than regular hash maps. Due to space concerns from PID churn, we initially used them for enqueued timestamps. Ultimately, we settled on <code>BPF_MAP_TYPE_HASH</code> with an increased size to mitigate this risk.</li><li id="0f38">The <code>BPF_CORE_READ</code> helper adds 20–30 nanoseconds per invocation. In the case of raw tracepoints, specifically those that are "BTF-enabled" (<code>tp_btf/*</code>), it is safe and more efficient to access the task struct members directly. Andrii Nakryiko recommends this approach in this <a href="https://nakryiko.com/posts/bpf-core-reference-guide/#btf-enabled-bpf-program-types-with-direct-memory-reads" rel="noopener ugc nofollow" target="_blank">blog post</a>.</li><li id="c6da">The <code>sched_switch</code>, <code>sched_wakeup</code>, and <code>sched_wakeup_new</code> are all triggered for kernel tasks, which are identifiable by their PID of 0. We found monitoring these tasks unnecessary, so we implemented several early exit conditions and conditional logic to prevent executing costly operations, such as accessing BPF maps, when dealing with a kernel task. Notably, kernel tasks operate through the scheduler queue like any regular process.</li></ul><h2 id="d4d7">Conclusion</h2><p id="6194">Our findings highlight the value of low-overhead continuous instrumentation of the Linux kernel with eBPF. We have integrated these metrics into customer dashboards, enabling actionable insights and guiding multitenancy performance discussions. We can also now use these metrics to refine CPU isolation strategies to minimize the impact of noisy neighbors. Additionally, thanks to these metrics, we've gained deeper insights into the Linux scheduler.</p><p id="4565">This work has also deepened our understanding of eBPF technology and underscored the importance of tools like <code>bpftop</code> for optimizing eBPF code. As eBPF adoption increases, we foresee more infrastructure observability and business logic shifting to it. One promising project in this space is <a href="https://github.com/sched-ext/scx" rel="noopener ugc nofollow" target="_blank">sched_ext</a>, which has the potential to revolutionize how scheduling decisions are made and tailored to specific workload needs.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite on Rails: The how and why of optimal performance (206 pts)]]></title>
            <link>https://fractaledmind.github.io/2024/04/15/sqlite-on-rails-the-how-and-why-of-optimal-performance/</link>
            <guid>41513648</guid>
            <pubDate>Wed, 11 Sep 2024 17:49:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fractaledmind.github.io/2024/04/15/sqlite-on-rails-the-how-and-why-of-optimal-performance/">https://fractaledmind.github.io/2024/04/15/sqlite-on-rails-the-how-and-why-of-optimal-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=41513648">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p><img src="https://fractaledmind.github.io/images/wrocloverb-2024/001-alt.png" alt=""></p>

<p>Over the last year or so, I have found myself on a journey to deeply understand how to run Rails applications backed by SQLite performantly and resiliently. In that time, <a href="https://fractaledmind.github.io/2024/04/11/sqlite-on-rails-isolated-connection-pools/">I</a> <a href="https://fractaledmind.github.io/2023/12/11/sqlite-on-rails-improving-concurrency/">have</a> <a href="https://fractaledmind.github.io/2024/01/02/sqlite-quick-tip-multiple-databases/">learned</a> <a href="https://fractaledmind.github.io/2023/09/10/enhancing-rails-sqlite-optimizing-compilation/">various</a> <a href="https://fractaledmind.github.io/2023/09/07/enhancing-rails-sqlite-fine-tuning/">lessons</a> that I want to share with you all now. I want to walk through where the problems lie, why they exist, and how to resolve them.</p>

<p>And to start, we have to start with the reality that…</p>

<p><img src="https://fractaledmind.github.io/images/wrocloverb-2024/022.png" alt=""></p>

<p>Unfortunately, running SQLite on Rails out-of-the-box isn’t viable today. But, with a bit of tweaking and fine-tuning, you can ship a very performant, resilient Rails application with SQLite. And my personal goal for <a href="https://github.com/rails/rails/milestone/87">Rails 8</a> is to make the out-of-the-box experience fully production-ready.</p>

<p>And so, I have spent the last year digging into the details to uncover what the issues are with SQLite on Rails applications as they exist today and how to resolve those issues. So, let me show you everything you need to build a production-ready SQLite-driven Rails application today…</p>

<p><img src="https://fractaledmind.github.io/images/wrocloverb-2024/023.png" alt=""></p>

<p>… Yeah, not too bad, huh? These three commands will set your app up for production success. You will get massive performance improvements, additional SQL features, and point-in-time backups. This is how you build a production-ready SQLite on Rails application today.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/024.png" alt=""></p>

<p>… And that’s all you need. Thank you. And I could genuinely stop the talk here. You know how and why to run SQLite in production with Rails. Those two gems truly are the headline, and if you take-away only 1 thing from this talk, let it be that slide.</p>

<p>But, given that this is a space for diving deep into complex topics, I want to walk through the exact problems and solutions that these gems package up.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/025.png" alt=""></p>

<p>To keep this journey practical and concrete, we will be working on <a href="http://github.com/fractaledmind/wrocloverb-2024">a demo app</a> called “Lorem News”. It is a basic Hacker News clone with posts and comments made by users but all of the content is Lorem Ipsum. This codebase will be the foundation for all of our examples and benchmarks.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/026.png" alt=""></p>

<p>Let’s observe how our demo application performs. We can use the <a href="https://github.com/hatoo/oha"><code>oha</code> load testing CLI</a> and the <a href="https://github.com/fractaledmind/wrocloverb-2024/blob/main/app/controllers/benchmarking_controller.rb">benchmarking routes</a> built into the app to simulate user activity in our app. Let’s start with a simple test where we sent 1 request after another for 5 seconds to our <code>post#create</code> endpoint.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/027.png" alt=""></p>

<p>Not bad. We see solid RPS and every request is successful. The slowest request is many times slower than the average, which isn’t great, but even that request isn’t above 1 second. I’ve certainly seen worse. Maybe I was wrong to say that the out-of-the-box experience with Rails and SQLite isn’t production-ready as of today.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/028.png" alt=""></p>

<p>Let’s try the same load test but send 4 concurrent requests in waves for 5 seconds.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/029.png" alt=""></p>

<p>All of a sudden things aren’t looking as good any more. We see a percentage of our requests are returning 500 error code responses.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/030.png" alt=""></p>

<p>If we look at our logs, we will see the first major problem that SQLite on Rails applications need to tackle …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/031.png" alt=""></p>

<p>… the <a href="https://www.sqlite.org/rescode.html#busy"><code>SQLITE_BUSY</code> exception</a>.</p>

<p>In order to ensure only one write operation occurs at a time, SQLite uses a write lock on the database. Only one connection can hold the write lock at a time. If you have multiple connections open to the database, this is the exception that is thrown when one connection attempts to acquire the write lock but another connection still holds it. Without any configuration, a web app with a connection pool to a SQLite database will have numerous errors in trying to respond to requests.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/032.png" alt=""></p>

<p>As your Rails application is put under more and more concurrent load, you will see a steady increase in the percentage of requests that error with the <code>SQLITE_BUSY</code> exception. What we need is a way to allow write queries to queue up and resolve linearly without immediately throwing an exception.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/033.png" alt=""></p>

<p>Enter <a href="https://www.sqlite.org/lang_transaction.html#immediate">immediate transactions</a>. Because of the global write lock, SQLite needs different transaction modes for different possible behaviors.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/034.png" alt=""></p>

<p>Let’s consider this transaction.</p>

<p>By default, SQLite uses a deferred transaction mode. This means that SQLite will not acquire the lock until a write operation is made inside the transaction. For this transaction, this means that the write lock won’t attempt to be acquired until …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/035.png" alt=""></p>

<p>… this line here, the third operation within the transaction.</p>

<p>In a context where you only have one connection or you have a large amount of transactions that only do read operations, this is great for performance, because it means that SQLite doesn’t have to acquire a lock on the database for every transaction, only for transactions that actually write to the database. The problem is that this is not the context Rails apps are in.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/036.png" alt=""></p>

<p>In a production Rails application, not only will you have multiple connections to the database from multiple threads, Rails will only wrap database queries that write to the database in a transaction. And, when we write our own explicit transactions, it is essentially a guarantee that we will include a write operation. So, in a production Rails application, SQLite will be working with multiple connections and every transaction will include a write operation. This is the opposite of the context that SQLite’s default deferred transaction mode is optimized for.</p>

<p>Our <code>SQLITE_BUSY</code> exceptions are arising from the fact that when SQLite attempts to acquire the write lock in the middle of a transaction and there is another connection holding the lock, SQLite cannot safely retry that transaction-bound query. Retrying in the middle of a transaction could break the serializable isolation that SQLite guarantees. Thus, when SQLite hits a busy exception when trying to upgrade a transaction, it can’t queue that query to retry acquiring the write lock later; it immediately throws the error and halts that transaction.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/037.png" alt=""></p>

<p>If we instead begin the transaction by explicitly declaring this an immediate transaction, SQLite will be able to queue this query to retry acquiring the write lock again later. This gives SQLite the ability to serialize the concurrent queries coming in by relying on a basic queuing system, even when some of those queries are wrapped in transactions.</p>

<p>So, how do we ensure that our Rails application makes all transactions immediate? …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/038.png" alt=""></p>

<p>… As of <a href="https://github.com/sparklemotion/sqlite3-ruby/releases/tag/v1.6.9">version 1.6.9</a>, the <a href="https://github.com/sparklemotion/sqlite3-ruby"><code>sqlite3-ruby</code> gem</a> allows you to configure the default transaction mode. Since Rails passes any top-level keys in your <code>database.yml</code> configuration directly to the <code>sqlite3-ruby</code> database initializer, you can easily ensure that Rails’ SQLite transactions are all run in IMMEDIATE mode.</p>

<p>Let’s make this change in our demo app and re-run our simple load test.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/039.png" alt=""></p>

<p>With one simple configuration change, our Rails app now handle concurrent load without throwing nearly any 500 errors! Though we do see some errors start to creep in at 16 concurrent requests. This is a signal that something is still amiss.</p>

<p>If we look now at the latency results from our load tests, we will see that this new problem quickly jumps out.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/040.png" alt=""></p>

<p>As the number of concurrent requests approaches and then surpasses the number of Puma workers our application has, our p99 latency skyrockets. But, interestingly, the actual request time stays stable, even under 3 times the concurrent load of our Puma workers. We will also see that once we start getting some requests taking approximately 5 seconds, we also start getting some 500 <code>SQLITE_BUSY</code> responses as well.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/041.png" alt=""></p>

<p>If that 5 seconds is ringing a bell, it is because that is precisely what our timeout is set to. It seems that as our application is put under more concurrent load than the number of Puma workers it has, more and more database queries are timing out. This is our next problem to solve.</p>

<p>This timeout option in our <code>database.yml</code> configuration file will be mapped to one of SQLite’s configuration pragmas…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/042.png" alt=""></p>

<p>SQLite’s <a href="https://www.sqlite.org/pragma.html#pragma_busy_timeout"><code>busy_timeout</code> configuration option</a>. Instead of throwing the <code>BUSY</code> exception immediately, you can tell SQLite to wait up to the timeout number of milliseconds. SQLite will attempt to re-acquire the write lock using a kind of exponential backoff, and if it cannot acquire the write lock within the timeout window, then and only then will the <code>BUSY</code> exception be thrown. This allows a web application to use a connection pool, with multiple connections open to the database, but not need to resolve the order of write operations itself. You can simply push queries to SQLite and allow SQLite to determine the linear order that write operations will occur in. The process will look something like this:</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/043.png" alt=""></p>

<p>Imagine our application sends 4 write queries to the database at the same moment.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/044.png" alt=""></p>

<p>One of those four will acquire the write lock first and run. The other three will be queued, running the backoff re-acquire logic. Once the first write query completes, …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/045.png" alt=""></p>

<p>… one of the queued queries will attempt to re-acquire the lock and successfully acquire the lock and start running. The other two queries will continue to stay queued and keep running the backoff re-acquire logic. Again, when the second write query completes, …</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/046.png" alt=""></p>

<p>… another query will have its backoff re-acquire logic succeed and will start running. Our last query is still queued and still running its backoff re-acquire logic.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/047.png" alt=""></p>

<p>Once the third query completes, our final query can acquire the write lock and run. So long as no query is forced to wait for longer than the timeout duration, SQLite will resolve the linear order of write operations on its own. This queuing mechanism is essential to avoiding <code>SQLITE_BUSY</code> exceptions. But, there is a major performance bottleneck lurking in the details of this feature for Rails applications.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/048.png" alt=""></p>

<p>Because SQLite is embedded within your Ruby process and the thread that spawns it, care must be taken to release Ruby’s global VM lock (GVL) when the Ruby-to-SQLite bindings execute SQLite’s C code. <a href="https://github.com/sparklemotion/sqlite3-ruby/issues/287#issuecomment-615346313">By design</a>, the <code>sqlite3-ruby</code> gem does not release the GVL when calling SQLite. For the most part, this is a reasonable decision, but for the <code>busy_timeout</code>, it greatly hampers throughput.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/049.png" alt=""></p>

<p>Instead of allowing another Puma worker to acquire Ruby’s GVL while one Puma worker is waiting for the database query to return, that first Puma worker will continue to hold the GVL even while the Ruby operations are completely idle waiting for the database query to resolve and run. This means that concurrent Puma workers won’t even be able to send concurrent write queries to the SQLite database and SQLite’s linear writes will force our Rails app to process web requests somewhat linearly as well. This radically slows down the throughput of our Rails app.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/050.png" alt=""></p>

<p>What we want is to allow our Puma workers to be able to process requests concurrently, passing the GVL amongst themselves as they wait on I/O. So, for Rails app using SQLite, this means that we need to unlock the GVL whenever a write query gets queued and is waiting to acquire the SQLite write lock.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/051.png" alt=""></p>

<p>Luckily, in addition to the <code>busy_timeout</code>, SQLite also provides the lower-level <a href="https://www.sqlite.org/c3ref/busy_handler.html"><code>busy_handler</code> hook</a>. The <code>busy_timeout</code> is nothing more than a specific <code>busy_handler</code> implementation provided by SQLite. Any application using SQLite can provide its own custom <code>busy_handler</code>. The <code>sqlite3-ruby</code> gem is a SQLite driver, meaning that it provides Ruby bindings for the C API that SQLite exposes. Since it provides <a href="https://github.com/sparklemotion/sqlite3-ruby/blob/055da734dafdbb01bb8cf59dbcdb475ea822683f/ext/sqlite3/database.c#L209-L220">a binding for the <code>sqlite3_busy_handler</code> C function</a>, we can write a Ruby callback that will be called whenever a query is queued.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/052.png" alt=""></p>

<p>Here is a Ruby implementation of the logic you will find in SQLite’s C source for its <code>busy_timeout</code>. Every time this callback is called, it is passed the count of the number of times this query has called this callback. That count is used to determine how long this query should wait to try again to acquire the write lock and how long it has already waited. By using <a href="https://docs.ruby-lang.org/en/master/Kernel.html#method-i-sleep">Ruby’s <code>sleep</code></a>, we can ensure that the GVL is released while a query is waiting to retry acquiring the lock.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/053.png" alt=""></p>

<p>By ensuring that the GVL is released while queries wait to retry acquiring the lock, we have massively improved our p99 latency even when under concurrent load.</p>

<p>But, there are still some outliers. If we look instead at the p99.99 latency, we will find another steadily increasing graph.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/054.png" alt=""></p>

<p>Our slowest queries get steadily slower the more concurrent load our application is under. This is another growth curve that we would like to flatten. But, in order to flatten it, we must understand why it is occurring.</p>

<p>The issue is that our Ruby re-implementation of SQLite’s <code>busy_timeout</code> logic penalizes “older queries”. This is going to kill our long-tail performance, as responses will get naturally segmented into the batch that had “young” queries and those that had “old” queries, because SQLite will naturally segment queries into such batches. To explain more clearly what I mean, let’s step through our Ruby <code>busy_timeout</code> logic a couple times.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/055.png" alt=""></p>

<p>The first time a query is queued and calls this timeout callback, the count is zero.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/056.png" alt=""></p>

<p>And since 0 is less than 12, we enter the <code>if</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/057.png" alt=""></p>

<p>We get the zero-th element in the delays array as our delay, which is 1.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/058.png" alt=""></p>

<p>We then take the first 0 elements of the delays array, which is an empty array, and sum those numbers together, which in this case sums to 0. This is how long the query has been delayed for already,</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/059.png" alt=""></p>

<p>With our timeout as 5000, 0 + 1 is not greater than 5000, so we fall through to the <code>else</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/060.png" alt=""></p>

<p>And we sleep for 1 millisecond before this callback is called again.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/061.png" alt=""></p>

<p>The tenth time this query calls this timeout callback, the count is, well, 10.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/062.png" alt=""></p>

<p>10 is still less than 12, so we enter the <code>if</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/063.png" alt=""></p>

<p>We get the tenth element in the delays array as our delay, which is 50.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/064.png" alt=""></p>

<p>We then take the first 10 elements of the delays array, that is the everything in the array up to but not including the tenth element, and sum those numbers together, which in this case sums to 178. This is how long the query has been delayed for already.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/065.png" alt=""></p>

<p>50 + 178 is still not greater than 5000, so we fall through to the <code>else</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/066.png" alt=""></p>

<p>And now we sleep for 50 milliseconds before this callback is called again.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/067.png" alt=""></p>

<p>Let’s consider the 58th time this query calls this timeout callback.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/068.png" alt=""></p>

<p>58 is greater than 12, so we fall through to the <code>else</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/069.png" alt=""></p>

<p>Once we are past the 12th call to this callback, we will always delay 100 milliseconds.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/070.png" alt=""></p>

<p>In order to calculate how long this query has already been delayed, we get the sum of the entire delays array and add the 100 milliseconds times however many times beyond 12 the query has retried. In this case, the sum of the entire delays array is 328, 58 minus 12 is 46 and 46 times 100 is 4600. So 4600 plus 328 is 4928. Up to this point, our query has been delayed for 4928 milliseconds.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/071.png" alt=""></p>

<p>100 + 4928 is 5028, which is indeed greater than 5000, so we enter the <code>if</code> block.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/072.png" alt=""></p>

<p>And finally we raise the exception.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/073.png" alt=""></p>

<p>I know that stepping through this code might be a bit tedious, but we all need to be on the same page understanding how SQLite’s <code>busy_timeout</code> mechanism handles queued queries. When I say it penalizes old queries, I mean that it makes them much more likely to become timed out queries under consistent load. To understand why, let’s go back to our queued queries…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/074.png" alt=""></p>

<p>Let’s track how many retries each query makes from our simple example above.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/075.png" alt=""></p>

<p>Our three remaining queries have retried once…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/076.png" alt=""></p>

<p>… and now the remaining two queries are, at best, on their second retry.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/077.png" alt=""></p>

<p>And our third query is, again at best, on its third retry. On the third retry, the delay is already 10 milliseconds. Let’s imagine that at this moment a new write query is sent to the database.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/078.png" alt=""></p>

<p>This new query immediately attempts to acquire the write lock, is denied and makes its zeroth call to the <code>busy_timeout</code> callback. It will be told to wait 1 millisecond. Our original query is waiting for 10 milliseconds, so this new query will get to retry again before our older query.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/079.png" alt=""></p>

<p>While the write lock is still held, our new query is only asked to wait 2 milliseconds next.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/080.png" alt=""></p>

<p>Even when the count is 2, it is only asked to wait 5 milliseconds. This new query will be allowed to retry to acquire the write lock <strong>three times</strong> before the original query is allowed to retry <em>once</em>.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/081.png" alt=""></p>

<p>These increasing backoffs greatly penalize older queries, such that any query that has to wait even just 3 retries is now much more likely to never acquire the write lock if there is a steady stream of write queries coming in.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/082.png" alt=""></p>

<p>So, what if instead of incrementally backing off our retries, we simply had every query retry at the same frequency, regardless of age? Doing so would also mean that we could do away with our <code>delays</code> array and re-think our <code>busy_handler</code> function altogether.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/083.png" alt=""></p>

<p>And that is precisely <a href="https://github.com/sparklemotion/sqlite3-ruby/pull/456">what we have done</a> in the <code>main</code> branch of the <code>sqlite3-ruby</code> gem. Unfortunately, as of today, this feature is not in a tagged release of the gem, but it should be released relatively soon. This Ruby callback releases the GVL while waiting for a connection using the <code>sleep</code> operation and always sleeps 1 millisecond. These 10 lines of code make a massive difference in the performance of your SQLite on Rails application.</p>

<p>Let’s re-run our benchmarking scripts and see how our p99.99 latency looks now…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/084.png" alt=""></p>

<p>Voila! We have flattened out the curve. There is still a jump with currency more than half the number of Puma workers we have, but after that jump our long-tail latency flatlines at around half a second.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/085.png" alt=""></p>

<p>So, when it comes to performance, there are 4 keys that you need to ensure are true of your next SQLite on Rails application…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/086.png" alt=""></p>

<p>We have covered the first three, but not the last. The <a href="https://www.sqlite.org/wal.html">write-ahead-log</a> allows SQLite to support multiple concurrent readers. The default <a href="https://www.sqlite.org/lockingv3.html">rollback journal mode</a> only allows for one query at a time, regardless of whether it is a read or a write. WAL mode allows for concurrent readers but only one writer at a time.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/087.png" alt=""></p>

<p>Luckily, <a href="https://github.com/rails/rails/pull/49349">starting with Rails 7.1</a>, Rails applies a better default configuration for your SQLite database. These changes are central to making SQLite work well in the context of a web application. If you’d like to learn more about what each of these configuration options are, why we use the values we do, and how this specific collection of configuration details improve things, I have <a href="https://fractaledmind.github.io/2023/09/07/enhancing-rails-sqlite-fine-tuning/">a blog post</a> that digs into these details.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/088.png" alt=""></p>

<p>Now, while this isn’t a requirement, there is a fifth lever we can pull to improve the performance of our application. Since we know that SQLite in WAL mode supports multiple concurrent reading connections but only one writing connection at a time, we can recognize that it is possible for the Active Record connection pool to be saturated with writing connections and thus block concurrent reading operations.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/089.png" alt=""></p>

<p>If your connection pool only has 3 connections, and you receive 5 concurrent queries, what happens if the 3 connections get picked up by three write queries?</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/090.png" alt=""></p>

<p>The remaining read queries have to wait until one of the write queries releases a connection. Ideally, since we are using SQLite in WAL mode, read queries should never need to wait on write queries. In order to ensure this, we will need to create two distinct connection pools—one for reading operation and one for writing operations.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/091.png" alt=""></p>

<p>We can leverage <a href="https://guides.rubyonrails.org/active_record_multiple_databases.html">Rails’ support for multiple databases</a> to achieve this result. Instead of pointing the reader and writer database configurations to separate databases, we point them at the same single database, and thus simply create two distinct and isolated connection pools with distinct connection configurations.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/092.png" alt=""></p>

<p>The reader connection pool will only consist of readonly connections…</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/093.png" alt=""></p>

<p>And the writer connection pool will only have one connection.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/094.png" alt=""></p>

<p>We can then configure our Active Records models to connect to the appropriate connection pool depending on the role.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/095.png" alt=""></p>

<p>What we want, conceptually, is for our requests to behave essentially like SQLite deferred transactions. Every request should default to using the reader connection pool, but whenever we need to write to the database, we switch to using the writer pool for just that operation. To set that up, we will use Rails’ <a href="https://guides.rubyonrails.org/active_record_multiple_databases.html#activating-automatic-role-switching">automatic role switching feature</a>.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/096.png" alt=""></p>

<p>By putting this code in an initializer, we will force Rails to set the default database connection for all web requests to be the reading connection pool. We also tweak the delay configuration since we aren’t actually using separate databases, only separate connections, we don’t need to ensure that requests “read your own writes” with a <code>delay</code>.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/097.png" alt=""></p>

<p>We can then patch the <code>transaction</code> method of the ActiveRecord adapter to force it to connection to the writing database.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/098.png" alt=""></p>

<p>Taken together, these changes enable our “deferred requests” utilizing isolated connection pools. And when testing against the comment create endpoint, we do see a performance improvement when looking at simple requests per second.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/099.png" alt=""></p>

<p>So, these are the 5 levels of performance improvement that you should make to your SQLite on Rails application.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/100.png" alt=""></p>

<p>But, you don’t need to walk through all of these enhancements in your Rails app. As I said at the beginning, you can simply install <a href="https://github.com/fractaledmind/activerecord-enhancedsqlite3-adapter">the enhanced adapter gem</a>.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/101.png" alt=""></p>

<p>And if you want to use the isolated connection pools, you can simply add this configuration to your application. This is a newer experimental feature, which is why you have to opt into it.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/110.png" alt=""></p>

<p>And, after all that, we are now done with how to make your SQLite on Rails application performant.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/115.png" alt=""></p>

<p>In the end, I hope that this exploration of the tools, techniques, and defaults for SQLite on Rails applications has shown you how powerful, performant, and flexible this approach is. Rails is legitimately the best web application framework for working with SQLite today. The community’s growing ecosystem of tools and gems is unparalleled. And today is absolutely the right time to start a SQLite on Rails application and explore these things for yourself.</p>

<p><img loading="lazy" src="https://fractaledmind.github.io/images/wrocloverb-2024/116.png" alt=""></p>

<p>I hope that you now feel confident in the hows (and whys) of optimal performance when running SQLite in production with Rails.</p>

<p>Thank you.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Algorithmic Wage Discrimination (140 pts)]]></title>
            <link>https://columbialawreview.org/content/on-algorithmic-wage-discrimination/</link>
            <guid>41513417</guid>
            <pubDate>Wed, 11 Sep 2024 17:20:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://columbialawreview.org/content/on-algorithmic-wage-discrimination/">https://columbialawreview.org/content/on-algorithmic-wage-discrimination/</a>, See on <a href="https://news.ycombinator.com/item?id=41513417">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					<div>
				<p>Recent technological developments related to the extraction and processing of data have given rise to concerns about a reduction of privacy in the workplace. For many low-income and subordinated racial minority workforces in the United States, however, on-the-job data collection and algorithmic decisionmaking systems are having a more profound yet overlooked impact: These technologies are fundamentally altering the experience of labor and undermining economic stability and job mobility. Drawing on a multi-year, first-of-its-kind ethnographic study of organizing on-demand workers, this Article examines the historical rupture in wage calculation, coordination, and distribution arising from the logic of informational capitalism: the use of granular data to produce unpredictable, variable, and personalized hourly pay.</p>
<p>The Article constructs a novel framework rooted in worker on-the- job experiences to understand the ascent of digitalized variable pay practices, or the importation of price discrimination from the consumer context to the labor context—what this Article identifies as algorithmic wage discrimination. Across firms, the opaque practices that constitute algorithmic wage discrimination raise fundamental questions about the changing nature of work and its regulation. What makes payment for labor in platform work fair? How does algorithmic wage discrimination affect the experience of work? And how should the law intervene in this moment of rupture? Algorithmic wage discrimination runs afoul of both longstanding precedent on fairness in wage setting and the spirit of equal pay for equal work laws. For workers, these practices produce unsettling moral expectations about work and remuneration. The Article proposes a nonwaivable restriction on these practices.</p>
<p><span>The full text of this Article can be found by clicking the PDF link to the left.</span></p>
			</div> <!--.content-abstract-->
		
					<p>* Professor of Law, University of California, Irvine; Postdoctoral Fellow, Stanford University; Ph.D. 2014, University of California at Berkeley; J.D. 2006, University of California at Berkeley School of Law; B.A. 2003, Stanford University. I thank Aziza Ahmed, Amna Akbar, Abbye Atkinson, Aslı Bâli, Corinne Blalock, James Brandt, Raúl Carillo, Angela Harris, Amy Kapczynski, K-Sue Park, Fernando Rojas, Karen Tani, and Noah Zatz, all of whom offered comments on an early conceptualization of this Article. I am also grateful to Yochai Benkler, Scott Cummings, Sam Harnett, Sarah Myers West, Aziz Rana, Aaron Shapiro, and Meredith Whittaker, who provided critical feedback on drafts and to the brilliant editors of the <em>Columbia Law Review</em>, especially Zakiya Williams Wells. This Article was written at the Center for Advanced Study in the Behavioral Sciences at Stanford University, where I was a fellow from 2022 to 2023. I dedicate it to John Crew, a wonderful mentor and dear friend whose lifelong dedication to justice and fairness shaped both my understandings of and ways of being in this world and who died while I was writing it.</p><!--.content-author-info-->
				
	</div><div id="">

				<h2>INTRODUCTION</h2>
<p>Over the past two decades, technological developments have ushered in extreme levels of workplace monitoring and surveillance across many sectors.
<cite>
	<span>
		<span>1</span>
		See, e.g., Ifeoma Ajunwa, Kate Crawford &amp; Jason Schultz, Limitless Worker Surveillance, 105 Calif. L. Rev. 735, 738–39 (2017) [hereinafter Ajunwa et al., Limitless Worker Surveillance]; Matthew T. Bodie, The Law of Employee Data: Privacy, Property, Governance, 97 Ind. L.J. 707, 712–17 (2022); Brishen Rogers, The Law and Political Economy of Workplace Technological Change, 55 Harv. C.R.-C.L. L. Rev. 531, 535–36 (2020).	</span>
	
</cite>
 These automated systems record and quantify workers’ movement or activities, their personal habits and attributes, and even sensitive biometric information about their stress and health levels.
<cite>
	<span>
		<span>2</span>
		See Ifeoma Ajunwa, Kate Crawford &amp; Joel S. Ford, Health and Big Data: An Ethical Framework for Health Information Collection by Corporate Wellness Programs, 44 J.L. Med. &amp; Ethics 474, 474–75, 477–78 (2016) (describing the comprehensive data collection practices and capacities of worker wellness programs).	</span>
	
</cite>
 Employers then feed amassed datasets on workers’ lives into machine learning systems to make hiring determinations, to influence behavior, to increase worker productivity, to intuit potential workplace problems (including worker organizing), and, as this Article highlights, to determine worker pay.
<cite>
	<span>
		<span>3</span>
		See, e.g., Annette Bernhardt, Linda Kresge &amp; Reem Suleiman, Berkeley Lab. Ctr., Data and Algorithms at Work: The Case for Workers’ Technology Rights 6, 15–17 (2021), https://laborcenter.berkeley.edu/wp-content/uploads/2021/11/Data-and-Algorithms-at-Work.pdf [https://perma.cc/TC3U-458E]. As employment law scholar Matthew Bodie has written in reference the role of data extraction at work under systems of informational capitalism:<br>
Workers find themselves on the wrong end of this data revolution. They are the producers of data, but the data flows seamlessly from their work and personal experience to corporate repositories. Employers can capture the data, aggregate it into meaningful pools, analyze it, and use it to further productivity. Individual employees cannot tap into that value, nor can independent contractors. They are trapped: the more data they provide, the more powerful their employers become.<br>
Bodie, supra note 1, at 736.	</span>
	
</cite>
</p>
<p>To date, policy concerns about growing technological surveillance in the workplace have largely mirrored the apprehensions articulated by consumer advocates. Scholars and advocates have raised concerns about the growing limitations on worker privacy and autonomy, the potential for society-level discrimination to seep into machine learning systems, and a general lack of transparency on workplace rules.
<cite>
	<span>
		<span>4</span>
		See generally Bernhardt et al., supra note 3 (arguing that data-driven technologies harm workers through discrimination and work intensification at the expense of safety, depriving workers of their autonomy and dignity); Ajunwa et al., Limitless Worker Surveillance, supra note 1 (“[T]here has been a shift in focus from collecting personally identifying information, such as health records, to wholly acquiring unprotected and largely unregulated proxies and metadata, such as wellness information, search queries, social media activity, and outputs of predictive ‘big data’ analytics.”); Bodie, supra note 1 (“As the data collected in this new environment has become increasingly individualized, the line between person as individual and person as employee has become significantly blurred.”); Rogers, supra note 1 (“[L]abor and employment laws . . . and the broader political economy of work that they help sustain, also encourage employers to use new technologies to exert power over workers.”). Labor law scholars Antonio Aloisi and Valerio De Stefano have argued convincingly in a comprehensive review of technology, law, and work that concerns about the supposed “disappearance of work” lost to algorithmic intelligence are less urgent than the myriad challenges raised by the incipient practices of algorithmic management at work. These nascent practices, they argue, have intensified any number of problems including the devaluation of work, the maldistribution of risks and privileges, the health and safety of workers, the assault on dignity, and of course, the destruction of individual and collective worker privacy. Antonio Aloisi &amp; Valerio De Stefano, Your Boss Is an Algorithm: Artificial Intelligence, Platform Work and Labour 9, 23–24, 98–101, 104–05 (2022).	</span>
	
</cite>
 For example, in October 2022, the White House Office of Science and Technology Policy released a non-legally-binding handbook identifying five principles that “should guide the design, use, and deployment of automated systems to protect the American public in the age of artificial intelligence.”
<cite>
	<span>
		<span>5</span>
		White House Off. of Sci. &amp; Tech. Pol’y, Blueprint for an AI Bill of Rights 3 (2022), https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf [https://perma.cc/A62C-TV47].	</span>
	
</cite>
 These principles called for automated systems that (1) were safe and effective, (2) protect individuals from discrimination, (3) offer users control over how their data is used, (4) provide notice and explanation that an automated system is being used, and (5) allow users access to a person who can remedy any problems they encounter.
<cite>
	<span>
		<span>6</span>
		Id. at 5–7.	</span>
	
</cite>
 The<em> Blueprint for an AI Bill of Rights</em> (hereinafter <em>Blueprint</em>) specified that these enumerated rights extended to “[e]mployment-related systems [such as] . . . workplace algorithms that inform all aspects of the terms and conditions of employment including, but not limited to, <em>pay</em> or <em>promotion</em>, hiring or termination algorithms, virtual or augmented reality workplace training programs, and electronic workplace surveillance and management systems.”
<cite>
	<span>
		<span>7</span>
		Id. at 53 (emphasis added).	</span>
	
</cite>
</p>
<p>Under each principle, the <em>Blueprint</em> provides “illustrative examples” of the kinds of harms that the principle is meant to address. One such example, used to specify what defines unsafe and ineffective automation in the workplace, involves an unnamed company that has installed AI-powered cameras in their delivery vans to monitor workers’ driving habits, ostensibly for “safety reasons.” The <em>Blueprint</em> states that the system “<em>incorrectly</em> penalized drivers when other cars cut them off . . . . As a result, drivers were <em>incorrectly</em> ineligible to receive a bonus.”
<cite>
	<span>
		<span>8</span>
		Id. at 17 (emphasis added) (citing Lauren Kaori Gurley, Amazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make, Vice (Sept. 20, 2021), https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing-drivers-for-mistakes-they-didnt-make [https://perma.cc/HSF4-EG4M]).	</span>
	
</cite>
 Thus, the specific harm identified is a mistaken calculation by an automated variable pay system developed by the company.</p>
<p>What the <em>Blueprint</em> does not specify, however, is that the company in question—Amazon—does not directly employ the delivery workers. Rather, the company contracts with Delivery Service Providers (DSPs), small businesses that Amazon helps to establish. In this putative nonemployment arrangement, Amazon does not provide to the DSP drivers workers’ compensation, unemployment insurance, health insur-ance, or the protected right to organize. Nor does it guarantee individual DSPs or their workers minimum wage or overtime compensation.
<cite>
	<span>
		<span>9</span>
		As economist Brian Callaci explains, since the DSPs legally employ the delivery drivers, the DSPs, rather than Amazon, bear “liability for accidents or workplace safety,” and DSP drivers, classified as Amazon’s contractors, “do not fall under Amazon’s $15 an hour minimum wage.” Brian Callaci, Entrepreneurship, Amazon Style, Am. Prospect (Sept. 27, 2021), https://prospect.org/api/content/1923a910-1d7c-11ec-8dbf-1244d5f7c7c6/ [https://perma.cc/‌AV2H-59YA]. Meanwhile, Amazon’s contracts with DSPs “[restrict] the wages the DSP can offer” drivers and mandate that drivers remain nonunion by stipulating that “they serve as at-will employees.” Id. If the drivers unionize, “Amazon can terminate the contract and find a new DSP, which is much easier than fighting a union campaign itself.” Id.	</span>
	
</cite>
 Instead, DSPs receive a variable hourly rate based on fluctuations in demand and routes, along with “bonuses” based on a quantified digital evaluation of on-the-job behavior, including “service, safety, [and] client experience.”
<cite>
	<span>
		<span>10</span>
		How Are Amazon DSPs Paid?, Route Consultant, https://www.routeconsultant.com/industry-insights/how-are-amazon-dsps-paid [https://perma.cc/684P-WLKB] (last visited Aug. 14, 2023). The scorecards that determine “bonuses” are calculated in constantly changing ways. The DSP scorecards I reviewed include four categories: safety and compliance, reliability, quality, and team. The “scores” for these categories—and for each driver employed by the DSP—are determined algorithmically. See also Peak Delivery Driver, Amazon DSP Scorecard Deep Dive, YouTube, at 1:04–1:58, 2:48–3:10 (Sept. 10, 2021), https://www.youtube.com/watch?v=-mBOYfBZs9I (on file with the <em>Columbia Law Review</em>). The example in the <em>Blueprint</em>, for instance, lowered the score enough to undermine the DSP’s ability to get a bonus. White House Off. of Sci. &amp; Tech. Pol’y, supra note 5, at 17. By contrast, Amazon is guaranteed the data it wants from the DSPs (they cannot reject the use of cameras, for example)—not just while the DSP is servicing Amazon but also for three years afterward. In addition to using such data to calculate bonuses, Amazon can also use it to terminate contracts, terminate specific “underperforming” workers, and punish DSPs with fees. Josh Eidelson &amp; Matt Day, Drivers Don’t Work for Amazon but Company Has Lots of Rules for Them, Det. News (May 5, 2021), https://www.detroitnews.com/story/business/2021/05/05/drivers-dont-work-amazon-but-company-has-lots-rules-them/4955413001/ [https://perma.cc/7REA-NKRU].	</span>
	
</cite>
 DSPs, while completely reliant on Amazon for business, must hire a team of drivers as employees.
<cite>
	<span>
		<span>11</span>
		When a DSP hires other drivers, it may appear more like a company that is legally separate from Amazon. This may protect Amazon from unionization efforts and downstream liability that it may otherwise incur based on allegations that the DSPs are its employees, not contractors. Callaci, supra note 9. It appears FedEx was the first delivery company to use this tactic after redrafting its contracts with drivers in response to Alexander v. FedEx Ground Package Sys., Inc., 765 F.3d 981 (9th Cir. 2014), the Ninth Circuit decision that held that its drivers were employees, not independent contractors. Rather than changing the drivers’ status in response to the decision, FedEx drafted its contracts to make the drivers appear more like independent contractors. V.B. Dubal, Winning the Battle, Losing the War?: Assessing the Impact of Misclassification Litigation on Workers in the Gig Economy, 2017 Wis. L. Rev. 739, 791–92. This included mandating that the drivers purchase more service areas, which in turn made drivers hire others to complete the deliveries. Id.	</span>
	
</cite>
 These Amazon-created and -controlled small businesses rely heavily on their automated “bonuses” to pay for support, repairs, and driver wages.
<cite>
	<span>
		<span>12</span>
		Lauren Kaori Gurley, Amazon’s AI Cameras Are Punishing Drivers for Mistakes They Didn’t Make, Vice (Sept. 20, 2021), https://www.vice.com/en/article/88npjv/amazons-ai-cameras-are-punishing-drivers-for-mistakes-they-didnt-make [https://perma.cc/HSF4-EG4M].	</span>
	
</cite>
 As one DSP owner–worker complained to an investigator, “Amazon uses these [AI surveillance] cameras allegedly to make sure they have a safer driving workforce, but they’re actually using them not to pay [us] . . . . They just take our money and expect that to motivate us to figure it out.”
<cite>
	<span>
		<span>13</span>
		Id. (internal quotation marks omitted) (quoting the owner of a Washington-based Amazon delivery company).	</span>
	
</cite>
</p>
<p>Presented with this additional information, we should ask again: What exactly is the harm of this automated system? Is it, as the <em>Blueprint</em> states, the algorithm’s <em>mistake</em>, which prevented the worker from getting his bonus? Or is it the structure of Amazon’s payment system, rooted in evasion of employment law, data extraction from labor, and digitalized control?</p>
<p>Amazon’s automated control structure and payment mechanisms represent an emergent and undertheorized firm technique arising from the logic of informational capitalism: the use of <em>algorithmic wage discrimination</em> to maximize profits and to exert control over worker behavior.
<cite>
	<span>
		<span>14</span>
		“Informational capitalism” or “information capitalism” as a descriptor of the contemporary digital-age world system is generally attributed to sociologist Manuel Castells. Castells first introduced the term in his three-volume study, <em>The Information Age</em>, published between 1996 and 1998. In describing a shift from industrial capitalism to information capitalism, Castells wrote in Volume I, “A technological revolution, centered around information technologies, is reshaping, at accelerated pace, the material basis of society. Economies throughout the world have become globally interdependent, introducing a new form of relationship between economy, state, and society, in a system of variable geometry.” Manuel Castells, The Rise of Network Society 1 (2d ed. 2000). In legal scholarship, Julie Cohen uses the term “informational capitalism” to explore the relationships between political, legal, and economic institutions amidst the propertized expansion of data and information exchange. See generally Julie Cohen, Between Truth and Power: The Legal Constructions of Informational Capitalism (2019).	</span>
	
</cite>
 “Algorithmic wage discrimination” refers to a practice in which individual workers are paid different hourly wages—calculated with ever-changing formulas using granular data on location, individual behavior, demand, supply, or other factors—for broadly similar work. As a wage-pricing technique, algorithmic wage discrimination encompasses not only digitalized payment for completed work but, critically, digitalized decisions to allocate work, which are significant determinants of hourly wages and levers of firm control. These methods of wage discrimination have been made possible through dramatic changes in cloud computing and machine learning technologies in the last decade.
<cite>
	<span>
		<span>15</span>
		Zephyr Teachout has created a useful taxonomy of five different forms of “personalized wages” that have recently emerged in the labor market: (1) extreme Taylorism, in which “[h]igh degrees of surveillance [result in] . . . rewarding productivity”; (2) gamification, in which employers use psychological tools to incentivize task completion; (3) behavioral price discrimination, in which workers get paid more if they make certain lifestyle choices, like exercising, which can be tracked through fitness apps; (4) dynamic labor pricing, which, she argues, is based primarily on demand; and (5) experimentation, in which firms test “assumptions about what will lead to the firm gathering the highest output for the wages it pays.” Zephyr Teachout, Algorithmic Personalized Wages, 51 Pol. &amp; Soc’y 436, 437, 442–44 (2023) [hereinafter Teachout, Algorithmic Personalized Wages].<br>
In all these instances, wages are rooted in data extracted from labor. My data indicate the potential to further simplify this taxonomy to two main ways of thinking about algorithmic wage discrimination: (1) wages based on productivity analysis alone (most evident in the employment context), and (2) wages based on productivity, supply, demand, and other personalized data used to minimize labor costs. This second form of algorithmic wage discrimination appears most commonly in on-demand work that treats workers like independent contractors.	</span>
	
</cite>
</p>
<p>Though firms have relied upon performance-based variable pay for some time (e.g., the use of bonuses and commission systems to influence worker behavior),
<cite>
	<span>
		<span>16</span>
		Nonalgorithmic variable payment systems with transparent payment structures are more familiar to many people. See, e.g., United Farm Workers (@UFWupdates), Twitter (Oct. 15, 2022), https://twitter.com/UFWupdates/status/1577795973476220930 (on file with the <em>Columbia Law Review</em>) (showing how California companies use a variable bonus system for some farmers’ pay). They are, nonetheless, controversial. Some critics in the human relations and management literature point to variable pay mechanisms as a contributor to income gaps by gender and race. See, e.g., Emilio J. Castilla, Gender, Race, and Meritocracy in Organizational Careers, 13 Am. J. Socio. 1479, 1502–17 (2008) (finding variable salary bias in salary increases and promotions on the basis of gender, race, and nationality). Others suggest variable pay has psychological costs for workers and other unforeseen consequences. See, e.g., Annette Cox, The Outcomes of Variable Pay Systems: Tales of Multiple Costs and Unforeseen Consequences, 16 Int’l J. Hum. Res. Mgmt. 1475, 1483–93 (2005) (discussing unexpected costs to both employers and employees resulting from variable salary systems).	</span>
	
</cite>
 my research on the on-demand ride hail industry suggests that algorithmic wage discrimination raises a new and distinctive set of concerns. In contrast to more traditional forms of variable pay, algorithmic wage discrimination—whether practiced through Amazon’s “bonuses” and scorecards or Uber’s work allocation systems, dynamic pricing, and wage incentives—arises from (and may function akin to) the practice of “price discrimination,” in which individual consumers are charged as much as a firm determines they may be willing to pay.
<cite>
	<span>
		<span>17</span>
		To date, scholars and analysts who have written about what this Article terms “algorithmic wage discrimination” have predominantly adopted the language of pricing, though they describe wage and not product pricing. For example, in her 2021 Enlund Lecture at DePaul University School of Law, Professor Zephyr Teachout referenced some of these practices as “labor price discrimination.” Zephyr Teachout, Professor, Fordham Univ. Sch. of L., Enlund Lecture at DePaul University School of Law (Apr. 15, 2021). Niels van Doorn, in an article analyzing the pay structures of on-demand Deliveroo riders in Berlin, describes “the algorithmic price-setting power of food delivery platforms,” which he understands as a “monopsonistic power that is not only market-making but also potentially livelihood-taking.” Niels van Doorn, At What Price? Labour Politics and Calculative Power Struggles in On-Demand Food Delivery, 14 Work Org. Lab. &amp; Globalisation, no. 1, 2020, at 136, 138. But adopting the language of “pricing” for wage setting is politically and legally consequential. Since at least the rise of neoliberalism, price controls in the United States (and elsewhere) have been highly disfavored as economic interferences in the “free market,” raising conservative critiques of socialism and “planned economies.” See Benjamin C. Waterhouse, Lobbying America: The Politics of Business From Nixon to NAFTA 10623, 13239 (2013) (describing how American businesses rejected government price setting in the Nixon, Ford, and Carter administrations). Wage controls in the form of minimum-wage and overtime laws, on the other hand, have been contested but culturally naturalized as a necessary (or at least, accepted) part of economic regulation. See Amina Dunn, Most Americans Support a $15 Federal Minimum Wage, Pew Rsch. Ctr. (Apr. 22, 2021), https://www.pewresearch.org/short-reads/2021/04/22/most-americans-support-a-15-federal-minimum-wage/ [https://perma.cc/CX5Z-YX9Z] (surveying support for minimum-wage laws across the United States). In this sense, conceptualizing the digitalized wages received by workers not as firm price determinations but as firm wage determinations is a critical political—and legal—corrective.	</span>
	
</cite>
 As a labor management practice, algorithmic wage discrimination allows firms to personalize and differentiate wages for workers in ways unknown to them, paying them to behave in ways that the firm desires, perhaps for as little as the system determines that the workers may be willing to accept.
<cite>
	<span>
		<span>18</span>
		See infra Part II.	</span>
	
</cite>
 Given the information asymmetry between workers and firms, companies can calculate the exact wage rates necessary to incentivize desired behaviors, while workers can only guess how firms determine their wages.
<cite>
	<span>
		<span>19</span>
		See Aaron Shapiro, Dynamic Exploits: Calculative Asymmetries in the On-Demand Economy, 35 New Tech. Work &amp; Emp. 162, 162–63 (2020) [hereinafter Shapiro, Dynamic Exploits: Calculative Asymmetries] (arguing that “independent service providers” for “on-demand service platforms” are workers and not independent contractors because the platforms set wages and “exhibit substantial information asymmetries”). Uber, for its part, has stated that “suggestions that Uber offers variable pricing based on user-profiling is completely unfounded and factually incorrect.” Cansu Safak &amp; James Farrar, Worker Info Exch., Managed by Bots: Data-Driven Exploitation in the Gig Economy 26 (2021), https://5b88ae42-7f11-4060-85ff-4724bbfed648.usrfiles.com/ugd/5b88ae_8d720d54443543e2a928267d354acd90.pdf [https://perma.cc/TLV3-R2EE] (internal quotation marks omitted) (quoting Letter from Uber Data Protection and Cybersecurity Team to Cansu Safak (Dec. 3, 2021), https://5b88ae42-7f11-4060-85ff-4724bbfed648.usrfiles.com/ugd/5b88ae_f12953beac7e4fd9b6057375cce212b5.pdf [https://perma.cc/LL6M-KVGV]). We have no way to judge the accuracy of this statement.<br>
Since a draft of this Article was posted online, Uber drivers have adopted the term “algorithmic wage discrimination,” testified to how it reflects how they are paid, and documented how they are offered different base pay for the exact same ride when sitting next to each other. See, e.g., The RideShare Guy, The Age of Algorithmic Wage Discrimination for Uber &amp; Lyft Drivers and More?!, YouTube, at 2:16 (Apr. 16, 2023), https://www.youtube.com/watch?v=MfFujB0IY6A (on file with the <em>Columbia Law Review</em>); The RideShare Guy, MORE Algorithmic Wage Discrimination?? Show Me The Money Club, YouTube, at 6:25, 1:01:03 ( June 20, 2023), https://www.youtube.com/watch?v=8mwzsB41-f4 (on file with the <em>Columbia Law Review</em>).	</span>
	
</cite>
</p>
<p>The <em>Blueprint</em> example underscores how algorithmic wage discrimination can be “ineffective” and rife with calculated mistakes that are difficult to ascertain and correct. But algorithmic wage discrimination also creates a labor market in which people who are doing the same work, with the same skill, for the same company, at the same time may receive different hourly pay.
<cite>
	<span>
		<span>20</span>
		See infra Part II.	</span>
	
</cite>
 Digitally personalized wages are often determined through obscure, complex systems that make it nearly impossible for workers to predict or understand their constantly changing, and frequently declining, compensation.
<cite>
	<span>
		<span>21</span>
		See infra Part II.	</span>
	
</cite>
</p>
<p>Drawing on anthropologist Karl Polanyi’s notion of <em>embeddedness</em>—the idea that social relations are embedded in economic systems
<cite>
	<span>
		<span>22</span>
		In 1957, Karl Polanyi wrote,<br>
Instead of economy being embedded in social relations, social relations are embedded in the economic system. The vital importance of the economic factor to the existence of society precludes any other result. For once the economic system is organized in separate institutions, based on specific motives and conferring a special status, society must be shaped in such a manner as to allow that system to function according to its own laws.<br>
Karl Polanyi, <em>The Great Transformation</em>: The Political and Economic Origins of Our Time 60 (Beacon Press 2001) (1944). One interpretation of this important excerpt, as used in this Article, is that Polanyi was referring to the ways in which society adapts to and reorganizes itself “by demanding new social institutions that can constrain market forces and compensate for market failures.” Bob Jessop &amp; Ngai-Ling Sum, Polanyi: Classical Moral Economist or Pioneer Cultural Political Economist?, 44 Östereichische Zeitschrift für Soziologie 153, 158 (2019). This, in essence, is what he calls the “embedded economy”: that in order to prevent a “Hobbesian war of all against all,” a market society must limit—through law, politics, and morality—the range of legitimate activities of economic actors motivated by material gain. Fred Block, Karl Polanyi and the Writing of The Great Transformation, 32 Theory &amp; Soc’y 275, 297 (2003).	</span>
	
</cite>
—this Article excavate the norms around payment that constitute what one might consider a moral economy of work to help situate this contemporary rupture in wages.
<cite>
	<span>
		<span>23</span>
		Various disciplines, including political theory, anthropology, and sociology, have explored the notion of “moral economy” in relationship to labor and work as a way to think about and assess various systems of economic distribution and their impacts on everyday life. See, e.g., William Greider, The Soul of Capitalism 39 (2003) (“The logic of capitalism is ingeniously supple and complete, self-sustaining and forward-looking. Except for one large incapacity: As a matter of principle, it cannot take society’s interests into account.”); James Bernard Murphy, The Moral Economy of Labor 42 (1993) (applying moral reason to the social division of labor and technology); Sharon C. Bolton, Maeve Houlihan &amp; Knut Laaser, Contingent Work and Its Contradictions: Towards a Moral Economy Framework, 111 J. Bus. Ethics 121, 123–124 (2012); Sharon C. Bolton &amp; Knut Laaser, Work, Employment and Society Through the Lens of Moral Economy, 27 Work Emp. &amp; Soc’y 508, 509 (2013) (using a moral economic approach in a sociological inquiry); Marion Fourcade, Philippe Steiner, Wolfgang Streeck &amp; Cornelia Woll, Moral Categories in the Financial Crisis 2 (Max Planck Sciences Po Ctr. on Coping With Instability in Mkt. Societies (MaxPo) Discussion Paper, Working Paper No. 13/1, 2013), https://www.econstor.eu/bitstream/10419/104613/1/757489362.pdf [https://perma.cc/4ZJ4-QYC4] (analyzing the reconfiguration of the moral economy surrounding income inequality in France following the 2008 financial crisis).	</span>
	
</cite>
 Although the United States–based system of work is largely regulated through contracts and strongly defers to the managerial prerogative,
<cite>
	<span>
		<span>24</span>
		See Gali Racabi, Abolish the Employer Prerogative, Unleash Work Law, 43 Berkeley J. Emp. &amp; Lab. L. 79, 82 (2022) (“The employer [or managerial] prerogative is the default governance rule in the workplace . . . .”). This legal deference to the managerial prerogative is controversial in the scholarly literature. See, e.g., id. at 138 (“[P]erhaps the employer prerogative’s most sinister effect is convincing work law movements, scholars, and activists that it is a state of nature, a necessary theoretical benchmark for both pragmatic and normative discussions of work law. It is not.”).	</span>
	
</cite>
 two restrictions on wages have emerged from social and labor movements: minimum-wage laws and antidiscrimination laws. Respectively, these laws set a price floor for the purchase of labor relative to time and prohibit identity-based discrimination in the terms, con-ditions, and privileges of employment, requiring firms to provide equal pay for equal work.
<cite>
	<span>
		<span>25</span>
		At the federal level, the Fair Labor Standards Act of 1938, 29 U.S.C. §§ 201–219 (2018), establishes a national floor for minimum-wage and overtime. Id. at §§ 203, 206, 207. The central federal laws that prohibit wage discrimination based on protected identities or classes are the Equal Pay Act, 29 U.S.C. § 206(d) (requiring that men and women in the same workplace be given equal pay for equal work); Title VII of the Civil Rights Act of 1964, 42 U.S.C. § 2000e-2 (2018) (prohibiting employment discrimination based on race, color, religion, sex, and national origin); the Age Discrimination in Employment Act, 29 U.S.C. §§ 623, 631 (prohibiting employment discrimination based on age for workers older than forty); and the Americans with Disabilities Act, 42 U.S.C. § 12112 (prohibiting employment discrimination based on disability).	</span>
	
</cite>
 Both sets of wage laws can be understood as forming a core moral foundation for most work regulation in the United States. In turn, certain ideals of fairness have become embedded in cultural and legal expectations about work. Part I examines how recently passed laws in California and Washington State, which specifically legalize algorithmic wage discrimination for certain firms, compare with and destabilize more than a century of legal and social norms around fair pay.</p>
<p>Part II draws on first-of-its-kind, long-term ethnographic research to understand the everyday, grounded experience of workers earning through and experiencing algorithmic wage discrimination. Specifically, Part II analyzes the experiences of on-demand ride-hail drivers in California before and after the passage of an important industry-initiated law, Proposition 22, which legalized this form of variable pay. This Part illuminates workers’ experiences under compensation systems that make it difficult for them to predict and ascertain their hourly wages. Then, Part II examines the practice of algorithmic wage discrimination in rela-tionship to workers’ on-the-job meaning making and their moral interpretations of their wage experiences.
<cite>
	<span>
		<span>26</span>
		The social construction of meaning is a central concern of sociologists and anthropologists who seek to account for the variability and diversity of human understandings and experiences. Compare Michèle Lamont, Meaning-Making in Cultural Sociology: Broadening Our Agenda, 29 Contemp. Socio. 602, 603–05 (2000) (offering a detailed taxonomy of sociological literature that takes up how people make sense of their worlds through their experiences of race, ethnicity, immigration, and inequality), with Richard A. Posner, Economic Analysis of Law 3–4 (9th ed. 2014) (describing rationality as grounded within self-interested economic maximization of scarce resources).	</span>
	
</cite>
 Though many drivers are attracted to on-demand work because they long to be free from the rigid scheduling structures of the Fordist work model,
<cite>
	<span>
		<span>27</span>
		Philosopher Antonio Gramsci used the term “Fordism” to refer to an emergent system of material production—routine, intensified labor—under the regime of Ford. But due in large part to corresponding political and economic forces, namely the laws and policies passed in response to upheaval during the Great Depression, the Fordist work structure in much of the mid-twentieth century often corresponded to an hourly (living) wage and a forty-hour work week. See Antonio Gramsci, Americanism and Fordism, in Selections From the Prison Notebooks of Antonio Gramsci 561, 56163 (Quentin Hoare &amp; Geoffrey Nowell Smith eds. and trans., 1999). For more on the demise of Fordism, see generally Luc Boltanski &amp; Ève Chiapello, The New Spirit of Capitalism (2007).	</span>
	
</cite>
 they still largely conceptualize their labor through the lens of that model’s payment structure: the hourly wage.
<cite>
	<span>
		<span>28</span>
		See Michael Dunn, Making Gigs Work: Digital Platforms, Job Quality and Worker Motivations, 35 New Tech. Work &amp; Emp. 232, 238–39, 241–42 (2020) (discussing the motivations of gig workers, including flexible work hours, despite often needing to maintain the same work structures as traditional employment). It should be noted that nothing about employment status necessitates an inflexible work schedule. This is a business decision associated with, not mandated by, employment. For a discussion of the history of businesses contesting the legal rules defining employment status to avoid legal responsibility for basic employment safeguards, see Veena B. Dubal, Wage Slave or Entrepreneur?: Contesting the Dualism of Legal Worker Identities, 105 Calif. L. Rev. 65, 86–88 (2017) [hereinafter Dubal, Wage Slave or Entrepreneur?]. Notably, the passage of California’s AB5 law made it much harder to misclassify workers in this way. See Hannah Johnston, Ozlem Ergun, Juliet Schor &amp; Lidong Chen, Is Employment Status Compatible With the On-Demand Platform Economy? Evidence From a Natural Experiment 6 (2021) (unpublished report) (on file with the <em>Columbia Law Review</em>). When at least one labor platform company, called Bring Your Package, went on to hire their previously contracted workers in anticipation of AB5 restrictions, this transition did not precipitate any reduction in workers’ desired scheduling flexibility nor in firm efficiency. See id. at 14, 24, 26–27.	</span>
	
</cite>
 Workers find that, in contrast to more standard wage dynamics, being directed by and paid through an app involves opacity, deception, and manipulation.
<cite>
	<span>
		<span>29</span>
		These findings comport with research findings from across sociology, communications studies, and media studies literatures on algorithmic management. See, e.g., Antonio Aloisi, Platform Work in Europe: Lessons Learned, Legal Developments and Challenges Ahead, 13 Euro. Lab. L.J. 4, 10–11 (2022) (discussing how platform manage-ment tends to unfold in misleading, opaque ways); Rafael Grohmann, Gabriel Pereira, Abel Guerra, Ludmila Costhek Abilio, Bruno Moreschi &amp; Amanda Jurno, Platform Scams: Brazilian Workers’ Experiences of Dishonest and Uncertain Algorithmic Management, 24 New Media &amp; Soc’y 1611, 1614 tbl.1 (2022) (presenting case studies of the types of dishonesty and deception that workers experience in platform work); Elke Schüßler, Will Attwood-Charles, Stefan Kirchner &amp; Juliet B. Schor, Between Mutuality, Autonomy and Domination: Rethinking Digital Platforms as Contested Relational Structures, 19 Socio-Econ. Rev. 1217, 1224 (2021) (outlining common theories of the position of power that platforms hold over their workers); Steven Vallas &amp; Juliet B. Schor, What Do Platforms Do? Understanding the Gig Economy, 46 Ann. Rev. Socio. 273, 279–81 (2020) (conducting a literature review of the predominant sociological views of platform work, which often conceptualize this work as an extension of existing neoliberal models of work without any of the worker protections); Daniel Susser, Beate Roessler &amp; Helen Nissenbaum, Technology, Autonomy, and Manipulation, 8 Internet Pol’y Rev., no. 2, 2019, at 1, 8 (explaining how gig economy services covertly influence an individual’s decision-making through “online manipulation”).	</span>
	
</cite>
 Those who are most economically dependent on income from on-demand work frequently describe their experience of algorithmic wage discrimination through the lens of gambling.
<cite>
	<span>
		<span>30</span>
		See infra section II.B.	</span>
	
</cite>
 As a normative matter, this Article contends that workers laboring for firms (especially large, well-financed ones like Uber, Lyft, and Amazon) should not be subject to the kind of risk and uncertainty associated with gambling as a condition of their work. In addition to the salient constraints on autonomy and threats to privacy that accompany the rise of on-the-job data collection, algorithmic wage discrimination poses significant problems for worker mobility, worker security, and worker collectivity, both on the job and outside of it. Because the on-demand workforces that are remunerated through algorithmic wage discrimination are primarily made up of immigrants and racial minority workers, these harmful economic impacts are also necessarily racialized.
<cite>
	<span>
		<span>31</span>
		In the United States, such work is conducted primarily by immigrants and subordinated minorities. Lyft estimates that 73% of their U.S. workforce identify as racial minorities. Lyft, Economic Impact Report 5 (2022), https://s27.q4cdn.com/263799617/<br>
files/doc_downloads/esg/Lyft-Economic-Impact-Report-2022.pdf [https://perma.cc/8BUG-NGAV]. One study estimates that in the San Francisco Bay Area in 2019, immigrants and people of color composed 78% of Uber and Lyft drivers, most of whom relied on these jobs as their primary source of income. Chris Benner, Erin Johansson, Kung Feng &amp; Hays Witt, UC Santa Cruz Inst. for Soc. Transformation, On-Demand and On-The-Edge: Ride-Hailing and Delivery Workers in San Francisco, Executive Summary 2 (2020), https://transform.ucsc.edu/wpcontent/uploads/2020/05/OnDemandOntheEdge_ExecSum.pdf [https://perma.cc/DFH8-7VSY]. In addition to the nationwide Lyft data, we know that in New York City, 90% of ride-hail drivers are immigrants, and in Seattle, ride-hail drivers are 50% Black and “nearly three times more likely to be immigrants than all Kings County workers.” James A. Parrott &amp; Michael Reich, Ctr. on Wage &amp; Emp. Dynamics &amp; New Sch. Ctr. for N.Y.C. Affs., A Minimum Compensation Standard for Seattle TNC Drivers 23 (2020), https://irle.berkeley.edu/files/2020/07/Parrott-Reich-Seattle-Report_July-2020.pdf [https://perma.cc/QA9F-FV47] [hereinafter Parrott &amp; Reich, Minimum Compensation Standard]; Ginia Bellafante, Uber and the False Hopes of the Sharing Economy, N.Y. Times (Aug. 9, 2018), https://www.nytimes.com/2018/08/09/nyregion/uber-nyc-vote-drivers-ride-sharing.html (on file with the <em>Columbia Law Review</em>).	</span>
	
</cite>
</p>
<p>Finally, Part III explores how workers and worker advocates have used existing data privacy laws and cooperative frameworks to address or at least to minimize the harms of algorithmic wage discrimination. In addition to mobilizing against violations of minimum-wage, overtime, and vehicle reimbursement laws, workers in California—drawing on the knowledge and experience of their coworkers in the United Kingdom—have developed a sophisticated understanding of the laws governing data at work.
<cite>
	<span>
		<span>32</span>
		See infra Part III.	</span>
	
</cite>
 In the United Kingdom, a self-organized group of drivers, the App Drivers &amp; Couriers Union, has not only successfully sued Uber to establish their worker status
<cite>
	<span>
		<span>33</span>
		Kate Duffy &amp; Theo Golden, Uber Just Lost a Major Legal Battle Over Whether Its UK Drivers Count as Workers and Are Entitled to Minimum Wage, Bus. Insider (Feb. 19, 2021), https://www.businessinsider.com/uber-driver-lost-uk-legal-battle-court-worker-rights<br>
-employment-2021-2 [https://perma.cc/CT27-K2ZP].	</span>
	
</cite>
 but also used the General Data Protection Regulation (GDPR) to lay claim to a set of positive rights concerning the data and algorithms that determine their pay.
<cite>
	<span>
		<span>34</span>
		Jeffrey Brown, In New European Lawsuit, Uber Drivers Claim Company’s Algorithm Fired Them, Geo. L. Tech. Rev. Legal Impressions (Nov. 2020), https://georgetownlawtechreview.org/in-new-european-lawsuit-uber-drivers-claim-companys-algorithm-fired-them/GLTR-11-2020/ [https://perma.cc/887P-RET4] (“The GDPR . . . imposes obligations on companies which collect personal information if that data is related to EU consumers, regardless of the consumer’s physical location in the world. Under Article 22, individuals have ‘the right not to be subject to a decision based solely on automated processing.’” (quoting Council Regulation 2016/679, art. 22, 2016 O.J. (L 119) 1 (EU))).	</span>
	
</cite>
 As a GDPR-like law went into effect in California in 2023, drivers there are positioned to do the same.
<cite>
	<span>
		<span>35</span>
		Cal. Civ. Code § 1798.100 (2023) (imposing limits on businesses’ collection of consumer personal information and requiring notice of the purposes behind data collection).	</span>
	
</cite>
 Other workers in both the United States and Europe have responded by creating “data cooperatives” to fashion some transparency around the data extracted from their labor, to attempt to understand their wages, and to assert ownership over the data they collect at work.
<cite>
	<span>
		<span>36</span>
		See infra section III.B.	</span>
	
</cite>
 In addition to examining both approaches to addressing algorithmic wage discrim-ination, this Article argues that the constantly changing nature of machine learning technologies and the asymmetrical power dynamics of the digitalized workplace minimize the impact of these attempts at trans-parency and may not mitigate the objective or subjective harms of algorithmic wage discrimination. Considering the potential for this form of discrimination to spread into other sectors of work, this Article proposes instead an approach that addresses the harms directly: a narrowly structured, nonwaivable peremptory ban on the practice.</p>
<p>While this Article is focused on algorithmic wage discrimination as a labor management practice in “on-demand” or “gig work” sectors, where workers are commonly treated as “independent contractors” without protections, its significance is not limited to that domain. So long as this practice does not run afoul of minimum-wage or antidiscrimination laws, nothing in the laws of work makes this form of digitalized variable pay illegal.
<cite>
	<span>
		<span>37</span>
		See supra note 25. Antitrust laws, however, are a more promising way to address these practices when and if workers are classified as independent contractors. Part III discusses a California lawsuit filed in 2022 by Rideshare Drivers United workers against Uber alleging that the company’s payment structures amount to price fixing and that it is violating state antifraud laws.	</span>
	
</cite>
 As Professor Zephyr Teachout argues, “Uber drivers’ experiences should be understood not as a unique feature of contract work, but as a preview of a new form of wage setting for large employers . . . .”
<cite>
	<span>
		<span>38</span>
		See Teachout, Algorithmic Personalized Wages, supra note 15, at 437.	</span>
	
</cite>
 The core motivations of labor platform firms to adopt algorithmic wage discrimination—labor control and wage uncertainty—apply to many other forms of work. Indeed, extant evidence suggests that algorithmic wage discrimination has already seeped into the healthcare and engineering sectors, impacting how porters, nurses, and nurse practitioners are paid.
<cite>
	<span>
		<span>39</span>
		For example, a company that brands itself “Uber for Hospitals” has developed AI staffing software for hospitals. This software uses “smart technology” to allocate work tasks and to judge the performance of porters, nurses, and nurse practitioners. See Nicky Godding, Oxford Tech Raises £9 Million for ‘Uber for Hospitals’ AI Platform, Bus. Mag. (May 21, 2020), https://thebusinessmagazine.co.uk/technology-innovation/oxford-tech-raises-9-million-for-uber-for-hospitals-ai-platform/ [https://perma.cc/8593-M9U7] (“Hospitals can use [this technology] to assign tasks to healthcare teams based on their location. . . . This helps to ensure . . . full visibility of vulnerable patient movement between departments, and connects porters directly with staff . . . .”). The technology company’s “performance analysis” may then be used to determine the pay for these healthcare workers. Id.<br>
IBM Japan is also using digital surveillance systems to help set wages for their workers. In 2019, the company introduced human relations software created by Watson to use as a “compensation advisor.” The Japan Metal, Manufacturing, Information and Telecommunication Workers’ Union ( JMITU), which represents IBM Japan workers, requested disclosure of the data the Watson AI acquired and used, an explanation for how it was evaluating workers, and how these evaluations were involved in the wage-setting process. IBM Japan refused to disclose the information. JMITU subsequently lodged a complaint with the Tokyo Labor Relations Commission. The union argues that the software is being used to unfairly target union members. According to one report, “[i]n awarding summer bonuses in June 2019, the individual performance rate assessed by the company was only 63.6% on average for union members, compared to an average of 100% for all [other] employees. In addition, an exceptional 0% assessment was made for many union members.” Hozumi Masashi (ほづみ まさし), AIによる賃金査定にどう向き合うか: 日本IBM事件(不当労働行為救済申立) の報告 [How to Face AI-Based Wage Assessments: Report on the IBM Japan Case (Unfair Labor Practice Relief Petition)], 338 季刊 労·働者の権利[Worker Rights Quarterly], no. 10, 2020, at 101, 102.	</span>
	
</cite>
 If left unaddressed, the practice will continue to be normalized in other employment sectors, including retail, restaurant, and computer science, producing new cultural norms around compensation for low-wage work.
<cite>
	<span>
		<span>40</span>
		See, e.g., Min Kyung Lee, Daniel Kusbit, Evan Metsky &amp; Laura Dabbish, Working With Machines: The Impact of Algorithmic and Data-Driven Management on Workers, in CHI 15: Proceedings of the 33rd Annual CHI Conference on Human Factors in Computing Systems 1603, 1603–04 (2015) (discussing how algorithms used across industries can produce new norms of allocation of, evaluation of, and compensation for work). Companies across the world use wage algorithms in both contracting and permanent employment settings to incentivize certain behaviors. Technology capitalists have foreshadowed its growth. See, e.g., Shawn Carolan, Opinion, What Proposition 22 Now Makes Possible, The Info. (Nov. 10, 2020), https://www.theinformation.com/articles/what-proposition-22-now-makes-possible (on file with the <em>Columbia Law Review</em>) (predicting increased venture capitalist investment in “all sorts of industries” after the passage of Proposition 22). As Tarleton Gillespie has warned regarding the power of algorithms, “[t]here is a case to be made that the working logics of these algorithms not only shape user practices, but also lead users to internalize their norms and priorities.” Tarleton Gillespie, The Relevance of Algorithms, in Media Technologies: Essays on Communication, Materiality, and Society 167, 187 (Tarleton Gillespie, Pablo J. Boczkowski &amp; Kirsten A. Foot eds., 2014).	</span>
	
</cite>
 The on-demand sector thus serves as an important and portentous site of forthcoming conflict over longstanding moral and political ideas about work and wages.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uber drivers in Kenya are ignoring the app and charging their own rates (102 pts)]]></title>
            <link>https://restofworld.org/2024/kenya-uber-rate-cards-fuel-prices/</link>
            <guid>41512328</guid>
            <pubDate>Wed, 11 Sep 2024 15:30:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2024/kenya-uber-rate-cards-fuel-prices/">https://restofworld.org/2024/kenya-uber-rate-cards-fuel-prices/</a>, See on <a href="https://news.ycombinator.com/item?id=41512328">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>Charles has been an Uber driver in Nairobi for nearly five<strong> </strong>years. But the last couple of years have been hard, with stubbornly high gas prices eating into&nbsp;his earnings.</p>



<p>In May, however, Charles, who requested to be identified only by his first name as he feared retribution from Uber, found a solution. Kenya’s Organization of Online Drivers (OOD) — a registered union with about 15,000 members — had begun circulating its own suggested fare chart, with rates that were at least 50% higher than Uber’s official fare. It allowed ride-hailing drivers to demand better rates from customers.&nbsp;</p>



<p>Charles now charges passengers according to the union’s chart, prioritizing customers who choose alternative payment methods like cash, or mobile banking services like M-Pesa that the app allows in Kenya. He actively turns down customers who want to pay using the Uber app.</p>



<figure><blockquote><p>“We tried to talk to Uber about adjusting the prices, but it was in vain, so we decided to take matters into our own hands to provoke them.”</p></blockquote></figure>



<p>He strategically places the laminated fare chart behind the driver’s seat and on the dashboard so customers are aware of the rates. Haggling with customers is not new for drivers in Kenya, but the union rate card has somewhat formalized higher fares, Charles told <em>Rest of World</em>. “We had been negotiating with customers before but this rate card became a unifying factor,” he said.</p>



<p>Initially, customers resisted paying the higher fares, Charles said. But now that the OOD’s rate card has become a common sight across Nairobi, passengers are used to it.</p>



<p>Justin Nyaga, chairperson of OOD, believes this is a form of peaceful pushback against Uber. “We tried to talk to Uber about adjusting the prices, but it was in vain, so we decided to take matters into our own hands to provoke them into discussing our terms and conditions,” he told <em>Rest of World</em>.</p>



<p>In a statement to <em>Rest of World</em>, Imran Manji, Uber’s head of East and South Africa, said that requesting payments above the app’s estimated rates is against company’s guidelines. “We encourage all riders to report such instances. We are currently reviewing the incidents reported to us,” Manji said.</p>



<p>Uber drivers around the world have complained about low earnings from the app, according to a <a href="https://fair.work/wp-content/uploads/sites/17/2024/02/Fairwork-Annual-Report-2023.pdf">2023 report</a> by Fairwork, a project at the Oxford Internet Institute that researches platform work. It seems natural for Kenyan taxi drivers to organize themselves and take this step so they can make a decent living, Mark Graham, director of Fairwork, told <em>Rest of World.</em></p>



<p>“At the end of the day, these drivers need to cover the costs of their vehicles, fuel, and insurance,” Graham said. “They often have families to support and are frequently the primary breadwinners. If they find that Uber isn’t providing fair earnings, it’s only natural that they would organize collectively to ensure they can make a decent living.”</p>


	<figure>
		<div>
		<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-40x57.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-600x1066.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-400x566.jpg 400w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-600x848.jpg 600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-1000x1414.jpg 1000w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827-1600x2262.jpg 1600w, https://149346090.v2.pressablecdn.com/wp-content/uploads/2024/08/Screenshot-StephanieWangari-20240827.jpg 1652w, " sizes="(max-width: 640px) 100vw, 600px" alt="">
		
		</p>
		</div>
				<figcaption itemprop="caption description">
		
		<span itemprop="copyrightHolder">Stephanie Wangari</span>
		</figcaption>
	</figure>


<p>In September 2022, the Kenyan government <a href="https://www.aljazeera.com/news/2022/9/15/kenyas-new-president-scraps-petrol-subsidy">removed its subsidy</a> on fuel, which caused <a href="https://www.thecitizen.co.tz/tanzania/news/east-africa-news/more-pain-at-the-pump-for-kenyan-motorists-as-fuel-prices-hit-new-records-4369470">gas prices</a> in the country to reach a record high. Even after the subsidy was reintroduced a year later, fuel costs in Kenya have <a href="https://www.businessdailyafrica.com/bd/economy/fuel-prices-drop-by-sh1-in-epra-may-review-4622820">remained high</a>.</p>



<p>Uber’s fares have not kept up with this rise in costs, making it hard for drivers to make ends meet,<strong> </strong>Nyaga said. When Uber entered Kenya in January 2015, a liter of gas cost 94.06 shillings (73 cents). This month, it cost roughly twice as much.</p>



<p>Kenyan taxi drivers have held several protests in the past two years, seeking a reduction in commissions that Uber charges. In <a href="https://www.businessdailyafrica.com/bd/corporate/companies/uber-cuts-commissions-to-18pc-4003712">October 2022</a>, the company reduced its commission in Kenya from 25% to 18% following protests. This July, taxi drivers in Nairobi held a five-day protest against low earnings, and requested Uber and Bolt to increase fares.</p>



<p>“Uber regularly makes pricing updates to ensure that drivers continue to have the opportunity to maximize their earnings while driving on the Uber app and at the same time, remaining at an affordable price point for riders,” an Uber spokesperson told<em> Rest of World</em> in a statement. Uber last increased fares on August 19, the spokesperson said. “This was done after a careful review of market conditions as well as after receiving feedback from drivers during regular roundtable discussions.”</p>



<p>The OOD rate card is a boon to drivers, but passengers have been increasingly frustrated.&nbsp;</p>



<p>Duncan Ndung’u, a businessman in Nairobi who uses Uber frequently to take his children to school, told <em>Rest of World</em> he spends several minutes before each ride in heated negotiations with drivers</p>



<p>“Once I make a request, the drivers ask if I’m willing to pay more than the app suggests, and when I decline, I have to request two or three times before finding someone willing to shuttle me. It is time-wasting,” Ndung’u said. “Nothing frustrates me more than requesting a ride, only to spend the entire journey negotiating with the driver about adding something on top of what the app suggests.”</p>



<p>He said his Uber rating has fallen in recent days and suspects it is probably because of his refusal to give in to the drivers’ demands.</p>



<p>In the coastal city of Mombasa, several taxi drivers have moved to apps like Little Cab and Yego, which charge passengers higher fares and have lower commissions for drivers, Eric Mbaabu, organizing secretary for the All Coast Online Digital Cabs Accord, told <em>Rest of World</em>. The shift happened after various unions were unsuccessful in reaching out to Uber over pricing issues, he said. In June 2023, 600 drivers in Mombasa <a href="https://cloud.kbc.co.ke/coast-taxi-drivers-sign-agreement-with-taxi-app-yego/">reportedly signed an agreement</a> with Yego, which promised them lower commissions.</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[David Chang on the long, hard, stupid way (191 pts)]]></title>
            <link>https://herbertlui.net/david-chang-on-the-long-hard-stupid-way/</link>
            <guid>41512207</guid>
            <pubDate>Wed, 11 Sep 2024 15:21:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://herbertlui.net/david-chang-on-the-long-hard-stupid-way/">https://herbertlui.net/david-chang-on-the-long-hard-stupid-way/</a>, See on <a href="https://news.ycombinator.com/item?id=41512207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I recently came across a great quote from <a href="https://ny.eater.com/2011/6/13/6675377/treme-watch-david-chang-gets-a-cameo">David Chang</a>, “Just because we’re a casual restaurant, doesn’t mean we don’t hold ourselves to fine dining standards. We try to do things the right way. That usually means doing things the long, hard, stupid way.”</p>
<p>David has elaborated <a href="https://www.pbs.org/wnet/americanmasters/podcast/restaurateur-and-chef-david-chang/">on this quote</a>:&nbsp;</p>
<blockquote>
<p>When your back is turned and you’re no longer supervising them and they’re on their own making the decision are they going to make the right decision versus the wrong decision? And the only thing that’s separating them from making the wrong decision is personal integrity because no one’s ever gonna make… No one’s going to know. So many times in a kitchen you’re trying to teach someone to do the dumb long way. And like let’s just say you have a cook that’s making a dish and one of the steps that they do can save them 30 minutes if they just cheat, and the reality is the end result will lead to no one ever knowing the difference. I have to try to create an environment where I’m trying to get that Cook to take the long stupid way. It’s so inefficient. And ultimately I think being a chef is one of the hardest jobs to motivate people because there’s no lure of a giant paycheck or bonus or stock options. You’re really trying to teach someone to better themselves through their own personal integrity.</p>
</blockquote>
<p>In real life, here’s what this looks like. David writes in <em>Eat a Peach</em>:</p>
<blockquote>
<p>For example, one of my favorite Majordomo dishes is a whole boiled chicken. We present the bird to the table in a big pot, bring it back to the kitchen to carve it, and then return with a beautiful platter of rice topped with the sliced breasts and two different sauces spooned over the top. Once guests are finished with that, we bring out a soup made from the carcass. It’s so good.</p>
</blockquote>
<p>One day, David gets this email from his team:</p>
<blockquote>
<p>We’ve been cooking a “presentation chicken” lately to help get the chickens out earlier. That is, when the first bird is fired, we usually cook two &amp; have one just for presenting so we can butcher the chicken that’s rested, while we slow the other chicken, which speeds up the time it takes between seeing the chicken &amp; receiving the rice. The extra chicken at the end of the night is also butchered into our stock the next day, since we always like to use those bones from carcasses for the soup that follows.</p>
</blockquote>
<p>His response:</p>
<blockquote>
<p>They had cooked enough services to realize they could improve flow and make things easier on the staff with a little bit of bait-and-switch. A smart decision and common practice. The guest would have no idea that the chicken they’d seen wasn’t the same one they were eating.&nbsp;</p>
<p>I wrote to Jude and the rest of the Domo team saying we would discuss it upon my return, which they accurately interpreted to mean we were going back to the hard way.&nbsp;</p>
<p>It had nothing to do with integrity. I didn’t care about fooling the diners. What concerned me was the precedent we were setting. I worried about the mindset of the server whose job it would be to parade a stunt chicken around the dining room. I was terrified of our culture stagnating. The dish was meant to be a difficult pickup that required constant coordination between the front and back of house. That was what made it great. If they wanted to sandbag it, they needed to figure out how they would make up for the lost energy elsewhere.</p>
</blockquote>
<p>There are a lot of reasons why the Momofuku brand of restaurants gained the popularity that it did, but no doubt this emphasis on quality is part of it. It’s this realm I think about when I’m discussing <a href="https://herbertlui.net/longcutting/">longcutting</a>, and <a href="https://herbertlui.net/to-succeed-in-the-age-of-ai-build-your-capacity-for-longcuts/">building the capacity for it</a>.</p>
<p>In a world where <a href="https://herbertlui.net/text-generation/">600 words are one click of a button and a prompt away</a>, writing a blog post—in 2024 at that!—<a href="https://herbertlui.net/this-is-probably-dumb/">sounds pretty stupid</a>. I write it because I think it will help me with all of the non-writing parts of writing. Asides from enjoying the practice—<a href="https://herbertlui.net/615-days-of-blogging-for-the-hell-of-it/">I write for fun</a>—it also <a href="https://twitter.com/scottbelsky/status/1762866202433745292">improves my taste</a>, keeps me finding important things to say, and helps me figure out the right contexts for stories.</p>
<figure></figure>
<figure></figure>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Tune LLaMa3.1 on Google Cloud TPUs (124 pts)]]></title>
            <link>https://github.com/felafax/felafax</link>
            <guid>41512142</guid>
            <pubDate>Wed, 11 Sep 2024 15:14:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/felafax/felafax">https://github.com/felafax/felafax</a>, See on <a href="https://news.ycombinator.com/item?id=41512142">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Felafax -- tune LLaMa3.1 on Google Cloud TPUs for 30% lower cost and scale seamlessly!</h2><a id="user-content-felafax----tune-llama31-on-google-cloud-tpus-for-30-lower-cost-and-scale-seamlessly" aria-label="Permalink: Felafax -- tune LLaMa3.1 on Google Cloud TPUs for 30% lower cost and scale seamlessly!" href="#felafax----tune-llama31-on-google-cloud-tpus-for-30-lower-cost-and-scale-seamlessly"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/felafax/felafax/blob/main/utils/assets/image.jpg"><img src="https://github.com/felafax/felafax/raw/main/utils/assets/image.jpg" alt="image"></a></p>
<p dir="auto">Felafax is a framework for continued-training and fine-tuning open source LLMs using <strong>XLA runtime</strong>. We take care of neceessary runtime setup and provide a Jupyter notebook out-of-box to just get started.</p>
<ul dir="auto">
<li>Easy to use.</li>
<li>Easy to configure all aspects of training (designed for ML researchers and hackers).</li>
<li>Easy to scale training from a single TPU VM with 8 cores to entire TPU Pod containing 6000 TPU cores (<strong>1000X</strong>)!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Goal</h2><a id="user-content-goal" aria-label="Permalink: Goal" href="#goal"></a></p>
<p dir="auto">Our goal at <a href="https://felafax.ai/" rel="nofollow">felafax</a> is to build infra to make it easier to run AI workloads on non-NVIDIA hardware (TPU, AWS Trainium, AMD GPU, and Intel GPU).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Currently supported models</h2><a id="user-content-currently-supported-models" aria-label="Permalink: Currently supported models" href="#currently-supported-models"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>LLaMa-3.1 JAX Implementation</strong> <math-renderer data-static-url="https://github.githubassets.com/static" data-run-id="ce6f78b4f9c0e9ab6960e51bd4f034c6">$${\color{red}New!}$$</math-renderer></p>
<ul dir="auto">
<li>Converted from PyTorch to JAX for improved performance</li>
<li>By default, runs 2-way data parallel and 2-way model parallel training (2 data parallel model copies and each model copy is sharded across two TPU chips).</li>
<li>Compatible with NVIDIA GPUs and TPUs</li>
<li>Full-precision training support</li>
</ul>
</li>
<li>
<p dir="auto"><strong>LLaMa-3/3.1 PyTorch XLA</strong></p>
<ul dir="auto">
<li>LoRA and full-precision training support</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Gemma2 Models (2B, 9B, 27B)</strong></p>
<ul dir="auto">
<li>Optimized for Cloud TPUs</li>
<li>Fast full-precision training</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><strong>For a hosted version with a seamless workflow, please visit <a href="https://app.felafax.ai/" rel="nofollow">app.felafax.ai</a></strong> 🦊.</p>
<p dir="auto">We are also <strong>onboarding people to try out Google's latest generation TPUs</strong>, if you are interested sign up to the waitlist <a href="https://tally.so/r/mRLeaQ" rel="nofollow">here</a>.</p>
<p dir="auto">If you prefer a self-hosted training version, follow the instructions below. These steps will guide you through launching a TPU VM on your Google Cloud account and starting a Jupyter notebook. With just 3 simple steps, you'll be up and running in under 10 minutes. 🚀</p>
<ol dir="auto">
<li>
<p dir="auto">Install gcloud command-line tool and authenticate your account (SKIP this STEP if you already have gcloud installed and have used TPUs before! 😎)</p>
<div dir="auto" data-snippet-clipboard-copy-content=" # Download gcloud CLI
 curl https://sdk.cloud.google.com | bash
 source ~/.bashrc

 # Authenticate gcloud CLI
 gcloud auth login

 # Create a new project for now
 gcloud projects create LLaMa3-tunerX --set-as-default

 # Config SSH and add
 gcloud compute config-ssh --quiet

 # Set up default credentials
 gcloud auth application-default login

 # Enable Cloud TPU API access
 gcloud services enable compute.googleapis.com tpu.googleapis.com storage-component.googleapis.com aiplatform.googleapis.com"><pre> <span><span>#</span> Download gcloud CLI</span>
 curl https://sdk.cloud.google.com <span>|</span> bash
 <span>source</span> <span>~</span>/.bashrc

 <span><span>#</span> Authenticate gcloud CLI</span>
 gcloud auth login

 <span><span>#</span> Create a new project for now</span>
 gcloud projects create LLaMa3-tunerX --set-as-default

 <span><span>#</span> Config SSH and add</span>
 gcloud compute config-ssh --quiet

 <span><span>#</span> Set up default credentials</span>
 gcloud auth application-default login

 <span><span>#</span> Enable Cloud TPU API access</span>
 gcloud services <span>enable</span> compute.googleapis.com tpu.googleapis.com storage-component.googleapis.com aiplatform.googleapis.com</pre></div>
</li>
<li>
<p dir="auto">Spin up a TPU v5-8 VM 🤠.</p>

<p dir="auto">Keep an eye on the terminal -- you might be asked to input SSH key password and need to put in your HuggingFace token.</p>
</li>
<li>
<p dir="auto">Clone the repo and install dependencies</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/felafax/felafax.git
cd felafax
pip install -r requirements.txt"><pre>git clone https://github.com/felafax/felafax.git
<span>cd</span> felafax
pip install -r requirements.txt</pre></div>
</li>
<li>
<p dir="auto">Open the Jupyter notebook at <code>https://localhost:888</code> and start fine-tuning!</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Credits:</h2><a id="user-content-credits" aria-label="Permalink: Credits:" href="#credits"></a></p>
<ul dir="auto">
<li>Google Deepmind's <a href="https://github.com/google-deepmind/gemma">Gemma repo</a>.</li>
<li><a href="https://github.com/young-geng/EasyLM">EasyLM</a> for great work on llama models in JAX</li>
<li>PyTorch XLA FSDP and SPMD testing done by <a href="https://github.com/HeegyuKim/torch-xla-SPMD">HeegyuKim</a>.</li>
<li>Examples from <a href="https://github.com/pytorch/xla/">PyTorch-XLA</a> repo.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">If you have any questions, please contact us at <a href="mailto:founders@felafax.ai">founders@felafax.ai</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists use magnetic nanotech to safely rewarm frozen tissues for transplant (146 pts)]]></title>
            <link>https://phys.org/news/2024-09-scientists-magnetic-nanotech-safely-rewarm.html</link>
            <guid>41511928</guid>
            <pubDate>Wed, 11 Sep 2024 14:51:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2024-09-scientists-magnetic-nanotech-safely-rewarm.html">https://phys.org/news/2024-09-scientists-magnetic-nanotech-safely-rewarm.html</a>, See on <a href="https://news.ycombinator.com/item?id=41511928">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/scientists-use-magneti.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2024/scientists-use-magneti.jpg" data-sub-html="Graphical abstract. Credit: <i>Nano Letters</i> (2024). DOI: 10.1021/acs.nanolett.4c03081">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/scientists-use-magneti.jpg" alt="Scientists use magnetic nanotech to safely rewarm frozen tissues for transplant" title="Graphical abstract. Credit: Nano Letters (2024). DOI: 10.1021/acs.nanolett.4c03081" width="800" height="505">
             <figcaption>
                Graphical abstract. Credit: <i>Nano Letters</i> (2024). DOI: 10.1021/acs.nanolett.4c03081
            </figcaption>        </figure>
    </div><p>Every day, people die waiting for an organ transplant. Time is at a premium, not just for those awaiting organs, but also for the organs themselves, which can deteriorate rapidly during transportation.</p>


										      
																																	<p>Looking to extend the viability of human tissues, researchers <a href="https://pubs.acs.org/doi/10.1021/acs.nanolett.4c03081" target="_blank">report</a> in<i>Nano Letters</i> their efforts to facilitate completely freezing, rather than cooling and then thawing, potentially life-saving organs. They demonstrate a magnetic nanoparticle's successful rewarming of animal tissues.</p>
<p>As of August 2024, more than 114,000 people are on the U.S. national transplant waiting list, according to the Organ Procurement and Transplantation Network, and about 6,000 annually will die before receiving an <a href="https://phys.org/tags/organ+transplant/" rel="tag">organ transplant</a>. One reason is the loss of organs in <a href="https://phys.org/tags/cold+storage/" rel="tag">cold storage</a> during transportation when delays cause them to warm prematurely.</p>
<p>Methods have been developed to quickly freeze organs for longer-term storage without risking damage from ice crystal formation, but ice crystals can also form during warming. To address this problem, Yadong Yin and colleagues advanced a technique known as nanowarming, pioneered by collaborator John Bischof, to employ <a href="https://phys.org/tags/magnetic+nanoparticles/" rel="tag">magnetic nanoparticles</a> and magnetic fields to thaw frozen tissues rapidly, evenly and safely.</p>
<p>Recently, Yin and a team developed magnetic nanoparticles—effectively extremely tiny bar magnets—that, when exposed to alternating magnetic fields, generated heat. And that heat rapidly thawed animal tissues stored at -238 degrees Fahrenheit (-150 degrees Celsius) in a solution of the nanoparticles and a cryoprotective agent.</p>

																																						
																																			<p>The researchers worried, however, that uneven distribution of the nanoparticles within the tissues might trigger overheating where the particles congregated, which could lead to <a href="https://phys.org/tags/tissue+damage/" rel="tag">tissue damage</a> and toxicity from the cryoprotective agent at elevated temperatures.</p>
<p>To reduce these risks, the researchers have continued their investigation, working on a two-stage approach that more finely controls nanowarming rates. They describe this process:</p>
<ul>
<li>Cultured cells or animal tissues were immersed in a solution containing magnetic nanoparticles and a cryoprotective substance and then frozen with liquid nitrogen.</li>
<li>In the first stage of thawing, as before, an alternating <a href="https://phys.org/tags/magnetic+field/" rel="tag">magnetic field</a> initiated rapid rewarming of <a href="https://phys.org/tags/animal+tissues/" rel="tag">animal tissues</a>.</li>
<li>As the samples approached the melting temperature of the cryoprotective agent, the researchers applied a horizontal static magnetic field.</li>
<li>The second field realigned the nanoparticles, effectively tapping the brakes on heat production.</li>
</ul>
<p>The heating slowed fastest in areas with more <a href="https://phys.org/tags/nanoparticles/" rel="tag">nanoparticles</a>, which dampened concerns about problematic hotspots. Applying the method to cultured human skin fibroblasts and to pig carotid arteries, the researchers noted that cell viability remained high after rewarming over a few minutes, suggesting the thawing was both rapid and safe.</p>
<p>The ability to finely control tissue rewarming moves us one step closer to long-term organ cryopreservation and the hope of more life-saving transplants for patients, the researchers say.</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												Sangmo Liu et al, Magnetic-Nanorod-Mediated Nanowarming with Uniform and Rate-Regulated Heating, <i>Nano Letters</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1021/acs.nanolett.4c03081" target="_blank">DOI: 10.1021/acs.nanolett.4c03081</a>
																						
																						</p>
																					</div>
                               											
																					
                              										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Scientists use magnetic nanotech to safely rewarm frozen tissues for transplant (2024, September 4)
												retrieved 11 September 2024
												from https://phys.org/news/2024-09-scientists-magnetic-nanotech-safely-rewarm.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Is 7 days a week the new norm (for YC)? (105 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41511506</link>
            <guid>41511506</guid>
            <pubDate>Wed, 11 Sep 2024 14:03:54 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41511506">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="41511506">
      <td><span></span></td>      <td><center><a id="up_41511506" href="https://news.ycombinator.com/vote?id=41511506&amp;how=up&amp;goto=item%3Fid%3D41511506"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=41511506">Is 7 days a week the new norm (for YC)?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_41511506">101 points</span> by <a href="https://news.ycombinator.com/user?id=bschmidt1">bschmidt1</a> <span title="2024-09-11T14:03:54.000000Z"><a href="https://news.ycombinator.com/item?id=41511506">9 hours ago</a></span> <span id="unv_41511506"></span> | <a href="https://news.ycombinator.com/hide?id=41511506&amp;goto=item%3Fid%3D41511506">hide</a> | <a href="https://hn.algolia.com/?query=Is%207%20days%20a%20week%20the%20new%20norm%20%28for%20YC%29%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=41511506&amp;auth=66101279c13fb8ab0fc1cada69a0ab633675b613">favorite</a> | <a href="https://news.ycombinator.com/item?id=41511506">71&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>I've worked for 2 major YC companies and done some one-off/contract work in recent years at a few others here in SF. Really like the startups coming out of YC.</p><p>I had to unfortunately voluntarily quit the last on-site contract due to it being 7 days a week (I tried to take off a Sunday, didn't work), on top of a requirement to be there until 8-9pm daily and occasionally pull all-nighters when there are sales demos (at the office until 7AM the next day - then stayed at work for the next day). I tried to stick it out, and even though I consider myself to have the best work ethic of anyone I know, being at an office nearly 24/7/365 to do basic React work with constantly changing directions/requirements felt pointless to say the least. It wasn't like we were building something with a clear goal in mind - and the founders did <i>nothing</i>. They would shoot hoops with the nerf basketball, leave for 2-3 hours at a time, come back with food, talk, etc.</p><p>Anyway, so I quit to find another gig, and in my first interview (YC company, just raised) the CEO says "We work weekends, so it's 7 days a week, just want to make sure that's ok with you".</p><p>So my question to the YC community is:</p><p>Is this the new norm for YC startups?</p><p>YC founders who are requiring 24/7/365 work:</p><p>What do you feel you're getting out of it? Better output? Or is it more about a person completely dedicating themselves to you?</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The CIA-in-Chile Scandal at 50 (129 pts)]]></title>
            <link>https://nsarchive.gwu.edu/briefing-book/chile/2024-09-09/cia-chile-scandal-50</link>
            <guid>41511050</guid>
            <pubDate>Wed, 11 Sep 2024 13:14:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nsarchive.gwu.edu/briefing-book/chile/2024-09-09/cia-chile-scandal-50">https://nsarchive.gwu.edu/briefing-book/chile/2024-09-09/cia-chile-scandal-50</a>, See on <a href="https://news.ycombinator.com/item?id=41511050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><strong>Washington, D.C., September 9, 2024</strong> – Fifty years ago, as the <em>New York Times</em> prepared to break a major exposé on CIA covert operations in Chile, the architect of those operations, Henry Kissinger, misled President Gerald Ford about clandestine U.S. efforts to undermine the elected government of Socialist Party leader Salvador Allende, documents posted today by the National Security Archive show. The covert operations were “designed to keep the democratic process going,” Kissinger briefed Ford in the Oval Office two days before the article appeared fifty years ago this week. According to Kissinger, “there was no attempt at a coup.”</p><p>“I saw the Chile story,” Ford told Kissinger on September 9, 1974. “Are there any repercussions?” Kissinger replied: “Not really.”</p><p>In fact, the front-page story written by investigative reporter Seymour Hersh—<a href="https://www.nytimes.com/1974/09/08/archives/cia-chief-tells-house-of-8million-campaign-against-allende-in-7073.html">“C.I.A. Chief Tells House Of $8 Million Campaign Against Allende in ‘70-’73”</a>—set in motion the biggest scandal on covert operations the intelligence community had ever experienced. Hersh’s September 8, 1974, article led directly to the formation of a special Senate committee, chaired by Senator Frank Church, that conducted the first major investigation of CIA covert actions in Chile and elsewhere and that was the first congressional body to evaluate the role of secret, clandestine operations in a democratic society. The political repercussions forced President Ford to publicly acknowledge the CIA operations in Chile while forcefully denying they had anything to do with fomenting a coup. The president’s White House lawyer subsequently advised Ford that his statement “was not fully consistent with the facts because all the facts had not been made known to you.”</p><p><iframe width="560" height="315" src="https://www.youtube.com/embed/pzmZRTto4PM?si=ED5a8IurvsJYrwXJ&amp;start=703" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p><p><em>At a September 16th press conference, Gerald Ford became the first president to publicly acknowledge and defend CIA covert operations, which he characterized as limited to protecting Chilean democratic institutions from the threat of Allende. He stated that the CIA actions were "in the best interest of the people in Chile, and certainly in our best interest." (See timecode 11:55 in tape for Ford's statement.)&nbsp;</em></p><p>The Senate investigation, which also revealed CIA assassination plots against foreign leaders, and a similar investigative effort in the House of Representatives led to legislation to enhance checks and balances on CIA operations and curtail the ability of future presidents to “plausibly deny” covert action programs abroad. White House documents reveal the acute consternation expressed by Ford and Kissinger that covert operations might be restrained. “We need a CIA and we need covert operations,” Ford told his Cabinet nine days after the <em>Times </em>exposé was published. The article and a flood of follow up CIA stories by Hersh, as Kissinger later conceded in his memoirs, “had the effect of a burning match in a gasoline depot.”</p><article>
  
      
            <p><a href="https://nsarchive.gwu.edu/sites/default/files/2024-09/image1.jpg"><img loading="lazy" src="https://nsarchive.gwu.edu/sites/default/files/styles/wide/public/2024-09/image1.jpg?itok=UsffHA9Q" alt="Stern's report and Kissinger's remarks" typeof="foaf:Image">

</a>
</p>
      
  </article>
<p><strong>THE LEAK THAT CHANGED HISTORY</strong></p><p>The Hersh story was based on a summary of secret testimony by CIA director William Colby and a legendary agency official, David Atlee Phillips, who provided an overview of covert operations against Allende in Chile during an executive session of the House Armed Services Committee on April 22, 1974. According to the summary, Colby informed the Committee that between 1962 and 1973, the ultra-secret “40 Committee,” which oversaw covert operations, had authorized the CIA to spend $11 million in Chile, including $8 million to “destabilize” the Allende government and “to precipitate its downfall.” The summary stated that “the agency activities were viewed as a prototype, or laboratory experiment, to test the techniques of heavy financial investment in efforts to discredit and bring down a government.”</p><p>The summary was drafted by a liberal congressman from Massachusetts, Michael J. Harrington, who had heard about Colby’s TOP SECRET testimony and requested special permission to review it. Harrington read the 48-page hearing transcript twice—on June 5, and June 12, 1974—and realized Colby’s testimony clearly contradicted previous denials by Kissinger and top CIA officials (during earlier hearings on the CIA and ITT’s operations in Chile) that there had been any covert efforts to undermine Allende.</p><p>Harrington shared his concern that CIA officers had committed perjury with Senator Frank Church’s staff director, Jerome Levinson. In his unpublished memoir, Levinson recalled that Harrington “asked what I thought he should do.” Levinson recommended that Harrington write a letter to the chairman of the Senate Foreign Relations Committee, Senator William Fulbright, requesting a full inquiry into the CIA’s role in Chile. On July 18, 1974, Harrington sent a lengthy letter to Fulbright, providing a summary of the secret CIA testimony and concluding that Congress and the American people “have a right to learn what was done in our name in Chile.”</p><p>After it became clear that Fulbright was not inclined to order a major investigation into the CIA role in Chile, Levinson decided to take the audacious step of calling attention to Colby’s still-secret testimony: he leaked the Harrington letter to Seymour Hersh. In early September, after lunch with Hersh at Jean-Pierre’s, a swanky D.C. restaurant, Levinson slipped Hersh a copy of the Harrington letter. On September 5, 1974, Hersh began calling State Department officials for comment on his forthcoming scoop, setting in motion a flurry of White House meetings, briefings and reports on what information Hersh might have obtained. On September 8, the <em>Times</em> published the story on the front page of its Sunday newspaper, generating a major scandal and eventually resulting in the prosecution of former CIA director Richard Helms for lying to Congress.</p><article>
  
      
            <p><a href="https://nsarchive.gwu.edu/sites/default/files/2024-09/image2_0.jpg"><img loading="lazy" src="https://nsarchive.gwu.edu/sites/default/files/styles/wide/public/2024-09/image2_0.jpg?itok=eUpJ9BST" alt="telcon" typeof="foaf:Image">

</a>
</p>
      
  </article>
<p><strong>REACTION OF THE CIA’s CHILEAN AGENTS</strong></p><p>The leak of Colby’s testimony forced the CIA to urgently contact its Chilean agents to ascertain the repercussions of the Hersh revelations on its network of assets and informants. In a revealing secret report four days after the <em>Times</em> article appeared, the CIA station transmitted the reactions of several Chilean operatives—identified by codenames such as FUBARGAIN, FUPOCKET and FUBRIG—who were embedded inside the Chilean military, the Chilean Christian Democrat political party, and the <em>El Mercurio</em> newspaper, which the CIA had financed as a bullhorn of opposition to the government of Salvador Allende. “Following Station agents were contacted, period 8-10 September, in connection with referenced revelations,” the Santiago Station informed CIA headquarters.</p><p>The agent codenamed “FUBRIG-2 “took the news calmly but was most concerned about implications of efforts of revelations and expressed opinion that system in Washington should be changed to prevent such leaks,” the CIA reported. “He was relieved that El Mercurio was not mentioned by name.”</p><p>According to this cable, the agent inside the Chilean military, FUBARGAIN-1, told the CIA that “General Pinochet did not seem very upset but [had] commented … that the disclosure ‘seemed to be a dumb thing to do.’” But the same agent told the CIA that other younger Chilean military officers interpreted the leak as a deliberate attempt to “damage [the] Junta and falsely cast doubt on their independence and role in bringing down Allende.” “Sum is that Chilean officer corps becoming increasingly baffled and resentful about U.S., according to this source.”</p><p><strong>STILL-SECRET DOCUMENTS</strong></p><p>Fifty years after the scandal broke over CIA operations in Chile, Colby’s original testimony before the House Armed Services Committee remains classified, as does the entire 48-page transcript of the closed hearing. Last year, the Chilean government officially requested that the Biden administration declassify those records as a gesture of “declassification diplomacy” for the 50th anniversary of the coup, but the CIA proved to be uncooperative.</p><p>“For the sake of historical accountability, it is imperative that the CIA declassify Colby’s testimony on Chile, as well as other relevant documentation,” stated Peter Kornbluh who directs the Archive’s Chile Documentation Project. As the 50th anniversary of the formation of the special Senate Committee to Study Government Operations with Respect to Intelligence Activities approaches in January 2025, the Archive also called on Senate leaders to initiate the release the Church Committee’s voluminous investigative archives on Chile and other countries targeted for covert regime change operations.</p><p>“A half century of secrecy surrounding these records,” Kornbluh noted, “must come to an end.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a browser using Servo as a web engine (134 pts)]]></title>
            <link>https://servo.org/blog/2024/09/11/building-browser/</link>
            <guid>41510402</guid>
            <pubDate>Wed, 11 Sep 2024 11:43:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://servo.org/blog/2024/09/11/building-browser/">https://servo.org/blog/2024/09/11/building-browser/</a>, See on <a href="https://news.ycombinator.com/item?id=41510402">Hacker News</a></p>
Couldn't get https://servo.org/blog/2024/09/11/building-browser/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[We spent $20 to achieve RCE and accidentally became the admins of .mobi (1278 pts)]]></title>
            <link>https://labs.watchtowr.com/we-spent-20-to-achieve-rce-and-accidentally-became-the-admins-of-mobi/</link>
            <guid>41510252</guid>
            <pubDate>Wed, 11 Sep 2024 11:19:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://labs.watchtowr.com/we-spent-20-to-achieve-rce-and-accidentally-became-the-admins-of-mobi/">https://labs.watchtowr.com/we-spent-20-to-achieve-rce-and-accidentally-became-the-admins-of-mobi/</a>, See on <a href="https://news.ycombinator.com/item?id=41510252">Hacker News</a></p>
Couldn't get https://labs.watchtowr.com/we-spent-20-to-achieve-rce-and-accidentally-became-the-admins-of-mobi/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Why Is Pave Legal? (950 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41510103</link>
            <guid>41510103</guid>
            <pubDate>Wed, 11 Sep 2024 10:50:55 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41510103">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=41510103: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Radicle 1.0 – A local-first, P2P alternative to GitHub (172 pts)]]></title>
            <link>https://radicle.xyz/2024/09/10/radicle-1.0.html</link>
            <guid>41509713</guid>
            <pubDate>Wed, 11 Sep 2024 09:40:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radicle.xyz/2024/09/10/radicle-1.0.html">https://radicle.xyz/2024/09/10/radicle-1.0.html</a>, See on <a href="https://news.ycombinator.com/item?id=41509713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p><em>Radicle is a peer-to-peer, local-first code
        collaboration stack built on Git.</em></p>
      <br>
      <header>
        <h2>Radicle 1.0</h2>
        <span>10.09.2024</span>
      </header>
      <p>On March 26th, we <a href="https://x.com/radicle/status/1772659708978991605?s=20">announced</a> the first release candidate for Radicle 1.0.
Today, after five months of feedback and 17 release candidates, we are ready to
launch Radicle <code>1.0</code>.</p>

<p>If you’ve been waiting for the right moment to try Radicle or to reintroduce
yourself to the stack, now is a great time to dive in: our <a href="https://radicle.xyz/">website</a>
and <a href="https://radicle.xyz/guides">guides</a> should have all the information you need to get started.
You can also browse the latest <a href="https://app.radicle.xyz/nodes/seed.radicle.xyz/rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5">code</a>, hosted on Radicle.</p>

<p>If you’ve been tagging along, thank you, it’s been a bumpy ride and we couldn’t
have done it without you!</p>

<h2 id="try-it-out">Try it out!</h2>

<p>You can grab the latest release with the following command:</p>

<div><pre><code>curl -sSf https://radicle.xyz/install | sh
</code></pre></div>

<p>Or head over to the <a href="https://radicle.xyz/download">download</a> section to download and verify the
binaries yourself.</p>

<h2 id="whats-in-the-box-">What’s in the box 📦</h2>

<p>Radicle 1.0 represents the culmination of <a href="https://radicle.xyz/history">years</a> of experimentation
and hard work from our team and community, where we set out to ensure that free
and open source software ecosystems can flourish without having to rely on the
whims of Big Tech. We designed Radicle with a first-principles approach, as a
natural extension to Git, expanding it to work in a collaborative, local-first,
peer-to-peer setting.</p>

<p>This milestone includes:</p>

<ul>
  <li>An extensible, homegrown, peer-to-peer gossip and sync <a href="https://radicle.xyz/guides/protocol">protocol</a>
built on the Git protocol.</li>
  <li>Social interactions such as issues, patches and code review, using our
extensible <a href="https://radicle.xyz/guides/protocol#collaborative-objects">Collaborative Objects</a> system which keeps all artifacts
in the repository.</li>
  <li>A secure authentication and authorization protocol using public key
cryptography, which allows all published content to be verified locally,
without centralized authority.</li>
  <li>An intuitive CLI that should be familiar to users of Git, as well as a web
frontend for browsing Radicle repositories and seed nodes.</li>
  <li>Privacy at the protocol level, with truly private repositories and built-in
<a href="https://radicle.xyz/guides/user/#4-embracing-the-onion">Tor support</a>.</li>
  <li>Reproducible and signed <a href="https://radicle.xyz/download">builds</a> for all Radicle binaries.</li>
</ul>

<p>To us, Radicle 1.0 means that Radicle is <em>ready to use</em>. It stands as a
testament that sovereign code forges are possible today, and in our opinion,
<em>necessary</em>.</p>

<p>We feel comfortable now inviting you to join us, replicate, and
<a href="https://radicle.xyz/guides/user">collaborate</a> within the Radicle network or even <a href="https://radicle.xyz/guides/seeder">run a seed
node</a>.</p>

<p>For an in-depth explanation of how Radicle works, check out our <a href="https://radicle.xyz/guides/protocol">Protocol
Guide</a>.</p>

<h2 id="stability-️">Stability ⛰️</h2>

<p>Radicle 1.0 marks our commitment to stability: from this release onwards, all
changes to the protocol will be designed in backwards compatible way, and any
necessary change on the CLI will include a seamless upgrade path.</p>

<p>We are aware that the release candidate phase was rockier than expected for
some, but we are now in a good place to slow things down and improve stability.</p>

<p>Along with this commitment will come a more dependable and streamlined release
process which starts with this release!</p>

<h2 id="future-plans-">Future plans 🔮</h2>

<p>There are several things in the pipeline that we intend to release when ready:</p>

<ul>
  <li>Native CI/CD capabilities</li>
  <li>The Radicle TUI (Terminal User Interface)</li>
  <li>Advanced code review functionality</li>
  <li>An inbox system for repository notifications</li>
  <li>Multi-device support and user profiles</li>
  <li>Support for other canonical references, such as tags</li>
  <li>Seed node moderation and management tools</li>
  <li>The Radicle desktop application</li>
</ul>

<h2 id="growing-ecosystem-">Growing ecosystem 🌱</h2>

<p>Outside of the core stack, the ecosystem is growing nicely:  an independent
team working on integrations &amp; tooling for Radicle has developed a <a href="https://app.radicle.at/nodes/seed.radicle.gr/rad:z3Makm6fsQQXmpSFE43DZqwupaEhk">VS
Code</a> and <a href="https://app.radicle.at/nodes/seed.radicle.gr/rad:z3WHS4GSf8hChLjGYfPkJY7vCxsBK">JetBrains</a> plugin. The Radicle network now also
comprises several deployments of the Radicle <a href="https://app.radicle.xyz/nodes/seed.radicle.xyz/rad:z4V1sjrXqjvFdnCUbxPFqd5p4DtH5">frontend</a>, and there are more
than 40 seed nodes operating on the network, freely replicating user content.</p>

<h2 id="invitation-to-forge-the-future-">Invitation to forge the future 🤝</h2>

<p>Once you <a href="https://radicle.xyz/guides/user#installation">install</a> Radicle and set up your identity, you’ll have access
to all public repositories on any public node, making it easier to explore and
contribute to the ecosystem. Compared to traditional self-hosted forges, which
often result in fragmented collaboration environments, Radicle represents an
evolutionary step for Git-based collaboration, with a single cryptographic
identity that works across nodes.</p>

<p>Lastly, in the spirit of free and open source software, we believe that power
lies in community. As we embark on further iterating the protocol and stack, we
also invite you to shape the future of Radicle with us. Consider joining us on
our <a href="https://radicle.zulipchat.com/">Zulip</a> instance. Your ideas and insights
are invaluable to our mission of creating a sovereign forge.</p>

<p>Together, we can make significant progress towards reclaiming the internet.</p>

<p><em>Free your code!</em></p>

<p>👾👾👾</p>

<hr>

<p><strong>Follow us</strong> on 🐘 <a href="https://toot.radicle.xyz/@radicle">Mastodon</a>, 🦋 <a href="https://bsky.app/profile/radicle.xyz">Bluesky</a> or 🐦
<a href="https://twitter.com/radicle">Twitter</a> to stay updated.</p>

<p><strong>Contribute</strong> to Radicle as a 🌱 <a href="https://radicle.xyz/guides/seeder">seeder</a>, 🧙
<a href="https://app.radicle.xyz/nodes/seed.radicle.xyz/rad:z3gqcJUoA1n9HaHKufZs5FCSGazv5">developer</a> or by 🪞 <a href="https://radicle.xyz/guides/user/#git-going-with-repositories">mirroring</a> your repositories on
the Radicle network.</p>

<p><strong>Join</strong> our community on 💬 <a href="https://radicle.zulipchat.com/">Zulip</a> and discuss your ideas to
improve Radicle.</p>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Max Headroom and the World of Pseudo-CGI (2013) (142 pts)]]></title>
            <link>https://www.cartoonbrew.com/cgi/max-headroom-and-the-strange-world-of-pseudo-cgi-82745.html</link>
            <guid>41509558</guid>
            <pubDate>Wed, 11 Sep 2024 09:11:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cartoonbrew.com/cgi/max-headroom-and-the-strange-world-of-pseudo-cgi-82745.html">https://www.cartoonbrew.com/cgi/max-headroom-and-the-strange-world-of-pseudo-cgi-82745.html</a>, See on <a href="https://news.ycombinator.com/item?id=41509558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <header>
      <a href="https://www.cartoonbrew.com/cgi" title="CGI">CGI</a><a href="https://www.cartoonbrew.com/ideas-commentary" title="Ideas/Commentary">Ideas/Commentary</a>      
      

      
    </header>
    
    
    <div> 
    <p>I’ve come across people who believe that Max Headroom, the Channel 4 character from the Eighties (pictured at top), was a genuine piece of computer animation. But although he was conceived by the animators Rocky Morton and Annabel Jankel  (of Cucumber Films fame) Max himself was portrayed by actor Matt Frewer, placed into latex makeup and a shiny costume and set amidst a range of technological tricks. </p>
<p>Half of the frames from the footage used in <em>Max Headroom</em> were removed in production, resulting in a juddery look to suggest animation shot on twos, and Frewer was bluescreened in front of a basic digital backdrop. The crew even added deliberate faults to the “animation” – such as the stammer which became Max’s trademark – to complete the effect.</p>
<p><iframe width="560" height="315" src="https://www.youtube.com/embed/0f_hWGCsY1g?si=xwIH6F-NBDJ-7doe" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p>
<p>This process seems somewhat surreal today, in our brave new world of Maya, Xtranormal and Blender. Max Headroom was created at a time when 3D CGI animation was desirable, but not always affordable; if the budget did not allow it, then the crew had to fake computer animation in front of the camera.</p><!--googleoff: index--><!--googleon: index-->
<p>Another good example of this can be found in the 1981 film <em>Escape from New York</em>. Early on in the movie we see what appears to be a wireframe model of Manhattan; in actual fact, a physical model was built for this sequence, with reflective tape placed along the edges of the buildings. Shot under ultraviolet light, this recreated the luminescent green-on-black effect of primitive CGI.</p>
<p><img loading="lazy" decoding="async" src="http://www.cartoonbrew.com/wp-content/uploads/2013/05/escapefromnewyork-580x391.jpg" alt="" title="escapefromnewyork" width="580" height="391" srcset="https://www.cartoonbrew.com/wp-content/uploads/2013/05/escapefromnewyork-580x391.jpg 580w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/escapefromnewyork-380x256.jpg 380w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/escapefromnewyork-444x300.jpg 444w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/escapefromnewyork-180x122.jpg 180w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/escapefromnewyork.jpg 600w" sizes="(max-width: 580px) 100vw, 580px"></p><p>There has even been an incident in which a budget imitation of CGI itself received a budget imitation. In 1987 an unidentified signal hacker managed to replace two television broadcasts with a <a rel="nofollow" target="_blank" href="http://www.youtube.com/watch?v=cycVTXtm0U0&amp;feature=youtu.be&amp;t=30s">mildly disturbing video</a> of a home-made <em>Max Headroom</em> show. In this improvised effort Max was portrayed by a man in a shop-bought mask, while the moving backdrops – in the original series, an example of genuine digital animation amongst the pseudo-CGI – were replaced with somebody offscreen wiggling a bit of corrugated metal about.</p>
<p>These are all extreme examples; during this period, it was more common for digital animation to be emulated using hand-drawn techniques. Often used as a visual motif in kids’ science fiction-themed cartoons (witness the cel animated wireframes in the <a rel="nofollow" target="_blank" href="https://www.youtube.com/watch?v=Iy6_jqHKcTI">opening sequence to <em>Transformers</em></a>) this approach was put to good use by <a rel="nofollow" target="_blank" href="http://www.rodlord.com/">Rod Lord’</a>s animation work on the <em>Hitchhiker’s Guide to the Galaxy</em> television series from 1981. Created using litho film and coloured gels, these sequences suggested digital graphics simply by combining glowing primarily-coloured images with a black background. An added plus was that the animation could get away with being a little bit jerky…</p>
<p><img loading="lazy" decoding="async" src="http://www.cartoonbrew.com/wp-content/uploads/2013/05/rodlord-babelfish-580x389.jpg" alt="" title="rodlord-babelfish" width="580" height="389" srcset="https://www.cartoonbrew.com/wp-content/uploads/2013/05/rodlord-babelfish-580x389.jpg 580w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/rodlord-babelfish-380x255.jpg 380w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/rodlord-babelfish-446x300.jpg 446w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/rodlord-babelfish-180x121.jpg 180w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/rodlord-babelfish.jpg 600w" sizes="(max-width: 580px) 100vw, 580px"></p><p><a rel="nofollow" target="_blank" href="http://www.youtube.com/watch?v=fL2FbGH2DDs">One sequence</a> in <em>Hitchhiker’s Guide</em> portrayed an intergalactic war as an early video game, a theme drawn upon by other animators: for example, in 1982 a British public information film used <em>Space Invaders</em>-like imagery to advise audiences on safe driving [see image below]. The biggest example of this, however, came when Disney produced an entire feature film based around the look of eighties arcade games: <em>Tron.</em></p>
<p><img loading="lazy" decoding="async" src="http://www.cartoonbrew.com/wp-content/uploads/2013/05/spaceinvadersstylefilm-580x398.jpg" alt="" title="spaceinvadersstylefilm" width="580" height="398" srcset="https://www.cartoonbrew.com/wp-content/uploads/2013/05/spaceinvadersstylefilm-580x398.jpg 580w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/spaceinvadersstylefilm-380x260.jpg 380w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/spaceinvadersstylefilm-436x300.jpg 436w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/spaceinvadersstylefilm-180x124.jpg 180w, https://www.cartoonbrew.com/wp-content/uploads/2013/05/spaceinvadersstylefilm.jpg 600w" sizes="(max-width: 580px) 100vw, 580px"></p><p><em>Tron</em> contained genuine CGI animation backed up with large amounts of compositing tricks based around matte effects and backlighting; this made the live action footage look as though it had been digitally processed. As a result, the film stands as arguably the premiere example of pseudo-CGI.</p>
<p>In her book <a rel="nofollow" target="_blank" href="http://www.amazon.com/gp/product/0253220963/ref=as_li_qf_sp_asin_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0253220963&amp;linkCode=as2&amp;tag=animationblast08"><em>British Animation: The Channel 4 Factor</em></a><img loading="lazy" decoding="async" src="http://www.assoc-amazon.com/e/ir?t=animationblast08&amp;l=as2&amp;o=1&amp;a=0253220963" width="1" height="1" alt=""> Clare Kitson remarked on the fact that Max Headroom, Channel 4’s biggest animated hit, was not actually animated. But as she went on to argue, perhaps it is time for a reappraisal:</p>
<blockquote><p>I wonder if we might indeed classify those sequences as animation nowadays. With the plethora of different technologies now employed, the previous narrow definition (which insisted that the movement itself must be created by the animator) seems a bit old-fashioned. These days anything that appears on a screen and moves but is not a record of real life – including creatures moved by motion capture – tends to fall under the animation umbrella… The current popular synonym for animation, ‘manipulated moving image’, seems to be made for Max.
</p></blockquote>
<p>Of course, if Max had been made using actual CGI he would have ended up as a creaky old relic, rather like the <a rel="nofollow" target="_blank" href="http://www.youtube.com/watch?v=lAD6Obi7Cag">“Money for Nothing”</a> video which came out the year after his debut.  Instead, Jankel, Morton and Frewer came up with a genuinely iconic creation that has aged surprisingly well.</p>
<p>Today, it is all too easy for animators to fall back on the tricks of their software and lose track of the wider aesthetic potential of their work. What <em>Max Headroom</em>—and, to an extent, some of the other pieces mentioned here—show is the opposite effect: digital animation spurring creativity in analogue work. They have an ingenuity and hand-made charm which is missing from so much modern computer animation.</p>
<p>Primitive digital imagery has had something of a resurgence across the past decade or so, to the point where pastiches of 8-bit pixel graphics have found their way into mainstream productions such as <em>Wreck-It Ralph</em>. Perhaps it is time that the animators and digital artists of today rediscovered the lesser-known cousin of this aesthetic: the strange world of pseudo-CGI.</p>
<hr>
<p><em><b>NEIL EMMETT</b> is the editor of <a rel="nofollow" target="_blank" href="http://ukanimation.blogspot.com/">The Lost Continent</a>, a fantastic resource devoted to British animation, past and present. This piece is an expanded version of a post that originally appeared on his site.</em></p>
     </div>
    
    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yaak is now open source (128 pts)]]></title>
            <link>https://yaak.app/blog/now-open-source</link>
            <guid>41509156</guid>
            <pubDate>Wed, 11 Sep 2024 08:05:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://yaak.app/blog/now-open-source">https://yaak.app/blog/now-open-source</a>, See on <a href="https://news.ycombinator.com/item?id=41509156">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>I hate to admit it, but <strong>the Reddit, HackerNews, and Lobsters folks were right.</strong></p>
<p>Three months ago I published <a href="https://yaak.app/blog/why-not-open-source">Why Not Open Source</a> as an
attempt to explain why Yaak <em>wouldn't</em> be open source. Having burnt out on OSS in the
past, I thought that others might find my decision-making process useful, so I shared it
to the usual places.</p>
<p>Yaak users mostly agreed, which makes sense, as they were already happy with the
close-source reality. The broader open-source community, however, strongly disagreed with
almost everything.</p>
<ul>
<li><a href="https://lobste.rs/s/qeqgiq">lobste.rs post</a></li>
<li><a href="https://www.reddit.com/r/opensource/comments/1dgis44/why_yaak_is_not_open_source/">Reddit r/opensource post</a></li>
<li><a href="https://news.ycombinator.com/item?id=41424910">HackerNews post</a></li>
</ul>
<p>Here are a few of the highlights:</p>
<blockquote>
<p>Don’t conflate “open source”/“free software” with github’s specific social model of
drive-by contributions, or even contributions at all...
→ <a href="https://lobste.rs/s/qeqgiq/why_yaak_is_not_open_source#c_k7tr74">lobste.rs</a></p>
</blockquote>
<blockquote>
<p>But all of those things are also true of closed source software.
→ <a href="https://news.ycombinator.com/item?id=41425136">ycombinator.com</a></p>
</blockquote>
<blockquote>
<p>The claims of this text are complete bullshit. No idea what this "app" is about,
anyways.
Dont need it. Can go to the trash can of history
→ <a href="https://www.reddit.com/r/opensource/comments/1dgis44/comment/l8qn0v9/">reddit.com</a></p>
</blockquote>
<p>While most of the replies weren't constructive, the quoted
<a href="https://lobste.rs/s/qeqgiq/why_yaak_is_not_open_source#c_k7tr74">500-word lobste.rs comment</a>
is absolute gold. It immediately got me thinking that <strong>maybe I was wrong.</strong> 🙈</p>
<p>I wanted feedback, and that's exactly what I got.</p>
<p>The main theme was that <em><strong>open source doesn't mean open contribution.</strong></em> You can still
get most of the benefits by simply making the code public:</p>
<ul>
<li>Open to security audits</li>
<li>Transparent functionality (no shady stuff)</li>
<li>Flexibility (can still fork and modify)</li>
<li>Runnable even if the developer leaves</li>
</ul>
<p>I wanted Yaak users to experience these benefits too, which is why Yaak is now open source
but closed (mostly) for contribution.</p>
<h2 id="-open-source-closedish-contribution">🙈 Open source, closed(ish) contribution</h2>
<p>Many commenters pointed to projects like SQLite which are open-source but don't allow
outside contribution. For some reason, I never even considered this to be a possibility.
While Googling for more projects like this, I came
across <a href="https://github.com/benbjohnson/litestream?tab=readme-ov-file#contribution-policy">Litestream</a>,
which was initially closed to contribution and eventually opened for bug-fixes only.</p>
<p>This model made perfect sense to me, so it's exactly what I'm doing. <strong>Yaak is now open
source under the MIT license and open to contribution for bug fixes only.</strong></p>
<p><em>Now the desktop Linux users can fix their own bugs (please don't hate me)</em> 😅</p>
<p>~ Greg</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AppleWatchAmmeter (252 pts)]]></title>
            <link>https://github.com/jp3141/AppleWatchAmmeter</link>
            <guid>41508752</guid>
            <pubDate>Wed, 11 Sep 2024 07:01:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jp3141/AppleWatchAmmeter">https://github.com/jp3141/AppleWatchAmmeter</a>, See on <a href="https://news.ycombinator.com/item?id=41508752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">AppleWatchAmmeter</h2><a id="user-content-applewatchammeter" aria-label="Permalink: AppleWatchAmmeter" href="#applewatchammeter"></a></p>
<p dir="auto">Turn your Apple Watch or any watch with an accessible magnetometer into an ammeter to measure DC currents.</p>
<p dir="auto">Apple Watches newer than Series 5 which have a compass also contain a magnetometer. The magnetometer is also sensitive to magnetic fields from nearby currents.</p>
<p dir="auto">This demonstration uses a coil of wire around the watch to alter the magnetic field sensed. A circular coil of wire with N turns and a diameter D will generate a magnetic field of B = u0.I/D (u0 is defined to be 4.π.10^-7); so for a watch with a diameter of about 48 mm (approximately Apple Watch 5), 5 turns with 1 A will generate a field of 5 * 4.π.10^-7 * 1/0.048 = 131 uT (or 1.3 gauss). Because the magnetometer is not centered in the coil (it's usually in the very bottom right corner of the watch), the actual sensitivity is a bit lower; 100 uT/A is a reasonable approximation that can be improved by calibrating.</p>
<p dir="auto">An app could easily be written to perform some calibration and zero offset as well as display current in Amperes. however nearly any app that can display raw magnetometer data can be used. I use 'Sensor-App' which is free. Note that you can only detect DC currents; the magnetometer is too slow to react to AC (e.g. household power) currents.</p>
<p dir="auto">In this demonstration, a 5-turn coil can show curent changes of about 10 mA (about 1 uT in the Z direction).</p>
<p dir="auto">The Earth's background magnetic field is around 60 uT (not only in a North-South direction). With a 5-turn winding, each 1 A generates an additional ~ 100 uT in the Z direction; there is some noise in the readings, but with care, 10 mA (1 uT) changes can be discerned.</p>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7691273/365412852-7a0e5de4-8679-49e0-82f7-6bd6a99a204e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwNzYxMDQsIm5iZiI6MTcyNjA3NTgwNCwicGF0aCI6Ii83NjkxMjczLzM2NTQxMjg1Mi03YTBlNWRlNC04Njc5LTQ5ZTAtODJmNy02YmQ2YTk5YTIwNGUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkxMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MTFUMTczMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDQzMjFlMGE1OTNmNGE1NTU2YjRiNWQ0NjU4NThjZTFkYTA5YjAwNTg1NTJiOTM1ZGRjYzBhMmQ3M2E3OTczMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.h_lD85_fPXR6nYdVzt5xsFIu1wb3kD_3sD2MJvlmAqM"><img width="483" alt="Watch Ammeter" src="https://private-user-images.githubusercontent.com/7691273/365412852-7a0e5de4-8679-49e0-82f7-6bd6a99a204e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwNzYxMDQsIm5iZiI6MTcyNjA3NTgwNCwicGF0aCI6Ii83NjkxMjczLzM2NTQxMjg1Mi03YTBlNWRlNC04Njc5LTQ5ZTAtODJmNy02YmQ2YTk5YTIwNGUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkxMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MTFUMTczMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDQzMjFlMGE1OTNmNGE1NTU2YjRiNWQ0NjU4NThjZTFkYTA5YjAwNTg1NTJiOTM1ZGRjYzBhMmQ3M2E3OTczMyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.h_lD85_fPXR6nYdVzt5xsFIu1wb3kD_3sD2MJvlmAqM"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/7691273/365460942-f63312f5-ba97-4c33-8e38-28b9935bb610.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwNzYxMDQsIm5iZiI6MTcyNjA3NTgwNCwicGF0aCI6Ii83NjkxMjczLzM2NTQ2MDk0Mi1mNjMzMTJmNS1iYTk3LTRjMzMtOGUzOC0yOGI5OTM1YmI2MTAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkxMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MTFUMTczMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2NkODI3M2IxNDI0NTI2YmQ5M2FkMWVlN2NlZDJkZGFkMTRlODI4YTczMjFlZmQ5MzhkZmQyOTFmMWRhZTNkOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.NEAEDPhVk_FOi8Bwx00Y3CwBaxKYmgcV72SWI753VtU"><img width="998" alt="Coil holder with wires" src="https://private-user-images.githubusercontent.com/7691273/365460942-f63312f5-ba97-4c33-8e38-28b9935bb610.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwNzYxMDQsIm5iZiI6MTcyNjA3NTgwNCwicGF0aCI6Ii83NjkxMjczLzM2NTQ2MDk0Mi1mNjMzMTJmNS1iYTk3LTRjMzMtOGUzOC0yOGI5OTM1YmI2MTAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkxMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MTFUMTczMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2NkODI3M2IxNDI0NTI2YmQ5M2FkMWVlN2NlZDJkZGFkMTRlODI4YTczMjFlZmQ5MzhkZmQyOTFmMWRhZTNkOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.NEAEDPhVk_FOi8Bwx00Y3CwBaxKYmgcV72SWI753VtU"></a>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenSSH Keystroke Obfuscation Bypass (140 pts)]]></title>
            <link>https://crzphil.github.io/posts/ssh-obfuscation-bypass/</link>
            <guid>41508530</guid>
            <pubDate>Wed, 11 Sep 2024 06:19:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://crzphil.github.io/posts/ssh-obfuscation-bypass/">https://crzphil.github.io/posts/ssh-obfuscation-bypass/</a>, See on <a href="https://news.ycombinator.com/item?id=41508530">Hacker News</a></p>
Couldn't get https://crzphil.github.io/posts/ssh-obfuscation-bypass/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Possibly all the ways to get loop-finding in graphs wrong (197 pts)]]></title>
            <link>https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/findloop/</link>
            <guid>41508355</guid>
            <pubDate>Wed, 11 Sep 2024 05:47:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/findloop/">https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/findloop/</a>, See on <a href="https://news.ycombinator.com/item?id=41508355">Hacker News</a></p>
Couldn't get https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/findloop/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>