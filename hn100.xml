<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 07 Jun 2024 11:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Ice - open source bar manager for macOS (293 pts)]]></title>
            <link>https://github.com/jordanbaird/Ice</link>
            <guid>40605532</guid>
            <pubDate>Fri, 07 Jun 2024 05:11:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/jordanbaird/Ice">https://github.com/jordanbaird/Ice</a>, See on <a href="https://news.ycombinator.com/item?id=40605532">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto">
    <p><a target="_blank" rel="noopener noreferrer" href="https://github.com/jordanbaird/Ice/blob/main/Ice/Assets.xcassets/AppIcon.appiconset/icon_256x256.png"><img src="https://github.com/jordanbaird/Ice/raw/main/Ice/Assets.xcassets/AppIcon.appiconset/icon_256x256.png" width="200" height="200"></a></p><p dir="auto"><h2 tabindex="-1" dir="auto">Ice</h2><a id="user-content-ice" aria-label="Permalink: Ice" href="#ice"></a></p>
</div>
<p dir="auto">Ice is a powerful menu bar management tool. While its primary function is hiding and showing menu bar items, it aims to cover a wide variety of additional features to make it one of the most versatile menu bar tools available.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/90936861/337269729-e256575b-880c-49ee-90a1-b6eabd4a7868.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMzcyNjk3MjktZTI1NjU3NWItODgwYy00OWVlLTkwYTEtYjZlYWJkNGE3ODY4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiN2Q5NGYwOTVkODFmNjY2MjhmOWUzY2FkNjc2MTJkMjJiY2UxZjljMjIwM2E0NGJiMzg2MDc3Yzk1NTYzZGImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ZAQaQeWfoC14wJuXpebfc2vSdt0FCoyRe0YbnYtszDU"><img src="https://private-user-images.githubusercontent.com/90936861/337269729-e256575b-880c-49ee-90a1-b6eabd4a7868.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMzcyNjk3MjktZTI1NjU3NWItODgwYy00OWVlLTkwYTEtYjZlYWJkNGE3ODY4LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiN2Q5NGYwOTVkODFmNjY2MjhmOWUzY2FkNjc2MTJkMjJiY2UxZjljMjIwM2E0NGJiMzg2MDc3Yzk1NTYzZGImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.ZAQaQeWfoC14wJuXpebfc2vSdt0FCoyRe0YbnYtszDU" alt="Banner"></a></p>
<p dir="auto"><a href="https://github.com/jordanbaird/Ice/releases/latest"><img src="https://camo.githubusercontent.com/01a1e5eace1f29146439e52ca905bb1944101aa8e1439ad28016634ab09d4cf2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f776e6c6f61642d6c61746573742d627269676874677265656e3f7374796c653d666c61742d737175617265" alt="Download" data-canonical-src="https://img.shields.io/badge/download-latest-brightgreen?style=flat-square"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d2da2d8531376e348989bf9b38fc124d66edb8d942c7b5124d2f05648d5f6531/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f532d626c75653f7374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/d2da2d8531376e348989bf9b38fc124d66edb8d942c7b5124d2f05648d5f6531/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d6d61634f532d626c75653f7374796c653d666c61742d737175617265" alt="Platform" data-canonical-src="https://img.shields.io/badge/platform-macOS-blue?style=flat-square"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e6fa7e0d369a9b73e179ef17bef802dc7f20905b64989f2b74028c9727936970/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726571756972656d656e74732d6d61634f5325323031342532422d6661346534393f7374796c653d666c61742d737175617265"><img src="https://camo.githubusercontent.com/e6fa7e0d369a9b73e179ef17bef802dc7f20905b64989f2b74028c9727936970/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f726571756972656d656e74732d6d61634f5325323031342532422d6661346534393f7374796c653d666c61742d737175617265" alt="Requirements" data-canonical-src="https://img.shields.io/badge/requirements-macOS%2014%2B-fa4e49?style=flat-square"></a>
<a href="https://jordanbaird.gumroad.com/l/ice" rel="nofollow"><img src="https://camo.githubusercontent.com/9bd6b0db10c755135ba6f7fa7e3f6771510682e448650ee11a55b98b9a1cac24/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f537570706f72742532302545322539442541342545462542382538462d3841324245323f7374796c653d666c61742d737175617265" alt="Support" data-canonical-src="https://img.shields.io/badge/Support%20%E2%9D%A4%EF%B8%8F-8A2BE2?style=flat-square"></a>
<a href="https://icemenubar.app/" rel="nofollow"><img src="https://camo.githubusercontent.com/cd06400f307d1a54e0c33e65a7d4541a059345a59cc24a0f809914b5025fc38a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f576562736974652d3031354642413f7374796c653d666c61742d737175617265" alt="Website" data-canonical-src="https://img.shields.io/badge/Website-015FBA?style=flat-square"></a>
<a href="https://github.com/jordanbaird/Ice/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/07b911e5dcbb76995245bb90ea5b4153f7c2f2f2ac8b688c0f4514da4e7f978a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a6f7264616e62616972642f4963653f7374796c653d666c61742d737175617265" alt="License" data-canonical-src="https://img.shields.io/github/license/jordanbaird/Ice?style=flat-square"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">Ice is currently in active development. Some features have not yet been implemented. Download the latest release <a href="https://github.com/jordanbaird/Ice/releases/latest">here</a> and see the roadmap below for upcoming features.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Simply <code>Command</code> + drag your menu bar items to rearrange them.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/jordanbaird/Ice/blob/main/Resources/rearranging.gif"><img src="https://github.com/jordanbaird/Ice/raw/main/Resources/rearranging.gif" alt="Rearranging" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install</h2><a id="user-content-install" aria-label="Permalink: Install" href="#install"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual Installation</h3><a id="user-content-manual-installation" aria-label="Permalink: Manual Installation" href="#manual-installation"></a></p>
<p dir="auto">Download the "Ice.zip" file from the <a href="https://github.com/jordanbaird/Ice/releases/latest">latest release</a> and move the unzipped app into your <code>Applications</code> folder.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebrew</h3><a id="user-content-homebrew" aria-label="Permalink: Homebrew" href="#homebrew"></a></p>
<p dir="auto">If you use <a href="https://brew.sh/" rel="nofollow">Homebrew</a>, you can install Ice using the following command:</p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install jordanbaird-ice"><pre>brew install jordanbaird-ice</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features/Roadmap</h2><a id="user-content-featuresroadmap" aria-label="Permalink: Features/Roadmap" href="#featuresroadmap"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Menu bar item management</h3><a id="user-content-menu-bar-item-management" aria-label="Permalink: Menu bar item management" href="#menu-bar-item-management"></a></p>
<ul>
<li> Hide menu bar items</li>
<li> "Always Hidden" menu bar section</li>
<li> Show hidden items when hovering over the menu bar</li>
<li> Show hidden items when an empty area in the menu bar is clicked</li>
<li> Show hidden items by scrolling or swiping in the menu bar</li>
<li> Automatically rehide</li>
<li> Hide application menus when they overlap with shown menu bar items</li>
<li> Drag and drop interface to arrange individual items</li>
<li> Search menu bar items</li>
<li> Display hidden items in a separate bar (e.g. for MacBooks with the notch)</li>
<li> Custom spacing between items</li>
<li> Profiles for menu bar layout</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Menu bar appearance</h3><a id="user-content-menu-bar-appearance" aria-label="Permalink: Menu bar appearance" href="#menu-bar-appearance"></a></p>
<ul>
<li> Menu bar tint (solid and gradient)</li>
<li> Menu bar shadow</li>
<li> Menu bar border</li>
<li> Custom menu bar shapes (rounded and/or split)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hotkeys</h3><a id="user-content-hotkeys" aria-label="Permalink: Hotkeys" href="#hotkeys"></a></p>
<ul>
<li> Toggle individual menu bar sections</li>
<li> Toggle application menus</li>
<li> Show/hide section divider icons</li>
<li> Temporarily show individual menu bar items</li>
<li> Enable/disable auto rehide</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Other</h3><a id="user-content-other" aria-label="Permalink: Other" href="#other"></a></p>
<ul>
<li> Launch at login</li>
<li> Automatic updates</li>
<li> Menu bar widgets</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why the name?</h2><a id="user-content-why-the-name" aria-label="Permalink: Why the name?" href="#why-the-name"></a></p>
<p dir="auto">Because your menu bar becomes like ice, allowing your menu bar items to slide away 🧊🧊🧊</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why not support earlier operating systems?</h2><a id="user-content-why-not-support-earlier-operating-systems" aria-label="Permalink: Why not support earlier operating systems?" href="#why-not-support-earlier-operating-systems"></a></p>
<p dir="auto">There are a number of system APIs that Ice uses that are only available starting with macOS 14.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Gallery</h2><a id="user-content-gallery" aria-label="Permalink: Gallery" href="#gallery"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Choose your own icon to show in the menu bar</h4><a id="user-content-choose-your-own-icon-to-show-in-the-menu-bar" aria-label="Permalink: Choose your own icon to show in the menu bar" href="#choose-your-own-icon-to-show-in-the-menu-bar"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/90936861/316565649-ded03af0-f6cf-4bc1-a094-af0cf32ead96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NjU2NDktZGVkMDNhZjAtZjZjZi00YmMxLWEwOTQtYWYwY2YzMmVhZDk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQyYjBmNDc2NzY4ZDA4MDkzODk0YzMwMGY0NzY3ODJmNGEyZjcxZDUyMzI1MzM2ODg3NjczZjUyNzYzYjEzNTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.OdzlaKbWN6tW8FHldIAzabXMTfl8l3ujByE9iSkkms0"><img src="https://private-user-images.githubusercontent.com/90936861/316565649-ded03af0-f6cf-4bc1-a094-af0cf32ead96.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NjU2NDktZGVkMDNhZjAtZjZjZi00YmMxLWEwOTQtYWYwY2YzMmVhZDk2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWQyYjBmNDc2NzY4ZDA4MDkzODk0YzMwMGY0NzY3ODJmNGEyZjcxZDUyMzI1MzM2ODg3NjczZjUyNzYzYjEzNTkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.OdzlaKbWN6tW8FHldIAzabXMTfl8l3ujByE9iSkkms0" alt="Custom Icon Example"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Customize the menu bar's appearance</h4><a id="user-content-customize-the-menu-bars-appearance" aria-label="Permalink: Customize the menu bar's appearance" href="#customize-the-menu-bars-appearance"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/90936861/316568231-99a59cc7-914a-4f8a-af4b-11a5193534da.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NjgyMzEtOTlhNTljYzctOTE0YS00ZjhhLWFmNGItMTFhNTE5MzUzNGRhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ0YTMzY2Q3MDY5NTdiYWE3OWNkY2ViYjI0ZDMzODlkZTI5MDU3OWFiZDI3NmFhZThhNGZhNjI2Y2NlZmMyMTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.15HYZuDRgDllKmNA3U944pqxgRskhkMERX1c0EePsmk"><img src="https://private-user-images.githubusercontent.com/90936861/316568231-99a59cc7-914a-4f8a-af4b-11a5193534da.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NjgyMzEtOTlhNTljYzctOTE0YS00ZjhhLWFmNGItMTFhNTE5MzUzNGRhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ0YTMzY2Q3MDY5NTdiYWE3OWNkY2ViYjI0ZDMzODlkZTI5MDU3OWFiZDI3NmFhZThhNGZhNjI2Y2NlZmMyMTAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.15HYZuDRgDllKmNA3U944pqxgRskhkMERX1c0EePsmk" alt="Menu Bar Appearance Example 1"></a>
<a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/90936861/316570684-ba3bd8d9-0eeb-447b-8a62-1bc811319132.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NzA2ODQtYmEzYmQ4ZDktMGVlYi00NDdiLThhNjItMWJjODExMzE5MTMyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk0ZGEzYzQ3NWJmMjcwMjcwMmVkMzE2Mjc0YTg3ODUzZGU3NDYxOGJlZTNkMWQwMjdiYTEyNTJmYzM3MGZjZDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8hYg4P60AFYj_4Oub82t_J7hClwIjWHijaofXPOqXbA"><img src="https://private-user-images.githubusercontent.com/90936861/316570684-ba3bd8d9-0eeb-447b-8a62-1bc811319132.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDU3MDMsIm5iZiI6MTcxNzc0NTQwMywicGF0aCI6Ii85MDkzNjg2MS8zMTY1NzA2ODQtYmEzYmQ4ZDktMGVlYi00NDdiLThhNjItMWJjODExMzE5MTMyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA3MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk0ZGEzYzQ3NWJmMjcwMjcwMmVkMzE2Mjc0YTg3ODUzZGU3NDYxOGJlZTNkMWQwMjdiYTEyNTJmYzM3MGZjZDQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8hYg4P60AFYj_4Oub82t_J7hClwIjWHijaofXPOqXbA" alt="Menu Bar Appearance Example 2"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Ice is available under the <a href="https://github.com/jordanbaird/Ice/blob/main/LICENSE">MIT license</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CPU Energy Meter: A tool for measuring energy consumption of Intel CPUs (104 pts)]]></title>
            <link>https://github.com/sosy-lab/cpu-energy-meter</link>
            <guid>40604596</guid>
            <pubDate>Fri, 07 Jun 2024 01:59:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sosy-lab/cpu-energy-meter">https://github.com/sosy-lab/cpu-energy-meter</a>, See on <a href="https://news.ycombinator.com/item?id=40604596">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto"><h2 tabindex="-1" dir="auto">CPU Energy Meter</h2><a id="user-content-cpu-energy-meter" aria-label="Permalink: CPU Energy Meter" href="#cpu-energy-meter"></a></p>
<p dir="auto"><a href="https://gitlab.com/sosy-lab/software/cpu-energy-meter/pipelines" rel="nofollow"><img src="https://camo.githubusercontent.com/3b2e2e44e005c60ee6d71f8c6809f83c193ed224c1d0b42bb2223967c4564a6b/68747470733a2f2f6769746c61622e636f6d2f736f73792d6c61622f736f6674776172652f6370752d656e657267792d6d657465722f6261646765732f6d61696e2f706970656c696e652e737667" alt="Build Status" data-canonical-src="https://gitlab.com/sosy-lab/software/cpu-energy-meter/badges/main/pipeline.svg"></a>
<a href="https://github.com/sosy-lab/cpu-energy-meter/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/1aaf5e71abd8dbe565cc0ed7ba58a18597fa5a29ac1f05fc2008cf6006b39a2f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4253442d2d332d2d636c617573652d627269676874677265656e2e737667" alt="BSD-3-Clause License" data-canonical-src="https://img.shields.io/badge/license-BSD--3--clause-brightgreen.svg"></a>
<a href="https://github.com/sosy-lab/cpu-energy-meter/releases"><img src="https://camo.githubusercontent.com/dd46eab6b8c7c31d612c1a172019367238c4833663dc6d26dcbfb1d715b93742/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f736f73792d6c61622f6370752d656e657267792d6d657465722e737667" alt="Releases" data-canonical-src="https://img.shields.io/github/release/sosy-lab/cpu-energy-meter.svg"></a>
<a href="https://zenodo.org/badge/latestdoi/46493895" rel="nofollow"><img src="https://camo.githubusercontent.com/4b9e215a2babde7837ff8b29e2f952dffc23d9f1e276e5a581f87a5bdd7d1b56/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f34363439333839352e737667" alt="DOI via Zenodo" data-canonical-src="https://zenodo.org/badge/46493895.svg"></a></p>
<p dir="auto">CPU Energy Meter is a Linux tool that allows to monitor power consumption of Intel CPUs
at fine time granularities (few tens of milliseconds).
Power monitoring is available for the following power domains:</p>
<ul dir="auto">
<li>per package domain (CPU socket)</li>
<li>per core domain (all the CPU cores on a package)</li>
<li>per uncore domain (uncore components, e.g., integrated graphics on client CPUs)</li>
<li>per memory node (memory local to a package, server CPUs only)</li>
<li>per platform (all devices in the platform that receive power from integrated
power delivery mechanism, e.g., processor cores, SOC, memory, add-on or
peripheral devices)</li>
</ul>
<p dir="auto">To do this, the tool uses a feature of Intel CPUs that is called <a href="https://en.wikipedia.org/wiki/Running_average_power_limit" rel="nofollow">RAPL (Running Average Power Limit)</a>,
which is documented in the <a href="https://software.intel.com/en-us/articles/intel-sdm" rel="nofollow">Intel Software Developers Manual</a>, Volume 3B Chapter 14.9.
RAPL is available on CPUs from the generation <a href="https://en.wikipedia.org/wiki/Sandy_Bridge" rel="nofollow">Sandy Bridge</a> and later.
Because CPU Energy Meter uses the maximal possible measurement interval
(depending on the hardware this is between a few minutes and an hour),
it causes negligible overhead.</p>
<p dir="auto">CPU Energy Meter is a fork of the <a href="https://software.intel.com/en-us/articles/intel-power-gadget-20" rel="nofollow">Intel Power Gadget</a>
and developed at the <a href="https://www.sosy-lab.org/" rel="nofollow">Software Systems Lab</a>
of the <a href="https://www.uni-muenchen.de/" rel="nofollow">Ludwig-Maximilians-Universität München (LMU Munich)</a>
under the <a href="https://github.com/sosy-lab/cpu-energy-meter/blob/main/LICENSE">BSD-3-Clause License</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">For Debian or Ubuntu the easiest way is to install from our <a href="https://launchpad.net/~sosy-lab/+archive/ubuntu/benchmarking" rel="nofollow">PPA</a>:</p>
<div data-snippet-clipboard-copy-content="sudo add-apt-repository ppa:sosy-lab/benchmarking
sudo apt install cpu-energy-meter"><pre><code>sudo add-apt-repository ppa:sosy-lab/benchmarking
sudo apt install cpu-energy-meter
</code></pre></div>
<p dir="auto">Alternatively, you can download our <code>.deb</code> package from <a href="https://github.com/sosy-lab/cpu-energy-meter/releases">GitHub</a>
and install it with <code>apt install ./cpu-energy-meter*.deb</code>.</p>
<p dir="auto">Dependencies of CPU Energy Meter are <a href="https://sites.google.com/site/fullycapable/" rel="nofollow">libcap</a>,
which is available on most Linux distributions in package <code>libcap</code> (e.g., Fedora)
or <code>libcap2</code> (e.g, Debian and Ubuntu: <code>sudo apt install libcap2</code>),
and a Linux kernel with the MSR module (available by default)</p>
<p dir="auto">Alternatively, for running CPU Energy Meter from source (quick and dirty):</p>
<div data-snippet-clipboard-copy-content="sudo apt install libcap-dev
sudo modprobe msr
make
sudo ./cpu-energy-meter"><pre><code>sudo apt install libcap-dev
sudo modprobe msr
make
sudo ./cpu-energy-meter
</code></pre></div>
<p dir="auto">It is also possible (and recommended) to run CPU Energy Meter without root.
To do so, the following needs to be done:</p>
<ul dir="auto">
<li>Load kernel modules <code>msr</code> and <code>cpuid</code>.</li>
<li>Add a group <code>msr</code>.</li>
<li>Add a Udev rule that grants access to <code>/dev/cpu/*/msr</code> to group <code>msr</code> (<a href="https://github.com/sosy-lab/cpu-energy-meter/blob/main/debian/additional_files/59-msr.rules">example</a>).</li>
<li>Run <code>chgrp msr</code>, <code>chmod 2711</code>, and <code>setcap cap_sys_rawio=ep</code> on the binary (<code>make setup</code> is a shortcut for this).</li>
</ul>
<p dir="auto">The provided Debian package in our <a href="https://launchpad.net/~sosy-lab/+archive/ubuntu/benchmarking" rel="nofollow">PPA</a>
and on <a href="https://github.com/sosy-lab/cpu-energy-meter/releases">GitHub</a> does these steps automatically
and lets all users execute CPU Energy Meter.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to use it</h2><a id="user-content-how-to-use-it" aria-label="Permalink: How to use it" href="#how-to-use-it"></a></p>
<div data-snippet-clipboard-copy-content="cpu-energy-meter [-d] [-e sampling_delay_ms] [-r]"><pre><code>cpu-energy-meter [-d] [-e sampling_delay_ms] [-r]
</code></pre></div>
<p dir="auto">The tool will continue counting the cumulative energy use of all supported CPUs
in the background and will report a key-value list of its measurements when it
receives SIGINT (Ctrl+C):</p>
<div data-snippet-clipboard-copy-content="+--------------------------------------+
| CPU-Energy-Meter            Socket 0 |
+--------------------------------------+
Duration                  2.504502 sec
Package                   3.769287 Joule
Core                      0.317749 Joule
Uncore                    0.010132 Joule
DRAM                      0.727783 Joule
PSYS                     29.792603 Joule"><pre><code>+--------------------------------------+
| CPU-Energy-Meter            Socket 0 |
+--------------------------------------+
Duration                  2.504502 sec
Package                   3.769287 Joule
Core                      0.317749 Joule
Uncore                    0.010132 Joule
DRAM                      0.727783 Joule
PSYS                     29.792603 Joule
</code></pre></div>
<p dir="auto">To get intermediate measurements, send signal <code>USR1</code> to the process.</p>
<p dir="auto">Optionally, the tool can be executed with parameter <code>-r</code>
to print the output as a raw (easily parsable) list:</p>
<div data-snippet-clipboard-copy-content="cpu_count=1
duration_seconds=3.241504
cpu0_package_joules=4.971924
cpu0_core_joules=0.461182
cpu0_uncore_joules=0.053406
cpu0_dram_joules=0.953979
cpu0_psys_joules=38.904785"><pre><code>cpu_count=1
duration_seconds=3.241504
cpu0_package_joules=4.971924
cpu0_core_joules=0.461182
cpu0_uncore_joules=0.053406
cpu0_dram_joules=0.953979
cpu0_psys_joules=38.904785
</code></pre></div>
<p dir="auto">The parameter <code>-d</code> adds debug output.
By default, CPU Energy Meter computes the necessary measurement interval automatically,
this can be overridden with the parameter <code>-e</code>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Uganda's surveillance state is built on national ID cards (124 pts)]]></title>
            <link>https://www.bloomberg.com/news/features/2024-06-04/uganda-yoweri-museveni-s-critics-targeted-via-biometric-id-system</link>
            <guid>40603692</guid>
            <pubDate>Thu, 06 Jun 2024 23:36:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/features/2024-06-04/uganda-yoweri-museveni-s-critics-targeted-via-biometric-id-system">https://www.bloomberg.com/news/features/2024-06-04/uganda-yoweri-museveni-s-critics-targeted-via-biometric-id-system</a>, See on <a href="https://news.ycombinator.com/item?id=40603692">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Make your program slower with threads (2014) (133 pts)]]></title>
            <link>https://brooker.co.za/blog/2014/12/06/random.html</link>
            <guid>40603625</guid>
            <pubDate>Thu, 06 Jun 2024 23:28:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brooker.co.za/blog/2014/12/06/random.html">https://brooker.co.za/blog/2014/12/06/random.html</a>, See on <a href="https://news.ycombinator.com/item?id=40603625">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post">


<p>How much do context switches matter?</p>

<p>Years ago, while taking a numerical methods course, I wrote some code to calculate the expected number of shared birthdays in a group. The code is very simple: each attempt constructs a vector of N birthdays, then counts the duplicates. The outer loop runs millions of attempts, and calculates the mean number of shared birthdays across all the samples. It’s little more than a tight loop around a pseudo-random number generator.</p>

<p>I was also learning about threading at the time, and decided that I could speed up my program by running it on the lab’s shiny dual-core machine. I knew that communicating between threads was expensive, so I had each of my threads calculate their attempts in parallel, and merge the results right at the end. I was expecting a great speedup. Much to my disappointment, though, the multi-threaded version was slower. Much, much, slower.</p>

<p>Much like the <a href="http://en.wikipedia.org/wiki/Birthday_problem">birthday paradox</a> runs counter to our intuition about statistics, the behavior of bad multi-threaded programs runs counter to our intuition about computer performance. We’re used to computers being much faster than they used to be, and single-threaded efficiency mattering less than it used to in most cases. Counter to that intuition, the gap between <em>good</em> and <em>bad</em> multithreaded programs has gotten worse over time.</p>

<p>To illustrate just how bad it can be, I replicated my program from back then. It’s not much more than a multi-threaded tight loop around <em>random(3)</em>. It’s nice and quick single-threaded: running 10 million attempts in under 7 seconds. Going up to two threads makes it a bit faster, down to less than 6 seconds. When we hit three threads (on my four core Haswell E3-1240), it all goes horribly wrong:</p>

<p><img src="https://s3.amazonaws.com/mbrooker-blog-images/threads_bar.png" alt=""></p>

<p>To figure out what’s wrong, we can turn to Linux’s excellent <a href="https://perf.wiki.kernel.org/index.php/Tutorial">perf</a> tool. Running the 1-thread and 4-thread versions with <em>perf stat</em> make it obvious that something’s going on. For 1 thread:</p>

<div><pre><code>     3,788,352 L1-dcache-load-misses #0.03% of all L1-dcache hits
43,399,424,441 instructions  #1.46  insns per cycle
           734 context-switches
</code></pre></div>

<p>and for four threads:</p>

<div><pre><code>  4,110,904,396 L1-dcache-load-misses #6.88% of all L1-dcache hits
248,853,610,160 instructions # 0.51  insns per cycle
     15,993,647 context-switches
</code></pre></div>

<p>Two things are going wrong here. One is that we’re seeing a more L1 cache misses with more threads, but the bigger issue is that we’re seeing <em>a whole lot more</em> context switches. The effect of both of these is visible in the much lower <em>instructions per cycle</em> of the second version. There’s no nice constant for the cost of a context switch, but a good modern estimate is around 3μs. Multiplying 3μs by 16 million context switches gives 48 seconds, which is a good hint that we’re headed in the right direction. So, what’s causing the context switches?</p>

<p>Back to <em>perf</em>, this time running <em>perf record</em> on the processes, followed by <em>perf report</em>. First, the top few rows for the single-threaded version:</p>

<div><pre><code># Overhead   Command  Shared Object    Symbol
# ........  ........  ..............   ........................
62.01%  birthday  libc-2.19.so         [.] msort_with_tmp.part.0
11.40%  birthday  libc-2.19.so         [.] __memcpy_sse2        
10.19%  birthday  birthday             [.] simulate
</code></pre></div>

<p>We’re spending 62% of the time sorting the array, which is used to find the duplicates. That’s about what I would have guessed. What about the version with four threads?</p>

<div><pre><code># Overhead   Command  Shared Object  Symbol
# ........  ........  .............  ............
46.80%  birthday  [kernel.kallsyms]  [k] _raw_spin_lock   
 8.86%  birthday  libc-2.19.so       [.] __random           
 3.42%  birthday  libc-2.19.so       [.] __lll_lock_wait_private
 3.23%  birthday  [kernel.kallsyms]  [k] try_to_wake_up       
 2.95%  birthday  libc-2.19.so       [.] __random_r        
 2.79%  birthday  libc-2.19.so       [.] msort_with_tmp.part.0
 2.10%  birthday  [kernel.kallsyms]  [k] futex_wake 
 1.46%  birthday  [kernel.kallsyms]  [k] system_call  
 1.35%  birthday  [kernel.kallsyms]  [k] get_futex_value_locked 
 1.15%  birthday  [kernel.kallsyms]  [k] futex_wait_setup  
 1.14%  birthday  [kernel.kallsyms]  [k] futex_wait 
</code></pre></div>

<p>Well, that’s suspicious. There aren’t any locks in my code, but there are a whole lot of references to locks in the trace. <em>raw_spin_lock</em> is obviously a candidate, and it’s suspicious to see so many <a href="http://en.wikipedia.org/wiki/Futex">futex</a>-related calls. Something’s taking locks, and the fact that <em>random</em> is near the top of the list makes it a likely candidate. Before we dive in there, though, let’s confirm that we’re doing a lot of syscalls:</p>

<div><pre><code>sudo perf stat -e 'syscalls:sys_e*' ./birthday
</code></pre></div>

<p>Which spits out a long list of system calls, most (like <em>mmap</em>) with just a handful of hits. There are two huge outliers:</p>

<div><pre><code>46,889,267 syscalls:sys_enter_futex
46,889,267 syscalls:sys_exit_futex
</code></pre></div>

<p>That confirms it, something’s taking a lot of futexes. Knowing whether it’s <em>random</em> or not requires a dive into the <em>glibc</em> source, which nearly instantly <a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=stdlib/random.c;h=c75d1d96adecf5ac894ca752a4c54647014bd746;hb=9752c3cdbce2b3b8338abf09c8b9dd9e78908b8a#l194">reveals something suspicious</a>:</p>

<div><pre><code> /* POSIX.1c requires that there is mutual exclusion for the `rand' and
  `srand' functions to prevent concurrent calls from modifying common
   data.  */
</code></pre></div>

<p>And, <a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=stdlib/random.c;h=c75d1d96adecf5ac894ca752a4c54647014bd746;hb=9752c3cdbce2b3b8338abf09c8b9dd9e78908b8a#l292">just a little bit further down</a>:</p>

<div><pre><code> __libc_lock_lock (lock);
 (void) __random_r (&amp;unsafe_state, &amp;retval);
 __libc_lock_unlock (lock);
</code></pre></div>

<p>Getting rid of the locks means getting rid of one of two things: shared state, or the necessity to prevent concurrent modification to that state. It seems like the former is easier: reasoning about a data-race-safe PRNG is tricky. There are a many good ways to get rid of shared state in the PRNG. Linux has one particularly convenient way: the C library exposes a reentrant random number generator called <a href="http://man7.org/linux/man-pages/man3/random_r.3.html">random_r</a> (which is used by <em>random</em>, as you can see from the snippet above). Dropping <em>random_r</em> in place of <em>random</em> has an amazing effect:</p>

<p><img src="https://s3.amazonaws.com/mbrooker-blog-images/threads_bar_second.png" alt=""></p>

<p>As expected, the context switches are way down and instructions per cycle is nicely improved:</p>

<div><pre><code>     4,166,540 L1-dcache-load-misses  # 0.04% of all L1-dcache hits
40,201,461,769 instructions # 1.43  insns per cycle
           572 context-switches
</code></pre></div>

<p>I recognize that spinning on a tight loop on <em>random</em> is a contrived example, but it’s not too far away from reality. Many programs that multi-thread for performance end up with library or system calls inside relatively tight loops. Our intuition about these things tends to follow <a href="http://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl’s law</a>. At worst, it’s tempting to think, these things count as a non-parallel portion of code and lower the maximum achievable parallel speedup. In the real world, though, that’s not the case. Multi-threaded programs can, and very often do, run much more slowly than the equivalent single-threaded program.</p>

<p>It’s just another thing that makes writing multi-threaded code difficult.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PSA: How to keep using adblockers on Chrome and Chromium (120 pts)]]></title>
            <link>https://gist.github.com/velzie/053ffedeaecea1a801a2769ab86ab376</link>
            <guid>40602886</guid>
            <pubDate>Thu, 06 Jun 2024 22:04:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/velzie/053ffedeaecea1a801a2769ab86ab376">https://gist.github.com/velzie/053ffedeaecea1a801a2769ab86ab376</a>, See on <a href="https://news.ycombinator.com/item?id=40602886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-manifest-v2-chrome-md">
    <article itemprop="text">
<ol dir="auto">
<li>google's manifest v3 has no analouge to the <code>webRequestBlocking</code> API, which is neccesary for (effective) adblockers to work</li>
<li>starting in chrome version 127, the transition to mv3 will start cutting off the use of mv2 extensions alltogether</li>
<li>this will inevitably piss of enterprises when their extensions don't work, so the <code>ExtensionManifestV2Availability</code> key was added and will presumably stay forever after enterprises complain enough</li>
</ol>
<p dir="auto">You can use this as a regular user, which will let you keep your mv2 extensions even after they're supposed to stop working</p>
<p dir="auto"><h2 dir="auto">Linux</h2><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<p dir="auto">In a terminal, run:</p>
<div dir="auto"><pre>sudo mkdir -p /etc/opt/chrome/policies/managed /etc/opt/chromium/policies/managed
<span>echo</span> <span><span>'</span>{ "ExtensionManifestV2Availability": 2 }<span>'</span></span> <span>|</span> sudo tee /etc/opt/chrome/policies/managed/policy.json /etc/opt/chromium/policies/managed/policy.json</pre></div>
<p dir="auto"><h2 dir="auto">ChromeOS</h2><a id="user-content-chromeos" aria-label="Permalink: ChromeOS" href="#chromeos"></a></p>
<ul dir="auto">
<li>enable developer mode</li>
<li>upon rebooting, go into vt2 (shift+ctrl+right arrow function key)</li>
<li>log in as root</li>
<li>type in <code>/usr/libexec/debugd/helpers/dev_features_rootfs_verification &amp;&amp; reboot</code></li>
<li>upon rebooting, go into vt2 again and log in as root</li>
<li>run the commands from the linux section</li>
</ul>
<p dir="auto"><h2 dir="auto">Windows</h2><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<p dir="auto">Open regedit, and create <code>Software\Policies\Google\Chrome\ExtensionManifestV2Availability</code> as a dword set to <code>0x00000002</code></p>
<p dir="auto"><h2 dir="auto">MacOS</h2><a id="user-content-macos" aria-label="Permalink: MacOS" href="#macos"></a></p>
<p dir="auto">In a terminal, run:
<code>defaults write com.google.Chrome ExtensionManifestV2Availability 2</code></p>
<p dir="auto">(note that i haven't tested this for mac. please let me know if it doesn't work)</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing's starliner docks at ISS after five thrusters unexpectedly shut off (113 pts)]]></title>
            <link>https://gizmodo.com/watch-boeing-starliner-dock-iss-astronauts-1851521917</link>
            <guid>40601776</guid>
            <pubDate>Thu, 06 Jun 2024 20:06:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/watch-boeing-starliner-dock-iss-astronauts-1851521917">https://gizmodo.com/watch-boeing-starliner-dock-iss-astronauts-1851521917</a>, See on <a href="https://news.ycombinator.com/item?id=40601776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>After back-to-back failed launch attempts, a crewed Starliner capsule finally <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/boeing-first-crewed-starliner-launch-iss-nasa-tense-1851512733&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/boeing-first-crewed-starliner-launch-iss-nasa-tense-1851512733">launched on Wednesday</a></span> to deliver two NASA astronauts to low Earth orbit. Boeing is testing its capsule’s ability to transport crews to the International Space Station (ISS), with Starliner set to dock to the orbital lab on Thursday.<br></p><div data-video-id="197027" data-monetizable="true" data-position="sidebar" data-video-title="Sonos First Ever Headphones Are Too Expensive For What They Offer" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="154" data-playlist="197027,197012,196999" data-current="197027"><div><p>Sonos First Ever Headphones Are Too Expensive For What They Offer</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/197027/197027_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/197027/197027_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/197027/197027_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/197027/197027_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/22588.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p><strong>Update: June 6, 1:45 p.m. ET: </strong>Starliner missed its first docking opportunity at 12:15 p.m. ET due to technical issues, prompting NASA to target another docking window starting at around 1:30 p.m. ET. Five of the spacecraft thrusters failed during its approach, and four were subsequently recovered. As a result, NASA astronaut Butch Wilmore had to assume manual control of the crew capsule for a while before it went back into its autonomous mode. The capsule finally docked with the ISS at 1:34 p.m. ET following a nail-biting journey.</p><p><strong>Update: June 6, 1:27 p.m. ET: </strong>Starliner’s docking has not gone smoothly, the spacecraft developed trouble with its reaction control system thrusters.</p><p><strong>Update: June 6, 12:40 p.m. ET</strong>: NASA has waved off Starliner’s first docking opportunity and is now targeting a backup at 1:33 p.m. ET.</p><p><em>Original article follows.</em></p><p>Starliner is scheduled to autonomously dock to the forward-facing port of the space station’s Harmony module at approximately 12:15 p.m. on June 6, where it will remain for about a week. The hatch is set to open at 2 p.m. ET to allow the crew to board the ISS.</p><p>The docking maneuver will be broadcast live on NASA’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://nasa.gov/nasatv&quot;,{&quot;metric25&quot;:1}]]" href="http://nasa.gov/nasatv" target="_blank" rel="noopener noreferrer">website</a></span>, as well as the space agency’s <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.youtube.com/@NASA&quot;,{&quot;metric25&quot;:1}]]" href="https://www.youtube.com/@NASA" target="_blank" rel="noopener noreferrer">YouTube channel</a></span>. The live stream is set to begin at 9:30 a.m. ET, and you can tune in  through the feed below.<br></p><p>Boeing’s Starliner capsule launched atop United Launch Alliance’s Atlas V rocket, carrying NASA astronauts Butch Wilmore and Suni Williams. “Two bold NASA astronauts are well on their way on this historic first test flight of a brand-new spacecraft,” NASA Administrator Bill Nelson said in a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.nasa.gov/news-release/liftoff-nasa-astronauts-pilot-first-starliner-crewed-test-to-station/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.nasa.gov/news-release/liftoff-nasa-astronauts-pilot-first-starliner-crewed-test-to-station/" target="_blank" rel="noopener noreferrer">statement</a></span>. “Human spaceflight is a daring task—but that’s why it’s worth doing.”</p><p>The <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/review-boeing-nasa-starliner-program-1851448548&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/review-boeing-nasa-starliner-program-1851448548">Crewed Flight Test</a></span> is part of NASA’s Commercial Crew Program and is meant to transport crew and cargo to and from the International Space Station (ISS) under a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://oig.nasa.gov/docs/IG-20-005.pdf&quot;,{&quot;metric25&quot;:1}]]" href="https://oig.nasa.gov/docs/IG-20-005.pdf" target="_blank" rel="noopener noreferrer">$4.3 billion</a></span> contract with the space agency. NASA’s other commercial partner, SpaceX, has so far launched eight crews to the space station.</p><p>During Starliner’s first crewed flight, Boeing will monitor a series of automatic spacecraft maneuvers while NASA will monitor space station operations throughout the flight. Meanwhile, the astronaut crew on board will be testing the environmental control system, the displays and control system, and maneuvering the thrusters, as well as other features of the spacecraft. </p><p>The two astronauts will provide a virtual tour of the Starliner capsule from space, which will be broadcast live on Saturday, June 8 at 8:50 a.m. on <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://nasa.gov/nasatv&quot;,{&quot;metric25&quot;:1}]]" href="http://nasa.gov/nasatv" target="_blank" rel="noopener noreferrer">NASA TV</a></span>.</p><p>It’s been a struggle for Boeing to reach this stage with its Starliner program, which has been marred by technical hiccups and delays over the past few years. Throughout it all, NASA remained dedicated to seeing its crew ride on board the company’s crew capsule.</p><p>“For many of us, this is a career-defining moment bringing on a new crew transportation capability for our agency and our nation,” Steve Stich, manager, Commercial Crew Program, at NASA’s Johnson Space Center in Houston, said in a statement. “We are going to take it one step at a time, putting Starliner through its paces, and remaining vigilant until Butch and Suni safely touch down back on Earth at the conclusion of this test flight.”</p><p><em>For more spaceflight in your life, follow us on </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;http://gizspaceflight/&quot;,{&quot;metric25&quot;:1}]]" href="http://gizspaceflight/" target="_blank" rel="noopener noreferrer"><em>X</em></a></span><em> and bookmark Gizmodo’s dedicated </em><span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/science/spaceflight&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/science/spaceflight"><em>Spaceflight page</em></a></span><em>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HP bricks customers laptops with faulty automatic BIOS upgrade (154 pts)]]></title>
            <link>https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/td-p/9081096/page/3</link>
            <guid>40601711</guid>
            <pubDate>Thu, 06 Jun 2024 20:01:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/td-p/9081096/page/3">https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/td-p/9081096/page/3</a>, See on <a href="https://news.ycombinator.com/item?id=40601711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text" id="bodyDisplay_7">
			
				
					
					
						<blockquote><hr><a href="https://h30434.www3.hp.com/t5/user/viewprofilepage/user-id/7882955">@Wojtek0586</a>&nbsp;wrote:<p>Hi Dioxaz, thanks for Your work!</p><p><span>It can be very useful!</span></p><p><span>I am surprised by the fact that HP has not made an official statement on this topic. These laptops are approaching 3 years old, we have literally a dozen or so days left in which they are covered by the manufacturer's warranty. What about units that are already out of warranty? The official update broke working equipment, who will pay for the repair? The scale of the phenomenon seems to be very large and concerns the entire world. Could this be planned obsolescence of the product?</span></p><hr></blockquote><p>It looks like there has been an official statement <a href="https://h30434.www3.hp.com/t5/Notebook-Operating-System-and-Recovery/HP-Probook-BIOS-Upgrade-1-17-failed/m-p/9085148/highlight/true#M661377" target="_self">according to a previous post</a>. But I have no idea of where this was published. Me too can't believe we're the only ones in those threads who ended up with bricked 455 G7s, this incident must have struck potentially millions of machines. However I can't seem to find anything related on the internet for the moment.</p><p>Fun fact: when this incident struck me a few days ago with two users at work reporting bricked machines due to a bad BIOS update, I thought I was the only one, started to panic and doubt my IT asset management skills. Just think of it, that dreaded BIOS update even escaped the GPO I made preventing driver updates, and we're indeed running a WSUS server with manually approved updates. How could that be? Well, a smile came back to my face when a teammate alerted me that HP deleted the BIOS update from their servers. Then I saw this thread, then another one, and another one... and I finally realised. <span title=":slightly_smiling_face:">🙂</span></p><p>And yep, my initial searches leading me to clueless people advising to use a GPO to prevent BIOS updates. I don't think they have any idea of what they're talking about (especially with that dreaded “<strong>Native OS Firmware Update Service</strong>” functionality). Speaking of which, that will be our next task: setting that “Native OS Firmware Update Service” to off on all machines we can in order to disable the UEFI Capsule BIOS update for good.</p>
					
				
			
			
				
			
			
				
			
			
			
			
			
			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI in software engineering at Google: Progress and the path ahead (181 pts)]]></title>
            <link>https://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead/</link>
            <guid>40601116</guid>
            <pubDate>Thu, 06 Jun 2024 18:59:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead/">https://research.google/blog/ai-in-software-engineering-at-google-progress-and-the-path-ahead/</a>, See on <a href="https://news.ycombinator.com/item?id=40601116">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-gt-publish-date="20240606">
                    
                    


<div>
        <p data-block-key="ygj61">In 2019, a software engineer — at Google or indeed anywhere else — would have heard of advances in machine learning, and how deep learning has become remarkably effective in fields such as computer vision or language translation. However, most of them would not have imagined, let alone experienced, the ways in which machine learning might benefit what <i>they</i> do.</p><p data-block-key="arj8a">Just five years later, in 2024, there is widespread enthusiasm among software engineers about how AI is helping write code. And a significant number of those have used ML-based autocomplete, whether it is using company internal tools at large companies, e.g., <a href="https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html">Google’s internal code completion</a>, or via commercially available products.</p><p data-block-key="5afae">In this blog, we present our newest AI-powered improvements within the context of the continuing transformation of Google’s internal software development tools, and discuss further changes that we expect to see in the coming 5 years. We also present our methodology on how to build AI products that deliver value for professional software development. Our team is responsible for the software development environments where Google engineers spend the majority of their time, including <i>inner loop</i> (e.g., IDE, <a href="https://abseil.io/resources/swe-book/html/ch19.html" target="_blank" rel="noopener noreferrer">code review</a>, <a href="https://developers.google.com/code-search" target="_blank" rel="noopener noreferrer">code search</a>), as well as <i>outer loop</i> surfaces (e.g., <a href="https://developers.google.com/issue-tracker" target="_blank" rel="noopener noreferrer">bug management</a>, planning). We illustrate that improvements to these surfaces can directly impact developer productivity and satisfaction, both metrics that we <a href="https://ieeexplore.ieee.org/document/9159122" target="_blank" rel="noopener noreferrer">monitor carefully</a>.</p>
    </div>

                    
                    


<div>
        <h2 data-block-key="ygj61">The challenge</h2><p data-block-key="3lfr4">An ongoing challenge in this domain is that AI technology is evolving quickly and it is hard to predict which ideas to explore first. There is often a significant gap between technically feasible demos and successful productization. We approach deployment of ideas to products with three guidelines:</p><ol><li data-block-key="kapf"><i>Prioritize by technical feasibility and impact</i>: Work on ideas wherein both technical feasibility has already been established and high (measureable) impact on engineers’ workflows is expected.</li><li data-block-key="5v1pu"><i>Learn quickly, to improve UX and model quality</i>: Focus on iterating quickly and extracting lessons learned, while safeguarding developer productivity and happiness. User experience is just as important as model quality.</li><li data-block-key="6ha0d"><i>Measure effectiveness</i>: As our goal is to increase <a href="https://ieeexplore.ieee.org/document/10372494" target="_blank" rel="noopener noreferrer">productivity</a> and satisfaction metrics, we need to extensively monitor these metrics.</li></ol>
    </div>

                    
                    


<div>
        <h2 data-block-key="ygj61">Applying LLMs to software development</h2><p data-block-key="1pl70">With the advent of <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">transformer</a> architectures, we started exploring how to apply LLMs to software development. LLM-based inline code completion is the most popular application of AI applied to software development: it is a natural application of LLM technology to use the code itself as training data. The UX feels natural to developers since word-level autocomplete has been a core feature of IDEs for many years. Also, it’s possible to use a rough measure of impact, e.g., the percentage of new characters written by AI. For these reasons and more, it made sense for this application of LLMs to be the first to deploy.</p><p data-block-key="9qvi8">Our <a href="https://blog.research.google/2022/07/ml-enhanced-code-completion-improves.html">earlier blog</a> describes the ways in which we improve user experience with code completion and how we measure impact. Since then, we have seen continued fast growth similar to <a href="https://arxiv.org/abs/2402.04141" target="_blank" rel="noopener noreferrer">other enterprise contexts</a>, with an acceptance rate by software engineers of 37%<footnote id="836bf139-6026-4010-9080-cf46311a03a0">[836bf1]</footnote> assisting in the completion of 50% of code characters<footnote id="5f21d2a5-56e2-4ad8-b41c-d21f3452375e">[5f21d2]</footnote>. In other words, the same amount of characters in the code are now completed with AI-based assistance as are manually typed by developers. While developers still need to spend time reviewing suggestions, they have more time to focus on code design.</p><p data-block-key="b4iqc">Key improvements came from both the models —&nbsp;larger models with improved coding capabilities, heuristics for constructing the context provided to the model, as well as tuning models on usage logs containing acceptances, rejections and corrections — and the UX. This cycle is essential for learning from practical behavior, rather than synthetic formulations.</p>
    </div>

                    
                    
    

<!-- mode: &#x27;&#x27; -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-Hero1.width-800.png" alt="AIforSE-Hero1">
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-Hero1.width-800.png" alt="AIforSE-Hero1" loading="lazy">
      
        <p data-block-key="qeeti"><i>Improving AI-based features in coding tools (e.g., in the IDE) with historical high quality data across tools and with usage data capturing user preferences and needs.</i></p>
      
    </picture>
  



                    
                    
    

<!-- mode: &#x27;&#x27; -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-LineGraph2.width-800.png" alt="AIforSE-LineGraph2">
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-LineGraph2.width-800.png" alt="AIforSE-LineGraph2" loading="lazy">
      
        <p data-block-key="qeeti"><i>Continued increase of the fraction of code created with AI assistance via code completion, defined as the number of accepted characters from AI-based suggestions divided by the sum of manually typed characters and accepted characters from AI-based suggestions. Notably, characters from copy-pastes are not included in the denominator.</i></p>
      
    </picture>
  



                    
                    


<div>
        <p data-block-key="ygj61">We use our extensive and high quality logs of internal software engineering activities across multiple tools, which we have curated over many years. This data, for example, enables us to represent fine-grained code edits, build outcomes, edits to resolve <a href="https://en.wikipedia.org/wiki/Software_build" target="_blank" rel="noopener noreferrer">build</a> issues, code copy-paste actions, fixes of pasted code, code reviews, edits to fix reviewer issues, and change submissions to a repository. The training data is an aligned corpus of code with task-specific annotations in input as well as in output. The design of the data collection process, the shape of the training data, and the model that is trained on this data was described in our <a href="https://blog.research.google/2023/05/large-sequence-models-for-software.html">DIDACT</a> blog. We continue to explore these powerful datasets with newer generations of foundation models available to us (discussed more below).</p><p data-block-key="7v102">Our next significant deployments were <a href="https://research.google/blog/resolving-code-review-comments-with-ml/">resolving code review comments</a> (&gt;8% of which are now addressed with AI-based assistance) and automatically adapting pasted code to the surrounding context (now responsible for ~2% of code in the IDE<footnote id="5f21d2a5-56e2-4ad8-b41c-d21f3452375e">[5f21d2]</footnote>). Further deployments include instructing the IDE to perform code edits with natural language and predicting <a href="https://research.google/blog/safely-repairing-broken-builds-with-ml/">fixes to build failures</a>. Other applications, e.g., <a href="https://arxiv.org/abs/2405.13565" target="_blank" rel="noopener noreferrer">predicting tips for code readability</a> following a similar pattern are also possible.</p><p data-block-key="a6led">Together, these deployed applications have been successful, highly-used applications at Google, with measurable impact on productivity in a real, industrial context.</p>
    </div>

                    
                    
    

<!-- mode: &#x27;&#x27; -->


  
    <div>
      <video playsinline="" muted="true" loop="true" preload="auto">
        <source src="https://storage.googleapis.com/gweb-research2023-media/media/AIforSE-ExampleIDE3.mp4" type="video/mp4">
      </video>
      <div aria-label="Video Play/pause">
        <p><span>play silent looping video</span>
          <span>pause silent looping video</span>
        </p>
        
      </div>
      
      <p data-block-key="a3e6t"><i>A demonstration of how a variety of AI-based features can work together to assist with coding in the IDE.</i></p>
      
    </div>
  



                    
                    


<div>
        <h2 data-block-key="kyiao">What we’ve learned</h2><p data-block-key="d7ocb">Our work so far has taught us several things:</p><ol><li data-block-key="fijbk">We achieved the highest <i>impact with UX that naturally blends into users’ workflows</i>. In all the above examples, a suggestion is presented to the user, taking them to the next step in their workflow with one tab or click. Experiments requiring the user to remember to trigger the feature have failed to scale.</li><li data-block-key="92tm">We observe that with AI-based suggestions, the <i>code author increasingly becomes a reviewer</i>, and it is <a href="https://research.google/pubs/if-its-what-i-wanted-thats-great-but-if-its-not-i-just-wasted-time-unpacking-the-perceived-costsbenefits-of-ml-enhanced-developer-tooling/">important to find a balance</a> between <a href="https://arxiv.org/abs/2205.06537" target="_blank" rel="noopener noreferrer">the cost of review and added value</a>. We typically address the tradeoff with acceptance rate targets.</li><li data-block-key="b83m8">Quick iterations with <i>online A/B experiments are key</i>, as offline metrics are often only rough proxies of user value. By surfacing our AI-based features on internal tooling, we benefit greatly from being able to easily launch and iterate, measure usage data, and ask users directly about their experience through UX research.</li><li data-block-key="g6lv"><i>High quality data</i> from activities of Google engineers across software tools, including interactions with our features, is essential for our model quality.</li></ol><p data-block-key="hc39">We observe across features that it’s <i>important to optimize for conversion</i> from the opportunity (mostly a user activity, shown at the top of the funnel below) to impact (applied AI assistance, at the bottom of the funnel), while removing bottlenecks from intermediate steps of the funnel by leveraging UX and model improvements.</p>
    </div>

                    
                    
    

<!-- mode: &#x27;&#x27; -->


  
    
    
    <picture>
      
      
      <source media="(min-width: 768px)" srcset="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-FunnelFinal4.width-800.png" alt="AIforSE-FunnelFinal4">
      
      <img src="https://storage.googleapis.com/gweb-research2023-media/images/AIforSE-FunnelFinal4.width-800.png" alt="AIforSE-FunnelFinal4" loading="lazy">
      
        <p data-block-key="ubz65"><i>An opportunity funnel starting from SWE actions down to actual application of ML-based suggestions. Opportunities are lost if the model prediction is not confident enough, the model doesn’t respond or responds too late, the prediction is subpar, the user doesn’t notice the prediction, and so on. We use UX and model improvements to harvest as many opportunities as we can.</i></p>
      
    </picture>
  



                    
                    


<div>
        <h2 data-block-key="kyiao">What’s next</h2><p data-block-key="5cg9f">Encouraged by our successes so far, we are doubling down on bringing the latest foundation models (<a href="https://blog.google/technology/ai/google-gemini-ai/" target="_blank" rel="noopener noreferrer">Gemini series</a>) infused with the developer data (as part of <a href="https://blog.research.google/2023/05/large-sequence-models-for-software.html">DIDACT</a>, mentioned above) to power existing and new applications of ML to software engineering in Google.</p><p data-block-key="44ic3">Across the industry, ML-based code completion has provided a major boost for software developers. While there are still opportunities to improve code generation, we expect the next wave of benefits to come from ML assistance in a broader range of software engineering activities, such as testing, code understanding and code maintenance; the latter being of particular interest in enterprise settings. These opportunities inform our own ongoing work. We also highlight two trends that we see in the industry:</p><ol><li data-block-key="6emfq">Human-computer interaction has moved towards natural language as a common modality, and we are seeing a shift towards using language as the interface to software engineering tasks as well as the gateway to informational needs for software developers, all integrated in IDEs.</li><li data-block-key="2pi30">ML-based automation of larger-scale tasks — from diagnosis of an issue to landing a fix — has begun to show initial evidence of feasibility. These possibilities are driven by innovations in <i>agents</i> and <i>tool use</i>, which permit the building of systems that use one or more LLMs as a component to accomplish a larger task.</li></ol><p data-block-key="dfn6u">To expand on the above successes toward these next generation capabilities, the community of practitioners and researchers working in this topic would benefit from common benchmarks to help move the field towards practical engineering tasks. So far, benchmarks have been focused mostly around code generation (e.g., <a href="https://github.com/openai/human-eval" target="_blank" rel="noopener noreferrer">HumanEval</a>). In an enterprise setting, however, benchmarks for a wider range of tasks could be particularly valuable, e.g., code migrations and production debugging. Some benchmarks, such as one for bug resolution (e.g., <a href="https://www.swebench.com/" target="_blank" rel="noopener noreferrer">SWEBench</a>), and prototypes targeting those benchmarks (e.g., from <a href="https://www.cognition-labs.com/blog" target="_blank" rel="noopener noreferrer">Cognition AI</a>) have been published. We encourage the community to come together to suggest more benchmarks to span a wider range of software engineering tasks.</p>
    </div>

                    
                    


<div>
        <h2 data-block-key="kyiao">Acknowledgements</h2><p data-block-key="98em"><i>This project is the result of work of many people from the Google Core Systems &amp; Experiences team and Google Deepmind, and</i> <a href="https://ai.googleblog.com/2023/05/large-sequence-models-for-software.html" target="_blank" rel="noopener noreferrer"><i>DIDACT</i></a><i>. This article was co-authored with Boris Bokowski (Google Coding Tools Director), Petros Maniatis (research), Ambar Murillo (UXR), and Alberto Elizondo (UXD). Deep gratitude goes to contributors to the various features: Adam Husting, Ahmed Omran, Alexander Frömmgen, Ambar Murillo, Ayoub Kachkach, Brett Durrett, Chris Gorgolewski, Charles Sutton, Christian Schneider, Danny Tarlow, Damien Martin-Guillerez, David Tattersall, Elena Khrapko, Evgeny Gryaznov, Fredde Ribeiro, Gabriela Surita, Guilherme Herzog, Henrik Muehe, Ilia Krets, Iris Chu, Juanjo Carin, Kevin Villela, Kristóf Molnár, Lera Kharatyan, Madhura Dudhgaonkar, Marcus Revaj, Nimesh Ghelani, Niranjan Tulpule, Pavel Sychev, Siddhant Sanyam, Stanislav Pyatykh, Stoyan Nikolov, Ugam Kumar, Tobias Welp, Vahid Meimand, Vincent Nguyen, Yurun Shen, and Zoubin Ghahramani. Thanks to Tom Small for creating graphics for this post.</i></p>
    </div>

                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dragonfly: A large vision-language model with multi-resolution zoom (128 pts)]]></title>
            <link>https://www.together.ai/blog/dragonfly-v1</link>
            <guid>40600775</guid>
            <pubDate>Thu, 06 Jun 2024 18:31:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.together.ai/blog/dragonfly-v1">https://www.together.ai/blog/dragonfly-v1</a>, See on <a href="https://news.ycombinator.com/item?id=40600775">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-richtext-element="rich-text"><p>We are excited to announce the launch of Dragonfly, a breakthrough instruction-tuning Vision-language architecture, that enhances fine-grained visual understanding and reasoning about image regions. </p><p>We are releasing the Dragonfly architecture, which uses multi-resolution zoom-and-select to enhance multi-modal reasoning while being context-efficient. We are also launching two new open-source models &nbsp;<a href="https://huggingface.co/togethercomputer/Llama-3-8B-Dragonfly-v1">Llama-3-8b-Dragonfly-v1</a> a general-domain model trained on 5.5 million image-instruction pairs and <a href="https://huggingface.co/togethercomputer/Llama-3-8B-Dragonfly-Med-v1">Llama-3-8b-Dragonfly-Med-v1</a> finetuned on additional 1.4 biomedical image-instruction data. Dragonfly demonstrates promising performance on vision-language benchmarks like commonsense visual QA and image captioning. Dragonfly-Med outperforms prior models, including Med-Gemini on multiple medical imaging tasks, showcasing its capabilities for high-resolution medical data.</p><p>Figure 1: Generated example from Llama-3-8B-Dragonfly</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661eeab92ebdcff8cda76f2_image%20(5).png" loading="lazy" alt=""></p><figcaption><strong>Question</strong>: Summarize the visual contents of the image.<br>‍<strong>Dragonfly Response</strong>: In the heart of a vibrant skatepark, a skateboarder is caught in a moment of pure exhilaration. The skateboarder, dressed in a black t-shirt adorned with a yellow graphic and black pants, is suspended in mid-air, performing an impressive trick on a concrete ramp. The skateboarder's arms are outstretched, adding balance to the daring stunt. The skatepark itself is a concrete playground, with the skateboarder's ramp being the main focus. In the background, palm trees sway gently, adding a touch of nature to the urban setting. A few spectators can be seen in the distance, their attention riveted on the airborne skateboarder. The image captures not just a moment, but a story of skill, courage, and the joy of skateboarding.</figcaption></figure><p>Figure 2: Generated examples from &nbsp;Llama-3-8B-Dragonfly-Med</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661eed892ebdcff8cda8fc9_image%20(9).png" loading="lazy" alt=""></p><figcaption><strong>Question</strong>: Write a detailed radiology note based on the chest X-ray.<br>‍<strong>Gold Answer</strong>: AP upright and lateral views of the chest were provided. Left chest wall pacer pack is again seen with leads extending into the right heart. Abandoned pacing leads are also noted in the right chest wall extending into the right heart. The heart remains moderately enlarged. Lung volumes are low, with equivocal ground-glass opacity on the frontal view, which appears less conspicuous on the lateral view most likely attributable to underpenetrated technique. No gross evidence for pneumonia or pulmonary edema. No large effusions are seen. There is no pneumothorax. Bony structures are intact.<br>‍<strong>Dragonfly Response</strong>: PA and lateral views of the chest provided. Left chest wall pacer device is again noted with leads extending into the region the right atrium and right ventricle. The heart remains mildly enlarged. There is mild hilar congestion without frank edema. No convincing signs of pneumonia. No large effusion or pneumothorax. Cardiomediastinal silhouette is stable. Bony structures are intact </figcaption></figure><h2>Dragonfly Model Architecture</h2><p><a href="https://arxiv.org/abs/2406.00977"><em>Arxiv Paper</em></a><em> with technical details and </em><a href="https://github.com/togethercomputer/Dragonfly"><em>codebase</em></a><em> for architecture implementation</em></p><p>Dragonfly is special because:</p><ol role="list"><li>Dragonfly employs two key strategies: multi-resolution visual encoding and zoom-in patch selection, which enables the model to focus more fine-grained details on image regions and provide better commonsense reasoning. Despite the model being optimized for capturing fine-grained image details, the model achieves good zero-shot performance at standard image-understanding benchmarks such as visual question-answering or natural-image-captioning.</li><li>We demonstrate our model's understanding and reasoning capabilities on biomedical tasks, which often require a fine-grained understanding of high-resolution image regions. By finetuning our general-domain model on a biomedical instruction-tuning dataset with 1.4 million biomedical image-text pairs, Dragonfly-med, our biomedical version of the model, achieves state-of-the-art or competitive performance on multiple biomedical benchmarks, including visual question answering, image captioning, and radiology report generation.</li></ol><p>Figure 3: Overview of the Dragonfly architecture</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66614db787f49c335e5c4118_image3.png" loading="lazy" alt=""></p></figure><p><strong>Multi-resolution Visual Encoding:</strong> Dragonfly processes images at multiple resolutions—low, medium, and high. Each image, depending on its resolution, is divided into sub-images that are encoded into visual tokens. These tokens are then projected into a language space, forming a concatenated sequence that feeds into the LLM. This method allows the model to handle larger images efficiently and improves the granularity of visual data processing.</p><p><strong>Zoom-in Patch Selection:</strong> To further refine the model's focus on important visual details, Dragonfly employs a selective approach for high-resolution images. It uses a novel zoom-in patch selection strategy to identify and retain only those high-resolution sub-images that provide the most significant visual information. This is achieved by comparing summary embeddings of medium and high-resolution sub-images to select the most semantically relevant patches. This targeted selection reduces redundancy and focuses on crucial content areas, thereby enhancing the overall model efficiency and fine-grained understanding on these areas.</p><p>Together, these strategies enable Dragonfly to achieve a detailed and efficient visual understanding, making it particularly adept at processing complex image data in specific domains.</p><p>Figures 4 and 5: Generated examples from Llama-3-8B-Dragonfly</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661f9c35d300e04e8ef323f_image%20(6).png" loading="lazy" alt=""></p><figcaption><strong>Figure 4 Question:</strong> Summarize the visual content of the image first and then answer the questions. What process does this diagram portray and which solar body is portrayed in this diagram?<br><strong>Dragonfly Response:</strong> The diagram portrays the process of photosynthesis in plants, which is a fundamental biological process where plants convert light energy from the sun into chemical energy in the form of glucose. The solar body portrayed in this diagram is the sun, which is shown at the top left corner, providing the light energy that is essential for photosynthesis.<p><strong>Figure 5 Question:</strong> Summarize the image and why it is funny?<br><strong>Dragonfly Response:</strong> The image is a humorous take on the Mona Lisa painting, featuring a dog in the place of the human subject. The dog's expression, combined with the iconic pose and attire, creates a playful and amusing effect. The painting is executed in the same style as the original Mona Lisa, with similar brushwork and color palette, which adds to the humor by juxtaposing the serious art historical context with the light-hearted subject matter.</p></figcaption></figure><h2>Dragonfly Model Evaluation</h2><p>We evaluate Dragonfly trained based on LLaMA-8B on five popular vision-language benchmarks that require strong commonsense reasoning and detailed image understanding, AI2D, ScienceQA, MMMU, MMVet, and POPE. AI2D and ScienceQA evaluate visual commonsense reasoning in the science domain. MMMU and MMVet focus on providing a comprehensive evaluation of the vision-language capabilities. POPE evaluates the object-level hallucination of the regional details. Dragonfly achieved competitive performance compared with other popular vision-language models, demonstrating the effectiveness of Dragonfly on commonsense reasoning and fine-grained understanding of image regions. The results are presented in the following table:</p><div>
<table>
  <tbody><tr>
    <th>Model</th>
    <th>AI2D</th>
    <th>ScienceQA</th>
    <th>MMMU</th>
    <th>MMVet</th>
    <th>POPE</th>
  </tr>
  <tr>
    <td>VILA</td>
    <td>-</td>
    <td>68.2</td>
    <td></td>
    <td>34.9</td>
    <td>85.5</td>
  </tr>
  <tr>
    <td>LLaVA-v1.5 (Vicuna-7B)</td>
    <td>54.8</td>
    <td>70.4</td>
    <td>35.3</td>
    <td>30.5</td>
    <td>85.9</td>
  </tr>
  <tr>
    <td>LLaVA-v1.6 (Mistral-7B)</td>
    <td>60.8</td>
    <td>72.8</td>
    <td>33.4</td>
    <td>44.8</td>
    <td>86.7</td>
  </tr>
  <tr>
    <td>QWEN-VL-chat</td>
    <td>52.3</td>
    <td>68.2</td>
    <td>35.9</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Dragonfly (LLaMA-8B)</td>
    <td>63.6</td>
    <td>80.5</td>
    <td>37.8</td>
    <td>35.9</td>
    <td>91.2</td>
  </tr>
</tbody></table>
</div><h2>Dragonfly-Med</h2><p>We partnered with the Zou group at Stanford Medicine to finetune Dragonfly on additional 1.4 million biomedical image instructions to develop the biomedical version, Dragonfly-Med. </p><p>Figures 6 and 7: Generated examples from &nbsp;Llama-3-8B-Dragonfly-Med</p><figure><p><img src="https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6661f97f1204614802b7b80f_image%20(7).png" loading="lazy" alt=""></p><figcaption><strong>Figure 4 Question: </strong>Please provide brief description of the image.<br><strong>Gold Answer: </strong>Abdominal enhanced CT shows a ring-like enhanced and central heterogeneous tumor in the pelvis.<br><strong>Dragonfly Response:</strong> Computed tomography scan of the abdomen and pelvis showing a large lobulated mass in the pelvic cavity.<br><strong><br>Figure 5 Question:</strong> Explain the visual content of the image.<br><strong>Gold Answer:</strong> GROSS: HEMATOLOGIC: Spleen: Chronic Myelogenous Leukemia: Gross natural color whole spleen massively enlarged.<br><strong>Dragonfly Response:</strong> HEMATOLOGIC: Spleen: Chronic Myelogenous Leukemia: Gross natural color external view of massively enlarged spleen with multiple infarcts seen from capsule.</figcaption></figure><p>Dragonfly-Med is evaluated on both visual question-answering, medical image captioning, and clinical report generation evaluation. Our model outperforms Med-Gemini, which was finetuned a customized version of Gemini 1.5 with about 5x more image-instruction data, on all the three question-answering datasets. The following two tables show the results on these benchmarks. The first table shows the performance on question-answering benchmarks and the second table presents the performance of biomedical image caption and report generation. </p><div>
<table>
  <tbody><tr>
    <th>Dataset</th>
    <th>Metric</th>
    <th>LLaVA-Med</th>
    <th>Med-Gemini</th>
    <th>SOTA</th>
    <th>Dragonfly-Med (LLaMA-8B)</th>
  </tr>
  <tr>
    <td rowspan="2">VQA-RAD</td>
    <td>Acc (closed)</td>
    <td>84.2</td>
    <td>69.7</td>
    <td>87.1</td>
    <td>77.4</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>50.1</td>
    <td>62.1</td>
    <td>59.6</td>
  </tr>
  <tr>
    <td rowspan="2">SLAKE</td>
    <td>Acc (closed)</td>
    <td>83.2</td>
    <td>84.8</td>
    <td>91.6</td>
    <td>90.4</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>75.8</td>
    <td>89.3</td>
    <td>88.8</td>
  </tr>
  <tr>
    <td rowspan="2">Path-VQA</td>
    <td>Acc (closed)</td>
    <td>91.7</td>
    <td>83.3</td>
    <td>91.7</td>
    <td>92.3</td>
  </tr>
  <tr>
    <td>Token F1</td>
    <td>-</td>
    <td>58.7</td>
    <td>62.7</td>
    <td>67.6</td>
  </tr>
</tbody></table>
</div><p>‍</p><div><table>
  <tbody><tr>
    <th>Dataset</th>
    <th>Metric</th>
    <th colspan="2">BiomedGPT SOTA</th>
    <th>Dragonfly-Med (LLaMA-8B)</th>
  </tr>
  <tr>
    <td rowspan="3">IU X-Ray</td>
    <td rowspan="3">ROUGE-L<p>METEOR</p><p>CIDEr</p></td>
    <td rowspan="3">28.5<p>12.9</p><p>40.1</p></td>
    <td rowspan="3">44.8<p>24.2</p><p>43.5</p></td>
    <td rowspan="3">28.5<p>29.7</p><p>58.8</p></td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">Peir Gross</td>
    <td rowspan="3">ROUGE-L<p>METEOR</p><p>CIDEr</p></td>
    <td rowspan="3">36.0<p>15.4</p><p>122.7</p></td>
    <td rowspan="3">36.0<p>15.4</p><p>122.7</p></td>
    <td rowspan="3">40.3<p>38.3</p><p>179.9</p></td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">ROCO</td>
    <td rowspan="3">ROUGE-L<p>METEOR</p><p>CIDEr</p></td>
    <td rowspan="3">18.2<p>7.8</p><p>24.2</p></td>
    <td rowspan="3">18.2<p>7.8</p><p>24.2</p></td>
    <td rowspan="3">19.3<p>15.1</p><p>40.1</p></td>
  </tr>
  <tr></tr>
  <tr></tr>
  <tr>
    <td rowspan="3">MIMIC CXR</td>
    <td rowspan="3">ROUGE-L<p>METEOR</p><p>CIDEr</p></td>
    <td rowspan="3">23.8<p>14.2</p><p>14.7</p></td>
    <td rowspan="3">33.5<p>19.0</p><p>50.9</p></td>
    <td rowspan="3">24.2<p>22.6</p><p>47.2</p></td>
  </tr>
  <tr></tr>
  <tr></tr>
</tbody></table></div><p>‍</p><p>Dragonfly-Med outperforms the existing state-of-the-art models across multiple benchmarks on the Path-VQA dataset for both accuracy (closed) and token F1 metrics. It attains an accuracy of 90.4% on the SLAKE dataset, which is close to the current state-of-the-art of 91.6%. Notably, Dragonfly-Med outperforms Med-Gemini, a model finetuned from Gemini-1.5 on 7 million biomedical data samples, on all VQA tasks we evaluated. On the image captioning task, Dragonfly-Med achieves state-of-the-art or competitive results on several metrics across these datasets. Notably, on the Peir Gross and ROCO datasets, Dragonfly-Med outperforms existing methods on all three metrics: ROUGE-L, METEOR, and CIDEr. Some of the baseline models are much larger than our current implementation. The zoom-and-select architecture of Dragonfly is especially powerful for medical image understanding as medical images are often very large and the salient regions are found in small patches.</p><h2>Conclusion and Future Work</h2><p>Dragonfly architecture provides a potential research direction on zooming in image regions to focus more selected fine-grained visual information. We trained two checkpoints based on LLaMA3-8B-Instruct and achieved promising results on both general-domain and biomedical-domain tasks. We hope this work could benefit the research community to explore more open-sourced multimodal research and apply AI on real-world problems.</p><p>We will keep improving the comprehensive capabilities of open-source multimodal models. In the future, we will explore new architectures, better visual encoding strategies, more comprehensive studies on how the data mixture should be and more scientific domains to provide benefits for broader fields.</p><h2>Acknowledgements</h2><p>We especially appreciate the contributions and collaborations of our partner, Stanford Medicine. We would also like to acknowledge the following resources that were instrumental in the development of Dragonfly:</p><ul role="list"><li>Meta LLaMA3: We utilized the LLaMA3 as our language model backbone for our current two checkpoints.</li><li>CLIP: Our current visual backbone is CLIP model from OpenAI</li><li>Our codebase is built upon codebases of Otter and LLaVA-UHD.</li></ul></div><div><div><p><h2>Q: Should I use the RedPajama-V2 Dataset out of the box?</h2></p></div><div><p>RedPajama-V2 is conceptualized as a pool of data that serves as a foundation for creating high quality datasets. The dataset is thus not intended to be used out of the box and, depending on the application, data should be filtered out using the quality signals that accompany the data. With this dataset, we take the view that the optimal filtering of data is dependent on the intended use. Our goal is to provide all the signals and tooling that enables this.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Interview with Lola De La Mata about tinnitus (112 pts)]]></title>
            <link>https://thequietus.com/interviews/lola-de-la-mata-oceans-on-azimuth-tinnitus-interview/</link>
            <guid>40600748</guid>
            <pubDate>Thu, 06 Jun 2024 18:28:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thequietus.com/interviews/lola-de-la-mata-oceans-on-azimuth-tinnitus-interview/">https://thequietus.com/interviews/lola-de-la-mata-oceans-on-azimuth-tinnitus-interview/</a>, See on <a href="https://news.ycombinator.com/item?id=40600748">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Multi-disciplinary artist Lola De La Mata speaks to Patrick Clarke about how her experience with severe tinnitus and vertigo, groundbreaking work in the field of audiology, and stigma around hearing loss in the musical community fed into her new album Oceans On Azimuth</p><div>




<p>“At least I had some good sake,” jokes Lola De La Mata. “The rest is quite unfortunate.” The English-born, French-Spanish multi-disciplinary artist is recalling a dinner in 2019 that would change her life and her artistic practice forever. As she recalls, a staff member plugged in the restaurant’s electric piano without checking that the master fader was at 0, which resulted in a deafening noise. De La Mata had already been experiencing low-level tinnitus for years prior – she blames the heavy metal shows that her parents took her to growing up – but the feedback in the restaurant was so severe that it would leave her dealing with catastrophic consequences. “It was a rupture,” she explains. “Afterwards my hearing in my left ear kept going through notches, I’d completely lose my hearing, or low floating tones would cover [other people’s] speech. Sometimes I’d wake up because I was hearing all this thunder, a cracking sound.”</p>



<p>The thunder, she was later informed by audiologists, was a good thing – the sound of her ear trying to repair itself. This was of scant solace, given that as well as extreme tinnitus, De La Mata developed vertigo. “I do have a chronic health condition, which made it difficult to pinpoint if it was that that was suddenly getting worse, or whether it was [the damage to the ear] that was causing neurological changes, but I literally couldn’t walk straight; I was having what looked like strokes where I would collapse.” A violinist, she was told by doctors to give up playing. When the COVID pandemic arrived a few months in, she was forced to shield because of ultimately false suspicions that she had MS. “I got really frustrated,” De La Mata says. “I wasn’t getting any of the answers I wanted. It was, ‘Your hearing is fine, you’re young, you’re healthy,’ and it’s like, well clearly I’m not if I can’t walk and people are feeding me.”&nbsp;</p>




<p>Over a year later, when she had recovered to the point that she could hear again through her left ear, though not as well as before, she was able to start processing her situation. First came a composition called ‘KOH-klee-uh’ via SA Recordings in 2022, the leading release in a four-part series called <em>The Hearing Experience</em> in which artists were invited to explore their relationship with the act of listening. An uneasy and often jarring mixture high-pitched rings, deep scrapes and dull thuds, she employed tuning forks used during the Rhine &amp; Weber hearing test, as well as a Canna Sonora, a rare instrument consisting of aluminium poles arranged vertically across a rack. “There’s a node in the centre, and when you rub [a pole], it activates a vibration. When you approach it with your hand, the vibration actually swarms into your palm, then you shoot it back. It’s absolutely wild.” In the process it delivers a ringing noise, equal parts grating and transfixing, that accurately mimics the sound of tinnitus.</p>



<iframe width="560" height="315" src="https://www.youtube.com/embed/h7umtdzHM6Q?si=J2os4pDmr8xKFLTs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>



<p>“At the beginning it was not a good project, it was just, ‘I hear a high-pitched sound, here’s a high-pitched sound!’” she laughs. The plan at that point was to create a number of pieces relating to different parts of the ear, but over time her scope broadened to become her new album <em>Oceans On Azimuth</em>, a visceral and intense thing that bristles with life in the same way her eardrum thundered as it tried to force itself back into shape. At the record’s launch at the Stephen Lawrence Gallery in Greenwich, a week before she speaks to tQ, De La Mata displays accompanying sculptures based on a 3D rendering of her ear canal while Jono Heale from the hearing protection company ACS takes similar moulds from attendees to form part of a future piece. Part of the venue is devoted to instruments arranged in preparation for De La Mata to perform following a panel discussion with Heale and the abstract turntablist Maria Chávez. There’s no Canna Sonora – only two of them currently exist in the country – but a number of the other instruments have been specially crafted for the project.</p>



<p>She begins her live performance by walking slowly through the crowd bearing one of them, a metal gong in the shape of an ear. It spins on its string as she strikes it, making the clanging sounds spin around our ears. As the resonances fade each time, the sounds of our surroundings – church bells, sirens, the crash of a dropped glass – feel amplified as they fill the emerging gaps. That is until De La Mata takes to the theremin, playing it first at a shrieking high, then low like a chainsaw. Suddenly it stops, and as a new electronic bass tone slowly builds the musician moves over to a violin, partly wrapped in tinfoil, which she plays scratchily and discordantly. She thumps the instrument with the bow, then scrapes its strings with the bow upside down. She scrapes the bow again against the rim of some hi-hats and knocks it against the inside of ear-shaped metal loops. Turning to her laptop she unleashes a monstrous barrage of beeps and gurgles and screeches from which a muffled rhythm gradually emerges. Occasionally recorded voices can be heard atop the fray. One is an academic-sounding figure who explains the concept of impedance matching – the way we have a ‘middle ear’ whose role is to take the vibrations that enter the ear and translate them so they can be interpreted through the liquid of the cochlea, through which sound travels differently. Elsewhere, a woman’s voice riffs on the phrase ‘pink noise’ – referring to an ambient sound that can be used in the treatment of tinnitus – which De La Mata has warped into a fragmented whirl.</p>




<figure><img decoding="async" width="683" height="1024" src="https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-683x1024.jpg" alt="Lola De La Mata holding an ear-shaped gong while stood in a cave" srcset="https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-683x1024.jpg 683w, https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-200x300.jpg 200w, https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-768x1152.jpg 768w, https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour-1024x1536.jpg 1024w, https://thequietus.com/app/uploads/2024/05/full-length_portrait_ear-gong_cave_colour.jpg 1280w" sizes="(max-width: 683px) 100vw, 683px" loading="lazy" fetchpriority="low"><figcaption>Photo by Rosie Terry Toogood</figcaption></figure>



<p>The voices belong to two employees of The Hudspeth Laboratory Of Sensory Neuroscience in New York, biophysicist Francesco Gianoli and resident musicologist Lana Norris respectively. De La Mata had come across the institution when, a few months after ‘KOH-klee-uh’ and her first foray into making music around her tinnitus, she searched online for academics in the field of audiology. She was intrigued that the laboratory’s director A.J. Hudspeth “kept employing words from artistic fields, like how the way hair bundles are distributed on the cochlea is almost like a piano in reverse. There was something that was accessible, not just academic.” You can hear that same impulse in Gianoli’s explanation of impedance matching, too, included on the album and in her live performance, where he compares the role of the middle ear to the bridge of a violin. “He wrote a master’s paper about it, and how there’s a possibility of creating the perfect bridge for each specific violin if you put in the right equations with the thickness of wood, the type, the width, etcetera.”</p>



<p>Throughout the project, De La Mata continues, she felt a duty to focus on this sense of the organic and the tangible, and not to rest too heavily on the digital. She became fascinated by the work of Maryanne Amacher, a composer and artist who worked with phenomena called auditory distortion products, or otoacoustic emissions, in which the ears themselves produce audible sound. “There’s a capacity of ‘playing the ear’,” as De La Mata puts it. When hearing the Canna Sonora, for instance, if you turn your head, it can feel like you’re catching extra tones that are being played, “like there’s a little moth fluttering on your ear,” whereas what’s really happening is that “your ear is pushing its own tone back out, it’s like a combination of the tones.”</p>



<p>She also started meeting with female double bass players (two of whom, Gwen Reed and Marianne Schofield, appear on <em>Oceans On Azimuth</em>). “There’s something about this oversized instrument that means if you’re a woman, who tend to be smaller than men, it means that your hands are used differently, your reach is different and the way your hips fit into the instrument, and that actually changes the vibration of the instrument.” There were parallels to be found, she explains, with the way in which different people’s ears will interpret sound differently due to their individual physiology. “We did a lot of experimentation, weaving tinfoil through [a double bass] to give it more distortion, slapping it and hitting it. I was just desperate to get into the ear. There’s a bit of, I don’t know if it’s the right word, but magic to how the ear functions, so it was nice to include instruments that felt surreal.”</p>




<figure><img decoding="async" width="1024" height="769" src="https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-1024x769.jpg" alt="A black and white photo of Lola De La Mata with an experimental instrument" srcset="https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-1024x769.jpg 1024w, https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-300x225.jpg 300w, https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-768x577.jpg 768w, https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy-1536x1154.jpg 1536w, https://thequietus.com/app/uploads/2024/05/headshot-4_ear-canal-ceramic_bw-copy.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px" loading="lazy" fetchpriority="low"></figure>



<p>Hudspeth was fascinated, and so within a couple of months of De La Mata’s reaching out, he invited her to join him in New York, which she did so thanks to an Arts Council grant. “I had already been meeting with instrument makers, so I already had some prototypes, so by the time I approached them I had something to come along with and discuss,” she continues. “They were desperate to show me all the things they’d built, these little studios, these mini-anechoic chambers, towers and towers of equipment.” One room contained zebrafish, a species who hear using hairs on their backs that are almost identical to the hairs on a human cochlea, and who are born translucent so that scientists can observe their nerves’ growth first hand. Elsewhere, the scientists showed her the 3D printed chambers they were developing to recreate the environment of a cochlea, “which was interesting because I’d been 3D printing all of my weird stuff. There was a lot of overlap.”</p>



<p>De La Mata had things to offer in return, pointing out how they might make use of soundproof rooms to deal with the noise produced by the fans of their machinery. Thanks to her experiments with an electromagnetic field recorder, which highlighted how noisy the lab could get, they were able to ascertain where previous experiments had run into issues. In turn, it also altered her view of science. “I’ve learned that it’s all about failure, and that they welcome it. People would open discussions by saying, ‘We have no idea how this thing works, we’re not there and we might never be there.’ And just as there’s a lot of failure in science, there’s a lot of failure in our communication, which actually allows spaces for questions, space to dream and innovate. So there was this common shared language, but we just had slightly different definitions of what it all meant, which was exciting. The last time I went to the lab, I met the head of the Paris branch of research at the Curie Institute that looks at hair cells and hair cell growth, and the way he’s studying it is by using etching techniques. In New York, they’re using photographic techniques as a way of measuring the movement of a hair cell. There’s no sure way or absolute truth to anything that they do, it’s all about experimentation and improvisation, which is not so different to what I’m doing.”&nbsp;</p>



<p>The most mind-blowing moment, not only for De La Mata but the scientists too, came when they managed to actually record the sounds that she heard in her ears – which now appear as ‘Left Ear’ and ‘Right Ear’ which begin sides A and B on the album – and in doing so opened up questions about the nature of tinnitus itself. “The NHS definition is that it’s a phantom sound that your brain is creating, that it isn’t something ‘real’, so you should try to ignore it.” By having De La Mata place her ear into an anechoic chamber, with an ultra-sensitive microphone perched in her ear canal, they were able to provide significant evidence to the contrary. “After the first recording of it, it was ‘There’s no way, this isn’t possible.’” They tried again with her breath held, and again with her tensing her ears, and again with other members of staff, but each time it became apparent that yes, the noises De La Mata hears are seemingly something physical. More intriguingly still, the two women whose ears were recorded, De La Mata and Lana Norris – the musicologist whose voice appears on the album’s ‘PINK Noise’, and who is also a choral director – were the only two people whose ears were found to produce spontaneous otoacoustic emissions. “It’s something to do with hormone difference, but they don’t really know why,” De La Mata says. Present in most children but believed to fade over time, they’re also found far more in musicians than in other adults, for reasons yet unknown. It all raises a lot of questions. “What I have is tinnitus by the definition we have now, but maybe that’s not correct. Maybe it’s something else,” De La Mata wonders aloud.&nbsp;</p>







<p>Given the scale of the unanswered questions, De La Mata has started a creative practice PhD to take things even further. For now, however, we have <em>Oceans On Azimuth</em>, a record that not only distils the complexity and nuance of its subject into an immediate and engaging listen, but also represents a major breach in the stigma that still surrounds hearing loss in the musical community. “If you’re told you have tinnitus, you’re often then told, ‘It’s the end of your career, you can’t be a musician anymore’, or ‘You’re not at optimal capacity’. There’s always a fear when you feel like you’re diverging from the normal with a body part or with health, or that you’re hearing something ‘wrong’. It’s something I experienced going to tinnitus support groups. Some people there were talking about tinnitus for the first time with someone other than themselves. There were people who had lost their job, who couldn’t see friends because they couldn’t deal with having any other noise. There were people who were suicidal.”&nbsp;</p>



<p><em>Oceans On Azimuth</em>, however, offers an alternative way of viewing things: an embrace of difference and subjectivity and a rejection of a binary idea of ‘good’ and ‘bad’ ears. “It was interesting to have that other way of thinking about it. The more I speak to biophysicists, the more I’m learning that although you create something very crisp and perfect when you’re thinking about the purity of sound, your ear will still always add distortion to it.”&nbsp; Her tinnitus is retriggered all the time by the reality of everyday life, but now, she says, “I’ve learnt to accept it.”</p>



<p><em><a href="https://loladelamata.bandcamp.com/album/oceans-on-azimuth">Lola De La Mata’s new album Oceans On Azimuth is out now</a></em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let rand = main as usize (2022) (128 pts)]]></title>
            <link>https://codeandbitters.com/main-as-usize/</link>
            <guid>40600384</guid>
            <pubDate>Thu, 06 Jun 2024 17:55:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codeandbitters.com/main-as-usize/">https://codeandbitters.com/main-as-usize/</a>, See on <a href="https://news.ycombinator.com/item?id=40600384">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                <p>The other day I made a joke on twitter, and learned some interesting things about raw pointers in Rust.</p>
<p>The abridged joke goes something like this:</p>
<blockquote>
<p>Yosh: What do you mean Rust doesn't ship with <code>rand</code> built-in?</p>
</blockquote>
<blockquote>
<p>Me: ASLR to the rescue!</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> rand = main as </span><span>usize</span><span>;
</span><span>    dbg!(rand);
</span><span>}
</span></code></pre>
</blockquote>
<p><img src="https://codeandbitters.com/images/rust_logo_hand_drawn_1b.png" alt="Rust Logo" width="320" height="256"></p>
<h4 id="part-1-explaining-the-joke">Part 1: Explaining the joke<a href="#part-1-explaining-the-joke"> §</a>
</h4>
<p>Explaining the joke is bad form, but there is some valuable technical detail here.</p>
<p>The most important thing is ASLR (Address Space Layout Randomization). When software has memory safety bugs like buffer overflows, it's easy for an attacker to blast hostile data into a process's stack. That hostile data could replace the address that the currently running function will return to, giving the attacker the ability to execute some arbitrary instructions.</p>
<p>This would be bad. One of the mitigations that engineers came up with is to have each program execute at a randomized virtual address, to make such attacks harder. It's debatable whether this is effective at turning away attacks, but that's the goal, and ASLR is enabled on almost every operating system in use today.</p>
<p>Let me annotate my joke program a little bit:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>// By calling this "rand", I'm pretending that this line of code is
</span><span>    </span><span>// a random number generator.
</span><span>    </span><span>let</span><span> rand = main as </span><span>usize</span><span>;
</span><span>    </span><span>//          ^        ^
</span><span>    </span><span>//          |        \-- that address as a pointer-sized integer
</span><span>    </span><span>//          |
</span><span>    </span><span>//          \-- the address of the main function
</span><span>
</span><span>    dbg!(rand); </span><span>// print out the result
</span><span>}
</span></code></pre>
<p>This program assumes that ASLR is used by the host operating system, which caused the program to run at a random address. Our program observes the address that <code>main</code> is located at, and uses that as our random value.</p>
<p>It's reasonable to wonder whether the address of <code>main</code> might be a static value, or that the Rust compiler might use some static address rather than recomputing at runtime due to ASLR. This isn't the case, though: experimentally we can verify that the value does change on each execution.</p>
<p>This is a dirty hack, and I don't recommend doing this in real programs. ASLR isn't a good random number generator. The address doesn't change that much, and under some conditions may not change at all. Even in the best circumstances, a program can only acquire one random value this way, so two different modules both using this trick would use the same value. Real random number generators are fast and readily available (at least, on any platform capable of using ASLR). Please use a well-regarded RNG istead of a hack like this.</p>
<h4 id="part-2-i-learn-things-about-rust-pointers">Part 2: I learn things about Rust pointers<a href="#part-2-i-learn-things-about-rust-pointers"> §</a>
</h4>
<p>The code I above isn't exactly what I posted on twitter. The original post looked like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> rand = main as </span><span>*const fn</span><span>() as </span><span>usize</span><span>;
</span><span>    dbg!(rand);
</span><span>}
</span></code></pre>
<p>This wasn't well-written code. The two-step cast is just habit, because of situations like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>print_address</span><span>(</span><span>int_ref</span><span>: &amp;</span><span>u32</span><span>) {
</span><span>    </span><span>let</span><span> px = int_ref as </span><span>*const u32 </span><span>as </span><span>usize</span><span>;
</span><span>    dbg!(px);
</span><span>}
</span></code></pre>
<p>In many cases Rust won't allow us to cast a reference address directly to an integer; we have to go by way of a raw pointer.</p>
<p>Function pointers don't work the same way, though.</p>
<p>I didn't realize that there is no such thing as a "raw function pointer" in Rust. <code>fn()</code> is itself a pointer type, so <code>*const fn()</code> is a raw pointer to a function pointer, which doesn't make sense in this context.</p>
<p>Since there is no syntax for "raw function pointer", the compiler will let you substitute any other raw pointer type. Several tutorials use <code>foo as *const ()</code> (pointer to unit) to temporarily hold an untyped function pointer.</p>
<p>For those expecting to the usual Rust guard rails, it's surprising that the compiler allows casting between arbitrary raw pointer types outside of an <code>unsafe</code> block. This feels really dangerous— even though we can't do anything with the pointer outside of an <code>unsafe</code> block, creating a raw pointer usually implies that an unsafe block will eventually do something with it. I kind of wish that this pointer casting required <code>unsafe</code>, just because this code should send up red flags, and probably deserves a close look during code review.</p>
<p>I feel a little bad that while making a joke by doing something that's a little evil, I accidentally inserted a really evil cast that is highly misleading to the reader.</p>
<p>If I wanted to go all-in on the evil cast I could do something like this:</p>
<pre data-lang="rust"><code data-lang="rust"><span>fn </span><span>main</span><span>() {
</span><span>    </span><span>let</span><span> rand = main as </span><span>*const </span><span>rand::rngs::OsRng as </span><span>usize</span><span>;
</span><span>    dbg!(rand);
</span><span>}
</span></code></pre>
<p>A few people pointed out that there are a few instances of programs that actually do try to harvest some randomness from the program's address. There's probably a place for a dirty hack like that, but I would feel a bit icky if I ever published code like that myself.</p>
<p>Thanks to @yoshuawuyts for setting up the joke, and to @eddyb for pointing out my pointer mistake, and everyone else who commented!</p>
<p>Comments? Get in touch on <a href="https://twitter.com/codeandbitters">twitter: @codeandbitters</a></p>

                            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Extracting concepts from GPT-4 (322 pts)]]></title>
            <link>https://openai.com/index/extracting-concepts-from-gpt-4/</link>
            <guid>40599749</guid>
            <pubDate>Thu, 06 Jun 2024 17:01:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/extracting-concepts-from-gpt-4/">https://openai.com/index/extracting-concepts-from-gpt-4/</a>, See on <a href="https://news.ycombinator.com/item?id=40599749">Hacker News</a></p>
Couldn't get https://openai.com/index/extracting-concepts-from-gpt-4/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen2 LLM Released (232 pts)]]></title>
            <link>https://qwenlm.github.io/blog/qwen2/</link>
            <guid>40599018</guid>
            <pubDate>Thu, 06 Jun 2024 16:01:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwenlm.github.io/blog/qwen2/">https://qwenlm.github.io/blog/qwen2/</a>, See on <a href="https://news.ycombinator.com/item?id=40599018">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen.jpg#center" width="100%"></figure><p><a href="https://github.com/QwenLM/Qwen2" target="_blank">GITHUB</a>
<a href="https://huggingface.co/Qwen" target="_blank">HUGGING FACE</a>
<a href="https://modelscope.cn/organization/qwen" target="_blank">MODELSCOPE</a>
<a href="https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct" target="_blank">DEMO</a>
<a href="https://discord.gg/yPEP2vHTu4" target="_blank">DISCORD</a></p><h2 id="introduction">Introduction</h2><p>After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:</p><ul><li>Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and <strong>Qwen2-72B</strong>;</li><li>Having been trained on data in <strong>27</strong> additional languages besides English and Chinese;</li><li>State-of-the-art performance in a large number of benchmark evaluations;</li><li>Significantly improved performance in coding and mathematics;</li><li>Extended context length support up to <strong>128K</strong> tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.</li></ul><p>We have opensourced the models in Hugging Face and ModelScope to you and we are looking forward to hearing from you!</p><h2 id="model-information">Model Information</h2><p>The Qwen2 series include base and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, Qwen2-72B. We illustrate the key information of the models in the following table:</p><table><thead><tr><th>Models</th><th>Qwen2-0.5B</th><th>Qwen2-1.5B</th><th>Qwen2-7B</th><th>Qwen2-57B-A14B</th><th>Qwen2-72B</th></tr></thead><tbody><tr><td># Params</td><td>0.49B</td><td>1.54B</td><td>7.07B</td><td>57.41B</td><td>72.71B</td></tr><tr><td># Non-Emb Params</td><td>0.35B</td><td>1.31B</td><td>5.98B</td><td>56.32B</td><td>70.21B</td></tr><tr><td>GQA</td><td>True</td><td>True</td><td>True</td><td>True</td><td>True</td></tr><tr><td>Tie Embedding</td><td>True</td><td>True</td><td>False</td><td>False</td><td>False</td></tr><tr><td>Context Length</td><td>32K</td><td>32K</td><td>128K</td><td>64K</td><td>128K</td></tr></tbody></table><p>Specifically, previously in Qwen1.5, only Qwen1.5-32B and Qwen1.5-110B have adopted Group Query Attention (GQA). This time, for all model sizes, we apply GQA so that they can enjoy the benefits of faster speed and less memory usage in model inference. For small models, we prefer the application of tying embedding as the large sparse embeddings take up a large proportion of the total model parameters.</p><p>In terms of the context length, all base language models have been pretrained on data of the context length of 32K tokens, and we observe satisfactory extrapolation capabilities up to 128K in PPL evaluation.
However, for instruction-tuned models, we are not satisfied with merely PPL evaluation; we need the models to be capable of correctly understanding long context and completing tasks.
In the table, we list the context length capabilities of instruction-tuned models, as assessed through the evaluation of the <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle in a Haystack</a> task. Notably, when augmented with YARN, both Qwen2-7B-Instruct and Qwen2-72B-Instruct models demonstrate an impressive capacity to handle context lengths extending up to 128K tokens.</p><p>Significant efforts were directed towards augmenting both the volume and quality of pretraining and instruction-tuning datasets across a diverse linguistic spectrum, beyond English and Chinese, to bolster its multilingual competencies. Although large language models possess an inherent capacity to generalize to other languages, we explicitly highlight the inclusion of 27 additional languages in our training:</p><table><thead><tr><th>Regions</th><th>Languages</th></tr></thead><tbody><tr><td>Western Europe</td><td>German, French, Spanish, Portuguese, Italian, Dutch</td></tr><tr><td>Eastern &amp; Central Europe</td><td>Russian, Czech, Polish</td></tr><tr><td>Middle East</td><td>Arabic, Persian, Hebrew, Turkish</td></tr><tr><td>Eastern Asia</td><td>Japanese, Korean</td></tr><tr><td>South-Eastern Asia</td><td>Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog</td></tr><tr><td>Southern Asia</td><td>Hindi, Bengali, Urdu</td></tr></tbody></table><p>Additionally, we have devoted significant effort to addressing code-switching, a frequent occurrence in multilingual evaluation. Consequently, our models’ proficiency in handling this phenomenon have notably enhanced. Evaluations using prompts that typically induce code-switching across languages confirm a substantial reduction in associated issues.</p><h2 id="performance">Performance</h2><p>Comparative assessments reveal substantial enhancements in performance for large-scale models (70B+ parameters) relative to Qwen1.5. Here our evaluation centers on the large-size model Qwen2-72B.
In terms of base language models, Qwen2-72B and state-of-the-art open models are evaluated for different capbilities including natural language understanding, knowledge acquisition, coding proficiency, mathematical skills, and multilingual abilities.
Benefiting from meticulously curated datasets and optimized training methods, Qwen2-72B exhibits superior performance compared to leading models such as Llama-3-70B. Notably, it surpasses the performance of its predecessor, Qwen1.5-110B, despite having fewer parameters.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-72b.jpg#center" width="100%"></figure><p>After extensive large-scale pre-training, we conduct post-training to further enhance Qwen’s intelligence, bringing it closer to human. This process further improves the model’s capabilities in areas such as coding, mathematics, reasoning, instruction following, multilingual understanding, and more. Additionally, it aligns the model’s output with human values, ensuring that it is helpful, honest, and harmless. Our post-training phase is designed with the principle of scalable training with minimal human annotation. Specifically, we investigate how to obtain high-quality, reliable, diverse and creative demonstration data and preference data with various automated alignment strategies, such as <a href="https://arxiv.org/pdf/2308.01825">rejection sampling</a> for math, execution feedback for coding and instruction-following, back-translation for creative writing, <a href="https://arxiv.org/pdf/2401.12474">scalable oversight</a> for role-play, etc. As for training, we apply a combination of supervised fine-tuning, reward model training and online DPO training. We also employ a novel <a href="https://arxiv.org/pdf/2405.17931">Online Merging Optimizer</a> to minimize the alignment tax. These collective efforts have significantly boosted the capabilities and intelligence of our models, as illustrated in the following table.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-72b-instruct.jpg#center" width="100%"></figure><p>We comprehensively evaluate Qwen2-72B-Instruct on 16 benchmarks across various domains. Qwen2-72B-Instruct strikes a balance between obtaining better capabilities and aligning well with human values. Specifically, Qwen2-72B-Instruct significantly surpasses Qwen1.5-72B-Chat across all benchmarks, and also reaches competitive performance compared with Llama-3-70B-Instruct.</p><p>In terms of smaller models, our Qwen2 models also outcompete the SOTA models of similar or even larger sizes. In comparison with the very recently released SOTA models, Qwen2-7B-Instruct can still demonstrate advantages across benchmarks, showing specifically outstanding performance on coding and Chinese-related metrics.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-7b.jpg#center" width="100%"></figure><h2 id="highlights">Highlights</h2><h2 id="coding--mathematics">Coding &amp; Mathematics</h2><p>We have persistently dedicated our efforts to enhance the advanced capabilities of Qwen, particularly in coding and mathematics. In coding, we have successfully integrated the code training experience and data from <a href="https://qwenlm.github.io/blog/codeqwen1.5/">CodeQwen1.5</a>, resulting in significant improvements in Qwen2-72B-Instruct across various programming languages. Regarding mathematics, by exploiting the extensive and high-quality datasets, Qwen2-72B-Instruct has reflects stronger capabilities in solving mathematic problems.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2-code-math.jpg#center" width="100%"></figure><h2 id="long-context-understanding">Long Context Understanding</h2><p>In Qwen2, all instruction-tuned models have been trained on 32k length contexts, and extrapolated to longer context lengths using techniques like <a href="https://arxiv.org/abs/2309.00071">YARN</a> or <a href="https://arxiv.org/abs/2402.17463">Dual Chunk Attention</a>.</p><p>The figure below shows our test results on the <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle in a Haystack</a>. Notably, Qwen2-72B-Instruct is capable of flawlessly handling information extraction tasks within a 128k context. Coupled with its inherent strong performance, it becomes the preferred choice for handling long text tasks when resources are sufficient.</p><p>Additionally, it’s worth noting the impressive capabilities of other models in the series: Qwen2-7B-Instruct nearly flawlessly handles contexts up to 128k in length, Qwen2-57B-A14B-Instruct manages contexts up to 64k, and the two smaller models in the lineup support contexts of 32k.</p><p>Alongside the long-context models, we have also open-sourced an agent solution for efficiently processing documents containing up to 1 million tokens. For more details, see <a href="https://qwenlm.github.io/blog/qwen-agent-2405/">our dedicated blog post on this topic</a>.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/qwen2_needle_in_haystack.png#center" width="100%"></figure><h2 id="safety-and-responsibility">Safety and Responsibility</h2><p>The table below presents the proportion of harmful responses generated by large models for four categories of multilingual unsafe querys(Illegal Activity, Fraud, Pornography, Privacy Violence). The test data was derived from <a href="https://github.com/verazuo/jailbreak_llms/tree/main">Jailbreak</a> and translated into multiple languages for evaluation. We find that Llama-3 does not effectively handle multilingual prompts, and therefore, it is not included in the comparison. Through significance testing (P_value), we found that the Qwen2-72B-Instruct model performs comparably to GPT-4 in terms of safety, and significantly outperforms the Mistral-8x22B model.</p><table><thead><tr><th>Language</th><th></th><th>Illegal Activity</th><th></th><th></th><th>Fraud</th><th></th><th></th><th>Pornography</th><th></th><th></th><th>Privacy Violence</th><th></th></tr></thead><tbody><tr><td></td><td>GPT-4</td><td>Mistral-8x22B</td><td>Qwen2-72B-Instruct</td><td>GPT-4</td><td>Mistral-8x22B</td><td>Qwen2-72B-Instruct</td><td>GPT-4</td><td>Mistral-8x22B</td><td>Qwen2-72B-Instruct</td><td>GPT-4</td><td>Mistral-8x22B</td><td>Qwen2-72B-Instruct</td></tr><tr><td>zh</td><td><strong>0%</strong></td><td>13%</td><td><strong>0%</strong></td><td><strong>0%</strong></td><td>17%</td><td><strong>0%</strong></td><td><strong>43%</strong></td><td>47%</td><td>53%</td><td><strong>0%</strong></td><td>10%</td><td><strong>0%</strong></td></tr><tr><td>en</td><td><strong>0%</strong></td><td>7%</td><td><strong>0%</strong></td><td><strong>0%</strong></td><td>23%</td><td><strong>0%</strong></td><td><strong>37%</strong></td><td>67%</td><td>63%</td><td><strong>0%</strong></td><td>27%</td><td>3%</td></tr><tr><td>ar</td><td><strong>0%</strong></td><td>13%</td><td><strong>0%</strong></td><td><strong>0%</strong></td><td>7%</td><td><strong>0%</strong></td><td><strong>15%</strong></td><td>26%</td><td><strong>15%</strong></td><td>3%</td><td>13%</td><td><strong>0%</strong></td></tr><tr><td>es</td><td><strong>0%</strong></td><td>7%</td><td><strong>0%</strong></td><td>3%</td><td><strong>0%</strong></td><td><strong>0%</strong></td><td><strong>48%</strong></td><td>64%</td><td>50%</td><td><strong>3%</strong></td><td>7%</td><td><strong>3%</strong></td></tr><tr><td>fr</td><td><strong>0%</strong></td><td>3%</td><td><strong>0%</strong></td><td><strong>3%</strong></td><td><strong>3%</strong></td><td>7%</td><td><strong>3%</strong></td><td>19%</td><td>7%</td><td><strong>0%</strong></td><td>27%</td><td><strong>0%</strong></td></tr><tr><td>ko</td><td><strong>0%</strong></td><td>4%</td><td><strong>0%</strong></td><td><strong>3%</strong></td><td>8%</td><td>4%</td><td>17%</td><td>29%</td><td><strong>10%</strong></td><td><strong>0%</strong></td><td>26%</td><td>4%</td></tr><tr><td>pt</td><td><strong>0%</strong></td><td>7%</td><td><strong>0%</strong></td><td><strong>3%</strong></td><td>7%</td><td><strong>3%</strong></td><td><strong>47%</strong></td><td>57%</td><td><strong>47%</strong></td><td><strong>4%</strong></td><td>26%</td><td><strong>4%</strong></td></tr><tr><td>th</td><td><strong>0%</strong></td><td>10%</td><td><strong>0%</strong></td><td>7%</td><td>23%</td><td><strong>3%</strong></td><td>13%</td><td>17%</td><td><strong>10%</strong></td><td>13%</td><td><strong>7%</strong></td><td><strong>7%</strong></td></tr><tr><td>vi</td><td><strong>0%</strong></td><td>4%</td><td><strong>0%</strong></td><td>4%</td><td>11%</td><td><strong>0%</strong></td><td><strong>22%</strong></td><td>26%</td><td><strong>22%</strong></td><td><strong>0%</strong></td><td><strong>0%</strong></td><td><strong>0%</strong></td></tr><tr><td>Average</td><td><strong>0%</strong></td><td>8%</td><td><strong>0%</strong></td><td>3%</td><td>11%</td><td><strong>2%</strong></td><td><strong>27%</strong></td><td>39%</td><td>31%</td><td>3%</td><td>16%</td><td><strong>2%</strong></td></tr></tbody></table><h2 id="developing-with-qwen2">Developing with Qwen2</h2><p>Now all models have been released in Hugging Face and ModelScope. Feel free to visit the model cards for detailed usages, and learn more information about each model, including its features, performance, etc.</p><p>For a long time, a lot of friends have been supporting the development of Qwen, including finetuning (<a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a>, <a href="https://github.com/hiyouga/LLaMA-Factory">Llama-Factory</a>, <a href="https://github.com/yangjianxin1/Firefly">Firefly</a>, <a href="https://github.com/modelscope/swift">Swift</a>, <a href="https://github.com/InternLM/xtuner">XTuner</a>), quantization (<a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a>, <a href="https://github.com/casper-hansen/AutoAWQ">AutoAWQ</a>, <a href="https://github.com/intel/neural-compressor">Neural Compressor</a>), deployment (<a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/sgl-project/sglang">SGL</a>, <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a>, <a href="https://github.com/NVIDIA/TensorRT-LLM">TensorRT-LLM</a>, <a href="https://github.com/openvinotoolkit/openvino">OpenVino</a>, <a href="https://github.com/huggingface/text-generation-inference">TGI</a>), API platforms (<a href="https://www.together.ai/">Together</a>, <a href="https://fireworks.ai/">Fireworks</a>, <a href="https://openrouter.ai/">OpenRouter</a>), local run (<a href="https://github.com/ml-explore/mlx">MLX</a>, <a href="https://github.com/ggerganov/llama.cpp">Llama.cpp</a>, <a href="https://ollama.com/">Ollama</a>, <a href="https://lmstudio.ai/">LM Studio</a>), Agent and RAG Frameworks (<a href="https://www.llamaindex.ai/">LlamaIndex</a>, <a href="https://www.crewai.com/">CrewAI</a>, <a href="https://github.com/OpenDevin/OpenDevin/">OpenDevin</a>) , Evaluation (<a href="https://chat.lmsys.org/">LMSys</a>, <a href="https://opencompass.org.cn/home">OpenCompass</a>, <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM Leaderboard</a>), model training (<a href="https://huggingface.co/cognitivecomputations">Dolphin</a>, <a href="https://github.com/OpenBuddy/OpenBuddy">Openbuddy</a>) etc. For how to use Qwen2 with the third-party frameworks, please refer to the respective documentation as well as our <a href="https://qwen.readthedocs.io/en/latest/">official documentation</a>.</p><figure><img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen2/logo-v3.jpg#center" width="80%"></figure><p>Still there are a number of teams and people not mentioned that have made contributions to Qwen. We sincerely thank them for the support, and we hope that our collaboration can boost the research and development of the opensource AI community.</p><h2 id="license">License</h2><p>This time, we change the licenses of our models to different ones. While Qwen2-72B as well as its instruction-tuned models still uses the original Qianwen License, all other models, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, and Qwen2-57B-A14B, turn to adopt <strong>Apache 2.0</strong>! We believe that the enhanced openness of our models to the community can accelerate the applications and commercial usages of Qwen2 all around the world.</p><h2 id="whats-next-for-qwen2">What’s Next for Qwen2?</h2><p>We are training larger Qwen2 models to further explore model scaling along with our recent data scaling. Additionally, we extend the Qwen2 language models to multimodal, capable of understanding both vision and audio information. In the near future, we will continue opensource new models to accelerate opensource AI. Stay tuned!</p><h2 id="citation">Citation</h2><p>We are going to release the technical report for Qwen2 very soon. Feel free to give us a cite!</p><pre tabindex="0"><code>@article{qwen2,
  title={Qwen2 Technical Report},
  year={2024}
}
</code></pre><h2 id="appendix">Appendix</h2><h2 id="base-language-model-evaluation">Base Language Model Evaluation</h2><p>The evaluation of base models mainly focuses on the model performance of natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, multilingual capability, etc.</p><p>The datasets for evaluation include:</p><p><strong>English Tasks</strong>: MMLU (5-shot), MMLU-Pro (5-shot), GPQA (5shot), Theorem QA (5-shot), BBH (3-shot), HellaSwag (10-shot), Winogrande (5-shot), TruthfulQA (0-shot), ARC-C (25-shot)</p><p><strong>Coding Tasks</strong>: EvalPlus (0-shot) (HumanEval, MBPP, HumanEval+, MBPP+), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)</p><p><strong>Math Tasks</strong>: GSM8K (4-shot), MATH (4-shot)</p><p><strong>Chinese Tasks</strong>: C-Eval(5-shot), CMMLU (5-shot)</p><p><strong>Multilingual Tasks</strong>: Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)</p><h3 id="qwen2-72b-performance">Qwen2-72B performance</h3><table><thead><tr><th>Datasets</th><th>DeepSeek-V2</th><th>Mixtral-8x22B</th><th>Llama-3-70B</th><th>Qwen1.5-72B</th><th>Qwen1.5-110B</th><th><strong>Qwen2-72B</strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td>#Activated Params</td><td>21B</td><td>39B</td><td>70B</td><td>72B</td><td>110B</td><td>72B</td></tr><tr><td>#Params</td><td>236B</td><td>140B</td><td>70B</td><td>72B</td><td>110B</td><td>72B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>78.5</td><td>77.8</td><td>79.5</td><td>77.5</td><td>80.4</td><td><strong>84.2</strong></td></tr><tr><td>MMLU-Pro</td><td>-</td><td>49.5</td><td>52.8</td><td>45.8</td><td>49.4</td><td><strong>55.6</strong></td></tr><tr><td>GPQA</td><td>-</td><td>34.3</td><td>36.3</td><td>36.3</td><td>35.9</td><td><strong>37.9</strong></td></tr><tr><td>Theorem QA</td><td>-</td><td>35.9</td><td>32.3</td><td>29.3</td><td>34.9</td><td><strong>43.1</strong></td></tr><tr><td>BBH</td><td>78.9</td><td>78.9</td><td>81.0</td><td>65.5</td><td>74.8</td><td><strong>82.4</strong></td></tr><tr><td>HellaSwag</td><td>87.8</td><td><strong>88.7</strong></td><td>88.0</td><td>86.0</td><td>87.5</td><td>87.6</td></tr><tr><td>WindoGrande</td><td>84.8</td><td>85.0</td><td><strong>85.3</strong></td><td>83.0</td><td>83.5</td><td>85.1</td></tr><tr><td>ARC-C</td><td>70.0</td><td><strong>70.7</strong></td><td>68.8</td><td>65.9</td><td>69.6</td><td>68.9</td></tr><tr><td>TruthfulQA</td><td>42.2</td><td>51.0</td><td>45.6</td><td><strong>59.6</strong></td><td>49.6</td><td>54.8</td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>45.7</td><td>46.3</td><td>48.2</td><td>46.3</td><td>54.3</td><td><strong>64.6</strong></td></tr><tr><td>MBPP</td><td>73.9</td><td>71.7</td><td>70.4</td><td>66.9</td><td>70.9</td><td><strong>76.9</strong></td></tr><tr><td>EvalPlus</td><td>55.0</td><td>54.1</td><td>54.8</td><td>52.9</td><td>57.7</td><td><strong>65.4</strong></td></tr><tr><td>MultiPL-E</td><td>44.4</td><td>46.7</td><td>46.3</td><td>41.8</td><td>52.7</td><td><strong>59.6</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>79.2</td><td>83.7</td><td>83.0</td><td>79.5</td><td>85.4</td><td><strong>89.5</strong></td></tr><tr><td>MATH</td><td>43.6</td><td>41.7</td><td>42.5</td><td>34.1</td><td>49.6</td><td><strong>51.1</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>81.7</td><td>54.6</td><td>65.2</td><td>84.1</td><td>89.1</td><td><strong>91.0</strong></td></tr><tr><td>CMMLU</td><td>84.0</td><td>53.4</td><td>67.2</td><td>83.5</td><td>88.3</td><td><strong>90.1</strong></td></tr><tr><td><em><strong>Multilingual</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mulit-Exam</td><td>67.5</td><td>63.5</td><td>70.0</td><td>66.4</td><td>75.6</td><td><strong>76.6</strong></td></tr><tr><td>Multi-Understanding</td><td>77.0</td><td>77.7</td><td>79.9</td><td>78.2</td><td>78.2</td><td><strong>80.7</strong></td></tr><tr><td>Multi-Mathematics</td><td>58.8</td><td>62.9</td><td>67.1</td><td>61.7</td><td>64.4</td><td><strong>76.0</strong></td></tr><tr><td>Multi-Translation</td><td>36.0</td><td>23.3</td><td><strong>38.0</strong></td><td>35.6</td><td>36.2</td><td>37.8</td></tr></tbody></table><h3 id="qwen2-57b-a14b">Qwen2-57B-A14B</h3><table><thead><tr><th>Datasets</th><th>Jamba</th><th>Mixtral-8x7B</th><th>Yi-1.5-34B</th><th>Qwen1.5-32B</th><th><strong><strong>Qwen2-57B-A14B</strong></strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td>#Activated Params</td><td>12B</td><td>12B</td><td>34B</td><td>32B</td><td>14B</td></tr><tr><td>#Params</td><td>52B</td><td>47B</td><td>34B</td><td>32B</td><td>57B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>67.4</td><td>71.8</td><td><strong>77.1</strong></td><td>74.3</td><td>76.5</td></tr><tr><td>MMLU-Pro</td><td>-</td><td>41.0</td><td><strong>48.3</strong></td><td>44.0</td><td>43.0</td></tr><tr><td>GPQA</td><td>-</td><td>29.2</td><td>-</td><td>30.8</td><td><strong>34.3</strong></td></tr><tr><td>Theorem QA</td><td>-</td><td>23.2</td><td>-</td><td>28.8</td><td><strong>33.5</strong></td></tr><tr><td>BBH</td><td>45.4</td><td>50.3</td><td><strong>76.4</strong></td><td>66.8</td><td>67.0</td></tr><tr><td>HellaSwag</td><td><strong>87.1</strong></td><td>86.5</td><td>85.9</td><td>85.0</td><td>85.2</td></tr><tr><td>Winogrande</td><td>82.5</td><td>81.9</td><td><strong>84.9</strong></td><td>81.5</td><td>79.5</td></tr><tr><td>ARC-C</td><td>64.4</td><td><strong>66.0</strong></td><td>65.6</td><td>63.6</td><td>64.1</td></tr><tr><td>TruthfulQA</td><td>46.4</td><td>51.1</td><td>53.9</td><td>57.4</td><td><strong>57.7</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>29.3</td><td>37.2</td><td>46.3</td><td>43.3</td><td><strong>53.0</strong></td></tr><tr><td>MBPP</td><td>-</td><td>63.9</td><td>65.5</td><td>64.2</td><td><strong>71.9</strong></td></tr><tr><td>EvalPlus</td><td>-</td><td>46.4</td><td>51.9</td><td>50.4</td><td><strong>57.2</strong></td></tr><tr><td>MultiPL-E</td><td>-</td><td>39.0</td><td>39.5</td><td>38.5</td><td><strong>49.8</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>59.9</td><td>62.5</td><td><strong>82.7</strong></td><td>76.8</td><td>80.7</td></tr><tr><td>MATH</td><td>-</td><td>30.8</td><td>41.7</td><td>36.1</td><td><strong>43.0</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>-</td><td>-</td><td>-</td><td>83.5</td><td><strong>87.7</strong></td></tr><tr><td>CMMLU</td><td>-</td><td>-</td><td>84.8</td><td>82.3</td><td><strong>88.5</strong></td></tr><tr><td><em><strong>Multilingual</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>-</td><td>56.1</td><td>58.3</td><td>61.6</td><td><strong>65.5</strong></td></tr><tr><td>Multi-Understanding</td><td>-</td><td>70.7</td><td>73.9</td><td>76.5</td><td><strong>77.0</strong></td></tr><tr><td>Multi-Mathematics</td><td>-</td><td>45.0</td><td>49.3</td><td>56.1</td><td><strong>62.3</strong></td></tr><tr><td>Multi-Translation</td><td>-</td><td>29.8</td><td>30.0</td><td>33.5</td><td><strong>34.5</strong></td></tr></tbody></table><h3 id="qwen2-7b">Qwen2-7B</h3><table><thead><tr><th>Datasets</th><th>Mistral-7B</th><th>Gemma-7B</th><th>Llama-3-8B</th><th>Qwen1.5-7B</th><th>Qwen2-7B</th></tr></thead><tbody><tr><td># Params</td><td>7.2B</td><td>8.5B</td><td>8.0B</td><td>7.7B</td><td>7.6B</td></tr><tr><td># Non-emb Params</td><td>7.0B</td><td>7.8B</td><td>7.0B</td><td>6.5B</td><td>6.5B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>64.2</td><td>64.6</td><td>66.6</td><td>61.0</td><td><strong>70.3</strong></td></tr><tr><td>MMLU-Pro</td><td>30.9</td><td>33.7</td><td>35.4</td><td>29.9</td><td><strong>40.0</strong></td></tr><tr><td>GPQA</td><td>24.7</td><td>25.7</td><td>25.8</td><td>26.7</td><td><strong>31.8</strong></td></tr><tr><td>Theorem QA</td><td>19.2</td><td>21.5</td><td>22.1</td><td>14.2</td><td><strong>31.1</strong></td></tr><tr><td>BBH</td><td>56.1</td><td>55.1</td><td>57.7</td><td>40.2</td><td><strong>62.6</strong></td></tr><tr><td>HellaSwag</td><td><strong>83.2</strong></td><td>82.2</td><td>82.1</td><td>78.5</td><td>80.7</td></tr><tr><td>Winogrande</td><td>78.4</td><td><strong>79.0</strong></td><td>77.4</td><td>71.3</td><td>77.0</td></tr><tr><td>ARC-C</td><td>60.0</td><td><strong>61.1</strong></td><td>59.3</td><td>54.2</td><td>60.6</td></tr><tr><td>TruthfulQA</td><td>42.2</td><td>44.8</td><td>44.0</td><td>51.1</td><td><strong>54.2</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>29.3</td><td>37.2</td><td>33.5</td><td>36.0</td><td><strong>51.2</strong></td></tr><tr><td>MBPP</td><td>51.1</td><td>50.6</td><td>53.9</td><td>51.6</td><td><strong>65.9</strong></td></tr><tr><td>EvalPlus</td><td>36.4</td><td>39.6</td><td>40.3</td><td>40.0</td><td><strong>54.2</strong></td></tr><tr><td>MultiPL-E</td><td>29.4</td><td>29.7</td><td>22.6</td><td>28.1</td><td><strong>46.3</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>52.2</td><td>46.4</td><td>56.0</td><td>62.5</td><td><strong>79.9</strong></td></tr><tr><td>MATH</td><td>13.1</td><td>24.3</td><td>20.5</td><td>20.3</td><td><strong>44.2</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>47.4</td><td>43.6</td><td>49.5</td><td>74.1</td><td><strong>83.2</strong></td></tr><tr><td>CMMLU</td><td>-</td><td>-</td><td>50.8</td><td>73.1</td><td><strong>83.9</strong></td></tr><tr><td><em><strong>Multilingual</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>47.1</td><td>42.7</td><td>52.3</td><td>47.7</td><td><strong>59.2</strong></td></tr><tr><td>Multi-Understanding</td><td>63.3</td><td>58.3</td><td>68.6</td><td>67.6</td><td><strong>72.0</strong></td></tr><tr><td>Multi-Mathematics</td><td>26.3</td><td>39.1</td><td>36.3</td><td>37.3</td><td><strong>57.5</strong></td></tr><tr><td>Multi-Translation</td><td>23.3</td><td>31.2</td><td><strong>31.9</strong></td><td>28.4</td><td>31.5</td></tr></tbody></table><h3 id="qwen2-05b--qwen2-15b">Qwen2-0.5B &amp; Qwen2-1.5B</h3><table><thead><tr><th>Datasets</th><th>Phi-2</th><th>Gemma-2B</th><th>MiniCPM</th><th>Qwen1.5-1.8B</th><th>Qwen2-0.5B</th><th>Qwen2-1.5B</th></tr></thead><tbody><tr><td>#Non-Emb Params</td><td>2.5B</td><td>2.0B</td><td>2.4B</td><td>1.3B</td><td>0.35B</td><td>1.3B</td></tr><tr><td>MMLU</td><td>52.7</td><td>42.3</td><td>53.5</td><td>46.8</td><td>45.4</td><td><strong>56.5</strong></td></tr><tr><td>MMLU-Pro</td><td>-</td><td>15.9</td><td>-</td><td>-</td><td>14.7</td><td>21.8</td></tr><tr><td>Theorem QA</td><td>-</td><td>-</td><td>-</td><td>-</td><td>8.9</td><td><strong>15.0</strong></td></tr><tr><td>HumanEval</td><td>47.6</td><td>22.0</td><td><strong>50.0</strong></td><td>20.1</td><td>22.0</td><td>31.1</td></tr><tr><td>MBPP</td><td><strong>55.0</strong></td><td>29.2</td><td>47.3</td><td>18.0</td><td>22.0</td><td>37.4</td></tr><tr><td>GSM8K</td><td>57.2</td><td>17.7</td><td>53.8</td><td>38.4</td><td>36.5</td><td><strong>58.5</strong></td></tr><tr><td>MATH</td><td>3.5</td><td>11.8</td><td>10.2</td><td>10.1</td><td>10.7</td><td><strong>21.7</strong></td></tr><tr><td>BBH</td><td><strong>43.4</strong></td><td>35.2</td><td>36.9</td><td>24.2</td><td>28.4</td><td>37.2</td></tr><tr><td>HellaSwag</td><td><strong>73.1</strong></td><td>71.4</td><td>68.3</td><td>61.4</td><td>49.3</td><td>66.6</td></tr><tr><td>Winogrande</td><td><strong>74.4</strong></td><td>66.8</td><td>-</td><td>60.3</td><td>56.8</td><td>66.2</td></tr><tr><td>ARC-C</td><td><strong>61.1</strong></td><td>48.5</td><td>-</td><td>37.9</td><td>31.5</td><td>43.9</td></tr><tr><td>TruthfulQA</td><td>44.5</td><td>33.1</td><td>-</td><td>39.4</td><td>39.7</td><td><strong>45.9</strong></td></tr><tr><td>C-Eval</td><td>23.4</td><td>28.0</td><td>51.1</td><td>59.7</td><td>58.2</td><td><strong>70.6</strong></td></tr><tr><td>CMMLU</td><td>24.2</td><td>-</td><td>51.1</td><td>57.8</td><td>55.1</td><td><strong>70.3</strong></td></tr></tbody></table><h2 id="instruction-tuned-model-evaluation">Instruction-tuned Model Evaluation</h2><h3 id="qwen2-72b-instruct">Qwen2-72B-Instruct</h3><table><thead><tr><th>Datasets</th><th>Llama-3-70B-Instruct</th><th>Qwen1.5-72B-Chat</th><th><strong>Qwen2-72B-Instruct</strong></th></tr></thead><tbody><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>82.0</td><td>75.6</td><td><strong>82.3</strong></td></tr><tr><td>MMLU-Pro</td><td>56.2</td><td>51.7</td><td><strong>64.4</strong></td></tr><tr><td>GPQA</td><td>41.9</td><td>39.4</td><td><strong>42.4</strong></td></tr><tr><td>TheroemQA</td><td>42.5</td><td>28.8</td><td><strong>44.4</strong></td></tr><tr><td>MT-Bench</td><td>8.95</td><td>8.61</td><td><strong>9.12</strong></td></tr><tr><td>Arena-Hard</td><td>41.1</td><td>36.1</td><td><strong>48.1</strong></td></tr><tr><td>IFEval (Prompt Strict-Acc.)</td><td>77.3</td><td>55.8</td><td><strong>77.6</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>81.7</td><td>71.3</td><td><strong>86.0</strong></td></tr><tr><td>MBPP</td><td><strong>82.3</strong></td><td>71.9</td><td>80.2</td></tr><tr><td>MultiPL-E</td><td>63.4</td><td>48.1</td><td><strong>69.2</strong></td></tr><tr><td>EvalPlus</td><td>75.2</td><td>66.9</td><td><strong>79.0</strong></td></tr><tr><td>LiveCodeBench</td><td>29.3</td><td>17.9</td><td><strong>35.7</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td><strong>93.0</strong></td><td>82.7</td><td>91.1</td></tr><tr><td>MATH</td><td>50.4</td><td>42.5</td><td><strong>59.7</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>61.6</td><td>76.1</td><td><strong>83.8</strong></td></tr><tr><td>AlignBench</td><td>7.42</td><td>7.28</td><td><strong>8.27</strong></td></tr></tbody></table><h3 id="qwen2-57b-a14b-instruct">Qwen2-57B-A14B-Instruct</h3><table><thead><tr><th>Datasets</th><th>Mixtral-8x7B-Instruct-v0.1</th><th>Yi-1.5-34B-Chat</th><th>Qwen1.5-32B-Chat</th><th><strong>Qwen2-57B-A14B-Instruct</strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td>#Activated Params</td><td>12B</td><td>34B</td><td>32B</td><td>14B</td></tr><tr><td>#Params</td><td>47B</td><td>34B</td><td>32B</td><td>57B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>71.4</td><td><strong>76.8</strong></td><td>74.8</td><td>75.4</td></tr><tr><td>MMLU-Pro</td><td>43.3</td><td>52.3</td><td>46.4</td><td><strong>52.8</strong></td></tr><tr><td>GPQA</td><td>-</td><td>-</td><td>30.8</td><td><strong>34.3</strong></td></tr><tr><td>TheroemQA</td><td>-</td><td>-</td><td>30.9</td><td><strong>33.1</strong></td></tr><tr><td>MT-Bench</td><td>8.30</td><td>8.50</td><td>8.30</td><td><strong>8.55</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>45.1</td><td>75.2</td><td>68.3</td><td><strong>79.9</strong></td></tr><tr><td>MBPP</td><td>59.5</td><td><strong>74.6</strong></td><td>67.9</td><td>70.9</td></tr><tr><td>MultiPL-E</td><td>-</td><td>-</td><td>50.7</td><td><strong>66.4</strong></td></tr><tr><td>EvalPlus</td><td>48.5</td><td>-</td><td>63.6</td><td><strong>71.6</strong></td></tr><tr><td>LiveCodeBench</td><td>12.3</td><td>-</td><td>15.2</td><td><strong>25.5</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>65.7</td><td><strong>90.2</strong></td><td>83.6</td><td>79.6</td></tr><tr><td>MATH</td><td>30.7</td><td><strong>50.1</strong></td><td>42.4</td><td>49.1</td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>-</td><td>-</td><td>76.7</td><td>80.5</td></tr><tr><td>AlignBench</td><td>5.70</td><td>7.20</td><td>7.19</td><td><strong>7.36</strong></td></tr></tbody></table><h3 id="qwen2-7b-instruct">Qwen2-7B-Instruct</h3><table><thead><tr><th>Datasets</th><th>Llama-3-8B-Instruct</th><th>Yi-1.5-9B-Chat</th><th>GLM-4-9B-Chat</th><th>Qwen1.5-7B-Chat</th><th>Qwen2-7B-Instruct</th></tr></thead><tbody><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>68.4</td><td>69.5</td><td><strong>72.4</strong></td><td>59.5</td><td>70.5</td></tr><tr><td>MMLU-Pro</td><td>41.0</td><td>-</td><td>-</td><td>29.1</td><td><strong>44.1</strong></td></tr><tr><td>GPQA</td><td><strong>34.2</strong></td><td>-</td><td><strong>-</strong></td><td>27.8</td><td>25.3</td></tr><tr><td>TheroemQA</td><td>23.0</td><td>-</td><td>-</td><td>14.1</td><td><strong>25.3</strong></td></tr><tr><td>MT-Bench</td><td>8.05</td><td>8.20</td><td>8.35</td><td>7.60</td><td><strong>8.41</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Humaneval</td><td>62.2</td><td>66.5</td><td>71.8</td><td>46.3</td><td><strong>79.9</strong></td></tr><tr><td>MBPP</td><td><strong>67.9</strong></td><td>-</td><td>-</td><td>48.9</td><td>67.2</td></tr><tr><td>MultiPL-E</td><td>48.5</td><td>-</td><td>-</td><td>27.2</td><td><strong>59.1</strong></td></tr><tr><td>Evalplus</td><td>60.9</td><td>-</td><td>-</td><td>44.8</td><td><strong>70.3</strong></td></tr><tr><td>LiveCodeBench</td><td>17.3</td><td>-</td><td>-</td><td>6.0</td><td><strong>26.6</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>79.6</td><td><strong>84.8</strong></td><td>79.6</td><td>60.3</td><td>82.3</td></tr><tr><td>MATH</td><td>30.0</td><td>47.7</td><td><strong>50.6</strong></td><td>23.2</td><td>49.6</td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>45.9</td><td>-</td><td>75.6</td><td>67.3</td><td><strong>77.2</strong></td></tr><tr><td>AlignBench</td><td>6.20</td><td>6.90</td><td>7.01</td><td>6.20</td><td><strong>7.21</strong></td></tr></tbody></table><h3 id="qwen2-05b-instruct--qwen2-15b-instruct">Qwen2-0.5B-Instruct &amp; Qwen2-1.5B-Instruct</h3><table><thead><tr><th>Datasets</th><th>Qwen1.5-0.5B-Chat</th><th><strong>Qwen2-0.5B-Instruct</strong></th><th>Qwen1.5-1.8B-Chat</th><th><strong>Qwen2-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU</td><td>35.0</td><td><strong>37.9</strong></td><td>43.7</td><td><strong>52.4</strong></td></tr><tr><td>HumanEval</td><td>9.1</td><td><strong>17.1</strong></td><td>25.0</td><td><strong>37.8</strong></td></tr><tr><td>GSM8K</td><td>11.3</td><td><strong>40.1</strong></td><td>35.3</td><td><strong>61.6</strong></td></tr><tr><td>C-Eval</td><td>37.2</td><td><strong>45.2</strong></td><td>55.3</td><td><strong>63.8</strong></td></tr><tr><td>IFEval (Prompt Strict-Acc.)</td><td>14.6</td><td><strong>20.0</strong></td><td>16.8</td><td><strong>29.0</strong></td></tr></tbody></table><h2 id="multilingual-capability-of-instruction-tuned-models">Multilingual capability of instruction-tuned models</h2><p>We compare Qwen2 instruction-tuned models with other recent LLMs on several cross-lingual open benchmarks as well as by human evaluation. For benchmarks, we show the results on 2 evaluation datasets:</p><ul><li><a href="https://github.com/nlp-uoregon/mlmm-evaluation">M-MMLU</a> from Okapi: multilingual commonsense evaluation (we evaluate with a subset on ar, de, es, fr, it, nl, ru, uk, vi, zh)</li><li><a href="https://arxiv.org/abs/2210.03057">MGSM</a>: math evaluation on languages including de, en, es, fr, ja, ru, th, zh and bn</li></ul><p>The results are averaged over languages for each benchmark and shown as follows:</p><table><thead><tr><th>Models</th><th>M-MMLU (5-shot)</th><th>MGSM (0-shot, CoT)</th></tr></thead><tbody><tr><td><strong><em>Proprietary LLMs</em></strong></td><td></td><td></td></tr><tr><td>GPT-4-0613</td><td>78.0</td><td>87.0</td></tr><tr><td>GPT-4-Turbo-0409</td><td>79.3</td><td>90.5</td></tr><tr><td>GPT-4o-0513</td><td>83.2</td><td>89.6</td></tr><tr><td>Claude-3-Opus-20240229</td><td>80.1</td><td>91.0</td></tr><tr><td>Claude-3-Sonnet-20240229</td><td>71.0</td><td>85.6</td></tr><tr><td><strong><em>Open-source LLMs</em></strong></td><td></td><td></td></tr><tr><td>command-r-plus-110b</td><td>65.5</td><td>63.5</td></tr><tr><td>Qwen1.5-7B-Chat</td><td>50.0</td><td>37.0</td></tr><tr><td>Qwen1.5-32B-Chat</td><td>65.0</td><td>65.0</td></tr><tr><td>Qwen1.5-72B-Chat</td><td>68.4</td><td>71.7</td></tr><tr><td><strong>Qwen2-7B-Instruct</strong></td><td><strong>60.0</strong></td><td><strong>57.0</strong></td></tr><tr><td><strong>Qwen2-57B-A14B-Instruct</strong></td><td><strong>68.0</strong></td><td><strong>74.0</strong></td></tr><tr><td><strong>Qwen2-72B-Instruct</strong></td><td><strong>78.0</strong></td><td><strong>86.6</strong></td></tr></tbody></table><p>For human evaluation, we compare Qwen2-72B-Instruct with GPT3.5, GPT4 and Claude-3-Opus using in-house evaluation set, which includes 10 languages ar, es, fr, ko, th, vi, pt, id, ja and ru (the scores range from 1~5):</p><table><thead><tr><th>Models</th><th>ar</th><th>es</th><th>fr</th><th>ko</th><th>th</th><th>vi</th><th>pt</th><th>id</th><th>ja</th><th>ru</th><th>Average</th></tr></thead><tbody><tr><td>Claude-3-Opus-20240229</td><td>4.15</td><td>4.31</td><td>4.23</td><td>4.23</td><td>4.01</td><td>3.98</td><td>4.09</td><td>4.40</td><td>3.85</td><td>4.25</td><td>4.15</td></tr><tr><td>GPT-4o-0513</td><td>3.55</td><td>4.26</td><td>4.16</td><td>4.40</td><td>4.09</td><td>4.14</td><td>3.89</td><td>4.39</td><td>3.72</td><td>4.32</td><td>4.09</td></tr><tr><td>GPT-4-Turbo-0409</td><td>3.44</td><td>4.08</td><td>4.19</td><td>4.24</td><td>4.11</td><td>3.84</td><td>3.86</td><td>4.09</td><td>3.68</td><td>4.27</td><td>3.98</td></tr><tr><td><strong>Qwen2-72B-Instruct</strong></td><td>3.86</td><td>4.10</td><td>4.01</td><td>4.14</td><td>3.75</td><td>3.91</td><td>3.97</td><td>3.83</td><td>3.63</td><td>4.15</td><td>3.93</td></tr><tr><td>GPT-4-0613</td><td>3.55</td><td>3.92</td><td>3.94</td><td>3.87</td><td>3.83</td><td>3.95</td><td>3.55</td><td>3.77</td><td>3.06</td><td>3.63</td><td>3.71</td></tr><tr><td>GPT-3.5-Turbo-1106</td><td>2.52</td><td>4.07</td><td>3.47</td><td>2.37</td><td>3.38</td><td>2.90</td><td>3.37</td><td>3.56</td><td>2.75</td><td>3.24</td><td>3.16</td></tr></tbody></table><p>Grouped by task types, the results are shown as follows:</p><table><thead><tr><th>Models</th><th>Knowledge</th><th>Understanding</th><th>Creation</th><th>Math</th></tr></thead><tbody><tr><td>Claude-3-Opus-20240229</td><td>3.64</td><td>4.45</td><td>4.42</td><td>3.81</td></tr><tr><td>GPT-4o-0513</td><td>3.76</td><td>4.35</td><td>4.45</td><td>3.53</td></tr><tr><td>GPT-4-Turbo-0409</td><td>3.42</td><td>4.29</td><td>4.35</td><td>3.58</td></tr><tr><td><strong>Qwen2-72B-Instruct</strong></td><td>3.41</td><td>4.07</td><td>4.36</td><td>3.61</td></tr><tr><td>GPT-4-0613</td><td>3.42</td><td>4.09</td><td>4.10</td><td>3.32</td></tr><tr><td>GPT-3.5-Turbo-1106</td><td>3.37</td><td>3.67</td><td>3.89</td><td>2.97</td></tr></tbody></table><p>These results demonstrate the strong multilingual capabilities of Qwen2 instruction-tuned models.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX's Starship rocket completes test flight, successfully splashes down (112 pts)]]></title>
            <link>https://www.cnbc.com/2024/06/06/spacex-starship-fourth-test-spaceflight.html</link>
            <guid>40598833</guid>
            <pubDate>Thu, 06 Jun 2024 15:47:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/06/06/spacex-starship-fourth-test-spaceflight.html">https://www.cnbc.com/2024/06/06/spacex-starship-fourth-test-spaceflight.html</a>, See on <a href="https://news.ycombinator.com/item?id=40598833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107425233" data-test="InlineImage"><p>SpaceX's Starship launches its fourth flight test from the company's Boca Chica launchpad, designed to eventually send astronauts to the moon and beyond, near Brownsville, Texas, U.S. , in this handout picture obtained on June 6, 2024.&nbsp;</p><p>Spacex | Via Reuters</p></div><div><p>SpaceX completed a test flight of its Starship rocket for the first time on Thursday, as the company pushed development of the mammoth vehicle past new milestones.</p><p>"Our first ever ship landing burn after a launch into space ... that was incredible," SpaceX communications manager Dan Huot said on the company's broadcast.</p><p><a href="https://www.cnbc.com/elon-musk/">Elon Musk's</a> company launched Starship at about 8:50 a.m. ET from its Starbase facility near Boca Chica, Texas.</p></div><div id="ArticleBody-InlineImage-107425169" data-test="InlineImage"><p>The SpaceX Starship launches during its fourth flight test from Boca Chica beach on June 06, 2024 in Brownsville, Texas.&nbsp;</p><p>Brandon Bell | Getty Images</p></div><div><p>A few minutes after launch, the rocket's booster successfully splashed down in the Gulf of Mexico, a new milestone for its development. This was the first time SpaceX returned the booster in one piece — a controlled return of the booster is key to the company's long-term goal of being able to launch and land Starship regularly, a practice it's made routine with its Falcon 9 rockets.</p><p>About an hour after the launch, Starship visibly survived reentry through the Earth's atmosphere, and the company confirmed that the rocket splashed down in the Indian Ocean to complete the mission. Starship appeared to withstand external damage during the intense heat of reentry, with debris apparent on the broadcast.</p><p>"Splashdown confirmed!" SpaceX posted on social media after the flight.</p><p>There were no people on board the fourth Starship spaceflight. The company's leadership has previously emphasized that SpaceX expects to fly hundreds of Starship missions before the rocket launches with any crew.</p></div><div id="ArticleBody-InlineImage-107425157" data-test="InlineImage"><p>The sun sets behind the SpaceX Starship ahead of its fourth flight test at Boca Chica beach on June 05, 2024 in Brownsville, Texas.&nbsp;</p><p>Brandon Bell | Getty Images</p></div><div><p>The Starship system is designed to be fully reusable and aims to become a new method of flying cargo and people beyond Earth. The rocket is also critical to NASA's plan to return astronauts to the moon. SpaceX won a multibillion-dollar contract from the agency to use Starship as a crewed lunar lander as part of NASA's Artemis moon program.</p><p>Shortly after the flight, NASA Administrator Bill Nelson congratulated SpaceX on its progress.</p><p>"We are another step closer to returning humanity to the Moon through Artemis — then looking onward to Mars," Nelson wrote in a social media post.</p><p>SpaceX has flown the full Starship rocket system on three spaceflight tests previously, with launches in <a href="https://www.cnbc.com/2023/04/20/spacex-starship-orbital-launch-attempt-live-updates.html">April 2023</a>, <a href="https://www.cnbc.com/2023/11/18/spacex-starship-second-rocket-launch.html">November</a> and <a href="https://www.cnbc.com/2024/03/14/spacex-starship-rocket-third-test-flight-launch.html">March</a>. Each of the test flights has achieved more milestones than the last, but in each result prior to Thursday the rocket was destroyed before the flight's end.</p><p>During the company's third test flight SpaceX tested new capabilities including opening and closing the payload door once in space —&nbsp;which would be how the rocket deploys payloads such as satellites on future missions — and&nbsp;<a href="https://www.cnbc.com/2023/12/05/spacex-plans-nasa-refueling-demonstration-for-next-starship-launch.html">transferring fuel&nbsp;</a>during the flight in a NASA demonstration.</p><p>SpaceX heavily emphasizes an approach of building "on what we've learned from previous flights" in its approach to develop Starship. The company says its strategy focuses on "recursive improvement" to the rocket, where even test flights with fiery outcomes represent progress toward its goal of a fully reusable rocket that can deliver people to the moon and Mars.</p><p>Musk last year said he expected the company to <a href="https://www.cnbc.com/2023/04/29/elon-musk-spacexs-starship-costing-about-2-billion-this-year.html">spend about $2 billion on Starship development</a> in 2023.</p></div><h2><a id="headline0"></a>The rocket</h2><div id="Placeholder-ArticleBody-Video-107029162" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000237329" aria-labelledby="Placeholder-ArticleBody-Video-107029162"><p><img src="https://image.cnbcfm.com/api/v1/image/107013976-1644535900772-S20_Chopstick_Stack_Desktop.jpg?v=1647016871&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Why Starship is indispensable for the future of SpaceX"><span></span><span></span></p></div><div><p>Starship is both the tallest and most powerful rocket ever launched. Fully stacked on the Super Heavy booster, Starship stands 397 feet tall and is about 30 feet in diameter.</p><p>The Super Heavy booster, which stands 232 feet tall, is what begins the rocket's journey to space. At its base are 33 Raptor engines, which together produce 16.7 million pounds of thrust – about double the 8.8 million pounds of thrust of NASA's Space Launch System rocket, which <a href="https://www.cnbc.com/2022/11/16/watch-live-nasa-launches-artemis-1-moon-mission.html">launched for the first time in 2022</a>.</p><p>Starship itself, at 165 feet tall, has six Raptor engines – three for use while in the Earth's atmosphere and three for operating in the vacuum of space.</p><p>The rocket is powered by liquid oxygen and liquid methane. The full system requires more than 10 million pounds of propellant for launch.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[lsix: Like "ls", but for images (279 pts)]]></title>
            <link>https://github.com/hackerb9/lsix</link>
            <guid>40598629</guid>
            <pubDate>Thu, 06 Jun 2024 15:29:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hackerb9/lsix">https://github.com/hackerb9/lsix</a>, See on <a href="https://news.ycombinator.com/item?id=40598629">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hackerb9/lsix/blob/master/README.md.d/thumb.png"><img src="https://github.com/hackerb9/lsix/raw/master/README.md.d/thumb.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">lsix</h2><a id="user-content-lsix" aria-label="Permalink: lsix" href="#lsix"></a></p>
<p dir="auto">Like "ls", but for images. Shows thumbnails in terminal using <a href="https://en.wikipedia.org/wiki/Sixel" rel="nofollow">sixel</a>
graphics.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Usage</h3><a id="user-content-basic-usage" aria-label="Permalink: Basic Usage" href="#basic-usage"></a></p>
<p dir="auto">Just typing <code>lsix</code> will show images in the current working directory.
You can also specify filenames and, of course, use shell wild cards
(e.g., <code>lsix *jpg *png</code>).</p>
<p dir="auto">Because lsix uses ImageMagick pretty much any image format will be
supported. However, some may be slow to render (like PDF), so lsix
doesn't show them unless you ask specifically. If you want to force a
listing of a certain type of image simply specify the filenames or
use a wildcard (<code>*.pdf</code> in the example below),.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hackerb9/lsix/blob/master/README.md.d/example1.png"><img src="https://github.com/hackerb9/lsix/raw/master/README.md.d/example1.png" alt="Example 1 of lsix usage" title="Most basic usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Expanding GIFs</h3><a id="user-content-expanding-gifs" aria-label="Permalink: Expanding GIFs" href="#expanding-gifs"></a></p>
<p dir="auto">If you specify a GIF (or actually any file that has multiple images in
it) on the command line, all the frames will get expanded and shown in
a montage. For example, <code>lsix nyancat.gif</code> shows all the frames. Note
that GIF stores some frames as only the pixels that differ from the
previous frame.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/hackerb9/lsix/blob/master/README.md.d/example2.png"><img src="https://github.com/hackerb9/lsix/raw/master/README.md.d/example2.png" alt="Example 2 of lsix usage" title="GIFs get expanded"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Terminal background color is detected</h3><a id="user-content-terminal-background-color-is-detected" aria-label="Permalink: Terminal background color is detected" href="#terminal-background-color-is-detected"></a></p>
<p dir="auto">You may have noticed that PNGs and SVG files have correct alpha
channel for the terminal background. That is because lsix uses
terminal escape sequences to try to figure out your foreground and
background colors. (Foreground is used for the text fill color.)</p>
<p dir="auto">In the first example below, after running <code>lsix</code> in a white on black
xterm, I sent an escape sequence to swap foreground and background
colors. When I ran it again, <code>lsix</code> detected it and changed the
background color to white. Of course, you can pick whatever default
colors you want (e.g., <code>xterm -bg blue</code>, in the second example below).</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hackerb9/lsix/blob/master/README.md.d/example3.png"><img src="https://github.com/hackerb9/lsix/raw/master/README.md.d/example3.png" alt="Example 3 of lsix usage" title="Reverse video works"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/hackerb9/lsix/blob/master/README.md.d/example4.png"><img src="https://github.com/hackerb9/lsix/raw/master/README.md.d/example4.png" alt="Example 4 of lsix usage" title="Even 'xterm -bg blue' works"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>
<p dir="auto">Detects if your terminal can display SIXEL graphics inline using <a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Sixel-Graphics" rel="nofollow">control sequences</a>.</p>
</li>
<li>
<p dir="auto">Works great over ssh. Perfect for manipulating those images on the
web server when you can't quite remember what each one was.</p>
</li>
<li>
<p dir="auto">Non-bitmap graphics often work fine (.svg, .eps, .pdf, .xcf).</p>
</li>
<li>
<p dir="auto">Automatically detects if your terminal, like xterm, can increase the
number of color registers to improve the image quality and does so.</p>
</li>
<li>
<p dir="auto">Automatically detects terminal's foreground and background colors.</p>
</li>
<li>
<p dir="auto">In terminals that support dtterm WindowOps, the number of tiles per
row will adjust appropriately to the window width.</p>
</li>
<li>
<p dir="auto">If there are many images in a directory (&gt;21), lsix will display them
one row at a time so you don't need to wait for the entire montage
to be created.</p>
</li>
<li>
<p dir="auto">If your filenames are too long, lsix will wrap the text before
passing it into ImageMagick's <code>montage</code>. (Without lsix, <code>montage</code> just
jumbles long filenames on top of one another.)</p>
</li>
<li>
<p dir="auto">You can easily change things like the width of each tile in the
montage, the font family, and point size by editing simple variables
at the top of the file. <em>(Tip: try <code>convert -list font</code> to see what
fonts you have on your machine.)</em></p>
</li>
<li>
<p dir="auto">Unicode filenames work fine, as long as your font has the glyphs.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Just put the <a href="https://github.com/hackerb9/lsix/blob/master/lsix"><code>lsix</code></a> file in your path (e.g., /usr/local/bin) and run
it. It's just a BASH shell script.</p>
<p dir="auto">The only prerequisite software is ImageMagick. If you don't have it
yet, your OS's package manager will make it easy to get. (E.g.,
<code>apt-get install imagemagick</code>).</p>
<p dir="auto">MacOS users may prefer to install lsix using <code>brew install lsix</code> which
installs ImageMagick, if necessary.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Your Terminal must support Sixel graphics</h2><a id="user-content-your-terminal-must-support-sixel-graphics" aria-label="Permalink: Your Terminal must support Sixel graphics" href="#your-terminal-must-support-sixel-graphics"></a></p>
<p dir="auto">I developed this using <a href="https://invisible-island.net/xterm/" rel="nofollow">xterm</a> in
vt340 emulation mode, but I believe this should work on
any Sixel compatible terminal. You may test your terminal by viewing a
single image, like so:</p>
<div data-snippet-clipboard-copy-content="convert  foo.jpg  -geometry 800x480  sixel:- "><pre><code>convert  foo.jpg  -geometry 800x480  sixel:- 
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">XTerm</h3><a id="user-content-xterm" aria-label="Permalink: XTerm" href="#xterm"></a></p>
<p dir="auto">Note that xterm does not have Sixel mode enabled by default, so you
need to either run it like so:</p>

<p dir="auto">Or, make vt340 the default terminal type for xterm. Add the following
to your <code>.Xresources</code> file and run <code>xrdb -merge .Xresources</code>.</p>
<div data-snippet-clipboard-copy-content="! Allow sixel graphics. (Try: &quot;convert -colors 16 foo.jpg sixel:-&quot;).
xterm*decTerminalID	:	vt340"><pre><code>! Allow sixel graphics. (Try: "convert -colors 16 foo.jpg sixel:-").
xterm*decTerminalID	:	vt340
</code></pre></div>
<p dir="auto">Further, some distributions, such as Fedora, appear to not compile <code>xterm</code>
with sixel support. In that case, try an alternate terminal, such as
<code>foot</code> or <code>mlterm</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">SIXEL compatible terminals</h3><a id="user-content-sixel-compatible-terminals" aria-label="Permalink: SIXEL compatible terminals" href="#sixel-compatible-terminals"></a></p>
<ul dir="auto">
<li>XTerm (tested)</li>
<li>MLterm (tested)</li>
<li>foot (tested)</li>
<li>Wezterm (tested)</li>
<li>Contour (tested)</li>
<li>iTerm2 for Apple MacOS (tested)</li>
<li>Konsole (reported)</li>
<li>yakuake (reported)</li>
<li>WSLtty for Microsoft Windows (reported)</li>
<li>MinTTY for Cygwin (Microsoft Windows) (reported)</li>
<li>Yaft for Linux framebuffer (tested)</li>
<li>VTE (special compilation, reported)</li>
<li>sixel-tmux (fork of tmux, reported)</li>
<li>ttyd (reported)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">SIXEL incompatible terminals</h3><a id="user-content-sixel-incompatible-terminals" aria-label="Permalink: SIXEL incompatible terminals" href="#sixel-incompatible-terminals"></a></p>
<ul dir="auto">
<li>MacOS Terminal, kitty</li>
<li>All standard libvte based terminals
<ul dir="auto">
<li>gnome-terminal</li>
<li>terminator</li>
<li>lxterm</li>
</ul>
</li>
<li>Alacritty (might work with <a href="https://github.com/alacritty/alacritty/pull/4763" data-hovercard-type="pull_request" data-hovercard-url="/alacritty/alacritty/pull/4763/hovercard">a patch</a>)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Because <code>lsix</code> is currently designed to be very simple, there are no
command line flags, no configuration files, no knobs to twiddle, or
frobs to frobnosticate. However, since the script is so simple, if you
want to make a change, it's pretty easy to do just by editing the
file. Everything is nicely commented with the most common default
variables at the top.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact the author</h2><a id="user-content-contact-the-author" aria-label="Permalink: Contact the author" href="#contact-the-author"></a></p>
<p dir="auto">I welcome feedback. If you use lsix and like it or have suggestions
for how it can be improved, please go ahead and send your thoughts to
me <a href="https://github.com/hackerb9/lsix/issues/new">@hackerb9</a> via
GitHub.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bugs</h2><a id="user-content-bugs" aria-label="Permalink: Bugs" href="#bugs"></a></p>
<ul dir="auto">
<li>
<p dir="auto">XTerm's reverse video mode (<code>xterm -rv</code>) is different from
specifying the foreground and background explicitly. There is a way
to detect the latter, but not the former. That means the background
color will be incorrect for folks who use XTerm's reverseVideo
resource. (See issue #20).</p>
</li>
<li>
<p dir="auto">XTerm's screen width is currently limited to 1000px due to a
misfeature which causes it to silently show nothing. This limitation
will be removed once xterm can handle images greater than 1000x1000.
[Last tested with XTerm(344)].</p>
</li>
<li>
<p dir="auto">Filenames that begin with "@" are special to ImageMagick and it'll
freak out if you don't prepend a directory. (<code>lsix ./@foo.png</code>)
(This is a bug in ImageMagick, not lsix).</p>
</li>
<li>
<p dir="auto">Specifying the empty string <code>""</code> as a filename makes ImageMagick hang.
(This appears to be an ImageMagick bug / misfeature).</p>
</li>
<li>
<p dir="auto">Long filenames are wrapped, but not intelligently. Would it
complicate this script too much to make it prefer to wrap on whites
space, dashes, underscores, and periods? Maybe.</p>
</li>
<li>
<p dir="auto">Directories specified on the command line are processed as if the
user had cd'd to that directory. It wouldn't be hard to implement
recursion, but is there actually a need? I'm reluctant to complicate
such a simple script with command line flags.</p>
</li>
<li>
<p dir="auto">If you run <code>lsix foo.avi</code>, you're asking for trouble.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Future Issues</h2><a id="user-content-future-issues" aria-label="Permalink: Future Issues" href="#future-issues"></a></p>
<ul dir="auto">
<li>
<p dir="auto">The Sixel standard doesn't appear to have a way to query the size of
the graphics screen. Reading the VT340 documentation, it appears
your program has to already know the resolution of the device you're
rendering on.</p>
<p dir="auto">XTerm, as of version 344, has added <a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Functions-using-CSI-_-ordered-by-the-final-character_s_" rel="nofollow">a control
sequence</a>
that solves the problem — <code>CSI ? Pi ; Pa ; Pv S</code> — but some
terminals, for example <code>mlterm</code>, haven't yet implemented it.</p>
<p dir="auto">There is an alternate way to read the window size using the dtterm
WindowOps extension but it is not quite the right solution as the
geometry of the Sixel graphics screen is not necessarily the same as
the window size. (For example, xterm limits the graphics geometry to
1000x1000, even though the window can actually be larger.) To help
with terminals such as mlterm, <code>lsix</code> will use the dtterm WindowOps
as a fallback.</p>
<p dir="auto">If neither solution works, <code>lsix</code> will assume you are on a VT340
(800x480) and can fit only 6 tiles per row.</p>
</li>
<li>
<p dir="auto">The Sixel standard also lacks a way to query the number of
color registers available. I used the extensions from <code>xterm</code> to do
so, but I do not know how widely implemented they are. If a terminal
does not respond, <code>lsix</code> presumes you're on an original vt340 and
uses only 16 color registers. (Sorry, 4-gray vt330 users! Time to
upgrade. ;-) )</p>
</li>
<li>
<p dir="auto">The <a href="https://kermitproject.org/" rel="nofollow">Kermit project</a> created a MS-DOS
terminal emulator that was popular in the late 1980s/early 1990s.
Its sixel implementation is not compatible with lsix because it
shows the graphics on a screen separate from the text. However, I
noticed one feature in its documentation: an escape sequence to
request the current graphics window size and number of colors:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content=" ESC [ ? 256 n                  Request screen size report

        Report is ESC [ ? 256; Ph; Pw; Pc n     for graphics systems

        where   Ph is screen height in dots
                Pw is screen width in dots
                Pc is number of colors (0, 1 or 16, for none, b/w, ega/vga)

        Report is ESC [ ? 24; 80; 0 n  for pure text mono systems."><pre><code> ESC [ ? 256 n                  Request screen size report

        Report is ESC [ ? 256; Ph; Pw; Pc n     for graphics systems

        where   Ph is screen height in dots
                Pw is screen width in dots
                Pc is number of colors (0, 1 or 16, for none, b/w, ega/vga)

        Report is ESC [ ? 24; 80; 0 n  for pure text mono systems.
</code></pre></div>
<p dir="auto">Did any other terminal emulators ever use the sequence? Would it be
worthwhile to add to <code>lsix</code>?</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://github.com/saitoha/libsixel">libsixel</a> is an excellent
project for writing programs that can output optimized Sixel
graphics commands. Because I have a lot of respect for the project,
I feel I should explain why <code>lsix</code> does not use libsixel.</p>
<ul dir="auto">
<li>
<p dir="auto">(a) I wanted lsix to work everywhere easily. Bash and imagemagick
are ubiquitous, so a shell script is a natural solution.</p>
</li>
<li>
<p dir="auto">(b) I wanted <code>lsix</code> to be simple enough that it could be easily
customized and extended by other people. (Including myself.)</p>
</li>
<li>
<p dir="auto">(c) ImageMagick has better support for reading different formats
than stb_image (the library used by libsixel's <code>img2sixel</code>). (For
example: xpm, svg, 16-bit png, and even sixel files are not
recognized by img2sixel). Since ImageMagick can read all of those
and write sixel output directly, it made sense to use it for both.</p>
</li>
<li>
<p dir="auto">(d) While libsixel is optimized and would surely be faster than
ImageMagick, it's overkill. For a simple directory listing, this
is plenty fast enough.</p>
</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Resources</h2><a id="user-content-resources" aria-label="Permalink: Resources" href="#resources"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://invisible-island.net/xterm/ctlseqs/ctlseqs.html" rel="nofollow">XTerm Control Sequences</a></p>
</li>
<li>
<p dir="auto"><a href="https://imagemagick.org/" rel="nofollow">ImageMagick</a></p>
</li>
<li>
<p dir="auto"><a href="https://vt100.net/docs/vt3xx-gp/" rel="nofollow">VT340 Programmer's Reference</a>:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://vt100.net/docs/vt3xx-gp/chapter14.html" rel="nofollow">Chapter 14</a>. Sixel Graphics.</p>
</li>
<li>
<p dir="auto"><a href="https://vt100.net/docs/vt3xx-gp/chapter16.html#S16.3" rel="nofollow">Chapter 16</a> Difference between Level 1 and Level 2 Sixel implementations.</p>
<p dir="auto"><em>Nota bene: this reference has the sense for DECSDM (sixel
display mode) reversed! The actual behaviour of the VT340 is
that when DECSDM is reset (the default), sixel scrolling is enabled.
This can be done by sending <em><code>Esc[?80l</code></em>, but lsix does not do
so as it would break many current terminal emulators.
See issue #41 for details.</em></p>
</li>
</ul>
</li>
<li>
<p dir="auto"><a href="https://archive.org/details/bitsavers_decstandar0VideoSystemsReferenceManualDec91_74264381" rel="nofollow">DEC STD 070 Video Systems Reference Manual</a>.
A weighty tome which covers nearly everything in exacting detail. I referred mostly to sections 4 (escape sequences) and 9 (sixel programming).</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/hackerb9/vt340test">VT340 Test</a>, a project to document the actual behaviour of the DEC VT340 hardware.</p>
</li>
<li>
<p dir="auto"><a href="http://www.vaxhaven.com/images/f/f7/EK-PPLV2-PM-B01.pdf" rel="nofollow">Digital ANSI-Compliant Printing Protocol: Level 2 Programming Reference Manual</a>, Chapter 5: Sixel Graphics. An excellent and reasonably clear discussion for anyone who wants to generate or parse sixel graphics.</p>
</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Right Not to Be Subjected to AI Profiling Based on Publicly Available Data (246 pts)]]></title>
            <link>https://link.springer.com/article/10.1007/s13347-023-00616-9</link>
            <guid>40597503</guid>
            <pubDate>Thu, 06 Jun 2024 14:03:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://link.springer.com/article/10.1007/s13347-023-00616-9">https://link.springer.com/article/10.1007/s13347-023-00616-9</a>, See on <a href="https://news.ycombinator.com/item?id=40597503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-article-body="true" data-track-component="article body">
                    <div id="Abs1-section" aria-labelledby="Abs1" data-title="Abstract" lang="en"><h2 id="Abs1">Abstract</h2><p>Social media data hold considerable potential for predicting health-related conditions. Recent studies suggest that machine-learning models may accurately predict depression and other mental health-related conditions based on Instagram photos and Tweets. In this article, it is argued that individuals should have a sui generis right not to be subjected to AI profiling based on publicly available data without their explicit informed consent. The article (1) develops three basic arguments for a right to protection of personal data trading on the notions of social control and stigmatization, (2) argues that a number of features of AI profiling make individuals more exposed to social control and stigmatization than other types of data processing (the exceptionalism of AI profiling), (3) considers a series of other reasons for and against protecting individuals against AI profiling based on publicly available data, and finally (4) argues that the EU General Data Protection Regulation does not ensure that individuals have a right not to be AI profiled based on publicly available data.</p></div>
                    
    


                    

                    

                    
                        
        
            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" data-track-component="inline-recommendations">
                <h3 id="inline-recommendations">Similar content being viewed by others</h3>
                
            </section>
        
            
        
    
                    

                    
                        
    

                    

                    
                        
                                <div>
                                    <div id="Sec1-section" data-title="Introduction"><h2 id="Sec1"><span>1 </span>Introduction</h2><div id="Sec1-content"><p>The analysis of social media data with artificial intelligence (AI) models holds considerable potential for predicting health-related conditions. In a study of Instagram photos, a machine-learning model was able to identify depressed users with greater precision than unassisted general practitioners (Reece &amp; Danforth, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Reece, A. G., &amp; Danforth, C. M. (2017). Instagram photos reveal predictive markers of depression. EPJ Data Science, 6(1), 15." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR55" id="ref-link-section-d32563071e340">2017</a>). Among others, the model used photo brightness and colours, number of comments, and likes as predictors and found that depression was associated with postings of bluer, darker, and greyer photos that received more comments, but fewer likes. In a related study of Twitter data, the developed model also achieved greater accuracy than unassisted physicians in identifying people with depression (Reece et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Reece, A. G., Reagan, A. J., Lix, K. L. M., Dodds, P. S., Danforth, C. M., &amp; Langer, E. J. (2017). Forecasting the onset and course of mental illness with Twitter data. Scientific Reports, 7(1), Art. 1." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR56" id="ref-link-section-d32563071e343">2017</a>). The study showed that the dominant contributors to the difference between depressed and healthy individuals were an increase in the use of negative words and a decrease in the use of positive words among depressed people. The study also found that increases in word count were positively associated with depression. Other studies have shown equally promising results for models using social media data for the prediction of mental disorders, including anxiety, bipolar, borderline personality disorder, schizophrenia, autism (Gkotsis et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2017" title="Gkotsis, G., Oellrich, A., Velupillai, S., Liakata, M., Hubbard, T. J. P., Dobson, R. J. B., &amp; Dutta, R. (2017). Characterisation of mental health conditions in social media using Informed Deep Learning. Scientific Reports, 7(1), Art. 1." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR22" id="ref-link-section-d32563071e346">2017</a>; Kim et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Kim, J., Lee, J., Park, E., &amp; Han, J. (2020). A deep learning model for detecting mental illness from user content on social media. Scientific Reports, 10(1), Art. 1." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR34" id="ref-link-section-d32563071e349">2020</a>; Kumar et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Kumar, A., Sharma, A., &amp; Arora, A. (2019). Anxious depression prediction in real-time social data. Proceeding of International Conference on Advanced Engineering, Science, Management and Technology, 1–7." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR35" id="ref-link-section-d32563071e352">2019</a>), and the risk of suicide and anorexia (Amini et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Amini, H., Mohammadi, E., &amp; Kosseim, L. (2022). Quick and (maybe not so) easy detection of anorexia in social media: To explainability and beyond, 141–158. In F. Crestani, D. E. Losada, &amp; J. Parapar (Eds), Early Detection of Mental Health Disorders by Social Media Monitoring: The First Five Years of the eRisk Project&nbsp;. Springer International Publishing." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR2" id="ref-link-section-d32563071e356">2022</a>; Coppersmith et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Coppersmith, G., Leary, R., Crutchley, P., &amp; Fine, A. (2018). Natural language processing of social media as screening for suicide risk. Biomedical Informatics Insights, 10, 1–11." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR13" id="ref-link-section-d32563071e359">2018</a>; Zirikly et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Zirikly, A., Resnik, P., Uzuner, Ö., &amp; Hollingshead, K. (2019). CLPsych 2019 Shared task: Predicting the degree of suicide risk in Reddit posts, 24-33. Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology. Minneapolis, Minnesota." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR68" id="ref-link-section-d32563071e362">2019</a>).</p><p>The increased predicting potential and widespread use of AI models across sectors has sparked an intense debate on how to regulate AI. A host of actors have issued a significant number of AI guidelines (Jobin et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Jobin, A., Ienca, M., &amp; Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature Machine Intelligence, 1(9), 389–399." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR30" id="ref-link-section-d32563071e368">2019</a>), and the EU Commission has recently issued a proposal for the regulation of AI. The promises and perils of mental health profiling based on social media data add fuel to this debate. Thus, predicting an identifiable individual’s health-related conditions (output data) based on social media data (input data) may benefit the individual in various ways, but it also raises significant issues of privacy. This article concerns an individual’s privacy rights in situations where the data used to make sensitive predictions has been made publicly available on, for instance, social media platforms. In particular, it is argued that individuals should be granted a sui generis legal right not to be subjected to AI profiling based on publicly available data without their explicit informed consent. As such, the legal right proposed is (1) a negative right that entitles individuals to non-interference with respect to AI profiling producing sensitive personal data (output data) based on online and publicly available personal data (input data), (2) it is a right that can be waived through informed consent, and (3) it is a pro tanto right, i.e. it is not an absolute or unconditional right, but a right that may be infringed under certain exceptional conditions (Frederick, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Frederick, D. (2014). Pro-tanto versus absolute rights. The Philosophical Forum, 45(4), 375–394." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR20" id="ref-link-section-d32563071e371">2014</a>).</p><p>The need for a sui generis legal right is substantiated through a four-step analysis. In the course of this analysis, it is argued:</p><ol>
                <li>
                  <span>1)</span>
                  
                    <p>that there are strong reasons for protecting personal data as such data may drive different types of social control and may lead to stigmatization</p>
                  
                </li>
                <li>
                  <span>2)</span>
                  
                    <p>that a number of features of AI profiling make individuals more exposed to social control and stigmatization than other types of data processing and that AI profiling thus poses a unique threat to individuals (the exceptionalism of AI profiling)</p>
                  
                </li>
                <li>
                  <span>3)</span>
                  
                    <p>that there are strong reasons for protecting public discourse and interaction on social media and that there are no obvious trumping concerns to the contrary</p>
                  
                </li>
                <li>
                  <span>4)</span>
                  
                    <p>that existing EU legislation—i.e. the General Data Protection Regulation (GDPR)—does not ensure individuals a right not to be AI profiled and that in any case, there are reasons for making it an explicit sui generis right</p>
                  
                </li>
              </ol><p>The article is ended by a few reflections on some of the key questions raised by the proposed arguments, and an agenda for future research is suggested.</p><p>All throughout the notion of ‘data’ will be used to denote both the data being produced by AI profiling, i.e. the output data, and the data on which the AI profiling is based, i.e. the input data. The context will determine the type of data being referred to.</p></div></div><div id="Sec2-section" data-title="Three Cases of AI Profiling Based on Social Media Data"><h2 id="Sec2"><span>2 </span>Three Cases of AI Profiling Based on Social Media Data</h2><div id="Sec2-content"><p>The four-step analysis provided in this article will involve three stylised cases of AI-driven mental health profiling. These cases are included in order to illustrate the various different contexts in which AI-driven mental health profiling may be conducted. The cases are as follows:</p>
                <blockquote><div>
                  <p>
                    <b>The Friends Case</b>
                  </p>
                  <p>Two close friends, A and B, are socializing on a regular basis. At their recent gatherings, B has been absentminded and shown some signs of a general lack of motivation. Being concerned about the friend but unwilling to confront B with weakly evidenced suspicions, A decides to do an AI-generated mental health profile of B. Therefore, A collects a series of photos that B has made publicly available on Instagram and feeds them a locally stored copy of the highly accurate mental health prediction model <i>Deepmood</i>. Much to the surprise of A, the model predicts that B is bipolar. A reports the findings to B and B’s family.</p>
                  <p>
                    <b>The Public Servant Case</b>
                  </p>
                  <p>A public servant, A, in charge of the unemployment benefit scheme at the local municipalities comes to suspect that a client, B, may be suffering from a chronic mental illness likely to keep B in long-term unemployment. Wanting to ensure that B is adequately handled by ‘the system’ based on objective information, but unwilling to cause unnecessary unrest, A decides to do an AI-generated mental health profiling of B based on a series of public tweets from B. A locally stored copy of the highly accurate mental health prediction model <i>Deepmood</i> suggests that B suffers from anxiety. A reports the findings to B.</p>
                  <p>
                    <b>The Prime Minister Case</b>
                  </p>
                  <p>A concerned citizen, A, fears that the upcoming general election may pave the way for a prime minister candidate, B, whom A suspects may be mentally unstable and thus unfit for the job. Believing that it is in the public interest to have reliable information about the mental health of B, A decides to do an AI-generated mental health profile of B. A collects a sample of public tweets from B and feeds them to a locally stored copy of the highly accurate mental health prediction model <i>Deepmood</i>. The model suggests that the candidate suffers from severe depression. A reports the findings to the general public.</p>
                </div></blockquote>
              </div></div><div id="Sec6-section" data-title="Reasons for Protecting Personal Data"><h2 id="Sec6"><span>3 </span>Reasons for Protecting Personal Data</h2><div id="Sec6-content"><p>The philosophical literature on the value and definition of privacy is vast (Leino-Kilpi et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Leino-Kilpi, H., Välimäki, M., Dassen, T., Gasull, M., Lemonidou, C., Scott, A., &amp; Arndt, M. (2001). Privacy: A review of the literature. International Journal of Nursing Studies, 38(6), 663–671." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR37" id="ref-link-section-d32563071e489">2001</a>). This article develops and considers three arguments in favour of a right to privacy specifically in relation to the use of personal data. As will become evident, the three arguments are particularly relevant for an analysis of the previously introduced cases involving AI profiling.</p><h3 id="Sec7"><span>3.1 </span>The Social Pressure Argument</h3><p>The argument from social pressure is an autonomy-based argument for the right to privacy regarding personal data. Essentially, the argument contends that access to personal data about individuals may be used to exert communicative pressure on the choices and actions of these individuals in ways and to an extent incompatible with their interests and preferences. Communicative social pressure can be defined the following way: A communicative act—verbal or nonverbal—performed by a sender, say A, amounts to social pressure on a recipient, say B, if and only if it succeeds in making B believe that certain choices and actions are associated with costs to an extent, making it less likely that B will make the relevant choice or perform the relevant action. In the friends case, A may use knowledge of B’s diagnosis to pressurize B into treatment by communicating in one way or another that if B remains untreated, it will have negative consequences not only for B, but also for family and friends. In the case of the prime minister, knowledge of B’s diagnosis can similarly be used to pressure B to abandon the candidacy. The influence of unwanted social pressure based on personal data may be limited through a right to privacy. Insofar as individuals have a right to exercise personal autonomy, where this is taken to include a right to protect themselves against unwanted social pressure, they should have a right to privacy limiting the access of others to data about them. Thus defined, a right to privacy empowers individuals to influence the ways and extent to which they are subjected to social pressure. It does so by letting individuals shape the social pressure by deciding the level, character, and with whom they share data about themselves.</p><p>Here, three observations must be made. First, the argument does not claim that access to accurate data is a necessary condition of social pressure, but only that it may be sufficient for others to be able to exercise such pressure. An individual may put pressure on another individual in various ways without having any knowledge of the latter. Second, it does not make any distinctions between different types of data about individuals, e.g. personal and non-personal data. One may claim, however, that personal data for various reasons is more effective in the attempt to put pressure on another individual and that individuals would therefore withhold such data to a larger extent than nonpersonal data. Third, the argument does not operate on a distinction between different methods of exerting social pressure on choices and actions. Social pressure methods are many and versatile. They cover a spectrum of verbal and non-verbal, explicit, and covert communicative acts that include persuasion, coercion, and manipulation. Persuasion can be defined as the act of pointing out verbally the good and bad consequences of a certain course of action (Powers, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Powers, P. (2007). Persuasion and coercion: A critical review of philosophical and empirical approaches. HEC Forum, 19(2), 125–143." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR53" id="ref-link-section-d32563071e502">2007</a>), and coercion as the verbal act of threatening to make someone worse off than he or she would otherwise be or ought to be (Nozick, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1969" title="Nozick, R. (1969). Coercion, 440-472.  In Morgenbesser, W. (Ed). Philosophy, Science, and Method: Essays in Honor of Ernest Nagel. St Martin's Press." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR46" id="ref-link-section-d32563071e505">1969</a>; Wertheimer, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1990" title="Wertheimer, A. (1990). Coercion. Princeton University Press." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR66" id="ref-link-section-d32563071e508">1990</a>). Manipulation covers a range of covert attempts at influencing other people through the tailoring of information, e.g. by lying or withholding information, by making exaggerations, and by framing information that is likely to lead others to believe what is false (Beauchamp &amp; Childress, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Beauchamp, T. L., &amp; Childress, J. F. (2001). Principles of biomedical ethics (5th ed). Oxford University Press, New York." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR6" id="ref-link-section-d32563071e511">2001</a>). While the ethical gravity of performing acts of persuasion, coercion, and manipulation differs significantly, the argument from social pressure is indifferent to such distinctions. It implies a right to be left alone in the broader sense of being able to limit all kinds of data-facilitated <i>social pressure</i>.</p><h3 id="Sec8"><span>3.2 </span>The ‘open future’ Argument</h3><p>The ‘open future’ argument is also an autonomy-based argument for the right to privacy. At the heart of the open future argument lies the observation that personal data shared by individuals at a certain point in their life may come to shape the future opportunities afforded to them by others in ways that run counter to their interests. In short, the ‘open future’ argument contends that access to the personal data of individuals can lead to unwanted interventions in future choices. A choice-set intervention can be defined as follows: An agent, say A, makes an intervention in the set of choices of another agent, say B, if and only if A, on the basis of having access to certain data about B, shapes the choices of B differently than they would otherwise have been. In the public servant case, the public servant’s knowledge of the diagnosis of client B can change the opportunities offered to B within social services. Similarly, the general public’s knowledge of the diagnosis of candidate B in the prime minister case may have consequences for the career opportunities offered by existing political parties. In both cases, the effects on the choice set of B can go against the interests and preferences of B. If people have a right to exercise personal autonomy, where this includes a right to protect themselves against unwanted influences on the options available to them in the future, they must also have a right to privacy limiting the access of others to the personal data that may be used for shaping their future choice set.</p><p>Several observations must be made here. First, the notion of <i>shaping future options</i> should be understood both quantitatively and qualitatively. An open future is both a matter of the number of available options, but also a matter of the availability of certain vital options (Garrett et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Garrett, J. R., Lantos, J. D., Biesecker, L. G., Childerhose, J. E., Chung, W. K., Holm, I. A., Koenig, B. A., McEwen, J. E., Wilfond, B. S., &amp; Brothers, K. (2019). Rethinking the “open future” argument against predictive genetic testing of children. Genetics in Medicine, 21(10), 2190–2198." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR21" id="ref-link-section-d32563071e531">2019</a>). While the prime minister candidate may not necessarily be robbed of a political career in the highest office, it may be a career under constant accusations of being unduly influenced by depression. Second, the argument from an ‘open future’ differs from the argument from social pressure. Although the exercise of social pressure may be effective in associating a certain choice with costs to an extent making this choice unlikely to be made, it may not limit or otherwise significantly alter the choice set prior to the choice situation obtaining. Moreover, social pressure requires a communicative act of some kind. In the following, the exercise of social pressure and choice-set interventions shall both be referred to as forms of <i>social control</i>. Third, the ‘open future’ argument presented here differs from the standard ‘open choice’ argument in relation to children. Feinberg famously argued that a child’s right to autonomy in adult life can be violated in advance through parental choices that limit a child’s opportunities in later life (Feinberg, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1980" title="Feinberg, J. (1980). The child’s right to an open future, 76-97.  In Feinberg, J. (1992). Freedom and Fulfillment: Philosophical Essays. Princeton University Press, New Jersey." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR19" id="ref-link-section-d32563071e537">1980</a>). The ‘open future’ argument advocated does not posit a future autonomy right that can be violated in advance. It is an individual’s right to personal autonomy in the present that grounds a right to act in the interest of protecting the future autonomy of the individual.</p><h3 id="Sec9"><span>3.3 </span>The Stigmatization Argument</h3><p>The stigmatization argument grounds the right to privacy in the potential harms of suffering stigmatization. According to Goffman, stigmatization occurs when a person is attributed with a discreditable trait that makes the person inferior, dangerous, and perhaps even inhuman, and when this act of discrediting leads to discrimination (Goffman, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1963" title="Goffman, E. (1963). Stigma: Notes on the management of spoiled identity. Simon &amp; Schuster, New York." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR23" id="ref-link-section-d32563071e548">1963</a>; Ploug, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Ploug, T. (2020). In Defence of informed consent for health record research—Why arguments from ‘easy rescue’, ‘no harm’ and ‘consent bias’ fail. BMC Medical Ethics, 21(1), 75." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR48" id="ref-link-section-d32563071e551">2020</a>). In a later reinterpretation of Goffman, stigmatization is taken to be a complex phenomenon at the societal level constituted by a process in which a difference between people is identified, labelled, and associated with negative stereotypes, followed by acts of segregation (‘us’ and ‘them’) and discrimination made possible by a greater social, economic, and political power of the labelling party (Link &amp; Phelan, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Link, B. G., &amp; Phelan, J. C. (2001). Conceptualizing stigma. Annual Review of Sociology, 27, 363–385." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR39" id="ref-link-section-d32563071e554">2001</a>; Ploug et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Ploug, T., Holm, S., &amp; Gjerris, M. (2015). The stigmatization dilemma in public health policy-the case of MRSA in Denmark. BMC Public Health, 15(1), 640." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR51" id="ref-link-section-d32563071e557">2015</a>). So defined, stigmatization clearly presupposes access to data about people. The identification and labelling of a difference between people—intentionally or unintentionally—is an act that requires access to data. In the three cases, access to AI-generated diagnoses would allow the friend, client, and candidate to be identified as different and labelled, e.g. ‘mentally ill’, and thus be included in a stigmatized group in society (more on this below). A right to privacy that limits access to certain data could protect people from stigmatization.</p><p>One may perhaps be tempted to object here that it seems as if the real moral concern with stigmatization is discrimination, i.e. morally unjustified differential treatment. However, research shows that stigmatization is associated with other harms to stigmatized individuals. The case of mental illness is highly illustrative. Studies show not only that mental illnesses are associated with negative stereotypes (Bhugra, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Bhugra, D. (1989). Attitudes towards mental illness. Acta Psychiatrica Scandinavica, 80(1), 1–12." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR7" id="ref-link-section-d32563071e563">1989</a>) but also indicate discrimination against mentally ill people in the labour market and in health care (Druss et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Druss, B. G., Zhao, L., Von Esenwein, S., Morrato, E. H., &amp; Marcus, S. C. (2011). Understanding excess mortality in persons with mental illness: 17-year follow up of a nationally representative US survey. Medical Care, 49(6), 599–604. JSTOR." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR16" id="ref-link-section-d32563071e566">2011</a>; Stuart, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2006" title="Stuart, H. (2006). Mental illness and employment discrimination. Current Opinion in Psychiatry, 19(5), 522–526." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR60" id="ref-link-section-d32563071e569">2006</a>). Therefore, the stigmatization elements introduced above—that is, labelling, negative stereotyping, segregation, discrimination, and power asymmetry—are present in relation to this group (Link &amp; Phelan, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2001" title="Link, B. G., &amp; Phelan, J. C. (2001). Conceptualizing stigma. Annual Review of Sociology, 27, 363–385." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR39" id="ref-link-section-d32563071e572">2001</a>). But there are other harms to this group than discrimination. Evidence on labelling of mentally ill people in healthcare suggests that it can lead to the self-application of labels and negative stereotypes, where this is associated with lower self-esteem, demoralization, income loss, unemployment, and self-discrimination (Corrigan et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Corrigan, P. W., Larson, J. E., &amp; Rüsch, N. (2009). Self-stigma and the “why try” effect: Impact on life goals and evidence-based practices. World Psychiatry, 8(2), 75–81." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR14" id="ref-link-section-d32563071e575">2009</a>; Link, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1987" title="Link, B. G. (1987). Understanding labeling effects in the area of mental disorders: An assessment of the effects of expectations of rejection. American Sociological Review, 52(1), 96–112. JSTOR." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR38" id="ref-link-section-d32563071e579">1987</a>). Studies of barriers to mental health treatment find that negative attitudes toward mental illnesses keep patients from seeking health care (Mojtabai et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Mojtabai, R., Olfson, M., Sampson, N. A., Jin, R., Druss, B., Wang, P. S., Wells, K. B., Pincus, H. A., &amp; Kessler, R. C. (2011). Barriers to mental health treatment: Results from the National Comorbidity Survey Replication. Psychological Medicine, 41(8), 1751–1761." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR44" id="ref-link-section-d32563071e582">2011</a>; Voorhees et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Voorhees, B. W. V., Fogel, J., Houston, T. K., Cooper, L. A., Wang, N.-Y., &amp; Ford, D. E. (2005). Beliefs and attitudes associated with the intention to not accept the diagnosis of depression among young adults. The Annals of Family Medicine, 3(1), 38–46." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR64" id="ref-link-section-d32563071e585">2005</a>). In turn, this can drive inequality in health care (Stuber et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Stuber, J., Meyer, I., &amp; Link, B. (2008). Stigma, prejudice, discrimination and health. Social Science &amp; Medicine, 67(3), 351–357." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR61" id="ref-link-section-d32563071e588">2008</a>). Studies on the effects of stigmatization of people with AIDS show that the feeling of stigmatization due to one’s health status is associated with anxiety, depression, distrust, and the disruption of normal social relationships (Crandall &amp; Coleman, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1992" title="Crandall, C. S., &amp; Coleman, R. (1992). Aids-related stigmatization and the disruption of social relationships. Journal of Social and Personal Relationships, 9(2), 163–177." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR15" id="ref-link-section-d32563071e591">1992</a>; Herek, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Herek, G. M. (1999). AIDS and stigma. American Behavioral Scientist, 42(7), 1106–1116." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR26" id="ref-link-section-d32563071e594">1999</a>). Similar findings of stigmatization and its harmful effects have been reported for smokers and obese people (Goldstein, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1991" title="Goldstein, J. (1991). The stigmatization of smokers: An empirical investigation. Journal of Drug Education, 21(2), 167–182." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR24" id="ref-link-section-d32563071e598">1991</a>; Hilbert et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Hilbert, A., Rief, W., &amp; Braehler, E. (2008). Stigmatizing attitudes toward obesity in a representative population-based sample. Obesity, 16(7), 1529–1534." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR27" id="ref-link-section-d32563071e601">2008</a>; Myers &amp; Rosen, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1999" title="Myers, A., &amp; Rosen, J. C. (1999). Obesity stigmatization and coping: Relation to mental health symptoms, body image, and self-esteem. International Journal of Obesity, 23(3), 221–230." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR45" id="ref-link-section-d32563071e604">1999</a>; Peretti-Watel et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2014" title="Peretti-Watel, P., Legleye, S., Guignard, R., &amp; Beck, F. (2014). Cigarette smoking as a stigma: Evidence from France. International Journal of Drug Policy, 25(2), 282–290." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR47" id="ref-link-section-d32563071e607">2014</a>; Stuber et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Stuber, J., Galea, S., &amp; Link, B. G. (2009). Stigma and smoking: The consequences of our good intentions. Social Service Review, 83(4), 585–609." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR62" id="ref-link-section-d32563071e610">2009</a>).</p><p>A comment must be made. In this section, it has been argued that the threat of stigmatization and the associated harms could and should ground a right to privacy, but perhaps less than stigmatization is needed to make this argument. Research suggests that negative stereotyping promulgated, for instance, in the media, is associated with a number of harms to members of the stereotyped groups. It may elicit situational responses of negative emotions and stress, induce self-discrimination in relation to a variety of activities, and impair cognitive and educational achievements (Appel &amp; Weber, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Appel, M., &amp; Weber, S. (2021). Do mass mediated stereotypes harm members of negatively stereotyped groups? A meta-analytical review on media-generated stereotype threat and stereotype lift. Communication Research, 48(2), 151–179." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR4" id="ref-link-section-d32563071e616">2021</a>). A right to privacy may protect against such weaker forms of singling out individuals on the basis of personal data.</p></div></div><div id="Sec10-section" data-title="Reasons for Protecting an Individual Against AI Profiling"><h2 id="Sec10"><span>4 </span>Reasons for Protecting an Individual Against AI Profiling</h2><div id="Sec10-content"><p>The three arguments developed above have foremost provided reasons for a <i>general</i> right to data privacy. Some of the considerations along the way have indicated that personal data may make an individual particularly vulnerable to attempts at social control and the harm of stigmatization. However, the arguments do not distinguish AI profiling from any other processing of personal data, and hence, they cannot in and of themselves sustain a sui generis right not to be subjected to automated profiling based on publicly available data. In this section, it is therefore argued that a number of features of AI profiling make individuals more exposed to attempts of social control and stigmatization and that—in doing so—AI profiling presents a particularly invasive kind of data processing.</p><h3 id="Sec11"><span>4.1 </span>The Exceptionalism of AI Profiling 1: Predictive Accuracy and Social Control</h3><p>As evidenced by the studies cited in the introduction, AI profiling differs from other types of data processing by potentially making highly accurate predictions about the future behaviour and dispositions of individuals. Mental health profiling as in the three cases is not only a matter of categorizing health problems in the past and present, but it also involves predicting mental health dispositions that will shape an individual’s future. In the debate on the exceptionalism of genetic data, it has been claimed that such data are exceptional in that they can be used as a predictor of future disease (McGuire et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="McGuire, A. L., Fisher, R., Cusenza, P., Hudson, K., Rothstein, M. A., McGraw, D., Matteson, S., Glaser, J., &amp; Henley, D. E. (2008). Confidentiality, privacy, and security of genetic and genomic test information in electronic health records: Points to consider. Genetics in Medicine, 10(7), 495–499." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR42" id="ref-link-section-d32563071e638">2008</a>). The counterargument has been that the predictive power of genetic data is limited, because genetic risk factors are only—with the exception of a few rare, highly penetrant genotypes—among many contributors to disease (Evans &amp; Burke, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2008" title="Evans, J. P., &amp; Burke, W. (2008). Genetic exceptionalism. Too much of a good thing? Genetics in Medicine, 10(7), Art. 7." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR18" id="ref-link-section-d32563071e641">2008</a>). Notably, AI profiling does not have such inherent limitations. AI profiling, e.g. in relation to diagnostics, can incorporate all social, genetic, and other data types relevant for making predictions about future disease. In doing so, AI profiling may make highly accurate predictions about the future behaviour and dispositions of individuals.</p><p>Predictive profiling of behaviour and dispositions exposes individuals more to attempts of social control than they would otherwise have been. Here is why. Attempts at socially controlling the future behaviour of individuals may—at least in some cases—rest on data-driven beliefs about their future behaviour and dispositions. Individuals for whom there are no (accurate) personal data indicating that they are disposed for smoking seem unlikely to be subjected to attempts of social control (with regard to smoking behaviour) to the same degree as individuals for whom such data exist. What matters here is first and foremost the existence and accessibility of personal data indicating that an individual may come to smoke in the future. However, secondly, it seems equally important that the data are accurate and therefore reliable. Although access to more or less reliable historic data evidencing that an individual once tried smoking may trigger attempts at socially controlling his or her future smoking behaviour, it seems that a reliable prediction of future smoking is more likely to do so. Third, it also matters whether the prediction concerns future behaviour or more deeply rooted dispositions, where a disposition may for present purposes be defined as a trait leading to repeated occurrences of a particular type of behaviour in particular circumstances. A prediction of a smoking disposition may trigger a more intensive effort at social control than a prediction of an isolated occurrence of smoking behaviour.</p><p>AI profiling may bring to the fore highly accurate predictions of future behaviour and dispositions that may lead to beliefs triggering various attempts of social control that would not otherwise have been attempted. Such a scenario is uniquely dependent on predictive profiling. To the extent that this scenario is considered possible—perhaps even likely—the predictive character of AI profiling makes an individual more exposed to attempts of social control.</p><h3 id="Sec12"><span>4.2 </span>The Exceptionalism of AI Profiling 2: Predictive Accuracy, Determinism, and Stigmatization</h3><p>The potential of AI profiling to make highly accurate predictions may also increase the risk of stigmatization. Although predictability and determinism are considered logically independent notions (Bishop, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2003" title="Bishop, R. C. (2003). On separating predictability and determinism. Erkenntnis, 58, 169–188." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR8" id="ref-link-section-d32563071e658">2003</a>; Rummens &amp; Cuypers, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2010" title="Rummens, S., &amp; Cuypers, S. E. (2010). Determinism and the paradox of predictability. Erkenntnis, 72(2), 233–249." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR58" id="ref-link-section-d32563071e661">2010</a>), they seem to play a similar role in our daily and practical lives. If someone’s behaviour and dispositions are fully predictable, then the common-sense interpretation of this is that the behaviour and dispositions could not have been otherwise—they are necessary and inescapable. The same holds for determinism. Fully determined behaviour or dispositions entail the inevitability of this behaviour and these dispositions. Why does it matter?</p><p>Studies on public beliefs in genetic and social determinism, i.e. that genetic and social factors determine human behaviour and personality, show that such beliefs are associated with social cognitive effects such as negative stereotyping, the formation of prejudice, and tendencies to discriminate by confining the civil rights of others (Keller, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2005" title="Keller, J. (2005). In genes we trust: The biological component of psychological essentialism and its relationship to mechanisms of motivated social cognition. Journal of Personality and Social Psychology, 88(4), 686–702." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR32" id="ref-link-section-d32563071e667">2005</a>; Rangel &amp; Keller, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2011" title="Rangel, U., &amp; Keller, J. (2011). Essentialism goes social: Belief in social determinism as a component of psychological essentialism. Journal of Personality and Social Psychology, 100(6), 1056." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR54" id="ref-link-section-d32563071e670">2011</a>). Furthermore, these studies also show that such beliefs are related to correlates of essentialist thinking, where the latter may be defined as the tendency of individuals to attribute to others an unalterable essence that makes them what they are and that explains their actions (Medin, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1989" title="Medin, D. L. (1989). Concepts and conceptual structure. American Psychologist, 44(12), 1469–1481." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR43" id="ref-link-section-d32563071e673">1989</a>; Yzerbyt et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1997" title="Yzerbyt, V., Rocher, S., &amp; Schadron, G. (1997). Stereotypes as explanations: A subjective essentialistic view of group perception, 20-50. In R. Spears, P. J. Oakes, N. Ellemers, &amp; S. A. Haslam (Eds.). The social psychology of stereotyping and group life. Blackwell Publishing." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR67" id="ref-link-section-d32563071e676">1997</a>). In combination, these findings are taken to suggest a mechanism whereby individuals attribute to others an essential nature—a set of dispositions—to the extent they believe their behaviour and dispositions to be genetically and socially determined, and this attribution of essence results in negative stereotyping, the formation of prejudice, and tendencies to discriminate.</p><p>The argument to be made here is simply this. If a belief in the predictability of behaviour and dispositions plays the same role as or may cause a belief in the determinism of behaviour and dispositions, then it may lead to essentialist thinking producing negative stereotyping and discrimination, which are defining components of stigmatization. AI profiling may produce highly accurate predictions and thus substantiate beliefs in the predictability of behaviour and dispositions. Hence, AI profiling may lead to stigmatization.</p><p>The argument is somewhat speculative, and more conceptual and empirical work is certainly needed. However, note that it has some intuitive appeal. If we were to believe that the client in the public servant case or the prime minister candidate in the prime minister case was <i>essentially</i> and <i>unalterably</i> mentally ill, then it does not seem wholly implausible that this would lead to negative stereotyping and discrimination. If we believe others to be machines, then we will treat them as machines. If the argument holds, it goes to show specifically that the predictive accuracy of AI profiling makes individuals more exposed to stigmatization.</p><h3 id="Sec13"><span>4.3 </span>The Exceptionalism of AI Profiling 3: A Purely Feature-Driven Approach</h3><p>The arguments in the previous two sections have a similar structure. It has been argued that a special feature of AI models, i.e. predictive accuracy, in combination with certain assumptions about human belief formation makes profiled individuals more exposed to social control and stigmatization. However, there are other features of AI profiling that in and of themselves make profiled individuals more exposed to social control and stigmatization.</p><p>AI modelling is versatile. Different predictive AI models can be trained on some data and subsequently applied to the same kind of data. In other words, it is possible to develop AI models that make different predictions based on the same input data. In the three cases, AI profiling based on publicly available photos and tweets serves the purpose of predicting mental health dispositions. The same publicly available data could potentially be used to predict other personal dispositions, including creditworthiness, risk of criminal behaviour and drug abuse, political leanings, and parental competencies. Any data made publicly available at any time could turn out to be a relevant input for an AI model that makes predictive profiling. The versatility of AI modelling implies that a vast amount of personal data may potentially be generated from limited publicly available data. The potential increase in the availability of personal data that may be used for the purpose of social control and stigmatization makes an individual more exposed to such interventions.</p><p>Predictive AI profiling is human-unpredictable. AI profiling may reveal personal data that would otherwise be hidden from a human being. In the three cases, the mental dispositions are not available to any human seeing the photos or reading the tweets, including the individual making them publicly available. There are no human interpretable signs that suggest that photos and tweets contain reliable data on mental dispositions. But the mental dispositions are also human unpredictable in a deeper sense. Thus, any other nonautomated method for making the same predictions based on the available data, e.g. statistical modelling, would be bounded by the availability of human expertise and other humanly limitations and may not deliver the same accuracy. The human unpredictability of AI profiling makes an individual more exposed to social control and stigmatization for two reasons. Firstly, because the unpredictability impedes an individual’s ability to exercise self-protection by withholding data. If it is impossible to foresee what predictions can be made from a set of public data, individuals cannot withhold data that they believe may generate unwelcome attempts of social control and that they believe would place them in groups already stigmatized or likely to become stigmatized. Avoiding entirely to make any data public to anyone seems practically impossible in a modern-day society with the growing digitization of human relations. Second, because AI makes predictable what is humanly unpredictable, it removes the protection against social control and stigmatization that is tied to human unpredictability. That no human being may predict mental dispositions just by looking at photos and tweets and that other nonautomated methods are bounded by the availability of human expertise constitutes a protection of individuals against social control and stigmatization. It means that the predictions will not be generated and therefore cannot feed attempts of social control and stigmatization.</p><p>AI technology and embedded models are highly transferable. Although the development of a high-performing AI model requires access to very large datasets, expert knowledge, hardware, and various other resources, the resulting model may be widely used by a host of different stakeholders. As imagined in the three cases, highly accurate predictive AI models may well be made available for public use. AI models differ in this respect from other technologies. Whole genome/exome sequencing is a comparable technology in that it can be used to map the complete DNA sequence of an individual from a sample of human tissue. This could, for instance, be human tissue left in public as fingerprints on a glass. The resulting genetic data can to some extent be used to make predictions about human dispositions and, in particular, disease risks. Contrary to AI technology, however, the whole genome/exome sequencing technology along with required lab facilities, biomedical expertise, etc. cannot be made publicly available. The inaccessibility of this technology appears to reduce the risk of being genetically profiled inadvertently, while the transferability of AI technology arguably increases the risk of being profiled by AI and thus increases the exposure of an individual to social control and stigmatization.</p><p>Finally, AI predictive profiling goes beyond an individual. AI predictions of dispositions will in certain cases reveal personal data not only about individuals, but also about their relatives. An AI model that predicts that an individual is likely to develop diabetes 2—known to be partly heritable and partly due to lifestyle—will also, as a matter of fact, have revealed an increased risk of diabetes 2 among close relatives. Such inferences will be warranted to a very different degree. However, insofar as AI predictive profiling may reveal personal data about relatives, it makes those relatives more exposed to social control and stigmatization than they would otherwise have been.</p></div></div><div id="Sec14-section" data-title="Reasons for and Against Protecting Publicly Available Online Data"><h2 id="Sec14"><span>5 </span>Reasons for and Against Protecting Publicly Available Online Data</h2><div id="Sec14-content"><p>The previous section provided ground for granting individuals a right not to be subjected to AI profiling. This article seeks, however, to sustain the right in relation to personal data made publicly available online, e.g. on social media platforms such as Twitter and Instagram. Notably, the availability of personal data on such platforms is to a large extent influenced by the data subjects themselves. If an individual refrains from any use of online services and social media platforms, little personal data would ceteris paribus be publicly available. In so doing individuals would therefore—presumably to a large extent—protect themselves against AI profiling based on online available, personal data. But if individuals may protect themselves against AI profiling by disengaging from online data sharing, then the right not to be AI profiled based on publicly available data seems to have been rendered somewhat superfluous. This section therefore explores, firstly, the reasons for protecting and promoting online data sharing and, secondly, the reasons for allowing AI profiling based on such data.</p><h3 id="Sec15"><span>5.1 </span>Social and Democratic Potentials of Online Data Sharing</h3><p>The online sharing of data may serve important social and democratic purposes.</p><p>Socially, it can be used to build and maintain relationships. Research finds that socializing is a primary motivation for information sharing on social networking sites (SNS) (Kümpel et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Kümpel, A. S., Karnowski, V., &amp; Keyling, T. (2015). News sharing in social media: A review of current research on news sharing users, content, and networks. Social Media + Society, 1(2), 2056305115610141." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR36" id="ref-link-section-d32563071e731">2015</a>). Studies also suggest that the use of SNS is positively related not only to the quality of friendships defined as satisfaction with friends, but also to social capital defined as the resources that accrue to an individual by virtue of his or her membership of a social network, e.g. the feeling of belonging to a community, the forming of new relationships, the feedback and support of other people in relation to various issues, and loneliness (Ahn, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Ahn, J. (2012). Teenagers’ experiences with social network sites: Relationships to bridging and bonding social capital. The Information Society, 28(2), 99–109.&nbsp;" href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR1" id="ref-link-section-d32563071e734">2012</a>; Antheunis et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Antheunis, M. L., Schouten, A. P., &amp; Krahmer, E. (2016). The role of social networking sites in early adolescents’ social lives. The Journal of Early Adolescence, 36(3), 348–371." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR3" id="ref-link-section-d32563071e737">2016</a>; Brandtzæg, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2012" title="Brandtzæg, P. B. (2012). Social networking sites: Their users and social implications A longitudinal study. Journal of Computer-Mediated Communication, 17, 467–488." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR11" id="ref-link-section-d32563071e740">2012</a>; Ellison et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2007" title="Ellison, N. B., Steinfield, C., &amp; Lampe, C. (2007). The benefits of Facebook “Friends:” Social capital and college students’ use of online social network sites. Journal of Computer-Mediated Communication, 12(4), 1143–1168." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR17" id="ref-link-section-d32563071e743">2007</a>). Other studies indicate that social media use with close friends is positively associated with the experience of feeling close to those friends (Kahlow et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2020" title="Kahlow, J. A., Coker, M. C., &amp; Richards, R. (2020). The multimodal nature of Snapchat in close relationships: Toward a social presence-based theoretical framework. Computers in Human Behavior, 111, 106409." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR31" id="ref-link-section-d32563071e747">2020</a>; Pouwels et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Pouwels, J. L., Valkenburg, P. M., Beyens, I., van Driel, I. I., &amp; Keijsers, L. (2021). Social media use and friendship closeness in adolescents’ daily lives: An experience sampling study. Developmental Psychology, 57(2), 309." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR52" id="ref-link-section-d32563071e750">2021</a>).</p><p>Democratically, it may be used to form and express opinions, to share information, and to engage in collective reasoning involving decision-makers at all levels of society, and thus, it may ultimately increase participation in democratic processes. A meta-analysis found that social media use generally is positively—but modestly—related to various forms of both online and offline political participation, e.g. voting, supporting campaigns, and protest activities (Skoric et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Skoric, M. M., Zhu, Q., Goh, D., &amp; Pang, N. (2016). Social media and citizen engagement: A meta-analytic review. New Media &amp; Society, 18(9), 1817–1839." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR59" id="ref-link-section-d32563071e756">2016</a>). Consistent with the findings of a previous meta-analysis (Boulianne, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2009" title="Boulianne, S. (2009). Does internet use affect engagement? A meta-analysis of research. Political Communication, 26(2), 193–211." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR9" id="ref-link-section-d32563071e759">2009</a>), it also found a positive—and moderate—relation between (1) the use of social media for informational purposes, i.e. seeking, gathering, and sharing of various kinds of information, and political participation, and (2) the use of social media for expressivist purposes, i.e. expressing oneself, articulating opinions, ideas, and thoughts, and political participation. Another meta-analysis similarly established a positive relationship between social media use and participation in political and civic life. However, the meta-analysis found it to be questionable whether such participation is a causal effect of social media use (Boulianne, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Boulianne, S. (2015). Social media use and participation: A meta-analysis of current research. Information, Communication &amp; Society, 18(5), 524–538." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR10" id="ref-link-section-d32563071e762">2015</a>). Other studies suggest that online viewing and sharing of news are positively related to political knowledge (Beam, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Beam, M. A. (2016). Clicking vs. sharing: The relationship between online news behaviors and political knowledge. Computers in Human Behavior, 59, 215–220." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR5" id="ref-link-section-d32563071e765">2016</a>).</p><p>Minimally, the evidence cited suggests that <i>certain kinds of</i> online data sharing can, <i>under certain conditions</i>, serve valuable social and democratic purposes. If so, then we have reason to protect—perhaps even promote—these kinds of online data sharing. However, this requires that privacy concerns be accommodated because such concerns may lead to withdrawal from online data sharing. A recent study found that privacy concerns defined as concerns about losing control of personal data on social networks are negatively related to social media participation measured as the frequency of interactions on SNS (Bright et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Bright, L. F., Lim, H. S., &amp; Logan, K. (2021). “Should I Post or Ghost?”: Examining how privacy concerns impact social media engagement in US consumers. Psychology &amp; Marketing, 38(10), 1712–1722." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR12" id="ref-link-section-d32563071e777">2021</a>). A similar study found that privacy concerns in relation to publicly available data are negatively associated with the amount and depth of personal data sharing on SNS (Gruzd &amp; Hernández-García, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2018" title="Gruzd, A., &amp; Hernández-García, Á. (2018). Privacy concerns and self-disclosure in private and public uses of social media. Cyberpsychology, Behavior, and Social Networking, 21(7), 418–428." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR25" id="ref-link-section-d32563071e780">2018</a>).</p><p>What has been attempted here is to underpin two simple claims, namely that online engagement in the form of data sharing <i>can</i> serve valuable social and democratic purposes and that privacy concerns <i>can</i> lead to disengagement from online data sharing. If both claims are true, they provide us with a reason to address the concerns of individuals regarding privacy. They imply that disengagement from online data sharing is an undesirable solution to such privacy concerns. The upshot for AI profiling based on publicly available data is this. If individuals sometime in the future, when AI predictive profiling has become more widespread, become aware that their publicly available data can be used for profiling in unpredictable ways, there is a risk that this may lead to disengagement from online data sharing, also by prospective politicians and decision-makers. This is undesirable as it may impede the full realization of the social and democratic benefits of data sharing.</p><p>While we have focused here on social and democratic aspects, there is also an extant literature on the health-related effects of social media usage. A recent umbrella review comparing five meta-analyses found that social media use is associated with higher levels of both well-being, i.e. happiness, positive affect, life satisfaction, and ill-being, i.e. i.e. depression, anxiety disorder, distress, and negative affect (Valkenburg et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Valkenburg, P. M., Meier, A., &amp; Beyens, I. (2022). Social media use and its impact on adolescent mental health: An umbrella review of the evidence. Current Opinion in Psychology, 44, 58–68." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR63" id="ref-link-section-d32563071e796">2022</a>). While such studies certainly add important nuances to the evidence on the effects of social media use, they do not rule the possibility that social media use may serve valuable social and democratic purposes. If privacy concerns—and in particular concerns related to AI profiling—can lead to disengagement from the relevant kinds of data sharing, there are substantial reasons for protecting such data sharing from AI profiling.</p><h3 id="Sec16"><span>5.2 </span>Other Trumping Concerns</h3><p>To fully substantiate the right not to be subjected to AI profiling based on publicly available, personal data, it remains to be considered whether other concerns may outweigh this right. These concerns may be related to specific purposes and contexts of AI profiling. Revisiting the three cases may therefore be helpful.</p><p>AI predictive profiling makes an individual significantly more exposed to unacceptable forms of social control and stigmatization and self-stigmatization, and it may lead to withdrawal from online data sharing otherwise conducive to social thriving and democratic health. As such, AI predictive profiling is a threat to basic human autonomy and wellbeing, as well as social life and democracy. These harms impose a burden of proof on the defender of the right to conduct such profiling in the three cases. For each case, the defender of AI profiling based on publicly available data must show either (1) that the harms of AI profiling are unlikely to obtain in the three cases and/or (2) that AI profiling without consent is a proportional measure, where this requires (A) that the benefits of doing AI profiling without consent outweigh the potential harms and (B) that AI profiling without consent is strictly necessary in order to obtain the benefits.</p><p>Consider the friends case. That household profiling should be harmless is not a credible proposition. There are no a priori reasons to think that profiling of friends and family members is not as likely to lead to overreaching social control and stigmatization as in any other case. Note that the effects of stigmatization do not require stigmatizing behaviour by friends and family. It only requires that a certain feature is stigmatized in the wider society. Consider now the benefits and the necessity of conducting the AI profiling. The benefit of the profiling is an accurate diagnosis that A may use to motivate B and B’s family to seek further health care assistance. Note that this benefit could likely have been achieved by less invasive alternative means. A could simply have presented B and B’s family with the suspicion of B having mental health issues. Note also that whether or not the AI profiling is necessary, it is still an open question, who should decide the weight of the benefits and harms? There are at least two reasons why B should decide the weight of the benefits and harms, i.e. that B should have a right to provide or deny informed consent to AI profiling based on publicly available data. Firstly, because the benefits and harms of the AI profiling are relative to B’s interests in getting the profiling data or maintaining privacy, B is ceteris paribus the best positioned to determine B’s interests and their weight. Secondly, because the potential harms to B—i.e. social control and stigmatization—for all we know are significant harms. They are ways of impacting others that run counter to fundamental values in our society, and it seems to be an equally fundamental principle that generally—if not always—when individuals are at risk of suffering significant harms, they should have the right to protect themselves against these harms.</p><p>In the public servant case, there is also the risk of overreaching social control and stigmatization. Psychiatric diagnoses are sticky labels that will form the way individuals are handled in the public system in the short- and long-term, and this carries a latent risk of producing overreaching social control. It may also lead to stigmatization and not least self-stigmatization. In this case, however, there are potential benefits both for the profiled individual and the public authorities. Thus, the diagnosis may not only come to benefit client B in terms of more adequate health care and social benefits, but it may also increase the efficiency of the social services by making more readily available personal health care information needed for an adequate distribution of social benefits. AI profiling based on publicly available data may be considered necessary in the sense of being a precondition of a maximally efficient public administration, but it is not necessary in the sense that there are no alternative ways of getting access to the information produced by the AI profiling. Thus, the benefits of such AI profiling for public administration and the wider society may ultimately be marginal. In any case, it seems as if such benefits are morally incomparable to the list of potential harms of AI profiling, including the potential harms to B. As argued above, the potential harms to client B speaks in favour of granting B the right to provide or deny informed consent to such profiling.</p><p>The prime minister case may for two interrelated reasons be taken to present a more fundamental problem for the attempt to ground a sui generis right not to be AI profiled. Thus, it may be argued that information about the mental health of the prime minister candidate is of public interest and that the public therefore have a right to this information. Relatedly, it may be argued that the AI profiling is covered by the right to freedom of expression. After all the concerned citizen is in this case profiling the candidate with the intent of voicing a concern over the fitness of the candidate for political office. Access to information of public interest and the right to free speech are undeniably instrumental to a flourishing democracy. However, what we have been arguing in this article is that AI profiling is not only a continuous threat to individual autonomy, wellbeing, and social life, but that it may also threaten democratic processes by potentially leading people to withdraw from the use of social media. Decision-makers, including the prime minister candidate, may withdraw from social media exchanges with the public, and in the longer run, the threat of being AI profiled for all sorts of dispositions may prevent people from engaging in politics altogether. Therefore, it remains to be shown how and why a right not to be AI profiled based on publicly available data limits freedom of expression and information in any profound sense, and why such limitation outweighs the negative effects of AI profiling on human autonomy and wellbeing as well as on social flourishing and democracy. Simply flagging the right to freedom of expression and information cannot reasonably be thought to do the job.</p><p>Relevant for the attempt to weigh benefits and harms in all three cases is the accuracy of the predictions of mental disorders AI model <i>Deepmood</i>. Thus, it may be asserted that a predictive accuracy, i.e. sensitivity and specificity, above that of health care professionals could and would increase the benefits to individuals and society in all three cases—and vice versa. In short, the accuracy of AI models should be a parameter for any attempt to weigh benefits and harms. The evidence cited in the opening lines of the article suggests that available models for predicting mental disorders perform better than unassisted physicians. A recent meta-analysis of the accuracy of AI diagnostic systems in the context of medical imaging and histopathology found the performance of deep-learning models to be equivalent to that of healthcare professionals (Liu et al., <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2019" title="Liu, X., Faes, L., Kale, A. U., Wagner, S. K., Fu, D. J., Bruynseels, A., Mahendiran, T., Moraes, G., Shamdas, M., Kern, C., Ledsam, J. R., Schmid, M. K., Balaskas, K., Topol, E. J., Bachmann, L. M., Keane, P. A., &amp; Denniston, A. K. (2019). A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: A systematic review and meta-analysis. The Lancet Digital Health, 1(6), e271–e297." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR40" id="ref-link-section-d32563071e826">2019</a>). Others have pointed out that the presumption of accuracy may not hold (Hofmann, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Hofmann, B. (2022). Too much, too mild, too early: Diagnosing the excessive expansion of diagnoses. International Journal of General Medicine, 15, 6441–6450." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR28" id="ref-link-section-d32563071e829">2022</a>). Two observations should be made here. Firstly, as argued in a previous section, there are reasons to believe that an increased accuracy will likely drive stronger attempts of social control and increased stigmatization. One may also hypothesize that increased accuracy will drive increased disengagement from online data sharing. Whether or not the benefits and harms will increase with an increased accuracy is ultimately to be settled empirically. For present purposes, it should simply be noted that an increased accuracy does not necessarily make a difference for the balancing of benefits and harms in the three examples. Secondly, as argued right above, there are strong reasons for believing that the weighing of benefits and harms should in many cases befall the individual. The weight of benefits and harms—including the chance of obtaining the benefits and suffering the harms (accuracy)—cannot be separated from the interests of individuals and not in cases where the individual may come to suffer significant harms.</p><p>In conclusion, the arguments presented here suggest that individuals should have a right not to be AI profiled in all three cases. While the right not to be AI profiled based on publicly available data admittedly is a pro tanto right, what is claimed here is that none of the three cases seem to qualify as exemptions from this right.</p></div></div><div id="Sec17-section" data-title="The GDPR and the Right not to be AI Profiled"><h2 id="Sec17"><span>6 </span>The GDPR and the Right not to be AI Profiled</h2><div id="Sec17-content"><p>For the overall purpose in this article of establishing the need for a sui generis legal right not to be AI profiled, it remains to be considered whether current legislation already incorporates such a right. Within the EU, privacy rights in relation to automated profiling and data use are covered in the EU General Data Protection Regulation (GDPR) (General Data Protection Regulation, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2016" title="Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation), OJ L 119, (2016). 
                  http://data.europa.eu/eli/reg/2016/679/oj
                  
                " href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR57" id="ref-link-section-d32563071e845">2016</a>). The GDPR contains no explicit provisions granting individuals a sui generis right not to be AI profiled. It may, however, entail a prohibition against this type of profiling and hence indirectly establish such a right in relation to the three cases.</p><h3 id="Sec18"><span>6.1 </span>Two Approaches to Deciding the Implications of the GDPR</h3><p>AI profiling for mental health disorders based on publicly available social media data is a type of data processing involving both personal data and special categories of personal data, i.e. health data. The GDPR contains a general prohibition against the processing of special categories of personal data such as health data, but lists several exemptions in article 9, including the use of special categories of personal data that have manifestly been made public by the data subject. Further processing of personal data must abide by the principles in article 5. In short, personal data must (a) be processed lawfully, fairly, and transparently; (b) be collected and used in relation to a specific purpose; (c) be limited to what is necessary for the specific purpose; (d) be accurate; (e) be anonymized or erased when the processing of identifiable data is no longer necessary; and (f) be processed securely. Article 6 lists lawful types of data processing and includes among these processings necessary to carry out a task of public interest or in the exercise of official authority. The fairness and transparency requirements are specified in articles 12 and 13. They may for present purposes be summarized as the requirement that the data subject is appropriately informed about the processing of personal data.</p><p>In the attempt to decide if the GDPR entails a right not to be AI profiled in the three cases, different approaches may be taken. One approach would be to analyze if the relevant examples of AI profiling can or cannot satisfy the aforementioned processing principles of the GDPR. If they cannot, the GDPR would effectively entail a right not to be AI profiled. Another approach—the one pursued here—would be to consider the scope of the principles and the extent to which the GDPR allows for them to be restricted. If the three cases fall outside the scope of the principles or they may be relevantly restricted, the GDPR would not ensure a right not to be AI profiled in the three cases.</p><h3 id="Sec19"><span>6.2 </span>The Three Cases and the Scope and Potential Restrictions of the GDPR Processing Principles</h3><p>The AI profiling in the friends case presumably falls outside the scope of the GDPR. According to article 2, the GDPR does not apply to the processing of personal data in relation to purely personal or household activities that have no connection to professional or commercial activities. In the friends case, the processing seems most adequately classified as a case of processing for the sake of personal and household activities. It clearly has no connection to professional or commercial activities. In consequence, the processing requirements do not apply to the friends case, and the mental health profiling would be wholly lawful.</p><p>Pertinent to the public servant case, the GDPR allows for national adaptions of the processing principles in article 5 (see above). Thus, article 6 stipulates that member states may introduce more specific provisions with respect to data processing necessary for the performance of a task carried out in the public interest or in the exercise of official authority. Article 23 allows member states to restrict legislatively the processing obligations in articles 12–22 if this is necessary for safeguarding other important objectives of the general public interest, including financial interests related to public health and social security. With reference to the preamble no. 10, article 6 and article 23, the Danish Parliament for instance in late 2021 adopted a legislative proposal permitting the national tax authorities to collect ‘blindly’ all types of personal data publicly available on social media platforms and to use these data for risk profiling of all citizens (Lov om ændring af lov om et indkomstregister, skatteindberetningsloven og skattekontrolloven, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Lov om ændring af lov om et indkomstregister, skatteindberetningsloven og skattekontrolloven, nr. L 73, 1 (2021). 
                  https://www.ft.dk/ripdf/samling/20211/lovforslag/l73/20211_l73_som_fremsat.pdf
                  
                " href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR41" id="ref-link-section-d32563071e869">2021</a>). The only requirement is that the authorities consider the data and profiling <i>necessary</i> for carrying out a task, e.g. the detection of taxation fraud. Analogously, the AI profiling in the public servant case may be considered necessary for carrying out a task of the social services and for maintaining an efficient public administration, and such profiling could reasonably be seen as protecting a financial interest related to social security. The conditions for restricting the processing principles are thus likely to be met in cases like the public servant case, and the GDPR therefore does not ensure individuals a right not to be AI profiled in such cases.</p><p>Finally, article 85 of the GDPR requires member states to reconcile the data protection regulation with the right to freedom of expression and information, where this includes the right to process data for journalistic purposes. As also stated in article 85, such processing may require that member states provide for exemptions and derogations from, among others, the processing principles in article 5. The GDPR thus allows for exemptions in cases like the prime minister case, where the mental health of the candidate arguably is of public interest. Hence, the GDPR does ensure political candidates a right not to be AI profiled in such cases.</p><h3 id="Sec20"><span>6.3 </span>Advantages of an Explicit Sui Generis Right not to be AI Profiled</h3><p>Without considering the implications of the processing principles for the three cases, it has nonetheless been argued here that the GDPR does not ensure a right not to be AI profiled in the three cases. But even if the processing principles could be shown to entail a right not to be AI profiled in the three cases and even if all of the above arguments suggesting that the processing principles may be restricted in relation to these cases fail, there may still be reasons for introducing an explicit sui generis right not to be AI profiled into AI legislation. First, the cases do not exhaust the space of the potential use of profiling, which may for instance also be used by employers for recruitment and selection processes and for various other forms of workforce management (Hunkenschroer &amp; Luetge, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Hunkenschroer, A. L., &amp; Luetge, C. (2022). Ethics of AI-enabled recruiting and selection: A review and research agenda. Journal of Business Ethics, 178, 977–1007." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR29" id="ref-link-section-d32563071e886">2022</a>; Kim &amp; Bodie, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2021" title="Kim, P., &amp; Bodie, M. T. (2021). Artificial intelligence and the challenges of workplace discrimination and privacy. 35 ABA Journal of Labor and Employment Law 289 Saint Louis U Legal Studies Research Paper No, 2021–26, 289–315." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR33" id="ref-link-section-d32563071e889">2021</a>). Second, the severity of the potential harms of AI profiling based on publicly available data suggest that in the future, where AI predictive profiling may have become publicly accessible, there is a need of legislation that is effective in regulating our profiling behaviour. One may venture that a rather simple right that unambiguously and unquestionably rules out most cases of AI profiling is more effective than a data protection regulation with negotiable implications and the possibility of special national adaptions. By introducing such a right, an important and complex legal question would a priori have been settled, and this arguably makes a difference for the regulatory effect of legislation.</p></div></div><div id="Sec21-section" data-title="Conclusion—Future Research"><h2 id="Sec21"><span>7 </span>Conclusion—Future Research</h2><div id="Sec21-content"><p>This article has substantiated the right not to be subjected to AI profiling based on publicly available data. This right entails that such profiling cannot be conducted without the explicit informed consent of the profiled subject. The line of argument pursued in this article raises several questions that warrant further discussion.</p><p>Why a right? Even if the gravity of the potential harms of AI profiling is acknowledged, it may still be considered an open question whether such harms should be addressed through a right or some other type of regulation prohibiting such profiling in one way or another. As was previously argued, the right not to be AI profiled without informed consent may be taken to follow from an individual’s right to protect their interests and their right to protect themselves against harm. It should also be noted, however, that a right of this kind so to speak ‘democratises’ regulation. It does not confine AI regulation to designated institutions and professions endowed with the competence and obligation to monitor and control AI development and use, but rather, it gives every individual an instrument for the control of AI use. In so doing, a right may increase the regulatory effectiveness over and above alternative types of regulation. This argument cannot be fully developed in this article but should be explored in future work.</p><p>Why a pro tanto right? From the outset of this article, the right not to be AI profiled has been suggested as a conditional right. Yet the potential exemptions from this right have not been specified. On the contrary, it has been argued that some of the obvious exemptions—e.g. freedom of expression and information—do not constitute reasonable exemptions. This question also warrants further work. The reason for making this choice here is twofold. We have in this article at no point argued that there are no exemptions to this right. Relatedly, it seems that it is always possible to design situations constituting exemptions to such rights. For instance, threats to national security or threats of terrorism could be candidates for making up such exemptions.</p><p>Why informed consent? The right not to be AI profiled can be waived through informed consent. Some would object that the provision of informed consent to data processing is highly routinized, i.e. provided as an unreflected, habitual act, and that the right not to be AI profiled as a consequence may lack any real power to protect individuals (Ploug &amp; Holm, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2013" title="Ploug, T., &amp; Holm, S. (2013). Informed consent and routinisation. Journal of Medical Ethics, 39(4), 214–218." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR49" id="ref-link-section-d32563071e910">2013</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2015" title="Ploug, T., &amp; Holm, S. (2015). Routinisation of informed consent in online health care systems. International Journal of Medical Informatics, 84(4), 229–236." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR50" id="ref-link-section-d32563071e913">2015</a>). The extent to which the problems of routinization may be overcome through careful design of the consent situation is a matter for future (empirical) work. The position taken in this article is that the weighing of the potential benefits and harms of AI profiling cannot be done ethically adequate without the involvement of the individuals potentially being profiled. The legitimacy of subjecting individuals to AI profiling at least partly derives from their involvement in the decision to do the profiling. As such informed consent is simply ethically necessary. The requirement of informed consent may not be sufficient to protect individuals from the potential harms of AI profiling. Other types of regulation protecting individuals from such harms may certainly be needed. Not least—as some have argued—because the requirement of informed consent may be a burden to individuals (Vredenburgh, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2022" title="Vredenburgh, K. (2022). The right to explanation*. Journal of Political Philosophy, 30(2), 209–229." href="https://link.springer.com/article/10.1007/s13347-023-00616-9#ref-CR65" id="ref-link-section-d32563071e916">2022</a>).</p><p>Finally, in the course of substantiating a sui generis right not to be AI profiled based on publicly available data, we have suggested several behavioural mechanisms and patterns that underlie social control and stigmatization and provided relevant evidence to the best of our ability. There is a need for further empirical studies on these matters. Note, however, that in the attempt to build our arguments around actual behavioural patterns and mechanism, this article constitutes a contribution to evidence-based policymaking in the field of privacy rights.</p></div></div>
                                </div>
                        
                    

                    <div id="data-availability-section" data-title="Data Availability"><h2 id="data-availability">Data Availability</h2><p>Not applicable.</p></div><div id="abbreviations-section" data-title="Abbreviations"><h2 id="abbreviations">Abbreviations</h2><div id="abbreviations-content"><dl><dt><dfn>
                    <i>AI</i>
                  :</dfn></dt><dd>
                    <p>Artificial intelligence</p>
                  </dd><dt><dfn>
                    <i>GDPR</i>
                  :</dfn></dt><dd>
                    <p>EU General Data Protection Regulation</p>
                  </dd><dt><dfn>
                    <i>EU</i>
                  :</dfn></dt><dd>
                    <p>European Union</p>
                  </dd><dt><dfn>
                    <i>SNS</i>
                  :</dfn></dt><dd>
                    <p>Social networking sites</p>
                  </dd></dl></div></div><div id="MagazineFulltextArticleBodySuffix" aria-labelledby="Bib1" data-title="References"><h2 id="Bib1">References</h2><div data-container-section="references" id="Bib1-content"><ul data-track-component="outbound reference" data-track-context="references section"><li><p id="ref-CR1">Ahn, J. (2012). Teenagers’ experiences with social network sites: Relationships to bridging and bonding social capital. <i>The Information Society,</i> <i>28</i>(2), 99–109.&nbsp;</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1080/01972243.2011.649394" data-track-item_id="10.1080/01972243.2011.649394" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F01972243.2011.649394" aria-label="Article reference 1" data-doi="10.1080/01972243.2011.649394">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Teenagers%E2%80%99%20experiences%20with%20social%20network%20sites%3A%20Relationships%20to%20bridging%20and%20bonding%20social%20capital&amp;journal=The%20Information%20Society&amp;doi=10.1080%2F01972243.2011.649394&amp;volume=28&amp;issue=2&amp;pages=99-109&amp;publication_year=2012&amp;author=Ahn%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR2">Amini, H., Mohammadi, E., &amp; Kosseim, L. (2022). Quick and (maybe not so) easy detection of anorexia in social media: To explainability and beyond, 141–158. In F. Crestani, D. E. Losada, &amp; J. Parapar (Eds), <i>Early Detection of Mental Health Disorders by Social Media Monitoring: The First Five Years of the eRisk Project&nbsp;</i>. Springer International Publishing.</p></li><li><p id="ref-CR3">Antheunis, M. L., Schouten, A. P., &amp; Krahmer, E. (2016). The role of social networking sites in early adolescents’ social lives. <i>The Journal of Early Adolescence,</i> <i>36</i>(3), 348–371.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1177/0272431614564060" data-track-item_id="10.1177/0272431614564060" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F0272431614564060" aria-label="Article reference 3" data-doi="10.1177/0272431614564060">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20social%20networking%20sites%20in%20early%20adolescents%E2%80%99%20social%20lives&amp;journal=The%20Journal%20of%20Early%20Adolescence&amp;doi=10.1177%2F0272431614564060&amp;volume=36&amp;issue=3&amp;pages=348-371&amp;publication_year=2016&amp;author=Antheunis%2CML&amp;author=Schouten%2CAP&amp;author=Krahmer%2CE">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR4">Appel, M., &amp; Weber, S. (2021). Do mass mediated stereotypes harm members of negatively stereotyped groups? A meta-analytical review on media-generated stereotype threat and stereotype lift. <i>Communication Research,</i> <i>48</i>(2), 151–179.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1177/0093650217715543" data-track-item_id="10.1177/0093650217715543" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F0093650217715543" aria-label="Article reference 4" data-doi="10.1177/0093650217715543">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 4" href="http://scholar.google.com/scholar_lookup?&amp;title=Do%20mass%20mediated%20stereotypes%20harm%20members%20of%20negatively%20stereotyped%20groups%3F%20A%20meta-analytical%20review%20on%20media-generated%20stereotype%20threat%20and%20stereotype%20lift&amp;journal=Communication%20Research&amp;doi=10.1177%2F0093650217715543&amp;volume=48&amp;issue=2&amp;pages=151-179&amp;publication_year=2021&amp;author=Appel%2CM&amp;author=Weber%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR5">Beam, M. A. (2016). Clicking vs. sharing: The relationship between online news behaviors and political knowledge. <i>Computers in Human Behavior</i>, 59, 215–220.</p></li><li><p id="ref-CR6">Beauchamp, T. L., &amp; Childress, J. F. (2001). <i>Principles of biomedical ethics</i> (5th ed). Oxford University Press, New York.</p></li><li><p id="ref-CR7">Bhugra, D. (1989). Attitudes towards mental illness. <i>Acta Psychiatrica Scandinavica,</i> <i>80</i>(1), 1–12.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1111/j.1600-0447.1989.tb01293.x" data-track-item_id="10.1111/j.1600-0447.1989.tb01293.x" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.1600-0447.1989.tb01293.x" aria-label="Article reference 7" data-doi="10.1111/j.1600-0447.1989.tb01293.x">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 7" href="http://scholar.google.com/scholar_lookup?&amp;title=Attitudes%20towards%20mental%20illness&amp;journal=Acta%20Psychiatrica%20Scandinavica&amp;doi=10.1111%2Fj.1600-0447.1989.tb01293.x&amp;volume=80&amp;issue=1&amp;pages=1-12&amp;publication_year=1989&amp;author=Bhugra%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR8">Bishop, R. C. (2003). On separating predictability and determinism. <i>Erkenntnis,</i> <i>58</i>, 169–188.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1023/A:1022003805614" data-track-item_id="10.1023/A:1022003805614" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1023%2FA%3A1022003805614" aria-label="Article reference 8" data-doi="10.1023/A:1022003805614">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=On%20separating%20predictability%20and%20determinism&amp;journal=Erkenntnis&amp;doi=10.1023%2FA%3A1022003805614&amp;volume=58&amp;pages=169-188&amp;publication_year=2003&amp;author=Bishop%2CRC">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR9">Boulianne, S. (2009). Does internet use affect engagement? A meta-analysis of research. <i>Political Communication,</i> <i>26</i>(2), 193–211.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1080/10584600902854363" data-track-item_id="10.1080/10584600902854363" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F10584600902854363" aria-label="Article reference 9" data-doi="10.1080/10584600902854363">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 9" href="http://scholar.google.com/scholar_lookup?&amp;title=Does%20internet%20use%20affect%20engagement%3F%20A%20meta-analysis%20of%20research&amp;journal=Political%20Communication&amp;doi=10.1080%2F10584600902854363&amp;volume=26&amp;issue=2&amp;pages=193-211&amp;publication_year=2009&amp;author=Boulianne%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR10">Boulianne, S. (2015). Social media use and participation: A meta-analysis of current research. <i>Information, Communication &amp; Society,</i> <i>18</i>(5), 524–538.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1080/1369118X.2015.1008542" data-track-item_id="10.1080/1369118X.2015.1008542" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1080%2F1369118X.2015.1008542" aria-label="Article reference 10" data-doi="10.1080/1369118X.2015.1008542">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=Social%20media%20use%20and%20participation%3A%20A%20meta-analysis%20of%20current%20research&amp;journal=Information%2C%20Communication%20%26%20Society&amp;doi=10.1080%2F1369118X.2015.1008542&amp;volume=18&amp;issue=5&amp;pages=524-538&amp;publication_year=2015&amp;author=Boulianne%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR11">Brandtzæg, P. B. (2012). Social networking sites: Their users and social implications A longitudinal study. <i>Journal of Computer-Mediated Communication,</i> <i>17</i>, 467–488.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1111/j.1083-6101.2012.01580.x" data-track-item_id="10.1111/j.1083-6101.2012.01580.x" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.1083-6101.2012.01580.x" aria-label="Article reference 11" data-doi="10.1111/j.1083-6101.2012.01580.x">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 11" href="http://scholar.google.com/scholar_lookup?&amp;title=Social%20networking%20sites%3A%20Their%20users%20and%20social%20implications%20A%20longitudinal%20study&amp;journal=Journal%20of%20Computer-Mediated%20Communication&amp;doi=10.1111%2Fj.1083-6101.2012.01580.x&amp;volume=17&amp;pages=467-488&amp;publication_year=2012&amp;author=Brandtz%C3%A6g%2CPB">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR12">Bright, L. F., Lim, H. S., &amp; Logan, K. (2021). “Should I Post or Ghost?”: Examining how privacy concerns impact social media engagement in US consumers. <i>Psychology &amp; Marketing,</i> <i>38</i>(10), 1712–1722.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1002/mar.21499" data-track-item_id="10.1002/mar.21499" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1002%2Fmar.21499" aria-label="Article reference 12" data-doi="10.1002/mar.21499">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 12" href="http://scholar.google.com/scholar_lookup?&amp;title=%E2%80%9CShould%20I%20Post%20or%20Ghost%3F%E2%80%9D%3A%20Examining%20how%20privacy%20concerns%20impact%20social%20media%20engagement%20in%20US%20consumers&amp;journal=Psychology%20%26%20Marketing&amp;doi=10.1002%2Fmar.21499&amp;volume=38&amp;issue=10&amp;pages=1712-1722&amp;publication_year=2021&amp;author=Bright%2CLF&amp;author=Lim%2CHS&amp;author=Logan%2CK">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR13">Coppersmith, G., Leary, R., Crutchley, P., &amp; Fine, A. (2018). Natural language processing of social media as screening for suicide risk. <i>Biomedical Informatics Insights,</i> <i>10</i>, 1–11.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1177/1178222618792860" data-track-item_id="10.1177/1178222618792860" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F1178222618792860" aria-label="Article reference 13" data-doi="10.1177/1178222618792860">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=Natural%20language%20processing%20of%20social%20media%20as%20screening%20for%20suicide%20risk&amp;journal=Biomedical%20Informatics%20Insights&amp;doi=10.1177%2F1178222618792860&amp;volume=10&amp;pages=1-11&amp;publication_year=2018&amp;author=Coppersmith%2CG&amp;author=Leary%2CR&amp;author=Crutchley%2CP&amp;author=Fine%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR14">Corrigan, P. W., Larson, J. E., &amp; Rüsch, N. (2009). Self-stigma and the “why try” effect: Impact on life goals and evidence-based practices. <i>World Psychiatry,</i> <i>8</i>(2), 75–81.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1002/j.2051-5545.2009.tb00218.x" data-track-item_id="10.1002/j.2051-5545.2009.tb00218.x" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1002%2Fj.2051-5545.2009.tb00218.x" aria-label="Article reference 14" data-doi="10.1002/j.2051-5545.2009.tb00218.x">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 14" href="http://scholar.google.com/scholar_lookup?&amp;title=Self-stigma%20and%20the%20%E2%80%9Cwhy%20try%E2%80%9D%20effect%3A%20Impact%20on%20life%20goals%20and%20evidence-based%20practices&amp;journal=World%20Psychiatry&amp;doi=10.1002%2Fj.2051-5545.2009.tb00218.x&amp;volume=8&amp;issue=2&amp;pages=75-81&amp;publication_year=2009&amp;author=Corrigan%2CPW&amp;author=Larson%2CJE&amp;author=R%C3%BCsch%2CN">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR15">Crandall, C. S., &amp; Coleman, R. (1992). Aids-related stigmatization and the disruption of social relationships. <i>Journal of Social and Personal Relationships,</i> <i>9</i>(2), 163–177.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1177/0265407592092001" data-track-item_id="10.1177/0265407592092001" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F0265407592092001" aria-label="Article reference 15" data-doi="10.1177/0265407592092001">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 15" href="http://scholar.google.com/scholar_lookup?&amp;title=Aids-related%20stigmatization%20and%20the%20disruption%20of%20social%20relationships&amp;journal=Journal%20of%20Social%20and%20Personal%20Relationships&amp;doi=10.1177%2F0265407592092001&amp;volume=9&amp;issue=2&amp;pages=163-177&amp;publication_year=1992&amp;author=Crandall%2CCS&amp;author=Coleman%2CR">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR16">Druss, B. G., Zhao, L., Von Esenwein, S., Morrato, E. H., &amp; Marcus, S. C. (2011). Understanding excess mortality in persons with mental illness: 17-year follow up of a nationally representative US survey. <i>Medical Care,</i> <i>49</i>(6), 599–604. JSTOR.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1097/MLR.0b013e31820bf86e" data-track-item_id="10.1097/MLR.0b013e31820bf86e" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1097%2FMLR.0b013e31820bf86e" aria-label="Article reference 16" data-doi="10.1097/MLR.0b013e31820bf86e">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 16" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20excess%20mortality%20in%20persons%20with%20mental%20illness%3A%2017-year%20follow%20up%20of%20a%20nationally%20representative%20US%20survey&amp;journal=Medical%20Care&amp;doi=10.1097%2FMLR.0b013e31820bf86e&amp;volume=49&amp;issue=6&amp;pages=599-604&amp;publication_year=2011&amp;author=Druss%2CBG&amp;author=Zhao%2CL&amp;author=Esenwein%2CS&amp;author=Morrato%2CEH&amp;author=Marcus%2CSC">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR17">Ellison, N. B., Steinfield, C., &amp; Lampe, C. (2007). The benefits of Facebook “Friends:” Social capital and college students’ use of online social network sites. <i>Journal of Computer-Mediated Communication,</i> <i>12</i>(4), 1143–1168.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1111/j.1083-6101.2007.00367.x" data-track-item_id="10.1111/j.1083-6101.2007.00367.x" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1111%2Fj.1083-6101.2007.00367.x" aria-label="Article reference 17" data-doi="10.1111/j.1083-6101.2007.00367.x">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 17" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20benefits%20of%20Facebook%20%E2%80%9CFriends%3A%E2%80%9D%20Social%20capital%20and%20college%20students%E2%80%99%20use%20of%20online%20social%20network%20sites&amp;journal=Journal%20of%20Computer-Mediated%20Communication&amp;doi=10.1111%2Fj.1083-6101.2007.00367.x&amp;volume=12&amp;issue=4&amp;pages=1143-1168&amp;publication_year=2007&amp;author=Ellison%2CNB&amp;author=Steinfield%2CC&amp;author=Lampe%2CC">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR18">Evans, J. P., &amp; Burke, W. (2008). Genetic exceptionalism. Too much of a good thing? <i>Genetics in Medicine,</i> <i>10</i>(7), Art. 7.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1097/GIM.0b013e31817f280a" data-track-item_id="10.1097/GIM.0b013e31817f280a" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1097%2FGIM.0b013e31817f280a" aria-label="Article reference 18" data-doi="10.1097/GIM.0b013e31817f280a">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Genetic%20exceptionalism.%20Too%20much%20of%20a%20good%20thing%3F&amp;journal=Genetics%20in%20Medicine&amp;doi=10.1097%2FGIM.0b013e31817f280a&amp;volume=10&amp;issue=7&amp;publication_year=2008&amp;author=Evans%2CJP&amp;author=Burke%2CW">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR19">Feinberg, J. (1980). The child’s right to an open future, 76-97.  In Feinberg, J. (1992). <i>Freedom and Fulfillment: Philosophical Essays</i>. Princeton University Press, New Jersey.</p></li><li><p id="ref-CR20">Frederick, D. (2014). Pro-tanto versus absolute rights. <i>The Philosophical Forum,</i> <i>45</i>(4), 375–394.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1111/phil.12044" data-track-item_id="10.1111/phil.12044" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1111%2Fphil.12044" aria-label="Article reference 20" data-doi="10.1111/phil.12044">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 20" href="http://scholar.google.com/scholar_lookup?&amp;title=Pro-tanto%20versus%20absolute%20rights&amp;journal=The%20Philosophical%20Forum&amp;doi=10.1111%2Fphil.12044&amp;volume=45&amp;issue=4&amp;pages=375-394&amp;publication_year=2014&amp;author=Frederick%2CD">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR21">Garrett, J. R., Lantos, J. D., Biesecker, L. G., Childerhose, J. E., Chung, W. K., Holm, I. A., Koenig, B. A., McEwen, J. E., Wilfond, B. S., &amp; Brothers, K. (2019). Rethinking the “open future” argument against predictive genetic testing of children. <i>Genetics in Medicine,</i> <i>21</i>(10), 2190–2198.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1038/s41436-019-0483-4" data-track-item_id="10.1038/s41436-019-0483-4" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41436-019-0483-4" aria-label="Article reference 21" data-doi="10.1038/s41436-019-0483-4">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Rethinking%20the%20%E2%80%9Copen%20future%E2%80%9D%20argument%20against%20predictive%20genetic%20testing%20of%20children&amp;journal=Genetics%20in%20Medicine&amp;doi=10.1038%2Fs41436-019-0483-4&amp;volume=21&amp;issue=10&amp;pages=2190-2198&amp;publication_year=2019&amp;author=Garrett%2CJR&amp;author=Lantos%2CJD&amp;author=Biesecker%2CLG&amp;author=Childerhose%2CJE&amp;author=Chung%2CWK&amp;author=Holm%2CIA&amp;author=Koenig%2CBA&amp;author=McEwen%2CJE&amp;author=Wilfond%2CBS&amp;author=Brothers%2CK">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR22">Gkotsis, G., Oellrich, A., Velupillai, S., Liakata, M., Hubbard, T. J. P., Dobson, R. J. B., &amp; Dutta, R. (2017). Characterisation of mental health conditions in social media using Informed Deep Learning. <i>Scientific Reports,</i> <i>7</i>(1), Art. 1.</p><p><a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 22" href="http://scholar.google.com/scholar_lookup?&amp;title=Characterisation%20of%20mental%20health%20conditions%20in%20social%20media%20using%20Informed%20Deep%20Learning&amp;journal=Scientific%20Reports&amp;volume=7&amp;issue=1&amp;publication_year=2017&amp;author=Gkotsis%2CG&amp;author=Oellrich%2CA&amp;author=Velupillai%2CS&amp;author=Liakata%2CM&amp;author=Hubbard%2CTJP&amp;author=Dobson%2CRJB&amp;author=Dutta%2CR">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR23">Goffman, E. (1963). <i>Stigma: Notes on the management of spoiled identity.</i> Simon &amp; Schuster, New York.</p></li><li><p id="ref-CR24">Goldstein, J. (1991). The stigmatization of smokers: An empirical investigation. <i>Journal of Drug Education,</i> <i>21</i>(2), 167–182.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.2190/Y71P-KXVJ-LR9H-H1MG" data-track-item_id="10.2190/Y71P-KXVJ-LR9H-H1MG" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.2190%2FY71P-KXVJ-LR9H-H1MG" aria-label="Article reference 24" data-doi="10.2190/Y71P-KXVJ-LR9H-H1MG">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20stigmatization%20of%20smokers%3A%20An%20empirical%20investigation&amp;journal=Journal%20of%20Drug%20Education&amp;doi=10.2190%2FY71P-KXVJ-LR9H-H1MG&amp;volume=21&amp;issue=2&amp;pages=167-182&amp;publication_year=1991&amp;author=Goldstein%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR25">Gruzd, A., &amp; Hernández-García, Á. (2018). Privacy concerns and self-disclosure in private and public uses of social media. <i>Cyberpsychology, Behavior, and Social Networking,</i> <i>21</i>(7), 418–428.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1089/cyber.2017.0709" data-track-item_id="10.1089/cyber.2017.0709" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1089%2Fcyber.2017.0709" aria-label="Article reference 25" data-doi="10.1089/cyber.2017.0709">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 25" href="http://scholar.google.com/scholar_lookup?&amp;title=Privacy%20concerns%20and%20self-disclosure%20in%20private%20and%20public%20uses%20of%20social%20media&amp;journal=Cyberpsychology%2C%20Behavior%2C%20and%20Social%20Networking&amp;doi=10.1089%2Fcyber.2017.0709&amp;volume=21&amp;issue=7&amp;pages=418-428&amp;publication_year=2018&amp;author=Gruzd%2CA&amp;author=Hern%C3%A1ndez-Garc%C3%ADa%2C%C3%81">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR26">Herek, G. M. (1999). AIDS and stigma. <i>American Behavioral Scientist,</i> <i>42</i>(7), 1106–1116.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1177/0002764299042007004" data-track-item_id="10.1177/0002764299042007004" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F0002764299042007004" aria-label="Article reference 26" data-doi="10.1177/0002764299042007004">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=AIDS%20and%20stigma&amp;journal=American%20Behavioral%20Scientist&amp;doi=10.1177%2F0002764299042007004&amp;volume=42&amp;issue=7&amp;pages=1106-1116&amp;publication_year=1999&amp;author=Herek%2CGM">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR27">Hilbert, A., Rief, W., &amp; Braehler, E. (2008). Stigmatizing attitudes toward obesity in a representative population-based sample. <i>Obesity,</i> <i>16</i>(7), 1529–1534.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1038/oby.2008.263" data-track-item_id="10.1038/oby.2008.263" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Foby.2008.263" aria-label="Article reference 27" data-doi="10.1038/oby.2008.263">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 27" href="http://scholar.google.com/scholar_lookup?&amp;title=Stigmatizing%20attitudes%20toward%20obesity%20in%20a%20representative%20population-based%20sample&amp;journal=Obesity&amp;doi=10.1038%2Foby.2008.263&amp;volume=16&amp;issue=7&amp;pages=1529-1534&amp;publication_year=2008&amp;author=Hilbert%2CA&amp;author=Rief%2CW&amp;author=Braehler%2CE">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR28">Hofmann, B. (2022). Too much, too mild, too early: Diagnosing the excessive expansion of diagnoses. <i>International Journal of General Medicine,</i> <i>15</i>, 6441–6450.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.2147/IJGM.S368541" data-track-item_id="10.2147/IJGM.S368541" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.2147%2FIJGM.S368541" aria-label="Article reference 28" data-doi="10.2147/IJGM.S368541">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 28" href="http://scholar.google.com/scholar_lookup?&amp;title=Too%20much%2C%20too%20mild%2C%20too%20early%3A%20Diagnosing%20the%20excessive%20expansion%20of%20diagnoses&amp;journal=International%20Journal%20of%20General%20Medicine&amp;doi=10.2147%2FIJGM.S368541&amp;volume=15&amp;pages=6441-6450&amp;publication_year=2022&amp;author=Hofmann%2CB">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR29">Hunkenschroer, A. L., &amp; Luetge, C. (2022). Ethics of AI-enabled recruiting and selection: A review and research agenda. <i>Journal of Business Ethics,</i> <i>178</i>, 977–1007.</p><p><a data-track="click||click_references" rel="noopener" data-track-label="10.1007/s10551-022-05049-6" data-track-item_id="10.1007/s10551-022-05049-6" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10551-022-05049-6" aria-label="Article reference 29" data-doi="10.1007/s10551-022-05049-6">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Ethics%20of%20AI-enabled%20recruiting%20and%20selection%3A%20A%20review%20and%20research%20agenda&amp;journal=Journal%20of%20Business%20Ethics&amp;doi=10.1007%2Fs10551-022-05049-6&amp;volume=178&amp;pages=977-1007&amp;publication_year=2022&amp;author=Hunkenschroer%2CAL&amp;author=Luetge%2CC">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR30">Jobin, A., Ienca, M., &amp; Vayena, E. (2019). The global landscape of AI ethics guidelines. <i>Nature Machine Intelligence,</i> <i>1</i>(9), 389–399.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1038/s42256-019-0088-2" data-track-item_id="10.1038/s42256-019-0088-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs42256-019-0088-2" aria-label="Article reference 30" data-doi="10.1038/s42256-019-0088-2">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 30" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20global%20landscape%20of%20AI%20ethics%20guidelines&amp;journal=Nature%20Machine%20Intelligence&amp;doi=10.1038%2Fs42256-019-0088-2&amp;volume=1&amp;issue=9&amp;pages=389-399&amp;publication_year=2019&amp;author=Jobin%2CA&amp;author=Ienca%2CM&amp;author=Vayena%2CE">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR31">Kahlow, J. A., Coker, M. C., &amp; Richards, R. (2020). The multimodal nature of Snapchat in close relationships: Toward a social presence-based theoretical framework. <i>Computers in Human Behavior,</i> <i>111</i>, 106409.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1016/j.chb.2020.106409" data-track-item_id="10.1016/j.chb.2020.106409" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.chb.2020.106409" aria-label="Article reference 31" data-doi="10.1016/j.chb.2020.106409">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 31" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20multimodal%20nature%20of%20Snapchat%20in%20close%20relationships%3A%20Toward%20a%20social%20presence-based%20theoretical%20framework&amp;journal=Computers%20in%20Human%20Behavior&amp;doi=10.1016%2Fj.chb.2020.106409&amp;volume=111&amp;publication_year=2020&amp;author=Kahlow%2CJA&amp;author=Coker%2CMC&amp;author=Richards%2CR">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR32">Keller, J. (2005). In genes we trust: The biological component of psychological essentialism and its relationship to mechanisms of motivated social cognition. <i>Journal of Personality and Social Psychology,</i> <i>88</i>(4), 686–702.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1037/0022-3514.88.4.686" data-track-item_id="10.1037/0022-3514.88.4.686" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0022-3514.88.4.686" aria-label="Article reference 32" data-doi="10.1037/0022-3514.88.4.686">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 32" href="http://scholar.google.com/scholar_lookup?&amp;title=In%20genes%20we%20trust%3A%20The%20biological%20component%20of%20psychological%20essentialism%20and%20its%20relationship%20to%20mechanisms%20of%20motivated%20social%20cognition&amp;journal=Journal%20of%20Personality%20and%20Social%20Psychology&amp;doi=10.1037%2F0022-3514.88.4.686&amp;volume=88&amp;issue=4&amp;pages=686-702&amp;publication_year=2005&amp;author=Keller%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR33">Kim, P., &amp; Bodie, M. T. (2021). Artificial intelligence and the challenges of workplace discrimination and privacy. <i>35 ABA Journal of Labor and Employment Law 289 Saint Louis U Legal Studies Research Paper No,</i> <i>2021–26</i>, 289–315.</p><p><a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 33" href="http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20intelligence%20and%20the%20challenges%20of%20workplace%20discrimination%20and%20privacy&amp;journal=35%20ABA%20Journal%20of%20Labor%20and%20Employment%20Law%20289%20Saint%20Louis%20U%20Legal%20Studies%20Research%20Paper%20No&amp;volume=2021%E2%80%9326&amp;pages=289-315&amp;publication_year=2021&amp;author=Kim%2CP&amp;author=Bodie%2CMT">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR34">Kim, J., Lee, J., Park, E., &amp; Han, J. (2020). A deep learning model for detecting mental illness from user content on social media. <i>Scientific Reports,</i> <i>10</i>(1), Art. 1.</p><p><a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 34" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20deep%20learning%20model%20for%20detecting%20mental%20illness%20from%20user%20content%20on%20social%20media&amp;journal=Scientific%20Reports&amp;volume=10&amp;issue=1&amp;publication_year=2020&amp;author=Kim%2CJ&amp;author=Lee%2CJ&amp;author=Park%2CE&amp;author=Han%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR35">Kumar, A., Sharma, A., &amp; Arora, A. (2019). Anxious depression prediction in real-time social data. <i>Proceeding of International Conference on Advanced Engineering, Science, Management and Technology,</i> 1–7.</p></li><li><p id="ref-CR36">Kümpel, A. S., Karnowski, V., &amp; Keyling, T. (2015). News sharing in social media: A review of current research on news sharing users, content, and networks. <i>Social Media + Society,</i> <i>1</i>(2), 2056305115610141.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1177/2056305115610141" data-track-item_id="10.1177/2056305115610141" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F2056305115610141" aria-label="Article reference 36" data-doi="10.1177/2056305115610141">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 36" href="http://scholar.google.com/scholar_lookup?&amp;title=News%20sharing%20in%20social%20media%3A%20A%20review%20of%20current%20research%20on%20news%20sharing%20users%2C%20content%2C%20and%20networks&amp;journal=Social%20Media%20%2B%20Society&amp;doi=10.1177%2F2056305115610141&amp;volume=1&amp;issue=2&amp;publication_year=2015&amp;author=K%C3%BCmpel%2CAS&amp;author=Karnowski%2CV&amp;author=Keyling%2CT">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR37">Leino-Kilpi, H., Välimäki, M., Dassen, T., Gasull, M., Lemonidou, C., Scott, A., &amp; Arndt, M. (2001). Privacy: A review of the literature. <i>International Journal of Nursing Studies,</i> <i>38</i>(6), 663–671.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1016/S0020-7489(00)00111-5" data-track-item_id="10.1016/S0020-7489(00)00111-5" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS0020-7489%2800%2900111-5" aria-label="Article reference 37" data-doi="10.1016/S0020-7489(00)00111-5">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 37" href="http://scholar.google.com/scholar_lookup?&amp;title=Privacy%3A%20A%20review%20of%20the%20literature&amp;journal=International%20Journal%20of%20Nursing%20Studies&amp;doi=10.1016%2FS0020-7489%2800%2900111-5&amp;volume=38&amp;issue=6&amp;pages=663-671&amp;publication_year=2001&amp;author=Leino-Kilpi%2CH&amp;author=V%C3%A4lim%C3%A4ki%2CM&amp;author=Dassen%2CT&amp;author=Gasull%2CM&amp;author=Lemonidou%2CC&amp;author=Scott%2CA&amp;author=Arndt%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR38">Link, B. G. (1987). Understanding labeling effects in the area of mental disorders: An assessment of the effects of expectations of rejection. <i>American Sociological Review,</i> <i>52</i>(1), 96–112. JSTOR.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.2307/2095395" data-track-item_id="10.2307/2095395" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.2307%2F2095395" aria-label="Article reference 38" data-doi="10.2307/2095395">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 38" href="http://scholar.google.com/scholar_lookup?&amp;title=Understanding%20labeling%20effects%20in%20the%20area%20of%20mental%20disorders%3A%20An%20assessment%20of%20the%20effects%20of%20expectations%20of%20rejection&amp;journal=American%20Sociological%20Review&amp;doi=10.2307%2F2095395&amp;volume=52&amp;issue=1&amp;pages=96-112&amp;publication_year=1987&amp;author=Link%2CBG">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR39">Link, B. G., &amp; Phelan, J. C. (2001). Conceptualizing stigma. <i>Annual Review of Sociology</i>, 27, 363–385.</p></li><li><p id="ref-CR40">Liu, X., Faes, L., Kale, A. U., Wagner, S. K., Fu, D. J., Bruynseels, A., Mahendiran, T., Moraes, G., Shamdas, M., Kern, C., Ledsam, J. R., Schmid, M. K., Balaskas, K., Topol, E. J., Bachmann, L. M., Keane, P. A., &amp; Denniston, A. K. (2019). A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: A systematic review and meta-analysis. <i>The Lancet Digital Health,</i> <i>1</i>(6), e271–e297.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1016/S2589-7500(19)30123-2" data-track-item_id="10.1016/S2589-7500(19)30123-2" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2FS2589-7500%2819%2930123-2" aria-label="Article reference 40" data-doi="10.1016/S2589-7500(19)30123-2">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 40" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparison%20of%20deep%20learning%20performance%20against%20health-care%20professionals%20in%20detecting%20diseases%20from%20medical%20imaging%3A%20A%20systematic%20review%20and%20meta-analysis&amp;journal=The%20Lancet%20Digital%20Health&amp;doi=10.1016%2FS2589-7500%2819%2930123-2&amp;volume=1&amp;issue=6&amp;pages=e271-e297&amp;publication_year=2019&amp;author=Liu%2CX&amp;author=Faes%2CL&amp;author=Kale%2CAU&amp;author=Wagner%2CSK&amp;author=Fu%2CDJ&amp;author=Bruynseels%2CA&amp;author=Mahendiran%2CT&amp;author=Moraes%2CG&amp;author=Shamdas%2CM&amp;author=Kern%2CC&amp;author=Ledsam%2CJR&amp;author=Schmid%2CMK&amp;author=Balaskas%2CK&amp;author=Topol%2CEJ&amp;author=Bachmann%2CLM&amp;author=Keane%2CPA&amp;author=Denniston%2CAK">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR41">Lov om ændring af lov om et indkomstregister, skatteindberetningsloven og skattekontrolloven, nr. L 73, 1 (2021). <a href="https://www.ft.dk/ripdf/samling/20211/lovforslag/l73/20211_l73_som_fremsat.pdf" data-track="click" data-track-action="external reference" data-track-label="https://www.ft.dk/ripdf/samling/20211/lovforslag/l73/20211_l73_som_fremsat.pdf">https://www.ft.dk/ripdf/samling/20211/lovforslag/l73/20211_l73_som_fremsat.pdf</a></p></li><li><p id="ref-CR42">McGuire, A. L., Fisher, R., Cusenza, P., Hudson, K., Rothstein, M. A., McGraw, D., Matteson, S., Glaser, J., &amp; Henley, D. E. (2008). Confidentiality, privacy, and security of genetic and genomic test information in electronic health records: Points to consider. <i>Genetics in Medicine,</i> <i>10</i>(7), 495–499.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1097/GIM.0b013e31817a8aaa" data-track-item_id="10.1097/GIM.0b013e31817a8aaa" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1097%2FGIM.0b013e31817a8aaa" aria-label="Article reference 42" data-doi="10.1097/GIM.0b013e31817a8aaa">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 42" href="http://scholar.google.com/scholar_lookup?&amp;title=Confidentiality%2C%20privacy%2C%20and%20security%20of%20genetic%20and%20genomic%20test%20information%20in%20electronic%20health%20records%3A%20Points%20to%20consider&amp;journal=Genetics%20in%20Medicine&amp;doi=10.1097%2FGIM.0b013e31817a8aaa&amp;volume=10&amp;issue=7&amp;pages=495-499&amp;publication_year=2008&amp;author=McGuire%2CAL&amp;author=Fisher%2CR&amp;author=Cusenza%2CP&amp;author=Hudson%2CK&amp;author=Rothstein%2CMA&amp;author=McGraw%2CD&amp;author=Matteson%2CS&amp;author=Glaser%2CJ&amp;author=Henley%2CDE">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR43">Medin, D. L. (1989). Concepts and conceptual structure. <i>American Psychologist,</i> <i>44</i>(12), 1469–1481.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1037/0003-066X.44.12.1469" data-track-item_id="10.1037/0003-066X.44.12.1469" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2F0003-066X.44.12.1469" aria-label="Article reference 43" data-doi="10.1037/0003-066X.44.12.1469">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 43" href="http://scholar.google.com/scholar_lookup?&amp;title=Concepts%20and%20conceptual%20structure&amp;journal=American%20Psychologist&amp;doi=10.1037%2F0003-066X.44.12.1469&amp;volume=44&amp;issue=12&amp;pages=1469-1481&amp;publication_year=1989&amp;author=Medin%2CDL">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR44">Mojtabai, R., Olfson, M., Sampson, N. A., Jin, R., Druss, B., Wang, P. S., Wells, K. B., Pincus, H. A., &amp; Kessler, R. C. (2011). Barriers to mental health treatment: Results from the National Comorbidity Survey Replication. <i>Psychological Medicine,</i> <i>41</i>(8), 1751–1761.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1017/S0033291710002291" data-track-item_id="10.1017/S0033291710002291" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1017%2FS0033291710002291" aria-label="Article reference 44" data-doi="10.1017/S0033291710002291">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 44" href="http://scholar.google.com/scholar_lookup?&amp;title=Barriers%20to%20mental%20health%20treatment%3A%20Results%20from%20the%20National%20Comorbidity%20Survey%20Replication&amp;journal=Psychological%20Medicine&amp;doi=10.1017%2FS0033291710002291&amp;volume=41&amp;issue=8&amp;pages=1751-1761&amp;publication_year=2011&amp;author=Mojtabai%2CR&amp;author=Olfson%2CM&amp;author=Sampson%2CNA&amp;author=Jin%2CR&amp;author=Druss%2CB&amp;author=Wang%2CPS&amp;author=Wells%2CKB&amp;author=Pincus%2CHA&amp;author=Kessler%2CRC">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR45">Myers, A., &amp; Rosen, J. C. (1999). Obesity stigmatization and coping: Relation to mental health symptoms, body image, and self-esteem. <i>International Journal of Obesity,</i> <i>23</i>(3), 221–230.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1038/sj.ijo.0800765" data-track-item_id="10.1038/sj.ijo.0800765" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fsj.ijo.0800765" aria-label="Article reference 45" data-doi="10.1038/sj.ijo.0800765">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 45" href="http://scholar.google.com/scholar_lookup?&amp;title=Obesity%20stigmatization%20and%20coping%3A%20Relation%20to%20mental%20health%20symptoms%2C%20body%20image%2C%20and%20self-esteem&amp;journal=International%20Journal%20of%20Obesity&amp;doi=10.1038%2Fsj.ijo.0800765&amp;volume=23&amp;issue=3&amp;pages=221-230&amp;publication_year=1999&amp;author=Myers%2CA&amp;author=Rosen%2CJC">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR46">Nozick, R. (1969). Coercion, 440-472.  In Morgenbesser, W. (Ed). <i>Philosophy, Science, and Method: Essays in Honor of Ernest Nagel</i>. St Martin's Press.</p></li><li><p id="ref-CR47">Peretti-Watel, P., Legleye, S., Guignard, R., &amp; Beck, F. (2014). Cigarette smoking as a stigma: Evidence from France. <i>International Journal of Drug Policy,</i> <i>25</i>(2), 282–290.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1016/j.drugpo.2013.08.009" data-track-item_id="10.1016/j.drugpo.2013.08.009" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.drugpo.2013.08.009" aria-label="Article reference 47" data-doi="10.1016/j.drugpo.2013.08.009">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Cigarette%20smoking%20as%20a%20stigma%3A%20Evidence%20from%20France&amp;journal=International%20Journal%20of%20Drug%20Policy&amp;doi=10.1016%2Fj.drugpo.2013.08.009&amp;volume=25&amp;issue=2&amp;pages=282-290&amp;publication_year=2014&amp;author=Peretti-Watel%2CP&amp;author=Legleye%2CS&amp;author=Guignard%2CR&amp;author=Beck%2CF">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR48">Ploug, T. (2020). In Defence of informed consent for health record research—Why arguments from ‘easy rescue’, ‘no harm’ and ‘consent bias’ fail. <i>BMC Medical Ethics,</i> <i>21</i>(1), 75.</p><p><a data-track="click||click_references" rel="noopener" data-track-label="10.1186/s12910-020-00519-w" data-track-item_id="10.1186/s12910-020-00519-w" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1186/s12910-020-00519-w" aria-label="Article reference 48" data-doi="10.1186/s12910-020-00519-w">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 48" href="http://scholar.google.com/scholar_lookup?&amp;title=In%20Defence%20of%20informed%20consent%20for%20health%20record%20research%E2%80%94Why%20arguments%20from%20%E2%80%98easy%20rescue%E2%80%99%2C%20%E2%80%98no%20harm%E2%80%99%20and%20%E2%80%98consent%20bias%E2%80%99%20fail&amp;journal=BMC%20Medical%20Ethics&amp;doi=10.1186%2Fs12910-020-00519-w&amp;volume=21&amp;issue=1&amp;publication_year=2020&amp;author=Ploug%2CT">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR49">Ploug, T., &amp; Holm, S. (2013). Informed consent and routinisation. <i>Journal of Medical Ethics,</i> <i>39</i>(4), 214–218.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1136/medethics-2012-101056" data-track-item_id="10.1136/medethics-2012-101056" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1136%2Fmedethics-2012-101056" aria-label="Article reference 49" data-doi="10.1136/medethics-2012-101056">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 49" href="http://scholar.google.com/scholar_lookup?&amp;title=Informed%20consent%20and%20routinisation&amp;journal=Journal%20of%20Medical%20Ethics&amp;doi=10.1136%2Fmedethics-2012-101056&amp;volume=39&amp;issue=4&amp;pages=214-218&amp;publication_year=2013&amp;author=Ploug%2CT&amp;author=Holm%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR50">Ploug, T., &amp; Holm, S. (2015). Routinisation of informed consent in online health care systems. <i>International Journal of Medical Informatics,</i> <i>84</i>(4), 229–236.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1016/j.ijmedinf.2015.01.003" data-track-item_id="10.1016/j.ijmedinf.2015.01.003" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.ijmedinf.2015.01.003" aria-label="Article reference 50" data-doi="10.1016/j.ijmedinf.2015.01.003">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 50" href="http://scholar.google.com/scholar_lookup?&amp;title=Routinisation%20of%20informed%20consent%20in%20online%20health%20care%20systems&amp;journal=International%20Journal%20of%20Medical%20Informatics&amp;doi=10.1016%2Fj.ijmedinf.2015.01.003&amp;volume=84&amp;issue=4&amp;pages=229-236&amp;publication_year=2015&amp;author=Ploug%2CT&amp;author=Holm%2CS">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR51">Ploug, T., Holm, S., &amp; Gjerris, M. (2015). The stigmatization dilemma in public health policy-the case of MRSA in Denmark. <i>BMC Public Health,</i> <i>15</i>(1), 640.</p><p><a data-track="click||click_references" rel="noopener" data-track-label="10.1186/s12889-015-2004-y" data-track-item_id="10.1186/s12889-015-2004-y" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1186/s12889-015-2004-y" aria-label="Article reference 51" data-doi="10.1186/s12889-015-2004-y">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20stigmatization%20dilemma%20in%20public%20health%20policy-the%20case%20of%20MRSA%20in%20Denmark&amp;journal=BMC%20Public%20Health&amp;doi=10.1186%2Fs12889-015-2004-y&amp;volume=15&amp;issue=1&amp;publication_year=2015&amp;author=Ploug%2CT&amp;author=Holm%2CS&amp;author=Gjerris%2CM">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR52">Pouwels, J. L., Valkenburg, P. M., Beyens, I., van Driel, I. I., &amp; Keijsers, L. (2021). Social media use and friendship closeness in adolescents’ daily lives: An experience sampling study. <i>Developmental Psychology,</i> <i>57</i>(2), 309.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1037/dev0001148" data-track-item_id="10.1037/dev0001148" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2Fdev0001148" aria-label="Article reference 52" data-doi="10.1037/dev0001148">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Social%20media%20use%20and%20friendship%20closeness%20in%20adolescents%E2%80%99%20daily%20lives%3A%20An%20experience%20sampling%20study&amp;journal=Developmental%20Psychology&amp;doi=10.1037%2Fdev0001148&amp;volume=57&amp;issue=2&amp;publication_year=2021&amp;author=Pouwels%2CJL&amp;author=Valkenburg%2CPM&amp;author=Beyens%2CI&amp;author=Driel%2CII&amp;author=Keijsers%2CL">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR53">Powers, P. (2007). Persuasion and coercion: A critical review of philosophical and empirical approaches. <i>HEC Forum,</i> <i>19</i>(2), 125–143.</p><p><a data-track="click||click_references" rel="noopener" data-track-label="10.1007/s10730-007-9035-4" data-track-item_id="10.1007/s10730-007-9035-4" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10730-007-9035-4" aria-label="Article reference 53" data-doi="10.1007/s10730-007-9035-4">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 53" href="http://scholar.google.com/scholar_lookup?&amp;title=Persuasion%20and%20coercion%3A%20A%20critical%20review%20of%20philosophical%20and%20empirical%20approaches&amp;journal=HEC%20Forum&amp;doi=10.1007%2Fs10730-007-9035-4&amp;volume=19&amp;issue=2&amp;pages=125-143&amp;publication_year=2007&amp;author=Powers%2CP">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR54">Rangel, U., &amp; Keller, J. (2011). Essentialism goes social: Belief in social determinism as a component of psychological essentialism. <i>Journal of Personality and Social Psychology,</i> <i>100</i>(6), 1056.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1037/a0022401" data-track-item_id="10.1037/a0022401" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1037%2Fa0022401" aria-label="Article reference 54" data-doi="10.1037/a0022401">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 54" href="http://scholar.google.com/scholar_lookup?&amp;title=Essentialism%20goes%20social%3A%20Belief%20in%20social%20determinism%20as%20a%20component%20of%20psychological%20essentialism&amp;journal=Journal%20of%20Personality%20and%20Social%20Psychology&amp;doi=10.1037%2Fa0022401&amp;volume=100&amp;issue=6&amp;publication_year=2011&amp;author=Rangel%2CU&amp;author=Keller%2CJ">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR55">Reece, A. G., &amp; Danforth, C. M. (2017). Instagram photos reveal predictive markers of depression. <i>EPJ Data Science,</i> <i>6</i>(1), 15.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1140/epjds/s13688-017-0110-z" data-track-item_id="10.1140/epjds/s13688-017-0110-z" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1140%2Fepjds%2Fs13688-017-0110-z" aria-label="Article reference 55" data-doi="10.1140/epjds/s13688-017-0110-z">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 55" href="http://scholar.google.com/scholar_lookup?&amp;title=Instagram%20photos%20reveal%20predictive%20markers%20of%20depression&amp;journal=EPJ%20Data%20Science&amp;doi=10.1140%2Fepjds%2Fs13688-017-0110-z&amp;volume=6&amp;issue=1&amp;publication_year=2017&amp;author=Reece%2CAG&amp;author=Danforth%2CCM">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR56">Reece, A. G., Reagan, A. J., Lix, K. L. M., Dodds, P. S., Danforth, C. M., &amp; Langer, E. J. (2017). Forecasting the onset and course of mental illness with Twitter data. <i>Scientific Reports,</i> <i>7</i>(1), Art. 1.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1038/s41598-017-12961-9" data-track-item_id="10.1038/s41598-017-12961-9" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1038%2Fs41598-017-12961-9" aria-label="Article reference 56" data-doi="10.1038/s41598-017-12961-9">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 56" href="http://scholar.google.com/scholar_lookup?&amp;title=Forecasting%20the%20onset%20and%20course%20of%20mental%20illness%20with%20Twitter%20data&amp;journal=Scientific%20Reports&amp;doi=10.1038%2Fs41598-017-12961-9&amp;volume=7&amp;issue=1&amp;publication_year=2017&amp;author=Reece%2CAG&amp;author=Reagan%2CAJ&amp;author=Lix%2CKLM&amp;author=Dodds%2CPS&amp;author=Danforth%2CCM&amp;author=Langer%2CEJ">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR57">Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation), OJ L 119, (2016). <a href="http://data.europa.eu/eli/reg/2016/679/oj" data-track="click" data-track-action="external reference" data-track-label="http://data.europa.eu/eli/reg/2016/679/oj">http://data.europa.eu/eli/reg/2016/679/oj</a></p></li><li><p id="ref-CR58">Rummens, S., &amp; Cuypers, S. E. (2010). Determinism and the paradox of predictability. <i>Erkenntnis,</i> <i>72</i>(2), 233–249.</p><p><a data-track="click||click_references" rel="noopener" data-track-label="10.1007/s10670-009-9199-1" data-track-item_id="10.1007/s10670-009-9199-1" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10670-009-9199-1" aria-label="Article reference 58" data-doi="10.1007/s10670-009-9199-1">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 58" href="http://scholar.google.com/scholar_lookup?&amp;title=Determinism%20and%20the%20paradox%20of%20predictability&amp;journal=Erkenntnis&amp;doi=10.1007%2Fs10670-009-9199-1&amp;volume=72&amp;issue=2&amp;pages=233-249&amp;publication_year=2010&amp;author=Rummens%2CS&amp;author=Cuypers%2CSE">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR59">Skoric, M. M., Zhu, Q., Goh, D., &amp; Pang, N. (2016). Social media and citizen engagement: A meta-analytic review. <i>New Media &amp; Society,</i> <i>18</i>(9), 1817–1839.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1177/1461444815616221" data-track-item_id="10.1177/1461444815616221" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1177%2F1461444815616221" aria-label="Article reference 59" data-doi="10.1177/1461444815616221">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 59" href="http://scholar.google.com/scholar_lookup?&amp;title=Social%20media%20and%20citizen%20engagement%3A%20A%20meta-analytic%20review&amp;journal=New%20Media%20%26%20Society&amp;doi=10.1177%2F1461444815616221&amp;volume=18&amp;issue=9&amp;pages=1817-1839&amp;publication_year=2016&amp;author=Skoric%2CMM&amp;author=Zhu%2CQ&amp;author=Goh%2CD&amp;author=Pang%2CN">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR60">Stuart, H. (2006). Mental illness and employment discrimination. <i>Current Opinion in Psychiatry,</i> <i>19</i>(5), 522–526.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1097/01.yco.0000238482.27270.5d" data-track-item_id="10.1097/01.yco.0000238482.27270.5d" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1097%2F01.yco.0000238482.27270.5d" aria-label="Article reference 60" data-doi="10.1097/01.yco.0000238482.27270.5d">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 60" href="http://scholar.google.com/scholar_lookup?&amp;title=Mental%20illness%20and%20employment%20discrimination&amp;journal=Current%20Opinion%20in%20Psychiatry&amp;doi=10.1097%2F01.yco.0000238482.27270.5d&amp;volume=19&amp;issue=5&amp;pages=522-526&amp;publication_year=2006&amp;author=Stuart%2CH">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR61">Stuber, J., Meyer, I., &amp; Link, B. (2008). Stigma, prejudice, discrimination and health. <i>Social Science &amp; Medicine,</i> <i>67</i>(3), 351–357.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1016/j.socscimed.2008.03.023" data-track-item_id="10.1016/j.socscimed.2008.03.023" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.socscimed.2008.03.023" aria-label="Article reference 61" data-doi="10.1016/j.socscimed.2008.03.023">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 61" href="http://scholar.google.com/scholar_lookup?&amp;title=Stigma%2C%20prejudice%2C%20discrimination%20and%20health&amp;journal=Social%20Science%20%26%20Medicine&amp;doi=10.1016%2Fj.socscimed.2008.03.023&amp;volume=67&amp;issue=3&amp;pages=351-357&amp;publication_year=2008&amp;author=Stuber%2CJ&amp;author=Meyer%2CI&amp;author=Link%2CB">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR62">Stuber, J., Galea, S., &amp; Link, B. G. (2009). Stigma and smoking: The consequences of our good intentions. <i>Social Service Review,</i> <i>83</i>(4), 585–609.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1086/650349" data-track-item_id="10.1086/650349" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1086%2F650349" aria-label="Article reference 62" data-doi="10.1086/650349">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 62" href="http://scholar.google.com/scholar_lookup?&amp;title=Stigma%20and%20smoking%3A%20The%20consequences%20of%20our%20good%20intentions&amp;journal=Social%20Service%20Review&amp;doi=10.1086%2F650349&amp;volume=83&amp;issue=4&amp;pages=585-609&amp;publication_year=2009&amp;author=Stuber%2CJ&amp;author=Galea%2CS&amp;author=Link%2CBG">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR63">Valkenburg, P. M., Meier, A., &amp; Beyens, I. (2022). Social media use and its impact on adolescent mental health: An umbrella review of the evidence. <i>Current Opinion in Psychology,</i> <i>44</i>, 58–68.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1016/j.copsyc.2021.08.017" data-track-item_id="10.1016/j.copsyc.2021.08.017" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.copsyc.2021.08.017" aria-label="Article reference 63" data-doi="10.1016/j.copsyc.2021.08.017">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 63" href="http://scholar.google.com/scholar_lookup?&amp;title=Social%20media%20use%20and%20its%20impact%20on%20adolescent%20mental%20health%3A%20An%20umbrella%20review%20of%20the%20evidence&amp;journal=Current%20Opinion%20in%20Psychology&amp;doi=10.1016%2Fj.copsyc.2021.08.017&amp;volume=44&amp;pages=58-68&amp;publication_year=2022&amp;author=Valkenburg%2CPM&amp;author=Meier%2CA&amp;author=Beyens%2CI">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR64">Voorhees, B. W. V., Fogel, J., Houston, T. K., Cooper, L. A., Wang, N.-Y., &amp; Ford, D. E. (2005). Beliefs and attitudes associated with the intention to not accept the diagnosis of depression among young adults. <i>The Annals of Family Medicine,</i> <i>3</i>(1), 38–46.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1370/afm.273" data-track-item_id="10.1370/afm.273" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1370%2Fafm.273" aria-label="Article reference 64" data-doi="10.1370/afm.273">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 64" href="http://scholar.google.com/scholar_lookup?&amp;title=Beliefs%20and%20attitudes%20associated%20with%20the%20intention%20to%20not%20accept%20the%20diagnosis%20of%20depression%20among%20young%20adults&amp;journal=The%20Annals%20of%20Family%20Medicine&amp;doi=10.1370%2Fafm.273&amp;volume=3&amp;issue=1&amp;pages=38-46&amp;publication_year=2005&amp;author=Voorhees%2CBWV&amp;author=Fogel%2CJ&amp;author=Houston%2CTK&amp;author=Cooper%2CLA&amp;author=Wang%2CN-Y&amp;author=Ford%2CDE">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR65">Vredenburgh, K. (2022). The right to explanation*. <i>Journal of Political Philosophy,</i> <i>30</i>(2), 209–229.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1111/jopp.12262" data-track-item_id="10.1111/jopp.12262" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1111%2Fjopp.12262" aria-label="Article reference 65" data-doi="10.1111/jopp.12262">Article</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 65" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20right%20to%20explanation%2A&amp;journal=Journal%20of%20Political%20Philosophy&amp;doi=10.1111%2Fjopp.12262&amp;volume=30&amp;issue=2&amp;pages=209-229&amp;publication_year=2022&amp;author=Vredenburgh%2CK">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR66">Wertheimer, A. (1990). <i>Coercion</i>. Princeton University Press.</p><p><a data-track="click||click_references" rel="nofollow noopener" data-track-label="10.1515/9781400859290" data-track-item_id="10.1515/9781400859290" data-track-value="book reference" data-track-action="book reference" href="https://doi.org/10.1515%2F9781400859290" aria-label="Book reference 66" data-doi="10.1515/9781400859290">Book</a>&nbsp;
    <a data-track="click||click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 66" href="http://scholar.google.com/scholar_lookup?&amp;title=Coercion&amp;doi=10.1515%2F9781400859290&amp;publication_year=1990&amp;author=Wertheimer%2CA">
                    Google Scholar</a>&nbsp;
                </p></li><li><p id="ref-CR67">Yzerbyt, V., Rocher, S., &amp; Schadron, G. (1997). Stereotypes as explanations: A subjective essentialistic view of group perception, 20-50. In R. Spears, P. J. Oakes, N. Ellemers, &amp; S. A. Haslam (Eds.). <i>The social psychology of stereotyping and group life</i>. Blackwell Publishing.</p></li><li><p id="ref-CR68">Zirikly, A., Resnik, P., Uzuner, Ö., &amp; Hollingshead, K. (2019). CLPsych 2019 Shared task: Predicting the degree of suicide risk in Reddit posts, 24-33. <i>Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology</i>. Minneapolis, Minnesota.</p></li></ul><p><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s13347-023-00616-9?format=refman&amp;flavour=references">Download references</a></p></div></div><div id="Ack1-section" data-title="Acknowledgements"><h2 id="Ack1">Acknowledgements</h2><p>I would like to thank Hanne Marie Motzfeld and Søren Holm for their valuable and insightful comments.</p></div><div id="author-information-section" aria-labelledby="author-information" data-title="Author information"><h2 id="author-information">Author information</h2><div id="author-information-content"><h3 id="affiliations">Authors and Affiliations</h3><ol><li id="Aff1"><p>Centre for Applied Ethics and Philosophy of Science, Department of Communication and Psychology, Aalborg University, A C Meyers Vænge, 2450, Copenhagen, SV, Denmark</p><p>Thomas Ploug</p></li></ol><div data-test="author-info"><p><span>Authors</span></p><ol><li id="auth-Thomas-Ploug-Aff1"><span>Thomas Ploug</span><div><p>You can also search for this author in
                        <span><a href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Thomas%20Ploug" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span>&nbsp;</span><a href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Thomas%20Ploug%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></li></ol></div><h3 id="contributions">Contributions</h3><p>Not applicable.</p><h3 id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:ploug@ikp.aau.dk">Thomas Ploug</a>.</p></div></div><div id="ethics-section" data-title="Ethics declarations"><h2 id="ethics">Ethics declarations</h2><div id="ethics-content">
              
              
                <h3 id="FPar1">Ethics Approval and Consent to Participate</h3>
                <p>Not applicable.</p>
              
              
                <h3 id="FPar2">Consent for Publication</h3>
                <p>Not applicable.</p>
              
              
                <h3 id="FPar3">Competing Interests</h3>
                <p>The author declares no competing interests.</p>
              
            </div></div><div id="additional-information-section" data-title="Additional information"><h2 id="additional-information">Additional information</h2><div id="additional-information-content"><h3>Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div><div id="rightslink-section" data-title="Rights and permissions"><h2 id="rightslink">Rights and permissions</h2><div id="rightslink-content">
                <p><b>Open Access</b> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">http://creativecommons.org/licenses/by/4.0/</a>.</p>
              <p><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=The%20Right%20Not%20to%20Be%20Subjected%20to%20AI%20Profiling%20Based%20on%20Publicly%20Available%20Data%E2%80%94Privacy%20and%20the%20Exceptionalism%20of%20AI%20Profiling&amp;author=Thomas%20Ploug&amp;contentID=10.1007%2Fs13347-023-00616-9&amp;copyright=The%20Author%28s%29&amp;publication=2210-5433&amp;publicationDate=2023-03-07&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY">Reprints and permissions</a></p></div></div><div id="article-info-section" aria-labelledby="article-info" data-title="About this article"><h2 id="article-info">About this article</h2><div id="article-info-content"><p><a data-crossmark="10.1007/s13347-023-00616-9" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s13347-023-00616-9" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></p><div><h3 id="citeas">Cite this article</h3><p>Ploug, T. The Right Not to Be Subjected to AI Profiling Based on Publicly Available Data—Privacy and the Exceptionalism of AI Profiling.
                    <i>Philos. Technol.</i> <b>36</b>, 14 (2023). https://doi.org/10.1007/s13347-023-00616-9</p><p><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s13347-023-00616-9?format=refman&amp;flavour=citation">Download citation</a></p><ul data-test="publication-history"><li><p>Received<span>: </span><span><time datetime="2022-10-10">10 October 2022</time></span></p></li><li><p>Accepted<span>: </span><span><time datetime="2023-02-22">22 February 2023</time></span></p></li><li><p>Published<span>: </span><span><time datetime="2023-03-07">07 March 2023</time></span></p></li><li><p><abbr title="Digital Object Identifier">DOI</abbr><span>: </span><span>https://doi.org/10.1007/s13347-023-00616-9</span></p></li></ul><h3>Keywords</h3></div></div></div>

                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Starship made it through re-entry, did its first ever landing burn (102 pts)]]></title>
            <link>https://twitter.com/SpaceX/status/1798715759193096245</link>
            <guid>40597434</guid>
            <pubDate>Thu, 06 Jun 2024 13:57:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/SpaceX/status/1798715759193096245">https://twitter.com/SpaceX/status/1798715759193096245</a>, See on <a href="https://news.ycombinator.com/item?id=40597434">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[3D Math Primer for Graphics and Game Development (2011) (117 pts)]]></title>
            <link>https://gamemath.com/</link>
            <guid>40597402</guid>
            <pubDate>Thu, 06 Jun 2024 13:54:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gamemath.com/">https://gamemath.com/</a>, See on <a href="https://news.ycombinator.com/item?id=40597402">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="BodyContainer">
<div>
    <p>Welcome
<img id="lightmodetoggle" src="https://gamemath.com/light_dark_toggle.png">
</p>

</div><p>
This is the home page of the book <i>3D Math Primer for Graphics and Game Development</i>.
<br>
You can now <a href="https://gamemath.com/book/intro.html">read the entire book</a> for free!
</p><p>
<a href="https://gamemath.com/book/intro.html">
<img src="https://gamemath.com/book/figs/cover_72dpi.jpg">
</a></p><h2>GDC Talks</h2>
<ul>
	<li><b>NEW!</b> GDC 2021. <a href="https://schedule.gdconf.com/session/math-in-game-development-summit-getting-there-in-style-intro-to-interpolation-and-control-systems/880908">
<i>Getting There in Style: Intro to interpolation and Control Systems</i></a>.  Some supplemental materials are available <a href="https://gamemath.com/gdc2021/">here</a>.
	</li><li>GDC 2018. <a href="https://www.gdcvault.com/play/1024933/Denial-of-Service">Denial-of-Service Mitigation</a>.
</li></ul>

<h2>FAQ</h2>

<h2>Where can I get a physical copy?</h2><p>
The book is now out of print, but you can probably find a used copy on
<a href="https://amazon.com/Math-Primer-Graphics-Game-Development-ebook/dp/B008KZU548/">amazon.com</a>.

</p><h2>Is there a PDF or e-reader format available?</h2><p>
Soon.  Note that the kindle vesion you can buy on amazon is a "print-replica"
and not really optimized for e-readers.

</p><h2>Who created the cover art?</h2>
<p>Eric Huang (<a href="https://twitter.com/trinketeric">@TrinketEric</a>).  We worked
together on <a>Guilty Party</a> at Wideload Games in Chicago.  Since then he created the
<a href="https://www.battlechefbrigade.com/">Battle Chef</a> games.
</p>

<h2>Can you recommend an open-source network transport library for games?</h2>
<p>
I get this question all the time, and I'm so glad you asked.  Yes, please check out
<a href="https://github.com/ValveSoftware/GameNetworkingSockets">GameNetworkingSockets</a>.
</p>

<h2>Where are the resources for the first edition?</h2>
<div>
	<p><img src="https://gamemath.com/1st_edition/1st_edition_cover.png">
    </p>
	<p><span><span>Figure&nbsp;1</span><span>Recycle any copies you own immediately</span></span>
</p></div>
<p>
The first edition was published in 2002.  You can tell it was 2002 because the cover
had a head rendered in <i>3DS Max</i> with rad math stuff projected onto it.  It absolutely
<i>screamed</i> "This is cool high tech math!"  Coordinate axes protruded from its ears and forehead
like a <i>Tron</i> version of the Q-Tip scene from <i>Better Off Dead</i>.
</p>
<p>
Like I said, it was pretty high tech.
</p>
<p>
If you happen to own one of these, please do me a favor and never show it to any person,
including yourself.  The second edition is much, <i>much</i> better.  And here it is for
free, so why would anybody read the first one?
</p>
<p>What's that?  You're not disuaded?  Fine.  Here's the <a href="https://gamemath.com/1st_edition/1st_edition_errata.pdf">errata</a>,
<a href="https://gamemath.com/1st_edition/answers_to_exercises_1_11.pdf">answers</a> to most of the exercises (<i>Yes</i>, they stop
at chapter 14 and <i>no</i> I will not be providing the answers to the last few chapters.), and some 
<a href="https://gamemath.com/1st_edition/code.zip">code</a> that is literally 20 years old.
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Martha Gellhorn, the only woman to report on the D-Day landings from the ground (144 pts)]]></title>
            <link>https://www.smithsonianmag.com/history/martha-gellhorn-was-the-only-woman-to-report-on-the-d-day-landings-from-the-ground-180984456/</link>
            <guid>40597249</guid>
            <pubDate>Thu, 06 Jun 2024 13:35:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smithsonianmag.com/history/martha-gellhorn-was-the-only-woman-to-report-on-the-d-day-landings-from-the-ground-180984456/">https://www.smithsonianmag.com/history/martha-gellhorn-was-the-only-woman-to-report-on-the-d-day-landings-from-the-ground-180984456/</a>, See on <a href="https://news.ycombinator.com/item?id=40597249">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

    <div data-article-body="">

        

      

      <p>Clouds of dust swirled and filled the night air as <a href="https://www.jfklibrary.org/hemingway/martha-gellhorn-hemingway">Martha Gellhorn</a> walked up a rocky road on <a href="https://en.normandie-tourisme.fr/discover/history/d-day-and-the-battle-of-normandy/d-day-landing-beaches/omaha-beach/">Omaha Beach</a>. At the end of the path, the American war correspondent paused for a moment. Suddenly, her senses were overwhelmed by the pleasant reminder of another time.</p>

<p>“It was perhaps the most surprising of all the day’s surprises to smell the sweet smell of summer grass, a smell of cattle and peace and sun that had warmed the earth some other time when summer was real,” she later wrote for <a href="https://www.unz.com/print/Colliers-1944aug05-00073/"><em>Collier’s</em> magazine</a>.</p>

<p>That moment stood in stark contrast to what Gellhorn had witnessed earlier: destroyed and smoldering vehicles, broken and bloodied bodies, gaping holes in the soft sand. The vestiges of battle from the day before—<a href="https://www.army.mil/d-day/history.html">D-Day</a>, June 6, 1944—lined the beach, where soldiers from the United States Army had <a href="https://www.army.mil/d-day/history.html">fought their way</a> ashore as part of the <a href="https://www.britannica.com/event/Normandy-Invasion">Allied invasion</a> of France during World War II.</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/D5-T3ZiTqZeb4nQYuT3j-5QyAZo=/fit-in/1072x0/filters:focal(960x777:961x778)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/66/a3/66a335ea-6933-4b6d-b7ef-9381a5721b5f/into_the_jaws_of_death_23-0455m_edit.jpeg" alt="American troops disembark at Omaha Beach on June 6, 1944." loading="lazy">
  

  <figcaption>
    
      American troops disembark at Omaha Beach on June 6, 1944.&nbsp;
    
    
      <span><a href="https://en.wikipedia.org/wiki/Omaha_Beach#/media/File:Into_the_Jaws_of_Death_23-0455M_edit.jpg" target="blank">Public domain via Wikimedia Commons</a></span>
    
  </figcaption>
</figure>
<p>Gellhorn was one of the first journalists—and the only <a href="https://www.military.com/history/how-martha-gellhorn-became-only-female-journalist-d-day.html">female correspondent</a>—to view that hellish scene 80 years ago. Lacking proper credentials, she lied her way onto a hospital ship traveling from England to France, then rode in a water ambulance to the still-dangerous Normandy shore as artillery shells from battleships roared overhead. Among other hazards, she endured snipers, landmines and strafing by German warplanes, all to get the story.</p>

<p>“Double and triple clap of gunfire,” Gellhorn wrote in her diary. “Unseen planes roar. Barrage balloons. Gun flashes. One close shell burst. … Explosions jar the ship.”</p>

<p>“The fighting went on for several days, and the landing craft were under attack,” says <a href="https://www.publishersweekly.com/pw/authorpage/caroline-moorehead.html">Caroline Moorehead</a>, author of <a href="https://amzn.to/3R7RYFA"><em>Gellhorn: A 20th-Century Life</em></a>. “People were still being killed as she was landing.”</p>

<hr>
<p>Gellhorn was a <a href="https://www.nationalww2museum.org/war/articles/martha-gellhorn-eyewitness-war">veteran war correspondent</a> who covered multiple conflicts over her six-decade career. Leading up to D-Day, she reported on the <a href="https://www.unz.com/print/Colliers-1937jul17-00012/">Spanish Civil War</a>, the rise of Nazi leader <a href="https://www.smithsonianmag.com/history/adolf-hitler-coup-prison-beer-hall-putsch-180983207/">Adolf Hitler</a> and the German <a href="https://www.theholocaustexplained.org/life-in-nazi-occupied-europe/foreign-policy-and-the-road-to-war/annexation-of-czechoslovakia/">annexation of Czechoslovakia</a> in 1938, the <a href="https://www.iwm.org.uk/history/a-short-history-of-the-winter-war">Winter War</a> between Finland and the Soviet Union in 1939, and the <a href="https://www.britannica.com/event/Second-Sino-Japanese-War">Japanese invasion of China</a> in 1941. She would later go on to report on the <a href="https://www.pbs.org/battlefieldvietnam/timeline/index1.html">Vietnam War</a>, the <a href="https://www.wilsoncenter.org/publication/the-1967-six-day-war">Six-Day War</a> in 1967 and other world crises.</p>

<p>Born in St. Louis in 1908, the future journalist was the only daughter of George Gellhorn, a German-born doctor, and <a href="https://historicmissourians.shsmo.org/edna-gellhorn/">Edna Fischel Gellhorn</a>, a suffragist who helped found the League of Women Voters. Gellhorn’s father and maternal grandfather were Jewish, while her maternal grandmother was Protestant. Though Gellhorn wasn’t religious, this dual background had a lasting influence on her, particularly when she became one of the first journalists to view the <a href="https://encyclopedia.ushmm.org/content/en/article/dachau">Dachau concentration camp</a> in 1945.</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/gWBeOXk6m-PXZX6lwzlZPcLykAY=/fit-in/1072x0/filters:focal(300x186:301x187)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/ce/ed/ceedfadc-44bc-4c79-9b81-d0ca9f25ae84/the_golden_lane.jpg" alt="As a child, Gellhorn joined her mother, Edna Fischel Gellhorn, in a protest for women's suffrage at the 1916 Democratic National Convention." loading="lazy">
  

  <figcaption>
    
      As a child, Gellhorn joined her mother, Edna Fischel Gellhorn, in a protest for women's suffrage at the 1916 Democratic National Convention.
    
    
      <span><a href="https://en.wikipedia.org/wiki/Martha_Gellhorn#/media/File:The_Golden_Lane.jpg" target="blank">Public domain via Wikimedia Commons</a></span>
    
  </figcaption>
</figure>
<p>Through her vocal support of pressing social issues, Edna helped shape the social consciousness that later pervaded her daughter’s writing. In 1916, Edna and 7-year-old Gellhorn marched in a rally for women’s suffrage at the Democratic National Convention in St. Louis.</p>

<p>“For Gellhorn, I think it was all about making the world a better place,” says <a href="https://www.nationalww2museum.org/contributors/maggie-hartley-edd">Maggie Hartley</a>, director of public engagement at the <a href="https://www.nationalww2museum.org/">National WWII Museum</a> in New Orleans. “Whenever I give a talk, I like to include <a href="https://archive.org/details/faceofwar0000gell/page/337/mode/1up?q=%22better+way%22">this quote</a> by her: ‘There has to be a better way to run the world, and we had better see that we get it.’”</p>

<p>Gellhorn attended <a href="https://www.brynmawr.edu/bulletin/dear-teecher">Bryn Mawr College</a> in Pennsylvania but did not graduate, instead leaving to pursue a career as a journalist. She wrote for several publications, including the <a href="https://newrepublic.com/authors/martha-gellhorn"><em>New Republic</em></a><em> </em>and the <em>Albany Times Union</em>, then joined the <a href="https://historicmissourians.shsmo.org/martha-gellhorn/">United Press</a> news agency in France. She was later fired from the bureau for reporting sexual harassment by a businessman with ties to the agency. In the mid-1930s, she worked as a <a href="https://www.newyorker.com/books/page-turner/chronicling-poverty-with-compassion-and-rage">field investigator</a> for the Federal Emergency Relief Administration and assisted first lady <a href="https://hazlitt.net/longreads/friendship-eleanor-roosevelt-and-martha-gellhorn">Eleanor Roosevelt</a> with correspondence and her magazine column. In 1937, she traveled to Madrid to report on the Spanish Civil War for <em>Collier’s</em>, a now-defunct weekly magazine that featured the bylines of the U.S.’s top journalists and writers.</p>

<p>Where Gellhorn distinguished herself from other reporters was the focus of her articles. Rather than writing about generals or details of the fighting, she chronicled the stories of civilians unable to escape the violence of war: children, women, the elderly and others threatened by flying bullets and exploding shrapnel.</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/-hDtJ5o-BwzUn56h5ULsDP6-2fg=/fit-in/1072x0/filters:focal(960x695:961x696)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/76/bc/76bca59b-75c8-4050-a6e1-f4170ed6b1ec/ernest_and_martha_hemingway_photograph_-_nara_-_192698.jpeg" alt="Martha Gellhorn (second from left), Ernest Hemingway (third from right) and Chinese military officers in 1941" loading="lazy">
  

  <figcaption>
    
      Martha Gellhorn (second from left), Ernest Hemingway (third from right) and Chinese military officers in 1941
    
    
      <span><a href="https://commons.wikimedia.org/wiki/Category:Martha_Gellhorn#/media/File:Ernest_and_Martha_Hemingway_Photograph_-_NARA_-_192698.jpg" target="blank">Public domain via Wikimedia Commons</a></span>
    
  </figcaption>
</figure>
<p>“Gellhorn was willing to put herself in positions where some journalists wouldn’t and then tell the stories that weren’t the traditional ‘what’s going on with a battle?’ or troop movements,” Hartley says. “It resonated, especially back home, where people were waiting on news about their loved ones. That personal take really did impact her audience.”</p>

<hr>
<p>For D-Day, Gellhorn was again working with <em>Collier’s</em>. She had hoped to be the publication’s lead reporter for the invasion, but her husband of four years, the novelist and writer <a href="https://www.smithsonianmag.com/smithsonian-institution/search-authentic-ernest-hemingway-180977368/">Ernest Hemingway</a>, stole the assignment. By the spring of 1944, the couple’s relationship was on the rocks, and Gellhorn believed her husband had taken the job out of spite.</p>

<p>“It was clear to her that the marriage was over,” says Moorehead, whose mother was friends with Gellhorn. “Looking at the letters she wrote to her friends on the boat on the way over [to England], they’re very sad in a way. They’re sort of an epitaph on their marriage.” (Gellhorn had used her connections to secure Hemingway a spot on a flight to London, but her trans-Atlantic journey proved <a href="https://warfarehistorynetwork.com/article/ernest-hemingways-war/">more perilous</a>: She spent more than two weeks traveling on board a Norwegian freighter packed with dynamite.)</p>

<p>Once in London, Hemingway arranged for transport to Normandy on the USS <a href="https://ww2ondeadline.com/2020/05/25/ernest-hemingway-d-day-london-car-crash-robert-capa/"><em>Dorothea L. Dix</em></a>. Gellhorn, meanwhile, didn’t have military approval to continue to France. In fact, the military denied all <a href="https://www.bbc.com/news/magazine-27677889">female journalists</a> permission to cover the Allied landings. But that wasn’t going to stop her.</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/fZfK8nvAXkIYtOr65M7H7ysnzjk=/fit-in/1072x0/filters:focal(1500x1193:1501x1194)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/7a/d1/7ad15dad-7810-4a91-b827-6cbc56bb40df/ernest_hemingway_with_colonel_charles_t_buck_lanham_september_18_1944_-_nara_-_192699.jpg" alt="Ernest Hemingway and Colonel Charles T. Lanham in Europe on September 18, 1944" loading="lazy">
  

  <figcaption>
    
      Ernest Hemingway and Colonel Charles T. Lanham in Europe on September 18, 1944
    
    
      <span><a href="https://commons.wikimedia.org/wiki/Category:Ernest_Hemingway_Photograph_Collection_at_the_John_F._Kennedy_Presidential_Library_and_Museum#/media/File:Ernest_Hemingway_with_Colonel_Charles_T." target="blank">Public domain via Wikimedia Commons</a></span>
    
  </figcaption>
</figure>
<p>Contrary to <a href="https://www.military.com/history/how-martha-gellhorn-became-only-female-journalist-d-day.html">several</a> <a href="https://www.nationalww2museum.org/war/articles/martha-gellhorn-eyewitness-war">sources</a>, Gellhorn was not on Omaha Beach on June 6. According to Moorehead, when the reporter learned D-Day was underway that morning, she was in London attending a Ministry of Information briefing. Soon after, she made her way to Southampton, where the HMHS <a href="https://maritimearchaeologytrust.org/wp-content/uploads/2021/03/Hospital-Ships_Southampton_Roger-Burns_WEB.pdf"><em>Prague</em></a> was docked. To get on board the hospital carrier, she lied to a military policeman about wanting to interview nurses, then locked herself in a bathroom until the ship was safely en route.</p>

<p>Complete with a surgical operating room and 422 beds, the <em>Prague </em>was the first hospital ship to reach Normandy, arriving at Omaha Beach early on the morning of June 7. Gellhorn spent most of the daylight hours watching as injured soldiers were moved from <a href="https://www.smithsonianmag.com/innovation/invention-won-world-war-ii-180972327/">Higgins boats</a> converted into <a href="https://www.ww2online.org/image/lcvp-transporting-wounded-servicemen-coast-normandy-france-1944">water ambulances</a> onto the hospital ship. She comforted the men as best she could.</p>

<p>“It will be hard to tell you of the wounded, there were so many of them,” she <a href="https://www.unz.com/print/Colliers-1944aug05-00014">wrote</a> in her <em>Collier’s</em> article, which was published on August 5, 1944, and later reprinted in one of her anthologies, <a href="https://amzn.to/4bF8cOV"><em>The Face of War</em></a>. “There was no time to talk; there was too much else to do. They had to be fed, as most of them had not eaten for two days. … They wanted water; the nurses and orderlies, working like demons, had to be found and called quickly to a bunk where a man suddenly and desperately needed attention.”</p>

<p>Gellhorn also described the stoicism of the American soldier:</p>

<blockquote>
<p>Men smiled who were in such pain that all they can really have wanted to do was turn their heads away and cry, and men made jokes when they needed their strength just to survive. All of them looked after one another, saying, “Give that boy a drink of water,” or “Miss, see that Ranger over there; he’s in bad shape. Could you go to him?”</p>
</blockquote><figure>
    <div data-type="video">
    <p>Martha Gellhorn: The War Correspondent who Covered D-Day</p>
    
    <p><img src="https://i.ytimg.com/vi/kIKq-H6RLFw/maxresdefault.jpg" data-video-id="kIKq-H6RLFw"></p>
  </div>
    
</figure>
<p>Because she spoke several languages, Gellhorn assisted doctors and nurses by translating for injured German prisoners of war and French civilians. When the enemy wounded made too much noise, she wrote in her diary, she told them “<em>ruhig</em>”—German for “quiet”—“on doctor’s orders, and they were all instantly silent.”</p>

<p>As night fell, word came back to the hospital ship that wounded men were still on the beach. Gellhorn jumped into a water ambulance and headed to land to help. Despite the dangers, she waded through the cold surf to search for the injured.</p>

<p>“Everyone was violently busy on that crowded, dangerous shore,” Gellhorn wrote for <em>Collier’s</em>. “We walked with the utmost care between the narrowly placed white tape lines that marked the mine-cleared path and headed for a tent marked with a red cross.”</p>

<p>Gellhorn helped load and tend to the wounded on a beached <a href="https://www.navytimes.com/news/your-navy/2019/06/06/the-unloved-unlovely-yet-indispensable-lst/">landing ship, tank</a>, or LST. Because of extreme tides, the vessel could not return to the hospital carrier until the wee hours of June 8. The soldiers were then taken on board the <em>Prague</em>, where the sickbeds were filled, and preparations were made for the return trip to England. Describing the scene in her article, Gellhorn wrote, “Piles of bloody clothing had been cut off and dumped out of the way in corners; coffee cups and cigarette stubs littered the decks, plasma bottles hung from cords, and all the fearful surgical apparatus for holding broken bones made shadows on the walls.”</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/vN7v7rFW2pnA58UJ5_7KUf_CXyI=/fit-in/1072x0/filters:focal(370x296:371x297)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/12/8c/128c5a84-9179-467a-b42e-05aff2024647/lci-convoy.jpeg" alt="A large landing craft convoy crosses the English Channel on June 6, 1944" loading="lazy">
  

  <figcaption>
    
      A large landing craft convoy crosses the English Channel on June 6, 1944
    
    
      <span><a href="https://en.wikipedia.org/wiki/Normandy_landings#/media/File:Lci-convoy.jpg" target="blank">Public domain via Wikimedia Commons</a></span>
    
  </figcaption>
</figure>
<p>Despite the high numbers of injuries, only one man died on the hospital ship. The rest made it across the English Channel, where they were transferred to military hospitals for surgery and rehabilitation.</p>

<p>Upon her return to London, Gellhorn was arrested by military police for traveling to Normandy <a href="https://ww2ondeadline.com/2020/06/07/d-day-martha-gellhorn-ernest-hemingway-colliers/comment-page-1/">without permission</a>. As punishment, she was sent to a training camp for American nurses and told she could go back to France when they did. By then, it was too late: She had already filed her story with <em>Collier’s</em>.</p>

<p>Unlike his wife, <a href="https://www.archives.gov/publications/prologue/2006/spring/hemingway.html#:~:text=Hemingway%20accompanied%20American%20troops%20as,22nd%20Regiment%20commanded%20by%20Col.">Hemingway</a> never went ashore at Normandy. On June 6, all he could do was watch from a landing craft as American soldiers fought their way onto Omaha Beach.</p>

<p>Even though Gellhorn scooped Hemingway, his story ran first. “<a href="https://www.unz.com/print/Colliers-1944jul22-00011/">Voyage to Victory</a>,” proclaimed the cover of <em>Collier’s</em> July 22, 1944, issue. The article identified Hemingway as “<em>Collier’s</em> famed war correspondent” and included a photo of the whiskered writer with Allied soldiers. Gellhorn’s story ran in the August 5 issue with the headline “<a href="https://www.unz.com/print/Colliers-1944aug05-00014/">The Wounded Come Home</a>.” No mention was made of the fact that she was the only female journalist on the ground at Omaha Beach.</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/-xNsGghd_zMrTABSJs97A9Eq7FA=/fit-in/1072x0/filters:focal(1912x2385:1913x2386)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/bc/dc/bcdc7b2e-aca9-4271-aaaf-d897efc4b12d/gettyimages-107806105.jpg" alt="A 1946 photo of Gellhorn" loading="lazy">
  

  <figcaption>
    
      A 1946 photo of Gellhorn
    
    
      <span>FPG / Archive Photos / Getty Images</span>
    
  </figcaption>
</figure>
<p>Gellhorn’s success rankled Hemingway, resulting in a <a href="https://www.literaryladiesguide.com/literary-musings/marriage-of-martha-gellhorn-ernest-hemingway/">further fracturing</a> of their already faltering marriage. Gellhorn, too, was unhappy with her husband’s <a href="https://www.telegraph.co.uk/culture/4723541/The-war-for-Marthas-memory.html">abuse</a>, alcoholism and infidelity. The couple divorced in 1945.</p>

<p>After D-Day, Gellhorn continued to cover the war in Europe. For months, word had <a href="https://www.smithsonianmag.com/history/the-holocaust-comic-that-brought-americans-into-the-nazis-gas-chambers-180980114/">filtered out</a> about the horrors unfolding at Dachau and other Nazi concentration camps. Gellhorn, one of the <a href="https://www.dw.com/en/the-liberation-of-dachau-75-years-ago/a-53270700">first journalists</a> to tour Dachau following its liberation in April 1945, thought she was prepared for the experience but quickly realized otherwise.</p>

<p>In her <em>Collier’s</em> article, titled “<a href="http://www.oldmagazinearticles.com/war-correspondent-martha-gellhorn-at-DACHAU-death-camp-pdf">Dachau: Experimental Murder</a>,” Gellhorn described the shock of what she had witnessed: “There suddenly but never to be believed were the bodies of the dead. They were everywhere. There were piles of them inside the oven room, outside the door and alongside the building. They were all naked, and behind the crematorium, the ragged clothing of the dead was neatly stacked.”</p>

<p>As Moorehead <a href="https://archive.org/details/marthagellhornli0000moor/page/284/mode/1up?q=%22something+changed%22">writes</a> in her biography of Gellhorn, “Something changed for [Gellhorn] that day; something to do with what she felt about memory and the past, and her own sense of optimism, and perhaps even about being Jewish. It was in Dachau, she said, that she really understood for the first time the true evil of man.”</p>

<figure>
  
    <img src="https://th-thumbnailer.cdn-si-edu.com/dPR7ZN8C9M649UXMvn7A8FrIu7Q=/fit-in/1072x0/filters:focal(2147x1329:2148x1330)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/9a/5f/9a5fc800-bb95-4dd3-8cda-0b9ea3de8a6c/gettyimages-515451612.jpg" alt="Gellhorn and Virginia Cowles attend the opening night of their play" loading="lazy">
  

  <figcaption>
    
      Gellhorn (third from left) later co-wrote a play about her wartime experiences.
    
    
      <span>Bettman via Getty Images</span>
    
  </figcaption>
</figure>
<p>Despite seeing so much death and destruction, Gellhorn continued to pursue her career as a war correspondent. She covered conflicts around the globe, including the <a href="https://granta.com/the-invasion-of-panama/">U.S. invasion of Panama</a> in 1989. With both her health and her vision failing, she retired to her apartment in London. Stricken with ovarian cancer and unable to perform even simple tasks, she swallowed a <a href="https://www.newyorker.com/news/letter-from-the-uk/a-memorial-for-the-remarkable-martha-gellhorn">cyanide pill</a> and <a href="https://www.nytimes.com/1998/02/17/arts/martha-gellhorn-daring-writer-dies-at-89.html">died</a> at age 89 in February 1998.</p>

<p>Moorehead revered Gellhorn and even named her daughter after the legendary journalist, who also <a href="https://www.amazon.com/stores/Martha-Gellhorn/author/B001H6PSGC?ref=ap_rdr&amp;isDramIntegrated=true&amp;shoppingPortalEnabled=true">wrote</a> both fiction and nonfiction books. But the biographer acknowledges that Gellhorn could be a difficult person.</p>

<p>“I admired her hugely,” Moorehead says. “I was a bit scared of her, though. She was very formidable. I thought she was a terrific reporter, and she was really brave. She could be quite tough, quite cruel, quite hard. She was very impatient with people not taking responsibility for themselves. She felt you had to face up to life and get on with it. She was old school and tough on herself.”</p>

<p>Gellhorn remained a reporter until the very last, squeezing in a final reporting <a href="https://www.lrb.co.uk/the-paper/v18/n16/martha-gellhorn/mean-streets-of-salvador">trip to Brazil</a> at age 85. She always wanted to know the story and how it affected those who were part of it.</p>

<p>“At the end of her life, she would invite people—mostly young journalists—over,” Moorehead says. “You’d go there for a drink. You’d take the lift up to her flat, and she would be leaning against the door. She’d give you a drink and ask you about yourself.”</p>

      
        
  <div>
      <p>Get the latest <strong>History</strong> stories in your inbox?</p>
      
    </div>


      

      
      

      
        


  
    
      
    
  

  


      

      
      

      
        
          <section>
            <nav>Filed Under:
              
                
                  <a href="https://www.smithsonianmag.com/tag/american-history/">American History</a>, 
                
              
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/american-writers/">American Writers</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/ernest-hemingway/">Ernest Hemingway</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/holocaust/">Holocaust</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/journalism/">Journalism</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/magazines/">Magazines</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/military/">Military</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/us-military/">US Military</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/warfare/">Warfare</a>, 
                
              
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/womens-history/">Women's History</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/world-war-ii/">World War II</a>, 
                
              
                
                  <a href="https://www.smithsonianmag.com/tag/writers/">Writers</a>
                
              
            </nav>
          </section>
        
      

    </div>

    

  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Roads of Ancient Rome Visualized in the Style of Modern Subway Maps (407 pts)]]></title>
            <link>https://www.openculture.com/2024/06/the-roads-of-ancient-rome-visualized-in-modern-subway-maps.html</link>
            <guid>40597216</guid>
            <pubDate>Thu, 06 Jun 2024 13:32:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2024/06/the-roads-of-ancient-rome-visualized-in-modern-subway-maps.html">https://www.openculture.com/2024/06/the-roads-of-ancient-rome-visualized-in-modern-subway-maps.html</a>, See on <a href="https://news.ycombinator.com/item?id=40597216">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" fetchpriority="high" decoding="async" src="https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938.png" alt="" width="1000" height="753" srcset="https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938.png 1000w, https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938-360x271.png 360w, https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938-240x181.png 240w, https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938-768x578.png 768w" sizes="(max-width: 1000px) 100vw, 1000px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938.png" data-srcset="https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938.png 1000w, https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938-360x271.png 360w, https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938-240x181.png 240w, https://cdn8.openculture.com/2024/06/05213309/Rome_III-01-1-e1496887332938-768x578.png 768w"></p>
<p>Sasha Tru­bet­skoy, for­mer­ly an under­grad at U. Chica­go, has cre­at­ed a <a href="https://sashamaps.net/docs/maps/roman-roads-original/">“sub­way-style dia­gram of the major Roman roads, based on the Empire of ca. 125 AD.”</a> Draw­ing on&nbsp;<a href="http://orbis.stanford.edu/">Stanford’s ORBIS mod­el</a>, <a href="https://pelagios.org/case-studies/customise-maps-ancient-greece-rome/">The Pela­gios Project</a>, and the <a href="https://roadsofromanbritain.org/antonine.html">Anto­nine Itin­er­ary</a>,&nbsp;Tru­bet­skoy’s map com­bines well-known his­toric roads, like the <a href="https://en.wikipedia.org/wiki/Appian_Way">Via Appia</a>, with less­er-known ones (in somes cas­es giv­en imag­ined names). If you want to get a sense of scale, it would take,&nbsp;Tru­bet­skoy tells us,&nbsp;“two months to walk on foot from Rome to Byzan­tium. If you had a horse, it would only take you a month.”</p>
<p>You can <a href="https://sashamaps.net/images/roman_roads_original.png">view the map in a larg­er for­mat here</a>. And if you&nbsp;<a href="https://sashat.itch.io/roman-roads/purchase">fol­low this link</a>&nbsp;and send Tru­bet­skoy a few bucks, he can email you a crisp PDF for print­ing. Find more focused, relat­ed maps by&nbsp;Tru­bet­skoy right here:</p>
<ul>
<li><a title="Permanent Link to The Roman Roads of Britain Visualized as a Subway Map" href="http://www.openculture.com/2017/08/the-roman-roads-of-britain-visualized-as-a-subway-map.html" rel="bookmark">The Roman Roads of Britain Visu­al­ized as a Sub­way Map</a></li>
<li><a title="Permanent Link to The Roman Roads of Spain &amp; Portugal Visualized as a Subway Map: Ancient History Meets Modern Graphic Design" href="http://www.openculture.com/2019/03/the-roman-roads-of-spain-portugal-visualized-as-a-subway-map.html" rel="bookmark">The Roman Roads of Spain &amp; Por­tu­gal Visu­al­ized as a Sub­way Map: Ancient His­to­ry Meets Mod­ern Graph­ic Design</a></li>
<li><a title="Permanent Link to All the Roman Roads of Italy, Visualized as a Modern Subway Map" href="http://www.openculture.com/2018/07/roman-roads-italy-visualized-modern-subway-map.html" rel="bookmark">All the Roman Roads of Italy, Visu­al­ized as a Mod­ern Sub­way Map</a></li>
</ul><p>
If you would like to sign up for Open Culture’s free email newslet­ter,&nbsp;<a href="https://www.openculture.com/newsletter-signup">please find it here</a>. Or fol­low our posts on <a href="https://www.threads.net/@openculture">Threads</a>, <a href="https://www.facebook.com/openculture">Face­book</a>, <a href="https://bsky.app/profile/openculture.bsky.social">BlueSky</a> or <a href="https://toot.community/@openculture">Mastodon</a>.

<i><span>If you would like to sup­port the mis­sion of Open Cul­ture, con­sid­er <a href="https://bit.ly/3EBHjtX">mak­ing&nbsp;a dona­tion to our site</a>. It’s hard to rely 100% on ads, and your <a href="https://bit.ly/3EBHjtX">con­tri­bu­tions</a> will help us con­tin­ue pro­vid­ing the best free cul­tur­al and edu­ca­tion­al mate­ri­als to learn­ers every­where. You can con­tribute through <a href="https://www.openculture.com/help-fund-open-culture">Pay­Pal</a>, <a href="https://bit.ly/3eB2GRB">Patre­on</a>, and Ven­mo (@openculture). Thanks!</span></i><i></i></p><p><strong>Relat­ed Con­tent:</strong></p>
<p><a title="Permanent Link to The Roman Roads and Bridges You Can Still Travel Today" href="https://www.openculture.com/2022/12/the-roman-roads-and-bridges-you-can-still-travel-today.html" rel="bookmark">The Roman Roads and Bridges You Can Still Trav­el Today</a></p>
<p><a title="Permanent Link to An Interactive Map Shows Just How Many Roads Actually Lead to Rome" href="https://www.openculture.com/2018/05/an-interactive-map-shows-just-how-many-roads-actually-lead-to-rome.html" rel="bookmark">An Inter­ac­tive Map Shows Just How Many Roads Actu­al­ly Lead to Rome</a></p>
<p><a title="Permanent Link to How to Make Roman Concrete, One of Human Civilization’s Longest-Lasting Building Materials" href="https://www.openculture.com/2022/12/how-to-make-roman-concrete-one-of-human-civilizations-longest-lasting-building-materials.html" rel="bookmark">How to Make Roman Con­crete, One of Human Civilization’s Longest-Last­ing Build­ing Mate­ri­als</a></p>


<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Super Heavy has splashed down in The Gulf of Mexico (702 pts)]]></title>
            <link>https://twitter.com/SpaceX/status/1798701489097183286</link>
            <guid>40596883</guid>
            <pubDate>Thu, 06 Jun 2024 13:02:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/SpaceX/status/1798701489097183286">https://twitter.com/SpaceX/status/1798701489097183286</a>, See on <a href="https://news.ycombinator.com/item?id=40596883">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Synthesize TikZ Graphics Programs for Scientific Figures and Sketches (101 pts)]]></title>
            <link>https://github.com/potamides/DeTikZify</link>
            <guid>40596711</guid>
            <pubDate>Thu, 06 Jun 2024 12:42:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/potamides/DeTikZify">https://github.com/potamides/DeTikZify</a>, See on <a href="https://news.ycombinator.com/item?id=40596711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeTi<em>k</em>Zify<br><sub><sup>Synthesizing Graphics Programs for Scientific Figures and Sketches with Ti<em>k</em>Z</sup></sub></h2><a id="user-content-detikzifysynthesizing-graphics-programs-for-scientific-figures-and-sketches-with-tikz" aria-label="Permalink: DeTikZify" href="#detikzifysynthesizing-graphics-programs-for-scientific-figures-and-sketches-with-tikz"></a></p>
<p dir="auto"><a href="https://arxiv.org/abs/2405.15306" rel="nofollow"><img src="https://camo.githubusercontent.com/19b4fa306c699ab3d009c8b335556282d14792dee6a856b12eab24ceea168bce/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f566965772532306f6e25323061725869762d4233314231423f6c6f676f3d6172786976266c6162656c436f6c6f723d67726179" alt="arXiv" data-canonical-src="https://img.shields.io/badge/View%20on%20arXiv-B31B1B?logo=arxiv&amp;labelColor=gray"></a>
<a href="https://huggingface.co/collections/nllg/detikzify-664460c521aa7c2880095a8b" rel="nofollow"><img src="https://camo.githubusercontent.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" alt="Hugging Face" data-canonical-src="https://img.shields.io/badge/View%20on%20Hugging%20Face-blue?labelColor=gray&amp;logo=data:image/png;base64,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"></a>
<a href="https://colab.research.google.com/drive/1hPWqucbPGTavNlYvOBvSNBAwdcPZKe8F" rel="nofollow"><img src="https://camo.githubusercontent.com/f5e0d0538a9c2972b5d413e0ace04cecd8efd828d133133933dfffec282a4e1b/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667" alt="Colab" data-canonical-src="https://colab.research.google.com/assets/colab-badge.svg"></a></p>
<p dir="auto">Creating high-quality scientific figures can be time-consuming and challenging,
even though sketching ideas on paper is relatively easy. Furthermore,
recreating existing figures that are not stored in formats preserving semantic
information is equally complex. To tackle this problem, we introduce
<a href="https://github.com/potamides/DeTikZify">DeTi<em>k</em>Zify</a>, a novel multimodal
language model that automatically synthesizes scientific figures as
semantics-preserving <a href="https://github.com/pgf-tikz/pgf">Ti<em>k</em>Z</a> graphics
programs based on sketches and existing figures. We also introduce an
MCTS-based inference algorithm that enables DeTi<em>k</em>Zify to iteratively refine
its outputs without the need for additional training.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description Showcase.mp4">Showcase.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/53401822/334310983-203d2853-0b5c-4a2b-9d09-3ccb65880cd3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDIxMDMsIm5iZiI6MTcxNzc0MTgwMywicGF0aCI6Ii81MzQwMTgyMi8zMzQzMTA5ODMtMjAzZDI4NTMtMGI1Yy00YTJiLTlkMDktM2NjYjY1ODgwY2QzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA2MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ0ZDlmMmRkYTE4MDNkMjAxNjI5MTQyM2VmYzhjMmZlNjc3MjVjZjNmNDY0MTZkYjE1ZDg1YjQ5MTEyYzdjYjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8QZh4mrcjXWfvT96J908iw3vjVYQ6bdopfIOiceW7js" data-canonical-src="https://private-user-images.githubusercontent.com/53401822/334310983-203d2853-0b5c-4a2b-9d09-3ccb65880cd3.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTc3NDIxMDMsIm5iZiI6MTcxNzc0MTgwMywicGF0aCI6Ii81MzQwMTgyMi8zMzQzMTA5ODMtMjAzZDI4NTMtMGI1Yy00YTJiLTlkMDktM2NjYjY1ODgwY2QzLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA2MDclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNjA3VDA2MzAwM1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ0ZDlmMmRkYTE4MDNkMjAxNjI5MTQyM2VmYzhjMmZlNjc3MjVjZjNmNDY0MTZkYjE1ZDg1YjQ5MTEyYzdjYjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.8QZh4mrcjXWfvT96J908iw3vjVYQ6bdopfIOiceW7js" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">If you encounter difficulties with installation and inference on your own
hardware, consider visiting our <a href="https://huggingface.co/spaces/nllg/DeTikZify" rel="nofollow">Hugging Face
Space</a> (if the space is
restarting the first run might take 10-15 minutes to download and load the
model). Should you experience long queues, you have the option to
<a href="https://huggingface.co/spaces/nllg/DeTikZify?duplicate=true" rel="nofollow">duplicate</a> it
with a paid private GPU runtime for a more seamless experience. Additionally,
you can try our demo on <a href="https://colab.research.google.com/drive/1hPWqucbPGTavNlYvOBvSNBAwdcPZKe8F" rel="nofollow">Google
Colab</a>.
However, setting up the environment there might take some time, and the free
tier only supports inference for the 1b models. Do not forget to read our
<a href="https://github.com/potamides/DeTikZify/tree/main/detikzify/webui#usage-tips">usage tips</a>!</p>
</div>
<p dir="auto">The Python package of DeTi<em>k</em>Zify can be easily installed using
<a href="https://pip.pypa.io/en/stable" rel="nofollow">pip</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install 'detikzify @ git+https://github.com/potamides/DeTikZify'"><pre>pip install <span><span>'</span>detikzify @ git+https://github.com/potamides/DeTikZify<span>'</span></span></pre></div>
<p dir="auto">Or, if your goal is to run the included <a href="https://github.com/potamides/DeTikZify/blob/main/examples">examples</a>, clone the
repository and install it in editable mode like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/potamides/DeTikZify
pip install -e DeTikZify"><pre>git clone https://github.com/potamides/DeTikZify
pip install -e DeTikZify</pre></div>
<p dir="auto">In addition, DeTi<em>k</em>Zify requires a full
<a href="https://www.tug.org/texlive" rel="nofollow">TeX Live 2023</a> installation,
<a href="https://www.ghostscript.com/" rel="nofollow">ghostscript</a>, and,
<a href="https://poppler.freedesktop.org/" rel="nofollow">poppler</a> which you have to install through
your package manager or via other means.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">If all required dependencies are installed, the full range of DeTi<em>k</em>Zify
features such as compiling, rendering, and saving Ti<em>k</em>Z graphics, and
MCTS-based inference can be accessed through its programming interface:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from operator import itemgetter

from detikzify.model import load
from detikzify.infer import DetikzifyPipeline
import torch

image = &quot;https://w.wiki/A7Cc&quot;
pipeline = DetikzifyPipeline(*load(
    base_model=&quot;nllg/detikzify-ds-7b&quot;,
    device_map=&quot;auto&quot;,
    torch_dtype=torch.bfloat16,
))

# generate a single TikZ program
fig = pipeline.sample(image=image)

# if it compiles, rasterize it and show it
if fig.is_rasterizable:
    fig.rasterize().show()

# run MCTS for 10 minutes and generate multiple TikZ programs
figs = set()
for score, fig in pipeline.simulate(image=image, timeout=600):
    figs.add((score, fig))

# save the best TikZ program
best = sorted(figs, key=itemgetter(0))[-1][1]
best.save(&quot;fig.tex&quot;)"><pre><span>from</span> <span>operator</span> <span>import</span> <span>itemgetter</span>

<span>from</span> <span>detikzify</span>.<span>model</span> <span>import</span> <span>load</span>
<span>from</span> <span>detikzify</span>.<span>infer</span> <span>import</span> <span>DetikzifyPipeline</span>
<span>import</span> <span>torch</span>

<span>image</span> <span>=</span> <span>"https://w.wiki/A7Cc"</span>
<span>pipeline</span> <span>=</span> <span>DetikzifyPipeline</span>(<span>*</span><span>load</span>(
    <span>base_model</span><span>=</span><span>"nllg/detikzify-ds-7b"</span>,
    <span>device_map</span><span>=</span><span>"auto"</span>,
    <span>torch_dtype</span><span>=</span><span>torch</span>.<span>bfloat16</span>,
))

<span># generate a single TikZ program</span>
<span>fig</span> <span>=</span> <span>pipeline</span>.<span>sample</span>(<span>image</span><span>=</span><span>image</span>)

<span># if it compiles, rasterize it and show it</span>
<span>if</span> <span>fig</span>.<span>is_rasterizable</span>:
    <span>fig</span>.<span>rasterize</span>().<span>show</span>()

<span># run MCTS for 10 minutes and generate multiple TikZ programs</span>
<span>figs</span> <span>=</span> <span>set</span>()
<span>for</span> <span>score</span>, <span>fig</span> <span>in</span> <span>pipeline</span>.<span>simulate</span>(<span>image</span><span>=</span><span>image</span>, <span>timeout</span><span>=</span><span>600</span>):
    <span>figs</span>.<span>add</span>((<span>score</span>, <span>fig</span>))

<span># save the best TikZ program</span>
<span>best</span> <span>=</span> <span>sorted</span>(<span>figs</span>, <span>key</span><span>=</span><span>itemgetter</span>(<span>0</span>))[<span>-</span><span>1</span>][<span>1</span>]
<span>best</span>.<span>save</span>(<span>"fig.tex"</span>)</pre></div>
<p dir="auto">For interactive use and additional usage tips, we recommend checking out our
<a href="https://github.com/potamides/DeTikZify/blob/main/detikzify/webui">web UI</a>, which can be started from the command line (use
<code>--help</code> for a list of all options):</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m detikzify.webui --light"><pre>python -m detikzify.webui --light</pre></div>
<p dir="auto">More involved examples, for example for evaluation and training, can be found
in the <a href="https://github.com/potamides/DeTikZify/blob/main/examples">examples</a> folder.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Model Weights &amp; Datasets</h2><a id="user-content-model-weights--datasets" aria-label="Permalink: Model Weights &amp; Datasets" href="#model-weights--datasets"></a></p>
<p dir="auto">We upload all our models and datasets to the <a href="https://huggingface.co/collections/nllg/detikzify-664460c521aa7c2880095a8b" rel="nofollow">Hugging Face
Hub</a>.
However, please note that for the public release of the DaTi<em>k</em>Z<sub>v2</sub>
dataset, we had to remove a considerable portion of Ti<em>k</em>Z drawings originating
from <a href="https://arxiv.org/" rel="nofollow">arXiv</a>, as the <a href="https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html" rel="nofollow">arXiv non-exclusive
license</a> does
not permit redistribution. We do, however, release our <a href="https://github.com/potamides/DaTikZ">dataset creation
scripts</a> and encourage anyone to recreate
the full version of DaTi<em>k</em>Z<sub>v2</sub> themselves.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments</h2><a id="user-content-acknowledgments" aria-label="Permalink: Acknowledgments" href="#acknowledgments"></a></p>
<p dir="auto">The implementation of the model architecture is largely based on
<a href="https://github.com/haotian-liu/LLaVA">LLaVA</a>. Our MCTS implementation takes
heavy inspiration from
<a href="https://github.com/namin/llm-verified-with-monte-carlo-tree-search">VerMCTS</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AeroSpace is an i3-like tiling window manager for macOS (361 pts)]]></title>
            <link>https://github.com/nikitabobko/AeroSpace</link>
            <guid>40596689</guid>
            <pubDate>Thu, 06 Jun 2024 12:39:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nikitabobko/AeroSpace">https://github.com/nikitabobko/AeroSpace</a>, See on <a href="https://news.ycombinator.com/item?id=40596689">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"> <p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/nikitabobko/AeroSpace/blob/main/xcode-app-bundle-launcher/Assets.xcassets/AppIcon.appiconset/icon.png"><img src="https://github.com/nikitabobko/AeroSpace/raw/main/xcode-app-bundle-launcher/Assets.xcassets/AppIcon.appiconset/icon.png" width="40%" height="40%"></a></p>
<div dir="auto"><h2 tabindex="-1" dir="auto">AeroSpace Beta <a href="https://github.com/nikitabobko/AeroSpace/actions/workflows/build.yml"><img src="https://github.com/nikitabobko/AeroSpace/actions/workflows/build.yml/badge.svg?branch=main" alt="Build"></a></h2><a id="user-content-aerospace-beta-" aria-label="Permalink: AeroSpace Beta " href="#aerospace-beta-"></a></div>
<p dir="auto">AeroSpace is an i3-like tiling window manager for macOS</p>
<ul dir="auto">
<li><a href="https://www.youtube.com/watch?v=UOl7ErqWbrk" rel="nofollow">YouTube Demo</a></li>
<li><a href="https://nikitabobko.github.io/AeroSpace/guide" rel="nofollow">AeroSpace Guide</a></li>
<li><a href="https://nikitabobko.github.io/AeroSpace/commands" rel="nofollow">AeroSpace Commands</a></li>
<li><a href="https://nikitabobko.github.io/AeroSpace/config-examples" rel="nofollow">AeroSpace Config Examples</a></li>
<li><a href="https://nikitabobko.github.io/AeroSpace/goodness" rel="nofollow">AeroSpace Goodness</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Project status</h2><a id="user-content-project-status" aria-label="Permalink: Project status" href="#project-status"></a></p>
<p dir="auto">Public Beta. Feedback is very much welcome</p>
<ul dir="auto">
<li>I encourage you to try AeroSpace and file GitHub issues if something doesn't work for you</li>
<li>I already use AeroSpace on daily basis and I'm happy with it</li>
<li><a href="https://nikitabobko.github.io/AeroSpace/guide" rel="nofollow">The documentation</a> covers all major things you need to know</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key features</h2><a id="user-content-key-features" aria-label="Permalink: Key features" href="#key-features"></a></p>
<ul dir="auto">
<li><strong>Manual</strong> tiling window manager based on a <a href="https://nikitabobko.github.io/AeroSpace/guide#tree" rel="nofollow">tree paradigm</a></li>
<li><a href="https://i3wm.org/" rel="nofollow">i3</a> inspired</li>
<li>AeroSpace employs its <a href="https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces" rel="nofollow">own emulation of virtual workspaces</a> instead of relying on native macOS Spaces due to <a href="https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces" rel="nofollow">their considerable limitations</a></li>
<li>Plain text configuration (dotfiles friendly). See: <a href="https://nikitabobko.github.io/AeroSpace/config-examples#default-config" rel="nofollow">default-config.toml</a></li>
<li>CLI scriptable</li>
<li>Doesn't require disabling SIP (System Integrity Protection)</li>
<li><a href="https://nikitabobko.github.io/AeroSpace/guide#multiple-monitors" rel="nofollow">Proper multi-monitor support</a> (i3-like paradigm)</li>
<li>Status menu icon displays current workspace name</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Install via <a href="https://brew.sh/" rel="nofollow">Homebrew</a> to get autoupdates (Preferred)</p>
<div data-snippet-clipboard-copy-content="brew install --cask nikitabobko/tap/aerospace"><pre><code>brew install --cask nikitabobko/tap/aerospace
</code></pre></div>
<p dir="auto"><a href="https://nikitabobko.github.io/AeroSpace/guide#manual-installation" rel="nofollow">Manual installation</a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">By using AeroSpace, you acknowledge that it's not <a href="https://developer.apple.com/documentation/security/notarizing_macos_software_before_distribution" rel="nofollow">notarized</a>.</p>
<p dir="auto">Notarization is a "security" feature by Apple.
You send binaries to Apple, and they either approve the binaries or not.
In reality, notarization is about building binaries the way Apple likes it.</p>
<p dir="auto">Let's be honest.
Tiling window manager is not something Apple will be totally ok with.
Even if they approve one version, it doesn't mean that they won't revoke it (yes, they can do it), or approve further versions.</p>
<p dir="auto">I don't have anything against notarization as a concept.
I specifically don't like the way Apple does notarization.
I don't have time to fight Apple.</p>
<p dir="auto"><a href="https://github.com/nikitabobko/homebrew-tap/blob/main/Casks/aerospace.rb">Homebrew installation script</a> is configured to
automatically delete <code>com.apple.quarantine</code> attribute, that's why the app should work out of the box, without any warnings that
"Apple cannot check AeroSpace for malicious software"</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing, creating issues, submitting pull requests</h2><a id="user-content-contributing-creating-issues-submitting-pull-requests" aria-label="Permalink: Contributing, creating issues, submitting pull requests" href="#contributing-creating-issues-submitting-pull-requests"></a></p>
<p dir="auto">See: <a href="https://github.com/nikitabobko/AeroSpace/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto">A notes on how to setup the project, build it, how to run the tests, etc. can be found here: <a href="https://github.com/nikitabobko/AeroSpace/blob/main/dev-docs/development.md">dev-docs/development.md</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Values of the project</h2><a id="user-content-values-of-the-project" aria-label="Permalink: Values of the project" href="#values-of-the-project"></a></p>
<p dir="auto"><strong>Values</strong></p>
<ul dir="auto">
<li>AeroSpace is targeted at advanced users and developers</li>
<li>Keyboard centric</li>
<li>Breaking changes (configuration files, CLI, behavior) are avoided as much as possible, but it must not let the software stagnate.
Thus breaking changes can happen, but with careful considerations and helpful message.
<a href="https://semver.org/" rel="nofollow">Semver</a> major version is bumped in case of a breaking change (It's all guaranteed once AeroSpace reaches 1.0 version, until then breaking changes just happen)</li>
<li>AeroSpace doesn't use GUI, unless necessarily
<ul dir="auto">
<li>AeroSpace will never provide a GUI for configuration.
For advanced users, it's easier to edit a configuration file in text editor rather than navigating through checkboxes in GUI.</li>
<li>Status menu icon is ok, because visual feedback is needed</li>
</ul>
</li>
<li>Provide <em>practical</em> features. Fancy appearance features are not <em>practical</em> (e.g. window borders, transparency, etc)</li>
<li>If "dark magic" (aka "private APIs", "code injections", etc) can be avoided, it must be avoided
<ul dir="auto">
<li>Right now, AeroSpace uses only a single private API to get window ID of accessibility object <code>_AXUIElementGetWindow</code>.
Everything else is <a href="https://developer.apple.com/documentation/applicationservices/axuielement_h" rel="nofollow">macOS public accessibility API</a>.</li>
<li>AeroSpace will never require you to disable SIP (System Integrity Protection). For example, yabai <a href="https://github.com/koekeishiya/yabai/issues/1863" data-hovercard-type="issue" data-hovercard-url="/koekeishiya/yabai/issues/1863/hovercard">requires you to disable SIP</a> to use some of its features.
AeroSpace will either find another way (such as <a href="https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces" rel="nofollow">emulation of workspaces</a>) or will not implement this feature at all (window transparency and window shadowing are not <em>practical</em> features)</li>
</ul>
</li>
</ul>
<p dir="auto"><strong>Non Values</strong></p>
<ul dir="auto">
<li>Play nicely with existing macOS features. If limitations are imposed then AeroSpace won't play nicely with existing macOS features
<ul dir="auto">
<li>E.g. AeroSpace doesn't acknowledge the existence of macOS Spaces, and it uses <a href="https://nikitabobko.github.io/AeroSpace/guide#emulation-of-virtual-workspaces" rel="nofollow">emulation of its own workspaces</a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tip of the day</h2><a id="user-content-tip-of-the-day" aria-label="Permalink: Tip of the day" href="#tip-of-the-day"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="defaults write -g NSWindowShouldDragOnGesture YES"><pre>defaults write -g NSWindowShouldDragOnGesture YES</pre></div>
<p dir="auto">Now, you can move windows by holding <code>ctrl</code>+<code>cmd</code> and dragging any part of the window (not necessarily the window title)</p>
<p dir="auto">Source: <a href="https://www.reddit.com/r/MacOS/comments/k6hiwk/keyboard_modifier_to_simplify_click_drag_of/" rel="nofollow">reddit</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Related projects</h2><a id="user-content-related-projects" aria-label="Permalink: Related projects" href="#related-projects"></a></p>
<ul dir="auto">
<li><a href="https://github.com/ianyh/Amethyst">Amethyst</a></li>
<li><a href="https://github.com/koekeishiya/yabai">yabai</a></li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brain overgrowth dictates autism severity, new research suggests (123 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-06-brain-overgrowth-dictates-autism-severity.html</link>
            <guid>40596535</guid>
            <pubDate>Thu, 06 Jun 2024 12:18:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-06-brain-overgrowth-dictates-autism-severity.html">https://medicalxpress.com/news/2024-06-brain-overgrowth-dictates-autism-severity.html</a>, See on <a href="https://news.ycombinator.com/item?id=40596535">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/brain-overgrowth-dicta-2.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/brain-overgrowth-dicta-2.jpg" data-sub-html="Culturing BCOs from human iPSCs. Credit: <i>Molecular Autism</i> (2024). DOI: 10.1186/s13229-024-00602-8">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/brain-overgrowth-dicta-2.jpg" alt="Brain overgrowth dictates autism severity, new research suggests" title="Culturing BCOs from human iPSCs. Credit: Molecular Autism (2024). DOI: 10.1186/s13229-024-00602-8" width="800" height="496">
             <figcaption>
                Culturing BCOs from human iPSCs. Credit: <i>Molecular Autism</i> (2024). DOI: 10.1186/s13229-024-00602-8
            </figcaption>        </figure>
    </div><p>Some children with autism experience profound, lifelong difficulties like developmental delay, social struggles and even the inability to speak. Others experience more mild symptoms that improve with time.</p>

                                        
                                                                                  
                                        
                                                                                                                                    
<p>The disparity in outcomes has been a mystery to scientists, until now. <a href="https://doi.org/10.1186/s13229-024-00602-8" target="_blank">A new study</a>, published in <i>Molecular Autism</i> by researchers at University of California San Diego, is the first to shed light on the matter. Among its findings: The biological basis for these two subtypes of <a href="https://medicalxpress.com/tags/autism/" rel="tag">autism</a> develops in utero.</p>
<p>Researchers used blood-based stem cells from 10 toddlers, ages 1 through 4, with idiopathic autism (in which no single-gene cause was identified) to create brain cortical organoids (BCOs), or models of the fetal cortex. They also created BCOs from six neurotypical toddlers.</p>
<p>Often referred to as gray matter, the cortex lines the outside of the brain. It holds tens of billions of <a href="https://medicalxpress.com/tags/nerve+cells/" rel="tag">nerve cells</a> and is responsible for essential functions like consciousness, thinking, reasoning, learning, memory, emotions and sensory functions.</p>
<p>Among their findings: The BCOs of toddlers with autism were significantly larger—roughly 40 percent—than those of neurotypical controls, according to two rounds of study performed in different years (2021 and 2022). Each round involved the creation of hundreds of organoids from each patient.</p>

<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/brain-overgrowth-dicta.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2024/brain-overgrowth-dicta.jpg" data-sub-html="Brain cortical organoids (BCOs) created by Dr. Alysson Muotri shown in a 2019 file photo. Muotri and colleagues at the University of California San Diego used stem cells from toddlers with autism and created BCOs from them. The stem cells of toddlers with autism developed into larger BCOs, they discovered. Toddlers with autism also had larger brain volumes, according to MRI. Credit: UC San Diego Health Sciences">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/brain-overgrowth-dicta.jpg" alt="Brain overgrowth dictates autism severity, new research suggests" title="Brain cortical organoids (BCOs) created by Dr. Alysson Muotri shown in a 2019 file photo. Muotri and colleagues at the University of California San Diego used stem cells from toddlers with autism and created BCOs from them. The stem cells of toddlers with autism developed into larger BCOs, they discovered. Toddlers with autism also had larger brain volumes, according to MRI. Credit: UC San Diego Health Sciences">
             <figcaption>
                Brain cortical organoids (BCOs) created by Dr. Alysson Muotri shown in a 2019 file photo. Muotri and colleagues at the University of California San Diego used stem cells from toddlers with autism and created BCOs from them. The stem cells of toddlers with autism developed into larger BCOs, they discovered. Toddlers with autism also had larger brain volumes, according to MRI. Credit: UC San Diego Health Sciences
            </figcaption>        </figure>
    </div>
<p>The researchers also found that abnormal BCO growth in toddlers with autism correlated with their disease presentation. The larger a toddler's BCO size, the more severe their social and language symptoms were later in life, and the larger their <a href="https://medicalxpress.com/tags/brain+structure/" rel="tag">brain structure</a> on MRI. Toddlers with excessively enlarged BCOs showed greater-than-typical volume in social, language and sensory brain areas when compared to neurotypical peers.</p>

                                                                                                        <!-- Google middle Adsense block -->
    
                                                                                                                                            <p>"The bigger the brain, the better isn't necessarily true," said Alysson Muotri, Ph.D., director of the Sanford Stem Cell Institute (SSCI) Integrated Space Stem Cell Orbital Research Center at the university. The SSCI is directed by Catriona Jamieson, M.D., Ph.D., a leading physician-scientist in cancer stem cell biology whose research explores the fundamental question of how space alters cancer progression.</p>
<p>"We found that in the brain organoids from toddlers with profound autism, there are more cells and sometimes more neurons—and that's not always for the best," added Muotri, who is also a professor in the Departments of Pediatrics and Cellular and Molecular Medicine at the UC San Diego School of Medicine.</p>
<p>What's more, the BCOs of all children with autism, regardless of severity, grew roughly three times faster than those of neurotypical children. Some of the largest brain organoids—from children with the most severe, persistent cases of autism—also saw the accelerated formation of neurons. The more severe a toddler's autism, the quicker their BCO grew—sometimes to the point of developing an excess of neurons.</p>
<p>Eric Courchesne, Ph.D., a professor in the School of Medicine's Department of Neurosciences who co-led the research with Muotri, called the study "one of a kind." Matching data on children with autism—including their IQs, symptom severity and imaging like MRIs—with their corresponding BCOs or similar stem cell-derived models makes an incredible amount of sense, he said. But oddly enough, such research hadn't been undertaken ahead of their work.</p>

<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/brain-overgrowth-dicta-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2024/brain-overgrowth-dicta-1.jpg" data-sub-html="Comparing ASD and Control BCO size distributions and BCO size correlation with ADOS social symptom severity. Credit: <i>Molecular Autism</i> (2024). DOI: 10.1186/s13229-024-00602-8">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/brain-overgrowth-dicta-1.jpg" alt="Brain overgrowth dictates autism severity, new research suggests" title="Comparing ASD and Control BCO size distributions and BCO size correlation with ADOS social symptom severity. Credit: Molecular Autism (2024). DOI: 10.1186/s13229-024-00602-8">
             <figcaption>
                Comparing ASD and Control BCO size distributions and BCO size correlation with ADOS social symptom severity. Credit: <i>Molecular Autism</i> (2024). DOI: 10.1186/s13229-024-00602-8
            </figcaption>        </figure>
    </div>
<p>"The core symptoms of autism are social affective and <a href="https://medicalxpress.com/tags/communication+problems/" rel="tag">communication problems</a>," said Courchesne, who also serves as co-director of the UC San Diego Autism Center of Excellence. "We need to understand the underlying neurobiological causes of those challenges and when they begin. We are the first to design an autism stem cell study of this specific and central question."</p>

                                                                                                                                            <p>It's long been assumed that autism, a complex pool of progressive disorders, begins prenatally and involves multiple stages and processes. While no two people with autism are like—just as no two neurotypical people are—those with the neurodevelopmental condition can generally be grouped into two categories: those who have severe social struggles and require lifelong care, and may even be nonverbal, and those who have a milder version of the condition who eventually develop good language skills and <a href="https://medicalxpress.com/tags/social+relationships/" rel="tag">social relationships</a>.</p>
<p>Scientists haven't been able to ascertain why at least two groups of individuals with autism exist. They also haven't been able to prenatally identify children with autism, let alone predict how severe their condition might be.</p>
<p>Now that Courchesne and Muotri have established that brain overgrowth begins in the womb, they hope to pinpoint its cause, in a bid to develop a therapy that might ease intellectual and social functioning for those with the condition.</p>
<p>Co-authors of the study include Vani Taluja, Sanaz Nazari, Caitlin M. Aamodt, Karen Pierce, Kuaikuai Duan, Sunny Stophaeros, Linda Lopez, Cynthia Carter Barnes, Jaden Troxel, Kathleen Campbell, Tianyun Wang, Kendra Hoekzema, Evan E. Eichler, Joao V. Nani, Wirla Pontes, Sandra Sanchez Sanchez, Michael V. Lombardo and Janaina S. de Souza.</p>


                                                                                                                                                                            
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    Eric Courchesne et al, Embryonic origin of two ASD subtypes of social symptom severity: the larger the brain cortical organoid size, the more severe the social symptoms, <i>Molecular Autism</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1186/s13229-024-00602-8" target="_blank">DOI: 10.1186/s13229-024-00602-8</a>
																								
																								</p>
																							</div>
                                        											
																					
                                                                                                                        
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Brain overgrowth dictates autism severity, new research suggests (2024, June 5)
                                                 retrieved 6 June 2024
                                                 from https://medicalxpress.com/news/2024-06-brain-overgrowth-dictates-autism-severity.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
    </channel>
</rss>