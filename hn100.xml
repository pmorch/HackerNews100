<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 05 Oct 2024 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[It's Time to Stop Taking Sam Altman at His Word (278 pts)]]></title>
            <link>https://www.theatlantic.com/technology/archive/2024/10/sam-altman-mythmaking/680152/</link>
            <guid>41749371</guid>
            <pubDate>Sat, 05 Oct 2024 12:17:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/technology/archive/2024/10/sam-altman-mythmaking/680152/">https://www.theatlantic.com/technology/archive/2024/10/sam-altman-mythmaking/680152/</a>, See on <a href="https://news.ycombinator.com/item?id=41749371">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header data-event-module="hero"><div><div><p>Understand AI for what it is, not what it might become.</p></div><div><figure><div data-flatplan-lead_figure_media="true"><picture><img alt="Photograph of Sam Altman" sizes="(min-width: 976px) 976px, 100vw" srcset="https://cdn.theatlantic.com/thumbor/5jtLZfNIDl-KsJwr6YwXHLCzkXM=/0x0:4800x2700/750x422/media/img/mt/2024/10/HR_1258550648/original.jpg 750w, https://cdn.theatlantic.com/thumbor/2-NVcZYYD5N8WHQFdXM5loRmrfk=/0x0:4800x2700/828x466/media/img/mt/2024/10/HR_1258550648/original.jpg 828w, https://cdn.theatlantic.com/thumbor/7ixFRAfgrl84vzHdnpxCaHDVJpk=/0x0:4800x2700/960x540/media/img/mt/2024/10/HR_1258550648/original.jpg 960w, https://cdn.theatlantic.com/thumbor/ricZNVcgmocI2el_zD6Se8AiX3w=/0x0:4800x2700/976x549/media/img/mt/2024/10/HR_1258550648/original.jpg 976w, https://cdn.theatlantic.com/thumbor/NDekaLljTZvaed6nGxnEFm5oaIE=/0x0:4800x2700/1952x1098/media/img/mt/2024/10/HR_1258550648/original.jpg 1952w" src="https://cdn.theatlantic.com/thumbor/7ixFRAfgrl84vzHdnpxCaHDVJpk=/0x0:4800x2700/960x540/media/img/mt/2024/10/HR_1258550648/original.jpg" id="article-lead-image" width="960" height="540"></picture></div><figcaption data-flatplan-lead_figure_caption="true">SeongJoon Cho / Bloomberg / Getty</figcaption></figure></div></div><div><p><time datetime="2024-10-04T16:57:23Z" data-flatplan-timestamp="true">October 4, 2024, 12:57 PM ET</time> </p></div><gpt-ad format="injector" sizes-at-0="mobile-wide" targeting-pos="injector-article-start" sizes-at-976="desktop-wide"></gpt-ad></header><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">OpenAI announced this week that it has raised $6.6 billion in new funding and that the company is now valued at $157 billion overall. This is quite a feat for an organization that <a data-event-element="inline link" href="https://www.nytimes.com/2024/09/25/technology/mira-murati-openai.html">reportedly</a> burns through $7 billion a year—far more cash than it brings in—but it makes sense when you realize that OpenAI’s primary product isn’t technology. It’s stories.</p><p data-flatplan-paragraph="true">Case in point: Last week, CEO Sam Altman published an online manifesto titled “<a data-event-element="inline link" href="https://ia.samaltman.com/">The Intelligence Age</a>.” In it, he declares that the AI revolution is on the verge of unleashing boundless prosperity and radically improving human life. “We’ll soon be able to work with AI that helps us accomplish much more than we ever could without AI,” he writes. Altman expects that his technology will fix the climate, help humankind establish space colonies, and discover all of physics. He predicts that we may have an all-powerful superintelligence “in a few thousand days.” All we have to do is feed his technology enough energy, enough data, and enough chips.</p><p data-flatplan-paragraph="true">Maybe someday Altman’s ideas about AI will prove out, but for now, his approach is textbook Silicon Valley mythmaking. In these narratives, humankind is forever on the cusp of a technological breakthrough that will transform society for the better. The hard technical problems have basically been solved—all that’s left now are the details, which will surely be worked out through market competition and old-fashioned entrepreneurship. <em>Spend billions now; make trillions later! </em>This was the story of the dot-com boom in the 1990s, and of nanotechnology in the 2000s. It was the story of cryptocurrency and robotics in the 2010s. The technologies never quite work out like the Altmans of the world promise, but the stories keep regulators and regular people sidelined while the entrepreneurs, engineers, and investors build empires. (<em>The Atlantic</em> recently entered a corporate partnership with OpenAI.)</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/technology/archive/2023/06/ai-regulation-sam-altman-bill-gates/674278/">Read: AI doomerism is a decoy</a></p><p data-flatplan-paragraph="true">Despite the rhetoric, Altman’s products currently feel less like a glimpse of the future and more like the mundane, buggy present. ChatGPT and DALL-E were cutting-edge technology in 2022. People tried the chatbot and image generator for the first time and were astonished. Altman and his ilk spent the following year speaking in stage whispers about the awesome technological force that had just been unleashed upon the world. Prominent AI figures were among the thousands of people who signed an <a data-event-element="inline link" href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">open letter in March 2023</a> to urge a six-month pause in the development of large language models ( LLMs) so that humanity would have time to address the social consequences of the impending revolution. Those six months came and went. OpenAI and its competitors have released other models since then, and although tech wonks have dug into their purported advancements, for most people, the technology appears to have plateaued. <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2023/03/gpt4-release-rumors-hype-future-iterations/673396/">GPT-4</a> now looks less like the precursor to an all-powerful superintelligence and more like … well, any other chatbot.</p><p data-flatplan-paragraph="true">The technology itself seems much smaller once the novelty wears off. You can use a large language model to compose an email or a story—but not a particularly original one. The tools still hallucinate (meaning they confidently assert false information). They still fail in <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2024/05/google-search-ai-overview-health-webmd/678508/">embarrassing and unexpected ways</a>. Meanwhile, the web is filling up with useless “<a data-event-element="inline link" href="https://nymag.com/intelligencer/article/ai-generated-content-internet-online-slop-spam.html">AI slop</a>,” LLM-generated trash that costs practically nothing to produce and generates pennies of advertising revenue for the creator. We’re in a race to the bottom that everyone saw coming and no one is happy with. Meanwhile, the search for product-market fit at a scale that would justify all the inflated tech-company valuations keeps coming up short. Even OpenAI’s latest release, o1, was accompanied by a <a data-event-element="inline link" href="https://x.com/sama/status/1834283100639297910">caveat</a> from Altman that “it still seems more impressive on first use than it does after you spend more time with it.”</p><p data-flatplan-paragraph="true">In Altman’s rendering, this moment in time is just a waypoint, “the doorstep of the next leap in prosperity.” He still argues that the deep-learning technique that powers ChatGPT will effectively be able to solve any problem, at any scale, so long as it has enough energy, enough computational power, and enough data. Many computer scientists are <a data-event-element="inline link" href="https://press.princeton.edu/books/hardcover/9780691249131/ai-snake-oil?srsltid=AfmBOoq0PglTl-ZvYI9vexaTNJouMOR1cqpznKeo_-DOv-KWBWCvQrZu">skeptical of this claim</a>, maintaining that multiple significant scientific breakthroughs stand between us and artificial general intelligence. But Altman projects confidence that his company has it all well in hand, that science fiction will soon become reality. He may need <a data-event-element="inline link" href="https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0">$7 trillion</a> or so to realize his ultimate vision—not to mention <a data-event-element="inline link" href="https://www.bloomberg.com/news/articles/2024-07-18/sam-altman-s-helion-energy-promises-fusion-power-by-2028">unproven fusion-energy technology</a>—but that’s peanuts when compared with all the advances he is promising.</p><p data-flatplan-paragraph="true">There’s just one tiny problem, though: Altman is no physicist. He is a serial entrepreneur, and quite clearly a talented one. He is one of Silicon Valley’s most revered talent scouts. If you look at Altman’s breakthrough successes, they all pretty much revolve around connecting early start-ups with piles of investor cash, not any particular technical innovation.</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/technology/archive/2024/09/sam-altman-openai-for-profit/680031/">Read: OpenAI takes its mask off</a></p><p data-flatplan-paragraph="true">It’s remarkable how similar Altman’s rhetoric sounds to that of <a data-event-element="inline link" href="https://a16z.com/the-techno-optimist-manifesto/">his fellow billionaire techno-optimists</a>. The project of techno-optimism, for decades now, has been to insist that if we just have faith in technological progress and free the inventors and investors from pesky regulations such as <a data-event-element="inline link" href="https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html">copyright law</a> and <a data-event-element="inline link" href="https://www.ftc.gov/news-events/news/press-releases/2024/09/ftc-announces-crackdown-deceptive-ai-claims-schemes">deceptive marketing</a>, then the marketplace will work its magic and everyone will be better off. Altman has made nice with lawmakers, insisting that artificial intelligence requires responsible regulation. But the company’s response to proposed regulation seems to be “<a data-event-element="inline link" href="https://time.com/6288245/openai-eu-lobbying-ai-act/">no, not like that</a>.” Lord, grant us regulatory clarity—but <a data-event-element="inline link" href="https://www.theverge.com/2024/8/21/24225648/openai-letter-california-ai-safety-bill-sb-1047">not just yet.</a></p><p data-flatplan-paragraph="true">At a high enough level of abstraction, Altman’s entire job is to keep us all fixated on an imagined AI future so we don’t get too caught up in the underwhelming details of the present. Why focus on how AI is being used to <a data-event-element="inline link" href="https://www.theatlantic.com/newsletters/archive/2024/09/ai-is-triggering-a-child-sex-abuse-crisis/680053/">harass and exploit children</a> when you can imagine the ways it will make your life easier? It’s much more pleasant fantasizing about a benevolent future AI, one that fixes the problems wrought by climate change, than dwelling upon the phenomenal <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2024/09/microsoft-ai-oil-contracts/679804/">energy</a> and <a data-event-element="inline link" href="https://www.theatlantic.com/technology/archive/2024/03/ai-water-climate-microsoft/677602/">water consumption</a> of actually existing AI today.</p><p data-flatplan-paragraph="true">Remember, these technologies already have a track record. The world can and should evaluate them, and the people building them, based on their results and their effects, not solely on their supposed potential.</p></section><div data-event-module="footer"><p><h3>About the Author</h3></p><div><address id="article-writer-0" data-event-element="author" data-event-position="1" data-flatplan-bio="true"><div><p><a href="https://www.theatlantic.com/author/david-karpf/" data-label="https://www.theatlantic.com/author/david-karpf/" data-action="click author - name">David Karpf</a> is an associate professor in the School of Media and Public Affairs at the George Washington University.</p></div></address></div></div><gpt-ad format="injector" sizes-at-0="mobile-wide,native,house" targeting-pos="injector-most-popular" sizes-at-976="desktop-wide,native,house"></gpt-ad></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux from Scratch (182 pts)]]></title>
            <link>https://www.linuxfromscratch.org/index.html</link>
            <guid>41747966</guid>
            <pubDate>Sat, 05 Oct 2024 05:43:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.linuxfromscratch.org/index.html">https://www.linuxfromscratch.org/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41747966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

     <p>Linux From Scratch (LFS) is a project that provides you with
        step-by-step instructions for building your own custom Linux system,
        entirely from source code.
     </p>

     <p>Currently, the Linux From Scratch organization consists of the following
        subprojects:
     </p>

     <ul id="subs">

       <li><a href="https://www.linuxfromscratch.org/lfs/">LFS</a> :: Linux From Scratch is the main book, the
       base from which all other projects are derived.</li>
       
       <li><a href="https://www.linuxfromscratch.org/blfs/">BLFS</a> :: Beyond Linux From Scratch helps you
       extend your finished LFS installation into a more customized and usable
       system.</li>
       
       <li><a href="https://www.linuxfromscratch.org/alfs/">ALFS</a> :: Automated Linux From Scratch provides
       tools for automating and managing LFS and BLFS builds.</li>

       <li><a href="https://www.linuxfromscratch.org/hints/">Hints</a> :: The Hints project is a collection of
       documents that explain how to enhance your LFS system in ways that are
       not included in the LFS or BLFS books.</li>

       <li><a href="https://www.linuxfromscratch.org/patches/">Patches</a> :: The Patches project serves as a
       central repository for all patches useful to an LFS user.</li> 

       <li><a href="https://www.linuxfromscratch.org/lfs/LFS-EDITORS-GUIDE.html">LFS Editor's Guide</a> :: A document that 
       describes the LFS development process.</li> 

       <li><a href="https://www.linuxfromscratch.org/museum/">Museum</a> :: Copies of ancient LFS and BLFS versions.</li> 
     </ul>

       <br>
      
     </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple releases Depth Pro, an AI model that rewrites the rules of 3D vision (107 pts)]]></title>
            <link>https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/</link>
            <guid>41747863</guid>
            <pubDate>Sat, 05 Oct 2024 05:10:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/">https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/</a>, See on <a href="https://news.ycombinator.com/item?id=41747863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-10-04T18:52:31+00:00" datetime="2024-10-04T18:52:31+00:00">October 4, 2024 11:52 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="421" src="https://venturebeat.com/wp-content/uploads/2024/10/nuneybits_Flat_vector_design_of_the_Apple_logo_integrated_into__6d245237-c107-4fb8-b31c-e089bfd3f5f2-1.webp?w=750" alt="Credit: VentureBeat made with Midjourney"></p><p><span>Credit: VentureBeat made with Midjourney</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary">

		<article id="content">
			<div>
				<div id="boilerplate_2682874"><!-- wp:paragraph -->
<p><em>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav" data-type="link" data-id="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav">Learn More</a></em></p>
<!-- /wp:paragraph -->

<!-- wp:separator {"opacity":"css","className":"is-style-wide"} -->
<hr>
<!-- /wp:separator --></div><p><a href="https://machinelearning.apple.com/">Apple’s AI research team</a> has developed a new model that could significantly advance how machines perceive depth, potentially transforming industries ranging from augmented reality to autonomous vehicles.</p>



<p>The system, called&nbsp;<a href="https://arxiv.org/pdf/2410.02073">Depth Pro</a>, is able to generate detailed 3D depth maps from single 2D images in a fraction of a second—without relying on the camera data traditionally needed to make such predictions.</p>



<p>The technology, detailed in a research paper titled&nbsp;<em>“<a href="https://arxiv.org/pdf/2410.02073">Depth Pro: Sharp Monocular Metric Depth in Less Than a Second</a>,”</em>&nbsp;is a major leap forward in the field of monocular depth estimation, a process that uses just one image to infer depth.</p>



<p>This could have far-reaching applications across sectors where real-time spatial awareness is key. The model’s creators, led by Aleksei Bochkovskii and Vladlen Koltun, describe&nbsp;Depth Pro&nbsp;as one of the fastest and most accurate systems of its kind.</p>



<figure><img fetchpriority="high" decoding="async" width="830" height="853" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12%E2%80%AFAM.png?w=584" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 830w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,308 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,789 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=584,600 584w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=52,52 52w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,411 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,771 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.26.12 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,594 578w" sizes="(max-width: 830px) 100vw, 830px"><figcaption>A comparison of depth maps from Apple’s Depth Pro, Marigold, Depth Anything v2, and Metric3D v2. Depth Pro excels in capturing fine details like fur and birdcage wires, producing sharp, high-resolution depth maps in just 0.3 seconds, outperforming other models in accuracy and detail. (credit: arxiv.org)</figcaption></figure>







<p>Monocular depth estimation has long been a challenging task, requiring either multiple images or metadata like focal lengths to accurately gauge depth.</p>



<p>But&nbsp;Depth Pro&nbsp;bypasses these requirements, producing high-resolution depth maps in just 0.3 seconds on a standard GPU. The model can create 2.25-megapixel maps with exceptional sharpness, capturing even minute details like hair and vegetation that are often overlooked by other methods.</p>



<p>“These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction,” the researchers explain in their paper. This architecture allows the model to process both the overall context of an image and its finer details simultaneously—an enormous leap from slower, less precise models that came before it.</p>



<figure><img decoding="async" width="846" height="953" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18%E2%80%AFAM.png?w=533" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 846w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,338 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,865 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=533,600 533w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,451 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,845 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.34.18 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,651 578w" sizes="(max-width: 846px) 100vw, 846px"><figcaption>A comparison of depth maps from Apple’s Depth Pro, Depth Anything v2, Marigold, and Metric3D v2. Depth Pro excels in capturing fine details like the deer’s fur, windmill blades, and zebra’s stripes, delivering sharp, high-resolution depth maps in 0.3 seconds. (credit: arxiv.org)</figcaption></figure>



<h2 id="h-metric-depth-zero-shot-learning">Metric depth, zero-shot learning</h2>



<p>What truly sets&nbsp;Depth Pro&nbsp;apart is its ability to estimate both relative and absolute depth, a capability called “metric depth.”</p>



<p>This means that the model can provide real-world measurements, which is essential for applications like augmented reality (AR), where virtual objects need to be placed in precise locations within physical spaces.</p>



<p>And&nbsp;Depth Pro&nbsp;doesn’t require extensive training on domain-specific datasets to make accurate predictions—a feature known as “zero-shot learning.” This makes the model highly versatile. It can be applied to a wide range of images, without the need for the camera-specific data usually required in depth estimation models.</p>



<p>“Depth Pro produces metric depth maps with absolute scale on arbitrary images ‘in the wild’ without requiring metadata such as camera intrinsics,” the authors explain. This flexibility opens up a world of possibilities, from enhancing AR experiences to improving autonomous vehicles’ ability to detect and navigate obstacles.</p>



<p>For those curious to experience Depth Pro firsthand, a <a href="https://huggingface.co/spaces/akhaliq/depth-pro">live demo</a> is available on the Hugging Face platform.</p>



<figure><img decoding="async" width="1387" height="375" src="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50%E2%80%AFAM.png?w=800" alt="" srcset="https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png 1387w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=300,81 300w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=768,208 768w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=800,216 800w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=400,108 400w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=750,203 750w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=578,156 578w, https://venturebeat.com/wp-content/uploads/2024/10/Screenshot-2024-10-04-at-11.35.50 https://venturebeat.com/ai/apple-releases-depth-pro-an-ai-model-that-rewrites-the-rules-of-3d-vision/AM.png?resize=930,251 930w" sizes="(max-width: 1387px) 100vw, 1387px"><figcaption>A comparison of depth estimation models across multiple datasets. Apple’s Depth Pro ranks highest overall with an average rank of 2.5, outperforming models like Depth Anything v2 and Metric3D in accuracy across diverse scenarios. (credit: arxiv.org)</figcaption></figure>



<h2 id="h-real-world-applications-from-e-commerce-to-autonomous-vehicles">Real-world applications: From e-commerce to autonomous vehicles</h2>



<p>This versatility has significant implications for various industries. In e-commerce, for example,&nbsp;Depth Pro&nbsp;could allow consumers to see how furniture fits in their home by simply pointing their phone’s camera at the room. In the automotive industry, the ability to generate real-time, high-resolution depth maps from a single camera could improve how self-driving cars perceive their environment, boosting navigation and safety.</p>



<p>“The method should ideally produce metric depth maps in this zero-shot regime to accurately reproduce object shapes, scene layouts, and absolute scales,” the researchers write, emphasizing the model’s potential to reduce the time and cost associated with training more conventional AI models.</p>



<h2 id="h-tackling-the-challenges-of-depth-estimation">Tackling the challenges of depth estimation</h2>



<p>One of the toughest challenges in depth estimation is handling what are known as “flying pixels”—pixels that appear to float in mid-air due to errors in depth mapping.&nbsp;Depth Pro&nbsp;tackles this issue head-on, making it particularly effective for applications like 3D reconstruction and virtual environments, where accuracy is paramount.</p>



<p>Additionally,&nbsp;Depth Pro&nbsp;excels in boundary tracing, outperforming previous models in sharply delineating objects and their edges. The researchers claim it surpasses other systems “by a multiplicative factor in boundary accuracy,” which is key for applications that require precise object segmentation, such as image matting and medical imaging.</p>



<h2 id="h-open-source-and-ready-to-scale">Open-source and ready to scale</h2>



<p>In a move that could accelerate its adoption, Apple has made&nbsp;Depth Pro&nbsp;open-source. The code, along with pre-trained model weights, is <a href="https://github.com/apple/ml-depth-pro" target="_blank" rel="noreferrer noopener">available on GitHub</a>, allowing developers and researchers to experiment with and further refine the technology. The repository includes everything from the model’s architecture to pretrained checkpoints, making it easy for others to build on Apple’s work.</p>



<p>The research team is also encouraging further exploration of&nbsp;Depth Pro’s potential in fields like robotics, manufacturing, and healthcare. “We release code and weights at&nbsp;<a href="https://github.com/apple/ml-depth-pro" target="_blank" rel="noreferrer noopener">https://github.com/apple/ml-depth-pro</a>,”&nbsp;the authors write, signaling this as just the beginning for the model.</p>



<h2 id="h-what-s-next-for-ai-depth-perception">What’s next for AI depth perception</h2>



<p>As artificial intelligence continues to push the boundaries of what’s possible,&nbsp;<em>Depth Pro</em>&nbsp;sets a new standard in speed and accuracy for monocular depth estimation. Its ability to generate high-quality, real-time depth maps from a single image could have wide-ranging effects across industries that rely on spatial awareness.</p>



<p>In a world where AI is increasingly central to decision-making and product development,&nbsp;<em>Depth Pro</em>&nbsp;exemplifies how cutting-edge research can translate into practical, real-world solutions. Whether it’s improving how machines perceive their surroundings or enhancing consumer experiences, the potential uses for&nbsp;<em>Depth Pro</em>&nbsp;are broad and varied.</p>



<p>As the researchers conclude, “Depth Pro dramatically outperforms all prior work in sharp delineation of object boundaries, including fine structures such as hair, fur, and vegetation.” With its open-source release,&nbsp;<em>Depth Pro</em>&nbsp;could soon become integral to industries ranging from autonomous driving to augmented reality—transforming how machines and people interact with 3D environments.</p>
<div id="boilerplate_2660155">
				<p><strong>VB Daily</strong></p>
				<p>Stay in the know! Get the latest news in your inbox daily</p>
				
				<p>By subscribing, you agree to VentureBeat's <a href="https://venturebeat.com/terms-of-service/">Terms of Service.</a></p>
				<p id="boilerplateNewsletterConfirmation">
					Thanks for subscribing. Check out more <a href="https://venturebeat.com/newsletters/">VB newsletters here</a>.
				</p>
				<p>An error occured.</p>
			</div>			</div><!-- .article-content -->

							
			
		</article><!-- #content .article-wrapper -->

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't squander public trust on bullshit (238 pts)]]></title>
            <link>https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit</link>
            <guid>41746180</guid>
            <pubDate>Fri, 04 Oct 2024 22:42:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit">https://livboeree.substack.com/p/dont-squander-public-trust-on-bullshit</a>, See on <a href="https://news.ycombinator.com/item?id=41746180">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>At 4.50am local time today, this statewide emergency alert was sent out to every cellphone in Texas:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg" width="1201" height="410" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:410,&quot;width&quot;:1201,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123540,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa86d5372-c29b-477f-8d4b-3fcc885a469c_1201x410.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>I don’t know who Seth Altman is, nor do I care. Why? Because Seth Altman’s offense took place in Lubbock, Texas. I live in Austin, Texas. Four hundred miles away. What I do care about however is the misuse of emergency alert systems and public trust.</p><p><span>Sending out a screeching alert to 30million+ people over 250 million square miles in the middle of the night should only be used in the absolute DIREST OF CIRCUMSTANCES… circumstances like “Texas is under threat from hurricane/chemical leak/nuclear weapons, seek shelter now!” It should </span><em>never</em><span> be used for something that’s utterly irrelevant to 99.99% of people. </span></p><p>Why? Because the public’s trust in government emergency protocols is already hanging by a thread, and in order for those protocols to work when we really need them, they will need to be received and listened to. Instead, Texans by the hundreds of thousands are now turning off their phone’s emergency alerts, possibly forever. Why would anyone with a life to lead leave them on and risk getting their sleep disrupted over personally inconsequential events hundreds of miles away? </p><p>Such an outcome could be truly dangerous for Texas in the long run. If and when a real major emergency strikes, we will no longer have this important tool of public awareness or coordination. And that’s just the second order effects! There are likely going to be some excess deaths today as a direct result—there are 30m+ people in Texas, many of whom are in weak cardiovascular health. I would not be surprised at all if hospitals report an spike in cardiovascular events today. Not to mention an increase in road accidents; Texas is a notoriously driving-heavy state, and few things are worse for safe driving than sleep impairment.</p><p>So I hope the local government takes a long hard look at its alert-pressing finger. We all know the lesson of the Boy Who Cried Wolf, exhausting his village with his over-zealous cries. Well this time the village is thirty million people. Heads need to roll.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs, Theory of Mind, and Cheryl's Birthday (191 pts)]]></title>
            <link>https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb</link>
            <guid>41745788</guid>
            <pubDate>Fri, 04 Oct 2024 21:34:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb">https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb</a>, See on <a href="https://news.ycombinator.com/item?id=41745788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;security_link_product_navbar&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>

                  <li>
      
      
</li>

                  <li>
      
      <div>
                <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;white_papers_ebooks_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;white_papers_ebooks_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:norvig/pytudes" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="PE5AWI3p_iSziKdZC8ErZLcqeumyHJV5L9EaTYKLjdtd6HPcmwaEpev76mcRxZhzajMzDiv0gUxEzL3nTiTiKQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="norvig/pytudes" data-current-org="" data-current-owner="norvig" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=norvig%2Fpytudes" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/norvig/pytudes/blob/main/ipynb/CherylMind.ipynb&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="21d4ffb04cbc87fb443820ed00ec47c8a2db97dff8dd678de0053e74fc7c83f8" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a>
          
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Max Schrems wins privacy case against Meta over data on sexual orientation (129 pts)]]></title>
            <link>https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86</link>
            <guid>41745181</guid>
            <pubDate>Fri, 04 Oct 2024 20:17:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86">https://apnews.com/article/facebook-meta-schrems-privacy-80fd4e6c59f48a3b583d6665af3ede86</a>, See on <a href="https://news.ycombinator.com/item?id=41745181">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>LONDON (AP) — The European Union’s top court said Friday that social media company Meta can’t use public information about a user’s sexual orientation obtained outside its platforms for personalized advertising under the bloc’s strict data privacy rules. </p><p>The decision from the Court of Justice of the European Union in Luxembourg is a <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/meta-facebook-instagram-whatsapp-ai-artificial-intelligence-8d6cb3424ee410c641d0acdb154601f5">victory for Austrian privacy activist Max Schrems</a></span>, who has been a thorn in the side of Big Tech companies over their compliance with 27-nation bloc’s data privacy rules. </p><p>The EU court issued its ruling after Austria’s supreme court asked for guidance in Schrems’ case on how to apply the privacy rules, known as the General Data Protection Regulation, or GDPR. </p><p>Schrems had complained that Facebook had processed personal data including information about his sexual orientation to target him with online advertising, even though he had never disclosed on his account that he was gay. The only time he had publicly revealed this fact was during a panel discussion. </p>
    

<p>“An online social network such as Facebook cannot use all of the personal data obtained for the purposes of targeted advertising, without restriction as to time and without distinction as to type of data,” the court said in a press release summarizing its decision. </p>



<p>Even though Schrems revealed he was gay in the panel discussion, that “does not authorise the operator of an online social network platform to process other data relating to his sexual orientation, obtained, as the case may be, outside that platform, with a view to aggregating and analysing those data, in order to offer him personalised advertising.” </p>
    
<p>Meta said it was awaiting publication of the court’s full judgment and that it “takes privacy very seriously.”</p><p>“Everyone using Facebook has access to a wide range of settings and tools that allow people to manage how we use their information,” the company said in a statement. </p>
    

<p>Schrems’ lawyer, Katharina Raabe-Stuppnig, lawyer representing Mr Schrems, welcomed the court’s decision. </p><p>“Meta has basically been building a huge data pool on users for 20 years now, and it is growing every day. However, EU law requires ‘data minimisation’,” she said in a statement. “Following this ruling only a small part of Meta’s data pool will be allowed to be used for advertising — even when users consent to ads.” </p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How were 70s versions of games like Pong built without a programmable computer? (199 pts)]]></title>
            <link>https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra</link>
            <guid>41745032</guid>
            <pubDate>Fri, 04 Oct 2024 19:57:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra">https://retrocomputing.stackexchange.com/questions/30698/how-were-the-70s-versions-of-pong-and-similar-games-implemented-without-a-progra</a>, See on <a href="https://news.ycombinator.com/item?id=41745032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>They were made by mostly avoiding 'computing' concepts altogether, and treating it more like a mechanical thing.</p>
<p>For example with Pong a major component is usually timers - every xth of a second the timer will emit a signal. You have timers calibrated to match the horizontal refresh of the screen, so they'll 'ring' at the same point on each scanline. Then you have timers calibrated to the vertical refresh, so they'll ring on the same scanline each frame.</p>
<p>The ball is then just two discrete timers for vertical and horizontal position, and their rings are sent through an AND gate that will raise the voltage going to the display when both are ringing causing a white dot to appear. The paddles build on this concept with a medium length timer that can be started and stopped to define the length.</p>
<p>To move the ball or paddles the timers can be advanced or delayed by a control signal. Or more accurately the timers are always paused at a certain point, like during half of the horizontal blanking period. This pausing is then shortened once to advance the timer (move left/up) or increased once to delay it (move right/down).</p>
<p>Since both the paddle and the ball timers are emitting a '1' when they are to be displayed you can impliment collision detection by performing another AND operation. So if both a paddle and the ball are being drawn at the same moment you know they've collided and can adjust a latch controlling the ball direction accordingly.</p>
<p>If you get into the Atari 2600 you'll find that it's really weird compared to other consoles (sprites with no clearly defined X coordinate, instead only the ability to place it at the actual current location of the CRT beam or nudge it a small amount either way), but that it starts to make a lot of sense when you realize they were implementing their Pong logic for a programmable chip.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mitmproxy 11: Full HTTP/3 Support (280 pts)]]></title>
            <link>https://mitmproxy.org/posts/releases/mitmproxy-11/</link>
            <guid>41744434</guid>
            <pubDate>Fri, 04 Oct 2024 18:52:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitmproxy.org/posts/releases/mitmproxy-11/">https://mitmproxy.org/posts/releases/mitmproxy-11/</a>, See on <a href="https://news.ycombinator.com/item?id=41744434">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>We are excited to announce the release of mitmproxy 11, which introduces full support for HTTP/3 in both transparent
and reverse proxy modes. We’re also bringing in a ton of DNS improvements that we’ll cover in this blog post.</p>
<h5 id="editorial-note"><em>Editorial Note:</em></h5>
<p><em>Hi! I’m <a href="https://mitmproxy.org/authors/gaurav-jain/">Gaurav Jain</a>, one of the students selected for this year’s Google Summer of Code program to work on mitmproxy.
During this summer, I’ve worked on improving various low-level networking parts of mitmproxy some of which include
HTTP/3 and DNS. You can find my project report <a href="https://gist.github.com/errorxyz/af6f26549e9122f3ff3b93fd9d257df1">here</a>.</em></p>
<h2 id="http3">HTTP/3</h2>
<p>HTTP/3 now “just works” for reverse proxies. Your mitmproxy instance will listen for
both TCP and UDP packets and handle all HTTP versions thrown at it:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmproxy --mode reverse:https://http3.is
</span></span></code></pre></div><p>Our transparent proxy modes now all support HTTP/3 as well:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmproxy --mode wireguard
</span></span><span><span>$ mitmproxy --mode local
</span></span><span><span>$ mitmproxy --mode transparent
</span></span></code></pre></div><p>We have successfully tested HTTP/3 support with Firefox, Chrome, various cURL builds, and other clients to iron out
compatibility issues.
The only major limitation we are aware of at this time is that Chrome <a href="https://issues.chromium.org/issues/40138351#comment15">does not trust user-added Certificate Authorities for QUIC</a>.
This means you will either need to provide a publicly trusted certificate (e.g. from Let’s Encrypt), start Chrome with
a <a href="https://www.chromium.org/quic/playing-with-quic/#generate-certificates">command line switch</a>, or accept that it falls back to HTTP/2. Alternatively, Firefox doesn’t do such shenanigans.
For more HTTP/3 troubleshooting tips, you can check out <a href="https://github.com/mitmproxy/mitmproxy/issues/7025#issuecomment-2351138170">#7025</a>.</p>
<p>Bringing HTTP/3 support to mitmproxy is a major effort that was started in 2022 by <a href="https://mitmproxy.org/authors/manuel-meitinger/">Manuel Meitinger</a> and <a href="https://mitmproxy.org/authors/maximilian-hils/">Maximilian Hils</a>.
QUIC and HTTP/3 make up an increasing share of network traffic in the wild, and we’re super excited to have this ready
and enabled by default now!</p>
<h2 id="improved-dns-support">Improved DNS Support</h2>
<p>With the advent of DNS <a href="https://blog.cloudflare.com/speeding-up-https-and-http-3-negotiation-with-dns/">HTTPS records</a> and new privacy enhancements such as <a href="https://en.wikipedia.org/wiki/Server_Name_Indication#Encrypted_Client_Hello">Encrypted Client Hello (ECH)</a>, mitmproxy’s DNS
functionality is becoming increasingly important. We’re happy to share multiple advancements on this front:</p>
<h4 id="support-for-query-types-beyond-aaaaa">Support for Query Types Beyond A/AAAA</h4>
<p>mitmproxy’s old DNS implementation used <code>getaddrinfo</code> to resolve queries. This is convenient because everything is taken
care of by libc, but the <code>getaddrinfo</code> API only supports A/AAAA queries for IPv4 and IPv6 addresses. It doesn’t allow us
to answer queries for e.g. <a href="https://blog.cloudflare.com/speeding-up-https-and-http-3-negotiation-with-dns/">HTTPS records</a>, which are used to signal HTTP/3 support.</p>
<p>To overcome this limitation, we’ve reimplemented our DNS support on top of <a href="https://github.com/hickory-dns/hickory-dns">Hickory&nbsp;DNS</a>, a Rust-based DNS library.
Using Hickory, we now obtain the operating system’s default nameservers on Windows, Linux, and macOS and forward
non-A/AAAA queries there. This behavior can also be customized with the new <a href="https://docs.mitmproxy.org/stable/concepts-options/#dns_name_servers"><code>dns_name_servers</code> option</a>:</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ mitmdump --mode dns --set dns_name_servers<span>=</span>8.8.8.8
</span></span></code></pre></div><p><img src="https://mitmproxy.org/posts/releases/mitmproxy-11/dns.png" alt="dns"></p>
<h4 id="skipping-etchosts">Skipping /etc/hosts</h4>
<p>By switching to Hickory, we now also have the option to ignore the system’s hosts
file (<code>/etc/hosts</code> on Linux) with the new <a href="https://docs.mitmproxy.org/stable/concepts-options/#dns_use_hosts_file"><code>dns_use_hosts_file</code> option</a>. We plan to move mitmproxy’s internal
DNS resolution to Hickory as well, at which point this feature will become incredibly useful in allowing transparent
redirection on the same machine for specific domains. At the moment, such a setup would cause mitmproxy to recursively
connect to itself because we always take the hosts file into account.</p>
<div><pre tabindex="0"><code data-lang="shell"><span><span>$ echo <span>"192.0.2.1 mitmproxy.org"</span> &gt;&gt; /etc/hosts
</span></span><span><span>
</span></span><span><span>$ mitmdump --mode dns
</span></span><span><span>$ dig @127.0.0.1 +short mitmproxy.org
</span></span><span><span>192.0.2.1
</span></span><span><span>
</span></span><span><span>$ mitmdump --mode dns --set dns_use_hosts_file<span>=</span>false
</span></span><span><span>$ dig @127.0.0.1 +short mitmproxy.org
</span></span><span><span>3.161.82.13
</span></span></code></pre></div><h4 id="support-for-dns-over-tcp">Support for DNS-over-TCP</h4>
<p>DNS uses UDP by default, but may also use TCP to support records that do not fit into a single UDP packet. mitmproxy has
previously gotten away with only supporting UDP, but now that we support arbitrary query types, message size and thus
TCP support is more important. Long story short, DNS-over-TCP works with mitmproxy 11!</p>
<h4 id="stripping-encrypted-client-hello-ech-keys">Stripping Encrypted Client Hello (ECH) Keys</h4>
<p>Unless a custom certificate is configured, mitmproxy uses the Server Name Indication (SNI) transmitted in the TLS
ClientHello to construct a valid certificate. Conversely, if no SNI is present, we may not be able
to generate a certificate that is trusted by the client.</p>
<p><a href="https://en.wikipedia.org/wiki/Server_Name_Indication#Encrypted_Client_Hello">Encrypted Client Hello (ECH)</a> is an exciting new technology to increase privacy on the web. In short, the client uses
the new DNS HTTPS records to obtain an ECH key before establishing a connection, and then already encrypts the initial
ClientHello handshake message with that key. If both DNS queries and handshake are encrypted, passive intermediaries
cannot learn the target domain, only the target IP address (which is not conclusive for shared hosting and Content Delivery
Networks). This is a great advancement for privacy, but also breaks mitmproxy’s way of generating certificates.
To fix this, mitmproxy now strips ECH keys from HTTPS records. This way the client has no keys to encrypt the initial
handshake message with, and mitmproxy still learns the target domain and can construct a matching certificate.</p>
<p>Of course, ECH adds complexity for us and sometimes makes mitmproxy harder to use for our users. Nonetheless, we are
excited to see these privacy advancements being made for the rest of the web!</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>This work supported by <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a> under the umbrella of the <a href="https://www.honeynet.org/">Honeynet&nbsp;Project</a>, and the
<a href="https://nlnet.nl/entrust/">NGI0 Entrust fund</a> established by <a href="https://nlnet.nl/">NLnet</a>. Thank you to my mentor <a href="https://mitmproxy.org/authors/maximilian-hils/">Maximilian Hils</a> for the
invaluable guidance and support.</p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: FFmpeg-over-IP – Connect to remote FFmpeg servers (131 pts)]]></title>
            <link>https://github.com/steelbrain/ffmpeg-over-ip</link>
            <guid>41743780</guid>
            <pubDate>Fri, 04 Oct 2024 17:49:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/steelbrain/ffmpeg-over-ip">https://github.com/steelbrain/ffmpeg-over-ip</a>, See on <a href="https://news.ycombinator.com/item?id=41743780">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">ffmpeg over IP</h2><a id="user-content-ffmpeg-over-ip" aria-label="Permalink: ffmpeg over IP" href="#ffmpeg-over-ip"></a></p>
<p dir="auto">Connect to remote ffmpeg servers. Are you tired of unsuccessfully trying to pass your GPU through to a docker
container running in a VM? So was I! <code>ffmpeg-over-ip</code> allows you to run an ffmpeg server on a machine with access
to a GPU (Linux, Windows, or Mac) and connect to it from a remote machine. The only thing you need is Node.js
installed and a shared filesystem (could be NFS, SMB, etc.) between the two machines.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><code>ffmpeg-over-ip</code> consists of two main parts, the server and the client. Both are packed neatly into single JS
files. You can download these from the <a href="https://www.npmjs.com/package/ffmpeg-over-ip?activeTab=code" rel="nofollow">npm interface</a> or by <code>npm install ffmpeg-over-ip</code> and then copying
them to the relevant places. You don't need any <code>node_modules</code> to run the server or the client.</p>
<p dir="auto">The javascript files require Node.js runtime to work. If you want standalone files that you can mount in a docker
container, you can find these in the <a href="https://github.com/steelbrain/ffmpeg-over-ip/releases">Github Releases</a>. On the releases page, you may have to click <strong>"Show all
assets"</strong> to see the files.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The server and the client are both configured using JSONC (JSON with comments) configuration files. The paths
of these files can be flexible. To identify which paths are being used, you can invoke either with <code>--debug-print-search-paths</code>.</p>
<p dir="auto">Template/example configuration files are provided in this repository for your convinience. Unless the server and the client
share the same filesystem, you may have to specify <code>rewrites</code> in the server configuration file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Both the server and the client files are executable, so long as there is a Node.js installation available. If you intend
to use this in a docker container, you can directly mount the client file to where the container would expect a regular
ffmpeg executable to be, ie <code>docker run -v ./path/to/client-bin:/usr/lib/jellyfin-ffmpeg/ffmpeg ...</code>.</p>
<p dir="auto">The server and the client communicate commands over HTTP, so make sure that whatever port you specify on the server is
allowed through the firewall.</p>
<p dir="auto">Assuming you <strong>download one of the release files</strong>, here's what the usage would look like</p>
<p dir="auto">On the client side</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./ffmpeg-over-ip-client --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.client.jsonc # Change the stuff you want
$ ./ffmpeg-over-ip-client <use like ffmpeg, add ffmpeg args here>"><pre>$ ./ffmpeg-over-ip-client --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.client.jsonc <span><span>#</span> Change the stuff you want</span>
$ ./ffmpeg-over-ip-client <span>&lt;</span>use like ffmpeg, add ffmpeg args here<span>&gt;</span></pre></div>
<p dir="auto">On the server side</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ ./ffmpeg-over-ip-server --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.server.jsonc # Change the stuff you want, especially the rewrites
$ ./ffmpeg-over-ip-server"><pre>$ ./ffmpeg-over-ip-server --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.server.jsonc <span><span>#</span> Change the stuff you want, especially the rewrites</span>
$ ./ffmpeg-over-ip-server</pre></div>
<p dir="auto">Assuming you want to <strong>download these from npm</strong>, here's how you would do it</p>
<p dir="auto">On the client side:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-client --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.client.jsonc # Change the stuff you want
$ ./node_modules/.bin/ffmpeg-over-ip-client <use like ffmpeg, add ffmpeg args here>"><pre>$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-client --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.client.jsonc ffmpeg-over-ip.client.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.client.jsonc <span><span>#</span> Change the stuff you want</span>
$ ./node_modules/.bin/ffmpeg-over-ip-client <span>&lt;</span>use like ffmpeg, add ffmpeg args here<span>&gt;</span></pre></div>
<p dir="auto">On the server side:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-server --debug-print-search-paths # See the places where it'll look for config
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc # Add config to one of the places
$ nano ffmpeg-over-ip.server.jsonc # Change the stuff you want, especially the rewrites
$ ./node_modules/.bin/ffmpeg-over-ip-server"><pre>$ npm install ffmpeg-over-ip
$ ./node_modules/.bin/ffmpeg-over-ip-server --debug-print-search-paths <span><span>#</span> See the places where it'll look for config</span>
$ cp template.ffmpeg-over-ip.server.jsonc ffmpeg-over-ip.server.jsonc <span><span>#</span> Add config to one of the places</span>
$ nano ffmpeg-over-ip.server.jsonc <span><span>#</span> Change the stuff you want, especially the rewrites</span>
$ ./node_modules/.bin/ffmpeg-over-ip-server</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The contents of this project are licensed under the terms of the MIT License.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Open source framework OpenAI uses for Advanced Voice (201 pts)]]></title>
            <link>https://github.com/livekit/agents</link>
            <guid>41743327</guid>
            <pubDate>Fri, 04 Oct 2024 17:01:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/livekit/agents">https://github.com/livekit/agents</a>, See on <a href="https://news.ycombinator.com/item?id=41743327">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<themed-picture data-catalyst-inline="true"><picture>
  <source media="(prefers-color-scheme: dark)" srcset="https://github.com/livekit/agents/raw/main/.github/banner_dark.png">
  <source media="(prefers-color-scheme: light)" srcset="https://github.com/livekit/agents/raw/main/.github/banner_light.png">
  <img alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png">
</picture></themed-picture>


<p>
Looking for the JS/TS library? Check out <a href="https://github.com/livekit/agents-js">AgentsJS</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">✨ [NEW] OpenAI Realtime API support</h2><a id="user-content--new-openai-realtime-api-support" aria-label="Permalink: ✨ [NEW] OpenAI Realtime API support" href="#-new-openai-realtime-api-support"></a></p>
<p dir="auto">We're partnering with OpenAI on a new <code>MultimodalAgent</code> API in the Agents framework. This class completely wraps OpenAI’s Realtime API, abstract away the raw wire protocol, and provide an ultra-low latency WebRTC transport between GPT-4o and your users’ devices. This same stack powers Advanced Voice in the ChatGPT app.</p>
<ul dir="auto">
<li>Try the Realtime API in our <a href="https://playground.livekit.io/" rel="nofollow">playground</a> [<a href="https://github.com/livekit-examples/realtime-playground">code</a>]</li>
<li>Check out our <a href="https://docs.livekit.io/agents/openai" rel="nofollow">guide</a> to building your first app with this new API</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is Agents?</h2><a id="user-content-what-is-agents" aria-label="Permalink: What is Agents?" href="#what-is-agents"></a></p>
<p dir="auto">The Agents framework allows you to build AI-driven server programs that can see, hear, and speak in realtime. Your agent connects with end user devices through a LiveKit session. During that session, your agent can process text, audio, images, or video streaming from a user's device, and have an AI model generate any combination of those same modalities as output, and stream them back to the user.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Plugins for popular LLMs, transcription and text-to-speech services, and RAG databases</li>
<li>High-level abstractions for building voice agents or assistants with automatic turn detection, interruption handling, function calling, and transcriptions</li>
<li>Compatible with LiveKit's <a href="https://github.com/livekit/sip">telephony stack</a>, allowing your agent to make calls to or receive calls from phones</li>
<li>Integrated load balancing system that manages pools of agents with edge-based dispatch, monitoring, and transparent failover</li>
<li>Running your agents is identical across localhost, <a href="https://github.com/livekit/livekit">self-hosted</a>, and <a href="https://cloud.livekit.io/" rel="nofollow">LiveKit Cloud</a> environments</li>
</ul>

<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install the core Agents library:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install livekit-agents"><pre>pip install livekit-agents</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Plugins</h2><a id="user-content-plugins" aria-label="Permalink: Plugins" href="#plugins"></a></p>
<p dir="auto">The framework includes a variety of plugins that make it easy to process streaming input or generate output. For example, there are plugins for converting text-to-speech or running inference with popular LLMs. Here's how you can install a plugin:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install livekit-plugins-openai"><pre>pip install livekit-plugins-openai</pre></div>
<p dir="auto">The following plugins are available today:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Plugin</th>
<th>Features</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-anthropic/" rel="nofollow">livekit-plugins-anthropic</a></td>
<td>LLM</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-azure/" rel="nofollow">livekit-plugins-azure</a></td>
<td>STT, TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-deepgram/" rel="nofollow">livekit-plugins-deepgram</a></td>
<td>STT</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-cartesia/" rel="nofollow">livekit-plugins-cartesia</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-elevenlabs/" rel="nofollow">livekit-plugins-elevenlabs</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-playht/" rel="nofollow">livekit-plugins-playht</a></td>
<td>TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-google/" rel="nofollow">livekit-plugins-google</a></td>
<td>STT, TTS</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-nltk/" rel="nofollow">livekit-plugins-nltk</a></td>
<td>Utilities for working with text</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-rag/" rel="nofollow">livekit-plugins-rag</a></td>
<td>Utilities for performing RAG</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-openai/" rel="nofollow">livekit-plugins-openai</a></td>
<td>LLM, STT, TTS, Assistants API, Realtime API</td>
</tr>
<tr>
<td><a href="https://pypi.org/project/livekit-plugins-silero/" rel="nofollow">livekit-plugins-silero</a></td>
<td>VAD</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation and guides</h2><a id="user-content-documentation-and-guides" aria-label="Permalink: Documentation and guides" href="#documentation-and-guides"></a></p>
<p dir="auto">Documentation on the framework and how to use it can be found <a href="https://docs.livekit.io/agents" rel="nofollow">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Example agents</h2><a id="user-content-example-agents" aria-label="Permalink: Example agents" href="#example-agents"></a></p>
<ul dir="auto">
<li>A basic voice agent using a pipeline of STT, LLM, and TTS [<a href="https://kitt.livekit.io/" rel="nofollow">demo</a> | <a href="https://github.com/livekit/agents/blob/main/examples/voice-pipeline-agent/minimal_assistant.py">code</a>]</li>
<li>Voice agent using the new OpenAI Realtime API [<a href="https://playground.livekit.io/" rel="nofollow">demo</a> | <a href="https://github.com/livekit-examples/realtime-playground">code</a>]</li>
<li>Super fast voice agent using Cerebras hosted Llama 3.1 [<a href="https://cerebras.vercel.app/" rel="nofollow">demo</a> | <a href="https://github.com/dsa/fast-voice-assistant/">code</a>]</li>
<li>Voice agent using Cartesia's Sonic model [<a href="https://cartesia-assistant.vercel.app/" rel="nofollow">demo</a>]</li>
<li>Agent that looks up the current weather via function call [<a href="https://github.com/livekit/agents/blob/main/examples/voice-pipeline-agent/function_calling_weather.py">code</a>]</li>
<li>Voice agent that performs a RAG-based lookup [<a href="https://github.com/livekit/agents/tree/main/examples/voice-pipeline-agent/simple-rag">code</a>]</li>
<li>Video agent that publishes a stream of RGB frames [<a href="https://github.com/livekit/agents/tree/main/examples/simple-color">code</a>]</li>
<li>Transcription agent that generates text captions from a user's speech [<a href="https://github.com/livekit/agents/tree/main/examples/speech-to-text">code</a>]</li>
<li>A chat agent you can text who will respond back with genereated speech [<a href="https://github.com/livekit/agents/tree/main/examples/text-to-speech">code</a>]</li>
<li>Localhost multi-agent conference call [<a href="https://github.com/dsa/multi-agent-meeting">code</a>]</li>
<li>Moderation agent that uses Hive to detect spam/abusive video [<a href="https://github.com/dsa/livekit-agents/tree/main/hive-moderation-agent">code</a>]</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's <a href="https://livekit.io/join-slack" rel="nofollow">Slack community</a>.</p>

<markdown-accessiblity-table><table>
<thead><tr><th colspan="2">LiveKit Ecosystem</th></tr></thead>
<tbody>
<tr><td>Realtime SDKs</td><td><a href="https://github.com/livekit/client-sdk-js">Browser</a> · <a href="https://github.com/livekit/client-sdk-swift">iOS/macOS/visionOS</a> · <a href="https://github.com/livekit/client-sdk-android">Android</a> · <a href="https://github.com/livekit/client-sdk-flutter">Flutter</a> · <a href="https://github.com/livekit/client-sdk-react-native">React Native</a> · <a href="https://github.com/livekit/rust-sdks">Rust</a> · <a href="https://github.com/livekit/node-sdks">Node.js</a> · <a href="https://github.com/livekit/python-sdks">Python</a> · <a href="https://github.com/livekit/client-sdk-unity">Unity</a> · <a href="https://github.com/livekit/client-sdk-unity-web">Unity (WebGL)</a></td></tr><tr></tr>
<tr><td>Server APIs</td><td><a href="https://github.com/livekit/node-sdks">Node.js</a> · <a href="https://github.com/livekit/server-sdk-go">Golang</a> · <a href="https://github.com/livekit/server-sdk-ruby">Ruby</a> · <a href="https://github.com/livekit/server-sdk-kotlin">Java/Kotlin</a> · <a href="https://github.com/livekit/python-sdks">Python</a> · <a href="https://github.com/livekit/rust-sdks">Rust</a> · <a href="https://github.com/agence104/livekit-server-sdk-php">PHP (community)</a></td></tr><tr></tr>
<tr><td>UI Components</td><td><a href="https://github.com/livekit/components-js">React</a> · <a href="https://github.com/livekit/components-android">Android Compose</a> · <a href="https://github.com/livekit/components-swift">SwiftUI</a></td></tr><tr></tr>
<tr><td>Agents Frameworks</td><td><b>Python</b> · <a href="https://github.com/livekit/agents-js">Node.js</a> · <a href="https://github.com/livekit/agent-playground">Playground</a></td></tr><tr></tr>
<tr><td>Services</td><td><a href="https://github.com/livekit/livekit">LiveKit server</a> · <a href="https://github.com/livekit/egress">Egress</a> · <a href="https://github.com/livekit/ingress">Ingress</a> · <a href="https://github.com/livekit/sip">SIP</a></td></tr><tr></tr>
<tr><td>Resources</td><td><a href="https://docs.livekit.io/" rel="nofollow">Docs</a> · <a href="https://github.com/livekit-examples">Example apps</a> · <a href="https://livekit.io/cloud" rel="nofollow">Cloud</a> · <a href="https://docs.livekit.io/home/self-hosting/deployment" rel="nofollow">Self-hosting</a> · <a href="https://github.com/livekit/livekit-cli">CLI</a></td></tr>
</tbody>
</table></markdown-accessiblity-table>

</article></div></div>]]></description>
        </item>
    </channel>
</rss>