<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 01 Oct 2025 16:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Minimal files and config for a PWA (106 pts)]]></title>
            <link>https://github.com/chr15m/minimal-pwa</link>
            <guid>45437326</guid>
            <pubDate>Wed, 01 Oct 2025 13:14:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/chr15m/minimal-pwa">https://github.com/chr15m/minimal-pwa</a>, See on <a href="https://news.ycombinator.com/item?id=45437326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      



    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>

      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p>

<react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  
  
</react-partial>





      

          

              




<header role="banner" data-is-top="true" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      

      <div>
        <div>
            <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>
          GitHub Copilot

        </p><p>

        Write better code with AI
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
        <p>
          GitHub Spark

            <span>
              New
            </span>
        </p><p>

        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
        <p>
          GitHub Models

            <span>
              New
            </span>
        </p><p>

        Manage and compare prompts
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
        <p>
          GitHub Advanced Security

        </p><p>

        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>
          Actions

        </p><p>

        Automate any workflow
      </p></div>

    
</a></li>

                  </ul>
                </div>
            <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>
          Codespaces

        </p><p>

        Instant dev environments
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>
          Issues

        </p><p>

        Plan and track work
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>
          Code Review

        </p><p>

        Manage code changes
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_platform_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>
          Discussions

        </p><p>

        Collaborate outside of code
      </p></div>

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_platform_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
        <p>
          Code Search

        </p><p>

        Find more, search less
      </p></div>

    
</a></li>

                  </ul>
                </div>
            
        </div>

          <p>
            <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;view_all_features&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}" href="https://github.com/features">
              View all features
              
</a>          </p>
      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>

                      <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                      <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://github.com/resources/events">
      Events &amp; Webinars

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://github.com/partners">
      Partners

    
</a></li>

                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                  </ul>
                </div>
</li>


                <li>
      

      <div>
                <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>
          GitHub Sponsors

        </p><p>

        Fund open source developers
      </p></div>

    
</a></li>

                  </ul>
                </div>
                <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
        <p>
          The ReadME Project

        </p><p>

        GitHub community articles
      </p></div>

    
</a></li>

                  </ul>
                </div>
                
            </div>
</li>


                <li>
      

      <div>

                  <ul>
                      <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>
          Enterprise platform

        </p><p>

        AI-powered developer platform
      </p></div>

    
</a></li>

                  </ul>
                </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;platform&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;platform_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:chr15m/minimal-pwa" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="m1iT7lo9LlWcjZIU5mQgCKXl1-KrcgGuINf6XNpLMWXY0tKmv0w-I64Xn52PlAeH99xzGeJX3_8yqNKtDaAcpg" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="chr15m/minimal-pwa" data-current-org="" data-current-owner="chr15m" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=chr15m%2Fminimal-pwa" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/chr15m/minimal-pwa&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d26cbfff95843f3710249c937aee2c8e38ec6ac3f667c6f5be2db9c37918a046" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-f197728f-3b54-4d92-82b1-03a5c5de041b" for="icon-button-ed09aa16-7e48-41bb-9455-f3380047c5bd" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.6c63a6de228d6520804d.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div>
</header>

      
    </div>

  








    


    






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-project-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  





    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div data-view-component="true" id="repo-content-pjax-container">      

<react-partial partial-name="repos-overview" data-ssr="true" data-attempted-ssr="true" data-react-profiling="false">
  
  
  <div data-target="react-partial.reactRoot"><div itemscope="" itemtype="https://schema.org/abstract"><h2>Repository files navigation</h2><nav aria-label="Repository files" data-variant="inset"><ul role="list"><li><a href="#" aria-current="page"><span data-component="icon"></span><span data-component="text" data-content="README">README</span></a></li></ul></nav></div><div data-hpc="true"><article itemprop="text"><p dir="auto">This is the minimal set of files for a "progressive web app" to be installable on Android and iOS.</p>
<p dir="auto">It contains the smallest possible <code>manifest.json</code> and service worker to trigger the install flow on Chrome.</p>
<p dir="auto">An even smaller implementation that fits in a single HTML file is in <a href="https://github.com/chr15m/minimal-pwa/blob/main/single-file-pwa.html">single-file-pwa.html</a>. It has a manifest.json that is dynamically generated from JavaScript, and it is installable without a service worker.</p>
</article></div></div>
</react-partial>


      </div>

</turbo-frame>


    </main>
  </div>

          



    <ghcc-consent id="ghcc" data-locale="en" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>




  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Detect Electron apps on Mac that hasn't been updated to fix the system wide lag (115 pts)]]></title>
            <link>https://gist.github.com/tkafka/e3eb63a5ec448e9be6701bfd1f1b1e58</link>
            <guid>45437112</guid>
            <pubDate>Wed, 01 Oct 2025 12:54:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/tkafka/e3eb63a5ec448e9be6701bfd1f1b1e58">https://gist.github.com/tkafka/e3eb63a5ec448e9be6701bfd1f1b1e58</a>, See on <a href="https://news.ycombinator.com/item?id=45437112">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
    Detect Electron apps on mac where the Electron hasn't yet been updated to fix the system wide lag
  </p><div id="file-readme-md" tabindex="0" role="region" aria-label="README.md content, created by tkafka on 12:53PM today.">
    <article itemprop="text"><p dir="auto"><h2 dir="auto">Electron Apps Causing System-Wide Lag on Tahoe</h2><a id="user-content-electron-apps-causing-system-wide-lag-on-tahoe" aria-label="Permalink: Electron Apps Causing System-Wide Lag on Tahoe" href="#electron-apps-causing-system-wide-lag-on-tahoe"></a></p>
<p dir="auto">See:</p>
<ul dir="auto">
<li><a data-error-text="Failed to load title" data-id="3412185866" data-permission-text="Title is private" data-url="https://github.com/electron/electron/issues/48311" data-hovercard-type="issue" data-hovercard-url="/electron/electron/issues/48311/hovercard?comment_id=3332181420&amp;comment_type=issue_comment" href="https://github.com/electron/electron/issues/48311#issuecomment-3332181420">electron/electron#48311 (comment)</a></li>
<li><a href="https://mjtsai.com/blog/2025/09/30/electron-apps-causing-system-wide-lag-on-tahoe/" rel="nofollow">https://mjtsai.com/blog/2025/09/30/electron-apps-causing-system-wide-lag-on-tahoe/</a></li>
</ul>
<p dir="auto">Fixed versions:</p>
<ul dir="auto">
<li>36.9.2</li>
<li>37.6.0</li>
<li>38.2.0</li>
<li>39.0.0</li>
<li>and all above 39</li>
</ul>
<p dir="auto">This script detects apps with not yet updated versions of Electron.</p>
<p dir="auto"><h2 dir="auto">Temporary workaround:</h2><a id="user-content-temporary-workaround" aria-label="Permalink: Temporary workaround:" href="#temporary-workaround"></a></p>
<p dir="auto">Run</p>
<div dir="auto"><pre>launchctl setenv CHROME_HEADLESS 1</pre></div>
<p dir="auto">on every system start. The CHROME_HEADLESS flag has a side effect of disabling Electron app window shadows, which makes them ugly, but also stops triggering the issue.</p>
<p dir="auto"><h2 dir="auto">Example output</h2><a id="user-content-example-output" aria-label="Permalink: Example output" href="#example-output"></a></p>
<p dir="auto">(as of 1st oct 2025 - it lists all electron apps, but none shows the ✅ checkmark so far)</p>
<pre><code>❌ OpenMTP.app: Electron 18.3.15 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ DaVinci Resolve.app: Electron 36.3.2 (Contents/Applications/Electron.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Electron.app: Electron 36.3.2 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Visual Studio Code.app: Electron 37.3.1 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Cursor.app: Electron 34.5.8 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Windsurf.app: Electron 34.4.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Claude.app: Electron 36.4.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Signal.app: Electron 38.1.2 (Contents/Frameworks/Electron Framework.framework/Electron Framework)
❌ Figma Beta.app: Electron 37.5.1 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Beeper Desktop.app: Electron 33.2.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Slack.app: Electron 38.1.2 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
</code></pre>
<p dir="auto"><h2 dir="auto">A bit of promo</h2><a id="user-content-a-bit-of-promo" aria-label="Permalink: A bit of promo" href="#a-bit-of-promo"></a></p>
<p dir="auto">If you'd appreciate a visual (Tufte-like) hour by hour forecast for iOS/Apple Watch/mac with nice widgets, I made one - check out 🌦️ <a href="https://apps.apple.com/app/apple-store/id1501958576" rel="nofollow">Weathergraph</a>.</p>
<p dir="auto">Thanks! Tomas</p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TigerBeetle is a most interesting database (242 pts)]]></title>
            <link>https://www.amplifypartners.com/blog-posts/why-tigerbeetle-is-the-most-interesting-database-in-the-world</link>
            <guid>45436534</guid>
            <pubDate>Wed, 01 Oct 2025 11:33:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.amplifypartners.com/blog-posts/why-tigerbeetle-is-the-most-interesting-database-in-the-world">https://www.amplifypartners.com/blog-posts/why-tigerbeetle-is-the-most-interesting-database-in-the-world</a>, See on <a href="https://news.ycombinator.com/item?id=45436534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>By many measures it’s safe to say that TigerBeetle is the most interesting database in the world. Like Costanza in Seinfeld, they seem to do the <em>opposite</em> of everyone else:</p><ul role="list"><li>Most teams write code fast. TigerBeetle tries to <strong>write code slow</strong>.</li><li>Most teams treat testing as a necessary evil. TigerBeetle is <strong>built entirely on Deterministic Simulation Testing (DST)</strong>.</li><li>Most teams build their software on top of loads of other software. TigerBeetle <strong>has zero dependencies</strong>.</li></ul><p>There’s even more. TigerBeetle enforces static memory allocation. They keep assertions enabled in production. They chose Viewstamped Replication over Raft, and even Zig instead of Rust!</p><p>This read is going to go behind the scenes of how TigerBeetle came to be, the incredibly novel software they’ve built, and all of the wacky, wonderful things that make them so special. Based on extensive interviews with the TigerBeetle team, we’re going to cover a few topics in technical detail:</p><ul role="list"><li>Why transactional databases should think in debits and credits, not SQL</li><li>An (actually) modern database: distributed by default, handling storage faults, and why TigerBeetle uses Zig</li><li>VOPR, TigerBeetle’s Deterministic Simulation Testing cluster</li><li>TigerStyle, and why you should use assertions</li></ul><p>Click on any section to jump straight there, if you’re curious.&nbsp;&nbsp;</p><h2><strong>Why we need a database that thinks in debits and credits</strong></h2><p>TigerBeetle’s website calls it “The Financial Transactions Database.” Its primitives are <strong>debits and credits</strong>, which are things you may be familiar with from your accounting requirement in college. And if you’re not a bank, you’re probably thinking this whole thing isn’t really for you. But Joran (TigerBeetle’s creator) would tell you otherwise: financial transactions, i.e. debits and credits, are actually <em>exactly</em> what transactional SQL was originally designed for.&nbsp;</p><p>Way back in 1985, Jim Gray (who would later win a Turing Award) wrote a seminal paper on transactions, titled <a href="https://jimgray.azurewebsites.net/papers/AMeasureOfTransactionProcessingPower.pdf">A Measure of Transaction Processing Power</a>. If you’ve heard of it before, it’s because in it, Gray defined a metric that 40 years later is <em>still</em> the most important measure for a database: <strong>TPS</strong>, or transactions per second. This would end up leading to such a fervent benchmark war among databases that an objective <em>council</em> – <a href="https://www.tpc.org/information/about/history5.asp">the TPC</a> – needed to be formed to moderate.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/68daca1c7780ffd190f23346_3d394e75.png" loading="lazy" alt=""></p><figcaption><em>The TPC in action, deciding on whether a young database had gone to the Dark Side (Oracle).</em>&nbsp;</figcaption></figure><p>But what does the “T” in TPS actually mean? What is a transaction?</p><p>Your first guess might be a SQL transaction, but that’s not it. Gray actually defined it as a <strong>business transaction</strong> derived from the real world. Which is the reason databases were invented in the first place: to power businesses. And indeed, <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2005/04/tr-2005-39.doc">20 years later</a>, Gray continued to see the standard measure of transaction processing as a “DebitCredit:”</p><blockquote><em>“A database system to debit a bank account, do the standard double-entry bookkeeping and then reply to the terminal.”</em></blockquote><p>Mind you, SQL had already been around since the 70s at this point. And yet the luminary Gray still chose the debit/credit model – because it was the <em>canonical example</em> of an everyday transaction. Debit/credit is the lingua franca of <a href="https://www.youtube.com/watch?v=lGyMiW6PnKI&amp;ab_channel=TuringAwardeeClips"><strong>what it means to transact</strong></a>. It is <em>not</em> just for accounting and banks. It’s the reason for a database to provide guarantees like ACID in the first place.</p><p>And yet, if you want to use a SQL database to implement debits and credits today, you are probably going to have a bad time. To handle one debit/credit, a typical system – like the <a href="https://mojaloop.io/">central bank switch</a> that Joran consulted on in 2020 – needs to query account balances, lock those rows, wait for decisions in code, then write back and record the debit/credit. All in all, you’re looking at <strong>10-20 SQL queries</strong> back and forth, while holding row locks across the network roundtrip time, <em>for each transaction</em>. This gets even worse when you consider the problem of hot rows, where many transactions often need to touch the same set of “house accounts”.&nbsp;</p><p>All the while (for better or worse), the world is moving faster and faster towards an “everything is a transaction” model. Countries like India and Brazil are doing billions of transactions per month in instant payments. With <a href="https://www.frbservices.org/financial-services/fednow">FedNow</a> in the U.S., we’re not far away from that reality either. Meanwhile, other sectors like energy, gaming, and cloud are all moving towards real-time billing. In less than a decade, the world has become at least three orders of magnitude more transactional. And yet the SQL databases we still use to power this are 20-30 years old. Can they hold up?</p><p><strong>This is where TigerBeetle comes in</strong>. They designed a state-of-the-art database, from the ground up, to power the next era of transactions. In TigerBeetle, a debit/credit is a first class primitive and 8,190 of them can pack into a single 1MiB query via a one solitary roundtrip to the database. They call it <a href="https://www.youtube.com/watch?v=yKgfk8lTQuE">“The 1000x Performance Idea,”</a> but in Joran’s words it’s “nothing special”.</p><p>They say databases take a decade to build. But TigerBeetle is complete and pretty much <a href="https://jepsen.io/analyses/tigerbeetle-0.16.11">Jepsen-proof</a> after just 3 and a half years. In June 2025, Kyle Kingsbury showed he was unable to break TigerBeetle’s foundations (he found 1 correctness bug in the read query engine, not affecting durability), even while corrupting the whole thing on every machine in various places.&nbsp;</p><p>The obvious question here – <strong>how</strong>? How did TigerBeetle ship a production-ready, Jepsen-passing consensus and storage engine in 3.5 years when it typically takes a decade or more?</p><h2><strong>An (actually) modern database: distributed by default, why TigerBeetle uses Zig, and handling storage faults</strong></h2><p>Imagine you wake up today and wisely decide to build a database from scratch. Instead of investing in the technology of 30 years ago – when the most popular relational databases today were built – you can pick <em>any</em> advancements in architecture, hardware, language, or research since then to implement. How would you build it? What would you utilize?</p><h3><strong>Distributed by default</strong></h3><p>One thing you’d probably start with is the deployment model.&nbsp;</p><p>When Postgres and MySQL were built, in a world of big iron (on-prem hardware), the dominant paradigm was <strong>single node</strong>. Now, in a world of shared cloud hardware, it’s <strong>distributed</strong>. It’s not safe enough to store your transactions only on a single disk or server. A modern database needs to replicate your transactions, with strict serializability, across machines, for redundancy, fault tolerance and high availability. And yet some of the most popular OLTP databases in the world today are still highly dependent on a single node architecture. Automated failover, at least with zero data loss in the cut over, is not always baked in by default.</p><p>So TigerBeetle built their database to be distributed by default. Doing that comes with some of the obvious things you need to do, like consensus. But the developer experience for running TigerBeetle distributed is very simple: you just install the binary on however many machines you want in the cluster. No async replication, no Zookeeper, etc. To make this possible, TigerBeetle invested heavily in their consensus protocol implementation, adopting the pioneering <a href="https://pmg.csail.mit.edu/papers/vr.pdf">Viewstamped Replication</a> from MIT. This is part of why TigerBeetle has zero dependencies, apart from the Zig toolchain — they literally invested in all their core dependencies.</p><h3><strong>Clock fault tolerance</strong></h3><p>Distributed by default also shows up in some unlikely places. For example: have you ever thought of a clock fault model?&nbsp;</p><p>Though it’s not technically required or advised for consensus – which uses logical clocks and not physical clocks – remember that TigerBeetle is a <em>transactions</em> database. The physical timestamps of transactions need to be accurate and comparable across different financial systems for auditing and compliance.</p><p>And here, readers will note that Linux has several clocks: <code>CLOCK_MONOTONIC_RAW</code>, <code>CLOCK_MONOTONIC</code> and <code>CLOCK_BOOTTIME</code>. All have slight but important differences. Which is the best monotonic clock to use? (clue: It doesn’t say <code>MONOTONIC</code> on the tin)</p><p>The challenge is that physical imperfections in hardware clocks cause clocks to tick at different speeds, so that time passes faster or slower than it should. These kinds of “drift” errors eventually add up to significant “skew” errors within a short space of time. Most of the time, Network Time Protocol (NTP) would correct for these errors. But if NTP silently stops working because of a partial network outage, then a highly available consensus cluster might otherwise be running blind, in the dark.</p><p>But even this is something TigerBeetle thought about. They combine <em>the majority of clocks</em> in the cluster to construct a fault-tolerant clock called “cluster time”. This cluster time then gets used to bring a server’s system time back into line if necessary, or shut down safely if TigerBeetle detects that there are too many faulty clocks (e.g. TigerBeetle can actually detect when something like Chrony, PTP, or NTP have stopped working and alert the operator).&nbsp;</p><p>They do this by tracking offset clock times between different TigerBeetle servers, sampling them, and passing them through <a href="https://en.wikipedia.org/wiki/Marzullo%27s_algorithm">Marzullo’s algorithm</a> to estimate the most accurate possible interval (again, just to get a sense of whether clocks are being synced by the underlying clock sync protocol correctly).</p><p>Small things like this are exactly why distributed by default is hard, and doesn’t work as an add-on for older database models. You can read more about this in TigerBeetle's <a href="https://tigerbeetle.com/blog/2021-08-30-three-clocks-are-better-than-one/">3 clocks are better than one</a> blog post.</p><p>‍</p><h3><strong>Handling storage faults</strong></h3><p>Another piece of “distributed by default” that deserves its own header is how TigerBeetle handles <strong>storage faults</strong> (or even the fact it handles them at all). Traditional databases assume that if disks fail, they do so predictably with a nice error message. For example, even <a href="https://www.sqlite.org/atomiccommit.html">SQLite’s docs</a> are clear that:</p><blockquote><em>SQLite does not add any redundancy to the database file for the purpose of detecting corruption or I/O errors. SQLite assumes that the data it reads is exactly the same data that it previously wrote.</em></blockquote><p>In reality, there are many more sinister possibilities: disks can silently return corrupt data, misdirect I/O (on the read or write path), or just suddenly get really slow (called <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/paper-1.pdf">gray failure</a> in the research), all without returning error codes.&nbsp;</p><p>TigerBeetle is built to be storage fault tolerant:</p><ul role="list"><li>TigerBeetle uses <a href="https://www.usenix.org/conference/fast18/presentation/alagappan">Protocol Aware Recovery</a> to remain available unless all copies of a piece of data get corrupted on every single replica.</li><li>All data in TigerBeetle is immutable, checksummed, and hash-chained, providing a strong guarantee that no corruption or tampering happened.</li><li>TigerBeetle puts as little software as possible between itself and the disk, including a custom page cache, writing data to disk with O_DIRECT, and even running on a raw block device directly (no filesystem necessary — to sidestep filesystem bugs <a href="https://news.ycombinator.com/item?id=36113828">which do tend to happen</a> from time to time).</li><li>They built their own implementation of LSM instead of using an off-the-shelf one – they call it an <a href="https://www.youtube.com/watch?v=yBBpUMR8dHw">LSM Forest</a>, which is something like 20 different LSM trees.</li></ul><p>As far as I’m aware TigerBeetle is the only distributed database that not only claims to be storage fault tolerant, but was also tested pretty hard and validated by Jepsen to be. If you have a local machine failure where even just a disk sector fails, then that storage engine is connected to the global consensus, and it can use the cluster to self heal. This is also a great example of why the modern database having access to modern research matters: <a href="https://www.usenix.org/conference/fast18/presentation/alagappan">Protocol-Aware Recovery</a>, which enables TigerBeetle to survive disk failures like this, is fairly recent (2018) research.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/68daca1c7780ffd190f23349_629bea61.png" loading="lazy" alt=""></p></figure><h3><strong>TigerBeetle in Zig</strong></h3><p>Another thing you’d think about when building a modern database from scratch is your choice of <strong>programming language</strong>. Postgres is written in C (c. 1970s), MySQL in C and C++ (1979), and MSSQL as well in C and C++. But programming languages have come a long way in the past 40 years. If you had your choice, what would you build a database in today?</p><p>The answer would probably be Rust or Zig. And indeed, TigerBeetle is built 100% in Zig:&nbsp;</p><ul role="list"><li>You get the whole C ecosystem available to you, extended with a phenomenal toolchain and compiler.</li><li>It’s easy to write, and especially easy to read, in some cases as easy as TypeScript (just a lot faster).</li><li>Zig lets you statically allocate memory, which is a core principle of TigerBeetle.</li><li>Zig has a great developer experience and you can learn it quickly (which ergo means you can get into the TigerBeetle src quickly).</li></ul><p>Of course, as new systems languages, Zig and Rust are related, and some of the early Rust team now work at TigerBeetle, including <a href="https://matklad.github.io/">Matklad</a> (creator of <a href="https://rust-analyzer.github.io/">Rust Analyzer</a>) and <a href="https://brson.github.io/">Brian Anderson</a> (co-creator of Rust with Graydon). They’ve <a href="https://matklad.github.io/2023/03/26/zig-and-rust.html">written extensively</a> about these languages and why Joran chose Zig in particular for TigerBeetle, given their design goals.</p><p>And here, of course, TigerBeetle is fanatical about static memory allocation, which I’ll talk more about in the next section. Not using dynamic memory allocation is “hard mode” in Rust (as matklad wrote about <a href="https://matklad.github.io/2022/10/06/hard-mode-rust.html">here</a>), but a breeze in Zig.</p><p>‍</p><h2><strong>Deterministic Simulation Testing and the VOPR</strong></h2><p>Sometimes, Deterministic Simulation Testing (DST) feels like the most transformational technology that the fewest developers know about. It’s a <a href="https://notes.eatonphil.com/2024-08-20-deterministic-simulation-testing.html">novel testing technique</a> made popular by the <a href="https://www.foundationdb.org/">FoundationDB</a> team (which now belongs to Apple); they used it to develop a more secure, bug-free distributed database in a shorter time span than arguably anyone had done before.&nbsp;</p><p>The <a href="https://www.youtube.com/watch?v=cHA8vyZvkCs">fundamentals of DST</a> go something like this. In distributed systems, there are essentially infinite combinations of concurrency issues: anything from lost messages to unpredictable thread execution order. You simply cannot use old-school unit and integration tests, or your system will suck. Formal verification, a more academic discipline that works on formulaic proofs that a program runs as intended, is too expensive and slow. So what are you to do?</p><p>The answer is a simulator that deterministically runs almost every possible scenario your system will face on a specific chronological timeline. The simulator accounts for external factors too, like issues with the OS, network, or disk, or simply different latencies. All in all, DST can give you the equivalent of years’ worth of testing in a very short time period (because time itself becomes deterministic—a while true loop); and DST is particularly well suited towards databases (I/O intensive, not compute intensive). If you’re familiar with Jepsen testing, think of it as <a href="https://antithesis.com/blog/is_something_bugging_you/">a subset of what DST can do</a>.&nbsp;</p><p>TigerBeetle is one of the most pioneering startups on the planet when it comes to DST. They’ve developed their own testing cluster – it’s nicknamed VOPR, short for Viewstamped Operation Replicator (<a href="https://www.youtube.com/watch?v=iRsycWRQrc8">after the WOPR simulator in the movie WarGames</a>). The VOPR constantly (and tirelessly) tests TigerBeetle under countless different conditions, covering everything from how nodes elect a leader to individual states and network faults. But it can simulate a whole distributed cluster virtually, all on a single thread.</p><p>As far as your author is aware, TigerBeetle’s VOPR is the single largest DST cluster on the planet. <a href="https://us13.campaign-archive.com/?u=32cd932058e988b44c838f7bc&amp;id=0c749f7b07">It runs on</a> 1,000 CPU cores, a number so unusually large that Hetzner sent them a special email asking if they were sure they wanted that many cores. The so-called VOPR-1000 is running 24x7x365, to catch rare conditions as far as possible before production. With time abstracted deterministically, and accelerated in the simulator by a factor of (roughly) 700x, this adds up to nearly 2 millennia of simulated runtime per day.</p><p>‍</p><h3><strong>But what if DST was fun?</strong></h3><p>Yea, distributed systems are cool. But you know what’s even cooler? Video games.</p><p>TigerBeetle turned DST into a game that lets you play through different failure scenarios in how the system reacts. You can play it <a href="https://sim.tigerbeetle.com/">here</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/67ebc022dbceaf64bee0f5c6/68daca1c7780ffd190f2334c_13e94615.png" loading="lazy" alt=""></p></figure><p>What’s perhaps even cooler is that this game is running an actual instance of the VOPR, simulating TigerBeetle…in your browser. It’s compiled to WebAssembly, and then TigerBeetle’s own engineers built a gaming frontend on top to visualize the real system</p><p>You can read more about how and why TigerBeetle built the simulator in <a href="https://tigerbeetle.com/blog/2023-07-11-we-put-a-distributed-database-in-the-browser/">this blog post</a>.</p><p>‍</p><h2><strong>TigerStyle and The Power of Ten</strong></h2><p>As you will continue to see with TigerBeetle, it is often not just the <em>what</em> they’ve built that catches the eye but also the <em>how</em>. There’s no better example than <strong>TigerStyle</strong>.</p><p><a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md">TigerStyle</a> is TigerBeetle’s engineering methodology, public on GitHub for all to see. Here’s how they describe it:</p><blockquote><em>“TigerBeetle's coding style is evolving. A collective give-and-take at the intersection of engineering and art. Numbers and human intuition. Reason and experience. First principles and knowledge. Precision and poetry. Just like music. A tight beat. A rare groove. Words that rhyme and rhymes that break. Biodigital jazz. This is what we've learned along the way. The best is yet to come.”</em></blockquote><p>Biodigital jazz is a term from <a href="https://en.wikipedia.org/wiki/Tron:_Legacy">Tron: Legacy</a>. In the context of the film, it represents the intertwining of human and digital elements, the chaotic yet structured nature of the “Grid” (the digital world), and the improvisational spirit of human potential within the confines of technology (I copied this from AI). For TigerBeetle, it’s an ethos of code; remembering to infuse everything they do with not just science, but art too.</p><p>More practically, TigerStyle lays out engineering and code principles for TigerBeetle, many derived from the original <a href="https://spinroot.com/gerard/pdf/P10.pdf">Power of Ten</a>, NASA’s tenets for writing foolproof code. TigerStyle spans from the thematic, like simplicity and elegance, to the applied, like how to name things. It’s even starting to impact other companies like Resonate and Turso; and <a href="https://youtu.be/tNZnLkRBYA8?t=11167">TigerStyle has even been discussed on Lex Fridman</a>. Here are a few highlights.</p><h3><strong>Using assertions, and the Power of Ten</strong></h3><p>Speaking of the Power of Ten…one of them (Rule 5) is about <strong>assertions</strong>. The idea is simple: explicitly encode your expectations of code behavior <em>while</em> you are writing it, not after the fact. You write them simply in a single line as booleans: assert(a &gt; b). TigerStyle calls for:</p><ul role="list"><li>Asserting all function arguments, return values, preconditions, and invariants. On average there should be at least 2 assertions per function.</li><li>Using assertions <em>instead</em> of comments when the assertion is both important and surprising.</li><li>Asserting the relationships between compile-time constants, so you can check a program’s design integrity before it even runs.</li><li>Not just assert what <em>should</em> happen, but also the negative space that you don’t expect – where interesting bugs can show up.</li></ul><p><a href="https://spinroot.com/gerard/pdf/P10.pdf">The Power of Ten</a> is an amazing artifact that covers so much more than just assertions…it’s a great resource for any modern programmer (and maybe we should train some LLMs on it too).</p><h3><strong>Thinking about performance</strong></h3><p>Much of TigerStyle centers around the idea that <em>writing</em> code is not the most important part of the cycle; instead, it’s <strong>reasoning about</strong> and <strong>designing</strong> the code. When it comes to performance, TigerStyle implores you to think about it from the start:&nbsp;</p><blockquote><em>“The best time to solve performance, to get the huge 1000x wins, is in the design phase, which is precisely when we can't measure or profile.”</em></blockquote><p>You should be doing basic napkin math on what TigerStyle calls “the four primary colors” – network, storage, memory, CPU – and how they’ll perform with respect to (“the two textures” — art!) bandwidth and latency. Then, there are a few more tactical tips, like distinguishing between the control plane and data plane, batching accesses, and extracting hot loops into stand-alone functions to reduce dependence on the compiler.&nbsp;</p><p>For more about TigerStyle, watch <a href="https://www.youtube.com/watch?v=w3WYdYyjek4&amp;t=5s&amp;ab_channel=TigerBeetle">Joran’s talk at Systems Distributed</a>.</p><h2>Try it out for yourself</h2><p>So is TigerBeetle a database? Yes. But it’s not much like any other database I’ve seen. They’ve taken modern research and applied it to an age-old form, giving their database unprecedented performance and stability guarantees. They’ve developed an art form around systems and storage engineering, and they haven’t forgotten to have fun along the way. And thanks to their clever use of DST, they were able to build this thing to Jepsen standards in only a few years.&nbsp;</p><p>You can get started with TigerBeetle <a href="https://tigerbeetle.com/#install">here</a> using a simple curl command.&nbsp;</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Our efforts, in part, define us (189 pts)]]></title>
            <link>https://weakty.com/posts/efforts/</link>
            <guid>45435825</guid>
            <pubDate>Wed, 01 Oct 2025 09:22:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://weakty.com/posts/efforts/">https://weakty.com/posts/efforts/</a>, See on <a href="https://news.ycombinator.com/item?id=45435825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>What happens when something we enjoy doing that took effort becomes effortless? And what happens if that original effort was a foundation on which we saw value in ourselves?</p>
<p>If our efforts, in part, define us, then our efforts have intrinsic value. Our efforts may help us understand a position we want to occupy, an identity we carry, or an outlook we present. This value contributes to an internal economy of joy, self-respect, fulfillment, happiness. When effortful things become effortless, what becomes of our position in these economies?</p>
<p>As you can see, I have a few questions here.</p>
<hr>
<p>I know someone who spent a part of their adult life taking beautiful photographs, developing them by hand, framing them, cataloging them. Along came the ubiquity of digital cameras and smartphones, and "film" became infinitely available. Offhandedly, one day, this person mentioned that with the proliferation of smart phone cameras, and the ease with which one can take photos, they had found that some days their desire to continue was diminishing, and their work had lost meaning.</p>
<p>Technology has a history of making effortful things effortless, and there is sometimes a hidden loss in that advancement.</p>
<p>I figure people are continually being left behind in a similar manner day-to-day. Technology continues advancing (for the most part), and more things that remain effortful will become effortless. And "we" (ie, the populations who can afford to sit around and have crises of identity on these topics) will be further pushed to re-evaluate certain parts of our definitions of self.</p>
<p>For myself, in the last 10 years, my work of writing code has largely defined what I do with my working time. Now I experience large swaths of that work being created and done by AI (sometimes amazingly well, sometimes poorly), and I find myself thinking of the photographer above. It's not my wish that people can't have access to a more effortless way to write code, but I feel a strange sadness that there is less left to the act of the craft.</p>
<p>I have had this note in a draft state for several weeks now because I still can’t quite come to terms with how I’m feeling about things. There are so many nuances and unclear thoughts rolling around in my head about this shift. I think the only thing that is vaguely clear is that none of this would matter if making money wasn’t at play. If I was just writing code, (or taking film photographs) for fun in my free time because I enjoyed it, well, I don’t think I’d be feeling so conflicted.</p>
<p>Being paid to work and presenting my capacities through my craft is an exchange that I have been able to derive value from in its effortful-ness. Often times I've worked on utterly boring tasks that I would have loved to have a tool that could automate. But I didn't. And even in those menial moments I did derive some pleasure in my capacities. Of course, when it came to the real challenges, that was where I felt a pleasure and value in putting forth effort.</p>
<p>As a consultant, I work in a lot of different places, often for brief stints of time. And at many of these places, I see a large push, top-down, to encourage people to use AI. These employees, previously having entered an employment agreement where their capacities and experience would be exchanged for money, are now being asked that their abilities be augmented. In this way, the level continues to skew toward privileging production, often without understanding and people using their own perspectives.</p>
<p>When I see sentiments similar to mine, I often see reactions where people say that AI is simply a tool and that you must learn to use it and incorporate it into your toolbox. That's fine. That's well and good. But all I'm trying to say here is that I feel a lack and a loss for something. I don't understand it yet.</p>
<p>The title of this post, <em>our efforts, in part, define us</em>, is just a phrase that popped into my head. I'm not really sure if I even believe it or if I've fully fleshed out this single statement. But some part of it rings true to me. I wonder what will happen to us and our efforts. Will we be driven into further niches that are effortful, that we can derive value from? Will we become vague blobs that are formless, ill-defined, and despondent?</p>
<p>All of this presupposes a few things —that one can (and/or <em>should</em>) aim to derive value from work, that a meaningful identity is constructed by doing effortful things, that people generally are happier when they can use their skills and experiences to make something. And what’s more, there is a fine-line here between glorifying people with experience deriving value, and sounding like a shitty gatekeeper.</p>
<p>I will continue working for various clients. I suspect I will continue hearing leadership push AI on employees. And I will continue observing how people respond to this. Of course, for many people a job is <em>just a job</em>, as they say, and they'll do whatever they can to get it done more quickly (or work several jobs at once). Those very same people might find more value from their efforts now that AI is making their jobs easier. They can turn to better supporting their family, following other interests outside of work, finding other meaningful things, etc.</p>
<p>But at this time, I don't really see how this won’t further trample people’s spirits in the realm of work, unless we also reshape our expectations of work itself.</p>
<p>Is it worth the effort?</p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I only use Google Sheets (172 pts)]]></title>
            <link>https://mayberay.bearblog.dev/why-i-only-use-google-sheets/</link>
            <guid>45435463</guid>
            <pubDate>Wed, 01 Oct 2025 08:06:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mayberay.bearblog.dev/why-i-only-use-google-sheets/">https://mayberay.bearblog.dev/why-i-only-use-google-sheets/</a>, See on <a href="https://news.ycombinator.com/item?id=45435463">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  <header>
    <a href="https://mayberay.bearblog.dev/">
      <h2>
        Maybe-Ray
      </h2>
    </a>
    <nav>
      <p><a href="https://mayberay.bearblog.dev/">Home</a> <a href="https://mayberay.bearblog.dev/blog/">Blog</a></p>

    </nav>
  </header>
  <main>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2025-09-30T13:10Z">
                    30 Sep, 2025
                </time>
            </i>
        </p>
    

    <p>To cut things short, always use the easiest solution to solve a particular problem and once that solution does not work for the business anymore reassess what the new requirements are and either try enhance the current solution or find an alternative that better solve the problem. In my case the easiest solution is often creating a new Google sheet.</p>
<p>I entered the workforce about 9 months ago and my optimism for building new tools and services that help the small starting up business I work for has all vanished. I work in an environment that changes every 2 months or so, as my boss finds a new business venture she wants to enter. This has me starting and stopping quite a few projects that could have been solved in an afternoon with a quick Google Sheet.</p>
<p>I have listed a few examples below of some of the projects I have wasted time on instead of making a Google Sheet:</p>
<ol>
<li><p>I spent 2 months designing and making an admin panel to manage and track incoming cargo for the business. This panel was supposed to help the business categorise and better manage packages and customer data. This admin panel was used twice and never again. A Google Sheet could have been easily used for this and is currently being used for this task.</p>
</li>
<li><p>Three weeks were spent creating an MVP for a quote system that automatically calculated the duty and taxes for people ordering certain goods. Zimbabwean taxes and duties are often very complex and having our customers know exactly what to pay would create a better customer journey and make the process faster since we would not have to wait on our third party duty processing company to reply to us on every customer inquiry. In the end, we saw one of our competitors tax and duty breakdown table and we just copied it and put it in a Google Sheet.</p>
</li>
<li><p>Spent 2 months researching, having meetings (often &gt; 1hr long) and looking for a good CRM to use for the business. I would sit down compare and contrast different feature`s and prices for all the different CRMs we were looking into. We ended up using the free version of Oddo, that is not used that much anyway within the business. To my surprise a few weeks ago i noticed that Google Sheets has a CRM template built into it.</p>
</li>
</ol>
<p>I'm not saying that making a Google Sheet is the best solution to every problem but often times in my situation it is. I usually end up in situations were I never know the full scope of the problem until we start doing the actual work.</p>
<p>This is not to say that we do not need to plan out a project. The team should discuss workflows and information they might need but until we start doing the actual work we do not know full scope of the problem.</p>
<p>Once the full scope of the problem is known then we can start creating or enhancing the solutions we have. This helps because you do not end up being stuck with an extra workload that in the best case does not require all the features you are adding and in the worst case spending time on a project that will fail. So it is in your best interest to use the most basic solution to solve a problem.</p>
<p>Doing the smallest and easiest solution to a problem as a way to get to know the full scope and then iterating after that if needs be is by far the best solution (for me).</p>
<p>There are some caveats to this approach, I know a few organisation that have a thousand row spreadsheets that keep track of all their business transactions and employee information.</p>
<p>Creating a Google Sheet only works in situations were we do not know the full scope of the problem. Personally, I'm still new to this and learning when the best solution is making a Google Sheet or not. I just want to save people's time and effort and not have them build something that will never be used. But like all advice, think carefully about your own situation before committing a lot of time and effort especially in a business setting. It is perfectly fine for you to build useless programs and software in your spare time, that's the whole fun of it.</p>


    

    
        

        
            


        
    


  </main>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Category Theory Illustrated – Natural Transformations (137 pts)]]></title>
            <link>https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/</link>
            <guid>45435422</guid>
            <pubDate>Wed, 01 Oct 2025 08:00:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/">https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/</a>, See on <a href="https://news.ycombinator.com/item?id=45435422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <!--<h1>Natural transformations</h1> -->
        
        

        

<blockquote>
  <p>I didn’t invent categories to study functors; I invented them to study natural transformations. — Saunders Mac Lane</p>
</blockquote>

<p>In this chapter, we will introduce the concept of a morphism between functors, or <em>natural transformation</em>. Understanding natural transformations will enable us to define category equality and some other advanced concepts.</p>

<p>Natural transformations really are at the heart of category theory, however, their importance is not obvious at first. So, before introducing them, I like to talk, once more, about the body of knowledge that this heart maintains (I am good with metaphors… in principle).</p>

<h2 id="equivalent-categories">Equivalent categories</h2>

<p>Our first section aims to introduce natural transformation as a motivating example for creating a way to say that two categories are equal. But for that, we need to understand what equal categories are and should be.</p>

<p>So, are you ready to hear about equivalent categories and natural transformations? Actually it is my opinion that you are not (no offence, they are just very hard!). So, we will take a longer route. I can put this next section anywhere in this book, and it would always be neither here nor there. But anyway, if you are studying math, you are probably interested in the <em>nature of the universe</em>. “What is the quintessential characteristic of all things in this world?” I hear you ask…</p>

<h2 id="objects-are-overrated-aka-heraclitus-was-right">Objects are overrated AKA Heraclitus was right!</h2>

<blockquote>
  <p>The world is the collection of facts, not of things. — Ludwig Wittgenstein</p>
</blockquote>

<p>What is the quintessential characteristic of all things in this world? Some 2500 years ago, the philosopher Parmenides gave an answer to this question, postulating that the nature of the universe is permanence, stasis. According to his view, what we perceive as processes/transformations/change is merely illusory appearances (“Whatever is is, and what is not cannot be”). He said that that things never really change, they only <em>appear</em> to change, or (another way to put it), only appearances change, but the <em>essence</em> does not (I think this is pretty much how the word “essence” came to exist).</p>

<p>Although far from obviously true, his view is easy for people to relate to — objects are all around us, everything we “see”, both literally (in real life), or metaphorically (in mathematics and other disciplines), can be viewed as <em>objects</em>, persisting through space and time. If we subscribe to this view, then we would think that the key to understanding the world is understanding <em>what objects are</em>. In my opinion, this is what set theory does, to some extent, as well as classical logic (Plato was influenced by Parmenides when he created his theory of forms).</p>

<p>However, there is another way to approach the question about the nature of the universe, which is equally compelling. Because, what is an object, when viewed by itself? Can we study an object in isolation? And will there anything left to study about it, once it is detached from its environment? If a given object undergoes a process to get all of it’s part replaced, is it still the same object?</p>

<p>Asking such questions might lead us to suspect that, although what we <em>see</em> when we look at the universe are the objects, it is the processes/relations/transitions or <em>morphisms</em> between the objects that are the real key to understanding it. For example, when we think hard about everyday objects we realize that each of them has a specific <em>functions</em> (note the term) without which, a thing would not be itself e.g. is a lamp that doesn’t glow, still a lamp? Is there food that is non-edible (or an edible item that isn’t food)? And this is even more valid for mathematical objects, which, without the functions that go between them, are not objects at all.</p>

<p>So, instead of thinking about objects that just happen to have some morphisms between them, we might take the opposite view and say <em>that objects are only interesting as sources and targets of morphisms.</em></p>

<p>Although old, dating back to Parmenides’ alleged rival Heraclitus, this view has been largely unexplored, until the 20th century, when a real mathematical revolution happened: Bertrand Russell created type theory, his student Ludwig Wittgenstein wrote a little book, from which the above quote comes, and this book inspired a group of mathematicians and logicians, known as the “Vienna circle”. Part of this group was Rudolph Carnap who coined the word “functor”…</p>

<h2 id="isomorphism-invariance">Isomorphism invariance</h2>

<p>An embodiment of Heraclitus’ view in the realm of category theory is the concept of <em>isomorphism invariance</em> that we implicitly touched several times.</p>

<p>All categorical constructions that we covered (products/coproducts, initial/terminal objects, functional objects in logic) are <em>isomorphism-invariant</em>. Or, equivalently, they define an objects <em>up to an isomorphism</em>. Or, in other words, if there are two or more objects that are isomorphic to one another, and one of them has a given property, then the rest of them would to also have this property as well.</p>

<p>In short, in category theory <strong>isomorphism = equality</strong>.</p>

<p>The key to understanding category theory lies in understanding isomorphism invariance. And the key to understanding isomorphism invariance are natural transformations.</p>

<h2 id="categorical-isomorphisms-are-not-isomorphism-invariant">Categorical isomorphisms are <em>not</em> isomorphism-invariant</h2>

<p>Let’s return to the question that we were pondering at the beginning of the previous chapter — what does it mean for two categories to be equal?</p>

<p>In the prev chapter, we talked a lot about how great isomorphisms are and how important they are for defining the concept of equality in category theory, but at the same time we said that <em>categorical isomorphisms</em> do not capture the concept of equality of categories.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/isomorphic_categories.svg" alt="Isomorphic categories"></p>

<p>This is because (though it may seem contradictory at first) <em>categorical isomorphisms are not isomorphism invariant</em>, i.e. categories that only differ by having some additional isomorphic objects aren’t isomorphic themselves.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equal_categories.svg" alt="Isomorphic categories"></p>

<p>For this reason, we need a new concept of equality of categories. A concept that would elucidate the <em>differences</em> between categories with different structure, but also the <em>sameness</em> of categories that have the same categorical structures, disregarding the differences that are irrelevant for category-theoretic standpoint. That concept is <em>equivalence</em>.</p>

<!--comic-->
<p><strong>Parmenides:</strong> This category surely cannot be equal to the other one — it has a different amount of objects!</p>

<p><strong>Heraclitus:</strong> Who cares bro, they are isomorphic.</p>

<h2 id="equivalences-are-isomorphism-invariant">Equivalences are isomorphism invariant</h2>

<p>To understand equivalent categories better, let’s go back to the functor between a given map and the area it represents (we will only consider the thin categories (AKA orders) for now). This functor would be invertible (and the categories — isomorphic) when the map should represent the area completely i.e. there should be arrow for each road and a point for each little place.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/isomorphic_map.svg" alt="Isomorphic categories"></p>

<p>Such a map is necessary if your goal is to know about all <em>places</em>, however, like we said, when working with category theory, we are not so interested in <em>places</em>, but in the <em>routes</em> that connect them i.e. we focus not on <em>objects</em> but on <em>morphisms</em>.</p>

<p>For example, if there are intersections that are positioned in such a way that there are routes from one and to the other and vice-versa a map may collapse them into one intersection and still show all routes that exist (the tree routes would be represented by the “identity route”).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_map.svg" alt="Equivalent categories"></p>

<p>These two categories are <em>not isomorphic</em> — going from one of them to the other and back again doesn’t lead you to the same object.</p>

<p>However, going from one of them to the other would lead you at least to an <em>isomorphic object</em>.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_map_equivalence.svg" alt="Equivalent categories"></p>

<p>In this case we say that the orders are <em>equivalent</em>.</p>

<h2 id="defining-equivalence-in-terms-of-objects">Defining equivalence in terms of objects</h2>

<p>We know that two orders are isomorphic if there are two functors, such that going from one to the other and back again leads you to the same object.</p>

<p>And two orders are equivalent if going from one of them to the other and back again leads you to the same object, <em>or to an object that is isomorphic to the one you started with.</em></p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders.svg" alt="Equivalent orders"></p>

<p>But when does this happen? To understand this, we plot the orders as a Hasse diagram.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_hasse.svg" alt="Equivalent orders"></p>

<p>You can see that, although not all objects are connected one-to-one, <em>all objects at a given level are connected to objects of the corresponding level</em>.</p>

<p>To formalize that notion, we remember the concept of <em>equivalence classes</em> that we covered in the chapter about orders. Let’s visualize the relationship of the equivalence classes of the two orders that we saw above.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_order_classes.svg" alt="Orders with isomorphic equivalence classes"></p>

<p>You can see that they are isomorphic. And that is no coincidence: two orders are equivalent precisely when the orders made of their equivalence classes are isomorphic.</p>

<p>This is a definition for equivalence of orders, but unfortunately, it does not hold for all categories — when we are working with orders, we can get away by just thinking about <em>objects</em>, but categories demands that we think about morphisms i.e. to prove two categories are equivalent, we should establish an isomorphism between their <em>morphisms</em>.</p>

<p>For example, the following two categories are <em>not</em> equivalent, although their equivalence classes are isomorphic — the category on the left has just one morphism, but the category on the right has two.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/unequal_categories.svg" alt="Non-equivalent categories"></p>

<p>One way of defining equivalence of categories is by generalizing the notion of equivalence classes of orders to what we call <em>skeletons</em> of categories, a skeleton of a category being a subcategory in which all objects that are isomorphic to one another are “merged” into one object (isomorphic objects are necessarily identical).</p>

<p>However, we will leave this (pardon my French) as an <em>exercise for the reader</em>. Why? We already did this when we generalized the notion of normal set-theoretic functions to <em>functors</em>, and so it makes more sense to build up on that notion. Also, we need a motivating example for introducing natural transformations, remember?</p>

<h2 id="defining-equivalence-in-terms-of-morphisms">Defining equivalence in terms of morphisms</h2>

<p>In the chapter about orders, we presented a definition of order <em>isomorphisms</em>, that is based on <em>objects</em>:</p>

<blockquote>
  <p>An order isomorphism is essentially an isomorphism  between the orders’ underlying sets (invertible function). However, besides their underlying sets, orders also have the arrows that connect them, so there is one more condition: in order for an invertible function to constitute an order isomorphism it has to <em>respect those arrows</em>, in other words it should be <em>order preserving</em>. More specifically, applying this function (let’s call it $F$) to any two elements in one set ($a$ and $b$) should result in two elements that have the same corresponding order in the other set (so $a ≤ b$ if and only if $F(a) ≤ F(b)$).</p>
</blockquote>

<p>That a way to define them, but it is not the best way. Now that we know about functors (which, as we said, serve as functions between the orders and other categories), we can devise a new, simpler definition, which would also be valid for all categories, not just orders, and for all forms of equality (isomorphism and equivalence).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/isomorphic_orders.svg" alt="isomorphic orders"></p>

<p>We begin with the definition of <strong>set isomorphism</strong>:</p>

<blockquote>
  <p>Two <strong>sets</strong> $A$ and $B$ are <strong>isomorphic</strong> (or $A ≅ B$) if there exist functions $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.</p>
</blockquote>

<p>To amend it so it is valid for all categories  by just replacing the word “function” with “functor” and “set” with “category”:</p>

<blockquote>
  <p>Two <strong>categories</strong> $A$ and $B$ are <strong>isomorphic</strong> (or $A \cong B$) if there exist <em>functors</em> $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.</p>
</blockquote>

<p><strong>Task 1:</strong> Check if that definition is valid.</p>

<p>Believe it or not, this definition, is just one find-and-replace operation away from the definition of <em>equivalence</em>. We get there only by replace equality with isomorphism (so, $=$ with $\cong$).</p>

<blockquote>
  <p>Two <strong>categories</strong> $A$ and $B$ are <strong>equivalent</strong> (or $A \simeq B$) if there exist <em>functors</em> $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{B}$ and $g \circ f \cong ID_{A}$.</p>
</blockquote>

<p>Like we said at the beginning, with isomorphisms, going back and forth brings us to the same object, while with equivalence the object is just <em>isomorphic</em> to the original one. This is truly all there is to it.</p>

<p>There is only one problem, though — <em>we never said what it means for functors to be isomorphic</em>.</p>

<h2 id="natural-transformations-natural-isomorphisms-and-categorical-equivalence">Natural transformations, natural isomorphisms and categorical equivalence</h2>

<p>So, how can we make the above definition “come to life”? The title of this chapter outlines the things we need to define:</p>

<ol>
  <li><em>Morphisms between functors</em> (called <em>natural transformations</em>).</li>
  <li><em>Functor isomorphisms</em> (called <em>natural isomorphisms</em>).</li>
  <li>Finally <em>categorical equivalences</em>.</li>
</ol>

<p>If this sounds complicated, remember that we are doing the same thing we always did — talking about isomorphisms.</p>

<p>In the very first chapter of this book, we introduced <em>set isomorphisms</em>, which are quite easy, and now we reached the point to examine <em>functor isomorphisms</em>. So, we are doing the same thing. 
<!--comic-->
Although actually…</p>

<p>But actually, natural transformations are quite different from morphisms and functors, (the definition is not “recursive”, like the definitions of functor and morphism are). This is because functions and functors are both morphisms between objects (or <em>1-morphisms</em>), while natural transformations are <em>morphisms between morphisms</em> (known as <em>2-morphisms</em>).</p>

<p>But enough talking, let’s draw some diagrams. We know that natural transformations are morphisms between functors, so let’s draw two functors.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_functors_objects.svg" alt="Two functors"></p>

<p>The functors have the same signature. Naturally. How else can there be morphisms between them?</p>

<p>Now, a functor is comprised of two mappings (object mapping and morphism mapping) so a mapping between functors, would consist of “object mapping mapping” and “morphism mapping mapping” (yes, I often do get in trouble with my choice of terminology, why do you ask?).</p>

<h2 id="object-mapping-mapping">Object mapping mapping</h2>

<p>Let’s first connect the object mappings of the two functors, creating what we called “object mapping mapping”.</p>

<p>It is simpler than it sounds when we realize that we only need to connect the object in functors’ <em>target category</em> — the objects in the source category would just always be the same for both functors, as both functors would include <em>all</em> object from the source category (as that is what functors (and morphisms in general) do). In other words, mapping the two functors’ object components involves nothing more than specifying a bunch of morphisms in the target category: one morphism for each object in the source category i.e. each object from the image of the first functor, should have one arrow coming from it (and to an object of the second functor, so, for example, our current source category has two objects and we specify two morphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation.svg" alt="Two functors and a natural transformation"></p>

<p>Note that this mapping does not map every object from the target category,  i.e. not all objects have arrows coming from them (e.g. here the black and blue square do not have arrows), although, in some cases, it <em>might</em>.</p>

<p><strong>Task 2:</strong> When exactly would the mapping encompass all objects?</p>

<h2 id="morphism-mapping-mapping">Morphism mapping mapping</h2>

<p>The morphism part might seem hard… until we realize that, once the connections between the object mappings are already established, there is only one way to connect the morphisms — we take each morphism of the source category and connect the two morphisms given by the two functors, in the target category. And that’s all there is to it.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_functors.svg" alt="Two functors"></p>

<p>Oh, actually, there is also this condition that the above diagram should commute (the naturality condition), but that happens pretty much automatically.</p>

<h2 id="the-naturality-condition">The naturality condition</h2>

<p>Just like anything else in category theory, natural transformations have some laws that they are required to pass. In this case it’s one law, typically called the naturality law, or the naturality condition.</p>

<p>Before we state this law, let’s recap where are we now: We have two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$, that map each object of the target of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some objects of the image of $G$. This is a <em>transformation</em>, but not necessarily a <em>natural</em> one. A transformation is natural, when this diagram commutes for all morphisms in $C$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation_square.svg" alt="The commuting square of a natural transformation"></p>

<p>i.e. a transformation is natural when every morphism $f$ in $C$ is mapped to morphisms $F(f)$ by $F$ and to $G(f)$ by $G$ (not very imaginative names, I know), in such a way, that we have $\alpha \circ F(f) = G(f) \circ \alpha$ i.e. when starting from the white square, when going right and then down (via the yellow square) is be equivalent to going down and then right (via the black one).</p>

<p>We may view a natural transformation is a mapping between morphisms and commutative squares: two functors and a natural transformation between two categories means that for each morphism in the source category of the functors, there exist one commutative square at the target category.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation_squares.svg" alt="Commuting squares of a natural transformation"></p>

<p>When we fully understand this, we realize that commutative squares are made of morphisms too, so, like morphisms, they compose — for any two morphisms with appropriate type signatures that have we can compose to get a third one, we have two naturality squares which compose the same way.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation_squares_composition.svg" alt="Composition of commuting squares of a natural transformation"></p>

<p>Which means natural transformation make up a…</p>

<p>(Oh wait, it’s too early for that, is it?)</p>

<h2 id="natural-isomorphisms">Natural isomorphisms</h2>

<p>After understanding natural transformations, natural isomorphisms, are a no-brainer: a natural transformation is just a family of morphisms in a given category that satisfy certain criteria, then what would a natural <em>isomorphism</em> be? That’s right — it is a family of <em>isomorphisms</em> that satisfy the same criteria. The diagram is the same as the one for ordinary natural transformation, except that $\alpha$ are not just ordinary morphisms, but isomorphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_isomorphism.svg" alt="Two functors and a natural transformation"></p>

<p>And the turning those morphisms into isomorphisms makes the diagram commute in more than one way i.e. if we have the naturality condition</p>

<p>$\alpha \circ F(f) = G(f) \circ \alpha$ i.e. the two paths going from <strong>white</strong> to <strong>blue</strong> are equivalent.</p>

<p>We also have:</p>

<p>$F(f) \circ  \alpha  =   \alpha  \circ G(f)$ i.e. the two paths going from <strong>black</strong> to <strong>yellow</strong> are also equivalent.</p>

<h2 id="constructing-categorical-equivalences">Constructing categorical equivalences</h2>

<p>I am sorry, what were we talking about again? Oh yeah — categorical equivalence. Remember that categorical equivalence is the reason why we tackle natural transformations and isomorphisms? Or perhaps it was the other way around? Never mind, let’s recap what we discussed so far:</p>

<ol>
  <li>
    <p>At the beginning of the section we introduced the notion of equivalence as two functors, such that going from one of them to the other and back again leads you to the same object, or to an <em>object that is isomorphic</em> to the one you started with.</p>
  </li>
  <li>
    <p>And then, we discussed that for categories that are not thin (thick?) the situation is a bit more complex since they can have more than one morphism between two objects, and we should worry not only about isomorphic objects, but about <em>isomorphic morphisms</em>.</p>
  </li>
</ol>

<p>Now, we will show how these two notions are formalized by the definition that we presented.</p>

<blockquote>
  <p>Two <strong>categories</strong> $A$ and $B$ are <strong>equivalent</strong> (or $A \simeq B$) if there exist <em>functors</em> $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{A}$ and $g \circ f \cong ID_{A}$.</p>
</blockquote>

<p>To understand, this how are the two related, let’s construct the identity functor of the category that we have been using as an example all this time. Note that we are drawing the one and the same category two times (as opposed to just drawing an arrow coming from each object to itself), to make the diagrams more readable.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_identity.svg" alt="The identity functor"></p>

<p>Then, we draw the composite of the two functors that establish an equivalence between the two categories, highlighting the 3 “interesting” objects, i.e. the ones due to which the categories aren’t isomorphic.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_composite.svg" alt="The composite functor between the two functors that make up the equivalence"></p>

<p>Now, we ask ourselves, in which cases does there exist an isomorphism between those two functors?</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_functors.svg" alt="An equivalence diagram"></p>

<p>The answer becomes trivial if we draw the isomorphism arrows connecting the three “interesting” objects in a different way (remember, this is the same category on the top and the bottom) — we can see that these are exactly the arrows that enable us to construct an isomorphism between the two functors (the others are just identity arrows).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_transformation.svg" alt="An equivalence diagram, showing a transformation"></p>

<p>And when would this isomorphism be such that preserves the structure of the category (so that each morphism from the output of the composite functor has an equivalent one in the output of the identity)? Exactly when the isomorphism is <em>natural</em> i.e. when every morphism is mapped to a commuting square, e.g. here is the commuting square of the morphism that is marked in red.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/equivalent_orders_natural_transformation.svg" alt=" An equivalence diagram, showing a natural transformation"></p>

<p>i.e. naturality condition assures us that the morphisms in the target of the functor behave in the same way as their counterparts in the source.</p>

<p>With this, we are finished with categorical equivalence, but not with natural transformations — natural transformations are a very general concept, and categorical equivalences are only a very narrow case of them.</p>

<h2 id="natural-transformations-in-programming-natural-transformations-on-the-list-functor">Natural transformations in programming. Natural transformations on the list functor</h2>

<p>In the course of this book, we learned that programming/computer science is the study of the category of types in programming languages. However (in order to avoid this being too obvious) in the computer science context, we use different terms for the standard category-theoretic concepts.</p>

<p>We learned that objects are known as <em>types</em>, products and coproducts are, respectively, <em>objects/tuple</em> types and <em>sum</em> types. And, in the last chapter, we learned that functors are known as <em>generic types</em>. Now it’s the time to learn what natural transformations are in this context. They are known as <em>(parametrically) polymorphic functions</em>.</p>

<h2 id="pointed-functors-again">Pointed functors again</h2>

<p>Now, suppose this sounds a bit vague. If only we had some example of a natural transformation in programming, that we can use… But wait, we did show a natural transformation in the previous chapter, when we talked about pointed functors.</p>

<p>That’s right, a functor is pointed when there is a natural transformation between it and the identity functor i.e. to have one green arrow for every object/type.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor.svg" alt="Pointed functor"></p>

<p>And this clearly is a natural transformation. As a matter of fact, if we get down to the nitty-gritty, we would see that it resembles a lot the equivalence diagram that we saw earlier — both transformations involve the identity functor, and both transformations have the same category as source and target, that’s why we can put everything in one circle (we don’t do that in the equivalence diagram, but that’s just a matter of presentation).</p>

<p>Actually, the only difference between the two transformations is that an equivalence is defined by a natural <em>natural isomorphism</em> of a given functors to the identity functor ( $ID \cong f \circ g $ and $ID \cong g \circ f$), while a pointed functor is defined by a one-way <em>natural transformation</em> from the identity functor ($ID \to f $)  i.e. the equivalence functor is pointed, but not the other way around).</p>

<h2 id="polymorphic-functions-as-natural-transformations">Polymorphic functions as natural transformations</h2>

<p>We said that a natural transformation is equivalent to a (parametrically) polymorphic function in programming. But wait, wasn’t natural transformation something else (and much more complicated):</p>

<blockquote>
  <p>Two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$. Morphisms that map each object of the target of $F$ (or the image of $F$ in $D$ as it is also called) to some object in the target of $G$.</p>
</blockquote>

<p>Indeed it is (I wasn’t lying to you, in case you are wondering), however, in the case of programming, the source and target categories of both functors are the same category ($Set$), so the whole condition regarding the functors’ type signatures can be dropped.</p>

<blockquote>
  <p>Two <del>functors</del> generic types $F$ and $G$ <del>that have the same type signature</del> and a family of morphisms in $Set$ (denoted $\alpha : Set \Rightarrow Set$) one for each object in $Set$, that map each target object of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some target objects of functor $G$.</p>
</blockquote>

<p>As we know from the last chapter, a functor in programming is a generic type (which, has to have the <code>map</code> function with the appropriate signature).</p>

<p>And what is a “family of morphisms in $Set$ one for each object in $Set$”? Well, the morphisms in the category $Set$ are functions, so that’s just a bunch of functions, one for each type.  In Haskell notation, if we denote a random type by the letter \(a\)), it is $alpha : \forall a. F a \to G a$.  But that’s exactly what polymorphic functions are.</p>

<p>Here is how would we write the above definition in a more traditional language  (we use capital <code>&lt;A&gt;</code> instead of $a$, as customary.</p>

<div><pre><code>
<span>function</span> <span>alpha</span><span>&lt;</span><span>A</span><span>&gt;</span><span>(</span><span>a</span><span>:</span> <span>F</span><span>&lt;</span><span>A</span><span>&gt;</span><span>)</span> <span>:</span> <span>G</span><span>&lt;</span><span>A</span><span>&gt;</span> <span>{</span>
<span>}</span>

</code></pre></div>

<p>Generic types work by replacing the <code>&lt;A&gt;</code> with some concrete type, like <code>string</code>, <code>int</code> etc. Specifically, the natural transformation from the identity functor to the list functor that puts each value in a singleton list looks like this $alpha :: \forall\ a. a \to List\ a$. Or in TypeScript:</p>

<div><pre><code>
<span>function</span> <span>array</span><span>&lt;</span><span>A</span><span>&gt;</span><span>(</span><span>a</span><span>:</span> <span>A</span><span>)</span> <span>:</span> <span>Array</span><span>&lt;</span><span>A</span><span>&gt;</span> <span>{</span>
    <span>return</span> <span>[</span><span>a</span><span>]</span>
<span>}</span>
</code></pre></div>

<h2 id="some-examples-of-natural-transformations">Some examples of natural transformations</h2>

<p>Once we rid ourselves of the feeling of confusion, that such an excessive amount of new terminology and concepts impose upon us (which can take years, by the way), we realize that there are, of course, many polymorphic functions/natural transformations that programmers use.</p>

<p>For example, in the previous chapter, we discussed one natural transformation/polymorphic function the function $\forall a.a \to [a]$ which puts every value in a singleton list. This function is a natural transformation between the identity functor and the list functor.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor_set_transformation.svg" alt="Natural transformation, defining a pointed functor in Set"></p>

<p>This is pretty much the only one that is useful with <em>this</em> signature (the others being $a \to [a, a]$, $a \to [a, a, a]$ etc.), but there are many examples with signature $list\ a \to list\ a$, such as the function to <em>reverse</em> a list.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/reverse_set_transformation.svg" alt="The natural transformation, for reversing a list in Set"></p>

<p>…or <em>take1</em> that retrieves the first element of a list</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/take_set_transformation.svg" alt="The natural transformation, for taking the first element of a list in Set"></p>

<p>or <em>flatten</em> a list of lists of things to a regular list of things (the signature of this one is a little different, it’s $list\ list\ a \to list\ a$).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/flatten_set_transformation.svg" alt="The natural transformation, for flattening a list in Set"></p>

<hr>

<p><strong>Task 3:</strong> Draw example naturality squares of the $reverse$ natural transformation.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/reverse_set_transformation_task.svg" alt="The natural transformation, for reversing a list in Set">
Do the same for the rest of the transformations.</p>

<hr>

<h2 id="the-naturality-condition-1">The naturality condition</h2>

<p>Before, we said that we shouldn’t worry too much about naturality, as it is satisfied every time. Statistically, however, this is not true — as far as I am concerned, about 99.999 percent of transformations aren’t really natural (I wonder if you can compute that percentage properly?). But at the same time, it just so happens (my favourite phrase when writing about maths) that all transformations that we care about <em>are</em> natural.</p>

<p>So, what does the naturality condition entail, in programming? To understand this, we construct some naturality squares of the transformations that we presented.</p>

<p>We choose two types that play the role of $a$, in our case $string$ and $num$ and one natural transformation, like the transformation between the identity functor and the list functor.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor_set.svg" alt="Pointed functor in Set"></p>

<p>The diagram commute when for all functions $f$, applying the $Ff$, the mapped/lifted version of $f$ with one functor (in our case this is just $F f : string \to num$ cause it is the identity functor), followed by ($alpha :: F b \to G\ b$), is equivalent to applying ($alpha:: F a \to G\ a$), and then the mapped version of $f$ with the other functor (in our case $G f :: List\ a \to List\ b$) i.e.</p><p>

\[\alpha \circ F\ f \cong G\ f \circ \alpha\]

</p><p>(in the programming world, you would also see it as something like  $\alpha (map\ f x) = map\ f (\alpha x)$, but note that here $map$ function means two different things on the two sides, Haskell is just smart enough to deduce which $fmap$ to use).</p>

<p>And in TypeScript, when we are talking specifically about the identity functor and the list functor, the equality is expressed as:</p>



<p>So, is this equation true in our case? To verify it, we take one last peak at the world of values.</p>

<p>We acquire an $f$, that is, we a function that acts on simple values (not lists), such as the function $length : string \to num$, which returns the number of characters a string has and convert it, (or <em>lift</em> it, as the terminology goes) to a function that acts on more complex values, using the list functor, (and the higher-order function $map$).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/lifted_function_f.svg" alt="A lifted function"></p>

<p>Then, we take the input and output types for this function (in this case $string$ and $num$), and the two morphisms of a natural transformation (e.g the abstract function $\forall a.a \to [a]$) that correspond to those two types.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor_set_transformations.svg" alt="Pointed functor in Set"></p>

<p>When we compose these two pairs of morphisms we observe that they indeed commute — we get two morphisms that are actually one and the same function.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/pointed_functor_set_internal.svg" alt="Pointed functor in Set"></p>

<p>The above square shows the transformation $\forall a.a \to [a]$ (which is between the identity functor and the list functor, here is another one, this time between the list functor and itself ($\forall a.[a] \to [a]$) — $reverse$</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/reverse_set_internal.svg" alt="Pointed functor in Set"></p>

<p>(and you can see that this would work not just for $length$, but for any other function).</p>

<p>So, why does this happen? Why do these particular transformations make up a commuting square for each and every morphism?</p>

<p>The answer is simple, at least in our specific case: the original, unlifted function $f :: a \to b$ (like our $length :: string \to num$) can only work on the individual values (not with structure), while the natural transformation functions, i.e. ones with signature  $list :: a \to list\ a$ only alter the structure, and not individual values. The naturality condition just says that these two types of functions can be applied in any order that we please, without changing the end result.</p>

<p>This means that if you have a sequence of natural transformations that you want to apply, (such as $reverse$ , $take$, $flatten$ etc) and some lifted functions ($F f$, $F g$), you can mix and match between the two sequences in any way you like and you will get the same result e.g.</p><p>

\[take1 \circ reverse \circ F\ f \circ F\ g\]

</p><p>is the same as</p><p>

\[take1 \circ F\ f \circ reverse \circ F\ g\]

</p><p>…or…</p><p>

\[F\ f \circ F\ g \circ take1 \circ reverse\]

</p><p>…or any other such sequence (the only thing that isn’t permitted is to flip the members of the two sequences — ($take1 \circ reverse$ is of course different from $reverse \circ take1$and if you have $F\ f \circ F\ g$, then $F\ g \circ F\ f$ won’t be permitted at all due to the different type signatures).</p>

<p><strong>Task 4:</strong> Prove the above results, using the formula of the naturality condition.</p>

<h2 id="non-natural-transformations">Non-natural transformations</h2>

<p>“Unnatural”, or “non-natural” transformations (let’s call them just <em>transformations</em>) are mentioned so rarely, that we might be inclined to ask if they exist. The answer is “yes and no”. Why yes? On one hand, transformations, consist of an innumerable amount of morphisms, forming an ever more innumerable amount of squares and obviously nothing stops some of these squares to be non-commuting.</p>

<p>For example, if we substitute one morphism from the family of morphisms that make up the natural transformation with some other random morphism that has the same signature, all squares that have this morphism as a component would stop commuting.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/unnatural_transformation_squares.svg" alt="Unnatural transformation"></p>

<p>This would result in something like an “almost-natural” transformation (e.g. an abstract function that reverses all lists, except lists of integers).</p>

<p>And in the category of sets, where morphisms are functions i.e. mappings between values, it is enough to move just one arrow of just one of those values in order to make the transformation “unnatural” (e.g. a function which reverses all lists, but one specific list).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/reverse_set_unnatural.svg" alt="Unnatural transformation in set --- like reverse, but one arrow is off"></p>

<p>Finally, if can just gather a bunch of random morphisms, one for each object, that fit the criteria, we get what I would call a “perfectly unnatural transformation” (but this is my terminology).</p>

<p>But, although they do exist, it is very hard to define non-natural transformations. For example, for categories that are <em>infinite</em>, there is no way to specify such “perfectly unnatural transformation” (ones where none of the squares commute) without resorting to randomness. And even transformations on finite categories, or the “semi-natural” transformations which we described above (the ones that include a single condition for a single value or type), are not possible to specify in some languages e.g. you can define such a transformation in Typescript, but not in Haskell.</p>

<p>To see why, let’s see what the type of a natural transformation is.</p><p>

\[\forall\ a.\ F a \to G a\]

</p><p>The key is that the definition should be valid <em>for all</em> types a. For this reason, there is no way for us to specify a different arrows for different types, without resorting to type downcasting, which is not permitted in languages like Haskell (as it breaks the principle of parametricity).</p>

<!--

-->

<h2 id="natural-transformations-again">Natural transformations again</h2>

<p>Now, after we saw the definition of natural transformations, it is time to see the definition of natural transformations (and if you feel that the quality of the humour in this book is deteriorating, that’s only because <em>things are getting serious</em>).</p>

<p>Let’s review again the commuting diagram that represents a natural transformation.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_functors.svg" alt="Two functors"></p>

<p>This diagram might prompt us into viewing natural transformations as some kind of “two-arrow functors” that have not one but two arrows coming from each of their morphisms — this notion, can be formalized, by using <em>product categories</em>.</p>

<p>Oh wait, I just realized we never covered product categories… but don’t worry, we will cover them now.</p>

<h2 id="product-groups-and-product-categories">Product groups and product categories</h2>

<p>We haven’t covered product categories, however some pages ago, when we covered monoids and groups, we talked about the concept of a <em>product group</em>. The good news is that product <em>categories</em> are a generalization of product <em>groups</em>…</p>

<p>The bad news is that you probably don’t remember much about product groups, as covered them briefly.</p>

<p>But don’t worry, we will do a more in-depth treatment now:</p>

<h2 id="product-groups">Product groups</h2>

<p>Given two groups $G$ and $H$, whose sets of elements can also be denoted $G$ and $H$…</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/groups_product.svg" alt="The Klein four as a product group"></p>

<p>(in this example we use two boolean groups, which we visualize as the groups of horizontal and vertical rotation of a square)</p>

<p>…the <em>product group</em> of these two groups is a group that has the cartesian product of these two sets $G \times H$ as its set of elements.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/klein_four_underlying_set.svg" alt="The Klein four as a product group"></p>

<p>And what can the group operation of such a group be? Well, I would say that out of the few possible groups operations for this set that <em>exist</em>, this is the <em>only</em> operation that is <em>natural</em> (I didn’t intend to involve natural transformation at this section, but they really do appear everywhere). So, let’s try to derive the operation of this group.</p>

<p>We know what a group operation is, in principle: A group operation combines two elements from the group into a third element i.e. it is a function with the following type signature:</p><p>

\[\circ :  (A, A) \to A\]

</p><p>or equivalently</p><p>

\[\circ :  A \to A \to A\]

</p><p>And for product groups, we said that the underlying set of the group (which we dubbed $A$ above) is a cartesian product of some other two sets which we dubbed $G$ and $H$. So, when we swap $A$ for $G \times H$ the definition becomes:</p><p>

\[\circ : G \times H \to G \times H \to G \times H\]

</p><p>i.e. the group operation takes one pair of elements from $G$ and $H$ and another pair of elements from $G$ and $H$, only to return — guess what — a pair of elements $G$ and $H$.</p>

<p>Let’s take an example. To avoid confusion, we take two totally different groups — the color-mixing group and the group of integers under addition. That would mean that a value of $G \times H$ would be a pair, containing a random color and a random number, and the operation would combine two combine two such pairs and produce another one.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_group_equations.svg" alt="Equations of the product of numbers and colors"></p>

<p>Now, the operation must produce a pair, containing a number and a color. Furthermore, it would be good if it produces a number <em>by using those two numbers</em>, not just picking one at random, and likewise for colors. And furthermore, we want it to work not just for monoids of numbers and colors, but all other monoids that can be given to us. It is obvious that there is only one solution, to get the elements of the new pair by combining the elements of the pairs given.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_group_solutions.svg" alt="Solutions of the product of numbers and colors"></p>

<p>And the operation of the product group of the two boolean groups which we presented earlier is the combination of the two operations</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/klein_four_as_product.svg" alt="The Klein four as a product group"></p>

<p>So, the general definition of the operation is the following ($g1$, $g2$ are elements of $G$ and $h1$ and $h2$ elements of $H$).</p><p>

\[(g1, h1) \circ (g2, h2) = ( (g1 \circ g2), (h1 \circ h2))\]

</p><p>And that are product groups.</p>

<h2 id="product-categories">Product categories</h2>

<p>We are back at tackling product <em>categories</em>.</p>

<p>Since we know what product <em>groups</em> are, and we know that groups are nothing but categories with just one object (and the group objects are the category’s morphisms, remember?), we are already almost there.</p>

<p>Here is a way to make a product category.</p>

<p>Take any two categories:</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_components.svg" alt="Product category - components"></p>

<p>Then take the set of all possible pairs of the objects of these categories.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_set.svg" alt="Product category - objects"></p>

<p>And, finally, we make a category out of that set by taking all morphisms coming from any of the two categories and replicate them to all pairs that feature some objects from their type signature, in the same way as we did for product groups (in this example, only one of the categories has morphisms).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_category.svg" alt="Product category"></p>

<p>This is the <em>product category</em> of the two categories.</p>

<h2 id="natural-transformations-as-functors-of-product-categories">Natural transformations as functors of product categories</h2>

<p>In this section we are interested with the products of one particular category, namely the category we called $2$, containing two objects and one morphism (stylishly represented in black and white).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/category_two.svg" alt="The category 2"></p>

<p>This category is the key to constructing a functor that is equivalent to a natural transformation:</p>

<ul>
  <li>Because it has two objects, it produces two copies of the source category.</li>
  <li>because the two objects are connected, the two copies are connected in the same way as the two “images” in the target category are connected.</li>
</ul>

<p>So, given a product category of $2$ and some other category $C$…</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_category_target_category.svg" alt="The category 2"></p>

<p>…there exist a natural transformation between $C$ and the product category $2\times C$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/product_category_natural_transformation.svg" alt="Product category"></p>

<p>Furthermore, this connection is two-way: any natural transformation from $C$ to some other category (call it $D$, as it is customary) can be represented as a functor $2 \times C \to D$.</p>

<p>That is, if we have a natural transformations $\alpha : F \Rightarrow G$ (where  $F: C \to D$ and  $G: C \to D$), then, we also have a functor  $2 \times C \to D$, such that if we take the subcategory of $2 \times C$ comprised of just those objects that have the $0$ object as part of the pair, and the morphisms between them, we get a functor that is equivalent to $F$, and if we consider the subcategory that contains $1$, then the functor is equivalent to $G$ (we write $\alpha(-,0)=F$ and $\alpha(-,1)=G$). Et voilà!</p>

<p><strong>Task 5:</strong> Show that the two definitions are equivalent.</p>

<p>This perspective helps us realize that a natural transformation can be viewed as a collection of commuting squares. The source functor defines the left-hand side of each square, the target functor — the right-hand side, and the transformation morphisms join these two sides.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/natural_transformation_notation.svg" alt="Notation for natural transformation"></p>

<p>We can even retrieve the structure of the source category of these functors, which (as categories are by definition structure and nothing more) is equivalent to retrieving the category itself.</p>

<!--

-->

<h2 id="composing-natural-transformations">Composing natural transformations</h2>

<p>Natural transformations are surely a different beast than normal morphisms and functors and so they don’t compose in the same way. However, they do compose and here we will show how.</p>

<h2 id="the-identity-natural-transformation">The identity natural transformation</h2>

<p>Let’s first get one trivial definition out of the way: for each functor, we have the identity natural transformation (actually a natural isomorphism) between it and itself.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/identity_natural_transformation.svg" alt="The identity natural transformation"></p>

<h2 id="horizontal-composition">Horizontal composition</h2>

<p>The setup for composing natural transformations may look complicated the first time you see it: we need three categories $C$, $D$ and $E$ (just as composition of morphisms requires three objects). We need a total of four functors, distributed on two pairs, one pair of functors that goes from $C$ to $D$ and one that goes from $D$ to $E$ (so we can compose these two pairs of functors together, to get a new pair of functors that go $C \to E$). However, we will try to keep it simple and we will treat the natural transformation as a map from a morphism to a commuting square. As we showed above, this mapping already contains the two functors in itself.</p>

<p>So, let’s say that we have the natural transformation $\alpha$ involving the $C \to D$ functors (which we usually call $F$ and $G$).</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_notation.svg" alt="Notation for natural transformation"></p>

<p>So, what will happen if we have one more transformation $\bar\alpha$ involving the functors that go $D \to E$ (which are labelled $F’$ and $G’$)? Well, since a natural transformation maps each morphism to a square, and a square contains four morphisms (two projections by the two functors and two components of the transformation), a square would be mapped to four squares.</p>

<p>Let’s start by drawing two of them for each projection of the morphism in $C$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_squares.svg" alt="Horizontal composition of natural transformation"></p>

<p>We have to have two more squares, corresponding to the two morphisms that are the components of the $\alpha$ natural transformation. However, these morphisms connect the objects that are the target of the two functors, objects that we already have on our diagram, so we just have to draw the connections between them.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition.svg" alt="Horizontal composition of natural transformation"></p>

<p>The result is an interesting structure which is sometimes visualized as a cube.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_cube.svg" alt="Horizontal composition of natural transformation"></p>

<p>More interestingly, when we compose the commuting squares from the sides of the cube horizontally, we see that it contains not one, but two bigger commuting squares (they look like <em>rectangles</em> in this diagram), visualized in grey and red. Both of them connect morphisms $F’Ff$ and $G’Gf$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_cube_commuting.svg" alt="Horizontal composition of natural transformation"></p>

<p>So, there is a natural transformation between the composite functor $F’ \circ F : C \to E$ and $G’ \circ G : C \to E$ — a natural transformation that is usually marked $\bar\alpha \bullet \alpha$ (with a black dot).</p>

<p><strong>Task 6:</strong> Show that natural transformations indeed compose i.e. that if you have natural transformations $F’Ff \Rightarrow F’Gf$  and  $F’Gf \Rightarrow G’Gf$ you have $F’Ff \Rightarrow G’Gf$.</p>

<h2 id="whiskering">Whiskering</h2>

<p>And an interesting special case of horizontal composition is horizontal composition involving the identity natural transformation: given a natural transformation $\bar\alpha$ involving functors with signature $D \to E$ and some functor with signature $F : C \to D$, we can take $\alpha$ to be the identity natural transformation between functor $F$ and itself and compose it with $\bar\alpha$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/horizontal_composition_whiskering.svg" alt="Horizontal composition of natural transformation"></p>

<p>We get a new natural transformation $\bar\alpha \bullet \alpha$, that is practically the same as the one we started with (i.e. the same as $\bar\alpha$) so what’s the deal? We just found a way to <em>extend</em> natural transformations, using functors: i.e   we can use a functor with signature $C \to D$ to extend a $D \to E$ natural transformation and make it $C \to E$.</p>

<p><strong>Task 7</strong>: Try to extend the natural transformation in the other direction (by taking $\bar\alpha$ to be identity).</p>

<p>So, this is how you compose natural transformations. It’s too bad that this is form of composition is different from the standard categorical composition. So, I guess natural transformations do not form a category, like we hoped they would…</p>

<p>Well, OK, there is actually another way of composing categories, which might actually work.</p>

<h2 id="vertical-composition">Vertical composition</h2>

<p>Recall that categorical composition involves three objects and two successive arrows between them. For vertical composition of natural transformations, we will need three (or more) <em>functors</em> with the same type signature, say $F, G, H: C \to D$ i.e. (same source and target category) and two successive <em>natural transformations</em> between those functors i.e. $\alpha: F \to G$ and $\beta: G \to H$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition.svg" alt="Vertical composition of natural transformations"></p>

<p>We can combine each morphism of the natural transformation $\alpha$ (e.g. $a: F \to G$) and the corresponding morphism of the natural transformation $\beta$ (say $b:G \to H$) to get a new morphism, which we call $b \circ a : F \to H$ (the composition operator is the  usual white circle, as opposed to the black one, which denotes horizontal composition). And the set of all such morphisms are precisely the components of a new natural transformation: $\beta \circ \alpha : F \to H$.</p>

<h2 id="categories-of-functors">Categories of functors</h2>

<p>Now, we are approaching the end of the chapter, we will introduce our category and call it quits. To do that, we first introduce a more compressed notation for vertical composition of natural transformations (where they do indeed look vertical).</p>

<p>We started this chapter by looking at category of sets and using internal diagrams, displaying the set elements as points and the sets/objects as collections.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition_internal.svg" alt="Vertical composition of natural transformations - internal diagram"></p>

<p><strong>Task 8:</strong> identify the function, the three functors, and the two natural transformations used in this diagram.</p>

<!--
answer
(A little note, if you want to understand the diagram better: $F$ and $G$ are the $List$ functor, $H$ is the $ID$ functor, $\alpha$ is  $reverse: List \to List$ and $\beta$ is $head : List \to ID$ and $f$ is $length : string \to int$)
-->

<p>Then, we quickly passed to normal external diagrams, where objects are points and categories are collections.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition.svg" alt="Vertical composition of natural transformations"></p>

<p>And now we go one more level further, and show the category of categories, where categories are points and functors are morphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition_cat.svg" alt="Vertical composition of natural transformations in Cat"></p>

<p>In this notation, we display natural transformations as (double) arrows between morphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/vertical_composition_cat_2.svg" alt="Vertical composition of natural transformations in Cat"></p>

<p>And you can already see the new category that is formed: For each two categories (like $C$ and $D$ in this case), there exists a category which has functors for objects and natural transformations as morphisms.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/functor_category.svg" alt="Vertical composition of natural transformations in Cat"></p>

<p>Natural transformations compose with vertical compositions, and, of course, the identity natural transformation is the identity morphism.</p>

<h2 id="interchange-law">Interchange law</h2>

<p>Vertical and horizontal composition of natural transformations are related to each other in the following way:</p>

<p>If we have (as we had) two successive natural transformations, in the vertical sense, like $\alpha: F \to G$ and $\beta: G \to H$.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/interchange_law_horizontal.svg" alt="The interchange law -- horizontal component"></p>

<p>And two successive ones, this time in horizontal sense e.g. $\bar\alpha: F’ \to G’$ and $\bar\beta: G’ \to H’$. (note that $\alpha$ has nothing to do with $\bar\alpha$ as $\beta$ has nothing to do with $\bar\beta$, we just call them that way to avoid using too many letters)</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/interchange_law_vertical.svg" alt="The interchange law -- vertical component"></p>

<p>And if the two pairs of natural transformations both start from the same category and the same functor, then the compositions of the two pairs of natural transformations obey the following law</p><p>

\[(β \circ α) \bullet (\bar β \circ \bar α) = (β \bullet \bar β) \circ (α \bullet \bar α)\]

</p><hr>

<p><strong>Task 9:</strong> Draw the paths of the two compositions of the transformations (on the two sides of the equation) and ensure that they indeed lead to the same place.</p>

<p><img src="https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/interchange_law.svg" alt="The interchange law"></p>

<hr>

<h2 id="2-categories">2-Categories</h2>

<p>At this point you might be wondering the following (although statistically you are more likely to wonder what the heck is all this about): We know that all categories are objects of $Cat$, the category of small categories, in which functors play the role of morphisms.</p>

<p>But, functors between given categories also form a category, under vertical composition. Which means that $Cat$ not only has (as any other category) morphisms between objects, <em>but</em> also has <em>morphisms between morphisms</em>. And furthermore, those two types of morphisms compose in this very interesting way.</p>

<p>So, what does that make of $Cat$? I don’t know, perhaps we can call natural transformations “2-morphisms” and $Cat$ is some kind of “2-category”?</p>

<p>But wait, actually it’s way too early for you to find out. We haven’t even covered limits…</p>

<!--

-->


        
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Type Theory and Functional Programming (1999) [pdf] (148 pts)]]></title>
            <link>https://www.cs.cornell.edu/courses/cs6110/2015sp/textbook/Simon%20Thompson%20textbook.pdf</link>
            <guid>45435100</guid>
            <pubDate>Wed, 01 Oct 2025 07:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cs.cornell.edu/courses/cs6110/2015sp/textbook/Simon%20Thompson%20textbook.pdf">https://www.cs.cornell.edu/courses/cs6110/2015sp/textbook/Simon%20Thompson%20textbook.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45435100">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[High-resolution efficient image generation from WiFi Mapping (109 pts)]]></title>
            <link>https://arxiv.org/abs/2506.10605</link>
            <guid>45434941</guid>
            <pubDate>Wed, 01 Oct 2025 06:33:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2506.10605">https://arxiv.org/abs/2506.10605</a>, See on <a href="https://news.ycombinator.com/item?id=45434941">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2506.10605">View PDF</a>
    <a href="https://arxiv.org/html/2506.10605v3">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>We present LatentCSI, a novel method for generating images of the physical environment from WiFi CSI measurements that leverages a pretrained latent diffusion model (LDM). Unlike prior approaches that rely on complex and computationally intensive techniques such as GANs, our method employs a lightweight neural network to map CSI amplitudes directly into the latent space of an LDM. We then apply the LDM's denoising diffusion model to the latent representation with text-based guidance before decoding using the LDM's pretrained decoder to obtain a high-resolution image. This design bypasses the challenges of pixel-space image generation and avoids the explicit image encoding stage typically required in conventional image-to-image pipelines, enabling efficient and high-quality image synthesis. We validate our approach on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi devices and cameras; and a subset of the publicly available MM-Fi dataset. The results demonstrate that LatentCSI outperforms baselines of comparable complexity trained directly on ground-truth images in both computational efficiency and perceptual quality, while additionally providing practical advantages through its unique capacity for text-guided controllability.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Eshan Ramesh [<a href="https://arxiv.org/show-email/ba45c6b6/2506.10605" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2506.10605v1" rel="nofollow">[v1]</a></strong>
        Thu, 12 Jun 2025 11:47:23 UTC (6,672 KB)<br>
            <strong><a href="https://arxiv.org/abs/2506.10605v2" rel="nofollow">[v2]</a></strong>
        Fri, 4 Jul 2025 12:27:28 UTC (6,672 KB)<br>
    <strong>[v3]</strong>
        Fri, 5 Sep 2025 11:39:36 UTC (9,960 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US government shuts down after Senate fails to pass last-ditch funding plan (145 pts)]]></title>
            <link>https://www.bbc.com/news/live/clylje0rmp2t</link>
            <guid>45434146</guid>
            <pubDate>Wed, 01 Oct 2025 04:05:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/live/clylje0rmp2t">https://www.bbc.com/news/live/clylje0rmp2t</a>, See on <a href="https://news.ycombinator.com/item?id=45434146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="list" spacing="6" tabindex="0" data-testid="postList"><li><div><article data-testid="content-post" id="asset:616adca4-cb19-483b-9d2f-1bd5ea2db538"><header><span><h3 type="normal"><span role="text"><span>What about the mail?</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 11:17 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="United States Postal Service delivery vehicle" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/2e247a53-d5ab-4d6e-acec-0858bf7ae774.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/2e247a53-d5ab-4d6e-acec-0858bf7ae774.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/2e247a53-d5ab-4d6e-acec-0858bf7ae774.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/2e247a53-d5ab-4d6e-acec-0858bf7ae774.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/2e247a53-d5ab-4d6e-acec-0858bf7ae774.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/2e247a53-d5ab-4d6e-acec-0858bf7ae774.jpg.webp 800w" width="1024" height="576"></span><span role="text"><span>Image source, </span>Getty Images</span></p></figure><p>The US Postal Service has previously been unaffected by government shutdowns, and it says this time it will be no different.</p><p>In a statement posted on its website earlier this week, the US Postal Service says all post offices will remain open for business as usual. </p><p>This is because it's an independent entity that is generally funded through the sale of its products and services, and not by tax dollars.</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:0c56d807-2e0e-4888-9890-0b596bc88755"><header><span><h3 type="normal"><span role="text"><span>How long could this shutdown last?</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 10:42 BST</span></span></span></span></h3></span></header>
      <p>We don't know exactly how long this government shutdown will last as it will only be resolved once Congress passes a funding bill.</p><p>As we've been reporting, this is not the first time such a shutdown has happened. </p><p>The longest in American history lasted 35 days and took place during Trump's first term in office.</p><p>In the past, shutdowns have ranged in length:</p><ul><li spacing="rich-text">
      The second longest shutdown was in December 1995 - <b>it lasted 21 days </b>
    </li><li spacing="rich-text">
      During Barack Obama's presidency, a shutdown <b>lasted 16 days</b> 
    </li></ul><ul><li spacing="rich-text">
      Between 1982 and 1987, four government shutdowns lasted <b>just one day each</b>
    </li><li spacing="rich-text">
      Under former President Bill Clinton, a federal shutdown <b>lasted 5 days</b>
    </li></ul>
    </article></div></li><li><div><article data-testid="content-post" id="asset:4a2940eb-82df-4826-9eab-2281f0bc8db5"><header><span><h3 type="normal"><span role="text"><span>Are you a federal worker in the US? Get in touch</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 10:41 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="Banner reading 'Your Voice Your BBC News', with headshots of three people" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/411ada14-5fe3-4dc2-a80d-29ed2668bb92.png.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/411ada14-5fe3-4dc2-a80d-29ed2668bb92.png.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/411ada14-5fe3-4dc2-a80d-29ed2668bb92.png.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/411ada14-5fe3-4dc2-a80d-29ed2668bb92.png.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/411ada14-5fe3-4dc2-a80d-29ed2668bb92.png.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/411ada14-5fe3-4dc2-a80d-29ed2668bb92.png.webp 800w" width="1510" height="110"></span></p></figure><p>If you're a federal worker in the US, and feel comfortable sharing your experience about the government shutdown, please consider reaching out.</p><p>You can get in touch in the following ways:   </p><ul><li spacing="rich-text">
      <a href="https://www.bbc.co.uk/send/u218210015">Use this form</a>
    </li></ul><ul><li spacing="rich-text">
      Email: <a href="mailto:bbcyourvoice@bbc.co.uk?subject=USGovernmentShutdown">bbcyourvoice@bbc.co.uk, external<span>, <!-- -->external</span></a>  
    </li></ul><ul><li spacing="rich-text">
      WhatsApp: <a href="https://api.whatsapp.com/send?phone=+447756165803">+44 7756 165803 , external<span>, <!-- -->external</span></a>  
    </li></ul><ul><li spacing="rich-text">
      <a href="https://www.bbc.co.uk/send/u16904890">Upload your pictures and video</a>   
    </li></ul><p> Please read our <a href="http://www.bbc.co.uk/usingthebbc/terms/">terms &amp; conditions</a> and <a href="http://www.bbc.co.uk/usingthebbc/privacy-policy/">privacy policy</a>.</p><p><i>In some cases a selection of your comments and questions will be published, displaying your name and location as you provide it unless you state otherwise. Your contact details will never be published.   </i></p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:e93cff57-e3a2-44c0-b2a6-e5694af145ef"><header><span><h3 type="normal"><span role="text"><span>Democrats call for 'credible' bipartisan action to end shutdown</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 10:09 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="Two men in suits at a podium" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/f0635c76-8333-4615-be75-3c848aa02660.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/f0635c76-8333-4615-be75-3c848aa02660.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/f0635c76-8333-4615-be75-3c848aa02660.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/f0635c76-8333-4615-be75-3c848aa02660.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/f0635c76-8333-4615-be75-3c848aa02660.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/f0635c76-8333-4615-be75-3c848aa02660.jpg.webp 800w" width="997" height="561"></span><span role="text"><span>Image source, </span>Reuters</span></p><figcaption><span>Image caption, </span><p>Chuck Schumer, the US Senate's top Democrat, on the right, and House Minority Leader Hakeem Jeffries, on the left, address the press</p></figcaption></figure><p>Two senior Democrats have released a joint statement calling for a bipartisan effort to end the government shutdown. </p><p>"Democrats remain ready to find a bipartisan path forward to reopen the government," says the statement, signed by Chuck Schumer, the US Senate's top Democrat, and House Minority Leader Hakeem Jeffries. </p><p>Continuing <a href="https://www.bbc.co.uk/news/live/clylje0rmp2t?post=asset%3A3b25b328-e9ea-41ed-83e7-fdad872cd031#post">the finger-pointing we've seen so far</a>, the statement places the blame for the shutdown on the Republicans. </p><p>"Donald Trump and the Republicans have now shut down the federal government because they do not want to protect the healthcare of the American people," it continues. </p><p>The Republicans, however, blame the Democrats for the impasse, with House Speaker Mike Johnson calling the party's demand to extend Obamacare tax credits a "red herring". </p><ul><li spacing="rich-text">
      <i>For more details on how we got here, read our </i><a href="https://www.bbc.co.uk/news/live/clylje0rmp2t?post=asset%3Ac912b3a0-5b46-4fd6-a7d7-028abc122bdb#post">explainer post from earlier today</a>
    </li></ul>
    </article></div></li><li><div><article data-testid="content-post" id="asset:c832e9fc-4982-4f5b-9fd8-3309dfce1318"><header><span><h3 type="normal"><span role="text"><span>Are flights still taking off - and three other big questions answered</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 09:47 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="TSA agents inspect security goods at an airport" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/15827c37-cbd5-438b-86b0-aa6f27c8b7c3.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/15827c37-cbd5-438b-86b0-aa6f27c8b7c3.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/15827c37-cbd5-438b-86b0-aa6f27c8b7c3.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/15827c37-cbd5-438b-86b0-aa6f27c8b7c3.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/15827c37-cbd5-438b-86b0-aa6f27c8b7c3.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/15827c37-cbd5-438b-86b0-aa6f27c8b7c3.jpg.webp 800w" width="1024" height="576"></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>Transportation Safety Administration (TSA) employees will continue working during the government shutdown</p></figcaption></figure><p><b>Will flights take off?</b></p><ul><li spacing="rich-text">
      Air traffic control and Transportation Safety Administration (TSA) workers are considered "essential" - <a href="https://www.bbc.co.uk/news/live/clylje0rmp2t?post=asset%3A7a1e9d35-5e8b-460e-811a-b9bbdceaad23#post">so will continue to work</a> 
    </li><li spacing="rich-text">
      But during the last shutdown, these workers increasingly began to call in sick, leading to delays in security checks at major airports
    </li><li spacing="rich-text">
      Flight systems might also "need to slow down, reducing efficiency" - according to airline representative Airlines for America
    </li></ul><p><b>Will government employees be paid?</b></p><ul><li spacing="rich-text">
      Around 750,000 federal workers will be taking unpaid leave each day, according to an estimate from the Congressional Budget Office (CBO)
    </li><li spacing="rich-text">
      The total daily cost of their compensation will be roughly $400m (£297m), the CBO says 
    </li><li spacing="rich-text">
      Members of Congress, <a href="https://www.bbc.co.uk/news/live/clylje0rmp2t?post=asset%3Adb4d8c3b-668e-4c19-b907-f5e16949903f#post">as we just reported</a>, will continue getting paid as its required by the US Constitution
    </li></ul><p><b>Will mail be delivered?</b></p><ul><li spacing="rich-text">
      The US Postal Service would be unaffected because it does not depend on Congress for funding
    </li><li spacing="rich-text">
      Post offices will stay open
    </li></ul><p><b>What about law enforcement services?</b></p><ul><li spacing="rich-text">
      Law enforcement officers will continue to work through the government shutdown - though over <a href="https://www.bbc.co.uk/news/live/clylje0rmp2t?post=asset%3Acacac2a3-9891-47f2-8e13-fe74c7248dff#post">200,000 of them will do so unpaid</a>
    </li><li spacing="rich-text">
      Also continuing their work as usual are those in border protection, in-hospital medical care, and air-traffic control
    </li></ul>
    </article></div></li><li><div><article data-testid="content-post" id="asset:d02da3fd-29da-4bd1-bf26-c8d155332105"><header><span><img src="https://static.files.bbci.co.uk/core/website/assets/static/news/incident-types/analysis.77b314ef10.svg" alt="Analysis" draggable="false"><h3 type="normal"><span role="text"><span>Why government shutdowns seem to be a uniquely American problem</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 09:28 BST</span></span></span></span></h3></span></header><p><span><strong>Robert Levinson King and Anthony Zurcher</strong><br>BBC News</span></p>
      <figure><p><span><img alt="The US Capitol building" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/befab3e5-be76-4123-bc17-dfb79f2b5f14.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/befab3e5-be76-4123-bc17-dfb79f2b5f14.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/befab3e5-be76-4123-bc17-dfb79f2b5f14.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/befab3e5-be76-4123-bc17-dfb79f2b5f14.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/befab3e5-be76-4123-bc17-dfb79f2b5f14.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/befab3e5-be76-4123-bc17-dfb79f2b5f14.jpg.webp 800w" width="1024" height="683"></span><span role="text"><span>Image source, </span>Getty Images</span></p></figure><p>The US government has now shut down eleven times over the past 40-plus years. </p><p>Meanwhile, in other countries, governments keep functioning, even in the midst of wars and constitutional crises. </p><p>So why does this uniquely American phenomenon keep happening?</p><p>America's federal system of government allows different branches of government to be controlled by different parties - a structure devised by the nation's founders to encourage deliberation.</p><p>That was until 1980. A narrow interpretation of the 1884 Anti-Deficiency Act, during Jimmy Carter's presidency, banned the government from entering into contracts without congressional approval. This took took a much stricter view: no budget, no spending.</p><p>That interpretation has set the US apart from other non-parliamentary democracies. </p><p>Now in the US, warring political parties seem all-too willing to use the day-to-day functioning of the government as a bargaining chip to extract demands from the other side.</p><ul><li spacing="rich-text">
      <i>Read more on </i><a href="https://www.bbc.co.uk/news/world-us-canada-66965637">how US government shutdowns have become a perennial phenomenon</a>
    </li></ul>
    </article></div></li><li><div><article data-testid="content-post" id="asset:db4d8c3b-668e-4c19-b907-f5e16949903f"><header><span><h3 type="normal"><span role="text"><span>Are people working for free now?</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 09:01 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="A man with glasses, a shirt and black jacket" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/524721f8-2629-4114-98d4-a1534e9be20e.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/524721f8-2629-4114-98d4-a1534e9be20e.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/524721f8-2629-4114-98d4-a1534e9be20e.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/524721f8-2629-4114-98d4-a1534e9be20e.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/592/cpsprodpb/vivo/live/images/2025/10/1/524721f8-2629-4114-98d4-a1534e9be20e.jpg.webp 592w" width="592" height="333"></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>Senator Andy Kim said he would forgo pay in the event of a government shutdown</p></figcaption></figure><p>As we just mentioned, there are thousands of essential workers who will still have to show up for work during the government shutdown. </p><p>While that is happening, federal workers will not receive new payslips.</p><p>So, depending on when their payday falls and how long the shutdown lasts for, many workers may not get paid on time. </p><p>Furloughed and essential employees will instead receive backpay when the government opens up again.</p><p>There is an exception to this: members of Congress, whose pay is protected under the US Constitution. </p><p>It's a convention that's been opposed by some lawmakers.</p><p>Andy Kim, a Democratic senator for New Jersey, said in a statement yesterday that he would forgo pay in the event of a shutdown, saying "government leaders shouldn't be playing with other people's chips".</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:59225d2e-11b7-46b6-ac27-4f80cb30e87c"><header><span><h3 type="normal"><span role="text"><span>Who has to show up for work, and who is staying home</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 08:48 BST</span></span></span></span></h3></span></header>
      <p>Ahead of the government shutdown, federal agencies began sharing their plans on their respective websites. </p><p>Some departments, like the Department Homeland Security, are <b>retaining</b> the majority of their staff - this means they are required to come to work as normal. </p><p>Many others, however, are <b>furloughing</b> their staff - this means they are not required to show up to work. </p><p>According to our partner, CBS News, the five government departments furloughing the most staff are:</p><ul><li spacing="rich-text">
      <b>Department of Defense (civilian staff)</b>: 334,904 furloughed, with 406,573 retained
    </li><li spacing="rich-text">
      <b>Department of Health: </b>32,460 furloughed, with 47,257 retained
    </li><li spacing="rich-text">
      <b>Department of Commerce:</b> 34,711 furloughed, with 8,273 retained
    </li><li spacing="rich-text">
      <b>Department of State:</b> 16,651 furloughed, with 10,344 retained
    </li><li spacing="rich-text">
      <b>Nasa: </b>15,094 furloughed, with 3,124 retained
    </li></ul>
    </article></div></li><li><div><article data-testid="content-post" id="asset:7a1e9d35-5e8b-460e-811a-b9bbdceaad23"><header><span><h3 type="normal"><span role="text"><span>Travelling to the US this week? Here's what you need to know</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 08:28 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="A man walks along the side of a building with a sign reading &quot;Smithsonian&quot;" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/c9d55870-f926-4527-b512-62a75aa8d32e.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/c9d55870-f926-4527-b512-62a75aa8d32e.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/c9d55870-f926-4527-b512-62a75aa8d32e.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/c9d55870-f926-4527-b512-62a75aa8d32e.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/c9d55870-f926-4527-b512-62a75aa8d32e.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/c9d55870-f926-4527-b512-62a75aa8d32e.jpg.webp 800w" width="4806" height="2703"></span><span role="text"><span>Image source, </span>EPA</span></p><figcaption><span>Image caption, </span><p>The Smithsonian Institution has said it has enough funds to remain open for a week</p></figcaption></figure><p>Federal services will be affected in different ways by the shutdown. So if you're planning to visit the US soon, you should plan ahead as it's very possible you'll encounter some disruptions.</p><p><b>Your flight plans may be affected</b></p><p>Air traffic control and Transportation Safety Administration (TSA) workers are considered "essential" - so will continue to work. However, as with other essential workers, they won't be paid until the shutdown resolves.</p><p>During the last shutdown, these workers increasingly began to call in sick, leading to airport delays.</p><p>The US Department of Transportation's shutdown plans outline that hiring and training of air traffic controllers will continue.</p><p><b>Public transport within the US should continue as normal</b></p><p>The Rail Passengers Association said last week that Amtrak, and transit systems, should not be affected in the short term. </p><p>Most other forms of public transit in the US - like city buses, subways, light rails, trams, and ferries - are not federally funded and so will run as normal.</p><p><b>Attractions and landmarks will be affected differently</b></p><p>An Interior Department contingency plan has outlined that national parks will remain partially open. </p><p>In Washington DC and New York, the Smithsonian Institution which owns many museums as well as the National Zoo has <a href="https://www.bbc.co.uk/news/live/clylje0rmp2t?post=asset%3A46077ee4-e49a-4f80-ac1f-7bab62937a27#post">said it has enough funds to remain open for a week</a>. </p><p>According to the American Alliance of Museums - the Smithsonian's museums lost an estimated one million visitors during the last government shutdown in late 2018.</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:aac584f5-7b6a-4706-8fe3-38865b70918a"><header><span><h3 type="normal"><span role="text"><span>Trump threatens mass layoffs as government shutdown starts</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 08:07 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="Donald Trump is seen in closeup" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/0640eb30-663d-408e-ae50-8837bcecab39.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/0640eb30-663d-408e-ae50-8837bcecab39.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/0640eb30-663d-408e-ae50-8837bcecab39.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/0640eb30-663d-408e-ae50-8837bcecab39.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/0640eb30-663d-408e-ae50-8837bcecab39.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/0640eb30-663d-408e-ae50-8837bcecab39.jpg.webp 800w" width="1000" height="563"></span><span role="text"><span>Image source, </span>Reuters</span></p></figure><p>Ahead of the government shutdown, President Trump suggested that "vast numbers of people" could be laid off - something that he said would be "irreversible".</p><p>Threats like that, says Eric Ham, a political analyst and former congressional staffer, could initially be viewed as a "tool" to try to get the Democrats to back down from their ongoing demands.</p><p>But now the government has shut down, more layoffs are "certainly something that could be on the table", he tells BBC News Channel. </p><ul><li spacing="rich-text">
      <b><i>For context:</i></b><i> The US president is the head of the executive branch of government, and was not involved in the Senate vote leading to the shutdown. </i>
    </li></ul><p>Trump, Ham says, could see this as an opportunity, as the US president has previously "made it clear" he wants to streamline the government.</p><p>Since taking office, Trump has fired thousands of federal workers through his cost-cutting initiative with the Department of Government Efficiency (Doge).</p><p>And then in a memo circulated by the White House last Thursday, it also <a href="https://www.bbc.co.uk/news/articles/c5y4de02x3wo">warned agencies to prepare for mass firings</a> in the event of a shutdown.</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:384f0717-b51c-4b47-a064-53159b52109e"><header><span><h3 type="normal"><span role="text"><span>From missed wages, to potential layoffs - here's how the shutdown can affect people</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 07:42 BST</span></span></span></span></h3></span></header><p><span><strong>Emer Moreau</strong><br>BBC News</span></p>
      <p>Wayne Winegarden has been speaking to BBC Radio 4’s Today programme, and he says the first place you're going to see an impact is on workers.</p><p>“If you work for the federal government, you’re not going to go to work, you're not going to get paid,” the senior fellow in business and economics at the Pacific Research Institute says.</p><p>“If you’re a contractor, again, you’re not going to get your payments.”</p><p>The longer the shutdown goes on, the more people will feel it - when the cash for social security payments runs out, families receiving those payments will take a hit.</p><p>If it goes on longer again, services like the US national parks could see their funding streams dry up.</p><p>Winegarden also notes that it's difficult to know if President Trump will use this shutdown as an excuse to accelerate his plans to make large cuts to the number of federal workers.</p><p>"We’ve seen that in past cuts where they’ve fired people and then realised ‘whoops, we need those people', and then hire them back."</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:f636dfa2-0f3f-42eb-a2f4-e9a32ea72bf5"><header><span><h3 type="normal"><span role="text"><span>Jobs, travel, national parks - what remains open, and what is now closed</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 07:33 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="A sign in front of the National Park Service reads: Because of the Federal Government SHUTDOWN, All National Parks Are CLOSED" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/76f35197-6075-491c-91d8-d290faaff71e.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/76f35197-6075-491c-91d8-d290faaff71e.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/76f35197-6075-491c-91d8-d290faaff71e.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/76f35197-6075-491c-91d8-d290faaff71e.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/76f35197-6075-491c-91d8-d290faaff71e.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/76f35197-6075-491c-91d8-d290faaff71e.jpg.webp 800w" width="974" height="548"></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>During the last shutdown in late 2018, the Trump administration made the decision to leave parks open, with few to no federal workers there to staff them</p></figcaption></figure><p>A memo released by the White House yesterday warned agencies that would be affected by the shutdown to "execute their plans for an orderly shutdown".</p><p>It explained that the duration of a shutdown is difficult to predict. Many agencies will be affected by the situation - but some services will continue.</p><p>Previous shutdowns have resulted in federal lands, including <b>National Parks and National Forests</b>, being closed to the public. This time around, however, national parks will remain partially open, according to an <a href="https://www.doi.gov/sites/default/files/documents/2025-09/doi-nps-lapse-plan2025930508.pdf">Interior Department contingency plan posted Tuesday evening.<span>, <!-- -->external</span></a></p><p>The Coalition to Protect America's National Parks penned a letter last week pushing for parks to close in the event of a shutdown for fear of damage to the landscapes and visitor safety.</p><p><b>Medicare and Medicaid,</b> social health programmes for the elderly and poor, will continue, but staffing shortages could lead to some interruptions to services.</p><p>The <b>National Flood Insurance Program</b> will be closed, which will affect property sales.</p><p><b>Food assistance programmes </b>will also be impacted, with the Supplemental Nutrition Program for Women, Infants, and Children (WIC) expected to rapidly run out of funds.</p><p><b>Border protection</b>, <b>in-hospital medical care</b>, <b>law enforcement </b>and <b>air-traffic control </b>should all continue.</p><p>A federal shutdown is likely to impact flyers, as there could potentially be long security queues, and delays caused by unpaid air traffic controllers choosing to stay home rather than work for free.</p><p>We've got <a href="https://www.bbc.co.uk/news/articles/cgj1p485p0no">more on this in our explainer</a>.</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:cacac2a3-9891-47f2-8e13-fe74c7248dff"><header><span><h3 type="normal"><span role="text"><span>Some law enforcement officers will continue working - but without pay</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 07:11 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="A woman with long brunette hair" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/43f2c9b6-829e-43bb-897b-3b225f223dba.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/43f2c9b6-829e-43bb-897b-3b225f223dba.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/43f2c9b6-829e-43bb-897b-3b225f223dba.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/43f2c9b6-829e-43bb-897b-3b225f223dba.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/43f2c9b6-829e-43bb-897b-3b225f223dba.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/43f2c9b6-829e-43bb-897b-3b225f223dba.jpg.webp 800w" width="997" height="561"></span><span role="text"><span>Image source, </span>Reuters</span></p></figure><p>Law enforcement officers will continue to work through the government shutdown, says Kristi Noem, who heads up the Department of Homeland Security (DHS). </p><p>"More than 200,000 of these patriots will go without pay," she writes on X.</p><p>Noem goes on to blame the Democrats for the shutdown, saying it's "forcing over 150,000 officers and nearly 50,000 members of the military - our frontline of defense - to continue protecting our nation without pay". </p><p>Her statement follows reporting by CBS News, the BBC's US partner, which found the DHS expected about 258,000 Homeland Security workers would be exempt from furlough in the event of a government shutdown. </p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:c912b3a0-5b46-4fd6-a7d7-028abc122bdb"><header><span><h3 type="normal"><span role="text"><span>How did we get here?</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 06:46 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="An empty Senate hallway with ornate flooring" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/7765dcaa-6d12-43dd-bc62-2011b76f127e.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/7765dcaa-6d12-43dd-bc62-2011b76f127e.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/7765dcaa-6d12-43dd-bc62-2011b76f127e.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/7765dcaa-6d12-43dd-bc62-2011b76f127e.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/7765dcaa-6d12-43dd-bc62-2011b76f127e.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/7765dcaa-6d12-43dd-bc62-2011b76f127e.jpg.webp 800w" width="5000" height="2814"></span><span role="text"><span>Image source, </span>Reuters</span></p><figcaption><span>Image caption, </span><p>The Senate emptied out overnight as lawmakers failed to find an agreement</p></figcaption></figure><p>As we've been reporting, the US government shut down almost two hours ago.</p><p>The Republican-controlled Senate failed to pass a last-minute government spending bill yesterday, which could have averted the shutdown.</p><p>The Republicans were pushing to pass a bill to extend government funding without other initiatives attached - known as a clean CR - or continuing resolution.</p><p>But they only have <b>53 seats</b> in the Senate - and need <b>60 votes</b> to pass such a bill.</p><p>This meant they needed the Democrats support to pass the bill - and the Democrats knew that.</p><p>The Democrats sought to capitalise on that leverage to try and advance their policy goals in health care, which included: </p><ul><li spacing="rich-text">
      Ensuring subsidies for health insurance for low-income individuals do not expire 
    </li><li spacing="rich-text">
      Reversing the Trump administration's cuts to Medicaid
    </li></ul><p>So they came to a standoff - with each party subsequently blaming the other for the shutdown - and one which won't be resolved until Congress passes a funding bill.</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:f4faada1-b971-4936-a262-d0d808ddb544"><header><span><h3 type="normal"><span role="text"><span>White House starts tracking length of government shutdown</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 06:25 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="A black screen shows a timer that reads 00:00:00 in orange" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/463464f8-773c-4a88-a7d7-d791ebf79a8a.png.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/463464f8-773c-4a88-a7d7-d791ebf79a8a.png.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/463464f8-773c-4a88-a7d7-d791ebf79a8a.png.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/463464f8-773c-4a88-a7d7-d791ebf79a8a.png.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/463464f8-773c-4a88-a7d7-d791ebf79a8a.png.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/463464f8-773c-4a88-a7d7-d791ebf79a8a.png.webp 800w" width="1252" height="646"></span><span role="text"><span>Image source, </span>White House</span></p></figure><p>A bit earlier, <a href="https://www.bbc.co.uk/news/live/clylje0rmp2t?post=asset%3Abdd03316-22d4-4050-aae6-133d03ef694c#post">we brought you the details</a> about how the White House had launched a shutdown countdown clock on its website.</p><p>That clock has now been swapped out for a timer that is now tracking how long the government remains shutdown.</p><p>The website now reads "Democrats have shutdown the government".</p><p><b><i>For context:</i></b><i> Both </i><a href="https://www.bbc.com/news/live/clylje0rmp2t?post=asset%3Aafcd4d58-a036-48f9-a361-b2c82a1e2508#post">Democrats and Republicans are laying blame on each other</a><i> when it comes to the shutdown. While Republicans control both chambers of Congress, they fall short of the 60 votes needed in the Senate to pass a spending bill.</i></p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:3b25b328-e9ea-41ed-83e7-fdad872cd031"><header><span><h3 type="normal"><span role="text"><span>Republicans and Democrats point fingers as shutdown comes into effect</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 06:04 BST</span></span></span></span></h3></span></header>
      <p>As the US federal government shutdown started an hour ago, members of both parties are laying the blame on one another. </p><p>From the Democrats: </p><ul><li spacing="rich-text">
      <b>Congressman Bill Foster</b> issues a statement saying Republicans bear responsibility for the shutdown, given they control the House, the Senate and the White House. Families across the country will "continue to pay the price" unless the Republicans come to the table, he adds.
    </li><li spacing="rich-text">
      <b>Representative Joe Morelle</b> says he's "deeply frustrated", and the shutdown was a direct result of "a brutal and incompetent administration" 
    </li><li spacing="rich-text">
      <b>California Governor  Gavin Newsom</b> shares on X a photo of Trump in his office, calling the president  a "very weak man who can't even do stairs"
    </li></ul><figure><p><span><img alt="Gavin Newsom" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/fb151098-f90f-4b7d-9fa1-dff519aff6f9.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/fb151098-f90f-4b7d-9fa1-dff519aff6f9.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/fb151098-f90f-4b7d-9fa1-dff519aff6f9.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/fb151098-f90f-4b7d-9fa1-dff519aff6f9.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/fb151098-f90f-4b7d-9fa1-dff519aff6f9.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/fb151098-f90f-4b7d-9fa1-dff519aff6f9.jpg.webp 800w" width="2024" height="1139"></span><span role="text"><span>Image source, </span>Reuters</span></p><figcaption><span>Image caption, </span><p>"Trump and his lapdog Republicans just shut down the federal government after refusing to protect Americans from an imminent spike in health insurance costs," Newsom adds.</p></figcaption></figure><p>And on the Republicans' side:</p><ul><li spacing="rich-text">
      The <b>Republican Conference</b> in the US House of Representatives shares on X a video of lights in the Capitol Building turning out, saying the Democrats are "putting illegal aliens first and hurting hardworking Americans in the process"
    </li></ul><ul><li spacing="rich-text">
      <b>Representative Dusty Johnson</b> says "shutdowns are stupid", adding that Democrats are "putting American workers' paychecks at risk"
    </li><li spacing="rich-text">
      Fellow <b>Representative Chuck Fleischmann</b> says that Democrats have "officially shut down our government", and "hardworking taxpayers will have to pay the bill"
    </li></ul><figure><p><span><img alt="Rep. Chuck Fleischmann" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/682c016a-fbb3-4dd3-a741-21ed4d163d4f.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/682c016a-fbb3-4dd3-a741-21ed4d163d4f.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/682c016a-fbb3-4dd3-a741-21ed4d163d4f.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/682c016a-fbb3-4dd3-a741-21ed4d163d4f.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/682c016a-fbb3-4dd3-a741-21ed4d163d4f.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/682c016a-fbb3-4dd3-a741-21ed4d163d4f.jpg.webp 800w" width="892" height="502"></span><span role="text"><span>Image source, </span>CQ-Roll Call, Inc via Getty Images</span></p><figcaption><span>Image caption, </span><p>Rep. Chuck Fleischmann</p></figcaption></figure><p>This is in line with the trend we <a href="https://www.bbc.com/news/live/clylje0rmp2t?post=asset%3Aafcd4d58-a036-48f9-a361-b2c82a1e2508#post">saw in the lead up</a> to the midnight deadline.</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:a1d8f109-4372-4baf-a906-4ea40a58e660"><header><span><h3 type="normal"><span role="text"><span>How investors around the world view the shutdown</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 05:45 BST</span></span></span></span></h3></span></header><p><span><span><img alt="" loading="lazy" src="https://ichef.bbci.co.uk/ace/standard/128/cpsprodpb//vivo/live/images/2025/6/13/35ebb458-a432-413f-b20e-4555510f2113.jpg.webp" width="64" height="64"></span></span><span><strong>Peter Hoskins</strong><br>Business reporter, Singapore</span></p>
      <p>As I reported earlier in the morning, global financial markets seem to be broadly taking the shutdown in their stride.</p><p>Investors appear to be mostly looking past the widely expected stoppage, as they see it as a temporary blip.</p><p>Here in the Asia-Pacific region, stock markets are mixed as investors focus on issues closer to home rather than the politics of Washington DC.</p><p>Japan's Nikkei 225 is about 1% lower after a business sentiment report disappointed, while the Nifty 50 in India is higher as the country's central bank kept interest rates on hold.</p><p>Back in the US, the main stock indexes closed higher on Tuesday, with the Dow Jones Industrial Average hitting a record high.</p><p>But there are some signs that that jitters may be creeping into Wall Street.</p><p>US stock futures are pointing to a lower open on Wednesday. Futures are contracts to buy or sell an underlying asset at a future date and are an indication of how markets will trade when they open.</p><p>Gold - which is seen as a safe haven for investments during times of uncertainty - has hit a new record high of more than $3,872 an ounce.</p><p>At the same time, the US dollar is hovering near a one-week low against other major currencies.</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:70339e26-1d76-432b-a419-24f736588b48"><header><span><h3 type="normal"><span role="text"><span>Federal furlough payments could cost $400m a day - official estimates</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 05:30 BST</span></span></span></span></h3></span></header>
      <p>A letter issued by the director of the Congressional Budget Office prior to the shutdown has given more details over the potential financial implications.</p><p>In the letter from Phillip Swagel, the office estimates 750,000 <b>federal employees</b> could be furloughed each day, with a total daily cost of around $400m (£297m).</p><p>It adds that this projected number of furloughed employees could vary daily, as some agencies might furlough more employees the longer the shutdown persists, while others might recall some initially furloughed
employees.</p><p><b>Members of Congress</b> will still be paid as their pay is required by the American constitution, the letter adds. </p><p>As for the <b>military</b>, they are required to work during a shutdown but won't be paid until after. </p><p>"The effects of a government shutdown on <b>business activity</b> are uncertain, and
their magnitude would depend on the duration of a shutdown and on decisions
made by the Administration," the letter adds. </p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:b7363eca-2950-4f61-b37d-ba0534860ceb"><header><span><h3 type="normal"><span role="text"><span>How long have previous shutdowns lasted?</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 05:07 BST</span></span></span></span></h3></span></header>
      <figure><p><span><img alt="A graph that shows US government shutdowns since 1980, marking the duration of each funding gap in days" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/161846a2-33f9-4327-89d0-47d2117f580d.png.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/161846a2-33f9-4327-89d0-47d2117f580d.png.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/161846a2-33f9-4327-89d0-47d2117f580d.png.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/161846a2-33f9-4327-89d0-47d2117f580d.png.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/161846a2-33f9-4327-89d0-47d2117f580d.png.webp 624w, https://ichef.bbci.co.uk/ace/standard/683/cpsprodpb/vivo/live/images/2025/10/1/161846a2-33f9-4327-89d0-47d2117f580d.png.webp 683w" width="683" height="682"></span></p></figure><p>Government shutdowns in the US are becoming more common, with <b>Donald Trump</b>'s first presidential term seeing three such instances. This included the longest shutdown in American history, lasting 35 days.</p><p>Before Trump, <b>Bill Clinton</b> held the previous record, with a 21-day shutdown in 1995 towards the end of his first term as president. Republicans had won control of both the House and the Senate halfway through Clinton's first term, and wanted to pass a budget that, among other things, limited spending for Medicare. </p><p>Similarly,<b> Barack Obama</b> endured a 16-day shutdown in 2013 over the then-president's proposed health care legislation.</p><p><b>Ronald Reagan</b>, a Republican president, oversaw the most shutdowns during his presidency - with eight recorded across his two terms in the 1980s. However, all of them were relatively short - the longest funding gap lasted a mere three days.</p>
    </article></div></li><li><div><article data-testid="content-post" id="asset:2af17150-0384-4372-b50b-d93c26f94ab7"><header><span><h3 type="normal"><span role="text"><span>US federal government shutdown officially begins</span><span><span data-testid="timestamp"><span data-testid="accessible-timestamp">published at 05:00 BST</span><p><span>Breaking</span></p></span></span></span></h3></span></header>
      <figure><p><span><img alt="The Ohio Clock outside the Senate Chamber strikes midnight at the U.S. Capitol" src="https://ichef.bbci.co.uk/ace/standard/640/cpsprodpb/vivo/live/images/2025/10/1/13ce39ed-d1d2-4b77-b4ee-f26110321ef6.jpg.webp" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/vivo/live/images/2025/10/1/13ce39ed-d1d2-4b77-b4ee-f26110321ef6.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/vivo/live/images/2025/10/1/13ce39ed-d1d2-4b77-b4ee-f26110321ef6.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/vivo/live/images/2025/10/1/13ce39ed-d1d2-4b77-b4ee-f26110321ef6.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/vivo/live/images/2025/10/1/13ce39ed-d1d2-4b77-b4ee-f26110321ef6.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/vivo/live/images/2025/10/1/13ce39ed-d1d2-4b77-b4ee-f26110321ef6.jpg.webp 800w" width="1024" height="576"></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>The Ohio Clock outside the Senate Chamber strikes midnight at the US Capitol</p></figcaption></figure><p>It's midnight in Washington DC, and the US federal government shutdown has officially begun.</p><p>This would now likely leave hundreds of thousands of workers on unpaid leave and halt many government programs and services. </p><p>The shutdown comes hours after the Republican-controlled Senate failed to pass a government spending bill. </p><p>This is the first government shutdown since 2018 and will see non-essential workers placed on unpaid leave. </p><p>You can read more about how we got to this point <a href="https://www.bbc.com/news/live/clylje0rmp2t?post=asset%3A5104dfec-d564-4a0f-bc73-2487a6846a00#post">here</a> - and stay with us as we bring you more updates.</p>
    </article></div></li></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An informational website about why I went to prison (168 pts)]]></title>
            <link>https://prison.josh.mn/</link>
            <guid>45434062</guid>
            <pubDate>Wed, 01 Oct 2025 03:44:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://prison.josh.mn/">https://prison.josh.mn/</a>, See on <a href="https://news.ycombinator.com/item?id=45434062">Hacker News</a></p>
Couldn't get https://prison.josh.mn/: Error: getaddrinfo ENOTFOUND prison.josh.mn]]></description>
        </item>
        <item>
            <title><![CDATA[The gaslit asset class (152 pts)]]></title>
            <link>https://blog.dshr.org/2025/09/the-gaslit-asset-class.html</link>
            <guid>45433866</guid>
            <pubDate>Wed, 01 Oct 2025 02:59:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.dshr.org/2025/09/the-gaslit-asset-class.html">https://blog.dshr.org/2025/09/the-gaslit-asset-class.html</a>, See on <a href="https://news.ycombinator.com/item?id=45433866">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-6556465880306785878" itemprop="description articleBody">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZX0fqPMMdG3KevN5anuO0O4Vzb2gPpCV0ipAxDl6iQODtpgJd_HhpF6M4tb9Oojlb1IkT-FtLa9nhxLbY7TcXMNJnYuLCS6yvxYcDiolSXwKHPTZ11bB-hr55RJYBwR9NM3q-7rdr8z_jgxed293EUpeU89cbjBJpPfy9NTbAlJgsOJg-_hxSrqFEs2bV/s2048/Grants.png"><img data-original-height="593" data-original-width="2048" height="58" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgZX0fqPMMdG3KevN5anuO0O4Vzb2gPpCV0ipAxDl6iQODtpgJd_HhpF6M4tb9Oojlb1IkT-FtLa9nhxLbY7TcXMNJnYuLCS6yvxYcDiolSXwKHPTZ11bB-hr55RJYBwR9NM3q-7rdr8z_jgxed293EUpeU89cbjBJpPfy9NTbAlJgsOJg-_hxSrqFEs2bV/w200-h58/Grants.png" width="200"></a>
James Grant invited me to address the annual conference of <a href="https://www.grantspub.com/"><i>Grant's Interest Rate Observer</i></a>. This was an intimidating prospect, the previous year's conference featured billionaires <a href="https://en.wikipedia.org/wiki/Scott_Bessent">Scott Bessent</a> and <a href="https://en.wikipedia.org/wiki/Bill_Ackman">Bill Ackman</a>. As usual, below the fold is the text of my talk, with the slides, links to the sources, and additional material in footnotes. Yellow background indicates textual slides.<br>
<span><a name="more"></a></span></p><h3>The Gaslit Asset Class</h3><p>
Before I explain that much of what you have been told about cryptocurrency technology is gaslighting, I should stress that I hold no long or short positions in cryptocurrencies, their derivatives or related companies. Unlike most people discussing them, I am not "<a href="https://blog.dshr.org/2022/02/talking-their-book.html">talking my book</a>".</p><p>

To fit in the allotted time, this talk focuses mainly on Bitcoin and omits many of the finer points. My text, with links to the sources and additional material in footnotes, will go up on my blog later today.</p><h3>Why Am I Here?</h3><p>
I imagine few of you would understand why a retired software engineer with more than forty years in Silicon Valley  was asked to address you on cryptocurrencies<sup><a href="#Footnote1">[1]</a></sup>.</p><p>
I was an early employee at <a href="https://en.wikipedia.org/wiki/Sun_Microsystems">Sun Microsystems</a> then <a href="https://blog.dshr.org/2025/05/the-dawn-of-nvidias-technology.html">employee #4 at Nvidia</a>, so I have been long Nvidia for more than 30 years. It has been a <a href="https://blog.dshr.org/2024/07/accelerated-computing.html">wild ride</a>. I quit after 3 years as part of fixing Nvidia's first near-death experience and immediately did 3 years as employee #12 at another startup, which also IPO-ed. If you do two in six years in your late 40s you get seriously burnt out.</p><p>

So my wife and I started a program at Stanford that is still running 27 years later. She was a career librarian at the Library of Congress and the Stanford Library. She was part of the team that, 30 years ago, pioneered the transition of academic publishing to the Web. She was also the person who <a href="https://blog.dshr.org/2024/02/the-stanford-digital-library-project.html">explained citation indices to Larry and Sergey</a>, which led to Page Rank.</p><p>

The academic literature has archival value. Multiple libraries hold complete runs on paper of the <a href="https://royalsocietypublishing.org/journal/rstl"><i>Philosophical Transactions of the Royal Society</i></a> starting 360 years ago<sup><a href="#Footnote1">[2]</a></sup>.
The interesting engineering problem we faced was how to enable libraries to deliver comparable longevity to Web-published journals.</p><h3>Five Years Before Satoshi Nakamoto</h3>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAPQ7O6lmB4paIa0yCsHim_SMJlYGWMWTdq1uYsdAJE7jubDbPJBWg-7jOC-mBYLX_G4NUSJKhJy3CW4bGU2rNoY3y2PsrU6_Icb5ELT5-ZfX8jR7jef16hRSYAfrRMJ_DqLZL5lIdfKS2-O0USJoK2lSnhaesAytJk_BesD_Z6ggub-rHsprp2zHuP5SB/s151/LOCKSS.logo.png"><img data-original-height="151" data-original-width="151" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiAPQ7O6lmB4paIa0yCsHim_SMJlYGWMWTdq1uYsdAJE7jubDbPJBWg-7jOC-mBYLX_G4NUSJKhJy3CW4bGU2rNoY3y2PsrU6_Icb5ELT5-ZfX8jR7jef16hRSYAfrRMJ_DqLZL5lIdfKS2-O0USJoK2lSnhaesAytJk_BesD_Z6ggub-rHsprp2zHuP5SB/s16000/LOCKSS.logo.png"></a>
I worked with a group of outstanding Stanford CS Ph.D. students to design and implement <a href="http://dx.doi.org/10.1145/945445.945451">a system for stewardship of Web content</a> modeled on the paper library system. The goal was to make it extremely difficult for even a powerful adversary to delete or modify content without detection. It is called <a href="https://lockss.org/">LOCKSS</a>, for Lots Of Copies Keep Stuff Safe; a decentralized peer-to-peer system secured by Proof-of-Work. We won a <a href="http://dx.doi.org/10.1145/945445.945451">"Best Paper" award</a> for it five years before Satoshi Nakamoto published his <a href="https://bitcoin.org/bitcoin.pdf">decentralized peer-to-peer system</a> secured by Proof-of-Work.  When he did, LOCKSS had been in production for a few years and we had learnt a lot about how difficult decentralization is in the online world.</p><p>

Bitcoin built on more than <a href="https://queue.acm.org/detail.cfm?id=3136559">two decades of research</a>. Neither we nor Nakamoto invented Proof-of-Work, <a href="https://doi.org/10.1007/3-540-48071-4_10">Cynthia Dwork and Moni Naor</a> published it in 1992. Nakamoto didn't invent blockchains, <a href="https://doi.org/10.1007/3-540-38424-3_32">Stuart Haber and W. Scott Stornetta</a> patented them in 1991. He was extremely clever in assembling well-known techniques into a cryptocurrency, but his only major innovation was the <a href="https://doi.org/10.1007/978-3-642-27739-9_1804-1">Longest Chain Rule</a>.</p><h3>Digital cash</h3><p>
The fundamental problem of representing cash in digital form is that a digital coin can be endlessly copied, thus you need some means to prevent each of the copies being spent. When you withdraw cash from an ATM, turning digital cash in your account into physical cash in your hand, the bank performs an atomic transaction against the database mapping account numbers to balances. The bank is trusted to prevent multiple spending.</p><p>

There had been several attempts at a cryptocurrency before Bitcoin.  The primary goals of the <a href="https://search.worldcat.org/title/1099341012">libertarians and cypherpunks</a> were that a cryptocurrency be as anonymous as physical cash, and that it not have a central point of failure that had to be trusted.  The only one to get any traction was David Chaum's <a href="https://en.wikipedia.org/wiki/DigiCash">DigiCash</a>; it was anonymous but it was centralized to prevent multiple spending and it involved banks.</p><h3>Nakamoto's <i>magnum opus</i></h3>
<div><p>
Bitcoin claims:
</p><ul>
<li>The system was <i>trustless</i> because it was <i>decentralized</i>.</li>
<li>It was a <i>medium of exchange</i> for buying and selling in the real world.</li>
<li>Transactions were <i>faster</i> and <i>cheaper</i> than in the existing financial system.</li>
<li>It was secured by <i>Proof-of-Work</i> and <i>cryptography</i>.</li>
<li>It was <i>privacy-preserving</i>.</li>
</ul>
</div><p>
When in November 2008 Nakamoto published <a href="https://bitcoin.org/bitcoin.pdf"><i>Bitcoin: A Peer-to-Peer Electronic Cash System</i></a> it was the peak of the <a href="https://en.wikipedia.org/wiki/2008_financial_crisis">Global Financial Crisis</a> and people were very aware that the financial system was broken (and it still is). Because it solved many of the problems that had dogged <a href="https://davidgerard.co.uk/blockchain/book/">earlier attempts at electronic cash</a>, it rapidly attracted a clique of enthusiasts. When Nakamoto went silent in 2010 they took over proseltyzing the system. The main claims they made were:</p><ul>
<li>The system was <i>trustless</i> because it was <i>decentralized</i>.</li>
<li>It was a <i>medium of exchange</i> for buying and selling in the real world.</li>
<li>Transactions were <i>faster</i> and <i>cheaper</i> than in the existing financial system.</li>
<li>It was secured by <i>Proof-of-Work</i> and <i>cryptography</i>.</li>
<li>It was <i>privacy-preserving</i>.</li>
</ul><p>
They are all either false or misleading. In most cases Nakamoto's own writings show he knew this. His acolytes were gaslighting.</p><h3>Trustless because decentralized (1)</h3><p>
Assuming that the Bitcoin network consists of a large number of roughly equal nodes, it randomly selects a node to determine the transactions that will form the next block. There is no need to trust any particular node because the chance that they will be selected is small.<sup><a href="#Footnote3">[3]</a></sup></p><div><p>
At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware. A server farm would only need to have one node on the network and the rest of the LAN connects with that one node.
</p><blockquote>
<small>
Satoshi Nakamoto 2<sup>nd</sup> November 2008
</small>
</blockquote><p>
The current system where every user is a network node is not the intended configuration for large scale. ... The design supports letting users just be users. The more burden it is to run a node, the fewer nodes there will be. Those few nodes will be big server farms. The rest will be client nodes that only do transactions and don’t generate.
</p><blockquote>
<small>
Satoshi Nakamoto: 29<sup>th</sup> July 2010
</small>
</blockquote>
</div><p>
But only three days after publishing his white paper, Nakamoto understood that this assumption would become false:</p><blockquote>
At first, most users would run network nodes, but as the network grows beyond a certain point, it would be left more and more to specialists with server farms of specialized hardware.
</blockquote><p>
He didn't change his mind. On 29<sup>th</sup> July 2010, less than five months before he went silent, he made the same point:</p><blockquote>
The current system where every user is a network node is not the intended configuration for large scale. ... The design supports letting users just be users. The more burden it is to run a node, the fewer nodes there will be. Those few nodes will be big server farms.
</blockquote><p>
"Letting users be users" necessarily means that the "users" have to trust the "few nodes" to include their transactions in blocks.  The very strong economies of scale of technology in general and "big server farms" in particular meant that the centralizing force described in W. Brian Arthur's 1994 book <a href="http://www.amazon.com/Increasing-Returns-Dependence-Economics-Cognition/dp/0472064967"><i>Increasing Returns and Path Dependence in the Economy</i></a> resulted in there being "fewer nodes". Indeed, on 13<sup>th</sup> June 2014 a single node controlled 51% of Bitcoin's mining, the <a href="https://hackingdistributed.com/p/2014/06/13/in-ghash-bitcoin-trusts/">GHash pool</a>.<sup><a href="#Footnote4">[4]</a></sup></p><h3>Trustless because decentralized (2)</h3><p>
In June 2022 <a href="https://aidenlab.org/bitcoin.pdf"><i>Cooperation among an anonymous group protected Bitcoin during failures of decentralization</i></a> by Alyssa Blackburn <i>et al</i> showed that it had not been decentralized from the very start. The same month a DARPA-sponsored report entitled <a href="https://assets-global.website-files.com/5fd11235b3950c2c1a3b6df4/62af6c641a672b3329b9a480_Unintended_Centralities_in_Distributed_Ledgers.pdf"><i>Are Blockchains Decentralized?</i></a> by a large team from the <a href="https://www.trailofbits.com/">Trail of Bits</a> security company examined the economic and many other centralizing forces affecting a wide range of blockchain implementations and concluded that the answer to their question is "No".<sup><a href="#Footnote5">[5]</a></sup></p><p>

The same centralizing economic forces apply to Proof-of-Stake blockchains such as Ethereum. Grant's <i>Memo to the bitcoiners</i> explained the process last February.</p><h3>Trustless because decentralized (3)</h3><p>
Another centralizing force drives pools like GHash. The network creates a new block and rewards the selected node about every ten minutes.  Assuming they're all state-of-the-art, there are currently about 15M rigs mining Bitcoin<sup><a href="#Footnote6">[6]</a></sup>. Their <a href="https://doi.org/10.1016/j.resconrec.2021.105901">economic life is around 18 months</a>, so only 0.5%% of them will ever earn a reward.  The owners of mining rigs pool their efforts, converting a small chance of a huge reward into a steady flow of smaller rewards. On average GHash was getting three rewards an hour.</p><h3>A medium of exchange (1)</h3>
<div><p>
Quote from: Insti, July 17, 2010, 02:33:41 AM</p><blockquote>
How would a Bitcoin snack machine work?<br>
<ol>
<li>You want to walk up to the machine. Send it a bitcoin.</li>
<li>?</li>
<li>Walk away eating your nice sugary snack. (Profit!)</li>
</ol>
You don’t want to have to wait an hour for you transaction to be confirmed.<p>

The vending machine company doesn’t want to give away lots of free candy.</p><p>

How does step 2 work?
</p></blockquote><p>
I believe it’ll be possible for a payment processing company to provide as a service the rapid distribution of transactions with good-enough checking in something like 10 seconds or less.
</p><blockquote>
<small>
Satoshi Nakamoto: 17<sup>th</sup> July 2010
</small>
</blockquote>
</div><p>
Bitcoin's ten-minute block time is a problem for real-world buying and selling<sup><a href="#Footnote7">[7]</a></sup>, but the problem is even worse. Network delays mean a transaction isn't final when you see it in a block. Assuming no-one controlled more than 10% of the hashing power, Nakamoto required another 5 blocks to have been added to the chain, so 99.9% finality would take an hour.  With a more realistic 30%, the rule should have been 23 blocks, with finality taking 4 hours<sup><a href="#Footnote8">[8]</a></sup>.</p><p>

Nakamoto's 17<sup>th</sup> July 2010 exchange with Insti shows he understood that the Bitcoin network couldn't be used for ATMs, vending machines, buying drugs or other face-to-face transactions because he went on to describe how a payment processing service layered on top of it would work.</p><h3>A medium of exchange (2)</h3>
<div><p>
assuming that the two sides are rational actors and the smart contract language is Turing-complete, there is no escrow smart contract that can facilitate this exchange without either relying on third parties or enabling at least one side to extort the other.</p><p>

two-party escrow smart contracts are ...  simply a game of who gets to declare their choice ﬁrst and commit it on the blockchain sooner, hence forcing the other party to concur with their choice. The order of transactions on a blockchain is essentially decided by the miners. Thus, the party with better connectivity to the miners or who is willing to pay higher transaction fees, would be able to declare their choice to the smart contract ﬁrst and extort the other party.
</p><blockquote>
<small>
Amir Kafshdar Goharshady, <a href="https://arxiv.org/abs/2110.09857"><i>Irrationality, Extortion, or Trusted Third-parties: Why it is Impossible to Buy and Sell Physical Goods Securely on the Blockchain</i></a>
</small>
</blockquote>
</div><p>
The situation is even worse when it comes to buying and selling real-world objects via programmable blockchains such as Ethereum<sup><a href="#Footnote9">[9]</a></sup>. In 2021 <a href="https://arxiv.org/abs/2110.09857">Amir Kafshdar Goharshady showed that</a><sup><a href="#Footnote10">[10]</a></sup>:</p><blockquote>
assuming that the two sides are rational actors and the smart contract language is Turing-complete, there is no escrow smart contract that can facilitate this exchange without either relying on third parties or enabling at least one side to extort the other.<br>
</blockquote><p>
Goharshady <a href="https://arxiv.org/abs/2110.09857">noted that</a>:</p><blockquote>
on the Ethereum blockchain escrows with trusted third-parties are used more often than two-party escrows, presumably because they allow dispute resolution by a human.
</blockquote><p>
And goes on to show that in practice trusted third-party escrow services are essential because two-party escrow smart contracts are:</p><blockquote>
simply a game of who gets to declare their choice ﬁrst and commit it on the blockchain sooner, hence forcing the other party to concur with their choice. The order of transactions on a blockchain is essentially decided by the miners. Thus, the party with better connectivity to the miners or who is willing to pay higher transaction fees, would be able to declare their choice to the smart contract ﬁrst and extort the other party.
</blockquote><p>
The choice being whether or not the good had been delivered. Given the current enthusiasm for <i>tokenization</i> of physical goods the market for trusted escrow services looks bright.</p><h3>Fast transactions</h3><p>
Actually the delay between submitting a transaction and finality is unpredictable and can be much longer than an hour. Transactions are validated by miners then added to the <a href="https://wiki.bitcoinsv.io/index.php/Transaction_Pools">mempool</a> of pending transactions where they wait until either:</p><ul>
<li>The selected network node chooses it as one of the most profitable to include in its block.</li>
<li>It reaches either its specified timeout or the default of 2 weeks.</li>
</ul>
<p>
This year the demand for transactions has been low, typically under 4 per second, so the backlog has been low, around 40K or under three hours. Last October it peaked at around 14 hours worth.</p><p>

The distribution of transaction wait times is highly skewed.  The median wait is typically around a block time. The proportion of low-fee transactions means the average wait is normally around 10 times that. But when everyone wants to transact the <a href="https://blog.dshr.org/2025/03/bitcoins-fee-spikes.html">ratio spikes</a> to over 40 times.</p><h3>Cheap transactions</h3>
<p>
There are two ways miners can profit from including a transaction in a block:</p><ul>
<li>The fee to be paid to the miner which the user chose to include in the transaction. In effect, transaction slots are auctioned off.</li>
<li>The transactions the miner included in the block to front- and back-run the user's transaction, called <a href="https://ethereum.org/en/developers/docs/mev/">Maximal Extractable Value</a><sup><a href="#Footnote11">[11]</a></sup>:<br>
<blockquote>
Maximal extractable value (MEV) refers to the maximum value that can be extracted from block production in excess of the standard block reward and gas fees by including, excluding, and changing the order of transactions in a block.
</blockquote></li>
</ul><p>
The block size limit means there is a fixed supply of transaction slots, about 7 per second, but the demand for them varies, and thus so does the price.  In normal times the auction for transaction fees means they are much smaller than the block reward. But when everyone wants to transact they suffer <a href="https://blog.dshr.org/2025/03/bitcoins-fee-spikes.html">massive spikes</a>.</p><h3>Secured by Proof-of-Work (1)</h3><p>
In cryptocurrencies "secured" means that the cost of an attack exceeds the potential loot.  The security provided by Proof-of-Work is linear in its cost, unlike techniques such as encryption, whose security is exponential in cost.  It is generally believed that it is impractical to reverse a Bitcoin transaction after about an hour because the miners are wasting such immense sums on Proof-of-Work.  Bitcoin pays these immense sums, but it doesn't get the decentralization they ostensibly pay for.</p><div><p>
Monero, a privacy-focused blockchain network, has been undergoing an attempted 51% attack — an existential threat to any blockchain. In the case of a successful 51% attack, where a single entity becomes responsible for 51% or more of a blockchain's mining power, the controlling entity could reorganize blocks, attempt to double-spend, or censor transactions.</p><p>

A company called Qubic has been waging the 51% attack by offering economic rewards for miners who join the Qubic mining pool. They claim to be "stress testing" Monero, though many in the Monero community have condemned Qubic for what they see as a malicious attack on the network or a marketing stunt.
</p><blockquote>
<small>
Molly White: <a href="https://www.web3isgoinggreat.com/?id=monero-51-attack"><i>Monero faces 51% attack</i></a>
</small>
</blockquote>
</div><p>
The advent of "mining as a service" about 7 years ago made 51% attacks against smaller Proof-of-Work alt-coin such as <a href="https://qz.com/1287701/bitcoin-golds-51-attack-is-every-cryptocurrencys-nightmare-scenario/">Bitcoin Gold</a> endemic. In August <a href="https://www.niemanlab.org/2025/08/independent-journalist-molly-white-knows-how-to-follow-the-memecoin/">Molly White</a> reported that <a href="https://www.web3isgoinggreat.com/?id=monero-51-attack"><i>Monero faces 51% attack</i></a>:</p><p>

In 2018's <a href="http://www.nber.org/papers/w24717"><i>The Economic Limits Of Bitcoin And The Blockchain</i></a> Eric Budish of the Booth School analyzed two versions of the 51% attack. I summarized his analysis of the classic multiple spend attack <a href="https://blog.dshr.org/2018/06/cryptocurrencies-have-limits.html">thus</a>:</p><blockquote>
Note that only Bitcoin and Ethereum among cryptocurrencies with "market cap" over $100M would cost more than $100K to attack. The total "market cap" of these 8 currencies is $271.71B and the total cost to 51% attack them is $1.277M or 4.7E-6 of their market cap.
</blockquote><p>
His key insight was that to ensure that 51% attacks were uneconomic, the reward for a block, implicitly the transaction tax, plus the fees had to be greater than the maximum value of the transactions in it. The total transaction cost (reward + fee) typically peaks around 1.8% but is normally between 0.6% and 0.8%, or around 150 times less than Budish's safety criterion.  The result is that a conspiracy between a few large pools could find it economic to mount a 51% attack.</p><h3>Secured by Proof-of-Work (2)</h3>
<div><p>
However, ∆<sub>attack</sub> is something of a “pick your poison” parameter. If ∆<sub>attack</sub> is small, then the system is vulnerable to the double-spending attack ... and the implicit transactions tax on economic activity using the blockchain has to be high. If ∆<sub>attack</sub> is large, then a short time period of access to a large amount of computing power can sabotage the blockchain.
</p><blockquote>
<small>
Eric Budish: <a href="http://www.nber.org/papers/w24717"><i>The Economic Limits Of Bitcoin And The Blockchain</i></a>
</small>
</blockquote>
</div><p>
But everyone assumes the pools won't do that. Budish further analyzed the effects of a multiple spend attack. It would be public, so it would in effect be sabotage, decreasing the Bitcoin price by a factor ∆<sub>attack</sub>. He <a href="http://www.nber.org/papers/w24717">concludes</a> that if the decrease is small, then double-spending attacks are feasible and the per-block reward plus fee must be large, whereas if it is large then access to the hash power of a few large pools can quickly sabotage the currency.</p><p>

The implication is that miners, motivated to keep fees manageable, believe ∆<sub>attack</sub> is large. Thus Bitcoin is secure because those who could kill the golden goose don't want to.</p><h3>Secured by Proof-of-Work (3)</h3>
<p>
The following year, in <a href="https://www.bis.org/publ/work765.pdf"><i>Beyond the doomsday economics of “proof-of-work” in cryptocurrencies</i></a>, Raphael Auer of the Bank for International Settlements showed that the problem Budish identified was inevitable<sup><a href="#Footnote12">[12]</a></sup>:</p><blockquote>
proof-of-work can only achieve payment security if mining income is high, but the transaction market cannot generate an adequate level of income. ... the economic design of the transaction market fails to generate high enough fees.
</blockquote><p>
In other words, the security of Bitcoin's blockchain depends upon inflating the currency with block rewards.  This problem is excerbated by Bitcoin's regular "halvenings" reducing the block reward. To maintain miner's current income after the next halvening in less than three years the "price" would need to be over $200K; security depends upon the "price" appreciating faster than 20%/year.</p><p>

Once the block reward gets small, safety requires the fees in a block to be worth more than the value of the transactions in it.  But everybody has decided to ignore Budish and Auer.</p><h3>Secured by Proof-of-Work (4)</h3>
<p>
In 2024 Soroush Farokhnia &amp; Amir Kafshdar Goharshady's <a href="https://hal.science/hal-04616643/"><i>Options and Futures Imperil Bitcoin's Security</i></a>:</p><blockquote>
showed that (i) a successful block-reverting attack does not necessarily require ... a majority of the hash power; (ii) obtaining a majority of the hash power ... costs roughly 6.77 billion ...  and (iii) Bitcoin derivatives, i.e. options and futures, imperil Bitcoin’s security by creating an incentive for a block-reverting/majority attack.
</blockquote><p>
They assume that an attacker would purchase enough state-of-the-art hardware for the attack. Given Bitmain's dominance in mining ASICs, such a purchase is unlikely to be feasible.</p><h3>Secured by Proof-of-Work (5)</h3>
<p>
But it would not be necessary. Mining is a very competitive business, and power is the major cost<sup><a href="#Footnote13">[13]</a></sup>. Making a profit requires both cheap power and early access to the latest, most efficient chips. So it wasn't a surprise that Ferreira <i>et al</i>'s <a href="https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=3320437"><i>Corporate capture of blockchain governance</i></a> showed that:</p><blockquote>
As of March 2021, the pools in Table 1 collectively accounted for 86% of the total hash rate employed. All but one pool (Binance) have known links to Bitmain Technologies, the largest mining ASIC producer.
<sup><a href="#Footnote14">[14]</a></sup>
</blockquote>
<h3>Secured by Proof-of-Work (6)</h3>
<p>
Bitmain, a Chinese company, exerts significant control of Bitcoin. China has firmly suppressed domestic use of cryptocurrencies, whereas the current administration seems intent on integrating them (and their inevitable grifts) into the US financial system. Except for Bitmain, no-one in China gets eggs from the golden goose.  This asymmetry provides China with an way to disrupt the US financial system.</p><p>
It would be important to prevent the disruption being attributed to China. A necessary precursor would therefore be to obscure the extent of Bitmain-affiliated pools' mining power. This has been a significant trend in the past year, note the change in the "unknown" in the graphs from 38 to 305. There could be other explanations, but whether or not intentionally this is creating a weapon.<sup><a href="#Footnote15">[15]</a></sup></p><h3>Secured by cryptography (1)</h3><p>
The dollars in your bank account are simply an entry in the bank's private ledger tagged with your name. You control this entry, but what you own is a claim on the bank<sup><a href="#Footnote16">[16]</a></sup>.  Similarly, your cryptocurrency coins are effectively an entry in a public ledger tagged with the public half of a key pair. The two differences are that:</p><ul>
<li>No ownership is involved, so you have no recourse if something goes wrong.</li>
<li>Anyone who knows the secret half of the key pair controls the entry. Since it is extremely difficult to stop online secrets leaking, something is likely to go wrong<sup><a href="#Footnote17">[17]</a></sup>.</li>
</ul>
<p>
The secret half of your key can leak via what Randall Munro depicted as a "<a href="https://xkcd.com/538/">wrench attack</a>", via phishing, social engineering, <a href="https://blog.dshr.org/2025/03/software-supply-chain-attack.html">software supply chain attacks</a><sup><a href="#Footnote18">[18]</a></sup>, and other forms of malware.  Preventing these risks requires you to maintain an <a href="https://blog.dshr.org/2022/02/inadequate-opsec.html">extraordinary level of operational security</a>.</p><h3>Secured by cryptography (2)</h3><p>
Even <i>perfect</i> opsec may not be enough. Bitcoin and most cryptocurrencies use two cryptographic algorithms, <a href="https://www.kraken.com/learn/how-do-cryptocurrencies-use-cryptography">SHA256 for hashing and ECDSA for signatures</a>.</p><div><p>
Quote from: llama on July 01, 2010, 10:21:47 PM</p><blockquote>
Satoshi, That would indeed be a solution if SHA was broken (certainly the more likely meltdown), because we could still recognize valid money owners by their signature (their private key would still be secure).<p>

However, if something happened and the signatures were compromised (perhaps integer factorization is solved, quantum computers?), then even agreeing upon the last valid block would be worthless.
</p></blockquote><p>
True, if it happened suddenly. If it happens gradually, we can still transition to something stronger. When you run the upgraded software for the first time, it would re-sign all your money with the new stronger signature algorithm. (by creating a transaction sending the money to yourself with the stronger sig)
</p><blockquote>
<small>
Satoshi Nakamoto: 10<sup>th</sup> July 2010
</small>
</blockquote>
</div><p>
On 10<sup>th</sup> July 2010 Nakamoto addressed the issue of what would happen if either of these algorithms were compromised.  There are three problems with his response; that compromise is likely in the near future, when it does Nakamoto's fix is inadequate, and there is a huge incentive for it to happen suddenly:</p><h3>Secured by cryptography (3)</h3><p>
Divesh Aggarwal <i>et al</i>'s 2019 paper <a href="https://arxiv.org/abs/1710.10377"><i>Quantum attacks on Bitcoin, and how to protect against them</i></a> noted that:
</p><blockquote>
the elliptic curve signature scheme used by Bitcoin is much more at risk, and could be completely broken by a quantum computer as early as 2027, by the most optimistic estimates.
</blockquote><p>
Their "most optimistic estimates" are likely to be correct; <a href="https://www.psiquantum.com/blueprint">PsiQuantum</a> expects to have two 1M qubit computers operational in 2027<sup><a href="#Footnote19">[19]</a></sup>. Each should be capable of breaking an ECDSA key in under a week.</p><p>

Bitcoin's transition to post-quantum cryptography faces a major problem because, to transfer coins from an ECDSA wallet to a post-quantum wallet, you need the key for the ECDSA wallet. Chainalysis <a href="https://blog.chainalysis.com/reports/money-supply">estimates that</a>:
</p><blockquote>
about 20% of all Bitcoins have been "lost", or in other words are sitting in wallets whose keys are inaccessible
</blockquote><p>
An example is the notorious <a href="https://en.wikipedia.org/wiki/Bitcoin_buried_in_Newport_landfill">hard disk in the garbage dump</a>. A sufficiently powerful quantum computer could recover the lost keys.</p><p>

The incentive for it to happen suddenly is that, even if Nakamoto's fix were in place, someone with access to the first sufficiently powerful quantum computer could transfer 20% of all Bitcoin, currently worth $460B, to <a href="https://blog.dshr.org/2025/05/the-740b-prize.html">post-quantum wallets they controlled</a>. This would be a 230x return on the investment in PsiQuantum.</p><h3>Privacy-preserving</h3>
<div><p>
privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone.</p><p>

As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner.</p><p>

Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.</p><blockquote>
<small>
Satoshi Nakamoto: <a href="https://bitcoin.org/bitcoin.pdf"><i>Bitcoin: A Peer-to-Peer Electronic Cash System</i></a>
</small>
</blockquote>
</div><p>
Nakamoto addressed the concern that, unlike DigiCash, because Bitcoin's blockchain was public it wasn't <a href="https://bitcoin.org/bitcoin.pdf">anonymous</a>:</p><blockquote>
privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone.
</blockquote><p>
This is true but misleading. In practice, users need to use exchanges and other services that can tie them to a public key.
There is a flourishing ecosystem of companies that deanonymize wallets by <a href="https://search.worldcat.org/title/1298713583">tracing the web of transactions</a>. Nakamoto <a href="https://bitcoin.org/bitcoin.pdf">added</a>:</p><blockquote>
As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner.
</blockquote><p>
This advice is just unrealistic. As <a href="https://blog.mollywhite.net/abuse-and-harassment-on-the-blockchain/">Molly White wrote</a><sup><a href="#Footnote20">[20]</a></sup>:</p><blockquote>
funds in a wallet have to come from somewhere, and it’s not difficult to infer what might be happening when your known wallet address suddenly transfers money off to a new, empty wallet. 
</blockquote><p>
Nakamoto <a href="https://bitcoin.org/bitcoin.pdf">acknowledged</a>:</p><blockquote>
Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.
</blockquote><p>
For more than a decade <a href="https://github.com/jlopp/physical-bitcoin-attacks/blob/master/README.md">Jamison Lopp</a> has been tracking what happens when a wallet with significant value is deanonymized, and it is a <a href="https://blog.dshr.org/2025/05/the-risks-of-hodl-ing.html">serious risk to life and limbs</a><sup><a href="#Footnote21">[21]</a></sup>.</p><h3>One more risk</h3><p>
I have steered clear of the financial risks of cryptocurrencies. It  may appear that the endorsement of the current administration has effectively removed their financial risk. But the technical and operational risks remain, and I should note another technology-related risk.</p><p>
Equities are currently being <a href="https://www.bloodinthemachine.com/p/the-ai-bubble-is-so-big-its-propping">inflated by the AI bubble</a>. The AI platforms are <a href="https://blog.dshr.org/2025/08/the-drugs-are-taking-hold.html">running the drug-dealer's algorithm</a>, "the first one's free", burning cash by offering their product free or massively under-priced. This cannot last; only <a href="https://www.zdnet.com/article/only-8-of-americans-would-pay-extra-for-ai-according-to-zdnet-aberdeen-research/">8% of their users would pay</a> even the current price. <a href="https://garymarcus.substack.com/p/openais-waterloo">OpenAI's August launch of GPT-5</a>, which was about <a href="https://www.theregister.com/2025/08/13/gpt_5_cost_cutting/">cost-cutting not better functionality</a>, and <a href="https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed">Anthropic's cost increases</a> were both panned by the customers who do pay.  AI may deliver some value, but it doesn't come close to the cost of delivering it<sup><a href="#Footnote22">[22]</a></sup>.</p><p>

There is likely to be an epic AI equity bust. <a href="https://www.ft.com/content/7052c560-4f31-4f45-bed0-cbc84453b3ce">Analogies</a> are being drawn to the <a href="https://www.noahpinion.blog/p/will-data-centers-crash-the-economy">telecom boom</a>, but <a href="https://www.economist.com/finance-and-economics/2025/09/07/what-if-the-ai-stockmarket-blows-up"><i>The Economist</i> reckons</a><sup><a href="#Footnote23">[23]</a></sup>:</p><blockquote>
the potential AI bubble lags behind only the three gigantic railway busts of the 19th century.
</blockquote>
<p>
History shows a fairly strong and increasing correlation between equities and cryptocurrencies, so they will get dragged down too. The automatic liquidation of leveraged long positions in DeFi will start, causing a self-reinforcing downturn. Periods of heavy load such as this tend to reveal bugs in IT systems, and especially in "smart contracts", as their assumptions of adequate resources and timely responses are violated.</p><p>
Experience shows that Bitcoin's limited transaction rate and the fact that the Ethereum computer that runs all the "smart contracts" is 1000 times slower than a $50 Raspberry Pi 4<sup><a href="#Footnote24">[24]</a></sup> lead to major slow-downs and fee spikes during panic selling, exacerbated by the fact that the panic sales are public<sup><a href="#Footnote25">[25]</a></sup>.</p><h3>Conclusion</h3><p>
The fascinating thing about cryptocurrency technology is the number of ways people have developed and how much they are willing to pay to avoid actually using it.  What other transformative technology has had people desperate not to use it?</p><p>

The whole of TradFi has been erected on this much worse infrastructure, including exchanges, <a href="https://blog.dshr.org/2022/04/grayscale-bitcoin-trust.html">closed-end funds</a>, ETFs, <a href="https://en.wikipedia.org/wiki/Hypothec#Hypothecation_and_rehypothecation">rehypothecation</a>, and derivatives.  Clearly, the only reason for doing so is to escape regulation and extract excess profits from what would otherwise be crimes.</p><h3>Footnotes</h3>
<ol start="1">
<li id="Footnote1">
The cause was the <a href="https://www.youtube.com/watch?v=twrduL8aNGE">video</a> of a talk I gave at Stanford in 2022 entitled <a href="https://blog.dshr.org/2022/02/ee380-talk.html"><i>Can We Mitigate The Externalities Of Cryptocurrencies?</i></a>. It was an updated version of a talk at the 2021 <a href="https://blog.dshr.org/2021/12/talk-at-ttivanguard-conference.html">TTI/Vanguard conference</a>. The talk conformed to <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines">Betteridge's Law of Headlines</a> in that the answer was "no".<br>
</li>
<li id="Footnote2">
Paper libraries form a model fault-tolerant system. It is highly replicated and decentralized. Libraries cooperate via inter-library loan and copy to deliver a service that is far more reliable than any individual library. 
</li>
<li id="Footnote3">
The importance Satoshi Nakamoto attached to trustlessness can be seen from his <a href="https://web.archive.org/web/20110822150926/https://p2pfoundation.ning.com/forum/topics/bitcoin-open-source">release note for Bitcoin 0.1</a>:
<blockquote>
The root problem with conventional currency is all the trust that's required to make it work. The central bank must be trusted not to debase the currency, but the history of fiat currencies is full of breaches of that trust. Banks must be trusted to hold our money and transfer it electronically, but they lend it out in waves of credit bubbles with barely a fraction in reserve. We have to trust them with our privacy, trust them not to let identity thieves drain our accounts. Their massive overhead costs make micropayments impossible.
</blockquote>
The problem with this ideology is that trust (but verify) is an incredibly effective optimization in almost any system. For example, Robert Putnam <i>et al</i>'s <a href="https://search.worldcat.org/title/52234023"><i>Making Democracy Work: Civic Traditions in Modern Italy</i></a> shows that the difference between the economies of Northern and Southern Italy is driven by the much higher level of trust in the North.<p>

Bitcoin's massive cost is a result of its lack of trust. Users pay this massive cost but they don't get a trustless system, they just get a system that makes the trust a bit harder to see.</p><p>

In response to Nakamoto's diatribe, note that:</p><ul>
<li>"trusted not to debase the currency", but Bitcoin's security depends upon debasing the currency.</li>
<li>"waves of credit bubbles", is a pretty good description of the cryptocurrency market.</li>
<li>"not to let identity thieves drain our accounts", see Molly White's <a href="https://www.web3isgoinggreat.com/"><i>Web3 is Going Just Great</i></a>.</li>
<li>"massive overhead costs". The current cost per transaction is <a href="https://www.blockchain.com/explorer/charts/cost-per-transaction">around $100</a>.</li>
</ul>
I rest my case.
</li>
<li id="Footnote4">
The problem of trusting mining pools is actually much worse. There is nothing to stop pools <strike>conspiring</strike> coordinating. In 2017 <a href="https://en.wikipedia.org/wiki/Vitalik_Buterin">Vitalik Buterin</a>, co-founder of Ethereum, published <a href="https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274"><i>The Meaning of Decentralization</i></a>:<br>
<blockquote>
In the case of blockchain protocols, the mathematical and economic reasoning behind the safety of the consensus often relies crucially on the uncoordinated choice model, or the assumption that the game consists of many small actors that make decisions independently. If any one actor gets more than 1/3 of the mining power in a proof of work system, they can gain outsized profits by selfish-mining. However, can we really say that the uncoordinated choice model is realistic when 90% of the Bitcoin network’s mining power is well-coordinated enough to show up together at the same conference? 
</blockquote>
See <a href="https://blog.dshr.org/2024/05/sufficiently-decentralized.html"><i>"Sufficiently Decentralized"</i></a> for a review of evidence from a Protos article entitled <a href="https://protos.com/new-research-suggests-bitcoin-mining-centralized-around-bitmain/"><i>New research suggests Bitcoin mining centralized around Bitmain</i></a> that concludes:<br>
<blockquote>
In all, it seems unlikely that up to nine major bitcoin mining pools use a shared custodian for coinbase rewards unless a single entity is behind all of their operations. 
</blockquote>
The "single entity" is clearly Bitmain.
</li>
<li id="Footnote5">
Peter Ryan, a reformed Bitcoin enthusiast, noted another form of centralization in <a href="https://www.compactmag.com/article/money-by-vile-means/"><i>Money by Vile Means</i></a>:<br>
<blockquote>
Bitcoin is anything but decentralized: Its functionality is maintained by a small and privileged clique of software developers who are funded by a centralized cadre of institutions. If they wanted to change Bitcoin’s 21 million coin finite supply, they could do it with the click of a keyboard.
</blockquote>
His account of the politics behind the argument over raising the Bitcoin block size should dispel any idea of Bitcoin's decentralized nature.  He also <a href="https://www.compactmag.com/article/money-by-vile-means/">notes</a>:<br>
<blockquote>
By one estimate from Hashrate Index, Foundry USA and Singapore-based AntPool control more than 50 percent of computing power, and the top ten mining pools control over 90 percent. Bitcoin blogger 0xB10C, who <a href="https://b10c.me/blog/015-bitcoin-mining-centralization/">analyzed</a> mining data as of April 15, 2025, found that centralization has gone even further than this, “with only six pools mining more than 95 percent of the blocks.”
</blockquote>
</li>
<li id="Footnote6">
The <a href="https://perfecthashrate.com/asics/antminer-s17-mining-hashrate/">Bitmain S17</a> comes in 4 versions with hash rates from 67 to 76 TH/s. Lets assume 70TH/s. As I write the Bitcoin hash rate is about 1 billion TH/s. So if they were all mid-range S17s there would be around 15M mining. If their economic life were 18 months, there would be 77,760 rewards. Thus only 0.5% of them would earn a reward.<p>

In December 2021 Alex de Vries and Christian Stoll <a href="https://doi.org/10.1016/j.resconrec.2021.105901">estimated that</a>:</p><blockquote>
The average time to become unprofitable sums up to less than 1.29 years. 
</blockquote>
It has been obvious since mining ASICs first hit the market that, apart from access to cheap or free electricity, there were two keys to profitable mining:<br>
<ol>
<li>Having close enough ties to Bitmain to get the latest chips early in their 18-month economic life.</li>
<li>Having the scale to buy Bitmain chips in the large quantities that get you early access.</li>
</ol>
</li>
<li id="Footnote7">
See David Gerard's account of Steve Early's experiences accepting Bitcoin in his chain of pubs in <a href="https://davidgerard.co.uk/blockchain/book/"><i>Attack of the 50 Foot Blockchain</i></a> Page 94.
<a href="https://www.kansascityfed.org/research/payments-system-research-briefings/us-consumers-use-of-cryptocurrency-for-payments/"><i>U.S. Consumers’ Use of Cryptocurrency for Payments</i></a> by Fumiko Hayashi and Aditi Routh of the Kansas City Fed reports that:<br>
<blockquote>
The share of U.S. consumers who report using cryptocurrency for payments—purchases, money transfers, or both—has been very small and has declined slightly in recent years. The light blue line in Chart 1 shows that this share declined from nearly 3 percent in 2021 and 2022 to less than 2 percent in 2023 and 2024.
</blockquote>
</li>
<li id="Footnote8">
User DeathAndTaxes on Stack Exchange explains the <a href="https://bitcoin.stackexchange.com/questions/1170/why-is-6-the-number-of-confirms-that-is-considered-secure">6 block rule</a>:<br>
<blockquote>
<a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Rdlmwa4wUnJuwPrbjcs9qy3a6vTGf343LjXoHcyTkQBs3zce4FdNsSC__zKbwanXirtqHq4QW2CI_urbBybfLi3LunH-ol3nPsIky49xE1UI6-vMCEFTS7C-DvBe3_l4_I-lZ46rO54i4a-B4nKrwSzNcM18CmvRMZOZY025NJNtyo5pAC_fondpV7wb/s416/SatoshiTable.png"><img data-original-height="416" data-original-width="380" height="200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg-Rdlmwa4wUnJuwPrbjcs9qy3a6vTGf343LjXoHcyTkQBs3zce4FdNsSC__zKbwanXirtqHq4QW2CI_urbBybfLi3LunH-ol3nPsIky49xE1UI6-vMCEFTS7C-DvBe3_l4_I-lZ46rO54i4a-B4nKrwSzNcM18CmvRMZOZY025NJNtyo5pAC_fondpV7wb/w183-h200/SatoshiTable.png" width="183"></a>
p is the chance of attacker eventually getting longer chain and reversing a transaction (0.1% in this case). q is the % of the hashing power the attacker controls. z is the number of blocks to put the risk of a reversal below p (0.1%).<p>

So you can see if the attacker has a small % of the hashing power 6 blocks is sufficient. Remember 10% of the network at the time of writing is ~100GH/s. However if the attacker had greater % of hashing power it would take increasingly longer to be sure a transaction can't be reversed.</p><p>

If the attacker had significantly more hashpower say 25% of the network it would require 15 confirmation to be sure (99.9% probability) that an attacker can't reverse it.
</p></blockquote>
For example, last May Foundry USA had more than 30% of the hash power, so the rule should have been 24 not 6, and finality should have taken 4 hours.
</li>
<li id="Footnote9">
To be fair, Ethereum has introduced at least one genuine innovation, <a href="https://blog.dshr.org/2023/06/flash-loans.html"><i>Flash Loans</i></a>. In <a href="https://bankunderground.co.uk/2023/05/25/flash-loans-flash-attacks-and-the-future-of-defi/"><i>Flash loans, flash attacks, and the future of DeFi</i></a> Aidan Saggers, Lukas Alemu and Irina Mnohoghitnei of the Bank of England provide an excellent overview of them.
Back in 2021 Kaihua Qin, Liyi Zhou, Benjamin Livshits, and Arthur Gervais from Imperial College posted <a href="https://arxiv.org/pdf/2003.03810.pdf"><i>Attacking the defi ecosystem with flash loans for fun and profit</i></a>, analyzing and optimizing two early flash loan attacks:<br>
<blockquote>
We show quantitatively how transaction atomicity increases the arbitrage revenue. We moreover analyze two existing attacks with ROIs beyond 500k%. We formulate finding the attack parameters as an optimization problem over the state of the underlying Ethereum blockchain and the state of the DeFi ecosystem. We show how malicious adversaries can efficiently maximize an attack profit and hence damage the DeFi ecosystem further. Specifically, we present how two previously executed attacks can be “boosted” to result in a profit of 829.5k USD and 1.1M USD, respectively, which is a boost of 2.37× and 1.73×, respectively. 
</blockquote>
They predicted an upsurge in attacks since "flash loans democratize the attack, opening this strategy to the masses". They were right, as you can see from Molly White's <a href="https://www.web3isgoinggreat.com/?collection=flash-loan-attack">list of flash loan attacks</a>.
</li>
<li id="Footnote10">
This is one of a whole series of <a href="https://blog.dshr.org/2022/09/impossibilities.html"><i>Impossibilities</i></a>, many imposed on Ethereum by fundamental results in computer science because it is a Turing-complete programming environment.
</li>
<li id="Footnote11">
For details of the story behind Miners' Extractable Value (MEV), see these posts:<br>
<ol>
<li><a href="https://blog.dshr.org/2020/11/the-order-flow.html"><i>The Order Flow</i></a> from November 2020.</li>
<li><a href="https://blog.dshr.org/2022/04/ethereum-has-issues.html"><i>Ethereum Has Issues</i></a> from April 2022.</li>
<li><a href="https://blog.dshr.org/2022/09/miners-extractable-value.html"><i>Miners' Extractable Value</i></a> From September 2022.</li>
</ol>

The first links to two must-read posts. The first is from Dan Robinson and Georgios Konstantopoulos, <a href="https://medium.com/@danrobinson/ethereum-is-a-dark-forest-ecc5f0505dff"><i>Ethereum is a Dark Forest</i></a>:<br>
<blockquote>
It’s no secret that the Ethereum blockchain is a highly adversarial environment. If a smart contract can be exploited for profit, it eventually will be. The frequency of new hacks indicates that some very smart people spend a lot of time examining contracts for vulnerabilities.<p>

But this unforgiving environment pales in comparison to the mempool (the set of pending, unconfirmed transactions). If the chain itself is a battleground, the mempool is something worse: a dark forest. 
</p></blockquote>
The second is from Samczsun, <a href="https://samczsun.com/escaping-the-dark-forest/"><i>Escaping the Dark Forest</i></a>. It is an account of how:<br>
<blockquote>
On September 15, 2020, a small group of people worked through the night to rescue over 9.6MM USD from a vulnerable smart contract.
</blockquote>
Note in particular that MEV poses a risk to the integrity of blockchains. In <a href="https://doi.org/10.48550/arXiv.2203.15930"><i>Extracting Godl [sic] from the Salt Mines: Ethereum Miners Extracting Value</i></a> Julien Piet, Jaiden Fairoze and Nicholas Weaver examine the use of transactions that avoid the mempool, finding that:<br>
<blockquote>
(i) 73% of private transactions hide trading activity or re-distribute miner rewards, and 87.6% of MEV collection is accomplished with privately submitted transactions, (ii) our algorithm finds more than $6M worth of MEV profit in a period of 12 days, two thirds of which go directly to miners, and (iii) MEV represents 9.2% of miners' profit from transaction fees.<p>

Furthermore, in those 12 days, we also identify four blocks that contain enough MEV profits to make time-bandit forking attacks economically viable for large miners, undermining the security and stability of Ethereum as a whole. 
</p></blockquote>
When they say "large miners" they mean more than 10% of the power.
</li>
<li id="Footnote12">
Back in 2016 Arvind Narayanan's group at Princeton had published a related instability in Carlsten <i>et al</i>'s <a href="http://randomwalker.info/publications/mining_CCS.pdf"><i>On the instability of bitcoin without the block reward</i></a>. Narayanan summarized the paper in a <a href="https://freedom-to-tinker.com/2016/10/21/bitcoin-is-unstable-without-the-block-reward/">blog post</a>:<br>
<blockquote>
Our key insight is that with only transaction fees, the variance of the miner reward is very high due to the randomness of the block arrival time, and it becomes attractive to fork a “wealthy” block to “steal” the rewards therein. 
</blockquote>
</li>
<li id="Footnote13">
The leading source of data on which to base Bitcoin's carbon footprint is the <a href="https://cbeci.org/"><i>Cambridge Bitcoin Energy Consumption Index</i></a>. As I write their central estimate is that Bitcoin consumes 205TWh/year, or between Thailand and Vietnam.
</li>
<li id="Footnote14">
Ferreira <i>et al</i> <a href="https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=3320437">write</a>:<br>
<blockquote>
AntPool and BTC.com are fully-owned subsidiaries of Bitmain. Bitmain is the largest investor in ViaBTC. Both F2Pool and BTC.TOP are partners of BitDeer, which is a Bitmain-sponsored cloud-mining service. The parent companies of Huobi.pool and OkExPool are strategic partners of Bitmain. Jihan Wu, Bitmain’s founder and chairman, is also an adviser of Huobi (one of the largest cryptocurrency exchanges in the world and the owner of Huobi.pool).
</blockquote>
This makes economic sense. Because mining rigs depreciate quickly, profit depends upon <a href="https://blog.dshr.org/2018/05/asics-and-mining-centralization.html">early access to the latest chips</a>.
</li>
<li id="Footnote15">
See <a href="https://blog.dshr.org/2025/05/who-is-mining-bitcoin.html"><i>Who Is Mining Bitcoin?</i></a> for more detail on the state of mining and its gradual obfuscation.
</li>
<li id="Footnote16">
In this context to say you "control" your entry in the bank's ledger is an oversimplification. You can instruct the bank to perform transactions against your entry (and no-one else's) but the bank can reject your instructions. For example if they would overdraw your account, or send money to a sanctioned account. The key point is that your ownership relationship with the bank comes with a dispute resolution system and the ability to reverse transactions. Your cryptocurrency wallet has neither.
</li><li id="Footnote17">
<a href="https://www.web3isgoinggreat.com/"><i>Web3 is Going Just Great</i></a> is Molly White's list of things that went wrong. The cumulative losses she tracks currently stand at over $79B.
</li>
<li id="Footnote18">
Your secrets are especially at risk if anyone in your software supply chain use a build system implemented using AI "vibe coding". David Gerard's <a href="https://pivot-to-ai.com/2025/08/29/vibe-coded-build-system-nx-gets-hacked-steals-vibe-coders-crypto/"><i>Vibe-coded build system NX gets hacked, steals vibe-coders’ crypto</i></a> details a truly beautiful example of the extraordinary level of incompetence this reveals.
</li>
<li id="Footnote19">
<a href="https://en.wikipedia.org/wiki/IBM_Heron">IBM's Heron</a>, which HSBC recently used to <a href="https://www.bloomberg.com/news/articles/2025-09-24/hsbc-says-it-s-beaten-wall-street-rivals-with-new-quantum-trial">grab headlines</a>, has 156 qubits.
</li>
<li id="Footnote20">
Molly White's <a href="https://blog.mollywhite.net/abuse-and-harassment-on-the-blockchain/"><i>Abuse and harassment on the blockchain</i></a> is an excellent overview of the privacy risks inherent to real-world transactions on public blockchain ledgers:<br>
<blockquote>
Imagine if, when you Venmo-ed your Tinder date for your half of the meal, they could now see every other transaction you’d ever made—and not just on Venmo, but the ones you made with your credit card, bank transfer, or other apps, and with no option to set the visibility of the transfer to “private”. The split checks with all of your previous Tinder dates? That monthly transfer to your therapist? The debts you’re paying off (or not), the charities to which you’re donating (or not), the amount you’re putting in a retirement account (or not)? The location of that corner store right by your apartment where you so frequently go to grab a pint of ice cream at 10pm? Not only would this all be visible to that one-off Tinder date, but also to your ex-partners, your estranged family members, your prospective employers. An abusive partner could trivially see you siphoning funds to an account they can’t control as you prepare to leave them.
</blockquote>
</li>
<li id="Footnote21">
In <a href="https://blog.dshr.org/2025/05/the-risks-of-hodl-ing.html"><i>The Risks Of HODL-ing</i></a> I go into the details of the attack on the <a href="https://www.nytimes.com/2025/04/24/magazine/crybercrime-crypto-minecraft.html">parents of Veer Chetal</a>, who had unwisely live-streamed the social engineering that stole $243M from a resident of DC.<p>

Anyone with significant cryptocurrency wallets needs to follow Jamison Lopp's <a href="https://github.com/jlopp/physical-bitcoin-attacks/blob/master/README.md">Known Physical Bitcoin Attacks</a>.
</p></li>
<li id="Footnote22">

Torsten Sløk's <a href="https://www.apolloacademy.com/ai-has-moved-from-a-niche-sector-to-the-primary-driver-of-all-vc-investment/"><i>AI Has Moved From a Niche Sector to the Primary Driver of All VC Investment</i></a> leads with this graph, one of the clearest signs that we're in a bubble.<p>

Whether AI delivers net value in most cases is debatable. "Vibe coding" is touted as the example of increasing productivity, but the <a href="https://pivot-to-ai.com/2025/07/11/ai-coders-think-theyre-20-faster-but-theyre-actually-19-slower/">experimental</a> <a href="https://mikelovesrobots.substack.com/p/wheres-the-shovelware-why-ai-coding">evidence</a> is that it decreases productivity. Kate Niederhoffer <i>et al</i>'s <i>Harvard Business Review</i> article <a href="https://hbr.org/2025/09/ai-generated-workslop-is-destroying-productivity"><i>AI-Generated "Workslop” Is Destroying Productivity</i></a> explains one effect:</p><blockquote>
Employees are using AI tools to create low-effort, passable looking work that ends up creating more work for their coworkers. On social media, which is increasingly clogged with low-quality AI-generated posts, this content is often referred to as “AI slop.” In the context of work, we refer to this phenomenon as “<a href="https://www.betterup.com/workslop">workslop</a>.” We define workslop as <i>AI generated work content that masquerades as good work, but lacks the substance to meaningfully advance a given task</i>.<p>

Here’s how this happens. As AI tools become more accessible, workers are increasingly able to quickly produce polished output: well-formatted slides, long, structured reports, seemingly articulate summaries of academic papers by non-experts, and usable code. But while some employees are using this ability to polish good work, others use it to create content that is actually unhelpful, incomplete, or missing crucial context about the project at hand. The insidious effect of workslop is that it shifts the burden of the work downstream, requiring the receiver to interpret, correct, or redo the work. In other words, it transfers the effort from creator to receiver.
</p></blockquote>
David Gerard's <a href="https://pivot-to-ai.com/2025/09/23/workslop-bad-study-but-an-excellent-word/"><i>Workslop: bad ‘study’, but an excellent word</i></a> points out that:<br>
<blockquote>
<i>Unfortunately</i>, this article pretends to be a writeup of a study — but it’s actually a promotional brochure for enterprise AI products. It’s an unlabeled advertising feature.
</blockquote>
And goes on to explain where the workslop comes from:<br>
<blockquote>
Well, you know how you get workslop — it’s when your boss mandates you use AI. He can’t say what he wants you to use it for. But you’ve been told. You’ve got metrics on how much AI you use. They’re watching and they’re measuring.
</blockquote>
Belle Lin and Steven Rosenbush's <a href="https://www.wsj.com/articles/stop-worrying-about-ais-return-on-investment-d5cbc822"><i>Stop Worrying About AI’s Return on Investment</i></a> describes goalposts being moved:<br>
<blockquote>
Return on investment has evaded chief information officers since AI started <a href="https://www.wsj.com/articles/companies-had-fun-experimenting-with-ai-now-they-have-to-show-the-returns-2a683592">moving from early experimentation to more mature implementations</a> last year. But while AI is still rapidly evolving, CIOs are recognizing that traditional ways of recognizing gains from the technology aren’t cutting it.<p>

Tech leaders at the WSJ Leadership Institute’s Technology Council Summit on Tuesday said racking up a few minutes of efficiency here and there don’t add up to a meaningful way of measuring ROI.
</p></blockquote>
Given the hype and the massive sunk costs, admitting that there is no there there would be a career-limiting move.<p>

None of this takes account of the productivity externalities of AI, such as <a href="https://www.404media.co/librarians-are-being-asked-to-find-ai-hallucinated-books/"><i>Librarians Are Being Asked to Find AI-Hallucinated Books</i></a>, <a href="https://pivot-to-ai.com/2025/02/15/how-ai-slop-generators-started-talking-about-vegetative-electron-microscopy/">academic journals' reviewers' time wasted by AI slop papers</a>, <a href="https://pivot-to-ai.com/2025/06/07/uk-high-court-to-lawyers-cut-the-chatgpt-or-else/">judges' time wasted with hallucinated citations</a>, a flood of generated <a href="https://www.theguardian.com/technology/2025/jul/10/ai-generated-child-sexual-abuse-videos-surging-online-iwf">child sex abuse videos</a>, <a href="https://www.noemamag.com/the-last-days-of-social-media/">the death of social media</a> and a <a href="https://www.washingtonpost.com/technology/2025/09/20/ai-hacking-cybersecurity-cyberthreats/">vast new cyberthreat landscape</a>.
</p></li>
<li id="Footnote23">
<i>The Economist</i> writes in <a href="https://www.economist.com/finance-and-economics/2025/09/07/what-if-the-ai-stockmarket-blows-up"><i>What if the AI stockmarket blows up?</i></a>:<br>
<blockquote>
we picked ten historical bubbles and assessed them on factors including spark, cumulative capex, capex durability and investor group. By our admittedly rough-and-ready reckoning, the potential AI bubble lags behind only the three gigantic railway busts of the 19th century.
</blockquote>
They <a href="https://www.economist.com/finance-and-economics/2025/09/07/what-if-the-ai-stockmarket-blows-up">note that</a>:<br>
<blockquote>
For now, the splurge looks fairly modest by historical standards. According to our most generous estimate, American AI firms have invested 3-4% of current American GDP over the past four years. British railway investment during the 1840s was around 15-20% of GDP. But if forecasts for data-centre construction are correct, that will change. What is more, an unusually large share of capital investment is being devoted to assets that depreciate quickly. Nvidia’s cutting-edge chips will look clunky in a few years’ time. We estimate that the average American tech firm’s assets have a shelf-life of just nine years, compared with 15 for telecoms assets in the 1990s.
</blockquote>
I think they are over-estimating the shelf-life. Like Bitcoin mining, power is a major part of AI opex. Thus the incentive to (a) retire older, less power-efficient hardware, and (b) adopt the latest data-center power technology, is overwhelming. Note that Nvidia is moving to a one-year product cadence, and even when they were on a two-year cadence Jensen claimed it wasn't worth running chips from the previous cycle. Note also that the current generation of AI systems is incompatible with the power infrastructure of older data centers, and this may well happen again in a future product generation.  For example, Caiwei Chen reports in <a href="https://www.technologyreview.com/2025/03/26/1113802/china-ai-data-centers-unused/"><i>China built hundreds of AI data centers to catch the AI boom. Now many stand unused</i></a>:<br>
<blockquote>
The local Chinese outlets <i>Jiazi Guangnian</i> and <i>36Kr</i> report that up to 80% of China’s newly built computing resources remain unused. 
</blockquote>
Rogé Karma makes the same point as <i>The Economist</i> in <a href="https://www.theatlantic.com/economy/archive/2025/09/ai-bubble-us-economy/684128/"><i>Just How Bad Would an AI Bubble Be?</i></a>:<br>
<blockquote>
An AI-bubble crash could be different. AI-related investments have already <a href="https://paulkedrosky.com/honey-ai-capex-ate-the-economy/">surpassed</a> the level that telecom hit at the peak of the dot-com boom as a share of the economy. In the first half of this year, business spending on AI added more to GDP growth than all consumer spending <i>combined</i>. Many experts believe that a major reason the U.S. economy has been able to weather tariffs and mass deportations without a recession is because all of this AI spending is acting, in the <a href="https://paulkedrosky.com/honey-ai-capex-ate-the-economy/">words</a> of one economist, as a “massive private sector stimulus program.” An AI crash could lead broadly to less spending, fewer jobs, and slower growth, potentially dragging the economy into a recession.
</blockquote>
</li>
<li id="Footnote24">
In 2021 Nicholas Weaver estimated that the Ethereum computer was 5000 times slower than a <a href="https://www.usenix.org/publications/loginonline/web3-fraud">Raspberry Pi 4</a>. Since then the gas limit has been raised making his current estimate only 1000 times slower.
</li>
<li id="Footnote25">
Prof. Hilary Allen writes in <a href="https://fintechdystopia.com/chapters/chapter4.html"><i>Fintech Dystopia</i></a> that:<br>
<blockquote>
if people do start dumping blockchain-based assets in fire sales, everyone will know immediately because the blockchain is publicly visible. This level of transparency will only add to the panic (at least, that’s what happened during the <a href="https://corpgov.law.harvard.edu/2023/05/22/anatomy-of-a-run-the-terra-luna-crash/">run on the Terra stablecoin</a> in 2022).<br>
...<br>
We also saw ... that assets on a blockchain can be pre-programmed to execute transactions without the intervention of any human being. In good times, this makes things more efficient – but the code will execute just as quickly in bad situations, even if everyone would be better off if it didn’t. 
</blockquote>
She <a href="https://fintechdystopia.com/chapters/chapter4.html">adds</a>:<br>
<blockquote>
When things are spiraling out of control like this, sometimes the best medicine is a pause. Lots of traditional financial markets close at the end of the day and on weekends, which provides a natural opportunity for a break (and if things are really bad, for emergency government intervention). But one of blockchain-based finance’s claims to greater efficiency is that operations continue 24/7. We may end up missing the pauses once they’re gone.
</blockquote>
In the 26<sup>th</sup> September <i>Grant's</i>, Joel Wallenberg notes that:<br>
<blockquote>
Lucrative though they may be, the problem with stablecoin deposits is that exposure to the crypto-trading ecosystem makes them inherently correlated to it and subject to runs in a new “crypto winter,” like that of 2022–23.  Indeed, since as much as 70% of gross stablecoin-transaction volume derives from automated arbitrage bots and high-speed trading algorithms, runs may be rapid and without human over-sight. What may be worse, the insured banks that could feed a stablecoin boom are the very ones that are likely to require taxpayer support if liquidity dries up, and Trump-style regulation is likely to be light.
</blockquote>
So the loophole in the GENIUS act for banks is likely to cause contagion from cryptocurrencies via stablecoins to the US banking system.
</li>
</ol>
<h3>Acknowledgments</h3><p>
This talk benefited greatly from critiques of drafts by Hilary Allen, David Gerard, Jon Reiter, Joel Wallenberg, and Nicholas Weaver.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CDC File Transfer (334 pts)]]></title>
            <link>https://github.com/google/cdc-file-transfer</link>
            <guid>45433768</guid>
            <pubDate>Wed, 01 Oct 2025 02:38:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google/cdc-file-transfer">https://github.com/google/cdc-file-transfer</a>, See on <a href="https://news.ycombinator.com/item?id=45433768">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CDC File Transfer</h2><a id="user-content-cdc-file-transfer" aria-label="Permalink: CDC File Transfer" href="#cdc-file-transfer"></a></p>
<p dir="auto">Born from the ashes of Stadia, this repository contains tools for syncing and
streaming files from Windows to Windows or Linux. The tools are based on Content
Defined Chunking (CDC), in particular
<a href="https://www.usenix.org/conference/atc16/technical-sessions/presentation/xia" rel="nofollow">FastCDC</a>,
to split up files into chunks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">At Stadia, game developers had access to Linux cloud instances to run games.
Most developers wrote their games on Windows, though. Therefore, they needed a
way to make them available on the remote Linux instance.</p>
<p dir="auto">As developers had SSH access to those instances, they could use <code>scp</code> to copy
the game content. However, this was impractical, especially with the shift to
working from home during the pandemic with sub-par internet connections. <code>scp</code>
always copies full files, there is no "delta mode" to copy only the things that
changed, it is slow for many small files, and there is no fast compression.</p>
<p dir="auto">To help this situation, we developed two tools, <code>cdc_rsync</code> and <code>cdc_stream</code>,
which enable developers to quickly iterate on their games without repeatedly
incurring the cost of transmitting dozens of GBs.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">CDC RSync</h2><a id="user-content-cdc-rsync" aria-label="Permalink: CDC RSync" href="#cdc-rsync"></a></p>
<p dir="auto"><code>cdc_rsync</code> is a tool to sync files from a Windows machine to a Linux device,
similar to the standard Linux <a href="https://linux.die.net/man/1/rsync" rel="nofollow">rsync</a>. It is
basically a copy tool, but optimized for the case where there is already an old
version of the files available in the target directory.</p>
<ul dir="auto">
<li>It quickly skips files if timestamp and file size match.</li>
<li>It uses fast compression for all data transfer.</li>
<li>If a file changed, it determines which parts changed and only transfers the
differences.</li>
</ul>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_rsync_recursive_upload_demo.gif"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_rsync_recursive_upload_demo.gif" alt="cdc_rsync demo" width="688" data-animated-image=""></a>
</p>
<p dir="auto">The remote diffing algorithm is based on CDC. In our tests, it is up to 30x
faster than the one used in <code>rsync</code> (1500 MB/s vs 50 MB/s).</p>
<p dir="auto">The following chart shows a comparison of <code>cdc_rsync</code> and Linux <code>rsync</code> running
under Cygwin on Windows. The test data consists of 58 development builds
of some game provided to us for evaluation purposes. The builds are 40-45 GB
large. For this experiment, we uploaded the first build, then synced the second
build with each of the two tools and measured the time. For example, syncing
from build 1 to build 2 took 210 seconds with the Cygwin <code>rsync</code>, but only 75
seconds with <code>cdc_rsync</code>. The three outliers are probably feature drops from
another development branch, where the delta was much higher. Overall,
<code>cdc_rsync</code> syncs files about <strong>3 times faster</strong> than Cygwin <code>rsync</code>.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_rsync_vs_cygwin_rsync.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_rsync_vs_cygwin_rsync.png" alt="Comparison of cdc_rsync and Linux rsync running in Cygwin" width="753"></a>
</p>
<p dir="auto">We also ran the experiment with the native Linux <code>rsync</code>, i.e syncing Linux to
Linux, to rule out issues with Cygwin. Linux <code>rsync</code> performed on average 35%
worse than Cygwin <code>rsync</code>, which can be attributed to CPU differences. We did
not include it in the figure because of this, but you can find it
<a href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_rsync_vs_cygwin_rsync_vs_linux_rsync.png">here</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How does it work and why is it faster?</h3><a id="user-content-how-does-it-work-and-why-is-it-faster" aria-label="Permalink: How does it work and why is it faster?" href="#how-does-it-work-and-why-is-it-faster"></a></p>
<p dir="auto">The standard Linux <code>rsync</code> splits a file into fixed-size chunks of typically
several KB.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/fixed_size_chunks.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/fixed_size_chunks.png" alt="Linux rsync uses fixed size chunks" width="258"></a>
</p>
<p dir="auto">If the file is modified in the middle, e.g. by inserting <code>xxxx</code> after <code>567</code>,
this usually means that <span>the modified chunks as well as
all subsequent chunks</span> change.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/fixed_size_chunks_inserted.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/fixed_size_chunks_inserted.png" alt="Fixed size chunks after inserting data" width="301"></a>
</p>
<p dir="auto">The standard <code>rsync</code> algorithm hashes the chunks of the remote "old" file
and sends the hashes to the local device. The local device then figures out
which parts of the "new" file matches known chunks.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/linux_rsync_animation.gif"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/linux_rsync_animation.gif" alt="Syncing a file with the standard Linux rsync" width="855" data-animated-image=""></a>
  <br>
  Standard rsync algorithm
</p>
<p dir="auto">This is a simplification. The actual algorithm is more complicated and uses
two hashes, a weak rolling hash and a strong hash, see
<a href="https://rsync.samba.org/tech_report/" rel="nofollow">here</a> for a great overview. What makes
<code>rsync</code> relatively slow is the "no match" situation where the rolling hash does
not match any remote hash, and the algorithm has to roll the hash forward and
perform a hash map lookup for each byte. <code>rsync</code> goes to
<a href="https://github.com/librsync/librsync/blob/master/src/hashtable.h">great lengths</a>
optimizing lookups.</p>
<p dir="auto"><code>cdc_rsync</code> does not use fixed-size chunks, but instead variable-size,
content-defined chunks. That means, chunk boundaries are determined by the
<em>local content</em> of the file, in practice a 64 byte sliding window. For more
details, see
<a href="https://www.usenix.org/conference/atc16/technical-sessions/presentation/xia" rel="nofollow">the FastCDC paper</a>
or take a look at <a href="https://github.com/google/cdc-file-transfer/blob/main/fastcdc/fastcdc.h">our implementation</a>.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/variable_size_chunks.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/variable_size_chunks.png" alt="cdc_rsync uses variable, content-defined size chunks" width="260"></a>
</p>
<p dir="auto">If the file is modified in the middle, only <span>the modified
chunks</span>, but not <span>subsequent chunks</span>
change (unless they are less than 64 bytes away from the modifications).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/variable_size_chunks_inserted.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/variable_size_chunks_inserted.png" alt="Content-defined chunks after inserting data" width="314"></a>
</p>
<p dir="auto">Computing the chunk boundaries is cheap and involves only a left-shift, a memory
lookup, an <code>add</code> and an <code>and</code> operation for each input byte. This is cheaper
than the hash map lookup for the standard <code>rsync</code> algorithm.</p>
<p dir="auto">Because of this, the <code>cdc_rsync</code> algorithm is faster than the standard
<code>rsync</code>. It is also simpler. Since chunk boundaries move along with insertions
or deletions, the task to match local and remote hashes is a trivial set
difference operation. It does not involve a per-byte hash map lookup.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_rsync_animation.gif"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_rsync_animation.gif" alt="Syncing a file with cdc_rsync" width="857" data-animated-image=""></a>
  <br>
  cdc_rsync algorithm
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">CDC Stream</h2><a id="user-content-cdc-stream" aria-label="Permalink: CDC Stream" href="#cdc-stream"></a></p>
<p dir="auto"><code>cdc_stream</code> is a tool to stream files and directories from a Windows machine to
a Linux device. Conceptually, it is similar to
<a href="https://github.com/libfuse/sshfs">sshfs</a>, but it is optimized for read speed.</p>
<ul dir="auto">
<li>It caches streamed data on the Linux device.</li>
<li>If a file is re-read on Linux after it changed on Windows, only the
differences are streamed again. The rest is read from the cache.</li>
<li>Stat operations are very fast since the directory metadata (filenames,
permissions etc.) is provided in a streaming-friendly way.</li>
</ul>
<p dir="auto">To efficiently determine which parts of a file changed, the tool uses the same
CDC-based diffing algorithm as <code>cdc_rsync</code>. Changes to Windows files are almost
immediately reflected on Linux, with a delay of roughly (0.5s + 0.7s x total
size of changed files in GB).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_stream_demo.gif"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_stream_demo.gif" alt="cdc_stream demo" width="688" data-animated-image=""></a>
</p>
<p dir="auto">The tool does not support writing files back from Linux to Windows; the Linux
directory is readonly.</p>
<p dir="auto">The following chart compares times from starting a game to reaching the menu.
In one case, the game is streamed via <code>sshfs</code>, in the other case we use
<code>cdc_stream</code>. Overall, we see a <strong>2x to 5x speedup</strong>.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/google/cdc-file-transfer/blob/main/docs/cdc_stream_vs_sshfs.png"><img src="https://github.com/google/cdc-file-transfer/raw/main/docs/cdc_stream_vs_sshfs.png" alt="Comparison of cdc_stream and sshfs" width="752"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Platforms</h2><a id="user-content-supported-platforms" aria-label="Permalink: Supported Platforms" href="#supported-platforms"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><code>cdc_rsync</code></th>
<th>From</th>
<th>To</th>
</tr>
</thead>
<tbody>
<tr>
<td>Windows x86_64</td>
<td>✓</td>
<td>✓ <sup>1</sup></td>
</tr>
<tr>
<td>Ubuntu 22.04 x86_64</td>
<td>✗ <sup>2</sup></td>
<td>✓</td>
</tr>
<tr>
<td>Ubuntu 22.04 aarch64</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>macOS 13 x86_64 <sup>3</sup></td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>macOS 13 aarch64 <sup>3</sup></td>
<td>✗</td>
<td>✗</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th><code>cdc_stream</code></th>
<th>From</th>
<th>To</th>
</tr>
</thead>
<tbody>
<tr>
<td>Windows x86_64</td>
<td>✓</td>
<td>✗</td>
</tr>
<tr>
<td>Ubuntu 22.04 x86_64</td>
<td>✗</td>
<td>✓</td>
</tr>
<tr>
<td>Ubuntu 22.04 aarch64</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>macOS 13 x86_64 <sup>3</sup></td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>macOS 13 aarch64 <sup>3</sup></td>
<td>✗</td>
<td>✗</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<span>
<p dir="auto"><sup>1</sup> Only local syncs, e.g. <code>cdc_rsync C:\src\* C:\dst</code>. Support for
remote syncs is being added, see
<a href="https://github.com/google/cdc-file-transfer/issues/61" data-hovercard-type="issue" data-hovercard-url="/google/cdc-file-transfer/issues/61/hovercard">#61</a>.<br>
<sup>2</sup> See <a href="https://github.com/google/cdc-file-transfer/issues/56" data-hovercard-type="issue" data-hovercard-url="/google/cdc-file-transfer/issues/56/hovercard">#56</a>.<br>
<sup>3</sup> See <a href="https://github.com/google/cdc-file-transfer/issues/62" data-hovercard-type="issue" data-hovercard-url="/google/cdc-file-transfer/issues/62/hovercard">#62</a>.</p>
</span>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Download the precompiled binaries from the
<a href="https://github.com/google/cdc-file-transfer/releases">latest release</a> to a
Windows device and unzip them. The Linux binaries are automatically deployed
to <code>~/.cache/cdc-file-transfer</code> by the Windows tools. There is no need to manually
deploy them. We currently provide Linux binaries compiled on
<a href="https://github.com/actions/runner-images">Github's latest Ubuntu</a> version.
If the binaries work for you, you can skip the following two sections.</p>
<p dir="auto">Alternatively, the project can be built from source. Some binaries have to be
built on Windows, some on Linux.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites for Building</h2><a id="user-content-prerequisites-for-building" aria-label="Permalink: Prerequisites for Building" href="#prerequisites-for-building"></a></p>
<p dir="auto">To build the tools from source, the following steps have to be executed on
<strong>both Windows and Linux</strong>.</p>
<ul dir="auto">
<li>Download and install Bazel from <a href="https://bazel.build/install" rel="nofollow">here</a>. See
<a href="https://github.com/google/cdc-file-transfer/actions">workflow logs</a> for the
currently used version.</li>
<li>Clone the repository.
<div data-snippet-clipboard-copy-content="git clone https://github.com/google/cdc-file-transfer"><pre><code>git clone https://github.com/google/cdc-file-transfer
</code></pre></div>
</li>
<li>Initialize submodules.
<div data-snippet-clipboard-copy-content="cd cdc-file-transfer
git submodule update --init --recursive"><pre><code>cd cdc-file-transfer
git submodule update --init --recursive
</code></pre></div>
</li>
</ul>
<p dir="auto">Finally, install an SSH client on the Windows machine if not present.
The file transfer tools require <code>ssh.exe</code> and <code>sftp.exe</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building</h2><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">The two tools CDC RSync and CDC Stream can be built and used independently.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CDC RSync</h3><a id="user-content-cdc-rsync-1" aria-label="Permalink: CDC RSync" href="#cdc-rsync-1"></a></p>
<ul dir="auto">
<li>On a Linux device, build the Linux components
<div data-snippet-clipboard-copy-content="bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_rsync_server"><pre><code>bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_rsync_server
</code></pre></div>
</li>
<li>On a Windows device, build the Windows components
<div data-snippet-clipboard-copy-content="bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_rsync"><pre><code>bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_rsync
</code></pre></div>
</li>
<li>Copy the Linux build output file <code>cdc_rsync_server</code> from
<code>bazel-bin/cdc_rsync_server</code> to <code>bazel-bin\cdc_rsync</code> on the Windows machine.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">CDC Stream</h3><a id="user-content-cdc-stream-1" aria-label="Permalink: CDC Stream" href="#cdc-stream-1"></a></p>
<ul dir="auto">
<li>On a Linux device, build the Linux components
<div data-snippet-clipboard-copy-content="bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_fuse_fs"><pre><code>bazel build --config linux --compilation_mode=opt --linkopt=-Wl,--strip-all --copt=-fdata-sections --copt=-ffunction-sections --linkopt=-Wl,--gc-sections //cdc_fuse_fs
</code></pre></div>
</li>
<li>On a Windows device, build the Windows components
<div data-snippet-clipboard-copy-content="bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_stream"><pre><code>bazel build --config windows --compilation_mode=opt --copt=/GL //cdc_stream
</code></pre></div>
</li>
<li>Copy the Linux build output files <code>cdc_fuse_fs</code> and <code>libfuse.so</code> from
<code>bazel-bin/cdc_fuse_fs</code> to <code>bazel-bin\cdc_stream</code> on the Windows machine.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">The tools require a setup where you can use SSH and SFTP from the Windows
machine to the Linux device without entering a password, e.g. by using key-based
authentication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuring SSH and SFTP</h3><a id="user-content-configuring-ssh-and-sftp" aria-label="Permalink: Configuring SSH and SFTP" href="#configuring-ssh-and-sftp"></a></p>
<p dir="auto">By default, the tools search <code>ssh.exe</code> and <code>sftp.exe</code> from the path environment
variable. If you can run the following commands in a Windows cmd without
entering your password, you are all set:</p>
<div data-snippet-clipboard-copy-content="ssh user@linux.device.com
sftp user@linux.device.com"><pre><code>ssh user@linux.device.com
sftp user@linux.device.com
</code></pre></div>
<p dir="auto">Here, <code>user</code> is the Linux user and <code>linux.device.com</code> is the Linux host to
SSH into or copy the file to.</p>
<p dir="auto">If additional arguments are required, it is recommended to provide an SSH config
file. By default, both <code>ssh.exe</code> and <code>sftp.exe</code> use the file at
<code>%USERPROFILE%\.ssh\config</code> on Windows, if it exists. A possible config file
that sets a username, a port, an identity file and a known host file could look
as follows:</p>
<div data-snippet-clipboard-copy-content="Host linux_device
	HostName linux.device.com
	User user
	Port 12345
	IdentityFile C:\path\to\id_rsa
	UserKnownHostsFile C:\path\to\known_hosts"><pre><code>Host linux_device
	HostName linux.device.com
	User user
	Port 12345
	IdentityFile C:\path\to\id_rsa
	UserKnownHostsFile C:\path\to\known_hosts
</code></pre></div>
<p dir="auto">If <code>ssh.exe</code> or <code>sftp.exe</code> cannot be found, you can specify the full paths via
the command line arguments <code>--ssh-command</code> and <code>--sftp-command</code> for <code>cdc_rsync</code>
and <code>cdc_stream start</code> (see below), or set the environment variables
<code>CDC_SSH_COMMAND</code> and <code>CDC_SFTP_COMMAND</code>, e.g.</p>
<div data-snippet-clipboard-copy-content="set CDC_SSH_COMMAND=&quot;C:\path with space\to\ssh.exe&quot;
set CDC_SFTP_COMMAND=&quot;C:\path with space\to\sftp.exe&quot;"><pre><code>set CDC_SSH_COMMAND="C:\path with space\to\ssh.exe"
set CDC_SFTP_COMMAND="C:\path with space\to\sftp.exe"
</code></pre></div>
<p dir="auto">Note that you can also specify SSH configuration via the environment variables
instead of using a config file:</p>
<div data-snippet-clipboard-copy-content="set CDC_SSH_COMMAND=C:\path\to\ssh.exe -p 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
set CDC_SFTP_COMMAND=C:\path\to\sftp.exe -P 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts"><pre><code>set CDC_SSH_COMMAND=C:\path\to\ssh.exe -p 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
set CDC_SFTP_COMMAND=C:\path\to\sftp.exe -P 12345 -i C:\path\to\id_rsa -oUserKnownHostsFile=C:\path\to\known_hosts
</code></pre></div>
<p dir="auto">Note the small <code>-p</code> for <code>ssh.exe</code> and the capital <code>-P</code> for <code>sftp.exe</code>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Google Specific</h4><a id="user-content-google-specific" aria-label="Permalink: Google Specific" href="#google-specific"></a></p>
<p dir="auto">For Google internal usage, set the following environment variables to enable SSH
authentication using a Google security key:</p>
<div data-snippet-clipboard-copy-content="set CDC_SSH_COMMAND=C:\gnubby\bin\ssh.exe
set CDC_SFTP_COMMAND=C:\gnubby\bin\sftp.exe"><pre><code>set CDC_SSH_COMMAND=C:\gnubby\bin\ssh.exe
set CDC_SFTP_COMMAND=C:\gnubby\bin\sftp.exe
</code></pre></div>
<p dir="auto">Note that you will have to touch the security key multiple times during the
first run. Subsequent runs only require a single touch.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CDC RSync</h3><a id="user-content-cdc-rsync-2" aria-label="Permalink: CDC RSync" href="#cdc-rsync-2"></a></p>
<p dir="auto"><code>cdc_rsync</code> is used similar to <code>scp</code> or the Linux <code>rsync</code> command. To sync a
single Windows file <code>C:\path\to\file.txt</code> to the home directory <code>~</code> on the Linux
device <code>linux.device.com</code>, run</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\file.txt user@linux.device.com:~"><pre><code>cdc_rsync C:\path\to\file.txt user@linux.device.com:~
</code></pre></div>
<p dir="auto"><code>cdc_rsync</code> understands the usual Windows wildcards <code>*</code> and <code>?</code>.</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\*.txt user@linux.device.com:~"><pre><code>cdc_rsync C:\path\to\*.txt user@linux.device.com:~
</code></pre></div>
<p dir="auto">To sync the contents of the Windows directory <code>C:\path\to\assets</code> recursively to
<code>~/assets</code> on the Linux device, run</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -r"><pre><code>cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -r
</code></pre></div>
<p dir="auto">To get per file progress, add <code>-v</code>:</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -vr"><pre><code>cdc_rsync C:\path\to\assets\* user@linux.device.com:~/assets -vr
</code></pre></div>
<p dir="auto">The tool also supports local syncs:</p>
<div data-snippet-clipboard-copy-content="cdc_rsync C:\path\to\assets\* C:\path\to\destination -vr"><pre><code>cdc_rsync C:\path\to\assets\* C:\path\to\destination -vr
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">CDC Stream</h3><a id="user-content-cdc-stream-2" aria-label="Permalink: CDC Stream" href="#cdc-stream-2"></a></p>
<p dir="auto">To stream the Windows directory <code>C:\path\to\assets</code> to <code>~/assets</code> on the Linux
device, run</p>
<div data-snippet-clipboard-copy-content="cdc_stream start C:\path\to\assets user@linux.device.com:~/assets"><pre><code>cdc_stream start C:\path\to\assets user@linux.device.com:~/assets
</code></pre></div>
<p dir="auto">This makes all files and directories in <code>C:\path\to\assets</code> available on
<code>~/assets</code> immediately, as if it were a local copy. However, data is streamed
from Windows to Linux as files are accessed.</p>
<p dir="auto">To stop the streaming session, enter</p>
<div data-snippet-clipboard-copy-content="cdc_stream stop user@linux.device.com:~/assets"><pre><code>cdc_stream stop user@linux.device.com:~/assets
</code></pre></div>
<p dir="auto">The command also accepts wildcards. For instance,</p>

<p dir="auto">stops all existing streaming sessions for the given user.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Troubleshooting</h2><a id="user-content-troubleshooting" aria-label="Permalink: Troubleshooting" href="#troubleshooting"></a></p>
<p dir="auto">On first run, <code>cdc_stream</code> starts a background service, which does all the work.
The <code>cdc_stream start</code> and <code>cdc_stream stop</code> commands are just RPC clients that
talk to the service.</p>
<p dir="auto">The service logs to <code>%APPDATA%\cdc-file-transfer\logs</code> by default. The logs are
useful to investigate issues with asset streaming. To pass custom arguments, or
to debug the service, create a JSON config file at
<code>%APPDATA%\cdc-file-transfer\cdc_stream.json</code> with command line flags.
For instance,</p>

<p dir="auto">instructs the service to log debug messages. Try <code>cdc_stream start-service -h</code>
for a list of available flags. Alternatively, run the service manually with</p>

<p dir="auto">and pass the flags as command line arguments. When you run the service manually,
the flag <code>--log-to-stdout</code> is particularly useful as it logs to the console
instead of to the file.</p>
<p dir="auto"><code>cdc_rsync</code> always logs to the console. To increase log verbosity, pass <code>-vvv</code>
for debug logs or <code>-vvvv</code> for verbose logs.</p>
<p dir="auto">For both sync and stream, the debug logs contain all SSH and SFTP commands that
are attempted to run, which is very useful for troubleshooting. If a command
fails unexpectedly, copy it and run it in isolation. Pass <code>-vv</code> or <code>-vvv</code> for
additional debug output.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introduction to Multi-Armed Bandits (2019) (135 pts)]]></title>
            <link>https://arxiv.org/abs/1904.07272</link>
            <guid>45431271</guid>
            <pubDate>Tue, 30 Sep 2025 21:08:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/1904.07272">https://arxiv.org/abs/1904.07272</a>, See on <a href="https://news.ycombinator.com/item?id=45431271">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/1904.07272">View PDF</a>
    <a href="https://arxiv.org/html/1904.07272v8">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises.
<br>The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence.
<br>The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Aleksandrs Slivkins [<a href="https://arxiv.org/show-email/16469ce6/1904.07272" rel="nofollow">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/1904.07272v1" rel="nofollow">[v1]</a></strong>
        Mon, 15 Apr 2019 18:17:01 UTC (510 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v2" rel="nofollow">[v2]</a></strong>
        Mon, 29 Apr 2019 20:45:01 UTC (510 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v3" rel="nofollow">[v3]</a></strong>
        Tue, 25 Jun 2019 14:39:03 UTC (536 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v4" rel="nofollow">[v4]</a></strong>
        Sun, 15 Sep 2019 02:06:22 UTC (557 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v5" rel="nofollow">[v5]</a></strong>
        Mon, 30 Sep 2019 00:15:42 UTC (543 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v6" rel="nofollow">[v6]</a></strong>
        Sat, 26 Jun 2021 20:15:32 UTC (639 KB)<br>
            <strong><a href="https://arxiv.org/abs/1904.07272v7" rel="nofollow">[v7]</a></strong>
        Sat, 8 Jan 2022 20:05:40 UTC (627 KB)<br>
    <strong>[v8]</strong>
        Wed, 3 Apr 2024 21:32:42 UTC (629 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mind the encryptionroot: How to save your data when ZFS loses its mind (140 pts)]]></title>
            <link>https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/</link>
            <guid>45431167</guid>
            <pubDate>Tue, 30 Sep 2025 20:58:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/">https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/</a>, See on <a href="https://news.ycombinator.com/item?id=45431167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>
While <a href="https://en.wikipedia.org/wiki/ZFS">ZFS</a> has a well-earned reputation for data integrity and reliability, OpenZFS native encryption has some <em>incredibly</em> <a href="https://github.com/openzfs/openzfs-docs/issues/494">sharp edges</a> that will cut you if you don't know where to be careful. Unfortunately, I learned this the hard way, standing in a pool of my own blood and tears after thoroughly lacerating myself. I very nearly permanently lost 8.5 TiB of data after performing what should've been a series of simple, routine ZFS operations but resulted in an undecryptable dataset. Time has healed the wound enough that I am no longer filled with anguish just thinking about it, so I will now share my experience in the hope that you may learn from my mistakes. Together, we'll go over the unfortunate series of events that led to this happening and how it could've been avoided, learn how ZFS actually works under the hood, use our newfound knowledge to debug and reproduce the issue at hand, and finally compile a modified version of ZFS to repair the corrupted state and rescue our precious data. This is the postmortem of that terrible, horrible, no good, very bad week…</p>
<hr>
<div>
  <h2>
    Table of Contents
  </h2>
  <nav id="TableOfContents">
<ul>
<li><a href="#headline-1">Part 1: An unfortunate series of events</a>
<ul>
<li><a href="#headline-2">The status quo</a>
</li>
<li><a href="#headline-3">Encrypting the old pool</a>
</li>
<li><a href="#headline-4">Decommissioning the old pool</a>
</li>
<li><a href="#headline-5">The realization</a>
</li>
</ul>
</li>
<li><a href="#headline-6">Part 2: Debugging the issue</a>
<ul>
<li><a href="#headline-7">Learning how ZFS actually works</a>
</li>
<li><a href="#headline-8">Learning how ZFS native encryption actually works</a>
</li>
<li><a href="#headline-9">Forming a hypothesis</a>
</li>
<li><a href="#headline-10">Creating a test environment</a>
</li>
<li><a href="#headline-11">Reproducing the issue</a>
</li>
</ul>
</li>
<li><a href="#headline-12">Part 3: Recovering our data</a>
<ul>
<li><a href="#headline-13">Theoretically easy to fix</a>
</li>
<li><a href="#headline-14">Not so easy in practice</a>
</li>
<li><a href="#headline-15">Idea for a hack</a>
</li>
<li><a href="#headline-16">A brief detour into pool histories</a>
</li>
<li><a href="#headline-17">Hacking ZFS to manually create a bookmark</a>
</li>
<li><a href="#headline-18">The final obstacle</a>
</li>
<li><a href="#headline-19">The moment of truth</a>
</li>
</ul>
</li>
<li><a href="#headline-20">Conclusion</a>
</li>
</ul>
</nav>
</div>

<hr>
<p>
<strong>Note:</strong> The issue covered in this postmortem only applies to OpenZFS native encryption. Oracle ZFS has its own encryption scheme which is different and, as far as I can tell, should not be vulnerable to this particular failure mode, though I have not personally tested it. Thank you to <a href="https://old.reddit.com/r/zfs/comments/1ntwrjx/mind_the_encryptionroot_how_to_save_your_data/ngyp0wx/">u/HobartTasmania</a> for pointing this out!</p>
<div id="outline-container-headline-1">
<h2 id="headline-1">
Part 1: An unfortunate series of events
</h2>
<div id="outline-text-headline-1">
<div id="outline-container-headline-2">
<h3 id="headline-2">
The status quo
</h3>
<div id="outline-text-headline-2">
<p>
In the beginning, there were two ZFS pools: <code>old</code> and <code>new</code> (names changed for clarity). Each pool was hosted on an instance of TrueNAS CORE 13.0-U5.1 located at two different sites about an hour's drive apart with poor Internet connectivity between them. For this reason, a third pool <a href="https://en.wikipedia.org/wiki/Sneakernet"><code>sneakernet</code></a> was periodically moved between the two sites and used to exchange snapshots of <code>old</code> and <code>new</code> datasets for backup purposes. ZFS dataset <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-snapshot.8.html">snapshots</a> would be indirectly relayed from <code>old</code> to <code>new</code> (and vice versa) using <code>sneakernet</code> as an intermediate ZFS <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-send.8.html">send</a>/<a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-recv.8.html">recv</a> source/destination (e.g. <code>old/foo@2023-06-01</code> -&gt; <code>sneakernet/old/foo@2023-06-01</code> -&gt; <code>new/old/foo@2023-06-01</code>).</p>
<p>
The <code>new</code> pool was natively encrypted from the very beginning. When ZFS snapshots were sent from <code>new</code> to <code>sneakernet/new</code> to <code>old/new</code>, they were sent <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-send.8.html#w">raw</a>, meaning that blocks were copied unmodified in their encrypted form. To decrypt and mount them on <code>sneakernet</code> or <code>old</code>, you would need to first load <code>new</code>'s <a href="https://openzfs.github.io/openzfs-docs/man/master/7/zfsprops.7.html#keyformat">hex</a> encryption key, which is stored in TrueNAS's SQLite database.</p>
<p>
The <code>old</code> pool, on the other hand, was created before the advent of native encryption and was unencrypted for the first part of its life. Because it's desirable to encrypt data at rest, an encrypted dataset <code>sneakernet/old</code> was created for <code>old</code> using a <a href="https://openzfs.github.io/openzfs-docs/man/master/7/zfsprops.7.html#keyformat">passphrase</a> encryption key when <code>sneakernet</code> was set up. Unencrypted snapshots were sent non-raw from <code>old</code> to <code>sneakernet/old</code>, where they were encrypted, and then sent raw from <code>sneakernet/old</code> to <code>new/old</code>. To decrypt and mount them on <code>sneakernet</code> or <code>new</code>, you would need to first load <code>sneakernet</code>'s passphrase encryption key.</p>
<p>
This was all tested thoroughly and snapshots were proven to be readable at each point on every pool.</p>
</div>
</div>
<div id="outline-container-headline-3">
<h3 id="headline-3">
Encrypting the old pool
</h3>
<div id="outline-text-headline-3">
<p>
Now that we had encrypted snapshots of <code>old</code> on <code>sneakernet/old</code>, we wanted to encrypt <code>old</code> itself. To do this, I simply took <code>old</code> offline during a maintenance window to prevent new writes, took snapshots of all datasets, sent them to <code>sneakernet/old</code>, and then sent the raw encrypted snapshots from <code>sneakernet/old</code> back to <code>old/encrypted</code>. Once I verified each dataset had been encrypted successfully, I destroyed the unencrypted dataset, updated the mount point of the encrypted dataset to that of the late unencrypted dataset, and then moved on to the next dataset. After all datasets were migrated, I used <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-change-key.8.html"><code>zfs change-key -i</code></a> to make all child datasets inherit from the new <code>old/encrypted</code> encryption root, and then changed the key of the encryption root from a passphrase to a hex key, since TrueNAS only supported automatically unlocking datasets with hex encryption keys. Finally, I issued a <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zpool-initialize.8.html"><code>zpool initialize</code></a> to overwrite all the unencrypted blocks which were now in unallocated space.</p>
<p>
<strong>Spoiler Alert:</strong> It may not be immediately obvious why, but changing the encryption key on <code>old/encryption</code> silently broke backups of <code>old</code> datasets. Snapshots would still send and recv successfully, but were no longer decryptable or mountable. Since the encryption key is not normally loaded, and we only load it when periodically testing the backups, we would not realize until it was too late.</p>
<p>
<strong>Lesson: Test backups continuously so you get immediate feedback when they break.</strong></p>
</div>
</div>
<div id="outline-container-headline-4">
<h3 id="headline-4">
Decommissioning the old pool
</h3>
<div id="outline-text-headline-4">
<p>
Later, the <code>old</code> pool was moved to the same site as the <code>new</code> pool, so we wanted to fully decommission <code>old</code> and migrate all its datasets to <code>new</code>. I began going about this in a similar way. I took <code>old</code> offline to prevent new writes, sent snapshots to <code>sneakernet/old</code>, and then to <code>new/old</code>. It was at this point that I made a very unfortunate mistake: I accidentally destroyed one dataset <code>old/encrypted/foo</code> <em>before</em> verifying the files were readable on <code>new/old/foo</code>, and I would soon realize that they were not.</p>
<p>
<strong>Lesson: Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.</strong></p>
</div>
</div>
<div id="outline-container-headline-5">
<h3 id="headline-5">
The realization
</h3>
<div id="outline-text-headline-5">
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>[</span>sam@newnas ~<span>]</span>$ <span>DATASET</span><span>=</span>foo<span>;</span> <span>[[</span> <span>$(</span>ssh sam@oldnas zfs list -H -o guid old/encrypted/<span>${</span><span>DATASET</span><span>}</span>@decomm<span>)</span> <span>=</span> <span>$(</span>zfs list -H -o guid sneakernet/old/<span>${</span><span>DATASET</span><span>}</span>@decomm<span>)</span> <span>]]</span> <span>&amp;&amp;</span> <span>echo</span> <span>"GUIDs match"</span> <span>||</span> <span>echo</span> <span>"GUIDs DO NOT MATCH"</span>
</span></span><span><span>GUIDs match
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ <span>DATASET</span><span>=</span>foo<span>;</span> <span>[[</span> <span>$(</span>zfs list -H -o guid sneakernet/old/<span>${</span><span>DATASET</span><span>}</span>@decomm<span>)</span> <span>=</span> <span>$(</span>zfs list -H -o guid new/old/<span>${</span><span>DATASET</span><span>}</span>@decomm<span>)</span> <span>]]</span> <span>&amp;&amp;</span> <span>echo</span> <span>"GUIDs match"</span> <span>||</span> <span>echo</span> <span>"GUIDs DO NOT MATCH"</span>
</span></span><span><span>GUIDs match
</span></span><span><span>
</span></span><span><span><span>[</span>sam@oldnas ~<span>]</span>$ sudo zfs destroy -r old/encrypted/foo
</span></span><span><span>
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ ls /mnt/new/old/foo
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ ls -a /mnt/new/old/foo
</span></span><span><span>. ..
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ zfs list -o name,mounted new/old/foo
</span></span><span><span>NAME         MOUNTED
</span></span><span><span>new/old/foo  no
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ sudo zfs mount new/old/foo
</span></span><span><span>cannot mount <span>'new/old/foo'</span>: Permission denied</span></span></code></pre></div>
<p>
What do you mean, permission denied? I am root!</p>
<p>
Crap, I already destroyed <code>old/encrypted/foo</code>. This is not good, but I can still restore it from the remaining copy on <code>sneakernet/old/foo</code>.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span><span>[</span>sam@newnas ~<span>]</span>$ sudo zfs load-key sneakernet/old
</span></span><span><span>Enter passphrase <span>for</span> <span>'sneakernet/old'</span>:
</span></span><span><span><span>[</span>sam@newnas ~<span>]</span>$ sudo zfs mount sneakernet/old/foo
</span></span><span><span>cannot mount <span>'sneakernet/old/foo'</span>: Permission denied</span></span></code></pre></div>
<p>
Oh no, <code>sneakernet/old</code> is broken too. This is <em>very not good</em>!</p>
<p>
In an act of desperation, I tried rebooting the machine, but it didn't change a thing.</p>
<p>
It is at this point that I realized:</p>
<ol>
<li>Something has gone terribly wrong to prevent datasets on both <code>sneakernet/old</code> and <code>new/old</code> from mounting.</li>
<li>Whatever it is, it's not likely going to be easy to diagnose or fix.</li>
<li>There's a very real possibility the data might be gone forever.</li>
</ol>
<p>I found myself in a hole and I wanted to stop digging. Fortunately, uptime was no longer critical for the <code>old</code> datasets after the relocation, so I could afford to step away from the keyboard, collect my thoughts, and avoid making the situation any worse that it already was.</p>
<hr>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-6">
<h2 id="headline-6">
Part 2: Debugging the issue
</h2>
<div id="outline-text-headline-6">
<p>
Once the worst of the overwhelming, visceral feelings that come with the realization that you may have just caused permanent data loss had subsided, I started to work the incident and try to figure out why the backups aren't mounting.</p>
<p>
As a precaution, I first <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zpool-export.8.html">exported</a> the <code>old</code> pool and took a forensic image of every disk in the pool. ZFS is a copy-on-write filesystem, so even though the dataset had been destroyed, most of the data was probably still on disk, just completely inaccessible with the normal ZFS tooling. In the worst case scenario, I may have had to try to forensically reconstruct the dataset from what was left on disk, and I didn't want to risk causing any more damage than I already had. Fortunately, I never had to use the disk images, but they still served as a valuable safety net while debugging and repairing.</p>
<p>
Next, I realized that if we are to have any chance of debugging and fixing this issue, I need to learn how ZFS actually works.</p>
<div id="outline-container-headline-7">
<h3 id="headline-7">
Learning how ZFS actually works
</h3>
<div id="outline-text-headline-7">
<p>
I unfortunately did not keep track of every resource I consumed, but in addition to reading the <a href="https://github.com/openzfs/zfs">source</a> and <a href="https://openzfs.github.io/openzfs-docs/">docs</a>, I found these talks by Jeff Bonwick, Bill Moore, and Matt Ahrens (the original creators of ZFS) to be particularly helpful in understanding the design and implementation of ZFS:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=NRoUC9P1PmA">ZFS: The Last Word in File Systems Part 1</a></li>
<li><a href="https://www.youtube.com/watch?v=TwCXVp_u86o">ZFS: The Last Word in File Systems Part 2</a></li>
<li><a href="https://www.youtube.com/watch?v=ybjdAEUfXzw">ZFS: The Last Word in File Systems Part 3</a></li>
<li><a href="https://www.youtube.com/watch?v=NXg86uBDSqI">How ZFS Snapshots Really Work</a></li>
</ul>
<p>I <em>highly</em> recommend watching them all despite their age and somewhat poor recording quality, but will summarize the relevant information for those who don't have 3 hours to spare.</p>
<p>
ZFS is a <a href="https://en.wikipedia.org/wiki/Copy-on-write#In_computer_storage">copy-on-write</a> filesystem, which means that it does not overwrite blocks in place when a write is requested. Instead, the updated contents are written to a newly allocated block, and the old block is freed, which keeps the filesystem consistent if a write is interrupted. All blocks of both data and metadata are arranged in a <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a> structure where each block pointer contains a checksum of the child block, which allows ZFS to detect both block corruption and misdirected/phantom reads/writes. This means that any write will cause the block's checksum to change, which will then cause the parent block's checksum to change (since the parent block includes the block pointer which includes checksum of the child block that changed), and so on, all the way up to the root of the tree which ZFS calls an uberblock.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_1.png" alt="zfs_1.png" title="zfs_1.png"></p>
<p>
Uberblocks are written atomically, and because of the Merkle tree structure, they always represent a consistent snapshot of the entire filesystem at a point in time. Writes are batched together into <a href="https://ahl.dtrace.org/2012/12/13/zfs-fundamentals-transaction-groups/">transaction groups</a> identified by a monotonically increasing counter, and each transaction group when synced to disk produces a new uberblock and associated filesystem tree. Taking a snapshot is then as simple as saving an uberblock and not freeing any of the blocks it points to.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_2.png" alt="zfs_2.png" title="zfs_2.png"></p>
<p>
In addition to the checksum, each block pointer also contains the transaction group id in which the child block was written, which is called the block's birth time or creation time. ZFS uses birth times to determine which blocks have been written before or after a snapshot. Any blocks with a birth time less than or equal to the snapshot's birth time, must have been written before the snapshot was taken, and conversely, any blocks with a birth time greater than the snapshot's birth time must have been written after the snapshot was taken.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_3.png" alt="zfs_3.png" title="zfs_3.png"></p>
<p>
One application of birth times is to generate incremental send streams between two snapshots. ZFS walks the tree but only needs to include blocks where the birth time is both greater than the first snapshot and less than or equal to the second snapshot. In fact, you don't even need to keep the data of the first snapshot around—you can create a <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-bookmark.8.html">bookmark</a> which saves the snapshot's transaction id (but none of the data blocks), delete the snapshot to free its data, and then use the bookmark as the source to generate the same incremental send stream.</p>
<p>
<strong>Spoiler Alert:</strong> <a href="https://en.wikipedia.org/wiki/Chekhov%27s_gun">Chekhov's</a> bookmark will become relevant later.</p>
</div>
</div>
<div id="outline-container-headline-8">
<h3 id="headline-8">
Learning how ZFS native encryption actually works
</h3>
<div id="outline-text-headline-8">
<p>
ZFS native encryption is a relatively new feature, which was first released in <a href="https://github.com/openzfs/zfs/releases/tag/zfs-0.8.0">OpenZFS 0.8.0</a> (2019) and subsequently made it into <a href="https://www.freebsd.org/releases/13.0R/relnotes/">FreeBSD 13.0</a> (2021) when OpenZFS was adopted.</p>
<p>
In addition to the <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-load-key.8.html#Encryption">docs</a>, I found this 2016 talk on <a href="https://www.youtube.com/watch?v=frnLiXclAMo">ZFS Native Encryption</a> by Tom Caputi (the original author of native encryption) to be helpful in understanding its design and implementation. Again, I will summarize the relevant information.</p>
<p>
ZFS native encryption works by encrypting dataset blocks with an symmetric authenticated encryption cipher suite (AES-256-GCM by default). To use native encryption, you must create a new dataset with <code>-o encryption=on</code> which generates a unique master key for the dataset. The dataset's master key is then used to derive block data encryption keys with a salted HKDF.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_4.png" alt="zfs_4.png" title="zfs_4.png"></p>
<p>
The master key can't be changed, so it is encrypted with a wrapping key which can be changed. The wrapping key is provided by the user with <code>zfs load-key</code> and can be changed with <code>zfs change-key</code> which re-encrypts the same master key with a new wrapping key.</p>
<p>
<img src="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/zfs_5.png" alt="zfs_5.png" title="zfs_5.png"></p>
<p>
The encrypted master keys are stored in each dataset since each dataset has its own master key, but the wrapping key parameters are stored on what is called the encryption root dataset. The encryption root may be the same encrypted dataset, or it may be a parent of the encrypted dataset. When a child encrypted dataset inherits from a parent encryption root, the encryption root's wrapping key is used to decrypt the child dataset's master key. This is how one key can be used to unlock a parent encryption root dataset and all child encrypted datasets that inherit from it at the same time instead of having to load a key for every single encrypted dataset.</p>
<p>
In our case, <code>new</code>, <code>sneakernet/new</code>, <code>sneakernet/old</code>, and <code>old/encrypted</code> are the encryption roots, and all child encrypted datasets inherit from them.</p>
</div>
</div>
<div id="outline-container-headline-9">
<h3 id="headline-9">
Forming a hypothesis
</h3>
<div id="outline-text-headline-9">
<p>
At this point, we now know enough to form a hypothesis as to what may have happened. Feel free to pause here and try to figure it out on your own.</p>
<p>
Recall that <code>sneakernet/old</code> was created using a passphrase encryption key, and <code>old/encrypted</code> was created by raw sending <code>sneakernet/old</code>, so it initially used the same passphrase derived wrapping encryption key. When the <code>old/encrypted</code> encryption key was changed from a passphrase to a hex key, ZFS must have changed the wrapping key parameters on the <code>old/encrypted</code> encryption root and re-encrypted all child encrypted dataset master keys with the new hex wrapping key. Crucially, a new snapshot of <code>old/encrypted</code> was never taken and sent to <code>sneakernet/old</code> because it ostensibly didn't contain any data and was just a container for the child datasets.</p>
<p>
<strong>Hypothesis:</strong> When subsequent snapshots were sent from <code>old</code> to <code>sneakernet</code>, the master keys of the child encrypted datasets were updated to be encrypted with the new hex wrapping key, but the <code>sneakernet/old</code> encryption root was never updated with the new hex wrapping key parameters because a new snapshot was never sent. Therefore, when we load the key for <code>sneakernet/old</code>, ZFS asks for the old passphrase, not a hex key, and when we try to mount <code>sneakernet/old/foo</code>, it tries and fails to decrypt its master key with the old passphrase wrapping key instead of the new hex wrapping key.</p>
<p>
If correct, this would explain the behavior we're seeing. To test this hypothesis, let's try to reproduce the issue in a test environment.</p>
</div>
</div>
<div id="outline-container-headline-10">
<h3 id="headline-10">
Creating a test environment
</h3>
<div id="outline-text-headline-10">
<p>
TrueNAS CORE 13.0-U5.1 is based on FreeBSD 13.1, despite the different minor version numbers, so we'll create a FreeBSD 13.1 VM to test in. Make sure to include the system source tree and install on UFS so that we can build OpenZFS and reload the ZFS kernel module without rebooting.</p>
<p>
TrueNAS CORE 13.0-U5.1 uses ZFS 2.1.11, so we'll want to build the same version from source for consistency. I started by reading the <a href="https://openzfs.github.io/openzfs-docs/Developer%20Resources/Building%20ZFS.html">Building ZFS</a> guide and following the steps documented there with some small modifications for FreeBSD since the page was clearly written with Linux in mind.</p>
<p>
First, install the dependencies we'll need.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo pkg install autoconf automake autotools git gmake python devel/py-sysctl sudo</span></span></code></pre></div>
<p>
Then, clone ZFS and check out tag zfs-2.1.11.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ git clone https://github.com/openzfs/zfs
</span></span><span><span>sam@zfshax:~ $ <span>cd</span> zfs
</span></span><span><span>sam@zfshax:~/zfs $ git checkout zfs-2.1.11
</span></span><span><span>sam@zfshax:~/zfs $ git show --summary
</span></span><span><span>commit e25f9131d679692704c11dc0c1df6d4585b70c35 <span>(</span>HEAD, tag: zfs-2.1.11<span>)</span>
</span></span><span><span>Author: Tony Hutter &lt;hutter2@llnl.gov&gt;
</span></span><span><span>Date:   Tue Apr <span>18</span> 11:44:34 <span>2023</span> -0700
</span></span><span><span>
</span></span><span><span>    Tag zfs-2.1.11
</span></span><span><span>
</span></span><span><span>    META file and changelog updated.
</span></span><span><span>
</span></span><span><span>    Signed-off-by: Tony Hutter &lt;hutter2@llnl.gov&gt;</span></span></code></pre></div>
<p>
Now, configure, build, and install ZFS.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sh autogen.sh
</span></span><span><span>sam@zfshax:~/zfs $ ./configure
</span></span><span><span>sam@zfshax:~/zfs $ gmake -s -j<span>$(</span>sysctl -n hw.ncpu<span>)</span>    <span># &lt;-- modified for FreeBSD</span>
</span></span><span><span>sam@zfshax:~/zfs $ sudo gmake install<span>;</span> sudo ldconfig  <span># &lt;-- modified for FreeBSD</span></span></span></code></pre></div>
<p>
Then, replace the FreeBSD's ZFS kernel module with the one we just built.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sudo kldunload zfs.ko  <span># Needed because zfs.sh only unloads openzfs.ko</span>
</span></span><span><span>sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh</span></span></code></pre></div>
<p>
Finally, verify we're running version 2.1.11 as desired.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sudo zfs version
</span></span><span><span>zfs-2.1.11-1
</span></span><span><span>zfs-kmod-2.1.11-1</span></span></code></pre></div>
</div>
</div>
<div id="outline-container-headline-11">
<h3 id="headline-11">
Reproducing the issue
</h3>
<div id="outline-text-headline-11">
<p>
Now we're ready to try reproducing the issue. This took some iteration to get right, so I wrote a bash script that starts from scratch on each invocation and then runs the commands needed to reproduce the corrupt state. After quite a bit of trial and error, I eventually produced a <a href="https://sambowman.tech/blog/posts/mind-the-encryptionroot-how-to-save-your-data-when-zfs-loses-its-mind/reproduce.sh">reproducer script</a> which does the following:</p>
<ol>
<li>Create 2 pools: <code>src</code> and <code>dst</code>.</li>
<li>Create <code>src/encryptionroot</code> using a passphrase encryption key.</li>
<li>Create <code>src/encryptionroot/child</code> which inherits <code>src/encryptionroot</code> as its encryption root.</li>
<li>Create files and take snapshots <code>src/encryptionroot@111</code> and <code>src/encryptionroot/child@111</code>.</li>
<li>Send raw snapshots <code>src/encryptionroot@111</code> and <code>src/encryptionroot/child@111</code> to <code>dst/encryptionroot</code> and <code>dst/encryptionroot/child</code> respectively.</li>
<li>Load encryption key for <code>dst/encryptionroot</code> using passphrase and mount encrypted datasets <code>dst/encryptionroot</code> and <code>dst/encryptionroot/child</code>. At this point, <code>src</code> and <code>dst</code> pools are in sync.</li>
<li>Change the <code>src/encryptionroot</code> encryption key from passphrase to hex.</li>
<li>Update files and take snapshots <code>src/encryptionroot@222</code> and <code>src/encryptionroot/child@222</code>.</li>
<li>Send a raw incremental snapshot of <code>src/encryptionroot/child@222</code> to <code>dst/encryptionroot/child</code>, but do <em>not</em> send <code>src/encryptionroot@222</code> which contains the key change!</li>
<li>Unmount <code>dst/encryptionroot</code> and <code>dst/encryptionroot/child</code> and unload the cached encryption key for <code>dst/encryptionroot</code>.</li>
<li>Load the encryption key for <code>dst/encryptionroot</code> using the passphrase since we didn't send the updated encryption root after changing the key.</li>
<li>Try to remount <code>dst/encryptionroot</code> and <code>dst/encryptionroot/child</code>.</li>
</ol>
<p>When we run the reproducer, the root encrypted dataset <code>dst/encryptionroot</code> mounts successfully and we can read the old file from the first snapshot, but the child encrypted dataset <code>dst/encryptionroot/child</code> fails to mount with <code>cannot mount 'dst/encryptionroot/child: Permission denied</code> just as we expected.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo ./reproduce &gt; /dev/null 2&gt;<span>&amp;</span><span>1</span>
</span></span><span><span>sam@zfshax:~ $ sudo zfs mount dst/encryptionroot/child
</span></span><span><span>cannot mount <span>'dst/encryptionroot/child'</span>: Permission denied</span></span></code></pre></div>



<details>
    <summary title="Click to interact"><span>Full reproducer script output (long!)</span></summary>
    <div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo ./reproduce
</span></span><span><span>
</span></span><span><span>Destroy pools and backing files <span>if</span> they exist.
</span></span><span><span>+ zpool destroy src
</span></span><span><span>+ zpool destroy dst
</span></span><span><span>+ rm -f /src.img
</span></span><span><span>+ rm -f /dst.img
</span></span><span><span>
</span></span><span><span>Create pools using sparse files.
</span></span><span><span>+ truncate -s 100M /src.img
</span></span><span><span>+ truncate -s 100M /dst.img
</span></span><span><span>+ zpool create -o <span>ashift</span><span>=</span><span>12</span> -m /src src /src.img
</span></span><span><span>+ zpool create -o <span>ashift</span><span>=</span><span>12</span> -m /dst dst /dst.img
</span></span><span><span>
</span></span><span><span>Create root encrypted dataset using a passphrase encryption key.
</span></span><span><span>+ <span>echo</span> <span>'hunter2!'</span>
</span></span><span><span>+ zfs create -o <span>encryption</span><span>=</span>on -o <span>keyformat</span><span>=</span>passphrase -o <span>keylocation</span><span>=</span>prompt src/encryptionroot
</span></span><span><span>
</span></span><span><span>Create child encrypted dataset which inherits src/encryptionroot as its encryption root.
</span></span><span><span>+ zfs create src/encryptionroot/child
</span></span><span><span>
</span></span><span><span>Create files in the root and child encrypted datasets and snapshot both.
</span></span><span><span>+ touch /src/encryptionroot/111
</span></span><span><span>+ touch /src/encryptionroot/child/111
</span></span><span><span>+ zfs snapshot -r src/encryptionroot@111
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>1</span> <span>]</span> Files and snapshots are on the src pool but not the dst pool yet.
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  passphrase  available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>NAME  ENCROOT  KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst   -        none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>111</span>
</span></span><span><span>/dst
</span></span><span><span>
</span></span><span><span>Send a raw replication stream of the src snapshots to the dst pool.
</span></span><span><span>+ zfs send --replicate --raw src/encryptionroot@111
</span></span><span><span>+ zfs recv dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Load encryption key <span>for</span> the dst encryption root using passphrase and mount the encrypted datasets.
</span></span><span><span>+ <span>echo</span> <span>'hunter2!'</span>
</span></span><span><span>+ zfs load-key dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot/child
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>2</span> <span>]</span> Files and snapshots are on both pools and in sync.
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  passphrase  available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  passphrase  available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      <span>3076413147413645477</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      <span>18246034838646533510</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>111</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>111</span>
</span></span><span><span>
</span></span><span><span>Change the src encryption root key from passphrase to hex.
</span></span><span><span>+ <span>echo</span> 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
</span></span><span><span>+ zfs change-key -o <span>keyformat</span><span>=</span>hex src/encryptionroot
</span></span><span><span>
</span></span><span><span>Update the files in the root and child encrypted datasets and snapshot both.
</span></span><span><span>+ mv /src/encryptionroot/111 /src/encryptionroot/222
</span></span><span><span>+ mv /src/encryptionroot/child/111 /src/encryptionroot/child/222
</span></span><span><span>+ zfs snapshot -r src/encryptionroot@222
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>3</span> <span>]</span> Updated files and snapshots are on the src pool but not the dst pool yet.
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>929742392566496732</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      <span>3076413147413645477</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  passphrase  available    yes      <span>18246034838646533510</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>111</span>
</span></span><span><span>
</span></span><span><span>Send a raw incremental snapshot of the child encrypted dataset to the dst pool.
</span></span><span><span>+ zfs send --raw -i src/encryptionroot/child@111 src/encryptionroot/child@222
</span></span><span><span>+ zfs recv -F dst/encryptionroot/child
</span></span><span><span>
</span></span><span><span>NOTE: The encryption key change on the src encryption root has not been sent to dst!
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>4</span> <span>]</span> File is updated in the dst child encrypted dataset but not the dst root encrypted dataset.
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>929742392566496732</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      <span>3076413147413645477</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      <span>18246034838646533510</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>
</span></span><span><span>NOTE: The updated file in the dst child encrypted dataset is only still readable because the encryption key is still loaded from before sending the snapshot taken after the key change.
</span></span><span><span>
</span></span><span><span>Unmount the dst encrypted datasets and and unload the cached encryption key.
</span></span><span><span>+ zfs unmount dst/encryptionroot
</span></span><span><span>+ zfs unload-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Load the encryption key <span>for</span> the dst encryption root using the passphrase since we did not send the updated encryption root after changing the key.
</span></span><span><span>+ <span>echo</span> <span>'hunter2!'</span>
</span></span><span><span>+ zfs load-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Try to remount dst encrypted datasets.
</span></span><span><span>+ zfs mount dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot/child
</span></span><span><span>cannot mount <span>'dst/encryptionroot/child'</span>: Permission denied
</span></span><span><span>+ <span>true</span>
</span></span><span><span>
</span></span><span><span><span>[</span> Checkpoint <span>5</span> <span>]</span> Mounting dst child encrypted dataset failed even though encryption key is ostensibly available. Hypothesis confirmed!
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1354282934008960312</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>12828913232342655944</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>929742392566496732</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>10447093816713688124</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>5247064584420489120</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  passphrase  available    yes      <span>3076413147413645477</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>14453618123048176778</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    no       <span>18246034838646533510</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>10173467213034806911</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>8161419639883744346</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>111</span>
</span></span><span><span>    └── child</span></span></code></pre></div></details>

<p>
Now that we understand and can reliably reproduce the issue, we're a big step closer to fixing it!</p>
<hr>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-12">
<h2 id="headline-12">
Part 3: Recovering our data
</h2>
<div id="outline-text-headline-12">
<div id="outline-container-headline-13">
<h3 id="headline-13">
Theoretically easy to fix
</h3>
<div id="outline-text-headline-13">
<p>
We know now that a child encrypted dataset will become unmountable if the following conditions are met:</p>
<ol>
<li>The wrapping encryption key on the encryption root is changed.</li>
<li>A snapshot of the child encrypted dataset that was taken after the key change is sent.</li>
<li>A snapshot of the encryption root that was taken after the key change is <em>not</em> sent.</li>
</ol>
<p><strong>Lesson: Always send a snapshot of the encryption root after changing the encryption key.</strong></p>
<p>
In theory, all we should have to do to fix it is send the latest snapshot of the encryption root.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo ./reproduce &gt; /dev/null 2&gt;<span>&amp;</span><span>1</span>
</span></span><span><span>sam@zfshax:~ $ sudo ./repair_snapshot
</span></span><span><span>
</span></span><span><span>HYPOTHESIS: The child encrypted dataset should become decryptable again <span>if</span> a snapshot containing the key change on the root encrypted dataset is sent.
</span></span><span><span>
</span></span><span><span>Send a raw incremental snapshot of the root encrypted dataset to the dst pool.
</span></span><span><span>+ zfs send --raw -i src/encryptionroot@111 src/encryptionroot@222
</span></span><span><span>+ zfs recv -F dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Unmount the dst encrypted datasets and and unload the cached encryption key.
</span></span><span><span>+ zfs unmount dst/encryptionroot
</span></span><span><span>+ zfs unload-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Load the encryption key <span>for</span> the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
</span></span><span><span>+ <span>echo</span> 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
</span></span><span><span>+ zfs load-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Try to remount dst encrypted datasets.
</span></span><span><span>+ zfs mount dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot/child
</span></span><span><span>
</span></span><span><span>RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>3822096046979704342</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>10687499872806328230</span>
</span></span><span><span>src/encryptionroot@111        src/encryptionroot  -           available    -        <span>16650389156603898046</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>157927145464667221</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>15788284772663365294</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>8879828033920251704</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>6286619359795670820</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>1835983340793043086</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  hex         available    yes      <span>6911130245015256647</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>16650389156603898046</span>
</span></span><span><span>dst/encryptionroot@222        dst/encryptionroot  -           available    -        <span>157927145464667221</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      <span>15804809318195285947</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>8879828033920251704</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>6286619359795670820</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span></span></span></code></pre></div>
</div>
</div>
<div id="outline-container-headline-14">
<h3 id="headline-14">
Not so easy in practice
</h3>
<div id="outline-text-headline-14">
<p>
Unfortunately, this isn't enough to fix <code>new</code> and <code>sneakernet</code>; there are no remaining snapshots or bookmarks left on the <code>old</code> encryption root from before the key change, and we can't generate an incremental send stream without one. Mapped to our reproduced example, this means that <code>src/encryptionroot@111</code> does not exist.</p>
<p>
You might think we could forcibly send the entire encryption root, but <code>zfs recv</code> will reject it no matter what you do.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 <span>|</span> sudo zfs recv dst/encryptionroot
</span></span><span><span>cannot receive new filesystem stream: destination <span>'dst/encryptionroot'</span> exists
</span></span><span><span>must specify -F to overwrite it
</span></span><span><span>
</span></span><span><span>sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 <span>|</span> sudo zfs recv -F dst/encryptionroot
</span></span><span><span>cannot receive new filesystem stream: destination has snapshots <span>(</span>eg. dst/encryptionroot@111<span>)</span>
</span></span><span><span>must destroy them to overwrite it
</span></span><span><span>
</span></span><span><span>sam@zfshax:~ $ sudo zfs destroy dst/encryptionroot@111
</span></span><span><span>sam@zfshax:~ $ sudo zfs send --raw src/encryptionroot@222 <span>|</span> sudo zfs recv -F dst/encryptionroot
</span></span><span><span>cannot receive new filesystem stream: zfs receive -F cannot be used to destroy an encrypted filesystem or overwrite an unencrypted one with an encrypted one</span></span></code></pre></div>
<p>
<strong>Lesson: Create bookmarks before destroying snapshots.</strong></p>
<p>
We need to find a way to create an incremental send stream that contains the key change, but how?. We could try to manually craft a send stream containing the new key, but that sounds tricky. There's got to be a better way!</p>
</div>
</div>
<div id="outline-container-headline-15">
<h3 id="headline-15">
Idea for a hack
</h3>
<div id="outline-text-headline-15">
<p>
Recall that a snapshot is not the only valid source for generating an incremental send stream. What if we had a bookmark?</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo ./reproduce &gt; /dev/null 2&gt;<span>&amp;</span><span>1</span>
</span></span><span><span>sam@zfshax:~ $ sudo ./repair_bookmark
</span></span><span><span>
</span></span><span><span>Replace the initial parent encrypted dataset snapshot with a bookmark.
</span></span><span><span>+ zfs bookmark src/encryptionroot@111 src/encryptionroot#111
</span></span><span><span>+ zfs destroy src/encryptionroot@111
</span></span><span><span>
</span></span><span><span>HYPOTHESIS: The child encrypted dataset should become decryptable again <span>if</span> a snapshot containing the key change on the root encrypted dataset is sent.
</span></span><span><span>
</span></span><span><span>Send a raw incremental snapshot of the root encrypted dataset to the dst pool using the bookmark.
</span></span><span><span>+ zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222
</span></span><span><span>+ zfs recv -F dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Unmount the dst encrypted datasets and and unload the cached encryption key.
</span></span><span><span>+ zfs unmount dst/encryptionroot
</span></span><span><span>+ zfs unload-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Load the encryption key <span>for</span> the dst encryption root using the hex key since we have now sent the updated encryption root after changing the key.
</span></span><span><span>+ <span>echo</span> 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef
</span></span><span><span>+ zfs load-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>Try to remount dst encrypted datasets.
</span></span><span><span>+ zfs mount dst/encryptionroot
</span></span><span><span>+ zfs mount dst/encryptionroot/child
</span></span><span><span>
</span></span><span><span>RESULT: Child encrypted dataset is decryptable again. Hypothesis confirmed!
</span></span><span><span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>src                           -                   none        -            yes      <span>1018261135296547862</span>
</span></span><span><span>src/encryptionroot            src/encryptionroot  hex         available    yes      <span>1985286651877572312</span>
</span></span><span><span>src/encryptionroot@222        src/encryptionroot  -           available    -        <span>4582898506955533479</span>
</span></span><span><span>src/encryptionroot#111        -                   -           -            -        <span>4964628655505655411</span>
</span></span><span><span>src/encryptionroot/child      src/encryptionroot  hex         available    yes      <span>12927592016081051429</span>
</span></span><span><span>src/encryptionroot/child@111  src/encryptionroot  -           available    -        <span>15551239789901400488</span>
</span></span><span><span>src/encryptionroot/child@222  src/encryptionroot  -           available    -        <span>11729357375613972731</span>
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>15258247229701443799</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  hex         available    yes      <span>17755083343181277380</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>4964628655505655411</span>
</span></span><span><span>dst/encryptionroot@222        dst/encryptionroot  -           available    -        <span>4582898506955533479</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      <span>364333975888407846</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>15551239789901400488</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>11729357375613972731</span>
</span></span><span><span>/src
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span>
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span></span></span></code></pre></div>
<p>
A bookmark works just as well as a snapshot for generating an incremental send stream, but we don't have a bookmark on <code>old</code> either. How is this any better?</p>
<p>
Unlike a snapshot, which is effectively an entire dataset tree frozen in time (very complex), a bookmark is a very simple object on disk which consists of:</p>
<ol>
<li>The GUID of the snapshot.</li>
<li>The transaction group the snapshot was created in.</li>
<li>The Unix timestamp when the snapshot was created.</li>
</ol>
<p>For example, this is what our bookmark looks like in <code>zdb</code>:</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zdb src/encryptionroot#111
</span></span><span><span>	<span>#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}</span></span></span></code></pre></div>
<p>
Note that <code>zdb</code> shows the GUID in hexadecimal versus <code>zfs get guid</code> which shows it in decimal, consistency be damned. The <code>redaction_obj</code> is optional and only used for <a href="https://openzfs.github.io/openzfs-docs/man/master/8/zfs-redact.8.html#redaction">redaction bookmarks</a>, so we can ignore it.</p>
<p>
A bookmark is simple enough that we could feasibly hack ZFS into manually writing one for us, provided that we can figure out the right values to use. The GUID and Unix timestamp don't really matter for generating an incremental send stream, so we could choose them arbitrarily if we had to, but the transaction group id really matters because that is what ZFS uses to determine which blocks to include.</p>
<p>
But how can we figure out what transaction group the snapshot was created in if neither the snapshot nor a bookmark of the snapshot still exist? I initially considered walking the dataset trees on each pool, diffing them to find the newest block present on both datasets, and using its transaction group id, but I found a much easier way with one of ZFS's lesser known features.</p>
</div>
</div>
<div id="outline-container-headline-16">
<h3 id="headline-16">
A brief detour into pool histories
</h3>
<div id="outline-text-headline-16">
<p>
I didn't know about <a href="https://docs.freebsd.org/en/books/handbook/zfs/#zfs-zpool-history">pool histories</a> before embarking on this unplanned journey, but they are now yet another thing I love about ZFS. Every pool allocates <a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/spa_history.c#L108-L111">0.1% of its space (128 KiB minimum, 1 GiB maximum)</a> to a ring buffer which is used to log every command that is executed on the pool. This can be used to forensically reconstruct the state of the pool over time.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zpool <span>history</span>
</span></span><span><span>History <span>for</span> <span>'dst'</span>:
</span></span><span><span>2025-09-01.00:00:00 zpool create -o <span>ashift</span><span>=</span><span>12</span> -m /dst dst /dst.img
</span></span><span><span>2025-09-01.00:00:00 zfs recv dst/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs load-key dst/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs recv -F dst/encryptionroot/child
</span></span><span><span>2025-09-01.00:00:00 zfs unload-key dst/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs load-key dst/encryptionroot
</span></span><span><span>
</span></span><span><span>History <span>for</span> <span>'src'</span>:
</span></span><span><span>2025-09-01.00:00:00 zpool create -o <span>ashift</span><span>=</span><span>12</span> -m /src src /src.img
</span></span><span><span>2025-09-01.00:00:00 zfs create -o <span>encryption</span><span>=</span>on -o <span>keyformat</span><span>=</span>passphrase -o <span>keylocation</span><span>=</span>prompt src/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs create src/encryptionroot/child
</span></span><span><span>2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
</span></span><span><span>2025-09-01.00:00:00 zfs send --replicate --raw src/encryptionroot@111
</span></span><span><span>2025-09-01.00:00:00 zfs change-key -o <span>keyformat</span><span>=</span>hex src/encryptionroot
</span></span><span><span>2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@222</span></span></code></pre></div>
<p>
ZFS also logs many internal operations in the pool history (search for <code>spa_history_log</code> in the source code) which can be viewed with the <code>-i</code> flag. For snapshots, this includes the transaction group (txg) id when the snapshot was created, which is exactly what we're looking for!</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zpool <span>history</span> -i src
</span></span><span><span>History <span>for</span> <span>'src'</span>:
</span></span><span><span>...
</span></span><span><span>2025-09-01.00:00:00 <span>[</span>txg:12<span>]</span> snapshot src/encryptionroot@111 <span>(</span>768<span>)</span>
</span></span><span><span>2025-09-01.00:00:00 <span>[</span>txg:12<span>]</span> snapshot src/encryptionroot/child@111 <span>(</span>770<span>)</span>
</span></span><span><span>2025-09-01.00:00:00 <span>(</span>3ms<span>)</span> ioctl snapshot
</span></span><span><span>    input:
</span></span><span><span>        snaps:
</span></span><span><span>            src/encryptionroot@111
</span></span><span><span>            src/encryptionroot/child@111
</span></span><span><span>        props:
</span></span><span><span>
</span></span><span><span>2025-09-01.00:00:00 zfs snapshot -r src/encryptionroot@111
</span></span><span><span>...</span></span></code></pre></div>
<p>
The GUID and creation timestamp we can easily get from the snapshot on <code>dst</code>.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zfs list -o name,guid,creation -p dst/encryptionroot@111
</span></span><span><span>NAME                                   GUID  CREATION
</span></span><span><span>dst/encryptionroot@111  <span>4964628655505655411</span>  <span>1756699200</span></span></span></code></pre></div>
<p>
Now that we know everything we need to create the bookmark, we just need to figure out a way to manually create a bookmark with arbitrary data.</p>
</div>
</div>
<div id="outline-container-headline-17">
<h3 id="headline-17">
Hacking ZFS to manually create a bookmark
</h3>
<div id="outline-text-headline-17">
<p>
To understand how ZFS creates a bookmark, we can trace the code path from <code>zfs bookmark</code> all the way down to <code>dsl_bookmark_add</code> which actually adds the bookmark node to the tree.</p>
<ul>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/cmd/zfs/zfs_main.c#L215">command_table</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/cmd/zfs/zfs_main.c#L7806">zfs_do_bookmark</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/lib/libzfs_core/libzfs_core.c#L1128">lzc_bookmark</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/lib/libzfs_core/libzfs_core.c#L172">lzc_ioctl</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/lib/libzutil/os/freebsd/zutil_compat.c#L95">zfs_ioctl_fd</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/lib/libzutil/os/freebsd/zutil_compat.c#L48">zcmd_ioctl_compat</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/zfs_ioctl.c#L7028">zfs_ioctl_register bookmark</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/zfs_ioctl.c#L3721">zfs_ioc_bookmark</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/dsl_bookmark.c#L613">dsl_bookmark_create</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/dsl_bookmark.c#L584">dsl_bookmark_create_sync</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/dsl_bookmark.c#L516">dsl_bookmark_create_sync_impl_book</a></li>
<li><a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/module/zfs/dsl_bookmark.c#L393">dsl_bookmark_node_add</a></li>
</ul>
<p>This is the bookmark structure physically written to disk:</p>
<p>
<a href="https://github.com/openzfs/zfs/blob/zfs-2.1.11/include/sys/dsl_bookmark.h#L31-L57">zfs/include/sys/dsl_bookmark.h</a></p>
<div><pre tabindex="0"><code data-lang="c"><span><span><span>/*
</span></span></span><span><span><span> * On disk zap object.
</span></span></span><span><span><span> */</span>
</span></span><span><span><span>typedef</span> <span>struct</span> <span>zfs_bookmark_phys</span> <span>{</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_guid</span><span>;</span>		<span>/* guid of bookmarked dataset */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_creation_txg</span><span>;</span>	<span>/* birth transaction group */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_creation_time</span><span>;</span>	<span>/* bookmark creation time */</span>
</span></span><span><span>
</span></span><span><span>	<span>/* fields used for redacted send / recv */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_redaction_obj</span><span>;</span>	<span>/* redaction list object */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_flags</span><span>;</span>		<span>/* ZBM_FLAG_* */</span>
</span></span><span><span>
</span></span><span><span>	<span>/* fields used for bookmark written size */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_referenced_bytes_refd</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_compressed_bytes_refd</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_uncompressed_bytes_refd</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_referenced_freed_before_next_snap</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_compressed_freed_before_next_snap</span><span>;</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_uncompressed_freed_before_next_snap</span><span>;</span>
</span></span><span><span>
</span></span><span><span>	<span>/* fields used for raw sends */</span>
</span></span><span><span>	<span>uint64_t</span> <span>zbm_ivset_guid</span><span>;</span>
</span></span><span><span><span>}</span> <span>zfs_bookmark_phys_t</span><span>;</span>
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>#define	BOOKMARK_PHYS_SIZE_V1	(3 * sizeof (uint64_t))
</span></span></span><span><span><span>#define	BOOKMARK_PHYS_SIZE_V2	(12 * sizeof (uint64_t))</span></span></span></code></pre></div>
<p>
Only the first 3 fields are required for v1 bookmarks, while v2 bookmarks contain all 12 fields. <code>dsl_bookmark_node_add</code> only writes a v2 bookmark if one of the 9 v2 fields are non-zero, so we can leave them all zero to write a v1 bookmark.</p>
<p>
After a few iterations, I had a patch which hijacks the normal <code>zfs bookmark pool/dataset#src pool/dataset#dst</code> code path to create a bookmark with arbitrary data when the source bookmark name is <code>missing</code>.</p>
<div><pre tabindex="0"><code data-lang="diff"><span><span>sam@zfshax:~/zfs $ git --no-pager diff
</span></span><span><span>sam@zfshax:~/zfs $ git --no-pager diff
</span></span><span><span><span>diff --git a/cmd/zfs/zfs_main.c b/cmd/zfs/zfs_main.c
</span></span></span><span><span><span>index 2d81ef31c..73b5d7e70 100644
</span></span></span><span><span><span></span><span>--- a/cmd/zfs/zfs_main.c
</span></span></span><span><span><span></span><span>+++ b/cmd/zfs/zfs_main.c
</span></span></span><span><span><span></span><span>@@ -7892,12 +7892,15 @@ zfs_do_bookmark(int argc, char **argv)
</span></span></span><span><span><span></span> 		default: abort();
</span></span><span><span> 	}
</span></span><span><span>
</span></span><span><span><span>+// Skip testing for #missing because it does not exist.
</span></span></span><span><span><span>+if (strstr(source, "#missing") == NULL) {
</span></span></span><span><span><span></span> 	/* test the source exists */
</span></span><span><span> 	zfs_handle_t *zhp;
</span></span><span><span> 	zhp = zfs_open(g_zfs, source, source_type);
</span></span><span><span> 	if (zhp == NULL)
</span></span><span><span> 		goto usage;
</span></span><span><span> 	zfs_close(zhp);
</span></span><span><span><span>+}
</span></span></span><span><span><span></span>
</span></span><span><span> 	nvl = fnvlist_alloc();
</span></span><span><span> 	fnvlist_add_string(nvl, bookname, source);
</span></span><span><span><span>diff --git a/module/zfs/dsl_bookmark.c b/module/zfs/dsl_bookmark.c
</span></span></span><span><span><span>index 861dd9239..fae882f45 100644
</span></span></span><span><span><span></span><span>--- a/module/zfs/dsl_bookmark.c
</span></span></span><span><span><span></span><span>+++ b/module/zfs/dsl_bookmark.c
</span></span></span><span><span><span></span><span>@@ -263,7 +263,12 @@ dsl_bookmark_create_check_impl(dsl_pool_t *dp,
</span></span></span><span><span><span></span> 		 * Source must exists and be an earlier point in newbm_ds's
</span></span><span><span> 		 * timeline (newbm_ds's origin may be a snap of source's ds)
</span></span><span><span> 		 */
</span></span><span><span><span>+// Skip looking up #missing because it does not exist.
</span></span></span><span><span><span>+if (strstr(source, "#missing") == NULL) {
</span></span></span><span><span><span></span> 		error = dsl_bookmark_lookup(dp, source, newbm_ds, &amp;source_phys);
</span></span><span><span><span>+} else {
</span></span></span><span><span><span>+		error = 0;
</span></span></span><span><span><span>+}
</span></span></span><span><span><span></span> 		switch (error) {
</span></span><span><span> 		case 0:
</span></span><span><span> 			break; /* happy path */
</span></span><span><span><span>@@ -545,12 +550,34 @@ dsl_bookmark_create_sync_impl_book(
</span></span></span><span><span><span></span> 	 *   because the redaction object might be too large
</span></span><span><span> 	 */
</span></span><span><span>
</span></span><span><span><span>+// Skip looking up #missing because it does not exist.
</span></span></span><span><span><span>+if (strstr(source_name, "#missing") == NULL) {
</span></span></span><span><span><span></span> 	VERIFY0(dsl_bookmark_lookup_impl(bmark_fs_source, source_shortname,
</span></span><span><span> 	    &amp;source_phys));
</span></span><span><span><span>+}
</span></span></span><span><span><span></span> 	dsl_bookmark_node_t *new_dbn = dsl_bookmark_node_alloc(new_shortname);
</span></span><span><span>
</span></span><span><span><span>+// Skip copying from #missing because it does not exist.
</span></span></span><span><span><span>+if (strstr(source_name, "#missing") == NULL) {
</span></span></span><span><span><span></span> 	memcpy(&amp;new_dbn-&gt;dbn_phys, &amp;source_phys, sizeof (source_phys));
</span></span><span><span> 	new_dbn-&gt;dbn_phys.zbm_redaction_obj = 0;
</span></span><span><span><span>+} else {
</span></span></span><span><span><span>+	// Manually set the bookmark parameters.
</span></span></span><span><span><span>+	new_dbn-&gt;dbn_phys = (zfs_bookmark_phys_t){
</span></span></span><span><span><span>+		.zbm_guid = 4964628655505655411,
</span></span></span><span><span><span>+		.zbm_creation_txg = 12,
</span></span></span><span><span><span>+		.zbm_creation_time = 1756699200,
</span></span></span><span><span><span>+		.zbm_redaction_obj = 0,
</span></span></span><span><span><span>+		.zbm_flags = 0,
</span></span></span><span><span><span>+		.zbm_referenced_bytes_refd = 0,
</span></span></span><span><span><span>+		.zbm_compressed_bytes_refd = 0,
</span></span></span><span><span><span>+		.zbm_uncompressed_bytes_refd = 0,
</span></span></span><span><span><span>+		.zbm_referenced_freed_before_next_snap = 0,
</span></span></span><span><span><span>+		.zbm_compressed_freed_before_next_snap = 0,
</span></span></span><span><span><span>+		.zbm_uncompressed_freed_before_next_snap = 0,
</span></span></span><span><span><span>+		.zbm_ivset_guid = 0,
</span></span></span><span><span><span>+	};
</span></span></span><span><span><span>+}
</span></span></span><span><span><span></span>
</span></span><span><span> 	/* update feature counters */
</span></span><span><span> 	if (new_dbn-&gt;dbn_phys.zbm_flags &amp; ZBM_FLAG_HAS_FBN) {
</span></span></code></pre></div>
<p>
To test, we recompile ZFS, reload the kernel module, and reimport the pools.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ gmake -s -j<span>$(</span>sysctl -n hw.ncpu<span>)</span>
</span></span><span><span>sam@zfshax:~/zfs $ sudo gmake install <span>&amp;&amp;</span> sudo ldconfig
</span></span><span><span>sam@zfshax:~/zfs $ sudo zpool <span>export</span> src <span>&amp;&amp;</span> sudo zpool <span>export</span> dst
</span></span><span><span>sam@zfshax:~/zfs $ sudo ./scripts/zfs.sh -r
</span></span><span><span>sam@zfshax:~/zfs $ sudo zpool import src -d / <span>&amp;&amp;</span> sudo zpool import dst -d /</span></span></code></pre></div>
<p>
Then, we create the bookmark ex nihilo using the magic bookmark name <code>missing</code>.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sudo zfs bookmark src/encryptionroot#missing src/encryptionroot#111
</span></span><span><span>sam@zfshax:~/zfs $ sudo zdb src/encryptionroot#111
</span></span><span><span>	<span>#111: {guid: 44e5e7755d23c673 creation_txg: 12 creation_time: 1756699200 redaction_obj: 0}</span></span></span></code></pre></div>
<p>
Success! We can now use the bookmark to generate an incremental send stream containing the new hex wrapping key parameters.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~/zfs $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 <span>|</span> zstreamdump
</span></span><span><span>BEGIN record
</span></span><span><span>	<span>hdrtype</span> <span>=</span> <span>1</span>
</span></span><span><span>	<span>features</span> <span>=</span> <span>1420004</span>
</span></span><span><span>	<span>magic</span> <span>=</span> 2f5bacbac
</span></span><span><span>	<span>creation_time</span> <span>=</span> 68d3d93f
</span></span><span><span>	<span>type</span> <span>=</span> <span>2</span>
</span></span><span><span>	<span>flags</span> <span>=</span> 0xc
</span></span><span><span>	<span>toguid</span> <span>=</span> 3f99b9e92cc0aca7
</span></span><span><span>	<span>fromguid</span> <span>=</span> 44e5e7755d23c673
</span></span><span><span>	<span>toname</span> <span>=</span> src/encryptionroot@222
</span></span><span><span>	<span>payloadlen</span> <span>=</span> <span>1028</span>
</span></span><span><span>nvlist version: <span>0</span>
</span></span><span><span>	<span>crypt_keydata</span> <span>=</span> <span>(</span>embedded nvlist<span>)</span>
</span></span><span><span>	nvlist version: <span>0</span>
</span></span><span><span>		<span>DSL_CRYPTO_SUITE</span> <span>=</span> 0x8
</span></span><span><span>		<span>DSL_CRYPTO_GUID</span> <span>=</span> 0x6196311f2622e30
</span></span><span><span>		<span>DSL_CRYPTO_VERSION</span> <span>=</span> 0x1
</span></span><span><span>		<span>DSL_CRYPTO_MASTER_KEY_1</span> <span>=</span> 0x6c 0x55 0x13 0x78 0x8c 0x2d 0x42 0xb5 0x9e 0x33 0x2 0x7e 0x73 0x3a 0x46 0x20 0xd2 0xf7 0x23 0x7d 0x7c 0x5d 0x5f 0x76 0x63 0x90 0xd2 0x43 0x6a 0xdd 0x63 0x2b
</span></span><span><span>		<span>DSL_CRYPTO_HMAC_KEY_1</span> <span>=</span> 0x85 0xd1 0xf3 0xba 0xed 0xec 0x6 0x28 0x36 0xd6 0x60 0x28 0x8d 0x2f 0x6f 0x14 0xc9 0x2b 0x6f 0xf4 0x19 0x23 0x2d 0xf 0x3d 0xe 0xc4 0x88 0x4 0x6d 0xca 0xb5 0x2d 0x4d 0x8 0x75 0x17 0x1c 0xe3 0xe7 0xe6 0x23 0x7 0x53 0x94 0xba 0xc7 0x4b 0xf5 0xde 0x8c 0x29 0xa3 0x27 0xdf 0x82 0x64 0x9d 0x92 0xb4 0xc1 0x26 0x5b 0x32
</span></span><span><span>		<span>DSL_CRYPTO_IV</span> <span>=</span> 0xdf 0x52 0x77 0xe8 0xf 0xfd 0xc2 0x42 0x66 0x88 0xb9 0xf0
</span></span><span><span>		<span>DSL_CRYPTO_MAC</span> <span>=</span> 0x54 0x54 0x15 0xa4 0x21 0x55 0x6b 0x4e 0x99 0xe7 0xf 0xef 0x9f 0x90 0x42 0x54
</span></span><span><span>		<span>portable_mac</span> <span>=</span> 0x3a 0xd6 0x30 0xc4 0x6a 0x2d 0x60 0x24 0x95 0xfc 0x99 0xbb 0xfa 0x10 0xa0 0x6b 0xc6 0x1 0xdd 0x1d 0x9 0xcd 0xa8 0x19 0xdf 0x57 0xb9 0x90 0x4f 0x2e 0x33 0xc1
</span></span><span><span>		<span>keyformat</span> <span>=</span> 0x2
</span></span><span><span>		<span>pbkdf2iters</span> <span>=</span> 0x0
</span></span><span><span>		<span>pbkdf2salt</span> <span>=</span> 0x0
</span></span><span><span>		<span>mdn_checksum</span> <span>=</span> 0x0
</span></span><span><span>		<span>mdn_compress</span> <span>=</span> 0x0
</span></span><span><span>		<span>mdn_nlevels</span> <span>=</span> 0x6
</span></span><span><span>		<span>mdn_blksz</span> <span>=</span> 0x4000
</span></span><span><span>		<span>mdn_indblkshift</span> <span>=</span> 0x11
</span></span><span><span>		<span>mdn_nblkptr</span> <span>=</span> 0x3
</span></span><span><span>		<span>mdn_maxblkid</span> <span>=</span> 0x4
</span></span><span><span>		<span>to_ivset_guid</span> <span>=</span> 0x957edeaa7123a7
</span></span><span><span>		<span>from_ivset_guid</span> <span>=</span> 0x0
</span></span><span><span>	<span>(</span>end crypt_keydata<span>)</span>
</span></span><span><span>
</span></span><span><span>END <span>checksum</span> <span>=</span> 14046201258/62f53166ccc36/14023a70758c3195/1e906f4670783cd
</span></span><span><span>SUMMARY:
</span></span><span><span>	Total DRR_BEGIN <span>records</span> <span>=</span> <span>1</span> <span>(</span><span>1028</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_END <span>records</span> <span>=</span> <span>1</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_OBJECT <span>records</span> <span>=</span> <span>7</span> <span>(</span><span>960</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_FREEOBJECTS <span>records</span> <span>=</span> <span>2</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_WRITE <span>records</span> <span>=</span> <span>1</span> <span>(</span><span>512</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_WRITE_BYREF <span>records</span> <span>=</span> <span>0</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_WRITE_EMBEDDED <span>records</span> <span>=</span> <span>0</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_FREE <span>records</span> <span>=</span> <span>12</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total DRR_SPILL <span>records</span> <span>=</span> <span>0</span> <span>(</span><span>0</span> bytes<span>)</span>
</span></span><span><span>	Total <span>records</span> <span>=</span> <span>26</span>
</span></span><span><span>	Total payload <span>size</span> <span>=</span> <span>2500</span> <span>(</span>0x9c4<span>)</span>
</span></span><span><span>	Total header <span>overhead</span> <span>=</span> <span>8112</span> <span>(</span>0x1fb0<span>)</span>
</span></span><span><span>	Total stream <span>length</span> <span>=</span> <span>10612</span> <span>(</span>0x2974<span>)</span></span></span></code></pre></div>
<p>
But we can't receive the send stream.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 <span>|</span> sudo zfs recv -F dst/encryptionroot
</span></span><span><span>cannot receive incremental stream: IV <span>set</span> guid missing. See errata <span>4</span> at https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER.</span></span></code></pre></div>
</div>
</div>
<div id="outline-container-headline-18">
<h3 id="headline-18">
The final obstacle
</h3>
<div id="outline-text-headline-18">
<p>
ZFS refuses the stream because it is missing a source IV set GUID (see <code>from_ivset_guid = 0x0</code> in the <code>zstreamdump</code> above). This is because we created a v1 bookmark which does not contain the IV set GUID like a v2 bookmark would.</p>
<p>
Since we know that the send stream is created using the right snapshots, we can temporarily disable checking IV set GUIDs to allow the snapshot to be received as described in <a href="https://openzfs.github.io/openzfs-docs/msg/ZFS-8000-ER/index.html#zfs-errata-4">errata 4</a>.</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check<span>=</span><span>1</span>
</span></span><span><span>vfs.zfs.disable_ivset_guid_check: <span>0</span> -&gt; <span>1</span>
</span></span><span><span>sam@zfshax:~ $ sudo zfs send --raw -i src/encryptionroot#111 src/encryptionroot@222 <span>|</span> sudo zfs recv -F dst/encryptionroot
</span></span><span><span>sam@zfshax:~ $ sudo sysctl vfs.zfs.disable_ivset_guid_check<span>=</span><span>0</span>
</span></span><span><span>vfs.zfs.disable_ivset_guid_check: <span>1</span> -&gt; <span>0</span>
</span></span><span><span>sam@zfshax:~ $ sudo zpool <span>export</span> dst
</span></span><span><span>sam@zfshax:~ $ sudo zpool import dst -d /
</span></span><span><span>sam@zfshax:~ $ sudo zpool scrub dst
</span></span><span><span>sam@zfshax:~ $ sudo zpool status -x
</span></span><span><span>all pools are healthy</span></span></code></pre></div>
</div>
</div>
<div id="outline-container-headline-19">
<h3 id="headline-19">
The moment of truth
</h3>
<div id="outline-text-headline-19">
<p>
And now for the moment of truth…</p>
<div><pre tabindex="0"><code data-lang="bash"><span><span>sam@zfshax:~ $ <span>echo</span> <span>"0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef"</span> <span>|</span> sudo zfs load-key dst/encryptionroot
</span></span><span><span>sam@zfshax:~ $ sudo zfs mount -a
</span></span><span><span>sam@zfshax:~ $ sudo zfs list -t all -o name,encryptionroot,keyformat,keystatus,mounted,guid -r dst
</span></span><span><span>NAME                          ENCROOT             KEYFORMAT   KEYSTATUS    MOUNTED   GUID
</span></span><span><span>dst                           -                   none        -            yes      <span>15258247229701443799</span>
</span></span><span><span>dst/encryptionroot            dst/encryptionroot  hex         available    yes      <span>17755083343181277380</span>
</span></span><span><span>dst/encryptionroot@111        dst/encryptionroot  -           available    -        <span>4964628655505655411</span>
</span></span><span><span>dst/encryptionroot@222        dst/encryptionroot  -           available    -        <span>4582898506955533479</span>
</span></span><span><span>dst/encryptionroot/child      dst/encryptionroot  hex         available    yes      <span>364333975888407846</span>
</span></span><span><span>dst/encryptionroot/child@111  dst/encryptionroot  -           available    -        <span>15551239789901400488</span>
</span></span><span><span>dst/encryptionroot/child@222  dst/encryptionroot  -           available    -        <span>11729357375613972731</span>
</span></span><span><span>sam@zfshax:~ $ tree --noreport --noreport /dst
</span></span><span><span>/dst
</span></span><span><span>└── encryptionroot
</span></span><span><span>    ├── <span>222</span>
</span></span><span><span>    └── child
</span></span><span><span>        └── <span>222</span></span></span></code></pre></div>
<p>
<a href="https://www.youtube.com/watch?v=30jNsCVLpAE&amp;t=931s">WE'RE GONNA LIVE!!!</a></p>
<p>
At this point, we can now reliably fix the issue in our test environment. All we need to do now is use our hacked ZFS build to create the bookmark on <code>old</code>, send an incremental snapshot of the encryption root with the new key to <code>sneakernet</code>, and then send that snapshot from <code>sneakernet</code> to <code>new</code>. I rebuilt ZFS again with the correct transaction group, GUID, and creation timestamp for <code>old</code>, repeated the same steps with the names changed, and thanks to our thorough testing, it worked on the first try!</p>
</div>
</div>
</div>
</div>
<div id="outline-container-headline-20">
<h2 id="headline-20">
Conclusion
</h2>
<div id="outline-text-headline-20">
<p>
After a week of intense research and debugging, I had rescued our data back from the brink and could again sleep soundly at night. While I appreciated the opportunity to learn more about ZFS, I can't help but think about how this entire incident <a href="https://how.complexsystems.fail/#3">could have been avoided at several key points</a> which translate directly into lessons learned:</p>
<ol>
<li>Test backups continuously so you get immediate feedback when they break.</li>
<li>Wait to make all destructive changes together at the very end instead of interspersed where they could accidentally be performed in the wrong order.</li>
<li>Always send a snapshot of the encryption root after changing the encryption key.</li>
<li>Create bookmarks before destroying snapshots.</li>
</ol>
<p>I hope that you may learn from my mistakes and avoid a similar incident. If you do happen to find yourself in a similar predicament, I'd love to hear from you regardless of whether this postmortem was helpful or not. My contact details can be found <a href="https://sambowman.tech/">here</a>.</p>
<p>
Knowing what I now know about ZFS native encryption, I find it difficult to recommend until the <a href="https://github.com/openzfs/openzfs-docs/issues/494">sharp edges</a> have all been filed down. In most cases, I'd prefer to encrypt the entire pool at the block device level and encrypt send streams with <a href="https://age-encryption.org/">age</a>. But if you really do need the flexibility offered by native encryption, always remember to mind the encryptionroot!</p>
</div>
</div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atuin Desktop: Runbooks That Run – Now Open Source (249 pts)]]></title>
            <link>https://blog.atuin.sh/atuin-desktop-open-source/</link>
            <guid>45431001</guid>
            <pubDate>Tue, 30 Sep 2025 20:44:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.atuin.sh/atuin-desktop-open-source/">https://blog.atuin.sh/atuin-desktop-open-source/</a>, See on <a href="https://news.ycombinator.com/item?id=45431001">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

    <article>

        <header>

                <a href="https://blog.atuin.sh/tag/news/">News</a>
            
                <p>Atuin Desktop looks like a doc, but runs like your terminal. Script blocks, embedded terminals, database clients and prometheus charts - all in one place.</p>

            <div>
                <p><a href="https://blog.atuin.sh/author/ellie/">
                                <img src="https://blog.atuin.sh/content/images/size/w160/2024/01/me2.jpg" alt="Ellie Huxtable">
                            </a>
                </p>
                
            </div>

            
        </header>

        <section>
            <figure data-kg-thumbnail="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final.mp4" poster="https://img.spacergif.org/v1/1852x1600/0a/spacer.png" width="1852" height="1600" loop="" autoplay="" muted="" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:27</span>
                        </p>
                        </div>
            </div>
            
        <img src="https://blog.atuin.sh/content/media/2025/09/atuin-demo-final_thumb.jpg"></figure><p>Most infrastructure is held together by five commands someone remembers when shit breaks. Docs are out of date, if they exist. The real answers? Buried in Slack threads, rotting in Notion, or trapped in someone's shell history.</p><p><a href="https://atuin.sh/?ref=blog.atuin.sh" rel="noreferrer">Atuin CLI</a>&nbsp;fixed part of this, with synced, searchable shell history. But history isn’t enough. Teams need workflows they can&nbsp;<strong>repeat, share, and trust</strong>.</p><p>That’s why we built Atuin Desktop. Runbooks that actually run. Now open beta, and fully&nbsp;<a href="https://github.com/atuinsh/desktop?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">open source</a>.</p><h2 id="what-is-atuin-desktop">What is Atuin Desktop?</h2><p>Atuin Desktop looks like a doc, but runs like your terminal.&nbsp;Built to make local developer workflows repeatable, shareable, and reliable.</p><figure><div><p><img src="https://blog.atuin.sh/content/images/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png" width="2000" height="1584" loading="lazy" alt="" srcset="https://blog.atuin.sh/content/images/size/w600/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 600w, https://blog.atuin.sh/content/images/size/w1000/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 1000w, https://blog.atuin.sh/content/images/size/w1600/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 1600w, https://blog.atuin.sh/content/images/size/w2400/2025/09/CleanShot-2025-09-30-at-11.59.58@2x.png 2400w" sizes="(min-width: 720px) 720px"></p><p><img src="https://blog.atuin.sh/content/images/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png" width="2000" height="1654" loading="lazy" alt="" srcset="https://blog.atuin.sh/content/images/size/w600/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 600w, https://blog.atuin.sh/content/images/size/w1000/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 1000w, https://blog.atuin.sh/content/images/size/w1600/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 1600w, https://blog.atuin.sh/content/images/size/w2400/2025/09/CleanShot-2025-09-30-at-12.56.17@2x.png 2400w" sizes="(min-width: 720px) 720px"></p></div></figure><p>Runbooks should run. Workflows shouldn't live in someone's head. Docs shouldn't rot the moment you write them. Scripts, database queries, HTTP requests and Prometheus charts - all in one place.</p><ul><li><strong>Kill context switching:</strong> Chain shell <a href="https://man.atuin.sh/blocks/executable/script/?ref=blog.atuin.sh" rel="noreferrer">scripts</a>, <a href="https://man.atuin.sh/blocks/databases/?ref=blog.atuin.sh" rel="noreferrer">database</a> queries, and <a href="https://man.atuin.sh/blocks/network/http/?ref=blog.atuin.sh" rel="noreferrer">HTTP</a> requests</li><li><strong>Docs that don't rot: </strong>execute directly + stay relevant</li><li><strong>Reusable automation: </strong>dynamic runbooks with <a href="https://man.atuin.sh/templating/?ref=blog.atuin.sh" rel="noreferrer">Jinja-style templating</a></li><li><strong>Local knowledge</strong>: Build runbooks from your real shell history</li><li><strong>Collaborative</strong>: Sync and share via <a href="https://man.atuin.sh/workspaces/?ref=blog.atuin.sh" rel="noreferrer">Git</a>, or in <a href="https://man.atuin.sh/hub/getting-started/?ref=blog.atuin.sh" rel="noreferrer">real-time</a> via our Hub</li></ul><div data-layout="immersive">
                    
                        <div>
                            <p dir="ltr"><span>Back in April we&nbsp;</span><a href="https://blog.atuin.sh/atuin-desktop-runbooks-that-run/" target="_blank" rel="noopener noreferrer nofollow"><span>launched the closed beta</span></a><span>.&nbsp;</span></p><p dir="ltr"><span>Thousands of you signed up, used it at your day jobs, and told us exactly what broke. We’ve listened, rebuilt, and now it’s ready for everyone.</span></p>
                        </div>
                    
                    
                        <p><a href="https://github.com/atuinsh/desktop/releases/latest?ref=blog.atuin.sh">
                            Download
                        </a>
                        
                    </p></div><h2 id="what%E2%80%99s-new-since-april">What’s new since April?</h2><p>Our early users gave us a lot of feedback, which we've used to build something much better. </p><ul><li>Offline, file based, Git/VCS-compatible&nbsp;<a href="https://man.atuin.sh/workspaces/?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">workspaces</a></li><li><a href="https://man.atuin.sh/hub/collaborative-editing/?ref=blog.atuin.sh#team-based-collaboration" rel="noopener noreferrer nofollow">Team</a>&nbsp;accounts with shared, realtime workspaces</li><li><a href="https://man.atuin.sh/blocks/executable/kubernetes?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">Kubernetes</a>&nbsp;integration for live state and monitoring</li><li><a href="https://man.atuin.sh/blocks/databases/mysql?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">MySQL</a>&nbsp;query blocks</li><li><a href="https://man.atuin.sh/blocks/executable/dropdown?ref=blog.atuin.sh" rel="noopener noreferrer nofollow">Dropdown</a>&nbsp;and more contextual blocks</li><li>A huge number of bug fixes, performance improvements, and UI upgrades</li></ul><h2 id="how-it%E2%80%99s-being-used">How it’s being used</h2><p>Atuin Desktop is already being used across engineering teams for serious, day-to-day work.</p><ul><li><strong>Automation and debugging:</strong> linking commands, monitoring systems, and tracking results</li><li><strong>Database operations:</strong> managing migrations, access control, and production queries</li><li><strong>Onboarding:</strong> getting started workflows new engineers can actually run</li><li><strong>Deploying and managing clusters:</strong> repeatable, documented automation for real environments</li><li><strong>Incident response:</strong> runbooks that execute instead of rotting in some internal wiki</li></ul><p>It’s become a shared system of record for the commands and processes that keep production alive.</p><h2 id="what%E2%80%99s-next">What’s next</h2><p>We’re just getting started! We've got a lot in the pipeline, including:</p><ul><li>Block dependencies and advanced execution flow</li><li>Run runbooks remotely and on CI</li><li>Audit logs and enhanced permissions</li><li>Comments and deeper collaboration</li><li>More block types<ul><li>Specify local networks, containers, and more</li><li>Tighter integration with authentication and cloud providers</li></ul></li><li>More polish, more speed, fewer bugs</li></ul><div data-layout="immersive">
                    
                        <p dir="ltr"><span>Stop copy-pasting from outdated wiki pages, and get started with Atuin Desktop</span></p>
                    
                    
                        <p><a href="https://github.com/atuinsh/desktop/releases/latest?ref=blog.atuin.sh">
                            Download
                        </a>
                        
                    </p></div><h2 id="getting-involved">Getting involved</h2><p>Atuin Desktop is now in open beta and open source under the Apache 2.0 license. Star it, fork it, break it: <a href="https://github.com/atuinsh/desktop?ref=blog.atuin.sh" rel="noopener">github.com/atuinsh/desktop</a></p><p>Infrastructure deserves better than rotting docs and tribal knowledge. Atuin Desktop is our attempt to fix that for everyone who’s ever said “I swear I’ve done this before.”</p><p><strong>Discord: </strong><a href="https://discord.gg/Fq8bJSKPHh?ref=blog.atuin.sh">discord.gg/Fq8bJSKPHh</a></p><p><strong>Forum: </strong><a href="https://forum.atuin.sh/?ref=blog.atuin.sh" rel="noreferrer">forum.atuin.sh</a></p>
        </section>

    </article>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Diff Algorithms (258 pts)]]></title>
            <link>https://flo.znkr.io/diff/</link>
            <guid>45430604</guid>
            <pubDate>Tue, 30 Sep 2025 20:09:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flo.znkr.io/diff/">https://flo.znkr.io/diff/</a>, See on <a href="https://news.ycombinator.com/item?id=45430604">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>For software engineers, diffs are a ubiquitous method for representing changes: We use diffs to
compare different versions of the same file (e.g., during code review or when trying to understand
the history of a file), to visualize the difference of a failing test compared with its
expectation, or to apply changes to source files automatically.</p>
<p>Every project I worked on professionally or privately eventually needed a diff to visualize a change
or to apply a patch. However, I have never been satisfied with any of the freely available diff
libraries. This was never really a problem professionally, but for private projects, I have copied
and modified my own library from project to project until I mentioned this to a colleague who set me
on the path to publish my Go library (a port of a previous C++ library I used to copy and modify).
<em>Boy, did I underestimate how close my library was to publishability!</em></p>
<p>Anyway, I did it and I learned a whole lot about diff algorithms. You can find my library at
<a href="https://znkr.io/diff">znkr.io/diff</a> and what I learned in this article. I am not finished learning
yet, so I plan to update this article as my understanding continues to evolve.</p>
<h2 id="existing-diff-libraries">Existing Diff Libraries<a href="#existing-diff-libraries"></a></h2>
<p>Let me start by explaining why I am dissatisfied with existing diff libraries. There are a number of
attributes that are important to me. Not all of these attributes are important for every use case,
but a diff library that I can use for all of my use cases needs to fulfill all of them.</p>
<p>Usually, the input to a diff algorithm is text, and most diff libraries only support that. However,
I occasionally have use cases where I need to compare things that are not text. So any diff library
that only supports text doesn't meet my needs; instead, I need support for <strong>arbitrary sequences</strong>.</p>
<p>The resulting diff output is intended to be readable by humans. Quite often, especially for text, a
good way to present a diff is in the <strong>unified format</strong>. However, it's not always the best
presentation. A diff library should make it easy to output a diff in unified format, but it should
also provide a way to customize the presentation by providing a <strong>structured result</strong>.</p>
<p>Besides the presentation, the content of a diff should make it easy for humans to understand the
diff. This is a somewhat subjective criterion, but there are a number of failure cases that are
easily avoided, and there's some research into <strong>diff readability</strong> to set a benchmark. On the other
hand, diffs should be <strong>minimal</strong> in that they should be as small as possible.</p>
<p>Last but not least, it's important that a diff library has a <strong>simple API</strong> and provides good
<strong>performance</strong> in both runtime and memory usage, even in worst-case
scenarios<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p>With that, we can evaluate existing diff libraries. For Go, I went through a number of libraries
and summarized them.</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Input</th>
<th>Output</th>
<th>API</th>
<th>Performance<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
<th>Diff<br>Readability</th>
<th>Diff<br>Minimality<sup id="fnref1:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/sergi/go-diff">diffmatchpatch</a></td>
<td>❌<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>❌<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup></td>
<td>🤔<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup></td>
<td>➖➖</td>
<td>➖</td>
<td>➖</td>
</tr>
<tr>
<td><a href="https://github.com/rogpeppe/go-internal/tree/master/diff">go-internal</a></td>
<td>❌<sup id="fnref1:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>❌<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup></td>
<td>😁</td>
<td>➕➕</td>
<td>➕➕</td>
<td>➕</td>
</tr>
<tr>
<td><a href="https://github.com/kylelemons/godebug/tree/master/diff">godebug</a></td>
<td>❌<sup id="fnref2:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>✅</td>
<td>😁</td>
<td>➖➖➖ /🧨<sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup></td>
<td>➕</td>
<td>➕➕</td>
</tr>
<tr>
<td><a href="https://github.com/mb0/diff">mb0</a></td>
<td>✅</td>
<td>❌<sup id="fnref1:4"><a href="#fn:4" role="doc-noteref">4</a></sup></td>
<td>😐<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup></td>
<td>➖➖</td>
<td>➕</td>
<td>➕➕</td>
</tr>
<tr>
<td><a href="https://github.com/aymanbagabas/go-udiff">udiff</a></td>
<td>❌<sup id="fnref3:3"><a href="#fn:3" role="doc-noteref">3</a></sup></td>
<td>✅</td>
<td>😁</td>
<td>➕<sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup></td>
<td>➖</td>
<td>➖➖<sup id="fnref1:9"><a href="#fn:9" role="doc-noteref">9</a></sup></td>
</tr>
</tbody>
</table>
<div><p>Beware</p><p>The way I assigned ➕ and ➖ in this table doesn't follow any scientific methodology
it's merely based on running a few benchmarks and comparing a few results by hand. If you're looking
for a diff library to fulfill your needs, I would like to encourage you to do your own comparisons.
You can find the code I used for these comparisons in <a href="https://github.com/znkr/diff/tree/main/internal/benchmarks">on
github</a>.</p>
</div>
<h2 id="challenges">Challenges<a href="#challenges"></a></h2>
<p>The results suggest that it's far from trivial to implement a good diff library, and the one I had
started out with wasn't much better. To understand why the existing libraries are as they are,
we need to take a peek into the implementation.</p>
<h3 id="complexity">Complexity<a href="#complexity"></a></h3>
<p>With the exception of go-internal, all libraries use <a href="http://www.xmailserver.org/diff2.pdf">Myers'
Algorithm</a> to compute the diff. This is a standard algorithm
that returns a minimal diff and has been in use for this purpose for decades. The algorithm has a
runtime complexity of 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <mi>N</mi>
      <mi>D</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(ND)</annotation>
  </semantics>
</math>
 where 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>N</mi>
    </mrow>
    <annotation encoding="application/x-tex">N</annotation>
  </semantics>
</math>
 is the number of input elements and 
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow>
      <mi>D</mi>
    </mrow>
    <annotation encoding="application/x-tex">D</annotation>
  </semantics>
</math>
 is the
edit distance between the two inputs. This means that the algorithm is very fast for inputs that are
similar, which is quite common. However, it's essentially quadratic in the worst case. That is, for
inputs that are very different, the complexity approaches 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
. Furthermore, the
algorithm comes in two variants with a space complexity of either 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
 or

<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N)</annotation>
  </semantics>
</math>
. Only godebug uses the variant with quadratic memory growth.</p>
<p>This means that <strong>it's relatively easy to write a well-performing diffing algorithm for small or
similar inputs, but it takes a very long time to complete for larger, less similar inputs</strong>. A
consequence of this is that we can't trust simple benchmarks; instead, we need to test the
worst-case scenario<sup id="fnref1:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p>As always in cases like this, we can improve the performance by approximating an optimal solution.
There are a number of heuristics that reduce the time complexity by trading off diff minimality. For
example, diffmatchpatch uses a deadline to stop the search for an optimal diff, and udiff uses a
an extremely aggressive heuristic.</p>
<p>Instead of improving Myers' runtime with heuristics, it's also often possible to find a diff using
only heuristics. go-internal uses <a href="https://bramcohen.livejournal.com/73318.html">patience diff</a>. The
heuristic is good enough that it alone almost always results in a good diff with a runtime
complexity of 
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo stretchy="false" form="prefix">(</mo>
      <mi>N</mi>
      <mspace width="0.17em"></mspace>
      <mi lspace="0.11111em">log</mi>
      <mspace width="0.17em"></mspace>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N \, \log \, N)</annotation>
  </semantics>
</math>
<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>. An additional advantage of
this algorithm is that it produces more readable diffs. However, patience diff can fail with very
large diffs, and it can only be implemented efficiently using a hash table, which restricts the
possible applications.</p>
<div><p>Histogram Diff</p><p>Besides patience diff, there's another interesting heuristic called histogram
diff. I still have to implement it and understand it better before writing about it here, though.</p>
</div>
<h3 id="readability">Readability<a href="#readability"></a></h3>
<p>Diff algorithms usually find a minimal diff or an approximation of one. However, except for trivial
cases, there are always multiple minimal diffs. For example, this simple diff</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_01.diff">example_01.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>a
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>b
</code></td>
            </tr><tr data-op="delete" data-x-lineno="2">
                <td>2</td>
                <td></td>
                <td>-</td>
                <td><code>c
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>d
</code></td>
            </tr></tbody>
</table>
<p>is as minimal as</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_02.diff">example_02.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>a
</code></td>
            </tr><tr data-op="delete" data-x-lineno="2">
                <td>2</td>
                <td></td>
                <td>-</td>
                <td><code>c
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>b
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>d
</code></td>
            </tr></tbody>
</table>
<p>Not all of the minimal or near-minimal diffs have the same readability for humans. For
example<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup>,</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_03.diff">example_03.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>{
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="6">
                <td></td>
                <td>6</td>
                <td>+</td>
                <td><code>}
</code></td>
            </tr><tr data-op="insert" data-y-lineno="7">
                <td></td>
                <td>7</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="8">
                <td>1</td>
                <td>8</td>
                <td> </td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="9">
                <td>2</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="10">
                <td>3</td>
                <td>10</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="11">
                <td>4</td>
                <td>11</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="12">
                <td>5</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="13">
                <td>6</td>
                <td>13</td>
                <td> </td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="14">
                <td>7</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="delete" data-x-lineno="8">
                <td>8</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="delete" data-x-lineno="10">
                <td>10</td>
                <td></td>
                <td>-</td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="12">
                <td>12</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="14">
                <td>14</td>
                <td></td>
                <td>-</td>
                <td><code>}
</code></td>
            </tr></tbody>
</table>
<p>is much more readable than the equally minimal and correct</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_04.diff">example_04.diff</a>
</caption>
<tbody><tr data-op="delete" data-x-lineno="1">
                <td>1</td>
                <td></td>
                <td>-</td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="3">
                <td>3</td>
                <td></td>
                <td>-</td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="4">
                <td>4</td>
                <td></td>
                <td>-</td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="4">
                <td>5</td>
                <td>4</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="6">
                <td>6</td>
                <td></td>
                <td>-</td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="6">
                <td>7</td>
                <td>6</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="7">
                <td>8</td>
                <td>7</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="8">
                <td></td>
                <td>8</td>
                <td>+</td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="9">
                <td>10</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="10">
                <td></td>
                <td>10</td>
                <td>+</td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="11">
                <td></td>
                <td>11</td>
                <td>+</td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="12">
                <td>12</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="14">
                <td>14</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="15">
                <td>15</td>
                <td>15</td>
                <td> </td>
                <td><code></code></td>
            </tr></tbody>
</table>
<p>Furthermore, if we relax minimality to accept approximations, the number of possible results
increases significantly.</p>
<p>For good diff readability, we have to select one solution from the many possible ones that is
readable for humans. Many people believe that the diff readability is determined by the algorithm.
However, that's only partially correct, because <strong>different <em>implementations</em> of the same algorithm
can produce vastly different results</strong>.</p>
<p>There's also been a lot of progress in the past years to improve diff readability. Perhaps the best
work about diff readability is <a href="https://github.com/mhagger/diff-slider-tools">diff-slider-tools</a> by
<a href="https://github.com/mhagger">Michael Haggerty</a>. He implemented a heuristic that's applied in a
post-processing step to improve the readability.</p>
<p>In fact, <code>example_03.diff</code> above was generated using this heuristic. The diff without the heuristic,
as generated by my implementation of Myers' linear-space variant, looks like this:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_03_no_indent_heuristic.diff">example_03_no_indent_heuristic.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>{
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="insert" data-y-lineno="6">
                <td></td>
                <td>6</td>
                <td>+</td>
                <td><code>}
</code></td>
            </tr><tr data-op="insert" data-y-lineno="7">
                <td></td>
                <td>7</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="8">
                <td>1</td>
                <td>8</td>
                <td> </td>
                <td><code>void Chunk_copy(Chunk *src, size_t src_start, Chunk *dst, size_t dst_start, size_t n)
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="9">
                <td>2</td>
                <td>9</td>
                <td> </td>
                <td><code>{
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="10">
                <td>3</td>
                <td>10</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(src, src_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="11">
                <td>4</td>
                <td>11</td>
                <td> </td>
                <td><code>    if (!Chunk_bounds_check(dst, dst_start, n)) return;
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="12">
                <td>5</td>
                <td>12</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="13">
                <td>6</td>
                <td>13</td>
                <td> </td>
                <td><code>    memcpy(dst-&gt;data + dst_start, src-&gt;data + src_start, n);
</code></td>
            </tr><tr data-op="delete" data-x-lineno="7">
                <td>7</td>
                <td></td>
                <td>-</td>
                <td><code>}
</code></td>
            </tr><tr data-op="delete" data-x-lineno="8">
                <td>8</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="9">
                <td>9</td>
                <td></td>
                <td>-</td>
                <td><code>int Chunk_bounds_check(Chunk *chunk, size_t start, size_t n)
</code></td>
            </tr><tr data-op="delete" data-x-lineno="10">
                <td>10</td>
                <td></td>
                <td>-</td>
                <td><code>{
</code></td>
            </tr><tr data-op="delete" data-x-lineno="11">
                <td>11</td>
                <td></td>
                <td>-</td>
                <td><code>    if (chunk == NULL) return 0;
</code></td>
            </tr><tr data-op="delete" data-x-lineno="12">
                <td>12</td>
                <td></td>
                <td>-</td>
                <td><code>
</code></td>
            </tr><tr data-op="delete" data-x-lineno="13">
                <td>13</td>
                <td></td>
                <td>-</td>
                <td><code>    return start &lt;= chunk-&gt;length &amp;&amp; n &lt;= chunk-&gt;length - start;
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="14">
                <td>14</td>
                <td>14</td>
                <td> </td>
                <td><code>}
</code></td>
            </tr></tbody>
</table>
<p>Notice that the deletion starts at the end of the preceding function and leaves a small
remainder of the function being deleted? Michael's heuristic fixes this problem and results in the
very readable <code>example_03.diff</code>.</p>
<div><p>It's not the algorithm</p><p><code>example_04.diff</code> was found using a different implementation of Myers'
linear-space variant. That is, both <code>example_03.diff</code> and <code>example_04.diff</code> used the same algorithm!
The differences stem from the implementation of that algorithm and from post-processing.</p>
</div>
<h2 id="a-new-diffing-library-for-go">A New Diffing Library for Go<a href="#a-new-diffing-library-for-go"></a></h2>
<p>I created <a href="https://znkr.io/diff">znkr.io/diff</a> to address these challenges in a way that works for
all my use cases. Let's reiterate what I want from a diffing library:</p>
<ul>
<li>The input can be text and arbitrary slices</li>
<li>The output should be possible in unified format and as a structured result</li>
<li>The API should be simple</li>
<li>The diffs should be minimal or near-minimal</li>
<li>The runtime and memory performance should be excellent</li>
</ul>
<p>This is a lot more than what any of the existing libraries provide. When I copied and modified my
old diffing library, I could adapt it to the use cases at hand. But a general-purpose diffing
library needs to be general enough to cover the vast majority of use cases. At the same time, it
needs to be extensible to make sure new features can be implemented without cluttering the API over
time.</p>
<p>Unfortunately, excellent performance and minimal results are somewhat in opposition to one another
and I ended up providing three different modes of operation: Default (balanced between performance
and minimality), Fast (sacrifice minimal results for faster speed), Optimal (minimal result whatever
the cost).</p>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Input</th>
<th>Output</th>
<th>API</th>
<th>Performance<sup id="fnref2:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
<th>Diff<br>Readability</th>
<th>Diff<br>Minimality<sup id="fnref3:2"><a href="#fn:2" role="doc-noteref">2</a></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>Default</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕➕</td>
<td>➕➕</td>
<td>➕➕</td>
</tr>
<tr>
<td>Fast</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕➕➕</td>
<td>➕➕</td>
<td>➕</td>
</tr>
<tr>
<td>Optimal</td>
<td>✅</td>
<td>✅</td>
<td>😁</td>
<td>➕</td>
<td>➕➕</td>
<td>➕➕</td>
</tr>
</tbody>
</table>
<div><p>Text Only</p><p>This table only applies to text (same as the table above), non-text inputs can have
a different performance (if they are not <code>comparable</code> or readability).</p>
</div>
<h3 id="api">API<a href="#api"></a></h3>
<p>To design this API, I started with the data structures that I wanted to use as a user of the API and
worked backwards from there. At a very high level, there are two structured representations of a
diff that have been useful to me: a flat sequence of all deletions, insertions, and matching
elements (called <em>edits</em>) and a nested sequence of consecutive changes (called <em>hunks</em>).</p>
<ul>
<li>Edits are what I use to represent edits in this article; they contain the full content of both
inputs and how one is transformed into the other.</li>
<li>Hunks are a great representation for unit tests, because they are empty if both inputs are
identical and they make it possible to visualize just the changes even if the inputs are large.</li>
</ul>
<h4 id="arbitrary-slices">Arbitrary Slices<a href="#arbitrary-slices"></a></h4>
<p>I started with the design for the most general case, arbitrary slices. The Go representation for
diffing slices I liked the most is this one (see also
<a href="https://pkg.go.dev/znkr.io/diff">znkr.io/diff</a>):</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>5</td>
        <td><code><span>// Op describes an edit operation.</span>
</code></td>
    </tr>
    <tr>
        <td>6</td>
        <td><code><span>type</span> Op int
</code></td>
    </tr>
    <tr>
        <td>7</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>8</td>
        <td><code><span>const</span> (
</code></td>
    </tr>
    <tr>
        <td>9</td>
        <td><code>	Match  Op = <span>iota</span> <span>// Two slice elements match</span>
</code></td>
    </tr>
    <tr>
        <td>10</td>
        <td><code>	Delete           <span>// A deletion from an element on the left slice</span>
</code></td>
    </tr>
    <tr>
        <td>11</td>
        <td><code>	Insert           <span>// An insertion of an element from the right side</span>
</code></td>
    </tr>
    <tr>
        <td>12</td>
        <td><code>)
</code></td>
    </tr>
    <tr>
        <td>13</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>14</td>
        <td><code><span>// Edit describes a single edit of a diff.</span>
</code></td>
    </tr>
    <tr>
        <td>15</td>
        <td><code><span>// - For Match, both X and Y contain the matching element.</span>
</code></td>
    </tr>
    <tr>
        <td>16</td>
        <td><code><span>// - For Delete, X contains the deleted element and Y is unset (zero value).</span>
</code></td>
    </tr>
    <tr>
        <td>17</td>
        <td><code><span>// - For Insert, Y contains the inserted element and X is unset (zero value).</span>
</code></td>
    </tr>
    <tr>
        <td>18</td>
        <td><code><span>type</span> Edit[T any] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>19</td>
        <td><code>	Op   Op
</code></td>
    </tr>
    <tr>
        <td>20</td>
        <td><code>	X, Y T
</code></td>
    </tr>
    <tr>
        <td>21</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>22</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>23</td>
        <td><code><span>// Hunk describes a sequence of consecutive edits.</span>
</code></td>
    </tr>
    <tr>
        <td>24</td>
        <td><code><span>type</span> Hunk[T any] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>25</td>
        <td><code>	PosX, EndX int       <span>// Start and end position in x.</span>
</code></td>
    </tr>
    <tr>
        <td>26</td>
        <td><code>	PosY, EndY int       <span>// Start and end position in y.</span>
</code></td>
    </tr>
    <tr>
        <td>27</td>
        <td><code>	Edits      []Edit[T] <span>// Edits to transform x[PosX:EndX] to y[PosY:EndY]</span>
</code></td>
    </tr>
    <tr>
        <td>28</td>
        <td><code>}
</code></td>
    </tr></tbody>
</table>
<p>The alternatives I have seen are variations and combinations of two themes. Either using slices to
represent edit operations in <code>Hunk</code></p>
<pre><code>type Hunk[T any] struct {
	Delete []T
	Insert []T
	Match  []T
}
</code></pre>
<p>Or using indices instead of elements</p>
<pre><code>type Edit struct {
	Op         Op
	PosX, PosY []int
}
</code></pre>
<p>All of these representations work, but I found that the representations above served my use cases
best. One little quirk is that <code>Edit</code> always contains both elements. This is often unnecessary, but
there are use cases where this is very important because the elements themselves might not be equal
(e.g., if they are pointers that are compared with a custom function).</p>
<p>Once the data structures were established, it was quite obvious that the simplest way to fill them
with diff data was to write two functions <a href="https://pkg.go.dev/znkr.io/diff#Edits"><code>diff.Edits</code></a> and
<a href="https://pkg.go.dev/znkr.io/diff#Hunks"><code>diff.Hunks</code></a> to return the diffs. I made them extensible by
using <a href="https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis">functional options</a>.</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>30</td>
        <td><code><span>// Edits compares the contents of x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>31</td>
        <td><code><span>// the other.</span>
</code></td>
    </tr>
    <tr>
        <td>32</td>
        <td><code><span>//
</span></code></td>
    </tr>
    <tr>
        <td>33</td>
        <td><code><span>// Edits returns one edit for every element in the input slices. If x and y are identical, the</span>
</code></td>
    </tr>
    <tr>
        <td>34</td>
        <td><code><span>// output will consist of a match edit for every input element.</span>
</code></td>
    </tr>
    <tr>
        <td>35</td>
        <td><code><span>func</span> Edits[T comparable](x, y []T, opts ...Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>36</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>37</td>
        <td><code><span>// Hunks compares the contents of x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>38</td>
        <td><code><span>// the other.</span>
</code></td>
    </tr>
    <tr>
        <td>39</td>
        <td><code><span>//
</span></code></td>
    </tr>
    <tr>
        <td>40</td>
        <td><code><span>// The output is a sequence of hunks. A hunk represents a contiguous block of changes (insertions</span>
</code></td>
    </tr>
    <tr>
        <td>41</td>
        <td><code><span>// and deletions) along with some surrounding context.</span>
</code></td>
    </tr>
    <tr>
        <td>42</td>
        <td><code><span>func</span> Hunks[T comparable](x, y []T, opts ...Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<p>The options allow for future extensibility and allow changing the behavior of these functions. For
example, the option <a href="https://pkg.go.dev/znkr.io/diff#Context"><code>diff.Context(5)</code></a> configures <code>Hunks</code>
to provide 5 elements of surrounding context.</p>
<p>However, the current API still doesn't allow <em>arbitrary slices</em>; it only allows slices of
<code>comparable</code> types. To fix this, I needed two other functions that provide a function to compare
two elements. The Go standard library uses the <code>Func</code> suffix for functions like this, so I followed
the lead:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/diff.go">diff.go</a>
</caption>
<tbody>
    <tr>
        <td>44</td>
        <td><code><span>// EditsFunc compares the contents of x and y using the provided equality comparison and returns the</span>
</code></td>
    </tr>
    <tr>
        <td>45</td>
        <td><code><span>// changes necessary to convert from one to the other.</span>
</code></td>
    </tr>
    <tr>
        <td>46</td>
        <td><code><span>func</span> EditsFunc[T any](x, y []T, eq <span>func</span>(a, b T) bool, opts ...Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>47</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>48</td>
        <td><code><span>// HunksFunc compares the contents of x and y using the provided equality comparison and returns the</span>
</code></td>
    </tr>
    <tr>
        <td>49</td>
        <td><code><span>// changes necessary to convert from one to the other.</span>
</code></td>
    </tr>
    <tr>
        <td>50</td>
        <td><code><span>func</span> HunksFunc[T any](x, y []T, eq <span>func</span>(a, b T) bool, opts ...Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<h4 id="text">Text<a href="#text"></a></h4>
<p>While this API works well to produce a structured result for arbitrary slices, it doesn't provide
output in unified format for text inputs. My first approach was to provide a helper function that
returns a diff in unified format: <code>diff.ToUnified(hunks []Hunk[string]) string</code>. However, this would
make getting a unified diff more complicated. Besides requiring two function calls, it would be
necessary to split the input into lines. This, in turn, can be done in different ways, e.g., by
stripping or keeping the line breaks, which opens the door to mistakes. It's much better to provide
a simple function for the entire use case.</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/textdiff.go">textdiff.go</a>
</caption>
<tbody>
    <tr>
        <td>7</td>
        <td><code><span>// Unified compares the lines in x and y and returns the changes necessary to convert from one to</span>
</code></td>
    </tr>
    <tr>
        <td>8</td>
        <td><code><span>// the other in unified format.</span>
</code></td>
    </tr>
    <tr>
        <td>9</td>
        <td><code><span>func</span> Unified[T string | []byte](x, y T, opts ...diff.Option) T
</code></td>
    </tr></tbody>
</table>
<p>I also moved this function to the <a href="https://pkg.go.dev/znkr.io/diff/textdiff"><code>textdiff</code></a> package to
highlight the difference in expected input.</p>
<p>Now, I also happen to have use cases where I need structured results for text diffs. It would be
very annoying if I had to split those into lines manually. Besides, I can make a few more
assumptions about text that allow for a slight simplification of the data structures:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/textdiff.go">textdiff.go</a>
</caption>
<tbody>
    <tr>
        <td>11</td>
        <td><code><span>// Edit describes a single edit of a line-by-line diff.</span>
</code></td>
    </tr>
    <tr>
        <td>12</td>
        <td><code><span>type</span> Edit[T string | []byte] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>13</td>
        <td><code>	Op   diff.Op <span>// Edit operation</span>
</code></td>
    </tr>
    <tr>
        <td>14</td>
        <td><code>	Line T       <span>// Line, including newline character (if any)</span>
</code></td>
    </tr>
    <tr>
        <td>15</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>16</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>17</td>
        <td><code><span>// Hunk describes a sequence of consecutive edits.</span>
</code></td>
    </tr>
    <tr>
        <td>18</td>
        <td><code><span>type</span> Hunk[T string | []byte] <span>struct</span> {
</code></td>
    </tr>
    <tr>
        <td>19</td>
        <td><code>	PosX, EndX int       <span>// Start and end line in x (zero-based).</span>
</code></td>
    </tr>
    <tr>
        <td>20</td>
        <td><code>	PosY, EndY int       <span>// Start and end line in y (zero-based).</span>
</code></td>
    </tr>
    <tr>
        <td>21</td>
        <td><code>	Edits      []Edit[T] <span>// Edits to transform x lines PosX..EndX to y lines PosY..EndY</span>
</code></td>
    </tr>
    <tr>
        <td>22</td>
        <td><code>}
</code></td>
    </tr>
    <tr>
        <td>23</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>24</td>
        <td><code><span>// Edits compares the lines in x and y and returns the changes necessary to convert from one to the</span>
</code></td>
    </tr>
    <tr>
        <td>25</td>
        <td><code><span>// other.</span>
</code></td>
    </tr>
    <tr>
        <td>26</td>
        <td><code><span>func</span> Edits[T string | []byte](x, y T, opts ...diff.Option) []Edit[T]
</code></td>
    </tr>
    <tr>
        <td>27</td>
        <td><code>
</code></td>
    </tr>
    <tr>
        <td>28</td>
        <td><code><span>// Hunks compares the lines in x and y and returns the changes necessary to convert from one to the</span>
</code></td>
    </tr>
    <tr>
        <td>29</td>
        <td><code><span>// other.</span>
</code></td>
    </tr>
    <tr>
        <td>30</td>
        <td><code><span>func</span> Hunks[T string | []byte](x, y T, opts ...diff.Option) []Hunk[T]
</code></td>
    </tr></tbody>
</table>
<h4 id="conclusion">Conclusion<a href="#conclusion"></a></h4>
<p>For the full API and examples for how to use it, please see the package documentation for
<a href="https://pkg.go.dev/znkr.io/diff">znkr.io/diff</a> and
<a href="https://pkg.go.dev/znkr.io/diff/textdiff">znkr.io/diff/textdiff</a>. I am certain that there are use
cases not covered by this API, but I feel confident that it can evolve to cover these use cases in
the future. For now, all my needs are fulfilled, but if you run into a situation that can't be
solved by this API or requires some contortions, please <a href="https://github.com/znkr/diff/issues/new">tell me about
it</a>.</p>
<h3 id="implementation">Implementation<a href="#implementation"></a></h3>
<p>To implement this API, we need to implement a diff algorithm. There are a couple of standard diff
algorithms that we can choose from. The choice of the algorithm as well as how it's implemented
matters for the readability of the result as well as the performance.</p>
<p>A good starting point for this project was Myers' algorithm, simply because it's the fastest
algorithm that can cover the whole API. In particular, the <code>...Func</code> variants for <code>any</code> types
instead of <code>comparable</code> can't make use of a hash map. Patience and Histogram require the use of a
hash map for an efficient implementation, so Myers' really is the only choice. Another advantage of
Myers' compared to Patience and Histogram is that it will return optimal results.</p>
<p>On the flip side, in the <a href="#existing-diff-libraries">comparison above</a>, it came out as relatively
slow compared to the patience diff algorithm and didn't produce the most readable results. It turns
out, however, that this can be mitigated and almost completely overcome for <code>comparable</code> types using
a combination of preprocessing, heuristics, and post-processing.</p>
<p>I am not going to cover the diff algorithm in detail here. There are a number of excellent articles
on the web that describe it<sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup>, but I recommend reading the
paper<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup>: All articles I have seen try to keep a distance from the theory that
makes this algorithm work, but that's not really helpful if you want to understand how and why this
algorithm works.</p>
<h4 id="preprocessing">Preprocessing<a href="#preprocessing"></a></h4>
<p>The most impactful way to improve the performance of Myers' algorithm is to reduce the problem size.
The simplest thing to do is to strip any common prefix and suffix. This is always possible and helps
a little. However, it can also reduce diff readability, because it will consume matching elements
eagerly.</p>
<p>For example, let's say we have this change:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_05.diff">example_05.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>package array
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>var m = []struct{
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="4">
                <td>4</td>
                <td>4</td>
                <td> </td>
                <td><code>    name  string
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="5">
                <td>5</td>
                <td>5</td>
                <td> </td>
                <td><code>    year  int
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="6">
                <td>6</td>
                <td>6</td>
                <td> </td>
                <td><code>}{
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="7">
                <td>7</td>
                <td>7</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="8">
                <td>8</td>
                <td>8</td>
                <td> </td>
                <td><code>        name: "Freak Out!",
</code></td>
            </tr><tr data-op="match" data-x-lineno="9" data-y-lineno="9">
                <td>9</td>
                <td>9</td>
                <td> </td>
                <td><code>        year: 1966,
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="10">
                <td>10</td>
                <td>10</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="insert" data-y-lineno="11">
                <td></td>
                <td>11</td>
                <td>+</td>
                <td><code>    {
</code></td>
            </tr><tr data-op="insert" data-y-lineno="12">
                <td></td>
                <td>12</td>
                <td>+</td>
                <td><code>        name: "Absolutely Free",
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="insert" data-y-lineno="14">
                <td></td>
                <td>14</td>
                <td>+</td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="11" data-y-lineno="15">
                <td>11</td>
                <td>15</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="16">
                <td>12</td>
                <td>16</td>
                <td> </td>
                <td><code>        name: "We're Only in It for the Money",
</code></td>
            </tr><tr data-op="match" data-x-lineno="13" data-y-lineno="17">
                <td>13</td>
                <td>17</td>
                <td> </td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="18">
                <td>14</td>
                <td>18</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="19">
                <td>15</td>
                <td>19</td>
                <td> </td>
                <td><code>}</code></td>
            </tr></tbody>
</table>
<p>If we eagerly consume the common prefix first and then the common suffix, the first 11 lines are
all identical and the so are the last 4. This in turn would result in a different diff:</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_05_strip_common_prefix_and_suffix.diff">example_05_strip_common_prefix_and_suffix.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>package array
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="2">
                <td>2</td>
                <td>2</td>
                <td> </td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="3">
                <td>3</td>
                <td>3</td>
                <td> </td>
                <td><code>var m = []struct{
</code></td>
            </tr><tr data-op="match" data-x-lineno="4" data-y-lineno="4">
                <td>4</td>
                <td>4</td>
                <td> </td>
                <td><code>    name  string
</code></td>
            </tr><tr data-op="match" data-x-lineno="5" data-y-lineno="5">
                <td>5</td>
                <td>5</td>
                <td> </td>
                <td><code>    year  int
</code></td>
            </tr><tr data-op="match" data-x-lineno="6" data-y-lineno="6">
                <td>6</td>
                <td>6</td>
                <td> </td>
                <td><code>}{
</code></td>
            </tr><tr data-op="match" data-x-lineno="7" data-y-lineno="7">
                <td>7</td>
                <td>7</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="8" data-y-lineno="8">
                <td>8</td>
                <td>8</td>
                <td> </td>
                <td><code>        name: "Freak Out!",
</code></td>
            </tr><tr data-op="match" data-x-lineno="9" data-y-lineno="9">
                <td>9</td>
                <td>9</td>
                <td> </td>
                <td><code>        year: 1966,
</code></td>
            </tr><tr data-op="match" data-x-lineno="10" data-y-lineno="10">
                <td>10</td>
                <td>10</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="11" data-y-lineno="11">
                <td>11</td>
                <td>11</td>
                <td> </td>
                <td><code>    {
</code></td>
            </tr><tr data-op="insert" data-y-lineno="12">
                <td></td>
                <td>12</td>
                <td>+</td>
                <td><code>        name: "Absolutely Free",
</code></td>
            </tr><tr data-op="insert" data-y-lineno="13">
                <td></td>
                <td>13</td>
                <td>+</td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="insert" data-y-lineno="14">
                <td></td>
                <td>14</td>
                <td>+</td>
                <td><code>    },
</code></td>
            </tr><tr data-op="insert" data-y-lineno="15">
                <td></td>
                <td>15</td>
                <td>+</td>
                <td><code>    {
</code></td>
            </tr><tr data-op="match" data-x-lineno="12" data-y-lineno="16">
                <td>12</td>
                <td>16</td>
                <td> </td>
                <td><code>        name: "We're Only in It for the Money",
</code></td>
            </tr><tr data-op="match" data-x-lineno="13" data-y-lineno="17">
                <td>13</td>
                <td>17</td>
                <td> </td>
                <td><code>        year: 1967,
</code></td>
            </tr><tr data-op="match" data-x-lineno="14" data-y-lineno="18">
                <td>14</td>
                <td>18</td>
                <td> </td>
                <td><code>    },
</code></td>
            </tr><tr data-op="match" data-x-lineno="15" data-y-lineno="19">
                <td>15</td>
                <td>19</td>
                <td> </td>
                <td><code>}</code></td>
            </tr></tbody>
</table>
<p>Fortunately, this is easy to fix in post processing.</p>
<p>Much more impactful, but only efficiently possible for <code>comparable</code> types, is to remove all elements
that are unique to either the left side or the right side, as those must always be deletions or
insertions. Non-<code>comparable</code> types can't be keys in a hash map in Go, which is necessary for
checking uniqueness. This preprocessing step <a href="https://github.com/znkr/diff/commit/37b4470eeb45867adcae1581907770041326e1b5">reduced the runtime by up to
99%</a> for a few
real-world worst-case diffs.</p>
<p>In contrast to the suffix and prefix removal, stripping unique elements doesn't have any readability
impact.</p>
<h4 id="heuristics">Heuristics<a href="#heuristics"></a></h4>
<p>Another very impactful way to improve the performance is <em>Anchoring</em>. It is based on <a href="https://bramcohen.livejournal.com/73318.html">patience
diff</a>. The word patience is a bit misleading, because
it's too easily associated with having to wait and it doesn't describe the heuristic very well
either. It works by finding elements that are occur exactly once on both the left and the right
side. When we matching up these unique pairs we create a segmentation of the input into smaller
parts that can be analyzed individually. Even better, we're very likely to find matching lines atop
and below such a pair of unique elements. This allows us to shrink the segments by stripping common
prefixes and suffixes. This heuristic <a href="https://github.com/znkr/diff/commit/feb7bda337f269935d80ee18e703e0940f406873">reduced the runtime by up to
95%</a>. Unfortunately,
finding unique elements and matching them up requires a hash map again which means that it can only
be used for <code>comparable</code> types.</p>
<p>There are two more heuristics that are I implemented. They help for non-<code>comparable</code> types and as a
backstop when the other heuristics don't work. Their main purpose is to avoid runaway quadratic
growth. The <em>Good Diagonal</em> heuristic stops searching for a better solution if we found a solution
that's good enough and the <em>Too Expensive</em> heuristic shortcuts the search if it becomes too
expensive which reduces the worst-case complexity from 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>2</mn>
      </msup>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^2)</annotation>
  </semantics>
</math>
 to

<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
  <semantics>
    <mrow>
      <mi>𝒪︀</mi>
      <mo form="prefix" stretchy="false">(</mo>
      <msup>
        <mi>N</mi>
        <mn>1.5</mn>
      </msup>
      <mspace width="0.17em"></mspace>
      <mi lspace="0.11111em">log</mi>
      <mspace width="0.17em"></mspace>
      <mi>N</mi>
      <mo form="postfix" stretchy="false">)</mo>
    </mrow>
    <annotation encoding="application/x-tex">\mathcal{O}(N^1.5 \, \log \, N)</annotation>
  </semantics>
</math>
.</p>
<p>However, heuristics like this trade diff minimality for performance, this is not always desirable.
Sometimes, a minimal diff is exactly what's required.
<a href="https://pkg.go.dev/znkr.io/diff#Optimal"><code>diff.Optimal</code></a> disables these heuristics to always find a
minimal diff irrespective of the costs.</p>
<h4 id="post-processing">Post-processing<a href="#post-processing"></a></h4>
<p>We established before that a diff algorithm finds one of many possible solutions. Given such a
solution we can discover more solutions by it locally and then selecting the best solution according
to some metric. This is exactly how <a href="https://github.com/mhagger">Michael Haggerty's</a> indentation
heuristic works for text.</p>
<p>For any given diff, we can often slide the edits up or down in a way that doesn't change the meaning
of a diff. For example,</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_06.diff">example_06.diff</a>
</caption>
<tbody><tr data-op="match" data-x-lineno="1" data-y-lineno="1">
                <td>1</td>
                <td>1</td>
                <td> </td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>  i
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>end
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="insert" data-y-lineno="5">
                <td></td>
                <td>5</td>
                <td>+</td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="6">
                <td>2</td>
                <td>6</td>
                <td> </td>
                <td><code>  i.upcase
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="7">
                <td>3</td>
                <td>7</td>
                <td> </td>
                <td><code>end
</code></td>
            </tr></tbody>
</table>
<p>has the same meaning as</p>
<table>
<caption>
    <a href="https://github.com/znkr/flo.znkr.io/tree/main/site/diff/example_06_indent_heuristic.diff">example_06_indent_heuristic.diff</a>
</caption>
<tbody><tr data-op="insert" data-y-lineno="1">
                <td></td>
                <td>1</td>
                <td>+</td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="insert" data-y-lineno="2">
                <td></td>
                <td>2</td>
                <td>+</td>
                <td><code>  i
</code></td>
            </tr><tr data-op="insert" data-y-lineno="3">
                <td></td>
                <td>3</td>
                <td>+</td>
                <td><code>end
</code></td>
            </tr><tr data-op="insert" data-y-lineno="4">
                <td></td>
                <td>4</td>
                <td>+</td>
                <td><code>
</code></td>
            </tr><tr data-op="match" data-x-lineno="1" data-y-lineno="5">
                <td>1</td>
                <td>5</td>
                <td> </td>
                <td><code>["foo", "bar", "baz"].map do |i|
</code></td>
            </tr><tr data-op="match" data-x-lineno="2" data-y-lineno="6">
                <td>2</td>
                <td>6</td>
                <td> </td>
                <td><code>  i.upcase
</code></td>
            </tr><tr data-op="match" data-x-lineno="3" data-y-lineno="7">
                <td>3</td>
                <td>7</td>
                <td> </td>
                <td><code>end</code></td>
            </tr></tbody>
</table>
<p>We call edits that can be slid up or down <em>sliders</em>. The question is, how do we select the best
slide? Michael collected human ratings for different sliders of the same diff and used them to
develop a heuristic to match these ratings:
<a href="https://github.com/mhagger/diff-slider-tools">diff-slider-tools</a>.</p>
<p>However, this heuristic only works for text and is tuned towards code instead of prose. I decided to
make it optional. It can be enabled with the
<a href="https://pkg.go.dev/znkr.io/diff/textdiff#IndentHeuristic"><code>textdiff.IndentHeuristic</code></a> option.</p>
<h4 id="diff-representation">Diff Representation<a href="#diff-representation"></a></h4>
<p>The representation used during the execution of the diff algorithm has a surprising impact on the
algorithm performance and result readability. This is not at all obvious, and so it took me a while
to figure out that the best approach is akin to a side-by-side view of a diff: You use two <code>[]bool</code>
slices to represent the left side and the right side respectively: <code>true</code> in the left side slice
represents a deletion and on the right side an insertion. <code>false</code> is a matching element.</p>
<p>This representation has four big advantages: It can be preallocated, the order in which edits are
discovered doesn't matter, it's easy to mutate during post-processing, and it's easy to generate
other representations from it.</p>
<h2 id="open-questions">Open Questions<a href="#open-questions"></a></h2>
<ul>
<li>What exactly is the reason that two different algorithms produce different results? - I looked
into this question a little, but I haven't found a conclusive answer yet.</li>
</ul>
<h2 id="conclusion-1">Conclusion<a href="#conclusion-1"></a></h2>
<p>Diff algorithms are relatively complicated by themselves, but they pale in comparison  to what's
necessary to provide a high-quality diff library. This article tries to explain what went into
my new diff library, but there's still more that I haven't implemented yet.</p>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inflammation now predicts heart disease more strongly than cholesterol (576 pts)]]></title>
            <link>https://www.empirical.health/blog/inflammation-and-heart-health/</link>
            <guid>45430498</guid>
            <pubDate>Tue, 30 Sep 2025 20:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.empirical.health/blog/inflammation-and-heart-health/">https://www.empirical.health/blog/inflammation-and-heart-health/</a>, See on <a href="https://news.ycombinator.com/item?id=45430498">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <div>   <p>Chronic inflammation has long been known to double your risk of heart disease, but prior to now,
inflammation has never been a SMuRF: <strong>s</strong>tandard <strong>m</strong>odifiable <strong>r</strong>isk <strong>f</strong>actor for heart disease.</p>
<p>The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone
measure inflammation (specifically, hs-CRP) via a blood test:</p>
<blockquote>
<p>Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. <a href="https://www.jacc.org/doi/10.1016/j.jacc.2025.08.047">American College of Cardiology</a></p>
</blockquote>
<p>There were a many interesting bits of evidence that led to this recommendation. The whole <a href="https://www.jacc.org/doi/10.1016/j.jacc.2025.08.047">article, published in JACC</a>, is worth a read, but this blog post extracts a few of the most interesting parts — or at
least, the parts I thought were most interesting.</p>
<p>
Want to skip ahead and measure your inflammation? Empirical Health's <a href="https://www.empirical.health/product/comprehensive-health-panel?utm_source=blog">advanced heart health panel</a> includes hs-CRP, ApoB, Lp(a), and other critical heart health biomarkers.
</p>
<hr>
<h2 id="inflammation-hs-crp-is-a-stronger-predictor-of-heart-disease-than-cholesterol"><a href="#inflammation-hs-crp-is-a-stronger-predictor-of-heart-disease-than-cholesterol">Inflammation (hs-CRP) is a stronger predictor of heart disease than cholesterol</a></h2>
<p>For decades, LDL cholesterol (or <a href="https://www.empirical.health/blog/apob-blood-test/">ApoB</a>) has been the main focus of cardiovascular risk assessment. But
this chart shows hs-CRP is actually a <em>stronger</em> predictor of heart disease than LDL.</p>
<p><img alt="Inflammation vs LDL cholesterol" loading="lazy" decoding="async" fetchpriority="auto" width="407" height="620" src="https://www.empirical.health/_astro/ldl_vs_inflammation.B5MkNi52_Z2655L8.webp"></p>
<p>Why? In some ways, <strong>cholesterol has become a victim of its own success.</strong> We now screen the whole population
for high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who
end up having heart attacks have lower cholesterol than they would naturally have. This means most of
the majority of residual risk for heart attacks will be found in biomarkers that aren’t SMuRFs.</p>
<p>Inflammation (hs-CRP) is one such non-SMuRF, one perhaps one of the strongest. This is especially true
in people already on statins or those without traditional risk factors (sometimes called “SMuRF-less” patients).
In these groups, cholesterol may be well controlled, but inflammation remains a key driver of events.</p>
<p>Of course, other traditional risk factors matter <em>in addition</em> to inflammation: blood pressure, HbA1c or
insulin resistance, eGFR (kidney function), and so on.</p>
<h2 id="what-can-you-actually-do-to-lower-inflammation"><a href="#what-can-you-actually-do-to-lower-inflammation">What can you actually do to lower inflammation?</a></h2>
<p>The ACC consensus reviews a range of clinical trials testing both drugs and lifestyle interventions for lowering inflammation and reducing cardiovascular risk. Here’s a summary of the clinical trials and their results:</p>



































































































































<table><thead><tr><th>Trial Name</th><th>Drug (Class)</th><th>Sample Size (n)</th><th>Population/NYHA Functional Class</th><th>Follow-Up</th><th>Primary Endpoint</th><th>Treatment Outcome</th></tr></thead><tbody><tr><td>ATTACH</td><td>Infliximab (TNF inhibitor)</td><td>150</td><td>NYHA III/IV HF</td><td>7 mo</td><td>Clinical status (composite score)</td><td>No improvement or worsening; deaths highest in high-dose infliximab</td></tr><tr><td>ACCLAIM</td><td>IVIG</td><td>2314</td><td>NYHA II-IV HF</td><td>10.2 mo</td><td>Composite all-cause mortality and CV hospitalization</td><td>No reduction in events; trend toward benefit in NYHA III and IV</td></tr><tr><td>CANTOS</td><td>Canakinumab (anti–IL-1β)</td><td>10,061</td><td>Prior MI; hsCRP ≥2 mg/L</td><td>3.7 y (median)</td><td>Nonfatal MI, nonfatal stroke, or CV death (MACE); HF-related mortality</td><td>Reduced MACE and HF events; no effect on all-cause mortality; primary endpoint events: 3.86% vs 4.50%</td></tr><tr><td>CIRT</td><td>Methotrexate</td><td>4,786</td><td>Stable MI plus CAD</td><td>2.3 y (median)</td><td>CV event rates</td><td>No effect on CV events, inflammation, or lipids</td></tr><tr><td>CLEAR SYNERGY</td><td>Colchicine</td><td>3,056</td><td>Acute MI plus PCI</td><td>22.6 mo</td><td>Death from CV causes, recurrent MI, ischemic stroke</td><td>No significant difference in primary endpoint</td></tr><tr><td>COLCOT</td><td>Colchicine</td><td>4,745</td><td>Acute MI patients</td><td>22.6 mo</td><td>CV event rates</td><td>CV events lower than placebo</td></tr><tr><td>LoDoCo2</td><td>Colchicine</td><td>5,522</td><td>Stable CAD</td><td>28.6 mo</td><td>Composite of CV death, nonfatal MI, ischemic stroke, or ischemia-driven revasc.</td><td>CV events lower than placebo</td></tr><tr><td>GISSI-HF</td><td>Rosuvastatin (statin)</td><td>4,574</td><td>NYHA II-IV HF</td><td>3.9 y</td><td>All-cause mortality and CV hospitalization</td><td>No effect on primary endpoints</td></tr><tr><td>JUPITER</td><td>Rosuvastatin (statin)</td><td>17,802</td><td>No CVD / LDL &lt;130 mg/dL; hsCRP ≥2 mg/L</td><td>1.9 y (median)</td><td>MI, stroke, arterial revascularization, hospitalization for unstable angina, or CV death</td><td>Reduced events (HR 0.56–0.69)</td></tr><tr><td>CORONA</td><td>Rosuvastatin (statin)</td><td>5,011</td><td>NYHA II-IV HF; ischemic etiology</td><td>32.8 mo</td><td>CV death, nonfatal MI, nonfatal stroke</td><td>No effect on primary endpoint</td></tr><tr><td>OPT-CHF</td><td>Etanercept (TNF inhibitor)</td><td>1,500</td><td>NYHA II-IV HF</td><td>6 mo</td><td>Death, hospitalization, or worsening HF</td><td>No effect on primary endpoint</td></tr><tr><td>DCMP</td><td>Prednisone (corticosteroid)</td><td>84</td><td>NYHA II-IV HF; biopsy-proven myocarditis</td><td>5.7 and 12.3 mo</td><td>Improvement in LVEF, survival, or combined outcome of death or transplantation</td><td>No significant benefit</td></tr><tr><td>RENEWAL</td><td>Etanercept (TNF inhibitor)</td><td>2,048</td><td>NYHA II-IV HF</td><td>6 mo</td><td>Composite outcome of death or hospitalization</td><td>No effect on primary endpoint</td></tr></tbody></table>
<p><strong>What works</strong> to lower inflammation?</p>
<ul>
<li><strong>Statins</strong> (especially in people with high hs-CRP): Substantial reduction in events, even when LDL is normal (JUPITER trial).</li>
<li><strong>Colchicine</strong>: Reduces recurrent events in people with established heart disease (COLCOT, LoDoCo2).</li>
<li><strong>Canakinumab</strong>: Reduces events but is expensive and increases infection risk (CANTOS).</li>
<li><strong>Lifestyle</strong>: Anti-inflammatory diets (Mediterranean, DASH), regular exercise, smoking cessation, and maintaining a healthy weight all lower hs-CRP and reduce risk.</li>
</ul>
<p><strong>What doesn’t work?</strong></p>
<ul>
<li>Some anti-inflammatory drugs (methotrexate, TNF inhibitors, corticosteroids) have not shown benefit in major trials.</li>
</ul>
<hr>
<h2 id="whats-a-normal-good-or-bad-hs-crp"><a href="#whats-a-normal-good-or-bad-hs-crp">What’s a normal, good, or bad hs-CRP?</a></h2>
<p>If you’ve already measured your hs-CRP (great!), then it’s ideally below &lt;1 mg/L. hs-CRP above 3 mg/L is
high risk:</p>
<p><img alt="Inflammation vs LDL cholesterol" loading="lazy" decoding="async" fetchpriority="auto" width="638" height="456" src="https://www.empirical.health/_astro/normal-inflammation-levels.BflUT68q_2dks4G.webp"></p>
<p>(If you’re in moderate or high ranges, see the section above for what to do.)</p>
<h2 id="are-other-biomarkers-of-inflammation-relevant"><a href="#are-other-biomarkers-of-inflammation-relevant">Are other biomarkers of inflammation relevant?</a></h2>
<p>The ACC evaluated other markers: IL-6, fibrinogen, neutrophil-to-lymphocyte ratio, EPA/AA ratio, and serum amyloid A.
These have also been shown to predict cardiovascular risk, but once hs-CRP is known, don’t add more signal.</p>
<p>In other words, you’re best off simply measuring hs-CRP, and then spending money elsewhere on heart health.</p>
<h2 id="other-interesting-bits"><a href="#other-interesting-bits">Other interesting bits</a></h2>
<p>The JACC article is packed with other interesting insights. These ones were interesting:</p>
<ul>
<li><strong>Imaging biomarkers</strong> (like CT, PET, MRI, and perivascular “fat attenuation index”) can detect vascular inflammation and may help predict coronary events, but are not yet ready for routine clinical use.</li>
<li><strong>Bempedoic acid</strong> is a newer cholesterol-lowering drug that also lowers hs-CRP, but its long-term outcomes are still being studied.</li>
<li><strong>Residual inflammatory risk</strong>: Even with well-controlled LDL on statins, many people still have elevated hs-CRP and ongoing risk—so inflammation should be addressed separately from cholesterol.</li>
<li><strong>Universal hs-CRP screening</strong> is now recommended by the ACC for both people with and without established heart disease.</li>
<li><strong>Colchicine (0.5 mg/d)</strong> is now FDA-approved as an adjunct for secondary prevention in stable ASCVD, but should be avoided in people with significant kidney or liver disease.</li>
<li><strong>Novel IL-6 inhibitors</strong> are being studied as future anti-inflammatory therapies for heart disease.</li>
</ul>
<h2 id="how-to-measure-your-inflammation"><a href="#how-to-measure-your-inflammation">How to measure your inflammation</a></h2>
<p>A simple blood test for hs-CRP is widely available and inexpensive. The ACC now recommends routine hs-CRP testing for both people at risk (primary prevention) and those with established heart disease (secondary prevention).</p>
  </div> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Making sure AI serves people and knowledge stays human (117 pts)]]></title>
            <link>https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/</link>
            <guid>45430048</guid>
            <pubDate>Tue, 30 Sep 2025 19:23:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/">https://diff.wikimedia.org/2025/09/30/making-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects/</a>, See on <a href="https://news.ycombinator.com/item?id=45430048">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-174601">
	<div>
			


<div><p>At the Wikimedia Foundation, <a href="https://diff.wikimedia.org/2021/12/09/what-the-wikimedia-foundations-new-human-rights-policy-means-for-our-movement/">we believe</a> that access to knowledge is a human right. Our mission is to ensure everyone, everywhere can access and share reliable information freely and openly on Wikipedia and other Wikimedia projects. Access to free and open knowledge, supported by the fundamental right to freedom of expression, empowers people to exercise many other rights enshrined in the <a href="https://www.un.org/en/about-us/universal-declaration-of-human-rights">Universal Declaration of Human Rights</a>, including the rights to education, artistic expression, economic advancement, and political participation. Today, we are sharing <a href="https://meta.wikimedia.org/wiki/Wikimedia_Foundation_Artificial_Intelligence_and_Machine_Learning_Human_Rights_Impact_Assessment">a human rights impact assessment (HRIA) on artificial intelligence (AI) and machine learning (ML)</a> that was carried out in 2024 to help the Foundation and Wikimedia volunteer communities better understand how these technologies may affect the exercise of human rights in our ecosystem.</p><p>Wikimedia projects, and Wikimedia volunteers everywhere, occupy a unique space in today’s online information ecosystem. This ecosystem is, however, rapidly evolving. The introduction and rapid advancement of emerging technologies such as large language models (LLMs) and other kinds of generative AI introduce opportunities as well as challenges related to the creation, access, and distribution of information. Generative AI is fundamentally changing how the public seeks, receives, and imparts information and ideas online, raising novel questions about the role and responsibility of the Foundation and Wikimedia volunteer communities in this ecosystem.&nbsp;</p><p>AI and ML are neither new to Wikimedia projects nor to the Wikimedia volunteers who make these projects possible. Both the Foundation and volunteer communities have developed numerous ML tools to support volunteers in contributing, editing, and curating the ever-growing volume of knowledge across the projects as far back as 2010. Several of these tools have harnessed ML and AI to assist volunteers with frequently recurring tasks such as identifying vandalism or flagging when citations are needed. Most tools currently used were developed before the introduction of generative AI. In the age of these emerging technologies, Wikimedia volunteers are contending with new questions:</p></div>



<ul>
<li>What, if any, role should AI play in terms of the knowledge shared on Wikimedia projects?</li>



<li>Given the widespread use of generative AI on the internet, how can we protect and strengthen the accuracy and integrity of knowledge on the Wikimedia projects?&nbsp;</li>



<li>How can ML and AI tools help strengthen, not replace, what humans do best: creating, cultivating, and sharing free knowledge?</li>



<li>How can LLMs and AI tools be used to translate content into new languages, while preserving reliability and cultural nuance and context?</li>



<li>How should the volunteer communities’ policies evolve to account for such uses of these new technologies?</li>
</ul>



<h2><strong><strong><strong>About the AI/ML Human Rights Impact Assessment (HRIA)</strong></strong></strong></h2>



<p>This HRIA is the latest outcome of the Foundation’s ongoing efforts to meet our <a href="https://foundation.wikimedia.org/wiki/Policy:Wikimedia_Human_Rights_Policy">commitment</a> to protect and uphold the human rights of all those who interact with Wikimedia projects. The Foundation commissioned it to identify and analyze the impacts, opportunities, and risks emanating from the use of AI and ML technologies in the Wikimedia ecosystem. The report was written and compiled by <a href="https://taraazresearch.org/">Taraaz Research</a>, a specialized research and advocacy organization working at the intersection of technology and human rights. In developing the report, Taraaz consulted Foundation staff, individual volunteers, volunteer affiliates, civil society organizations, and external subject matter experts, though the report does not represent the views or shared consensus of any of these groups. Instead, the report offers suggestions for further inquiry, policy, and technology investment based on the state of the Wikimedia projects and technology from October 2023 to August 2024 when the research was conducted. Furthermore, the findings in the report represent potential areas of risk and opportunity. The report does not identify any actual observed risks, harms, opportunities, or benefits that have resulted from the use of ML or AI technologies on Wikimedia projects.</p>



<h2><strong><strong><strong>What are the findings of this report?</strong></strong></strong></h2>



<div><p>This report considered risks emanating from three different categories of issues relating to AI/ML on Wikimedia projects: tools developed in-house by Foundation staff to support the work of volunteer editors; Generative AI (GenAI) and its potential for marginal human rights risks in the Wikimedia context; and content on Wikimedia projects that may be used for external machine learning (ML) development.</p><p>It is important to note that the findings contained in this report reflect potential harms that could occur in the future. The report does not find that any such harms have occurred. Rather, it explains that these harms could occur if AI is employed or leveraged at scale in certain ways on Wikimedia projects without proper mitigations in place.</p><p>The report found that AI/ML tools developed by the Foundation to support volunteer editors have the potential to contribute positively to several human rights, such as freedom of expression and the right to education, among others. Nonetheless, certain risks exist that stem from known limitations of AI/ML-enabled tools: for example, the possibilities of perpetuating or amplifying existing gaps and biases in knowledge representation or incorrectly flagging or marking content for deletion. Such risks, if they were to materialize at scale, could have negative impacts on the human rights of Wikimedia volunteers.</p><p>Furthermore, the report considered in broad terms what new risks external GenAI tools could introduce to Wikimedia projects. The researchers determined that GenAI could increase the scale, speed, and sophistication of harmful content generation, including for disinformation campaigns and to attack individual Wikimedia volunteers or their communities. These tools could also automate the creation of misleading content across multiple languages simultaneously, making its detection and moderation more challenging, and play a role in generating large volumes of personalized, abusive content targeting specific individuals or communities. These risks, among others identified, could negatively affect the human rights of Wikimedia volunteers and, even, the general public if not properly mitigated.</p><p>Finally, the report considered the downstream risks of how content from Wikimedia projects are used in the training of large language models (LLMs). While the Wikimedia Foundation cannot control how freely and openly licensed content from the Wikimedia projects is used by the general public, we do have a duty to safeguard risks to human rights that could result from downstream impacts. The researchers identified concerns about how the outputs of LLMs partially trained on Wikimedia content could represent risk in terms of bias and representation, data quality and accuracy, privacy risks, and issues related to cultural sensitivity. As such, they recommended monitoring for these potential risks, although they also found that ongoing data-quality initiatives and equity-focused programs already mitigate the risks in question, since these programs address content and representation gaps across language communities.</p><p>Within each of these focus areas, the report notes that the Foundation and Wikimedia volunteer communities have also already implemented many strategies and processes to mitigate the identified risks while providing recommendations for additional mitigation measures as well. Given the prominence of Wikimedia projects in the online information ecosystem, it is critical that we consider new risks emerging from technologies as rapidly evolving and growing as AI and ML. Importantly, the discussions and conclusions in this report allow us to contemplate such potential harms early and to plan how we can best mitigate them proactively.</p></div>



<h2><strong><strong><strong>What does this HRIA report mean for the Wikimedia projects and volunteer communities?</strong></strong></strong></h2>



<div><p>Since we <a href="https://diff.wikimedia.org/2022/07/12/what-does-the-wikimedia-foundations-human-rights-impact-assessment-mean-for-the-wikimedia-movement/">published</a> our first HRIA in July 2022, the Foundation has been clear that implementing many of these reports’ recommendations requires the buy-in and collaboration of the global volunteer communities. It will take time to discuss this HRIA’s findings and recommendations with the volunteer communities in order to decide how best to work together on their implementation, but our actions will be more effective for having done so.</p><p>We are publishing this HRIA report to help the Foundation and volunteer communities explore and address the profound societal impacts that might come from the interaction of AI technologies and the Wikimedia projects in the coming years. Wikimedia communities around the world are already grappling with important decisions about how to establish clear policies for appropriate use of generative AI on the projects, or whether any such uses even exist. We hope that considering the risks and opportunities identified in this report will help guide community discussions and decisions to make sure that the projects can continue to contribute positively to the online information ecosystem and our global society.&nbsp;</p></div>



<h2><strong><strong><strong>How can Wikimedians learn more and give feedback?</strong></strong></strong></h2>



<div><p>We want to hear from you! What questions do you have? What are your thoughts on the risks and recommendations discussed in the report? What is your community already doing, or what would you like to do, to responsibly harness the benefits of AI and ML on Wikimedia projects?</p><p>Over the coming months, we will create opportunities to hear directly from your communities and you about the findings and recommendations of this report as well as your perspectives on the opportunities and risks associated with AI and ML in the Wikimedia ecosystem. You can already leave your thoughts and comments <a href="https://meta.wikimedia.org/w/index.php?title=Talk:Wikimedia_Foundation_Artificial_Intelligence_and_Machine_Learning_Human_Rights_Impact_Assessment">on the HRIA’s Talk page</a> or join us at one of the following conversations on this topic:</p></div>



<ul>
<li><a href="https://wikimedia.zoom.us/webinar/register/WN_9qwyMXETTwugi0R9uhqvEw">21 November (12:00 UTC): Global Advocacy Community Conversation Hour</a></li>



<li><a href="https://wikimedia.zoom.us/webinar/register/WN_vmFR3DM4QqqMF6I6fhLHxA">21 November (17:00 UTC): Global Advocacy Community Conversation Hour </a></li>
</ul>
				<div id="translate-post">
					<p><img src="https://diff.wikimedia.org/wp-content/themes/interconnection/assets/images/translate-post.jpg" alt="">
					</p>

					<div>
						<h2>Can you help us translate this article?</h2>

						<p>In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?</p>

													<p><a href="https://diff.wikimedia.org/wp-login.php?redirect_to=%2F2025%2F09%2F30%2Fmaking-sure-ai-serves-people-and-knowledge-stays-human-wikimedia-foundation-publishes-a-human-rights-impact-assessment-on-the-interaction-of-ai-and-machine-learning-with-wikimedia-projects%2F%23translate-post">Start translation</a>
												</p></div>
				</div>
				
		</div>

	<!-- .entry-footer -->

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing has started working on a 737 MAX replacement (297 pts)]]></title>
            <link>https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</link>
            <guid>45428482</guid>
            <pubDate>Tue, 30 Sep 2025 17:31:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df">https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df</a>, See on <a href="https://news.ycombinator.com/item?id=45428482">Hacker News</a></p>
Couldn't get https://www.wsj.com/business/airlines/boeing-has-started-working-on-a-737-max-replacement-40a110df: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Sora 2 (270 pts)]]></title>
            <link>https://openai.com/index/sora-2/</link>
            <guid>45428122</guid>
            <pubDate>Tue, 30 Sep 2025 17:04:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/sora-2/">https://openai.com/index/sora-2/</a>, See on <a href="https://news.ycombinator.com/item?id=45428122">Hacker News</a></p>
Couldn't get https://openai.com/index/sora-2/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Sora 2 (833 pts)]]></title>
            <link>https://openai.com/index/sora-2/</link>
            <guid>45427982</guid>
            <pubDate>Tue, 30 Sep 2025 16:55:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/sora-2/">https://openai.com/index/sora-2/</a>, See on <a href="https://news.ycombinator.com/item?id=45427982">Hacker News</a></p>
Couldn't get https://openai.com/index/sora-2/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Sculptor, the Missing UI for Claude Code (160 pts)]]></title>
            <link>https://imbue.com/sculptor/</link>
            <guid>45427697</guid>
            <pubDate>Tue, 30 Sep 2025 16:35:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://imbue.com/sculptor/">https://imbue.com/sculptor/</a>, See on <a href="https://news.ycombinator.com/item?id=45427697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="-1" id="___gatsby"><header><a href="https://imbue.com/"><img src="data:image/svg+xml;base64,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" alt="Imbue logo"></a><nav><a aria-current="page" href="https://imbue.com/sculptor/"><p>Sculptor</p></a><a href="https://imbue.com/our-work/"><p>Our Work</p></a><a href="https://imbue.com/company/"><p>Company</p></a><a href="https://imbue.com/careers/"><p>Careers</p></a><a href="https://imbue.com/blog/"><p>Blog</p></a></nav></header><div><hr color="pride"><div><div><p><img src="https://imbue.com/static/parallel-agents-ac8d6f0192da6b575005401e71bbe66a.png" alt="Spin up parallel agents"></p><h3>Spin up parallel agents</h3><p>Each Claude works in its own container. You get safe execution and parallel agents without the hassle of git worktrees.</p></div><div><p><img src="https://imbue.com/static/instantly-pair-d0b4bc629d9146f1be7d0930e8200813.png" alt="Instantly test agent changes"></p><h3>Instantly test agent changes</h3><p>Switch seamlessly between agents with Pairing Mode.</p></div><div><p><img src="https://imbue.com/static/merge-f3c7f16c8450713cf46eed9700091655.png" alt="Merge without conflicts"></p><h3>Merge without conflicts</h3><p>Merge the changes you like and throw out the ones you don't. Sculptor helps you resolve merge conflicts.</p></div></div></div><div><div><div><hr color="envy"><div><p>I've been moving more and more of my coding off of Cursor and on to Sculptor btw.</p>
<p>The vibes are good, and the experience has been pretty nice.</p></div></div><div><hr color="peace"><p>Sculptor lets me maintain this level of craftiness to software development without losing the edge you get from AI tools.</p></div></div><div><div><hr color="pride"><p>Wow, this is slick!!</p><div><p>Dale Hamilton</p><p>Staff Engineer, Universe</p></div></div><div><hr color="excitement"><div><p>At first I thought, 'why do I need this container?'. But when I realized Sculptor was actually solving the pain of concurrent agents on different branches, it made total sense.</p>
<p>It's like—oh wow I don't have to manage that mess anymore.</p></div><div><p>Patrick Sulin</p><p>Technical Lead, InterSystems</p></div></div></div><div><div><hr color="joy"><p>The killer feature for me is parallelization. I can kick off multiple tasks at once without spinning up a whole new environment every time. It feels like the tooling is finally here to support the kind of workflows I've always wanted.</p><div><p>Robert Starmer</p><p>CEO, Kumulus Technologies</p></div></div><div><hr color="sadness"><p>I compared Claude Code running in Max Mode with Sculptor, and Sculptor's results and overall intelligence were better. I've already merged around 5K lines—it's a great product! Kudos to you guys.</p><div><p>Andrew Gabriel</p><p>Founder, Monostate.ai</p></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A $196 fine-tuned 7B model outperforms OpenAI o3 on document extraction (192 pts)]]></title>
            <link>https://arxiv.org/abs/2509.22906</link>
            <guid>45427634</guid>
            <pubDate>Tue, 30 Sep 2025 16:31:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2509.22906">https://arxiv.org/abs/2509.22906</a>, See on <a href="https://news.ycombinator.com/item?id=45427634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2509.22906">View PDF</a>
    <a href="https://arxiv.org/html/2509.22906v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>This paper presents Extract-0, a 7-billion parameter language model specifically optimized for document information extraction that achieves performance exceeding models with parameter counts several orders of magnitude larger. Through a novel combination of synthetic data generation, supervised fine-tuning with Low-Rank Adaptation (LoRA), and reinforcement learning via Group Relative Policy Optimization (GRPO), Extract-0 achieves a mean reward of 0.573 on a benchmark of 1,000 diverse document extraction tasks, outperforming GPT-4.1 (0.457), o3 (0.464), and GPT-4.1-2025 (0.459). The training methodology employs a memory-preserving synthetic data generation pipeline that produces 280,128 training examples from diverse document sources, followed by parameterefficient fine-tuning that modifies only 0.53% of model weights (40.4M out of 7.66B parameters). The reinforcement learning phase introduces a novel semantic similarity-based reward function that handles the inherent ambiguity in information extraction tasks. This research demonstrates that task-specific optimization can yield models that surpass general-purpose systems while requiring substantially fewer computational resource.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Henrique Godoy [<a href="https://arxiv.org/show-email/90d16046/2509.22906" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 26 Sep 2025 20:34:43 UTC (1,102 KB)<br>
</p></div></div>]]></description>
        </item>
    </channel>
</rss>