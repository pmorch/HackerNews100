<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 16 May 2025 10:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[BuyMeACoffee silently dropped support for many countries, and nobody cares (204 pts)]]></title>
            <link>https://zverok.space/blog/2024-08-08-bmac-snafu.html</link>
            <guid>44002333</guid>
            <pubDate>Fri, 16 May 2025 06:23:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zverok.space/blog/2024-08-08-bmac-snafu.html">https://zverok.space/blog/2024-08-08-bmac-snafu.html</a>, See on <a href="https://news.ycombinator.com/item?id=44002333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><strong>Silent changes in payment methods on big creator funding platforms raise some unpleasant questions.</strong></p>

<p><strong>UPD Aug 13, 2024:</strong> The kind-of-official reply from BuyMeACoffee, and <a href="#upd-aug-13-2024">my response to it</a>.</p>

<p><strong>UPD Aug 14, 2024:</strong> A support email to one of the users gives BuyMeACoffee‚Äôs reasons, and also they <a href="#upd-aug-14-2024">blocked me on X (Twitter)</a>.</p>

<h2 id="what-happened">What happened</h2>

<p>Recently, many Ukrainian creators have reported problems with payouts from <a href="https://buymeacoffee.com/">BuyMeACoffee</a>, a creator funding/crowdfunding platform.</p>

<p>At first, the reported support answers were typical corporative ‚Äúwe are sorry that we don‚Äôt care,‚Äù citing ‚Äúcompliance‚Äù and ‚Äúpolicy updates.‚Äù</p>

<blockquote data-conversation="none"><p lang="uk" dir="ltr">–í –æ—Ä–∏–≥—ñ–Ω–∞–ª—ñ, —â–æ–± –≤–µ—Å—Ç–µ—Ä–Ω–∏ —á–∏—Ç–∞–ª–∏ —è–∫—ñ —Ç–∞–º —î –Ω—ñ–∫—á–µ–º–∏ <a href="https://t.co/JxoGgamYLq">pic.twitter.com/JxoGgamYLq</a></p>‚Äî Kar√• Íë≠üá∫üá¶ (@BzickOff) <a href="https://twitter.com/BzickOff/status/1819341225024983310?ref_src=twsrc%5Etfw">August 2, 2024</a></blockquote>


<p>Those reports were widely shared, and our predominantly feeling was ‚Äúshocked but not surprised‚Äù: it is not the first time when a big platform has decided that they are too busy to try to distinguish non-occupied parts of a large country with a 30+m population from occupied parts, and decided they don‚Äôt care. Just recently, for example, the Mercury payment system <strong><a href="https://x.com/United24media/status/1815707715307479244">just blacklisted the entirety of Ukraine</a></strong> alongside Iran and North Korea.</p>

<p>But in a few days, it turned out (again, from support messages shared on Twitter) that BuyMeACoffee <strong>just dropped support for Payoneer</strong> (which works in Ukraine), leaving Stripe (which doesn‚Äôt) as the <strong>only payout method</strong>.</p>

<blockquote data-conversation="none"><p lang="uk" dir="ltr">4. –ë–æ –±–∞–π –º—ñ –∫–æ—Ö–≤–µ –º–∞–ª–∞ –≤ –≥—É–∑–Ω–æ —Å–≤–æ—ó—Ö —Å–ø–æ–∂–∏–≤–∞—á—ñ–≤ —Ç–∞ –∑–∞–∫—Ä–∏–ª–∞ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–≤–æ–¥–∏—Ç–∏ –∫–æ—à—Ç–∏ –Ω–∞–ø—Ä—è–º—É –Ω–∞ –∫–∞—Ä—Ç—É —á–µ—Ä–µ–∑ Wise, –∞ —Ç–∞–∫–æ–∂ —á–µ—Ä–µ–∑ Payoneer. –Ñ–¥–∏–Ω–æ –º–æ–∂–ª–∏–≤–∏–π –º–µ—Ç–æ–¥ –≤–∏–≤–æ–¥—É —Å—Ç–∞—î —Å–µ—Ä–≤—ñ—Å Stripe —è–∫–∏–π –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∏–π –≤ –£–∫—Ä–∞—ó–Ω—ñ —Ç–∞ –¥–æ 14 —Å–µ—Ä–ø–Ω—è –≤—Å—ñ –∞–≤—Ç–æ—Ä–∏ –∑–º–æ–∂—É—Ç—å –≤–≤–æ—Å—Ç–∞–Ω–Ω—î –≤–∏–≤–µ—Å—Ç–∏ —Å–≤–æ—ó –∫–æ—à—Ç–∏. <a href="https://t.co/XIQdNHVllu">pic.twitter.com/XIQdNHVllu</a></p>‚Äî AdrianZP (@AdrianZPcity) <a href="https://twitter.com/AdrianZPcity/status/1821252119875436595?ref_src=twsrc%5Etfw">August 7, 2024</a></blockquote>


<h2 id="did-they-though">Did they, though?</h2>

<p>Currently BuyMeACoffee‚Äôs <a href="https://help.buymeacoffee.com/en/articles/6258038-supported-countries-for-payouts-on-buy-me-a-coffee">support page</a> simply states:</p>

<blockquote>
  <p>Buy Me a Coffee supports payouts to creators in several countries, facilitated through our payment provider, Stripe.</p>
</blockquote>

<p>According to the Internet Archive, in <a href="https://web.archive.org/web/20240229183241/https://help.buymeacoffee.com/en/collections/1907909-payments">February</a>, there was another link to a <a href="https://web.archive.org/web/20231028113814/https://help.buymeacoffee.com/en/articles/6258038-payment-supported-countries">page</a> that listed Stripe and Payoneer, while <a href="https://web.archive.org/web/20240521195730/https://help.buymeacoffee.com/en/collections/1907909-payments">May‚Äôs snapshot</a> already has only Stripe-related links.</p>

<p>So, the change <em>in the documentation</em> happened somewhere between February (that‚Äôs when I personally used it for the last time) and May: Internet Archive doesn‚Äôt provide other snapshots between those two dates.</p>

<p>So, at least the support‚Äôs private answers were aligned with the documentation.</p>

<p><strong>At the same time, there are a couple of funny facts about this deprecation.</strong></p>

<p><strong>First,</strong>  I know for a fact that some Ukrainian creators were receiving payouts via Payoneer somewhere around ~1 month ago; I even saw a credible claim about last week! (I just sent mine, which trickled from a moderately popular blog, will see whether it will come through; it is funny if it will. At least my configured Payoneer payout method is currently active in the BMaC dashboard.)</p>

<p>But more importantly, <strong>there was NO communication about the change</strong>. You wouldn‚Äôt find anything on <a href="https://x.com/buymeacoffee">their Twitter</a> or in the <a href="https://building.buymeacoffee.com/changelog/">public changelog</a> (screenshotted at 2024-08-08 11:05:30 GMT+0300‚Äîjust in case something will magically appear there retroactively):</p>

<p><img src="https://zverok.space/img/2024-08-08-bmac/image00.png" alt="">
<img src="https://zverok.space/img/2024-08-08-bmac/image01.png" alt="">
<img src="https://zverok.space/img/2024-08-08-bmac/image02.png" alt="">
<img src="https://zverok.space/img/2024-08-08-bmac/image03.png" alt=""></p>

<p>As you can see, <strong>there is nothing about payout updates</strong> after the <a href="https://building.buymeacoffee.com/changelog/improved-payout-experience">Nov‚Äô23 post</a>, which mentioned Payoneer and Wise as available options alongside Stripe.</p>

<p>I assure you that <strong>there was no email communication to the creators either</strong>.</p>

<h2 id="how-bad-is-this">How bad is this?</h2>

<p>The difference is public lists of supported countries between <a href="https://www.payoneer.com/resources/global-payment-capabilities/">Payoneer</a> and <a href="https://help.buymeacoffee.com/en/articles/6258038-supported-countries-for-payouts-on-buy-me-a-coffee">Stripe</a> are <strong>95 (ninety-five)</strong> countries and territories. I am not sure that all 95 of those don‚Äôt have working Stripe (say, maybe the Faroe Islands just listed on Payoneer separately while being actually served by Stripe via Danish banks?), but it seems to be <em>a lot</em> anyway.</p>

<p>I‚Äôd like to know what the experience of BMaC users from those countries is like. But what I know for sure is that in Ukraine, a lot of people use the platform as a source of income (sometimes even a primary one). My subscriptions, for example, include but are not limited to:</p>

<ul>
  <li>A singer-songwriter turned paramedic who uses BMaC donations to record her songs on short vacations between duties;</li>
  <li>A writer and culturology scholar turned soldier who funds his studies about Ukrainian culture and history;</li>
  <li>A girl whose two brothers have fallen on the frontlines, and she funds her absolutely unique book club to connect to people and share reading experience with them;</li>
  <li>A small business owner from Kharkiv who uses a ‚Äúbuild in public‚Äù approach for a small coffee shop (and recently got mobilized into Armed Forces);</li>
  <li>‚Ä¶and so on.</li>
</ul>

<p>None of them have BMaC as their primary income, and still, it is a sign of support from their peers and the possibility to continue doing what‚Äôs important to them.</p>

<p><strong>There is no way for any of us to receive the money accumulated on BMaC</strong> anymore. What looks like ‚Äútechnical‚Äù change for some American is realistically <strong>prohibitive</strong> for those not privileged enough to live in Stripe-supported countries. The money is still ‚Äúthere‚Äù, so probably no lawyer can say BMaC has ‚Äústolen them‚Äù‚Äîyou just can‚Äôt neither receive ‚Äúyour‚Äù money nor, at least, give them back to those who sent them.</p>

<p>Note that some BMaC accounts have a lot of supporters, and many of those use ‚Äúyearly‚Äù payments as a sign of their support‚Äîand all of it is currently in some technological limbo.</p>

<p>Now, I am just a mere developer, and I don‚Äôt know much about payout regulations and legal facts. I <em>might</em> charitably assume there was a good reason for dropping Payoneer support (maybe a business one, or maybe it is actually related to some government regulations). The fact itself is extremely distressing for many, but hey, that‚Äôs their business, what do we know.</p>

<p><strong>But the way this policy was implemented‚Äîno prior notice, no choice, no explicit communication of the reasons and possibilities‚Äîis absolutely fascinating, to say the least.</strong> And, honestly, support‚Äôs evasive behavior (and inconsistent reports about ‚Äúsome Payoneer payouts might still work‚Äù) just adds insult to injury.</p>

<p>I am not sure that any service that handles people‚Äôs money as their <em>primary</em> activity, and that does it this way, can be ever trusted. Just sayin‚Äô.</p>

<p>Have a good day, and have a screenshot of the latest message from BuyMeACoffee‚Äôs ever-positive Twitter!</p>

<p><img src="https://zverok.space/img/2024-08-08-bmac/image04.png" alt=""></p>

<p>Such a nice vibe! <em>(Neither their primary account nor <a href="https://x.com/jijosunny">the founder‚Äôs personal one</a>, which both have a history of replying to all relevant tweets, are currently giving any signs they are aware of the situation.)</em></p>

<p>PS: As a side note, <a href="https://en.ain.ua/2023/04/10/what-is-wrong-with-patreon-and-why-ukrainians-urge-canceling-it/">here is a story</a> about why Patreon is not an option for Ukrainians. And <a href="https://www.patreon.com/wargonzoo">here</a> is russian war correspondent‚Äôs Patreon account, mentioned in the story, still proudly present on the platform (though seemingly dormant).</p>

<h2 id="upd-aug-13-2024">UPD: Aug 13, 2024</h2>

<p>So, folks! After a week of poking, we have a kind-of-official answer from <a href="https://x.com/buymeacoffee">@buymeacoffee</a>!</p>

<p>Let‚Äôs unpack it!</p>

<blockquote data-conversation="none"><div lang="en" dir="ltr"><p>This is simply not true. Let us clarify:</p><p>1. We never hold funds. If for some reason we cannot payout (potential fraud, bank account issues), the amount is refunded.</p><p>2. Not one Ukrainian creator's payout is held. We paid out to thousands of Ukrainian creators this week. Please‚Ä¶</p></div>‚Äî Buy Me a Coffee (@buymeacoffee) <a href="https://twitter.com/buymeacoffee/status/1823085434605605055?ref_src=twsrc%5Etfw">August 12, 2024</a></blockquote>


<p><strong>0.</strong> For some reason, this answer is a reply to one of those repeatedly asked them. Not even a standalone X post, nor a blog/changelog entry, nor a newsletter to creators.</p>

<p>So, if you aren‚Äôt a creator who constantly buggers them on X-formerly-Twitter (or follows all the discussions), you might still be unaware of the policy change or the existence of some answers.</p>

<p>The official feeds are still clear of the topic.</p>

<blockquote>
  <ol>
    <li>We never hold funds. If [‚Ä¶] we cannot payout [‚Ä¶] the amount is refunded.</li>
  </ol>
</blockquote>

<p>I believe this <strong>intention</strong> (well, anything less would be a legal suicide), but I would still like to hear:
a. How the ‚Äúcannot payout‚Äù decided and whether ‚Äúthe creator doesn‚Äôt have payout method we like, they should try another one‚Äù qualifies
b. A success story of real refunding (say, $5 to thousands of people) and whether it really works or just a claim.</p>

<blockquote>
  <ol>
    <li>Not one Ukrainian creator‚Äôs payout is held. [‚Ä¶] Please reply with the creator @ if you know of any.</li>
  </ol>
</blockquote>

<p>I <strong>want to believe</strong> that. In my original thread, and many other threads and discussions, I saw a LOT of people with negative experience of interaction with support.</p>

<p>Hopefully, <strong>ALL</strong> of Ukrainian creators who reported payout problems and unhelpful support responses are paid now? (And <strong>hopefully</strong>, it is not by means of ‚Äújust find an EU friend who will agree to receive a payment for you‚Äù?.. Because I saw stories like that).</p>

<p>Also, dear BuyMeACoffee, ‚Äúplease @-us if you know a creator who has problems‚Äù would be <strong>a lot more persuasive</strong> if it wasn‚Äôt in the effing <em>replies under just one tweet</em>.</p>

<p>Just sayin‚Äô</p>

<blockquote>
  <ol>
    <li>Almost all of our creators use our standard payout method powered by Stripe.</li>
  </ol>
</blockquote>

<p>Yeah, the so-called ‚ÄúFirst World‚Äù. But I get it. I do.</p>

<blockquote>
  <p>We‚Äôve decided to focus on this for [‚Ä¶] reasons which are important for the sustainability of the business.</p>
</blockquote>

<p>That‚Äôs your business, and that‚Äôs your right to do so. As inconvenient as it might be for us, we have no say here. But the way it was communicated‚Ä¶</p>

<blockquote>
  <p>we‚Äôre planning to continue offering an alternative payout method [‚Ä¶] for countries that are not supported. As we figure this out, we can confirm that it is business as usual for creators from Ukraine</p>
</blockquote>

<p>It would be such a nice message if communicated clearly &amp; timely üôè</p>

<blockquote>
  <p>We‚Äôd appreciate it if you stop spamming our socials, spreading misinformation about us such as ‚Äúwe blocked withdrawals to Ukraine‚Äù. We‚Äôll have no option but to block you and move on.</p>
</blockquote>

<p>You know what?</p>

<p><strong>We‚Äôd appreciate</strong> if you would be clear, honest, and public about the changes you make.</p>

<p><strong>We‚Äôd appreciate</strong> your support not changing the story from the initial ‚Äúit‚Äôs compliance standards‚Äù, to further ‚Äúyour payout method is not supported, it is your problem‚Äù to final (after the public outrage) ‚Äúoh of course we would help everyone üôè‚Äù.</p>

<p><strong>We‚Äôd appreciate</strong> the public communication of the situation (and no, the reply in one thread in X that ends with passive aggression is not it, thank you very much!)</p>

<p><strong>We‚Äôd appreciate</strong> having this convo, like, a couple of weeks ago.</p>

<p><strong>In conclusion</strong>, I don‚Äôt believe BMaC being malicious or scammy in this situation (and I <em>do want</em> to believe they are looking for a solution that will work for us, too). But the amount of neglect and disregard that was displayed in the way the change was implemented and communicated, and in the interactions with support, makes it almost a textbook case of <strong>bad communication</strong>.</p>

<h2 id="upd-aug-14-2024">UPD: Aug 14, 2024</h2>

<p>One of the Ukrainian creators received an unusually honest explanation of the reasons behind dropping support for Payoneer and Wise: ‚ÄúIncompatible with our upcoming features.‚Äù So, like, a bit more convenience for the First World users, at a price of excluding those not that privileged. (Note also that in this communication there is no sign of that promise of that ‚Äúplanning to continue offering an alternative payout method [‚Ä¶] for countries that are not supported‚Äù which was sweetening their <strong>public</strong> communication.)</p>

<blockquote><div lang="en" dir="ltr"><p>I am disabling <a href="https://twitter.com/buymeacoffee?ref_src=twsrc%5Etfw">@buymeacoffee</a> account tomorrow.</p><p>The company is irresponsibly punches its Ukrainian clients in the guts by disabling Wise and making payouts impossible.</p><p>This isn‚Äôt just about switching platforms; each Ukrainian creator will lose their entire subscriber base. This‚Ä¶ <a href="https://t.co/BPnuAgcutk">pic.twitter.com/BPnuAgcutk</a></p></div>‚Äî Kate from Kharkiv (@BohuslavskaKate) <a href="https://twitter.com/BohuslavskaKate/status/1823303795322822679?ref_src=twsrc%5Etfw">August 13, 2024</a></blockquote>


<p><strong>And another update:</strong> After several days of persistent, yet polite, poking their public X account, asking for public statement (more public than a reply in a thread), BuyMeACoffee <a href="https://x.com/zverok/status/1823757570240340466">just blocked me</a>, reportedly ‚Äúfor spreading misinformation‚Äù:</p>

<p><img src="https://zverok.space/img/2024-08-08-bmac/image05.png" alt=""></p>

<p>Note there is new, previously never stated, pieces of information: about Payoneer working till Nov.1‚Äînot ‚Äúdo your last payout in August,‚Äù as it was previously; and about Wise which they will ‚Äúcontinue to offer‚Äù (currently, to the best of my understanding, they aren‚Äôt).</p>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama's new engine for multimodal models (214 pts)]]></title>
            <link>https://ollama.com/blog/multimodal-models</link>
            <guid>44001087</guid>
            <pubDate>Fri, 16 May 2025 01:43:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/blog/multimodal-models">https://ollama.com/blog/multimodal-models</a>, See on <a href="https://news.ycombinator.com/item?id=44001087">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p><img src="https://ollama.com/public/blog/multimodal.png" alt="multimodality Ollama"></p>

<p>Ollama now supports multimodal models via Ollama‚Äôs new engine, starting with new vision multimodal models:</p>

<ul>
<li><a href="https://ollama.com/library/llama4">Meta Llama 4</a></li>
<li><a href="https://ollama.com/library/gemma3">Google Gemma 3</a></li>
<li><a href="https://ollama.com/library/qwen2.5vl">Qwen 2.5 VL</a></li>
<li><a href="https://ollama.com/library/mistral-small3.1">Mistral Small 3.1</a></li>
<li>and more <a href="https://ollama.com/search?c=vision">vision models</a>.</li>
</ul>

<h2>General Multimodal Understanding &amp; Reasoning</h2>

<h3>Llama 4 Scout</h3>

<pre><code>ollama run llama4:scout
</code></pre>

<p>(Note: this is a 109 billion parameter, mixture-of-experts model.)</p>

<p>Example: asking location-based questions about a video frame:</p>

<p><img src="https://ollama.com/public/blog/multimodal-example1-min.png" alt="video frame analysis example"></p>

<p><img src="https://ollama.com/public/blog/multimodal-example1-answer-min.png" alt="video frame analysis example answer"></p>

<p>You can then ask follow-up questions:</p>

<pre><code>ollama@ollamas-computer ~ % ollama run llama4:scout
&gt;&gt;&gt; what do you see in this image?  /Users/ollama/Downloads/multimodal-example1.png 
Added image '/Users/ollama/Downloads/multimodal-example1.png'
The image depicts a scenic waterfront area with a prominent clock tower at its center. The clock tower, likely the Ferry Building Clock Tower in San Francisco, features a tall structure with a clock face on each side and an American flag flying atop. 

In the foreground, there are several buildings, including what appears to be a marketplace or shopping center, with a parking lot and a street lined with cars. The background showcases a large body of water, possibly a bay or harbor, with a bridge visible in the distance. The sky above is blue with white clouds, suggesting a daytime setting with pleasant weather conditions.

Overall, the image presents a picturesque view of a waterfront area, highlighting the iconic clock tower and the surrounding architecture, as well as the natural beauty of the bay and sky.

&gt;&gt;&gt; how far is this building from the stanford campus in palo alto? 
The distance between the Ferry Building in San Francisco and the Stanford University campus in Palo Alto is approximately 35 miles (56 kilometers). The drive typically takes around 45-60 minutes, depending on traffic conditions.

&gt;&gt;&gt; what's the best way to get there? 
The best way to get to Stanford University from the Ferry Building in San Francisco depends on your preferences and budget. Here are a few options:

1. **By Car**: Take US-101 South to CA-85 South, then continue on CA-101 South. The journey takes around 45-60 minutes, depending on traffic.

2. **By Train and Shuttle**: Take Caltrain from the San Francisco Station (4th and King Streets) to the Palo Alto Station. From there, you can take the Stanford University shuttle (Red and White) to campus. The total travel time is around 1 hour and 15 minutes.

3. **By Ride-hailing Services**: Uber or Lyft from San Francisco to Stanford University takes around 45-60 minutes, depending on traffic.

Please check for any service disruptions or schedule changes before your trip.
</code></pre>

<h3>Gemma 3</h3>

<pre><code>ollama run gemma3
</code></pre>

<p>Example of giving multiple images and asking about their relations.</p>

<p>This demonstrates how a user can input multiple images at once, or do so via follow up prompts and ask about the images.</p>

<p><img src="https://ollama.com/public/blog/multi-images.png" alt="multiple images"></p>

<p><img src="https://ollama.com/public/blog/gemma3-answer.png" alt="gemma 3 answers"></p>

<pre><code>ollama@ollamas-computer ~ % ollama run gemma3
&gt;&gt;&gt; /Users/ollama/Downloads/img1.png /Users/ollama/Downloads/img2.png /Users/ollama/Downloads/img3.png /Users/
... ollama/Downloads/img4.png tell me what animal appears in all 4 images? 

Added image '/Users/ollama/Downloads/img1.png'
Added image '/Users/ollama/Downloads/img2.png'
Added image '/Users/ollama/Downloads/img3.png'
Added image '/Users/ollama/Downloads/img4.png'

The animal featured in all four images is a **llama**.

&gt;&gt;&gt; is there a marine mammal in one of the images? 
Yes! You are absolutely right to spot that. 

In the first image, there‚Äôs a **dolphin** (whale) boxing with the llama.

&gt;&gt;&gt; in the boxing image, is the llama more likely to win or the whale? 

That‚Äôs a fantastic question! Given the image, the llama is *much* more likely to win. 

Here‚Äôs why:

*   **Punching Power:** The llama is clearly throwing a punch with a closed fist, demonstrating intent and force.

*   **Whale‚Äôs Reaction:** The whale is clearly recoiling in pain and fear.

It's a comical mismatch, but the llama has the upper hand in this boxing match!
</code></pre>

<h2>Document scanning</h2>

<h3>Qwen 2.5 VL</h3>

<pre><code>ollama run qwen2.5vl
</code></pre>

<p>Example of using Qwen 2.5 VL for character recognition.</p>

<p><img src="https://ollama.com/public/blog/check-example.jpg" alt="check example"></p>

<p><img src="https://ollama.com/public/blog/check-example-answer-min.png" alt="check answer"></p>

<p>Example understanding and translating vertical Chinese spring couplets to English.</p>

<p><img src="https://ollama.com/public/blog/spring-couplets.jpg" alt="spring couplets example"></p>

<p><img src="https://ollama.com/public/blog/spring-couplets-answer.png" alt="spring couplets answer"></p>

<h2>Ollama‚Äôs new multimodal engine</h2>

<p><img src="https://ollama.com/public/blog/ollama-car.png" alt="Ollama‚Äôs new car"></p>

<p>Ollama has so far relied on the <a href="https://github.com/ggml-org/llama.cpp">ggml-org/llama.cpp</a> project for model support and has instead focused on ease of use and model portability.</p>

<p>As more multimodal models are released by major research labs, the task of supporting these models the way Ollama intends became more and more challenging.</p>

<p>We set out to support a new engine that makes multimodal models first-class citizens, and getting Ollama‚Äôs partners to contribute more directly the community - the GGML tensor library.</p>

<p><strong>What does this mean?</strong></p>

<p>To sum it up, this work is to improve the reliability and accuracy of Ollama‚Äôs local inference, and to set the foundations for supporting future modalities with more capabilities - i.e. speech, image generation, video generation, longer context sizes, improved tool support for models.</p>

<p>Let‚Äôs break down a couple specific areas:</p>

<h4>Model modularity</h4>

<p>Our goal is to confine each model‚Äôs ‚Äúblast radius‚Äù to itself‚Äîimproving reliability and making it easier for creators and developers to integrate new models.</p>

<p>Today, <em>ggml/llama.cpp</em> offers first-class support for text-only models. For multimodal systems, however, the <strong>text decoder</strong> and <strong>vision encoder</strong> are split into separate models and executed independently. Passing image embeddings from the vision model into the text model therefore demands model-specific logic in the orchestration layer that can break specific model implementations.</p>

<p>Within Ollama, each model is fully self-contained and can expose its own projection layer, aligned with how that model was trained. This isolation lets model creators implement and ship their code without patching multiple files or adding cascading <code>if</code> statements. They no longer need to understand a shared multimodal projection function or worry about breaking other models‚Äîthey can focus solely on their own model and its training.</p>

<p>Examples of how some models are implemented are available on <a href="https://github.com/ollama/ollama/tree/main/model/models">Ollama‚Äôs GitHub repository</a>.</p>

<h4>Accuracy</h4>

<p>Large images produce large number of tokens which may exceed the batch size. Processing this correctly with the right positional information is challenging specifically when a single image crosses boundaries.</p>

<p>Ollama adds metadata as it processes images to help improve accuracy.</p>

<p>Some examples:</p>

<ul>
<li><p>Should causal attention be on / off?</p></li>

<li><p>Is it possible to split the image embeddings into batches for processing, and if possible, what are the boundaries when accounting for quality of output, and the computer being used for inference? If an image is split in the wrong place, the quality of output goes down. This is usually defined by the model, and can be checked in its paper?</p></li>
</ul>

<p>Many other local inference tools implement this differently; while a similar result may be achieved, it does not follow how the models were designed and trained.</p>

<h4>Memory management</h4>

<p><strong>Image caching</strong></p>

<p>Once an image is processed, Ollama caches it so later prompts are faster; the image remains in cache while it is still being used and is not discarded for memory-cleanup limits.</p>

<p><strong>Memory estimation &amp; KV cache optimizations</strong></p>

<p>Ollama collaborates with hardware manufacturers and an operating system partner to make sure the correct hardware metadata is detected for Ollama to better estimate and optimize for memory usage. For many firmware releases, partners will validate/test it against Ollama to minimize regression and to benchmark against new features.</p>

<p>Ollama has some KV cache optimizations to improve how memory can be efficiently used. Ollama configures causal attention at the individual model level instead of configuring as a group.</p>

<p>Examples:</p>

<ul>
<li><p>Google DeepMind‚Äôs Gemma 3 leverages sliding window attention, and Ollama can leverage that to allocate a subset or a portion of the model‚Äôs context length to improve performance, and because of the memory efficiency, this means we can increase the context length of the model on the same system or use the remaining memory for higher concurrency.</p></li>

<li><p>To uniquely support <strong>Meta‚Äôs Llama 4 Scout and Maverick models</strong>, Ollama has implemented chunked attention, attention tuning to support longer context size, specific 2D rotary embedding, and in the mixture-of-experts type of model.</p></li>
</ul>

<p>If a model‚Äôs attention layer isn‚Äôt fully implemented, such as sliding window attention or chunked attention, it may still <em>‚Äòwork‚Äô</em>. However, because this isn‚Äôt how the model was trained, the end user may begin to see erratic or degraded output by the model itself over time. This becomes especially prominent the longer the context / sequence due to cascading effects.</p>

<h3>What‚Äôs next</h3>

<ul>
<li>Support longer context sizes</li>
<li>Support thinking / reasoning</li>
<li>Tool calling with streaming responses</li>
<li>Enabling computer use</li>
</ul>

<h3>Acknowledgements</h3>

<p><strong>Models</strong></p>

<p>Thank you to the teams working to release openly available and powerful vision models - Google DeepMind, Meta Llama, Alibaba Qwen, Mistral, IBM Granite, and the many countless people &amp; labs in the community.</p>

<p><strong>GGML</strong></p>

<p>Thank you to the GGML team for the tensor library that powers Ollama‚Äôs inference ‚Äì accessing GGML directly from Go has given a portable way to design custom inference graphs and tackle harder model architectures not available before in Ollama.</p>

<p><strong>Hardware partners</strong></p>

<p>Thank you to hardware partners for collaborating and helping to improve inference on a large number of devices - NVIDIA, AMD, Qualcomm, Intel, and Microsoft.</p>

    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I was a Theranos whistleblower. Here's what I think Elizabeth Holmes is up to (108 pts)]]></title>
            <link>https://www.statnews.com/2025/05/15/theranos-whistleblower-tyler-shultz-commentary-elizabeth-holmes-billy-evans-haemanthus-startup/</link>
            <guid>44001041</guid>
            <pubDate>Fri, 16 May 2025 01:34:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.statnews.com/2025/05/15/theranos-whistleblower-tyler-shultz-commentary-elizabeth-holmes-billy-evans-haemanthus-startup/">https://www.statnews.com/2025/05/15/theranos-whistleblower-tyler-shultz-commentary-elizabeth-holmes-billy-evans-haemanthus-startup/</a>, See on <a href="https://news.ycombinator.com/item?id=44001041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I always knew Elizabeth Holmes would have a second act. But I‚Äôm shocked it‚Äôs starting while she‚Äôs still behind bars.</p>



<p>Elizabeth is now serving an 11-year sentence for defrauding investors in Theranos and has been <a href="https://www.statnews.com/2024/01/19/elizabeth-holmes-theranos-banned-government-health-programs/">banned</a> from participating in federal health programs. The company was built on her claim that, at 19 years old, she had invented a device capable of running hundreds of tests from just a single drop of blood. It was a bold vision that captivated the world, and a story everyone wanted to believe ‚Äî <a href="https://www.statnews.com/2019/05/01/from-protegee-to-whistleblower-a-former-theranos-scientist-says-elizabeth-holmes-should-come-forward-and-apologize/">including me</a>.</p>



<p>Now her partner, Billy Evans, has founded a company called <a href="https://www.statnews.com/2025/05/12/elizabeth-holmes-theranos-partner-new-biotech-startup-haemanthus-analysts-reaction/">Haemanthus</a>, which also touts a diagnostic blood-testing device that uses only small amounts of blood. While Elizabeth has no legal affiliation with the company, her fingerprints and pricks are all over it. Even the naming convention feels familiar: Instead of combining the Greek roots of therape√≠a and diagnosis like she did with Theranos, this time it‚Äôs haema and anthos<em> </em>‚Äî Greek for ‚Äúblood‚Äù and ‚Äúflower.‚Äù&nbsp;</p>


<figure><a href="https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434.jpg"><img decoding="async" width="1024" height="1536" src="https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-1024x1536.jpg" alt="" srcset="https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-1024x1536.jpg 1024w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-328x492.jpg 328w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-768x1152.jpg 768w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-500x750.jpg 500w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-2048x3072.jpg 2048w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-1600x2400.jpg 1600w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-640x960.jpg 640w, https://www.statnews.com/wp-content/uploads/2025/05/STAT_HealthTech_2022_434-384x576.jpg 384w" sizes="(min-width: 625px) and (max-width: 767px) 719px, (min-width: 768px) and (max-width: 899px) 835px, (min-width: 900px) and (max-width: 1023px) 635px, (min-width: 1024px) and (max-width: 1199px) 803px, (min-width: 1200px) and (max-width: 1439px) 994px, (min-width: 1440px) 982px, 576px"></a><figcaption><span>Tyler Shultz</span><wbr><cite>STAT</cite></figcaption></figure>



<p>Since she reported to prison about two years ago, by far the most common question people have asked me is: ‚ÄúDid Elizabeth start Theranos with the intention to revolutionize health care, or did she intend to commit fraud from the start?‚Äù I think the answer is neither. While I wasn‚Äôt there in the early days, I believe Elizabeth started Theranos in the pursuit of fame. She wanted to be the female Steve Jobs. She wanted to be known as the youngest self-made female billionaire, fly in private jets, advise the president, rub shoulders with Mark Zuckerberg and Elon Musk. She wanted to be on the cover of magazines. And she got what she wanted. Just a few months ago, she was on the cover of People magazine. Let me repeat: Even from prison, she‚Äôs landing on the cover of People magazine.</p>



<p>As the news of Haemanthus broke, I started to hear a similar question: ‚ÄúDo you think Haemanthus intends to revolutionize health care, or do you think it‚Äôs another fraud?‚Äù Once again ‚Äî neither. I think it‚Äôs just another chapter of her narrative.&nbsp;</p>



<p>Anyone who thought this story was over doesn‚Äôt know Elizabeth and her ability to craft a narrative. When she leaves prison, there‚Äôs no doubt she‚Äôll manage to appear remorseful without admitting guilt. And if she‚Äôs credited with launching a company from a prison cell? That‚Äôs more legendary than the Hewlett-Packard garage or Mark Zuckerberg‚Äôs dorm room. It‚Äôs a story so compelling, people will want it to be true.</p>



<p>Elizabeth is more than a masterful storyteller. She writes the story. Every detail is meticulously chosen to build her narrative, captivate her audience, and distort reality. The black turtleneck, deep voice, college dropout ‚Äî the parallels with Steve Jobs were no accident.&nbsp;</p>





<p>The very first time I met her, Elizabeth told my grandfather, George Shultz (who served in the&nbsp;Cabinet in the Nixon and Reagan administrations and became a member of the Theranos board), and then-Sen. Dianne Feinstein that Quest and LabCorp would eventually try to take her down. She was planting the seeds of a conspiracy years before she would need to lean on it. She also told us Theranos would never go public, preempting pressure from investors. She completely controlled the narrative from day one.&nbsp;</p>



<p>The Elizabeth I knew would openly talk about her decision to not have friends, to never date, to never have kids, all because she was devoted to Theranos. (This, of course, was another lie. She was dating Sunny Balwani, the COO and president of the company.) However, when motherhood made for a useful character reboot, she became one ‚Äî twice. Her first child arrived just days before her trial was set to begin, and her second between her conviction and sentencing.&nbsp;</p>



<p>Before her sentencing, the <a href="https://www.nytimes.com/2023/05/07/business/elizabeth-holmes-theranos-interview.html" target="_blank" rel="noopener">New York Times</a> wrote about how she had become ‚ÄúLiz,‚Äù the devoted mother and volunteer who had ditched the black turtleneck. The article even included carefully posed photographs of Elizabeth with her two children. I feel profound sadness for those kids. Most parents would try to shield their children from that type of attention, but Elizabeth put them front and center, using them the same way she used to use her black turtleneck. They were props to support the narrative she was writing.&nbsp;</p>



<p>Similarly, I suspect that Haemanthus‚Äô purpose is not to build a product that will improve the lives of people (or their pets ‚Äî materials for potential investors say that they plan to start with veterinary medicine). Nor is it to make VC returns.&nbsp;</p>


		
		


<p>The purpose is to build something that works, so that she can point to it and say, ‚ÄúSee, I would have figured it out. Theranos was never a fraud.‚Äù If Haemanthus‚Äô technology does indeed work and she eventually makes that claim, whatever reporter she‚Äôs chosen to interview her needs to ask, ‚ÄúHow many tens of thousands of people were you willing to hurt on the path to building something that worked?‚Äù&nbsp;</p>



<p>This raises another question I‚Äôve been asked this week: ‚ÄúDo you think the technology will work?‚Äù Probably not, but I don‚Äôt have enough information to really know. Raman spectroscopy has been used for nearly 100 years and hasn‚Äôt achieved the resolution needed to perform this type of blood testing from a few drops of blood. Maybe two additional light sources, as described in Haemanthus‚Äô patent, combined with AI solves the resolution problems, but consider me a skeptic until a third party validates its method. Over the past few years a number of people have reached out to me saying they‚Äôve invented ‚Äúthe Theranos that works,‚Äù and several of them have concepts similar to this: apply AI to a mountain of noise and get a thousand test results. While many actually do have innovative approaches, none have come close to realizing the Theranos vision.&nbsp;</p>





<p>It‚Äôs been reported that Haemanthus aims to raise $50 million, and I have little doubt it‚Äôll succeed. Billy Evans is independently wealthy, an heir to a Southern California hotel chain, so it‚Äôs likely a significant portion will be self-funded. And I can think of at least one outside investor who might get involved. Tim Draper, Elizabeth‚Äôs former neighbor and one of Theranos‚Äô earliest backers, has maintained an unusual and unwavering loyalty to her. Even after her conviction, he continues to publicly defend her. Draper seems to be as invested in the narrative as she is, and is in a position to fund a company simply for the sake of that narrative.&nbsp;</p>



<p>My advice to prospective Haemanthus investors is this: Ask yourself what motivates Elizabeth Holmes. Money? Possibly. But when she walks out of prison, she‚Äôll be returning to a partner who‚Äôs already a millionaire many times over. Mission? I think she‚Äôs thoroughly proven that she doesn‚Äôt care about improving the lives of patients. The real question is: Will Elizabeth serve the purpose of your fund ‚Äî or by investing in her, will you simply be serving her purpose of writing her rise-from-the-ashes narrative?</p>



<p>Failed founders deserve second chances. But at Theranos, Elizabeth had a thousand chances ‚Äî opportunities to listen to her scientists, to tell the truth to her board and investors, to change course and right the ship. At each opportunity she consistently showed poor judgement and a lack of character. Even when the Wall Street Journal began reporting on the company, she still had hundreds of millions in the bank, world-class scientists on payroll, and the most prestigious board in Silicon Valley. In that moment, she had more resources, talent, and opportunity than most founders will ever see ‚Äî and she blew it.</p>



<p>She‚Äôs not Edison. This isn‚Äôt going to work on the thousand-and-first try.</p>



<p><em><a href="https://www.tyler-shultz.com/" target="_blank" rel="noopener">Tyler Shultz</a> is a scientist, founder, and whistleblower best known for exposing fraud at Theranos and advocating for ethics in innovation.</em></p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Teal ‚Äì A statically-typed dialect of Lua (136 pts)]]></title>
            <link>https://teal-language.org/</link>
            <guid>44000759</guid>
            <pubDate>Fri, 16 May 2025 00:40:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://teal-language.org/">https://teal-language.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44000759">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="below-the-fold">

<!---------------------------------------------------------------------------->

<h2>About Teal</h2>
<p>
Teal is a statically-typed dialect of <a href="https://www.lua.org/">Lua</a>.
It extends Lua with type annotations, allowing you to specify arrays, maps and records,
as well as interfaces, union types and generics.
</p>
<p>
It aims to fill a niche similar to that of TypeScript in the JavaScript world,
but adhering to Lua's spirit of minimalism, portability and embeddability.
</p>
<p>
Is it implemented as a compiler, <a href="https://github.com/teal-language/tl">tl</a>,
which compiles <tt>.tl</tt> source code into <tt>.lua</tt> files.
</p>

<!---------------------------------------------------------------------------->

<h2>A taste of Teal</h2>
<p>
Here is a quick taste of what Teal code looks like:
</p>
<pre><b>local function</b> add(a: <span>number</span>, b: <span>number</span>): <span>number</span>
   <b>return</b> a + b
<b>end</b>

<b>local</b> s = add(1, 2)
print(s)
</pre>
<p>
...or check out this page's source code!
</p>
<p>
You can also play with the Teal compiler right from your browser,
using the <a href="https://teal-playground.netlify.app/" target="_blank">Teal Playground</a>.
</p>

<!---------------------------------------------------------------------------->

<h2>Install Teal</h2>
<p>
If you're using <a href="https://luarocks.org/">LuaRocks</a>, you can install the compiler with:
</p>
<pre>luarocks install tl
</pre>
<p>
You can also find <a href="https://github.com/teal-language/tl/releases">pre-compiled
binaries</a> for Linux and Windows.</p>
<p>
To build larger projects, you probably won't want to run <tt>tl</tt> on each
file individually. We recommend using <a href="https://github.com/teal-language/cyan">Cyan</a>,
the build tool designed for Teal.
</p>
<p>
You can also install <a href="https://github.com/teal-language/vscode-teal/">vscode-teal</a> for
Visual Studio Code integration, <a href="https://github.com/teal-language/teal-language-server/">teal-language-server</a>
for NeoVim and others, and <a href="https://github.com/teal-language/awesome-teal">more</a>.
</p>

<!---------------------------------------------------------------------------->

<h2>Documentation</h2>
<p>
You can browse the Teal documentation <a href="https://teal-language.org/book/index.html">online</a>.
</p>
<p>
Here are some recorded talks discussing the history of Lua and types,
outlining the motivations behind Teal and talking about the project's progress:
</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=OPyBQRndLUk">Minimalism versus types (2019)</a></li>
<li><a href="https://www.youtube.com/watch?v=HfnjUCRzRKU">Minimalistic typed Lua is here (2020)</a></li>
<li><a href="https://www.youtube.com/watch?v=OqXbnaDR8QY">What's next for Teal (2021)</a></li>
<li><a href="https://www.youtube.com/watch?v=OPyBQRndLUk">Five years of Teal: minimalism versus growth in language design (2024)</a></li>
<li><a href="https://www.youtube.com/watch?v=Uq_8bckDxaU">What should Teal be? - musings on FOSS project directions (2025)</a></li>
</ul>



<!---------------------------------------------------------------------------->

<h2>Community</h2>
<p>
Development happens on <a href="https://github.com/teal-language">GitHub</a>.
</p>
<p>
We have a <a href="https://github.com/teal-language/tl/discussions">community forum</a> hosted by GitHub.
You can also chat with us via Matrix at <a href="https://matrix.to/#/#teal-language_community:gitter.im">#teal-language_community:gitter.im</a>.
</p>
<p>
Teal is a project started by <a href="https://hisham.hm/">Hisham Muhammad</a>,
developed by a <a href="https://github.com/teal-language/tl/graphs/contributors">growing number of contributors</a>
and is written using Teal itself!
</p>

<!---------------------------------------------------------------------------->

<h2>License</h2>
<p>
Teal is free and open source software.<br>Its <a href="https://github.com/teal-language/tl/blob/master/LICENSE">license</a> is MIT, the same as Lua.
</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA keeps ancient Voyager 1 spacecraft alive with Hail Mary thruster fix (283 pts)]]></title>
            <link>https://www.theregister.com/2025/05/15/voyager_1_survives_with_thruster_fix/</link>
            <guid>44000700</guid>
            <pubDate>Fri, 16 May 2025 00:29:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/05/15/voyager_1_survives_with_thruster_fix/">https://www.theregister.com/2025/05/15/voyager_1_survives_with_thruster_fix/</a>, See on <a href="https://news.ycombinator.com/item?id=44000700">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>NASA has revived a set of thrusters on the nearly 50-year-old Voyager 1 spacecraft after declaring them inoperable over two decades ago.&nbsp;</p>
<p>It's a nice long-distance engineering win for the team at NASA's Jet Propulsion Laboratory, responsible for keeping the <a href="https://www.theregister.com/2024/11/09/voyager_closes_in_on_a/">venerable Voyager spacecraft</a> flying - and a critical one at that, as clogging fuel lines threatened to derail the backup thrusters currently in use.&nbsp;</p>
<p>The things you have to deal with when your spacecraft is operating more than four decades beyond its original mission plan, eh? Voyager 1 launched in 1977.</p>

    

<p>JPL <a href="https://www.jpl.nasa.gov/news/nasas-voyager-1-revives-backup-thrusters-before-command-pause/?utm_source=iContact&amp;utm_medium=email&amp;utm_campaign=1-nasajpl&amp;utm_content=voyager20250514" rel="nofollow">reported</a> Wednesday that the maneuver, completed in March, restarted Voyager 1's primary roll thrusters, which are used to keep the spacecraft aligned with a tracking star. That guide star helps keep its high-gain antenna aimed at Earth, now over 15.6 billion miles away, and far beyond the reach of any telescope.</p>

        


        

<p>Those primary roll thrusters stopped working in 2004 after a pair of internal heaters lost power. Voyager engineers long believed they were broken and unfixable. The backup roll thrusters in use are now at risk due to residue buildup in their fuel lines, which could cause failure as early as this fall.</p>
<p>Without roll thrusters, Voyager 1 would lose its ability to stay properly oriented and eventually drift out of contact. To make matters worse, the only antenna on Earth with enough power to send commands to the Voyager probes - the 230-foot-wide DSS-43 dish in Australia - is undergoing an upgrade shutdown until next February, with only a couple of brief operational windows in August and December.</p>

        

<p>While other dishes around the world can still receive data from the Voyagers, those windows are the only opportunities to send commands to the spacecraft, which remain the most distant human-made objects in existence.</p>
<ul>

<li><a href="https://www.theregister.com/2025/05/14/saudi_cubesat_sls/">Saudi CubeSat gets golden ticket on doomed SLS rocket</a></li>

<li><a href="https://www.theregister.com/2025/05/08/nasa_jpl_director_quits/">NASA JPL boss bails for 'personal reasons' as budget cuts bite</a></li>

<li><a href="https://www.theregister.com/2025/05/02/nasa_funding_slashed/">Trump wants to fire quarter of NASA budget into black hole ‚Äì and not in a good way</a></li>

<li><a href="https://www.theregister.com/2025/05/08/nasa_budget_reaction/">ESA feeling weightless and unwanted amid proposed NASA cuts</a></li>
</ul>
<h3>Do and maybe die; don't do and definitely die</h3>
<p>With a hard deadline in place, the Voyager team had few options. So it turned to the primary roll thrusters that failed in 2004 - with a big IF in mind.&nbsp;</p>
<p><em>If</em> those heaters weren't actually dead, and <em>if</em> a power switch had been flipped by a circuit disturbance, then restoring that switch might bring the thrusters back online and preserve control of the spacecraft if the backup system failed.</p>
<p>The idea was to restore heater power to the primary roll thrusters, then let Voyager 1 drift far enough from its guide star that its onboard system would automatically fire the thrusters to correct its course.</p>
<p>If the heaters were still off when the dormant thrusters auto-fired due to attitude drift, "it could trigger a small explosion," JPL noted. And since it takes the radio signal more than 23 hours to travel from Voyager back to Earth, the team wouldn't know for an entire day.&nbsp;</p>
<blockquote>

<p>It was yet another miracle save for Voyager</p>
</blockquote>
<p>Fortunately, as the return signal arrived, the Voyager team saw signs that the thruster heaters were back online and the effort was a success.&nbsp;</p>
<p>"These thrusters were considered dead. And that was a legitimate conclusion," Voyager mission propulsion leader Todd Barber said in JPL's report. "It's just that one of our engineers had this insight that maybe there was this other possible cause and it was fixable. It was yet another miracle save for Voyager."</p>

        

<p>The Voyager missions have faced many challenges since leaving Earth more than 47 years ago. Most recently, Voyager 1 spent months <a href="https://www.theregister.com/2024/03/14/voyager_1_not_dead/">returning gibberish</a> instead of usable data before being fixed, and both probes have had to <a href="https://www.theregister.com/2025/03/06/voyager_power_down/">switch off</a> <a href="https://www.theregister.com/2024/10/03/voyager_2_plasma_instrument/">scientific instruments</a> as power dwindles and systems fail. <a href="https://www.theregister.com/2024/09/12/voyager_power_issues/">Thruster</a> <a href="https://www.theregister.com/2017/12/04/voyager_1_mission_extension_thanks_assmbler_code_and_thrusters_unused_since_1980/">issues</a> have also been a problem for Voyager 1 before, but the little craft that could <a href="https://www.theregister.com/2024/06/14/voyager_1_restored/">continue to soldier on</a> at the edge of interstellar space.</p>
<p>Both spacecraft will go dark eventually, closing our most distant eye into the universe beyond our solar system. But not quite yet.&nbsp;¬Æ</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A leap year check in three instructions (335 pts)]]></title>
            <link>https://hueffner.de/falk/blog/a-leap-year-check-in-three-instructions.html</link>
            <guid>43999748</guid>
            <pubDate>Thu, 15 May 2025 21:57:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hueffner.de/falk/blog/a-leap-year-check-in-three-instructions.html">https://hueffner.de/falk/blog/a-leap-year-check-in-three-instructions.html</a>, See on <a href="https://news.ycombinator.com/item?id=43999748">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>
      With the following code, we can check whether a year 0 ‚â§ <i>y</i> ‚â§ 102499 is a leap year with only about 3 CPU instructions:
    </p>
  
    <pre><code>bool is_leap_year_fast(uint32_t y) {
    return ((y * 1073750999) &amp; 3221352463) &lt;= 126976;
}</code></pre>
      
    <p>How does this work? The answer is surprisingly complex. This article explains it, mostly to have some fun with bit-twiddling; at the end, I'll briefly discuss the practical use.</p>
      
    <p>This is how a leap year check is typically implemented:</p>

    <pre><code>bool is_leap_year(uint32_t y) {
    if ((y % 4) != 0) return false;
    if ((y % 100) != 0) return true;
    if ((y % 400) == 0) return true;
    return false;
}</code></pre>

    <p>We are using the <a href="https://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar">proleptic Gregorian calendar</a>, which extends the Gregorian calendar backward from its introduction in 1582 and includes a year 0. Thus, we don't need to treat years before 1582 any differently. For simplicity, we ignore negative years and use an unsigned year.</p>

    <h2>Optimizing the standard approach</h2>

    <p>Let's first do some simple speedups so that we get a good baseline. I'm not sure where to assign credit ‚Äì these tricks have probably been reinvented independently many times.</p>

    <p>We can replace <code>(y % 100) != 0</code> by <code>(y % 25) != 0</code>: we already know that <code>y</code> is a multiple of 2<sup>2</sup>, so if it is also a multiple of 5<sup>2</sup>, it is a multiple of 2<sup>2</sup> ‚ãÖ 5<sup>2</sup> = 100. Similarly, we can replace <code>(y % 400) == 0</code> by <code>(y % 16) == 0</code>: we already know that <i>y</i> is a multiple of 5<sup>2</sup>, so if it is also a multiple of 2<sup>4</sup>, it is a multiple of 5<sup>2</sup> ‚ãÖ 2<sup>4</sup> = 400.</p>

    <pre><code>bool is_leap_year1(uint32_t y) {
    if ((y % 4) != 0) return false;
    if ((y % 25) != 0) return true;
    if ((y % 16) == 0) return true;
    return false;
}</code></pre>
      
    <p>This is useful because we can now replace modulo by 4 and by 16 with bit masking. There is a further trick well-known to compiler implementors on how to lower the modulo by 25. Compiling <code>(x % 25) != 0</code> with gcc and translating back to C, we get <code>x * 3264175145 &gt; 171798691</code>. With multiplication at a typical latency of 3 cycles and modulo of at least 20 cycles, this is a great improvement. I will only give an intuition on how it works; more details can be found in
    </p><ul>
      <li><a href="https://doi.org/10.1002/spe.2689">Faster remainder by direct computation: Applications to compilers and software libraries</a> paper by Daniel Lemire, Owen Kaser, and Nathan Kurz for lowering modulo by a constant in general;</li>
      <li><a href="https://doi.org/10.1002/spe.3172">Euclidean affine functions and their application to calendar algorithms</a> by Cassio Neri and Lorenz Schneider more specifically for calendar calculations; and</li>
      <li><a href="https://davecturner.github.io/2020/08/07/leapyear-optimization.html">Identifying leap years</a> by David Turner for the leap year check (including formal proofs!).</li>
    </ul>

    <p>Where do the magic numbers come from? We have</p>

    <p><span>2<sup>32</sup> ‚ãÖ 19/25 = 3264175144.96 (exactly).</span>
    </p>

    <p>So by multiplying with 3264175145, we calculate approximately the fractional part of multiplying with (19/25). If this was an exact multiplication, we would get a fractional part of zero for multiples of 25. We multiply with a number that is too high by 0.04 though, so the error can be up to 0.04 ‚ãÖ (2<sup>32</sup> - 1) = 171798691.8, which is where the second magic number comes from.</p>

    <p>This trick doesn't work quite as nicely for <code>x % 100</code>, where we need one more fixup instruction, so the reduction from <code>y % 100</code> to <code>y % 25</code> was helpful.</p>

    <p>Our leap year check now looks like this:
    </p><pre><code>bool is_leap_year2(uint32_t y) {
    if ((y &amp; 3) != 0) return false;
    if (y * 3264175145u &gt; 171798691u) return true;
    if ((y &amp; 15) == 0) return true;
    return false;
}</code></pre>

    <p>Note that modern compilers like gcc or clang will produce <a href="https://godbolt.org/z/GeK6Trfx6">something</a> like <code>is_leap_year2</code> from <code>is_leap_year1</code>, so there is not much point in doing this in C source, but it might be useful in other programming languages.</p>

    <p>This code typically compiles to branching assembly. In practice, inputs are typically fairly predictable, so that is not necessarily bad. If we want to avoid branch misprediction penalties, at the price of also slowing down the best case, we can shuffle things around a bit and get <a href="https://godbolt.org/z/PWs8saMYd">branchless code</a> (and also a decent code golf candidate):</p>
    <pre><code>bool is_leap_year3(uint32_t y) {
    return !(y &amp; ((y % 25) ? 3 : 15));
}</code></pre>

    <p>If you want to see some more calendar calculation speedups, make sure to check out <a href="https://hueffner.de/falk/blog/Optimizing%20with%20Novel%20Calendrical%20Algorithms">https://jhpratt.dev/blog/optimizing-with-novel-calendrical-algorithms/</a> by Jacob Pratt.

    </p><h2>Finding a bit-twiddling approach</h2>

    <p>Can we improve the leap year calculation by giving up on correctness for all inputs? After all, we typically don't care whether the year 3584536493 is a leap year; indeed, Python, C#, and Go only support years 0 (or 1) to 9999 (at which point the drift relative to the seasons is already more than 4 days). My thought was that if something shorter exists, it will basically look like some weird hashing using magic constants, so I wanted to try some small forms and guess the constants by brute force. The form <code>(y * f) &lt;= t</code> seems useful but is not powerful enough yet. One of my candidates was adding a mask: <code>((y * f) &amp; m) &lt;= t</code>. Now we have 96 bits to guess, which is not feasible by brute force alone. Let's use <a href="https://github.com/Z3Prover/z3">z3</a>, a solver that supports bitvector constraints that is perfectly suited for this:</p>

    <pre><code>import z3

BITS = 32
f, m, t, y = z3.BitVecs('f m t y', BITS)

def target(y):
    return z3.And((y &amp; 3) == 0, z3.Or(z3.URem(y, 25) != 0, (y &amp; 15) == 0))

def candidate(x):
    return z3.ULE((x * f) &amp; m, t)

solver = z3.Solver()
solver.add(z3.ForAll(y, z3.Implies(z3.ULE(y, 400),
                                   candidate(y) == target(y))))

if solver.check() == z3.sat:
    print(f'found solution: {solver.model()}')
else:
    print('no solution found')
</code></pre>

    <p>In a few seconds, this finds constants that yield the correct result for a nontrivial year range. Extending the range, I eventually found constants that yield the correct result from year 0 up to the year 102499, in about half an hour of computation time, and proved that this is the optimum for 32 bits:</p>

    <pre><code>bool is_leap_year_fast(uint32_t y) {
    const uint32_t f = 1073750999u;
    const uint32_t m = 3221352463u;
    const uint32_t t = 126976u;
    return ((y * f) &amp; m) &lt;= t;
}</code></pre>

    <h2>Explanation</h2>

    <p>How does it work? It seems surprising, almost magic, that we can squeeze all these computations into three instructions. However, the considerations above give us most of the tools to understand it.</p>

    <p>Here's our constants in binary, with four relevant bit ranges indicated by letters:</p>

    <pre>   <span>A</span>                 <span>D</span>
f: <span>01</span>0000000000000000<span>10001111010111</span>
m: <span>11</span>0000000000000<span>11111</span>00000000<span>1111</span>
t: <span>00</span>0000000000000<span>11111</span>00000000<span>0000</span>
                  <span>B</span>            <span>C</span>
</pre>

    <p>Let's first consider for the product <i>p</i> := <i>y</i> ‚ãÖ <i>f</i> what the masking with <i>m</i> and comparison with <i>t</i> does. In block <i>A</i>, the bits in <i>t</i> are 0, and therefore, the result is false as soon as any bit in <i>A</i> is set in <i>p</i>. Otherwise, block <i>B</i> becomes relevant. Here all bits in <i>t</i> are 1, so the result is true as soon as any bit in <i>B</i> is unset in <i>p</i>. Otherwise, for block <i>C</i> we require that all bits are unset in <i>p</i>. In this way, a bunch of comparisons of bit ranges are all united into a single <code>&lt;=</code>.</p>

    <p>Thus, we could rewrite <code>is_leap_year_fast</code> as follows:
    </p><pre><code>bool is_leap_year_fast2(uint32_t y) {
    uint32_t p = y * 1073750999u;
    const uint32_t A = 0b11000000000000000000000000000000;
    const uint32_t B = 0b00000000000000011111000000000000;
    const uint32_t C = 0b00000000000000000000000000001111;
    if ((p &amp; A) != 0) return false;
    if ((p &amp; B) != B) return true;
    if ((p &amp; C) == 0) return true;
    return false;
}</code></pre>

    <p>That looks suspiciously like <code>is_leap_year2</code>! And indeed, the three conditions have exactly the same purpose. We'll show
      </p><ol>
	<li> <code>(p &amp; A) != 0</code> triggers when <code>(y % 4) != 0</code>;</li>
	<li> <code>(p &amp; B) != B</code> triggers when <code>(y % 100) != 0</code>;</li>
	<li> <code>(p &amp; C) == 0</code> triggers when <code>(y % 16) == 0</code> (and therefore <code>(y % 400) == 0</code>, since we already know <i>y</i> is a multiple of 25).</li>
      </ol>

    <h3>Two easy cases: (1) and (3)</h3>
          
    <p>(1): The 1 bit in <i>A</i> in <i>f</i> reproduces the two low bits of <i>y</i> into <i>p</i> at <i>A</i>. This cannot be clobbered by the result of the multiplication with the bits in <i>D</i>: the maximum value we can get is 102499 ‚ãÖ (f &amp; D) = 940428325, which has only 30 bits. Thus, checking that A is zero in p is equivalent to checking whether <i>y</i> is 0 modulo 4.</p>

    <p>(3): Checking that none of the lowest 4 bits is set in <i>p</i> is checking whether <i>p</i> is 0 modulo 16. However, we actually want to check <i>y</i>. This is no problem: It suffices to look only at the lowest 4 bits of <i>f</i>, and <i>f</i> is 1111<sub>2</sub> = 15 there. Multiplying with 15 = 3 ‚ãÖ 5 introduces no additional factor of 2, so divisibility by 16 is unchanged.
    

    </p><h3>The interesting case: (2)</h3>

    <p>Next, let's try to find out for which numbers <i>p</i> &amp; <i>B</i> ‚â† <i>B</i>. For this, the 1 bit in <i>f</i> &amp; <i>A</i> does not play a role, so consider the bits in <i>f</i> &amp; <i>D</i>. They are 10001111010111<sub>2</sub> = 9175. Let's see for which numbers the test hits.</p>

    <pre><code>&gt;&gt;&gt; B = 0b00000000000000011111000000000000
&gt;&gt;&gt; s = [y for y in range(5000) if ((y * 9175) &amp; B) == B]
&gt;&gt;&gt; for i in range(0, len(s), 16): print(*(f'{n:4d}' for n in s[i:i+16]))
  14   57   71  100  114  157  171  200  214  257  271  300  314  357  371  400
 414  457  471  500  514  557  571  600  614  657  671  700  714  757  771  800
 814  857  871  900  914  957  971 1000 1014 1057 1071 1100 1114 1157 1171 1200
1214 1257 1271 1300 1314 1357 1371 1400 1414 1457 1471 1500 1514 1557 1571 1600
1614 1657 1671 1700 1714 1757 1771 1800 1814 1857 1871 1900 1914 1957 1971 2000
2014 2057 2071 2100 2114 2157 2171 2200 2214 2257 2271 2300 2314 2357 2371 2400
2414 2457 2471 2500 2514 2557 2571 2600 2614 2657 2671 2700 2714 2757 2771 2800
2814 2857 2871 2900 2914 2957 2971 3000 3014 3057 3071 3100 3114 3157 3171 3200
3214 3257 3271 3300 3314 3357 3371 3400 3414 3457 3471 3500 3514 3557 3571 3600
3614 3657 3671 3700 3714 3757 3771 3800 3814 3857 3871 3900 3914 3957 3971 4000
4014 4057 4071 4100 4114 4157 4200 4214 4257 4300 4314 4357 4400 4414 4457 4500
4514 4557 4600 4614 4657 4700 4714 4757 4800 4814 4857 4900 4914 4957
</code></pre>

    <p>Multiples of 100 are here as desired, but also a bunch of other numbers. This is not a problem as long as none of them are a multiple of 4, since those were already filtered out in the previous step. Also, 0 is missing, but that's not a problem either, since 0 is also a multiple of 400.</p>

    <p>Let's try to understand the pattern. At first glance, it looks very simple: we have *14, *57, *71, and *00. However, from 4171 on, *71 drops out (did you notice?). We also have new patterns popping up later. Let's analyze this. The following Python program</p>
<pre><code>def test(y):
    B = 126976
    return ((y * 9175) &amp; B) == B

active = set()
for y in range(120000):
    r = y % 100
    if test(y):
        if r not in active:
            print(f'{y:6}: started *{r:02}')
            active.add(r)
    else:
        if r in active:
            print(f'{y:6}: stopped *{r:02}')
            active.remove(r)
</code></pre><p>
outputs
</p><pre><code>    14: started *14
    57: started *57
    71: started *71
   100: started *00
  4171: stopped *71
 32843: started *43
 36914: stopped *14
 65586: started *86
 69657: stopped *57
 98329: started *29
102500: stopped *00
</code></pre>

    <p>So, from 102500 on, we no longer catch multiples of 100, which explains why 102499 is the last number for which <code>is_leap_year_fast</code> returns the correct result. We also see that below that, no number is a multiple of 4 except for the multiples of 100 (conveniently, we can check that knowing only the last two decimal digits). This concludes the proof of condition (2), if we trust this brute-force enumeration; but let's continue and understand better why we get exactly these numbers.</p>

    <p>Let's look into why we're getting the multiples of 100 first. The factor 9175 is close to a multiple of 1/100 in 17-bit fixed point representation:</p>
    <p><span>2<sup>17</sup> ‚ãÖ 7/100 = 9175.04 (exactly).</span>
    </p>

    <p>Multiplying a multiple of 100 by 9175.04 gives an integer (a multiple of 7) at bits 17 and above and 17 zero bits below that. E.g.</p>
    <p><span>9175.04 ‚ãÖ 500 = 10001100000000000000000<sub>2</sub>, with 100011<sub>2</sub> = 35 = 5 ‚ãÖ 7.</span>
    </p>

    <p>Multiplying a multiple of 100 by 9175 gives slightly less:</p>
    <p><span>9175 ‚ãÖ 500 = 10001100000000000000000<sub>2</sub> ‚àí 500 ‚ãÖ 0.04 = 10001011111111111101100<sub>2</sub>.</span>
    </p>

    <p>In general, subtracting a little bit from a number ending in a lot of zeroes creates a number that ends in a lot of ones, except at the very end. Here, we check the 5 bits in <i>B</i>. For <i>y</i> a multiple of 100, these are guaranteed be all 1 until the accumulated error reaches the lower end of <i>B</i>, which happens only after <i>y</i> = 2<sup>12</sup> / 0.04 = 102400, so this fits.</p>

    <p>Now where do the other numbers like 14, 57, and 71 come from? Let's look at it from a different angle. We have 9175 = 2<sup>17</sup> ‚ãÖ 0.06999969482421875 (exactly) and <i>B</i> = 2<sup>17</sup> ‚ãÖ 0.96875, so
    </p><p><span><i>p</i> &amp; <i>B</i> </span>             <span>= <i>B</i></span>
      <span>‚áî {<i>y</i> ‚ãÖ 0.06999969482421875} </span>  <span>‚â• 0.96875, where {<i>x</i>} is the fractional part of <i>x</i></span>
      <span>‚áî 6.999969482421875<i>y</i> mod 100 </span> <span>‚â• 96.875</span>
    </p>

    <p>This is another way to see why multiples of 100 are accepted: For them, 7<i>y</i> mod 100 is 0, so 6.999969482421875<i>y</i> mod 100 comes out slightly below 100, and drops below 96.875 only after <i>y</i> = (100 ‚àí 96.875) / (7 ‚àí 6.999969482421875) = 102400.

    </p><p>To understand the other numbers showing up in our sequence, let's first consider what the solutions would be if we had a clean 7 in this inequality:</p>
    <p><span>7y mod 100 </span> <span>‚â• 96.875</span>
      <span>‚áî 7y mod 100 </span> <span>‚àà {97, 98, 99}.</span>
    </p>

    <p>To find the solutions to this, we first need the modular inverse of 7 modulo 100, that is, the number <i>x</i> such that 7<i>x</i> mod 100 = 1. We can calculate it by using the extended Euclidean algorithm, or just using <a href="https://www.wolframalpha.com/input?i=modular+inverse+of+7+modulo+100">some online calculator</a>, which tells us the result is 43. Then the solutions are 43 ‚ãÖ 97 (mod 100), 43 ‚ãÖ 98 (mod 100), and 43 ‚ãÖ 99 (mod 100), which is 71, 14, and 57 (mod 100), respectively. This explains why we're initially seeing numbers of the form *14, *57, and *71. It also explains why we stop seeing e.g. *71 after 4071: While 7 ‚ãÖ 4171 = 29197, we have 6.999969482421875 ‚ãÖ 4171 = 29196.872711181640625, which (modulo 100) is less than 96.875. Similarly, 32843 pops up because the accumulated error (7 ‚àí 6.999969482421875) ‚ãÖ 32843 = 1.002288818359375 exceeds one. With a bit more effort here, we could manually reproduce the output of the Python program above and check that none of these numbers is a multiple of 4.</p>

    <h2>Extension to other bitwidths</h2>

    <p>Now that we understand how the trick works, we can try to find parameters for other bitwidths. The variable parts are the location of block <i>B</i> and the fraction of 100 in <i>f</i> &amp; <i>D</i>.</p>

    <pre><code>uint64_t score(uint64_t f, uint64_t m, uint64_t t) {
      for (uint64_t y = 0; ; y++)
          if ((((y * f) &amp; m) &lt;= t) != is_leap_year(y))
              return y;
  }
  
  int main() {
      uint64_t best_score = 0;
      for (int k = 0; k &lt; BITS; k++) {
          for (int k2 = 0; k2 &lt; k; k2++) {
              uint64_t t = (1ULL &lt;&lt; k) - (1ULL &lt;&lt; k2);
              uint64_t m = (0b11ULL &lt;&lt; (BITS - 2)) | t | 0b1111;
              for (int n = 0; n &lt; 100; n++) {
                  uint64_t f = (0b01ULL &lt;&lt; (BITS - 2)) | (((1ULL &lt;&lt; k) * n) / 100);
                  uint64_t new_score = score(f, m, t);
                  if (new_score &gt; best_score) {
                      printf("%llu %llu %llu: %llu (%d %d %d)\n",
                             f, m, t, new_score, k, k - k2, n);
                      best_score = new_score;
                  }
              }
          }
      }
      return 0;
  }
  </code></pre>

    <p>For <code>BITS = 64</code>, in about 7 minutes we find <i>f</i> = 4611686019114582671, <i>m</i> = 13835058121854156815, <i>t</i> = 66571993088, which is correct up to <i>y</i> = 5965232499. This is nice because 5965232499 &gt; 2<sup>32</sup>, so any 32-bit year can be tested correctly with this variant.</p>

    <p>Is this the highest we can go with 64 bit? Perhaps there's some other constants that work even better? I couldn't immediately find a way to prove it, so I employed the tried-and-true method of getting someone else to do it for me by <a href="https://codegolf.stackexchange.com/q/275505/116815">posting it to the Code Golf StackExchange</a>. Indeed after only 1h user ovs posted a very good result, and two days later user Exalted Toast posted a <a href="https://codegolf.stackexchange.com/a/275541">proof</a> that 5965232499 is indeed the best possible range for 64 bits, also employing the z3 solver.</p>

    <h2>Benchmark</h2>

    <p>Getting meaningful benchmarks is tricky, since the function takes just such a tiny amount of time, and moreover for the branching versions execution time depends on the patterns of the input. Let's try two extreme cases: always the year 2025, and completely random years. These are the results of the <a href="https://hueffner.de/falk/blog/benchmark.cc">benchmark</a> on an i7-8700K (Coffee Lake, 4.7 GHz), compiled wirh <code>g++ -O3 -fno-tree-vectorize</code>:</p>

    <table>
      <thead>
          <tr>
              <th></th>
              <th>2025 (ns)</th>
              <th>random (ns)</th>
          </tr>
      </thead>
      <tbody>
            <tr>
              <td><code>is_leap_year</code></td>
              <td>0.65</td>
              <td>2.61</td>
            </tr>
            <tr>
              <td><code>is_leap_year2</code></td>
              <td>0.65</td>
              <td>2.75</td>
            </tr>
            <tr>
              <td><code>is_leap_year3</code></td>
              <td>0.67</td>
              <td>0.88</td>
            </tr>
            <tr>
              <td><code>is_leap_year_fast</code></td>
              <td>0.69</td>
              <td>0.69</td>
          </tr>
       </tbody>
    </table>

    <p>Some weird things stand out here:</p>
    <ul>
    <li><code>is_leap_year2</code> is actually marginally slower than <code>is_leap_year</code> in the random case. This is surprising because <code>y % 100</code> needs one instruction more than the trick in <code>is_leap_year2</code>.</li>
    <li><code>is_leap_year3</code> is a bit slower for random data than for the fixed value. This is surprising because it doesn't do any branching.</li>
    </ul>

    <p>I have no explanation for this other than benchmarking is hard.</p>

    <p>The new function <code>is_leap_year_fast</code> is 3.8 times faster than the standard implementation for random data, and about 6% slower for the perfectly predictable input. Overall, that seems pretty solid.</p>


    <h2>Conclusion</h2>

    <p>So, is this really worthwhile? Should we replace e.g. the <a href="https://github.com/python/cpython/blob/5cdd49b3f4cbdcf0472a65fd0c723912c3d48211/Modules/_datetimemodule.c#L416">CPython datetime implementation</a> with this trick? Well, it depends. Most years being queried in practice will be the current year, or at least fairly predictable, in which case we don't have a great advantage. To fully justify a change, ideally we would have a benchmark with realistic data that employs a leap year check as a subroutine, rather than just microbenchmarking. I would be happy to hear about any such results!</p>

    
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Initialization in C++ is bonkers (2017) (155 pts)]]></title>
            <link>https://blog.tartanllama.xyz/initialization-is-bonkers/</link>
            <guid>43999492</guid>
            <pubDate>Thu, 15 May 2025 21:27:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.tartanllama.xyz/initialization-is-bonkers/">https://blog.tartanllama.xyz/initialization-is-bonkers/</a>, See on <a href="https://news.ycombinator.com/item?id=43999492">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        


        

        <p><span itemprop="articleBody"><p>C++ pop quiz time: what are the values of <code>a.a</code> and <code>b.b</code> on the last line in <code>main</code> of this program?</p>

<figure><pre><code data-lang="cpp"><span>#include</span> <span>&lt;iostream&gt;</span><span>
</span>
<span>struct</span> <span>foo</span> <span>{</span>
    <span>foo</span><span>()</span> <span>=</span> <span>default</span><span>;</span>
    <span>int</span> <span>a</span><span>;</span>
<span>};</span>

<span>struct</span> <span>bar</span> <span>{</span>
    <span>bar</span><span>();</span>
    <span>int</span> <span>b</span><span>;</span>
<span>};</span>

<span>bar</span><span>::</span><span>bar</span><span>()</span> <span>=</span> <span>default</span><span>;</span>

<span>int</span> <span>main</span><span>()</span> <span>{</span>
    <span>foo</span> <span>a</span><span>{};</span>
    <span>bar</span> <span>b</span><span>{};</span>
    <span>std</span><span>::</span><span>cout</span> <span>&lt;&lt;</span> <span>a</span><span>.</span><span>a</span> <span>&lt;&lt;</span> <span>' '</span> <span>&lt;&lt;</span> <span>b</span><span>.</span><span>b</span><span>;</span>
<span>}</span></code></pre></figure>

<p>The answer is that <code>a.a</code> is <code>0</code> and <code>b.b</code> is indeterminate, so reading it is undefined behaviour. Why? Because initialization in C++ is bonkers.</p>

<hr>

<h4 id="default--value--and-zero-initialization">Default-, value-, and zero-initialization</h4>

<p>Before we get into the details which cause this, I‚Äôll introduce the concepts of default-, value- and zero-initialization. Feel free to skip this section if you‚Äôre already familiar with these.</p>

<figure><pre><code data-lang="cpp"><span>T</span> <span>global</span><span>;</span>       <span>//zero-initialization, then default-initialization</span>

<span>void</span> <span>foo</span><span>()</span> <span>{</span>
    <span>T</span> <span>i</span><span>;</span>         <span>//default-initialization</span>
    <span>T</span> <span>j</span><span>{};</span>       <span>//value-initialization (C++11)</span>
    <span>T</span> <span>k</span> <span>=</span> <span>T</span><span>();</span>   <span>//value-initialization</span>
    <span>T</span> <span>l</span> <span>=</span> <span>T</span><span>{};</span>   <span>//value-initialization (C++11)</span>
    <span>T</span> <span>m</span><span>();</span>       <span>//function-declaration</span>

    <span>new</span> <span>T</span><span>;</span>       <span>//default-initialization</span>
    <span>new</span> <span>T</span><span>();</span>     <span>//value-initialization</span>
    <span>new</span> <span>T</span><span>{};</span>     <span>//value-initialization (C++11)</span>
<span>}</span>

<span>struct</span> <span>A</span> <span>{</span> <span>T</span> <span>t</span><span>;</span> <span>A</span><span>()</span> <span>:</span> <span>t</span><span>()</span> <span>{}</span> <span>};</span> <span>//t is value-initialized</span>
<span>struct</span> <span>B</span> <span>{</span> <span>T</span> <span>t</span><span>;</span> <span>B</span><span>()</span> <span>:</span> <span>t</span><span>{}</span> <span>{}</span> <span>};</span> <span>//t is value-initialized (C++11)</span>
<span>struct</span> <span>C</span> <span>{</span> <span>T</span> <span>t</span><span>;</span> <span>C</span><span>()</span>       <span>{}</span> <span>};</span> <span>//t is default-initialized</span></code></pre></figure>

<p>The rules for these different initialization forms are fairly complex, so I‚Äôll give a simplified outline of the C++11 rules (C++14 even changed some of them, so those value-initialization forms can be aggregate initialization). If you want to understand all the details of these forms, check out the relevant cppreference.com articles<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>, or see the standards quotes at the bottom of the article.</p>

<ul>
  <li><strong>default-initialization</strong> ‚Äì If <code>T</code> is a class, the default constructor is called; if it‚Äôs an array, each element is default-initialized; otherwise, no initialization is done, resulting in indeterminate values.</li>
  <li><strong>value-initialization</strong> ‚Äì If <code>T</code> is a class, the object is default-initialized (after being zero-initialized if <code>T</code>‚Äôs default constructor is not user-provided/deleted); if it‚Äôs an array, each element is value-initialized; otherwise, the object is zero-initialized.</li>
  <li><strong>zero-initialization</strong> ‚Äì Applied to static and thread-local variables before any other initialization. If <code>T</code> is scalar (arithmetic, pointer, enum), it is initialized from <code>0</code>; if it‚Äôs a class type, all base classes and data members are zero-initialized; if it‚Äôs an array, each element is zero-initialized.</li>
</ul>

<p>Taking the simple example of <code>int</code> as <code>T</code>, <code>global</code> and all of the value-initialized variables will have the value <code>0</code>, and all other variables will have an indeterminate value. Reading these indeterminate values results in undefined behaviour.</p>

<hr>

<h4 id="back-to-our-original-example">Back to our original example</h4>

<p>Now we have the necessary knowledge to understand what‚Äôs going on in my original example. Essentially, the behaviours of <code>foo</code> and <code>bar</code> are changed by the different location of <code>=default</code> on their constructors. Again, the relevant standards passages are down at the bottom of the page if you want them, but the jist is this:</p>

<p>Since the constructor for <code>foo</code> is defaulted on its first declaration, it is not technically <em>user-provided</em> ‚Äì I‚Äôll explain what this term means shortly, just accept this standardese for now. The constructor for <code>bar</code>, conversely, is only defaulted at its definition, so it <em>is</em> user-provided. Put another way, if you don‚Äôt want your constructor to be user-provided, be sure to write <code>=default</code> when you declare it rather than define it like that elsewhere. This rule makes sense when you think about it: without having access to the definition of a constructor, a translation unit can‚Äôt know if it is going to be a simple compiler-generated one, or if it‚Äôs going to send a telegram to the Moon to retrieve some data and block until it gets a response.</p>

<p>The default constructor being user-provided has a few consequences for the class type. For example, you can‚Äôt default-initialize a const-qualified object if it lacks a user-provided constructor, the notion being that if the object should only be set once, it better be initialised with something reasonable:</p>

<figure><pre><code data-lang="cpp"><span>const</span> <span>int</span> <span>my_int</span><span>;</span>            <span>//ill-formed, no user-provided constructor</span>
<span>const</span> <span>std</span><span>::</span><span>string</span> <span>my_string</span><span>;</span> <span>//well-formed, has a user-provided constructor</span>

<span>const</span> <span>foo</span> <span>my_foo</span><span>;</span> <span>//ill-formed, no user-provided constructor</span>
<span>const</span> <span>bar</span> <span>my_bar</span><span>;</span> <span>//well-formed, has a user-provided constructor</span></code></pre></figure>

<p>Additionally, in order to be <a href="http://en.cppreference.com/w/cpp/concept/TrivialType">trivial</a> (and therefore <a href="http://en.cppreference.com/w/cpp/concept/PODType">POD</a>) or an <a href="http://en.cppreference.com/w/cpp/language/aggregate_initialization">aggregate</a>, a class must have no user-provided constructors. Don‚Äôt worry if you don‚Äôt know those terms, it suffices to know that whether your constructors are user-provided or not modifies some of the restrictions of what you can do with that class and how it acts.</p>

<p>For our first example, however, we‚Äôre interested in how user-provided constructors interact with initialization rules. The language mandates that both <code>a</code> and <code>b</code> are value-initialized, but only <code>a</code> is additionally zero-initialized. Zero-initialization for <code>a</code> gives <code>a.a</code> the value <code>0</code>, whereas <code>b.b</code> is not initialized at all, giving us undefined behaviour if we attempt to read it. This is a very subtle distinction which has inadvertently changed our program from executing safely to summoning nasal demons/eating your cat/ordering pizza/your favourite undefined behaviour metaphor.</p>

<p>Fortunately, there‚Äôs a simple solution. At the risk of repeating advice which has been given many times before, <strong>initialize your variables.</strong></p>

<p><b>Seriously.</b></p>

<p><b>Do it.</b></p>

<p><b>INITIALIZE YOUR GORRAM VARIABLES.</b></p>

<p>If the designer of <code>foo</code> and <code>bar</code> decides that they should be default constructible, they should initialize their contents with some sensible values. If they decide that they should <em>not</em> be default constructible, they should delete the constructors to avoid issues.</p>

<figure><pre><code data-lang="cpp"><span>struct</span> <span>foo</span> <span>{</span>
    <span>foo</span><span>()</span> <span>:</span> <span>a</span><span>{</span><span>0</span><span>}</span> <span>{}</span> <span>//initialize to 0 explicitly</span>
    <span>int</span> <span>a</span><span>;</span>
<span>};</span>

<span>struct</span> <span>bar</span> <span>{</span>
    <span>bar</span><span>()</span> <span>=</span> <span>delete</span><span>;</span> <span>//delete constructor</span>
    <span>//insert non-default constructor which does something sensible here</span>
    <span>int</span> <span>b</span><span>;</span>
<span>};</span></code></pre></figure>

<p>Internalising this way of thinking about initialization is key to writing unsurprising code. If you‚Äôve profiled your code and found a bottleneck caused by unnecessary initialization, then sure, optimise it, but you best be certain that the extra performance is worth the possible headaches and money spent to keep the code safe.</p>

<p>If you still aren‚Äôt convinced that C++ initialization rules are crazy-complex, take a minute to think of all the forms of initialization you can think of. My answers after the line.</p>

<hr>

<p>Done? How many did you come up with? In perusal of the standard, I counted <em>eighteen</em> different forms of initialization<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup>. Here they are with a short example/description:</p>

<ul>
  <li>default: <code>int i;</code></li>
  <li>value: <code>int i{};</code></li>
  <li>zero: <code>static int i;</code></li>
  <li>constant: <code>static int i = some_constexpr_function();</code></li>
  <li>static: zero- or constant-initialization</li>
  <li>dynamic: not static initialization</li>
  <li>unordered: dynamic initialization of class template static data members which are not explicitly specialized</li>
  <li>ordered: dynamic initialization of other non-local variables with static storage duration</li>
  <li>non-trivial: when a class or aggregate is initialized by a non-trivial constructor</li>
  <li>direct: <code>int i{42}; int j(42);</code></li>
  <li>copy: <code>int i = 42;</code></li>
  <li>copy-list: <code>int i = {42};</code></li>
  <li>direct-list: <code>int i{42};</code></li>
  <li>list: either copy-list or direct-list</li>
  <li>aggregate: <code>int is[3] = {0,1,2};</code></li>
  <li>reference: <code>const int&amp; i = 42; auto&amp;&amp; j = 42;</code></li>
  <li>implicit: default or value</li>
  <li>explicit: direct, copy, or list</li>
</ul>

<p>Don‚Äôt try to memorise all of these rules; therein lies madness. Just be careful, and keep in mind that C++‚Äôs initialization rules are there to pounce on you when you least expect it. Explicitly initialize your variables, and if you ever fall in to the trap of thinking C++ is a sane language, remember this:</p>

<p><strong>In C++, you can give your program undefined behaviour by changing the point at which you tell the compiler to generate something it was probably going to generate for you anyway.</strong></p>

<hr>

<h4 id="appendix-standards-quotes">Appendix: Standards quotes</h4>

<p>All quotes from N4140 (essentially C++14).</p>

<blockquote para="[dcl.fct.def.default]/5">
  <p>Explicitly-defaulted functions and implicitly-declared functions are collectively called defaulted functions,
and the implementation shall provide implicit definitions for them (12.1 12.4, 12.8), which might mean
defining them as deleted. <strong>A function is user-provided if it is user-declared and not explicitly defaulted or
deleted on its first declaration.</strong> A user-provided explicitly-defaulted function (i.e., explicitly defaulted after its first declaration) is defined at the point where it is explicitly defaulted; if such a function is implicitly defined
as deleted, the program is ill-formed.</p>
</blockquote>

<blockquote para="[dcl.init]/6-8">
  <p>To <em>zero-initialize</em> an object or reference of type <code>T</code> means:</p>

  <ul>
    <li>if <code>T</code> is a scalar type (3.9), the object is initialized to the value obtained by converting the integer literal
0 (zero) to T</li>
    <li>if <code>T</code> is a (possibly cv-qualified) non-union class type, each non-static data member and each base-class subobject is zero-initialized and padding is initialized to zero bits;</li>
    <li>if <code>T</code> is a (possibly cv-qualified) union type, the object‚Äôs first non-static named data member is zero-initialized and padding is initialized to zero bits;</li>
    <li>if <code>T</code> is an array type, each element is zero-initialized;</li>
    <li>if <code>T</code> is a reference type, no initialization is performed.</li>
  </ul>

  <p>To <em>default-initialize</em> an object of type <code>T</code> means:</p>

  <ul>
    <li>if <code>T</code> is a (possibly cv-qualified) class type (Clause 9), the default constructor (12.1) for <code>T</code> is called (and the initialization is ill-formed if <code>T</code> has no default constructor or overload resolution (13.3) results in an
ambiguity or in a function that is deleted or inaccessible from the context of the initialization);</li>
    <li>if <code>T</code> is an array type, each element is default-initialized;</li>
    <li>otherwise, no initialization is performed.
If a program calls for the default initialization of an object of a const-qualified type <code>T</code>, <code>T</code> shall be a class type with a user-provided default constructor.</li>
  </ul>

  <p>To <em>value-initialize</em> an object of type <code>T</code> means:</p>

  <ul>
    <li>if <code>T</code> is a (possibly cv-qualified) class type (Clause 9) with either no default constructor (12.1) or a default constructor that is user-provided or deleted, then the object is default-initialized;</li>
    <li>if <code>T</code> is a (possibly cv-qualified) class type without a user-provided or deleted default constructor, then the object is zero-initialized and the semantic constraints for default-initialization are checked, and if <code>T</code> has a non-trivial default constructor, the object is default-initialized;</li>
    <li>if <code>T</code> is an array type, then each element is value-initialized;</li>
    <li>otherwise, the object is zero-initialized.</li>
  </ul>
</blockquote>

<blockquote para="[basic.start.init]/2">
  <p>Variables with static storage duration (3.7.1) or thread storage duration (3.7.2) shall be zero-initialized (8.5)
before any other initialization takes place. [‚Ä¶]</p>
</blockquote>

</span></p><p><small>
          <i></i>
            c++
          </small>
        </p>
        

      </div><p>
        I <i></i> feedback.<br>
        Let me know what you think of this article on twitter <a href="https://www.twitter.com/TartanLlama">@TartanLlama</a> or leave a comment below!
      </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stack Overflow is almost dead (115 pts)]]></title>
            <link>https://blog.pragmaticengineer.com/stack-overflow-is-almost-dead/</link>
            <guid>43999125</guid>
            <pubDate>Thu, 15 May 2025 20:41:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pragmaticengineer.com/stack-overflow-is-almost-dead/">https://blog.pragmaticengineer.com/stack-overflow-is-almost-dead/</a>, See on <a href="https://news.ycombinator.com/item?id=43999125">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
    <article>

        

        <section id="content">
            <p><em>Originally published in </em><a href="https://newsletter.pragmaticengineer.com/p/the-pulse-134?ref=blog.pragmaticengineer.com" rel="noreferrer"><em>The Pragmatic Engineer Newsletter</em></a><em>.</em></p><p>Four months ago, we asked <a href="https://blog.pragmaticengineer.com/are-llms-making-stackoverflow-irrelevant/" rel="noreferrer">Are LLMs making Stack Overflow irrelevant?</a> Data at the time suggested that the answer is likely "yes:"</p><figure><img src="https://blog.pragmaticengineer.com/content/images/2025/05/image-6.png" alt="" loading="lazy" width="1456" height="859" srcset="https://blog.pragmaticengineer.com/content/images/size/w600/2025/05/image-6.png 600w, https://blog.pragmaticengineer.com/content/images/size/w1000/2025/05/image-6.png 1000w, https://blog.pragmaticengineer.com/content/images/2025/05/image-6.png 1456w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Number of questions asked per month on StackOverflow. Data source: </em></i><a href="https://gist.github.com/hopeseekr/f522e380e35745bd5bdc3269a9f0b132?ref=blog.pragmaticengineer.com#file-stackoverflow-new-questions-over-time-2009-2024-csv" target="_blank" rel="noopener noreferrer nofollow"><i><em>this Gist</em></i></a></figcaption></figure><p>Since then, things at Stack Overflow went from bad to worse. The volume of questions asked has nearly dried up, new data shows:</p><figure><img src="https://blog.pragmaticengineer.com/content/images/2025/05/image-4.png" alt="" loading="lazy" width="936" height="744" srcset="https://blog.pragmaticengineer.com/content/images/size/w600/2025/05/image-4.png 600w, https://blog.pragmaticengineer.com/content/images/2025/05/image-4.png 936w" sizes="(min-width: 720px) 720px"><figcaption><i><em>Questions have slumped to levels last seen when Stack Overflow launched in 2009. Source: Stack Overflow Data Explorer (SEDE) / Marc Gravell&nbsp;</em></i><a href="https://x.com/marcgravell?ref=blog.pragmaticengineer.com" rel=""><i><em>on X</em></i></a></figcaption></figure><p>This graph&nbsp;<a href="https://x.com/marcgravell/status/1922922817143660783?ref=blog.pragmaticengineer.com">was shared</a>&nbsp;by&nbsp;<a href="https://x.com/marcgravell?ref=blog.pragmaticengineer.com">Marc Gravell</a>, a top 10 all-time contributor to Stack Overflow. Let‚Äôs look closer at the data:</p><figure><img src="https://blog.pragmaticengineer.com/content/images/2025/05/image-5.png" alt="" loading="lazy" width="1456" height="727" srcset="https://blog.pragmaticengineer.com/content/images/size/w600/2025/05/image-5.png 600w, https://blog.pragmaticengineer.com/content/images/size/w1000/2025/05/image-5.png 1000w, https://blog.pragmaticengineer.com/content/images/2025/05/image-5.png 1456w" sizes="(min-width: 720px) 720px"><figcaption><span>Decline started around 2014</span></figcaption></figure><p>A few things stand out:</p><ul><li><strong>2014:</strong>&nbsp;questions started to decline, which was also when Stack Overflow&nbsp;<a href="https://meta.stackoverflow.com/questions/251175/stack-overflow-is-not-yet-a-vast-wasteland-a-history-of-moderator-tooling?ref=blog.pragmaticengineer.com">significantly improved moderator efficiency</a>. From then, questions were closed faster, many more were closed, and ‚Äúlow quality‚Äù questions were removed more efficiently. This tallies with my memory of feeling that site moderators had gone on a power trip by closing legitimate questions.&nbsp;<em>I stopped asking questions around this time because the site felt unwelcome.</em></li><li><strong>March 2020:&nbsp;</strong>a big jump in traffic due to pandemic-induced lockdowns and forced remote working. Instead of asking colleagues, devs Googled and visited Stack Overflow for help</li><li><strong>June 2020:</strong>&nbsp;questions start to decline, faster than before.&nbsp;<em>Even though we did not know at the time, this was stilll two years from ChatGPT launching!</em></li><li><strong>June 2021:</strong>&nbsp;Stack Overflow sold for $1.8B to private equity investor, Prosus.&nbsp;<em>In hindsight, the founders ‚Äì Jeff Atwood and Joel Spolsky ‚Äì sold with near-perfect timing, before terminal decline.</em></li><li><strong>November 2022:</strong>&nbsp;as soon as ChatGPT came out, the number of questions asked declined rapidly. ChatGPT is faster and it‚Äôs trained on StackOverflow data, so the quality of answers is similar. Plus, ChatGPT is polite and answers all questions, in contrast to StackOverflow moderators.</li><li><strong>May 2025:</strong>&nbsp;the number of monthly questions is as low as when Stack Overflow launched in 2009.</li></ul><p>In January, I asked&nbsp;<a href="https://newsletter.pragmaticengineer.com/i/154495287/are-llms-making-stackoverflow-irrelevant?ref=blog.pragmaticengineer.com">if LLMs are making Stack Overflow irrelevant</a>. We now have an answer, and sadly, it‚Äôs a ‚Äúyes.‚Äù The question seems to be&nbsp;<em>when</em>&nbsp;Stack Overflow will wind down operations, or the owner sells the site for comparative pennies, not&nbsp;<em>if&nbsp;</em>it will happen.</p><p>Even without LLMs, it‚Äôs possible StackOverflow would have eventually faded into irrelevance ‚Äì perhaps driven by moderation policy changes or something else that started in 2014. LLMs have certainly accelerated its fall. It's a true shame for a site that helped so many developers get "unstuck" ‚Äì while successfully gamifying helping other developers on the internet in the early 2010s.</p><p>I'll certainly miss having a space on the internet to ask questions and receive help ‚Äì not from an AI, but from fellow, human developers. While Stack Overflow's days are likely numbered: I'm sure we'll see spaces where developers hang out and help each other continue to be popular ‚Äì whether they are in the form of Discord servers, WhatsApp or Telegram groups, or something else.</p><p><em>Update on 15 May: updated the last two paragraphs to make it a more positive outlook. I really did love StackOverflow from when it launched, and it made a big and positive difference in my professional growth in those early years ‚Äì I still remember the pride of getting my first upvote on first a question, and eventually on more and more answers as well. Too bad that all good things come to an end. Thanks to Andrew for his thoughtful note.</em></p><hr><p>This was one out of five topics from latest <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-134?ref=blog.pragmaticengineer.com" rel="noreferrer">The Pulse issue</a>. <a href="https://newsletter.pragmaticengineer.com/p/the-pulse-134?ref=blog.pragmaticengineer.com" rel="noreferrer">The full issue</a> additionally covers:</p><ul><li><strong>Industry pulse.&nbsp;</strong>Google‚Äôs CEO doing customer support, coding model recommendations from Cursor, AI dev tools company valuations soar, OpenAI still a nonprofit ‚Äì but with more clarity on stock, and will we get an answer to whether copyrighted materials can be used to train AI models?</li><li><strong>Could big job cuts at Microsoft become more regular?&nbsp;</strong>6,000 people (about 3% of staff) let go at Microsoft. Based on the company‚Äôs history, mass layoffs happen more than in the past. Satya Nadella is an empathetic leader, but also doesn‚Äôt shy away from axing jobs.</li><li><strong>Google: high performers get more bonus, low performers get less.</strong>&nbsp;Not exactly a controversial change, but another example of the search giant becoming similar to other tech companies. Places like Uber have implemented this approach before.</li><li><strong>Notes on rolling out Cursor and Claude Code.&nbsp;</strong>A 40-person dev team at Workforce.com with a Ruby on Rails codebase started to use AI tools and agents. Results so far are pretty good: productivity gains are real if modest, and there‚Äôs l<a href="https://ghiculescu.substack.com/p/nobody-codes-here-anymore?ref=blog.pragmaticengineer.com">earnings on how to best use them</a>&nbsp;from cofounder,&nbsp;<a href="https://ghiculescu.substack.com/p/nobody-codes-here-anymore?ref=blog.pragmaticengineer.com">Alex Ghiculescu</a>.</li></ul><p><a href="https://newsletter.pragmaticengineer.com/p/the-pulse-134?ref=blog.pragmaticengineer.com" rel="noreferrer"><strong>Read the full issue here.</strong></a></p>


            <!-- Newsletter -->
            <p>
              <a href="https://newsletter.pragmaticengineer.com/about">Subscribe to my weekly newsletter</a> to get articles like this in your inbox. It's a pretty good read - and the <a href="https://substack.com/top/technology">#1 tech newsletter</a> on Substack.
            </p>
            
        </section>

        
    </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tek ‚Äì A music making program for 24-bit Unicode terminals (136 pts)]]></title>
            <link>https://codeberg.org/unspeaker/tek</link>
            <guid>43998707</guid>
            <pubDate>Thu, 15 May 2025 19:56:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://codeberg.org/unspeaker/tek">https://codeberg.org/unspeaker/tek</a>, See on <a href="https://news.ycombinator.com/item?id=43998707">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		




	


		

		
		<div>
			<p><span><span aria-label="crab">ü¶Ä</span> a colorful music making program for your linux terminal <span aria-label="crab">ü¶Ä</span></span>
				
			</p>
		</div>
		<div id="repo-topics">
			
			<p><a href="https://codeberg.org/explore/repos?q=daw&amp;topic=1">daw</a><a href="https://codeberg.org/explore/repos?q=groovebox&amp;topic=1">groovebox</a><a href="https://codeberg.org/explore/repos?q=jack&amp;topic=1">jack</a><a href="https://codeberg.org/explore/repos?q=jackaudio&amp;topic=1">jackaudio</a><a href="https://codeberg.org/explore/repos?q=lv2&amp;topic=1">lv2</a><a href="https://codeberg.org/explore/repos?q=music&amp;topic=1">music</a><a href="https://codeberg.org/explore/repos?q=rust&amp;topic=1">rust</a><a href="https://codeberg.org/explore/repos?q=sampler&amp;topic=1">sampler</a><a href="https://codeberg.org/explore/repos?q=sequencer&amp;topic=1">sequencer</a><a href="https://codeberg.org/explore/repos?q=tui&amp;topic=1">tui</a>
			
		</p></div>
		
		

		
		








	
	



	
	



	
	



	
	



	
	



	
	



	
	



	
	



	
	



	
	





		
		
<div>
	<div>
		
			<a href="https://codeberg.org/unspeaker/tek/commits/branch/main">
				 <b>1751</b> commits
			</a>
			<a href="https://codeberg.org/unspeaker/tek/branches">
				 <b>4</b> branches
			</a>
			
				<a href="https://codeberg.org/unspeaker/tek/tags">
					 <b>11</b> tags
				</a>
			
			<p><span data-tooltip-content="git: 5 MiB, lfs: 0 B">
				
				 <b>5</b> MiB
			</span>
		
	</p></div>
	
	<a id="language-stats-bar" data-panel="#language-stats-legend">
		
		
		
		
		
		
		
		
		
	</a>
	<div id="language-stats-legend">
		
		<p><i></i>
			<span>
				
					Rust
				
			</span>
			98.4%
		</p>
		
		<p><i></i>
			<span>
				
					Just
				
			</span>
			0.9%
		</p>
		
		<p><i></i>
			<span>
				
					Shell
				
			</span>
			0.4%
		</p>
		
		<p><i></i>
			<span>
				
					Nix
				
			</span>
			0.3%
		</p>
		
	</div>
	
</div>


		
		
		
		<div>
			<div>
				






	








				
					
					
					
					
					<a id="new-pull-request" role="button" href="https://codeberg.org/unspeaker/tek/compare/main...main" data-tooltip-content="Compare">
						
					</a>
				
				
				
					<p><a href="https://codeberg.org/unspeaker/tek/find/branch/main">Find a file</a>
				

				

				
				
			</p></div>
			<div>
				
				
					
					<div id="cite-repo-modal">
	<p>
		Cite this repository
	</p>
	<div id="citation-panel">
				<p><span id="citation-copy-bibtex" data-text="">
BibTeX
</span>


</p><a id="goto-citation-btn" href="https://codeberg.org/unspeaker/tek/src/main/" data-tooltip-content="Find a file">
					
				</a>
			</div>
	
</div>

				
				
			</div>
		</div>
		
			
			
			
			
			
			<table id="repo-files-table">
	<thead>
		<tr>
			<th colspan="2">
				<div>
						
	
		<p><img loading="lazy" alt="" src="https://codeberg.org/avatars/d519d36199e89735a26401f8b318761bba46b0fec030ce83b49580fcbb91ec3c?size=48" title="ü§´‚ãØüòâ" width="24" height="24">
		
			<a title="unspeaker" href="https://codeberg.org/unspeaker"><strong>unspeaker</strong></a>
		
	
	<a rel="nofollow" href="https://codeberg.org/unspeaker/tek/commit/9aeb792f7dc0d4ee115d619d62916710ca1a3e9f">
		<span>9aeb792f7d</span>
		
	</a></p><a data-tippy="commit-statuses" href="https://codeberg.org/unspeaker/tek/actions/runs/1551/jobs/0">
			

	






		</a>
	
	<div>
	<p>
		
			Some checks are pending
		

		
	</p>

	<div>
				

	






				<p>/ build (push) <span>Waiting to run</span></p>
				<div>
					
					<p><span><a href="https://codeberg.org/unspeaker/tek/actions/runs/1551/jobs/0">Details</a></span>
				</p></div>
			</div>
</div>


	
	<p><span title="wip: new old arranger scenes"><span><a href="https://codeberg.org/unspeaker/tek/commit/9aeb792f7dc0d4ee115d619d62916710ca1a3e9f">wip: new old arranger scenes</a></span>
		
	</span>


					</p></div>
			</th>
			<th><relative-time prefix="" tense="past" datetime="2025-05-15T23:06:15+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 23:06:15 +03:00</relative-time></th>
		</tr>
	</thead>
	<tbody>
		
		
			
			
			
			<tr data-entryname=".forgejo" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.forgejo/workflows" title=".forgejo/workflows">
									
									
									<span>.forgejo</span>/workflows
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/0fa6d31c7e463db0be8d7c2645ba84c908e4d90e">ci: disable cache</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-19T23:07:37+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-19 23:07:37 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname=".old" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.old" title=".old">
									
									
									.old
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/e3b12a1d3638b7a7fa351ccd78cbfe751ef69675">add old tengri demo</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-08T22:06:40+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-08 22:06:40 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="build" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/build" title="build">
									
									
									build
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/8e9d7dc9a1b6f1dde51107d63834437a35b3e3ea">document build directory</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-20T00:46:25+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-20 00:46:25 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="config" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/config" title="config">
									
									
									config
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/9aeb792f7dc0d4ee115d619d62916710ca1a3e9f">wip: new old arranger scenes</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-15T23:06:15+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 23:06:15 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="crates" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/crates" title="crates">
									
									
									crates
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/9aeb792f7dc0d4ee115d619d62916710ca1a3e9f">wip: new old arranger scenes</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-15T23:06:15+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 23:06:15 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="deps" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/deps" title="deps">
									
									
									deps
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/094d5dd451792f39990a00d8f4b632d25c2631e3">wip: reimplement arranger because i'm sick of that shit</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-15T00:01:26+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 00:01:26 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="target" data-ready="true">
				<td>
					<span>
						
							
								
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/target" title="target">
									
									
									target
									
								</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/4ba88bfd6dce48a627bb4c6f2a1cb679688280ef">groovebox: layout</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-15T14:45:06+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-15 14:45:06 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname=".dockerignore" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.dockerignore" title=".dockerignore">.dockerignore</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/c439528cfca459b4f794ba3ea49cee8318bb8568">add containerized release build</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-19T01:20:41+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-19 01:20:41 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname=".gitignore" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.gitignore" title=".gitignore">.gitignore</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/4d3f308c1054eecb6bca065e4ec8e8457b310c3b">add stub target</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-20T00:42:22+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-20 00:42:22 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname=".gitmodules" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/.gitmodules" title=".gitmodules">.gitmodules</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/393634a1a4768ff494f3129b7c08df35f4966cb3">local dockerized build</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-19T03:23:09+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-19 03:23:09 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="architecture.svg" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/architecture.svg" title="architecture.svg">architecture.svg</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/47c34d0077c3c91ffe8fad857c0b280baab45a15">move some docs to root</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2024-12-30T21:56:52+01:00" data-tooltip-content="" data-tooltip-interactive="true">2024-12-30 21:56:52 +01:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="Cargo.lock" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/Cargo.lock" title="Cargo.lock">Cargo.lock</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/6ce83fb27a60f0a2532520a39596adc253081b2d">wip: down to 25 errors woo</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-14T02:13:16+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-14 02:13:16 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="Cargo.toml" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/Cargo.toml" title="Cargo.toml">Cargo.toml</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/6286d6982460f65394e2125833f3349baf49aa99">wip: device: reenable lv2 support</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-04T18:23:44+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-04 18:23:44 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="CONTRIBUTING.md" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/CONTRIBUTING.md" title="CONTRIBUTING.md">CONTRIBUTING.md</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/393634a1a4768ff494f3129b7c08df35f4966cb3">local dockerized build</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-04-19T03:23:09+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-04-19 03:23:09 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="Justfile" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/Justfile" title="Justfile">Justfile</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/d647fc68e942300c6ba0109633df32ad4fe87581">just: connect to all firefox jack outputs</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-11T19:18:19+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-11 19:18:19 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="LICENSE" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/LICENSE" title="LICENSE">LICENSE</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/0c6484d733df4567fd8dfba48847c033a2a78e91">add license</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-03-24T03:18:18+02:00" data-tooltip-content="" data-tooltip-interactive="true">2025-03-24 03:18:18 +02:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="README.md" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/README.md" title="README.md">README.md</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/5c74ffd91611ee346f91a224d17e7f401758f693">docs: remove mention of wasd keybinds</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-05-09T22:02:09+03:00" data-tooltip-content="" data-tooltip-interactive="true">2025-05-09 22:02:09 +03:00</relative-time></td>
			</tr>
		
			
			
			
			<tr data-entryname="shell.nix" data-ready="true">
				<td>
					<span>
						
							
								<a href="https://codeberg.org/unspeaker/tek/src/branch/main/shell.nix" title="shell.nix">shell.nix</a>
							
						
					</span>
				</td>
				<td>
					<span>
						
							
							<a href="https://codeberg.org/unspeaker/tek/commit/3837dbd47bff70ecf07c92e7b63119fe2e500486">add thiserror to edn module; 38.57% total cov</a>
						
					</span>
				</td>
				<td><relative-time prefix="" tense="past" datetime="2025-02-26T15:09:44+02:00" data-tooltip-content="" data-tooltip-interactive="true">2025-02-26 15:09:44 +02:00</relative-time></td>
			</tr>
		
	</tbody>
</table>

	<div id="readme">

	

	<h4>
		<div>
			
				
				<p><strong><a href="#readme">README.md</a></strong>
			
		</p></div>
		
	</h4>
	<div>
			
				
<p dir="auto">a music making program for <a href="https://sw.kovidgoyal.net/kitty/" rel="nofollow">24-bit unicode terminals</a>.</p>
<p dir="auto">written in <a href="https://www.rust-lang.org/" rel="nofollow">rust</a>
with <a href="https://ratatui.rs/" rel="nofollow">ratatui</a> on <a href="https://docs.rs/crossterm/latest/crossterm/" rel="nofollow">crossterm</a>
for <a href="https://jackaudio.org/" rel="nofollow">jack</a> and <a href="https://www.pipewire.org/" rel="nofollow">pipewire</a>.</p>
<p dir="auto"><strong>tek</strong> is available as <a href="https://codeberg.org/unspeaker/tek#building-from-source" rel="nofollow">source</a>,
<a href="https://codeberg.org/unspeaker/tek/releases" rel="nofollow">statically linked binaries</a>, and on the
<a href="https://codeberg.org/unspeaker/tek#arch-linux" rel="nofollow">aur</a>.</p>
<p dir="auto">author is reachable via <a href="https://mastodon.social/@unspeaker" rel="nofollow"><strong>mastodon</strong> <code>@unspeaker@mastodon.social</code></a>
or <a href="https://matrix.to/#/@unspeaker:matrix.org" rel="nofollow"><strong>matrix</strong> <code>@unspeaker:matrix.org</code></a></p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://codeberg.org/unspeaker/tek/attachments/5014ff4d-9ece-4862-90de-3bc6573eacf6" target="_blank" rel="nofollow noopener"><img src="https://codeberg.org/unspeaker/tek/attachments/5014ff4d-9ece-4862-90de-3bc6573eacf6" alt="Screenshot of Arranger Mode"></a></td>
<td><a href="https://codeberg.org/unspeaker/tek/releases/download/0.2.0-rc.7/Screenshot%20From%202025-01-02%2023-18-05.png" target="_blank" rel="nofollow noopener"><img src="https://codeberg.org/unspeaker/tek/releases/download/0.2.0-rc.7/Screenshot%20From%202025-01-02%2023-18-05.png" alt="Screenshot of Groovebox Mode"></a></td>
</tr>
</tbody>
</table>
<h2 id="user-content-usage" dir="auto">usage</h2>
<ul dir="auto">
<li><strong>requirements:</strong> linux; jack or pipewire; 24-bit terminal (i use <code>kitty</code>)</li>
<li><strong>recommended:</strong> midi controller; samples in wav format; lv2 plugins.</li>
</ul>
<h2 id="user-content-keymaps" dir="auto">keymaps</h2>
<ul dir="auto">
<li>Arranger:
<ul dir="auto">
<li>arrows: navigate</li>
<li>tab: enter editor</li>
<li><code>q</code>: enqueue clip</li>
<li>space: play/pause</li>
</ul>
</li>
<li>Editor:
<ul dir="auto">
<li>arrows: navigate</li>
<li><code>,</code> / <code>.</code>: change note length</li>
<li>enter: write note</li>
<li><code>-</code> / <code>=</code>: zoom midi editor</li>
<li><code>z</code>: zoom lock/unlock</li>
<li>del: delete</li>
</ul>
</li>
<li>Global:
<ul dir="auto">
<li>esc: options menu</li>
<li>f1: help/command list</li>
<li>f2: rename</li>
<li>f6: save</li>
<li>f9: load</li>
</ul>
</li>
</ul>
<h2 id="user-content-installation" dir="auto">installation</h2>
<h3 id="user-content-binary-download" dir="auto">binary download</h3>
<p dir="auto">you can download <a href="https://codeberg.org/unspeaker/tek/releases/tag/0.2.0" rel="nofollow">tek 0.2.0 "almost static"</a>
from codeberg releases. this standalone binary release, should work on any glibc-based system.</p>
<h3 id="user-content-from-distro-repositories" dir="auto">from distro repositories</h3>
<p dir="auto"><a href="https://repology.org/project/tek/versions" rel="nofollow"><img src="https://repology.org/badge/vertical-allrepos/tek.svg" alt="Packaging status"></a></p>
<h4 id="user-content-arch-linux" dir="auto">arch linux</h4>
<p dir="auto"><a href="https://aur.archlinux.org/packages/tek" rel="nofollow">tek 0.2.0-rc7</a> is available as a package in the AUR.
you can install it using your preferred AUR helper (e.g. <code>paru</code>):</p>
<pre><code>paru -S tek
</code></pre><h3 id="user-content-building-from-source" dir="auto">building from source</h3>
<p dir="auto">requires docker.</p>
<pre><code>git clone --recursive -b 0.2 https://codeberg.org/unspeaker/tek
cd tek                             # enter directory
cat bin/release-glibc.sh           # preview build script
sudo bin/release-glibc.sh          # run build script
sudo cp bin/tek /usr/local/bin/tek # install
</code></pre><h2 id="user-content-design-goals" dir="auto">design goals</h2>
<ul dir="auto">
<li>
<p dir="auto">inspired by trackers and hardware sequencers,
but with the critical feature that 90s samplers lack:
able to <strong>resample, i.e. record while playing!</strong></p>
</li>
<li>
<p dir="auto"><strong>pop-up scratchpad for musical ideas.</strong>
low resource consumption, can stay open in background.
but flexible enough to allow expanding on compositions</p>
</li>
<li>
<p dir="auto"><strong>human- and machine- readable project format</strong>
simple representation for project data
enable scripting and remapping.</p>
</li>
</ul>

			
		</div>
</div>



		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The unreasonable effectiveness of an LLM agent loop with tool use (352 pts)]]></title>
            <link>https://sketch.dev/blog/agent-loop</link>
            <guid>43998472</guid>
            <pubDate>Thu, 15 May 2025 19:33:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sketch.dev/blog/agent-loop">https://sketch.dev/blog/agent-loop</a>, See on <a href="https://news.ycombinator.com/item?id=43998472">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      
      <p><em>2025-05-15</em> by Philip Zeyliger</p>
      <p>
  <span>My co-workers and I have been working on an AI Programming Assistant called</span>
  <span><a href="https://sketch.dev/">Sketch</a></span>
  <span> for the last few months. The thing I've been most surprised by is how shockingly simple the main loop of using an LLM with tool use is:</span>
</p>

<pre><code>def loop(llm):
    msg = user_input()
    while True:
        output, tool_calls = llm(msg)
        print("Agent: ", output)
        if tool_calls:
            msg = [ handle_tool_call(tc) for tc in tool_calls ]
        else:
            msg = user_input()
</code></pre>

<p>
  <span>There's some pomp and circumstance to make the above work (</span><span><a href="https://sketch.dev/blog/agent_loop.py">here's the full script</a></span><span>)</span>
  <span>, but the core idea is the above 9 lines. Here, <code>llm()</code> is a function that sends the system prompt, the conversation so far, and the next message to the LLM API.</span>
</p>

<p>
  <span>Tool use is the fancy term for "the LLM returns some output that corresponds to a schema," and, in the full script, we tell the LLM (in its system prompt and tool description prompts) that it has access to <code>bash</code>.</span>
</p>

<p>
  <span>With just that one very general purpose tool, the current models (we use Claude 3.7 Sonnet extensively) can nail many problems, some of them in "one shot." Whereas I used to look up an esoteric git operation and then cut and paste, now I just ask Sketch to do it. Whereas I used to handle git merges manually, now I let Sketch take a first pass. Whereas I used to change a type and go through the resulting type checker errors one by one (or, let's be real, with <code>perl -pie</code> ridiculousness), I give it a shot with Sketch. If appropriately prompted, the agentic loop can be persistent. If you don't have some tool installed, it'll install it. If your `grep` has different command line options, it adapts. (It can also be infuriating! "Oh, this test doesn't pass... let's just skip it," it sometimes says, maddeningly.)</span>
</p>

<p>
  <span>For many workflows, agentic tools specialize. Sketch's quiver of tools is not just <code>bash</code>, as we've found that a handful of extra tools improve the quality, speed up iterations, and facilitate better developer workflows. Tools that let the LLM edit text correctly are surprisingly tricky. Seeing the LLM struggle with <code>sed</code> one-liners re-affirms that visual (as opposed to line) editors are a marvel.</span>
</p>

<p>
  <span>I have no doubt that agent loops will get incorporated into more day to day automation tedium that's historically been too specific for general purpose tools and too esoteric and unstable to automate traditionally. I keep thinking of how much time I've spent correlating stack traces with git commits, and how good LLMs are at doing a first pass on it. We'll be seeing more custom, <span>ad hoc</span>, throw-away LLM agent loops in our <code>bin/</code> directories. Grab your favorite bearer token and give it a shot.</span>
</p>

<p>
<span><i>Also published at <span><a href="https://philz.dev/blog/agent-loop/">philz.dev/blog/agent-loop/</a>.</span></i></span><i>
</i></p><i>
</i> 
      <p>
        <a href="https://sketch.dev/">sketch.dev</a> ¬∑
        <a href="https://merde.ai/">merde.ai</a> ¬∑ <a href="https://pi.dev/">pi.dev</a>
      </p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The current state of TLA‚Å∫ development (120 pts)]]></title>
            <link>https://ahelwer.ca/post/2025-05-15-tla-dev-status/</link>
            <guid>43998115</guid>
            <pubDate>Thu, 15 May 2025 18:53:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ahelwer.ca/post/2025-05-15-tla-dev-status/">https://ahelwer.ca/post/2025-05-15-tla-dev-status/</a>, See on <a href="https://news.ycombinator.com/item?id=43998115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
      <article role="main">
        <p>The <a href="https://conf.tlapl.us/2025-etaps/">2025 TLA‚Å∫ Community Event</a> was held last week on May 4th at McMaster University in Hamilton, Ontario, Canada.
It was a satellite event to <a href="https://etaps.org/2025/">ETAPS 2025</a>, which I also attended, and plan to write about in the near future.
I gave a talk somewhat-hucksterishly titled <em>It‚Äôs never been easier to write TLA‚Å∫ tooling!</em> which I will spin into a general account of the state of TLA‚Å∫ development here.
The conference talks were all recorded, so if you‚Äôd like this blog post in video form you can watch it below:</p>

    <p>
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube-nocookie.com/embed/KrhZebeRn90?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" title="YouTube video"></iframe>
    </p>

<h2 id="outline">Outline</h2>
<p>The thesis of this post is that almost all the dreams &amp; desires we have for TLA‚Å∫ are downstream of making it easy to develop in or on TLA‚Å∫ language tooling.
We break this down into three parts:</p>
<ol>
<li>Overview of existing TLA‚Å∫ language tooling</li>
<li>Overcoming the legacy code challenge</li>
<li>Some TLA‚Å∫ development ideas for the near and middle future</li>
</ol>
<p>I‚Äôm very optimistic about the future of TLA‚Å∫, despite the challenges.
We now have the <a href="https://foundation.tlapl.us/">TLA‚Å∫ Foundation</a>, which has very generously been paying me a comfortable living wage to work on the tools for the past six months.
I will talk about some of the things I‚Äôve accomplished.
Being paid to work on FOSS is a blessed existence; if that sounds appealing to you as well, the TLA‚Å∫ Foundation is <a href="https://foundation.tlapl.us/grants/2024-grant-program/index.html">looking to fund more contributors</a>!</p>

<p>This is a summary of what TLA‚Å∫ language tooling exists, to stoke the imagination for what tools you could contribute to or build upon.</p>
<h3 id="parsers">Parsers</h3>
<p>Parsers are of course the foundation of any language tooling, because your tools need to know what some source code actually says.
We have a decent stable of them:</p>
<ul>
<li><a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools">SANY</a> (Syntactic ANalYzer) - our flagship parser, a Java-based recursive descent parser generated by <a href="https://github.com/javacc/javacc">JavaCC</a>.
This is the only parser that really handles all the syntax and does a lot of semantic checks (including level-checking, which is the closest thing TLA‚Å∫ has to type checking) that other parsers don‚Äôt.</li>
<li><a href="https://github.com/tlaplus/tlapm">The TLAPM parser</a> - a parser-combinator-based OCaml parser, this is used by the TLA‚Å∫ Proof System.
It handles almost all TLA‚Å∫ syntax and does some rudimentary semantic analysis.</li>
<li><a href="https://github.com/tlaplus-community/tree-sitter-tlaplus/">tree-sitter-tlaplus</a> - I wrote &amp; <a href="https://ahelwer.ca/post/2023-01-11-tree-sitter-tlaplus/">presented this</a> at the 2021 TLA‚Å∫ Community Event.
Like all <a href="https://tree-sitter.github.io/tree-sitter/">tree-sitter</a> grammars, it uses a JavaScript-based DSL to generate a LR(1) parser in C along with some handwritten C to handle context-sensitive syntax.
This makes it very portable, and a strong tree-sitter ecosystem has developed with nice bindings for Python, Rust, JavaScript, and other languages.
This parser only handles syntax, no semantic checks, but that‚Äôs good enough for a lot of tools.
Its LR(1) nature also makes it a great static analyzer for ambiguity introduced by <a href="https://github.com/tlaplus/rfcs/issues/9">proposed TLA‚Å∫ language extensions</a>.</li>
</ul>
<p>Those are the big three.
There are a couple others floating around, in varying stages of completion: <a href="https://pypi.org/project/tla/">one in Python</a>, <a href="https://github.com/ret/specifica">Haskell</a>, <a href="https://github.com/bugarela/tla-transmutation">Haskell (again)</a>, and <a href="https://codeberg.org/tlaplus/rstla">Rust</a> (I got halfway through writing that last one myself before getting lost in the sauce of the Rust type system &amp; burning out).</p>
<h3 id="interpreters">Interpreters</h3>
<p>Moving up the stack.
For the longest time we had only one interpreter, and now we have two!</p>
<ul>
<li>The one used by the <a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools/src/tlc2">TLC</a> (Temporal Logic Checker) finite model checker - it‚Äôs written in Java and uses SANY for parsing.
It also now has a REPL.</li>
<li><a href="https://github.com/will62794/spectacle">Spectacle</a> - presented by <a href="https://will62794.github.io/">William Schultz</a> at the <a href="https://conf.tlapl.us/2024/">2024 TLA‚Å∫ conference</a>,  this is a web-based interpreter written in JavaScript using tree-sitter-tlaplus as a parser.
I think this tool is really neat!
It has a large focus on sharing specs &amp; traces with colleagues.</li>
</ul>
<h3 id="model-checkers">Model Checkers</h3>
<p>Again moving up the stack.
We now have <em>three</em> model checkers!</p>
<ul>
<li><a href="https://github.com/tlaplus/tlaplus/tree/e61495931507f5ad5ec232be9e65fe8f2a1bf379/tlatools/org.lamport.tlatools/src/tlc2">TLC</a> - mentioned previously, is the only fully-featured model checker.
It‚Äôs a finite state model checker, with all the benefits (predictable &amp; understandable performance) &amp; drawbacks (combinatoric state explosion) that implies.
It is the only checker that fully handles liveness, in particular refinement - a banner TLA‚Å∫ language feature, and arguably its <em>raison d‚Äô√™tre</em>.
It is written in Java and uses SANY for parsing.</li>
<li><a href="https://github.com/apalache-mc/apalache">Apalache</a> - a symbolic model checker that uses <a href="https://github.com/Z3Prover/z3">Z3</a> to model-check TLA‚Å∫ specifications.
It handles some liveness properties but not all.
It is written in Scala, and also uses SANY for parsing.</li>
<li><a href="https://informal.systems/">Spectacle</a> - this was recently updated to do basic safety checking by running a breadth-first search of the model‚Äôs state space, in the browser!
Once you do the difficult work of developing a TLA‚Å∫ interpreter it is not too much extra effort to add safety checking (liveness checking is where it really starts to get hairy).
As covered above, this is written in JavaScript and uses tree-sitter-tlaplus for parsing.</li>
</ul>
<h3 id="others">Others</h3>
<p>Here‚Äôs a grab bag of other notable TLA‚Å∫ language tools:</p>
<ul>
<li><a href="https://github.com/tlaplus/tlapm">TLAPM</a> - a system for validating proofs written in TLA‚Å∫‚Äôs proof language.
It‚Äôs written in OCaml and uses its own parser, described above.
TLA‚Å∫ proofs are translated to obligations for discharge by an array of backend provers including <a href="https://www.cl.cam.ac.uk/research/hvg/Isabelle/">Isabelle</a>, <a href="https://github.com/Z3Prover/z3">Z3</a>, and <a href="https://github.com/zenon-prover/zenon">Zenon</a>.</li>
<li><a href="https://apalache-mc.org/docs/apalache/typechecker-snowcat.html">The Snowcat Type Checker</a> - this is bundled with Apalache and verifies TLA‚Å∫ type annotations encoded in comments.
I think it‚Äôs a very neat project that is underutilized.
Modulo some weirdness around conversions between functions and sequences, many industry-produced TLA‚Å∫ specs have variables spanning a small set of values and would benefit from catching type errors at parse time.</li>
<li><a href="https://github.com/FedericoPonzi/tlaplus-formatter">Tlaplus-formatter</a> &amp; <a href="https://github.com/domodwyer/tlafmt">tlafmt</a> - we now have not one but <em>two</em> TLA‚Å∫ formatters!
The former is written in Java and uses SANY for parsing, while the latter is written in Rust and uses tree-sitter-tlaplus.</li>
<li><a href="https://github.com/tlaplus/tlapm/tree/e9b8bb51818f0b454384e8d94fe614899a0aaa78/lsp">LSP Server</a> - written by <a href="https://github.com/kape1395">Karolis Petrauskas</a>, this is aimed at improving support for the TLA‚Å∫ proof language in the TLA‚Å∫ VS Code extension.
It is written in OCaml on top of the TLAPM parser.</li>
<li><a href="https://github.com/tlaplus-community/tlauc">TLAUC</a> - the TLA‚Å∫ Unicode Converter, written by myself in Rust using tree-sitter-tlaplus.
This converts your TLA‚Å∫ specs between their LaTeX-like ASCII and Unicode representations.
I introduced <a href="https://ahelwer.ca/post/2024-05-28-tla-unicode/">Unicode support to TLA‚Å∫</a> at the <a href="https://conf.tlapl.us/2024/">2024 TLA‚Å∫ conference</a>.
Write your TLA‚Å∫ specs in Unicode!
It‚Äôs great!</li>
<li><a href="https://github.com/tlaplus/vscode-tlaplus/">VS Code Extension</a> - slowly replacing the Eclipse-based TLA‚Å∫ Toolbox as the IDE of choice, this is written in TypeScript and uses regexes to cobble together some concept of TLA‚Å∫ parsing.
Don‚Äôt get me wrong, it‚Äôs really a nice project!
It recently <a href="https://github.com/tlaplus/vscode-tlaplus/pull/367">bundled SANY</a> so is offloading more and more functionality onto a real parser.</li>
</ul>
<h3 id="summing-it-up">Summing it up</h3>
<p>I think the current state of the TLA‚Å∫ tooling ecosystem is <em>strong</em>.
This section was quite long once I wrote it all out.
People don‚Äôt just want to use TLA‚Å∫, they want to make it better!
That‚Äôs a really good energy to have in the community.
A lot of people don‚Äôt like being tied to the JVM but alternatives are slowly taking shape.</p>
<h2 id="overcoming-the-legacy-code-challenge">Overcoming the Legacy Code Challenge</h2>
<p>Many working software engineers have read (or at least heard of) this book:</p>
<p><img src="https://ahelwer.ca/post/2025-05-15-tla-dev-status/legacy-code.jpg" alt=""></p>
<p>It has a concise &amp; objective definition of legacy code that I like: legacy code is code without tests.
However, as I‚Äôve grown in my career I‚Äôve arrived at a fuzzier definition: legacy code is code not in living knowledge.
The codebase or even a particular module does not exist in totality inside the heads of the people who work on it.
We‚Äôve probably all written software from scratch and know how easy it is to make massive changes because you know how every part works.
If you hand the same task to a codebase newcomer you can reasonably expect it to take 10-100x as long to effect the same changes, and the codebase end state will be much less coherent.</p>
<p>We have a large shortfall of living knowledge in the TLA‚Å∫ project, which I will further elaborate in the next section.
The thesis of this section is simple: we must overcome this challenge if TLA‚Å∫ is to <em><strong>thrive</strong></em>.
TLA‚Å∫ will trundle along on inertia for quite a while, but eventually find its way into irrelevance if we do not conquer the challenge before us.</p>
<p>One last note on living knowledge: it is interlinked with tests.
Once living knowledge is lost it must be rebuilt from scratch, and that is only possible if you can write tests for the codebase.
Poke &amp; prod it, see how it flows and is interlinked.
Most of <em>Working Effectively with Legacy Code</em> deals with how to bootstrap your way to safely making changes in a codebase that lacks tests, when writing tests <em>requires</em> changing the codebase to make it testable.</p>
<h3 id="the-challenge-for-tla">The Challenge for TLA‚Å∫</h3>
<p>First, a quick history:
Leslie Lamport published the paper <a href="https://lamport.azurewebsites.net/pubs/pubs.html#old-tla-src"><em>A Temporal Logic of Actions</em></a> in April of 1990.
Jean-Charles Gregoir wrote the initial SANY TLA‚Å∫ parser in the late 1990s.
The TLC model checker was written over 25 years ago, announced in the 1999 paper <a href="https://lamport.azurewebsites.net/pubs/pubs.html#yuanyu-model-checking"><em>Model Checking TLA‚Å∫ Specifications</em></a> by Yuan Yu and Panagiotis Manolios which described writing an interpreter &amp; safety checker for TLA‚Å∫.
By 2002 the liveness checker <a href="https://lamport.azurewebsites.net/pubs/pubs.html#spec-and-verifying">had been developed</a>.
The tools were subsequently heavily modified around 2010 for the release of <a href="https://lamport.azurewebsites.net/tla/tla2.html">TLA‚Å∫ 2</a>, which added the proof language.
Around this time Inria &amp; Microsoft Research partnered to write TLAPM so the proof language could be formally checked.
Inria funding lapsed in the late 2010s and TLAPM development slowed dramatically.
Development on the Apalache symbolic model checker began in the late 2010s, and accelerated with <a href="https://ahelwer.ca/post/2025-05-15-tla-dev-status/informal.systems">Informal Systems</a> bringing it in-house in the early 2020s, but development has similarly slowed dramatically since Informal Systems spun out Apalache near the end of 2024.
The <a href="https://foundation.tlapl.us/">TLA‚Å∫ Foundation</a> was founded to support TLA‚Å∫ as it was spun out from Microsoft Research in the leadup to Leslie Lamport‚Äôs retirement at the end of 2024.</p>
<p>We will focus on the challenge of maintaining SANY and TLC.
By comparison to SANY &amp; TLC, TLAPM and Apalache are much less approachable by non-academic software engineers and I would be interested in watching talks on how their development can be ramped up &amp; made sustainable.</p>
<p>So, we have a large quarter-century-old Java codebase that is difficult to unit test due to prolific global static state, where all the original authors have long since left the project, and where the three regular core contributors lack knowledge of its many parts.
Each of us focuses on a different area: I tend to work on the parser, <a href="https://github.com/lemmy">Markus Kuppe</a> tends to work on the model checker, and <a href="https://calvin.loncaric.us/">Calvin Loncaric</a> tends to work on the interpreter &amp; is also our resident true Java expert.
There are many things we want to do but are restrained by the unknowns.
This all sounds somewhat dire, so why am I optimistic?</p>
<h3 id="the-dream-and-our-blessings">The Dream and Our Blessings</h3>
<p>Let‚Äôs start with a dream: <strong>a world where we are bold and unafraid to make large changes to the core TLA‚Å∫ tools</strong>.
How do we get there?</p>
<p>Thankfully, we start from a position of strength.
Language tooling is a blessed area of software engineering.
It is one of the few places where program requirements are relatively complete &amp; unambiguous.
It is actually possible to <em>complete</em> a parser!
Usually starting a software project is kind of like casting a curse upon yourself, where you must continually provide it your labor or it will disappear from relevance and all your work will be for nought.
Not so here.
Of course, conventional language tooling demands do become stronger over time.
Parsers have to be useful in a language server context now, which requires better error recovery than was previously standard.
But these are from relatively slow changes in development culture and only crop up every decade or so.</p>
<h3 id="concrete-strategies">Concrete Strategies</h3>
<p>Enough faffing around.
Here are the three strategies I think will bring us from dream to reality:</p>
<ol>
<li><strong>Tests</strong> - Of course!
The TLA‚Å∫ Foundation has been funding me to put a <em>ton</em> of effort into developing <a href="https://github.com/tlaplus/rfcs/tree/2a772d9dd11acec5d7dedf30abfab91a49de48b8/language_standard/tests">standardized implementation-independent test suites</a> for TLA‚Å∫ parsers, then applying them to the existing parsers.
We now have a nice source-code-input/expected-AST-output syntax suite applied to the three prominent parsers (good for conformance!) along with a semantic-level test suite with lots of in-language assertions about levels and references (thanks to <a href="https://lobste.rs/s/58jah7/what_implementation_independent_test">lobste.rs users</a> for some great suggestions on its design) and - my favorite - a set of 150ish TLA‚Å∫ parse inputs that should each trigger a specific standardized error code in any conformant parser.
Combined, I think these make every TLA‚Å∫ parser open for development!
We are no longer constrained to analyzing bugs by git-bisecting &amp; comparing inscrutable code; just write a test and trace it in the debugger!</li>
<li><strong>Developer Onboarding</strong> - We must go beyond documentation.
Treat the codebase as a fixed artifact to be studied &amp; learned by any new contributor, then develop a syllabus for it.
The TLA‚Å∫ Foundation has been funding me for this in various ways, such as writing <a href="https://github.com/tlaplus/tlaplus/blob/e61495931507f5ad5ec232be9e65fe8f2a1bf379/DEVELOPING.md">developing</a> &amp; <a href="https://github.com/tlaplus/tlaplus/blob/e61495931507f5ad5ec232be9e65fe8f2a1bf379/CONTRIBUTING.md">contributing</a> guides, highlighting good first issues in <a href="https://foundation.tlapl.us/blog/index.html">monthly development updates</a>, and - most substantially - writing a long tutorial on <a href="https://docs.tlapl.us/creating:start">how to write your <em>own</em> minimal TLA‚Å∫ model checker</a>!
This last one is very fun and is based on the excellent free online textbook <a href="https://craftinginterpreters.com/"><em>Crafting Interpreters</em></a> by Robert Nystrom.
Give it a look!</li>
<li><strong>Grants &amp; Stipends</strong> - The fundamental contradiction of FOSS development is that every substantial FOSS project requires <em>full-time</em> commitment for long periods of time, but people need to eat and it‚Äôs hard to make money giving something away for free.
When I first got into this game I thought Linux was something people developed on their evenings and weekends.
That‚Äôs true for some people, but the overwhelming majority of code is contributed by people working on Linux full-time!
A lot of digital ink has been spilled about how to manage the fundamental FOSS contradiction.
Foundations with corporate sponsorship are one method, and that‚Äôs what we have.
The TLA‚Å∫ Foundation can fund people to work on the tools, and they are.
Interested?
<a href="https://foundation.tlapl.us/grants/2024-grant-program/index.html">Join us</a>!</li>
</ol>
<h3 id="summing-it-up-1">Summing it up</h3>
<p>So, we are starting from a position of strength and have concrete strategies to overcome the remaining challenges.
We should also recognize that our struggle is shared.
People in the TLA‚Å∫ community occasionally say things like ‚ÄúTLA‚Å∫ is not a programming language‚Äù, but we face the same challenges as the developers of every other programming language out there!
Even the exotic parts of TLA‚Å∫ are ‚Äújust‚Äù algorithms.
By which I mean learning algorithms is relatively easy, fun, &amp; contained compared to the challenge of developing an unknown codebase.
As complicated as the liveness checking algorithm is, it‚Äôs ‚Äújust‚Äù an algorithm on graphs that an ordinary developer can learn in a few days of study, given a suitable tutorial.
And it‚Äôs our job to write that tutorial and make it fun.</p>
<h2 id="whats-next">What‚Äôs Next?</h2>
<p>I ended the talk with a few ideas I had for TLA‚Å∫ tool development in the near future.
One, inspired by attendance at <a href="https://bugbash.antithesis.com/">Antithesis BugBash</a> last month, is generative testing for the TLA‚Å∫ tools!
It‚Äôs all well &amp; good to lock me in an office to crank out hundreds of test cases, but what if that process could be automated?
It would certainly be more fun to develop than a large static test corpus.
Actually it‚Äôs good to have both: static test cases to nail down the core language functionality, and generative tests to explore the nooks &amp; crannies.</p>
<p>I also tentatively proposed simplifications to the TLA‚Å∫ syntax.
I‚Äôm a bit of a weird guy who <em>likes</em> the TLA‚Å∫ syntax (especially in its Unicode form) but other people do struggle with it, as <a href="https://www.hillelwayne.com/">Hillel Wayne</a> can attest from his work teaching TLA‚Å∫ both online and offline.
I‚Äôm not a purist who is against evolving the syntax.
I have what I think is <a href="https://github.com/tlaplus/rfcs/issues/10">quite a nice RFC</a> to combine &amp; disambiguate the set filter &amp; set map syntax.
Larger changes will have to be debated.
This is design &amp; consensus work; the actual code changes will likely not be substantial in comparison.</p>
<p>I also want to improve the SANY API &amp; consumption experience so people can easily build their own tools instead of requesting features be upstreamed.
A new API has <a href="https://github.com/tlaplus/tlaplus/pull/1125">already been prototyped</a>, and all SANY unit tests are currently exercising it.
Markus has also put effort into automatically <a href="https://oss.sonatype.org/content/repositories/snapshots/org/lamport/tla2tools/">publishing a Java package</a> from the rolling release.
Finally, we need to write a collection of how-to guides for consuming &amp; developing against the package.
There are actually quite a few projects &amp; features in TLA‚Å∫ land that exist but have not had documentation written &amp; disseminated for them.
I think I could spend an entire year writing only tutorials &amp; documentation.
I am coming closer to being persuaded that would actually be a good use of time, maybe even the best use of my time for TLA‚Å∫.</p>
<h3 id="the-one-billion-states-per-minute-initiative">The One Billion States Per Minute Initiative</h3>
<p>I wanted to end the talk with a bang so here it is: I think we can increase the TLC model checker throughput to 1 billion states per minute (1000x speedup) by writing a <em>bytecode interpreter</em>.
Currently, TLC clocks about 1 million states per minute.
Calvin has done some <em>very</em> preliminary prototyping with transpiling TLA‚Å∫ specs to C++ then compiling &amp; running them as little breadth-first search programs, and saw some head-turning numbers.
The TLC interpreter is a tree-walk interpreter written in Java.
As optimized as it is, there‚Äôs a limit to what can be done with the format.
This would be a bold, substantial project but I think it is worth exploring.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Thanks for reading!
I hope this has invigorated &amp; inspired you to consider developing your own TLA‚Å∫ tooling or contribute to the existing tools, while giving a nice overview of the current state of things.</p>
<h2 id="links--discussions">Links &amp; Discussions</h2>
<p><a href="https://cse.buffalo.edu/~demirbas/">Murat Demirbas</a> wrote <a href="https://muratbuffalo.blogspot.com/2025/05/modular-verification-of-mongodb.html">a blog post</a> version of his talk at the TLA‚Å∫ community event, on using TLA‚Å∫ to specify cross-shard transactions in MongoDB.
He also wrote a post <a href="https://muratbuffalo.blogspot.com/2025/05/notes-from-tla-community-event.html">summarizing other talks</a> from the event.</p>
<p><a href="https://emptysqua.re/blog/about/">A. Jesse Jiryu Davis</a> wrote <a href="https://emptysqua.re/blog/are-we-serious-about-statistical-properties-tlaplus/">a blog post</a> version of his TLA‚Å∫ community event talk, on the possibility of adding statistical &amp; performance modeling capabilities to TLA‚Å∫.
He also posted <a href="https://emptysqua.re/blog/2025-tlaplus-community-event/">his notes</a> about other talks at the event.</p>
<p>Discuss this post here:</p>
<ul>
<li><a href="https://lobste.rs/s/auqf51/current_state_tla_development">lobste.rs</a></li>
<li><a href="https://discuss.systems/@ahelwer/114513355067720408">Mastodon</a></li>
<li><a href="https://news.ycombinator.com/item?id=43998115">Hacker News</a></li>
<li><a href="https://www.linkedin.com/posts/ahelwer_the-current-state-of-tla-development-activity-7328855373866504192-uv45">LinkedIn</a></li>
</ul>


        
          
        

        

        
      </article>

      
        
      


      

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Harvard Law paid $27 for a copy of Magna Carta. It's an original (182 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/15/world/europe/harvard-law-magna-carta-original.html</link>
            <guid>43997830</guid>
            <pubDate>Thu, 15 May 2025 18:26:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/15/world/europe/harvard-law-magna-carta-original.html">https://www.nytimes.com/2025/05/15/world/europe/harvard-law-magna-carta-original.html</a>, See on <a href="https://news.ycombinator.com/item?id=43997830">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/15/world/europe/harvard-law-magna-carta-original.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Baby Is Healed with First Personalized Gene-Editing Treatment (896 pts)]]></title>
            <link>https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html</link>
            <guid>43997636</guid>
            <pubDate>Thu, 15 May 2025 18:06:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html">https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html</a>, See on <a href="https://news.ycombinator.com/item?id=43997636">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2025/05/15/health/gene-editing-personalized-rare-disorders.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Tinfoil (YC X25): Verifiable Privacy for Cloud AI (126 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43996555</link>
            <guid>43996555</guid>
            <pubDate>Thu, 15 May 2025 16:19:00 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43996555">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <tbody><tr id="43996555">
      <td><span></span></td>      <td><center><a id="up_43996555" href="https://news.ycombinator.com/vote?id=43996555&amp;how=up&amp;goto=item%3Fid%3D43996555"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=43996555">Launch HN: Tinfoil (YC X25): Verifiable Privacy for Cloud AI</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_43996555">101 points</span> by <a href="https://news.ycombinator.com/user?id=FrasiertheLion">FrasiertheLion</a> <span title="2025-05-15T16:19:00 1747325940"><a href="https://news.ycombinator.com/item?id=43996555">10 hours ago</a></span> <span id="unv_43996555"></span> | <a href="https://news.ycombinator.com/hide?id=43996555&amp;goto=item%3Fid%3D43996555">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Tinfoil%20%28YC%20X25%29%3A%20Verifiable%20Privacy%20for%20Cloud%20AI&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=43996555&amp;auth=3ccb71ca1f7cf63db9871486b2141cb96771e089">favorite</a> | <a href="https://news.ycombinator.com/item?id=43996555">81&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hello HN! We‚Äôre Tanya, Sacha, Jules and Nate from Tinfoil: <a href="https://tinfoil.sh/">https://tinfoil.sh</a>. We host models and AI workloads on the cloud while guaranteeing zero data access and retention. This lets us run open-source LLMs like Llama, or Deepseek R1 on cloud GPUs without you having to trust us‚Äîor any cloud provider‚Äîwith private data.</p><p>Since AI performs better the more context you give it, we think solving AI privacy will unlock more valuable AI applications, just how TLS on the Internet enabled e-commerce to flourish knowing that your credit card info wouldn't be stolen by someone sniffing internet packets.</p><p>We come from backgrounds in cryptography, security, and infrastructure. Jules did his PhD in trusted hardware and confidential computing at MIT, and worked with NVIDIA and Microsoft Research on the same, Sacha did his PhD in privacy-preserving cryptography at MIT, Nate worked on privacy tech like Tor, and I (Tanya) was on Cloudflare's cryptography team. We were unsatisfied with band-aid techniques like PII redaction (which is actually undesirable in some cases like AI personal assistants) or ‚Äúpinky promise‚Äù security through legal contracts like DPAs. We wanted a real solution that replaced trust with provable security.</p><p>Running models locally or on-prem is an option, but can be expensive and inconvenient. Fully Homomorphic Encryption (FHE) is not practical for LLM inference for the foreseeable future. The next best option is using secure enclaves: a secure environment on the chip that no other software running on the host machine can access. This lets us perform LLM inference in the cloud while being able to prove that no one, not even Tinfoil or the cloud provider, can access the data. And because these security mechanisms are implemented in hardware, there is minimal performance overhead.</p><p>Even though we (Tinfoil) control the host machine, we do not have any visibility into the data processed inside of the enclave. At a high level, a secure enclave is a set of cores that are reserved, isolated, and locked down to create a sectioned off area. Everything that comes out of the enclave is encrypted: memory and network traffic, but also peripheral (PCIe) traffic to other devices such as the GPU. These encryptions are performed using secret keys that are generated inside the enclave during setup, which never leave its boundaries. Additionally, a ‚Äúhardware root of trust‚Äù baked into the chip lets clients check security claims and verify that all security mechanisms are in place.</p><p>Up until recently, secure enclaves were only available on CPUs. But NVIDIA confidential computing recently added these hardware-based capabilities to their latest GPUs, making it possible to run GPU-based workloads in a secure enclave.</p><p>Here‚Äôs how it works in a nutshell:</p><p>1. We publish the code that should run inside the secure enclave to Github, as well as a hash of the compiled binary to a transparency log called Sigstore</p><p>2. Before sending data to the enclave, the client fetches a signed document from the enclave which includes a hash of the running code signed by the CPU manufacturer. It then verifies the signature with the hardware manufacturer to prove the hardware is genuine. Then the client fetches a hash of the source code from a transparency log (Sigstore) and checks that the hash equals the one we got from the enclave. This lets the client get verifiable proof that the enclave is running the exact code we claim.</p><p>3. With the assurance that the enclave environment is what we expect, the client sends its data to the enclave, which travels encrypted (TLS) and is only decrypted inside the enclave.</p><p>4. Processing happens entirely within this protected environment. Even an attacker that controls the host machine can‚Äôt access this data.
We believe making end-to-end verifiability a ‚Äúfirst class citizen‚Äù is key. Secure enclaves have traditionally been used to remove trust from the cloud provider, not necessarily from the application provider. This is evidenced by confidential VM technologies such as Azure Confidential VM allowing ssh access by the host into the confidential VM. Our goal is to provably remove trust both from ourselves, aka the application provider, as well as the cloud provider.</p><p>We encourage you to be skeptical of our privacy claims. Verifiability is our answer. It‚Äôs not just us saying it‚Äôs private; the hardware and cryptography let you check. Here‚Äôs a guide that walks you through the verification process: <a href="https://docs.tinfoil.sh/verification/attestation-architecture">https://docs.tinfoil.sh/verification/attestation-architectur...</a>.</p><p>People are using us for analyzing sensitive docs, building copilots for proprietary code, and processing user data in agentic AI applications without the privacy risks that previously blocked cloud AI adoption.</p><p>We‚Äôre excited to share Tinfoil with HN!</p><p>* Try the chat (<a href="https://tinfoil.sh/chat">https://tinfoil.sh/chat</a>): It verifies attestation with an in-browser check. Free, limited messages, $20/month for unlimited messages and additional models</p><p>* Use the API (<a href="https://tinfoil.sh/inference">https://tinfoil.sh/inference</a>): OpenAI API compatible interface. $2 / 1M tokens</p><p>* Take your existing Docker image and make it end to end confidential by deploying on Tinfoil. Here's a demo of how you could use Tinfoil to run a deepfake detection service that could run securely on people's private videos: <a href="https://www.youtube.com/watch?v=_8hLmqoutyk" rel="nofollow">https://www.youtube.com/watch?v=_8hLmqoutyk</a>. Note: This feature is not currently self-serve.</p><p>* Reach out to us at contact@tinfoil.sh if you want to run a different model or want to deploy a custom application, or if you just want to learn more!</p><p>Let us know what you think, we‚Äôd love to hear about your experiences and ideas in this space!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td></td></tr>  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Don't Like NumPy (425 pts)]]></title>
            <link>https://dynomight.net/numpy/</link>
            <guid>43996431</guid>
            <pubDate>Thu, 15 May 2025 16:05:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynomight.net/numpy/">https://dynomight.net/numpy/</a>, See on <a href="https://news.ycombinator.com/item?id=43996431">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
    
  <section>
    <p>They say you can‚Äôt truly hate someone unless you loved them first. I don‚Äôt know if that‚Äôs true as a general principle, but it certainly describes my relationship with NumPy.</p>

<p><a href="https://numpy.org/">NumPy</a>, by the way, is some software that does computations on arrays in Python. It‚Äôs insanely popular and has had a huge influence on all the popular machine learning libraries like PyTorch. These libraries share most of the same issues I discuss below, but I‚Äôll stick to NumPy for concreteness.</p>

<p>NumPy makes easy things easy. Say <code>A</code> is a <code>5√ó5</code> matrix, <code>x</code> is a length-5 vector, and you want to find the vector <em>y</em> such that <code>Ay=x</code>. In NumPy, that would be:</p>

<div><pre><code><span>y</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span> <span>x</span><span>)</span>
</code></pre></div>

<p>So elegant! So clear!</p>

<p>But say the situation is even a <em>little</em> more complicated. Say <code>A</code> is a stack of 100 <code>5√ó5</code> matrices, given as a <code>100√ó5√ó5</code> array. And say <code>x</code> is a stack of 100 length-5 vectors, given as a <code>100√ó5</code> array. And say you want to solve <code>A·µ¢y·µ¢=x·µ¢</code> for <code>1‚â§i‚â§100</code>.</p>

<p>If you could use loops, this would be easy:</p>

<div><pre><code><span>y</span> <span>=</span> <span>np</span><span>.</span><span>empty_like</span><span>(</span><span>x</span><span>)</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>100</span><span>):</span>
    <span>y</span><span>[</span><span>i</span><span>,:]</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>[</span><span>i</span><span>,:,:],</span> <span>x</span><span>[</span><span>i</span><span>,:])</span>
</code></pre></div>

<p>But you can‚Äôt use loops. To some degree, this is a limitation of loops being slow in Python. But nowadays, everything is GPU and if you‚Äôve got big arrays, you probably don‚Äôt want to use loops in any language. To get all those transistors firing, you need to call special GPU functions that will sort of split up the arrays into lots of little pieces and process them in parallel.</p>

<p>The good news is that NumPy knows about those special routines (at least if you use <a href="https://github.com/jax-ml/jax">JAX</a> or <a href="https://cupy.dev/">CuPy</a>), and if you call <code>np.linalg.solve</code> correctly, it will use them.</p>

<p>The bad news is that no one knows how do that.</p>

<p>Don‚Äôt believe me? OK, which of these is right?</p>

<div><pre><code><span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>,</span><span>axis</span><span>=</span><span>0</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>,</span><span>axes</span><span>=</span><span>[[</span><span>1</span><span>,</span><span>2</span><span>],</span><span>1</span><span>])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>T</span><span>,</span> <span>x</span><span>.</span><span>T</span><span>)</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>T</span><span>,</span> <span>x</span><span>).</span><span>T</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span> <span>x</span><span>[</span><span>None</span><span>,:,:])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>[:,:,</span><span>None</span><span>])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>,</span><span>x</span><span>[:,:,</span><span>None</span><span>])[:,:,</span><span>0</span><span>]</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>[:,:,:,</span><span>None</span><span>],</span><span>x</span><span>[:,</span><span>None</span><span>,</span><span>None</span><span>,:])</span>
<span>y</span> <span>=</span> <span>linalg</span><span>.</span><span>solve</span><span>(</span><span>A</span><span>.</span><span>transpose</span><span>([</span><span>1</span><span>,</span><span>2</span><span>,</span><span>0</span><span>]),</span><span>x</span><span>[:,:,</span><span>None</span><span>]).</span><span>T</span>
</code></pre></div>

<p>No one knows. And let me show you something else. Here‚Äôs the <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html">documentation</a>:</p>

<p><img src="https://dynomight.net/img/numpy/solve.png" alt="np.linalg.solve"></p>

<p>Read that. Meditate on it. Now, notice: You <em>still</em> don‚Äôt know how to solve <code>A·µ¢y·µ¢=x·µ¢</code> for all <code>i</code> at once. Is it even possible? Did I lie when I said it was?</p>

<p>As far as I can tell, what people actually do is try random variations until one seems to work.</p>

<h2 id="why-numpy-bad">Why NumPy bad</h2>

<p>NumPy is all about applying operations to arrays. When the arrays have 2 or fewer dimensions, everything is fine. But if you‚Äôre doing something even mildly complicated, you inevitably find yourself with some operation you want to apply to some dimensions of array <code>A</code>, some other dimensions of array <code>B</code>, and some <em>other</em> dimensions of array <code>C</code>. And NumPy has no theory for how to express that.</p>

<p>Let me show you what I mean. Suppose:</p>

<ul>
  <li><code>A</code> is a <code>K√óL√óM</code> array</li>
  <li><code>B</code> is a <code>L√óN</code> array</li>
  <li><code>C</code> is a <code>K√óM</code> array</li>
</ul>

<p>And say that for each <code>k</code> and <code>n</code>, you‚Äôd like to compute the mean over the <code>L</code> and <code>M</code> dimensions. That is, you want</p>

<p>&nbsp;&nbsp;  <code>D<sub>kn</sub> = 1/(LM) √ó ‚àë<sub>lm</sub> A<sub>klm</sub> B<sub>ln</sub> C<sub>km</sub>.</code></p>

<p>To do that, you‚Äôve got two options. The first is to use grotesque dimension alignment tricks:</p>

<div><pre><code><span>D</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span>
        <span>np</span><span>.</span><span>mean</span><span>(</span>
            <span>A</span><span>[:,:,:,</span><span>None</span><span>]</span> <span>*</span>
            <span>B</span><span>[</span><span>None</span><span>,:,</span><span>None</span><span>,:]</span> <span>*</span>
            <span>C</span><span>[:,</span><span>None</span><span>,:,</span><span>None</span><span>],</span>
        <span>axis</span><span>=</span><span>1</span><span>),</span>
    <span>axis</span><span>=</span><span>1</span><span>)</span>
</code></pre></div>

<p>The hell, you ask? Why is <code>None</code> everywhere? Well, when indexing an array in NumPy, you can write <code>None</code> to insert a new dimension. <code>A</code> is <code>K√óL√óM</code>, but <code>A[:,:,:,None]</code> is <code>K√óL√óM√ó</code><strong><code>1</code></strong>. Similarly, <code>B[None,:,None,:]</code> is <strong><code>1</code></strong><code>√óL√ó</code><strong><code>1</code></strong><code>√óN</code> and <code>C[:,None,:,None]</code> is <code>K√ó</code><strong><code>1</code></strong><code>√óM√ó</code><strong><code>1</code></strong>. When you multiply these together, NumPy ‚Äúbroadcasts‚Äù all the size-1 dimensions to give a <code>K√óL√óM√óN</code> array. Then, the <code>np.mean</code> calls average over the <code>L</code> and <code>M</code> dimensions.</p>

<p>I think this is bad. I‚Äôve been using NumPy for years and I still find it impossible to write code like that without <em>always</em> making mistakes.</p>

<p>It‚Äôs also borderline-impossible to read. To prove this, I just flipped a coin and introduced a bug above if and only if the coin was tails. Is there a bug? Are you <em>sure</em>? No one knows.</p>

<p>Your second option is to desperately try to be clever. Life is short and precious, but if you spend a lot of yours reading the NumPy documentation, you might eventually realize that there‚Äôs a function called <a href="https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html"><code>np.tensordot</code></a>, and that it‚Äôs possible to make it do much of the work:</p>

<div><pre><code><span>D</span> <span>=</span> <span>(</span><span>1</span><span>/</span><span>L</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>mean</span><span>(</span>
                <span>np</span><span>.</span><span>tensordot</span><span>(</span><span>A</span><span>,</span> <span>B</span><span>,</span> <span>axes</span><span>=</span><span>[</span><span>1</span><span>,</span><span>0</span><span>])</span> <span>*</span>
                <span>C</span><span>[:,:,</span><span>None</span><span>],</span>
            <span>axis</span><span>=</span><span>1</span><span>)</span>
</code></pre></div>

<p>That‚Äôs correct. (I promise.) But why does it work? What exactly is <code>np.tensordot</code> doing? If you saw that code in some other context, would you have the slightest idea what was happening?</p>

<p>Here‚Äôs how I‚Äôd do it, if only I could use loops:</p>

<div><pre><code><span>D</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>K</span><span>,</span><span>N</span><span>))</span>  
<span>for</span> <span>k</span> <span>in</span> <span>range</span><span>(</span><span>K</span><span>):</span>  
    <span>for</span> <span>n</span> <span>in</span> <span>range</span><span>(</span><span>N</span><span>):</span>  
        <span>a</span> <span>=</span> <span>A</span><span>[</span><span>k</span><span>,:,:]</span>  
        <span>b</span> <span>=</span> <span>B</span><span>[:,</span><span>n</span><span>]</span>  
        <span>c</span> <span>=</span> <span>C</span><span>[</span><span>k</span><span>,:]</span>  
        <span>assert</span> <span>a</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>L</span><span>,</span><span>M</span><span>)</span>  
        <span>assert</span> <span>b</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>L</span><span>,)</span>  
        <span>assert</span> <span>c</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>M</span><span>,)</span>  
        <span>D</span><span>[</span><span>k</span><span>,</span><span>n</span><span>]</span> <span>=</span> <span>np</span><span>.</span><span>mean</span><span>(</span><span>a</span> <span>*</span> <span>b</span><span>[:,</span><span>None</span><span>]</span> <span>*</span> <span>c</span><span>[</span><span>None</span><span>,:])</span>
</code></pre></div>

<p>People who‚Äôve written too much NumPy may find that clunky. I suspect that‚Äôs a wee bit of Stockholm Syndrome. But surely we can agree that it‚Äôs <em>clear</em>.</p>

<p>In practice, things are often even worse. Say that <code>A</code> had shape <code>M√óK√óL</code> rather than <code>K√óL√óM</code>. With loops, no big deal. But NumPy requires you to write monstrosities like <code>A.transpose([1,2,0])</code>. Or should that be <code>A.transpose([2,0,1])</code>? What shapes do those produce? No one knows.</p>

<p>Loops were better.</p>

<h2 id="ok-i-lied">OK I lied</h2>

<p>There is a third option:</p>

<div><pre><code><span>D</span> <span>=</span> <span>1</span><span>/</span><span>(</span><span>L</span><span>*</span><span>M</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'klm,ln,km-&gt;kn'</span><span>,</span> <span>A</span><span>,</span> <span>B</span><span>,</span> <span>C</span><span>)</span>
</code></pre></div>

<p>If you‚Äôve never seen Einstein summation before, that might look terrifying. But remember, our goal is to find</p>

<p>&nbsp;&nbsp; D<sub>kn</sub> = 1/(LM) √ó ‚àë<sub>lm</sub> A<sub>klm</sub> B<sub>ln</sub> C<sub>km</sub>.</p>

<p>The string in the above code basically gives labels to the indices in each of the three inputs (<code>klm,ln,km</code>) and the target indices for the output (<code>-&gt;kn</code>). Then, <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html"><code>np.einsum</code></a> multiplies together the corresponding elements of the inputs and sums over all indices that aren‚Äôt in the output.</p>

<p>Personally, I think <code>np.einsum</code> is one of the rare parts of NumPy that‚Äôs actually good. The strings are a bit tedious, but they‚Äôre worth it, because the overall function is easy(ish) to understand, is completely explicit, and is quite general and powerful.</p>

<p>Except, how does <code>np.einsum</code> achieve all this? It uses indices. Or, more precisely, it introduces a tiny <em>domain-specific language</em> based on indices. It doesn‚Äôt suffer from NumPy‚Äôs design flaws because it refuses to play by NumPy‚Äôs normal rules.</p>

<p>But <code>np.einsum</code> only does a few things. (<a href="https://github.com/arogozhnikov/einops">Einops</a> does a few more.) What if you want to apply some other function over various dimensions of some arrays? There is no <code>np.linalg.einsolve</code>. And if you create your own function, there‚Äôs <em>certainly</em> no ‚ÄúEinstein‚Äù version of <em>it</em>.</p>

<p>I think <code>np.einsum</code>‚Äôs goodness shows that NumPy went somewhere.</p>

<h2 id="intermission">Intermission</h2>

<p>Here‚Äôs a <a href="https://www.nga.gov/artworks/45858-burning-old-south-church-bath-maine">painting</a> which feels analogous to our subject.</p>

<p><img src="https://dynomight.net/img/numpy/burning.jpg" alt=""></p>

<h2 id="where-did-numpy-go-wrong">Where did NumPy go wrong?</h2>

<p>Here‚Äôs what I want from an array language. I ain‚Äôt particular about syntax, but it would be nice if:</p>

<ol>
  <li>When you want to do something, it‚Äôs ‚Äúobvious‚Äù how to do it.</li>
  <li>When you read some code, it‚Äôs ‚Äúobvious‚Äù what it does.</li>
</ol>

<p>Wouldn‚Äôt that be nice? I think NumPy doesn‚Äôt achieve these because of its original sin: It took away indices and replaced them with broadcasting. And broadcasting cannot fill indices‚Äô shoes.</p>

<h2 id="i-dont-love-numpy-broadcasting">I don‚Äôt love NumPy broadcasting</h2>

<p>NumPy‚Äôs core trick is broadcasting. Take this code:</p>

<div><pre><code><span>A</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>1</span><span>,</span><span>2</span><span>],[</span><span>3</span><span>,</span><span>4</span><span>],[</span><span>5</span><span>,</span><span>6</span><span>]])</span>
<span>B</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>10</span><span>,</span><span>20</span><span>])</span>
<span>C</span> <span>=</span> <span>A</span> <span>*</span> <span>B</span>
<span>print</span><span>(</span><span>C</span><span>)</span>
</code></pre></div>

<p>This outputs:</p>

<div><pre><code>[[ 10  40]
 [ 30  80]
 [ 50 120]]
</code></pre></div>

<p>Here, <code>A</code> is a <code>3√ó2</code> array, and <code>B</code> is a length-<code>2</code> array. When you multiply them together, <code>B</code> is ‚Äúbroadcast‚Äù to the shape of <code>A</code>, meaning the first column of <code>A</code> is multiplied with <code>B[0]=10</code> and the second is multiplied with <code>B[1]=20</code>.</p>

<p>In simple cases, this seems good. But I don‚Äôt love it. One reason is that, as we saw above, you often have to do gross things to the dimensions to get them to line up.</p>

<p>Another reason is that it isn‚Äôt explicit or legible. Sometimes <code>A*B</code> multiplies element-by-element, and sometimes it does more complicated things. So every time you see <code>A*B</code>, you have to figure out which case in the <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html">broadcasting conventions</a> is getting triggered.</p>

<p>But the real problem with broadcasting is how it infects everything else. I‚Äôll explain below.</p>

<h2 id="i-dont-like-numpy-indexing">I don‚Äôt like NumPy indexing</h2>

<p>Here‚Äôs a riddle. Take this code:</p>

<div><pre><code><span>A</span> <span>=</span> <span>np</span><span>.</span><span>ones</span><span>((</span><span>10</span><span>,</span><span>20</span><span>,</span><span>30</span><span>,</span><span>40</span><span>))</span>
<span>i</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>])</span>
<span>j</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>0</span><span>],[</span><span>1</span><span>]])</span>
<span>B</span> <span>=</span> <span>A</span><span>[:,</span><span>i</span><span>,</span><span>j</span><span>,:]</span>
</code></pre></div>

<p>What shape does <code>B</code> have?</p>

<p>It turns out the answer is <code>10√ó2√ó3√ó40</code>. That‚Äôs because the <code>i</code> and <code>j</code> indices get broadcast to a shape of <code>2√ó3</code> and then something something mumble mumble mumble. Try to convince yourself it makes sense.</p>

<p>Done? OK, now try these:</p>

<div><pre><code><span>C</span> <span>=</span> <span>A</span><span>[:,:,</span><span>i</span><span>,</span><span>j</span><span>]</span>
<span>D</span> <span>=</span> <span>A</span><span>[:,</span><span>i</span><span>,:,</span><span>j</span><span>]</span>
<span>E</span> <span>=</span> <span>A</span><span>[:,</span><span>1</span><span>:</span><span>4</span><span>,</span><span>j</span><span>,:]</span>
</code></pre></div>

<p>What shapes do these have?</p>

<ul>
  <li>
    <p><code>C</code> is <code>10√ó20√ó2√ó3</code>. This seems logical, given what happened with <code>B</code> above.</p>
  </li>
  <li>
    <p>What about <code>D</code>? It is <code>2√ó3√ó10√ó30</code>. Now, for some reason, the <code>2</code> and <code>3</code> go at the beginning?</p>
  </li>
  <li>
    <p>And what about <code>E</code>? Well, ‚Äúslices‚Äù in Python exclude the endpoint, so <code>1:4</code> is equivalent to <code>[1,2,3]</code> which is equivalent to <code>i</code>, and so <code>E</code> is the same as <code>B</code>. Hahaha, just kidding! <code>E</code> is <code>10√ó3√ó2√ó1√ó40</code>.</p>
  </li>
</ul>

<p>Yes, that is what happens. Try it if you don‚Äôt believe me! I understand why NumPy does this, because I‚Äôve absorbed <a href="https://numpy.org/doc/stable/user/basics.indexing.html">this 5000 word document</a> that explains how NumPy indexing works. But I want that time back.</p>

<details>
  <summary>
For fun, I tried asking a bunch of AI models to figure out what shapes those arrays have. Here were the results:
</summary>

  <p>I used this query:</p>

  <blockquote>
    <p>Take this python code</p>

    <p>A = np.ones((10,20,30,40))<br>
i = np.array([1,2,3])<br>
j = np.array([[0],[1]])<br>
B = A[:,i,j,:]<br>
C = A[:,:,i,j]<br>
D = A[:,i,:,j]<br>
E = A[:,1:4,j,:]</p>

    <p>what shapes do B, C, D, and E have?</p>
  </blockquote>

  <p>Claude 3.7 used ‚Äúextended thinking‚Äù. Here are all the incorrect outputs:</p>

  <table>
    <thead>
      <tr>
        <th>AI</th>
        <th><code>B</code></th>
        <th><code>C</code></th>
        <th><code>D</code></th>
        <th><code>E</code></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>GPT 4.1</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó2√ó3√ó30</td>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td>Grok 3</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó3√ó30√ó2</td>
        <td>10√ó3√ó2√ó40</td>
      </tr>
      <tr>
        <td>Claude 3 Opus</td>
        <td>10√ó3√ó2√ó30</td>
        <td>10√ó20√ó3√ó2</td>
        <td>10√ó3√ó30√ó2</td>
        <td>10√ó3√ó2√ó40</td>
      </tr>
      <tr>
        <td>Llama 4 Maverick</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó3√ó30√ó2</td>
        <td>10√ó3√ó2√ó40</td>
      </tr>
      <tr>
        <td>o3</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó2√ó3√ó30</td>
        <td>&nbsp;</td>
      </tr>
      <tr>
        <td>Claude 3.7</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>10√ó3√ó30√ó2</td>
        <td>10√ó3√ó2√ó40</td>
      </tr>
    </tbody>
  </table>

</details>

<table>
  <thead>
    <tr>
      <th>AI</th>
      <th><code>B</code></th>
      <th><code>C</code></th>
      <th><code>D</code></th>
      <th><code>E</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT 4.1</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>‚úîÔ∏è</td>
    </tr>
    <tr>
      <td>Grok 3</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Claude 3 Opus</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Llama 4 Maverick</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>o3</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>‚úîÔ∏è</td>
    </tr>
    <tr>
      <td>Claude 3.7</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Gemini 2.5 Pro</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
    </tr>
    <tr>
      <td>DeepSeek R1</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
      <td>‚úîÔ∏è</td>
    </tr>
  </tbody>
</table>

<p>(DeepSeek‚Äôs chain of thought used ‚Äúwait‚Äù 76 times. It got everything right the first time, but when I tried it again, it somehow got <code>B</code>, <code>C</code>, and <code>D</code> all wrong, but <code>E</code> right.)</p>

<p>This is insane. Using basic features should not require solving crazy logic puzzles.</p>

<p>You might think, ‚ÄúOK, I‚Äôll just limit myself to indexing in simple ways.‚Äù Sounds good, except sometimes you <em>need</em> advanced indexing. And even if you‚Äôre doing something simple, you still need to be careful to avoid the crazy cases.</p>

<p>This again makes everything non-legible. Even if you‚Äôre just reading code that uses indexing in a simple way, how do you <em>know</em> it‚Äôs simple? If you see <code>A[B,C]</code>, that could be doing almost anything. To understand it, you need to remember the shapes of <code>A</code>, <code>B</code>, and <code>C</code> and work through all the cases. And, of course, <code>A</code>, <code>B</code>, and <code>C</code> are often produced by <em>other</em> code, which you <em>also</em> need to think about‚Ä¶</p>

<h2 id="i-dont-like-numpy-functions">I don‚Äôt like NumPy functions</h2>

<p>Why did NumPy end up with a <code>np.linalg.solve(A,B)</code> function that‚Äôs so confusing? I imagine they first made it work when <code>A</code> is a 2D array and and <code>b</code> is a 1D or 2D array, just like the mathematical notation of <code>A‚Åª¬πb</code> or <code>A‚Åª¬πB</code>.</p>

<p>So far so good. But then someone probably came along with a 3D array.  If you could use loops, the solution would be ‚Äúuse the old function with loops‚Äù. But you can‚Äôt use loops. So there were basically three options:</p>

<ol>
  <li>They could add some extra <code>axes</code> argument, so the user can specify which dimensions to operate over. Maybe you could write <code>solve(A,B,axes=[[1,2],1])</code>.</li>
  <li>They could create different  functions with different names for different situations. Maybe <code>solve_matrix_vector</code> would do one thing, <code>solve_tensor_matrix</code> would do another.</li>
  <li>They could add a Convention: Some arbitrary choice for how <code>solve</code> will internally try to line up the dimensions. Then it‚Äôs the user‚Äôs problem to figure out and conform to those Conventions.</li>
</ol>

<p>All these options are bad, because none of them can really cope with the fact that there are a combinatorial number of different cases. NumPy chose: All of them. Some functions have <code>axes</code> arguments. Some have different versions with different names. Some have Conventions. Some have Conventions <em>and</em> <code>axes</code> arguments. And some don‚Äôt provide any vectorized version at all.</p>

<p>But the <em>biggest</em> flaw of NumPy is this: Say you create a function that solves some problem with arrays of some given shape. Now, how do you apply it to particular dimensions of some larger arrays? The answer is: You re-write your function from scratch in a much more complex way. The basic principle of programming is abstraction‚Äîsolving simple problems and then using the solutions as building blocks for more complex problems. NumPy doesn‚Äôt let you do that.</p>

<h2 id="attention-please">Attention please</h2>

<p>One last example to show you what I‚Äôm talking about. Whenever I whine about NumPy, people always want to see an example with <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">self-attention</a>, the core trick behind modern language models. So fine. Here‚Äôs an implementation, which I humbly suggest is better than all 227 versions I found when I searched for ‚Äúself-attention numpy‚Äù:</p>

<div><pre><code><span># self attention by your friend dynomight
</span>
<span>input_dim</span> <span>=</span> <span>4</span>  
<span>seq_len</span> <span>=</span> <span>4</span>  
<span>d_k</span> <span>=</span> <span>5</span>  
<span>d_v</span> <span>=</span> <span>input_dim</span>  
  
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
<span>W_q</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_k</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_v</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>input_dim</span><span>,</span> <span>d_v</span><span>)</span>  
  
<span>def</span> <span>softmax</span><span>(</span><span>x</span><span>,</span> <span>axis</span><span>):</span>  
    <span>e_x</span> <span>=</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>x</span> <span>-</span> <span>np</span><span>.</span><span>max</span><span>(</span><span>x</span><span>,</span> <span>axis</span><span>=</span><span>axis</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>))</span>  
    <span>return</span> <span>e_x</span> <span>/</span> <span>np</span><span>.</span><span>sum</span><span>(</span><span>e_x</span><span>,</span> <span>axis</span><span>=</span><span>axis</span><span>,</span> <span>keepdims</span><span>=</span><span>True</span><span>)</span>  
  
<span>def</span> <span>attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>):</span>  
    <span>d_k</span> <span>=</span> <span>W_k</span><span>.</span><span>shape</span><span>[</span><span>1</span><span>]</span>  
    <span>Q</span> <span>=</span> <span>X</span> <span>@</span> <span>W_q</span>  
    <span>K</span> <span>=</span> <span>X</span> <span>@</span> <span>W_k</span>  
    <span>V</span> <span>=</span> <span>X</span> <span>@</span> <span>W_v</span>  
    <span>scores</span> <span>=</span> <span>Q</span> <span>@</span> <span>K</span><span>.</span><span>T</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>d_k</span><span>)</span>  
    <span>attention_weights</span> <span>=</span> <span>softmax</span><span>(</span><span>scores</span><span>,</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>  
    <span>return</span> <span>attention_weights</span> <span>@</span> <span>V</span>  
</code></pre></div>

<p>This is fine. Some of the <code>axis</code> stuff is a little obscure, but whatever.</p>

<p>But what language models really need is <em>multi-head</em> attention, where you sort of do attention several times in parallel and then merge the results. How do we do that?</p>

<p>First, let‚Äôs imagine we lived in a sane world where we were allowed to use abstractions. Then you could just call the previous function in a loop:</p>

<div><pre><code><span># multi-head self attention by your friend dynomight
# if only we could use loops
</span>
<span>n_head</span> <span>=</span> <span>2</span>
  
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
<span>W_q</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_k</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_k</span><span>)</span>  
<span>W_v</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>input_dim</span><span>,</span> <span>d_v</span><span>)</span>  
<span>W_o</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>randn</span><span>(</span><span>n_head</span><span>,</span> <span>d_v</span><span>,</span> <span>input_dim</span> <span>//</span> <span>n_head</span><span>)</span>

<span>def</span> <span>multi_head_attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>,</span> <span>W_o</span><span>):</span>  
    <span>projected</span> <span>=</span> <span>[]</span>  
    <span>for</span> <span>n</span> <span>in</span> <span>range</span><span>(</span><span>n_head</span><span>):</span>  
        <span>output</span> <span>=</span> <span>attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>[</span><span>n</span><span>,:,:],</span> <span>W_k</span><span>[</span><span>n</span><span>,:,:],</span> <span>W_v</span><span>[</span><span>n</span><span>,:,:])</span>  
        <span>my_proj</span> <span>=</span> <span>output</span> <span>@</span> <span>W_o</span><span>[</span><span>n</span><span>,:,:]</span>  
        <span>projected</span><span>.</span><span>append</span><span>(</span><span>my_proj</span><span>)</span>  
    <span>projected</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>projected</span><span>)</span>  
  
    <span>output</span> <span>=</span> <span>[]</span>  
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>seq_len</span><span>):</span>  
        <span>my_output</span> <span>=</span> <span>np</span><span>.</span><span>ravel</span><span>(</span><span>projected</span><span>[:,</span><span>i</span><span>,:])</span>  
        <span>output</span><span>.</span><span>append</span><span>(</span><span>my_output</span><span>)</span>  
    <span>return</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>final</span><span>)</span>  
</code></pre></div>

<p>Looks stupid, right? Yes‚Äîthank you! Cleverness is bad.</p>

<p>But we don‚Äôt live in a sane world. So instead you need to do this:</p>

<div><pre><code><span># multi-head self attention by your friend dynomight
# all vectorized and bewildering
</span>
<span>def</span> <span>multi_head_attention</span><span>(</span><span>X</span><span>,</span> <span>W_q</span><span>,</span> <span>W_k</span><span>,</span> <span>W_v</span><span>,</span> <span>W_o</span><span>):</span>  
    <span>d_k</span> <span>=</span> <span>W_k</span><span>.</span><span>shape</span><span>[</span><span>-</span><span>1</span><span>]</span>  
    <span>Q</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hij-&gt;hsj'</span><span>,</span> <span>X</span><span>,</span> <span>W_q</span><span>)</span>  
    <span>K</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hik-&gt;hsk'</span><span>,</span> <span>X</span><span>,</span> <span>W_k</span><span>)</span>  
    <span>V</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'si,hiv-&gt;hsv'</span><span>,</span> <span>X</span><span>,</span> <span>W_v</span><span>)</span>  
    <span>scores</span> <span>=</span> <span>Q</span> <span>@</span> <span>K</span><span>.</span><span>transpose</span><span>(</span><span>0</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>d_k</span><span>)</span>  
    <span>weights</span> <span>=</span> <span>softmax</span><span>(</span><span>scores</span><span>,</span> <span>axis</span><span>=-</span><span>1</span><span>)</span>
    <span>output</span> <span>=</span> <span>weights</span> <span>@</span> <span>V</span>  
    <span>projected</span> <span>=</span> <span>np</span><span>.</span><span>einsum</span><span>(</span><span>'hsv,hvd-&gt;hsd'</span><span>,</span> <span>output</span><span>,</span> <span>W_o</span><span>)</span>  
    <span>return</span> <span>projected</span><span>.</span><span>transpose</span><span>(</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>2</span><span>).</span><span>reshape</span><span>(</span><span>seq_len</span><span>,</span> <span>input_dim</span><span>)</span>  
</code></pre></div>

<p>Ha! Hahahahahaha!</p>

<h2 id="so-what-then">So what then?</h2>

<p>To be clear, I‚Äôm only suggesting that NumPy is ‚Äúthe worst array language other than all the other array languages‚Äù. What‚Äôs the point of complaining if I don‚Äôt have something better to suggest?</p>

<p>Well, actually I do have something better to suggest. I‚Äôve made a prototype of a ‚Äúbetter‚Äù NumPy that I think retains all the power while eliminating all the sharp edges. I thought this would just be a short motivational introduction, but after I started writing, the evil took hold of me and here we are 3000 words later.</p>

<p>Also, it‚Äôs probably wise to keep some distance between one‚Äôs raving polemics and one‚Äôs constructive array language API proposals. So I‚Äôll cover my new thing next time.</p>

  </section>
  
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coinbase says hackers bribed staff to steal customer data, demanding $20M ransom (327 pts)]]></title>
            <link>https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html</link>
            <guid>43996307</guid>
            <pubDate>Thu, 15 May 2025 15:52:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html">https://www.cnbc.com/2025/05/15/coinbase-says-hackers-bribed-staff-to-steal-customer-data-and-are-demanding-20-million-ransom.html</a>, See on <a href="https://news.ycombinator.com/item?id=43996307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107132326" data-test="InlineImage"><p>Jakub Porzycki | Nurphoto | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/COIN/">Coinbase</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> on Thursday reported that cybercriminals bribed overseas support agents to steal customer data to use in social engineering attacks. The incident may cost Coinbase up to $400 million to fix, the company estimated.</p><p>The crypto exchange operator received an email on May 11 from someone claiming they obtained information about certain Coinbase customer accounts as well as other internal Coinbase documentation, including materials relating to customer service and account management systems, Coinbase reported in a <a href="https://www.sec.gov/ix?doc=/Archives/edgar/data/0001679788/000167978825000094/coin-20250514.htm" target="_blank">Securities and Exchange Commission filing</a>.</p><p>The company's shares were down more than 6% in morning trading.</p><p>The email demanded money in exchange for not publicly disclosing the information, but Coinbase says it has not paid the demand and is cooperating with law enforcement on the investigation of the incident.</p><p>Although passwords and private keys were not compromised, affected data included sensitive data such as names, addresses, phone numbers and emails; masked bank account numbers and identifiers as well as the last four digits of Social Security numbers; government ID images and account balances, the company said.</p><p>"Cyber criminals bribed and recruited a group of rogue overseas support agents to steal Coinbase customer data to facilitate social engineering attacks," the company said in a <a href="https://www.coinbase.com/blog/protecting-our-customers-standing-up-to-extortionists" target="_blank">blog post</a>. "These insiders abused their access to customer support systems to steal the account data for a small subset of customers. No passwords, private keys, or funds were exposed and Coinbase Prime accounts are untouched. We will reimburse customers who were tricked into sending funds to the attacker."</p><p>Coinbase had detected the breach independently in previous months, per the filing. It immediately terminated the employees involved, warned customers whose information may have been accessed and enhanced its fraud monitoring protections.</p><p>The threat actor paid overseas contractors and employees in support rolls to obtain the information, it said.</p><p>"We're cooperating closely with law enforcement to pursue the harshest penalties possible and will not pay the $20 million ransom demand we received," the company said in the blog. "Instead we are establishing a $20 million reward fund for information leading to the arrest and conviction of the criminals responsible for this attack."</p><p>Coinbase operates the largest crypto exchange in the U.S. In the past week it <a href="https://www.cnbc.com/2025/05/08/coinbase-acquires-crypto-derivatives-exchange-deribit-for-2point9-billion.html">announced an acquisition</a> that is expected to help it expand its global reach and gained <a href="https://www.cnbc.com/2025/05/13/coinbase-stock-enters-sp-500-a-watershed-moment-for-crypto-industry-analysts-say.html">entry to the benchmark S&amp;P 500</a> stock index, which will take effect next week. On the earnings call last week, CEO Brian Armstrong discussed his ambition to make Coinbase "the <a href="https://www.cnbc.com/2025/05/11/coinbase-aims-to-be-worlds-number-1-financial-service-app-in-10-years.html">No. 1 financial services app</a> in the world" in the next five to 10 years.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[California sent residents' personal health data to LinkedIn (173 pts)]]></title>
            <link>https://themarkup.org/pixel-hunt/2025/04/28/how-california-sent-residents-personal-health-data-to-linkedin</link>
            <guid>43995302</guid>
            <pubDate>Thu, 15 May 2025 14:13:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://themarkup.org/pixel-hunt/2025/04/28/how-california-sent-residents-personal-health-data-to-linkedin">https://themarkup.org/pixel-hunt/2025/04/28/how-california-sent-residents-personal-health-data-to-linkedin</a>, See on <a href="https://news.ycombinator.com/item?id=43995302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><em>The Markup, now a part of CalMatters, uses investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up for </em><a href="https://mrkup.org/XvjZS"><em>Klaxon</em></a><em>, a newsletter that delivers our stories and tools directly to your inbox.</em></p>
<!-- no template found for topper block -->




<p>The website that lets Californians shop for health insurance under the Affordable Care Act, coveredca.com, has been sending sensitive data to  LinkedIn, forensic testing by The Markup has revealed.&nbsp;</p>



<p>As visitors filled out forms on the website, trackers on the same pages told LinkedIn their answers to questions about whether they were blind, pregnant, or used a high number of prescription medications. The trackers also monitored whether the visitors said they were transgender or possible victims of domestic abuse. </p>





<p>Covered California, the organization that operates the website, removed the trackers as The Markup and CalMatters reported this article. The organization said they were removed ‚Äúdue to a marketing agency transition‚Äù in early April.&nbsp;</p>



<p>In a statement, Kelly Donohue, a spokesperson for the agency, confirmed that data was sent to LinkedIn as part of an advertising campaign. Since&nbsp; being informed of the tracking, ‚Äúall active advertising-related tags across our website have been turned off out of an abundance of caution,‚Äù she added.&nbsp;</p>



<p>‚ÄúCovered California has initiated a review of our websites and information security and privacy protocols to ensure that no analytics tools are impermissibly sharing sensitive consumer information,‚Äù Donohue said, adding that they would ‚Äúshare additional findings as they become available, taking any necessary steps to safeguard the security and privacy of consumer data.‚Äù</p>





<div>
	
	<ol>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-interface-360x443.png" alt="Screenshot of the interface for coveredca.com displaying a respondent‚Äôs selection for being pregnant">
</picture></div>
				</figure>
			</li>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/pregnant-backend-360x443.png" alt="Screenshot highlighting the line of code where coveredca.com shares the respondent‚Äôs pregnancy status with LinkedIn">
</picture></div>
				</figure>
			</li>
		
	</ol>
	<p>
		When an individual indicated they were pregnant, the information was sent to LinkedIn via the Insight Tag.

&nbsp;
	</p>
	<p><span>Credit:</span>
		
	</p>
</div>


<p>Visitors who filled out health information on the site may have had their data tracked for more than a year, according to Donohue, who said the LinkedIn campaign began in February 2024.&nbsp;</p>





<p>The Markup observed the trackers directly in February and March of this year. It confirmed most ad trackers, including the Meta ‚Äúpixel‚Äù tracker, as well as all third-party cookies, have been removed from the site as of April 21.&nbsp;</p>



<p>Since 2014, more than 50 million Americans have <a href="https://home.treasury.gov/news/press-releases/jy2567" target="_blank">signed up</a> for health insurance through state exchanges like Covered California. They were set up under the <a href="https://calmatters.org/tag/affordable-care-act/" target="_blank">Affordable Care Act</a>, signed into law by President Barack Obama 15 years ago. States can either operate their exchange websites in partnership with the federal government or independently, <a href="https://www.commonwealthfund.org/publications/maps-and-interactives/aca-state-marketplace-models-and-key-policy-decisions" target="_blank">as California does</a>.&nbsp;</p>



<p>Covered California operates as an independent entity within the state government. Its <a href="https://board.coveredca.com/" target="_blank">board</a> is appointed by the governor and Legislature.&nbsp;</p>



<p>In March, <a href="https://www.coveredca.com/newsroom/news-releases/2025/03/24/with-record-high-enrollment-covered-california-celebrates-the-15th-anniversary-of-the-historic-affordable-care-act/" target="_blank">Covered California announced</a> that, after four years of increasing enrollment, a record of nearly 2 million people were covered by health insurance through the program. In all, the organization said, about one in six Californians were at one point enrolled through Covered California. Between 2014 and 2023, the uninsured rate fell from 17.2% to 6.4%, according to the organization, the largest drop of any state during that time period. This <a href="https://calmatters.org/explainers/california-health-care-coverage/" target="_blank">coincided with a series of eligibility </a>expansions to <a href="https://calmatters.org/tag/medi-cal/" target="_blank">Medi-Cal</a>, the state‚Äôs health insurance program for lower-income households.</p>



  



<p>Experts expressed alarm at the idea that those millions of people could have had sensitive health data sent to a private company without their knowledge or consent. Sara Geoghegan, senior counsel at the Electronic Privacy Information Center, said it was ‚Äúconcerning and invasive‚Äù for a health insurance website to be sending data that was ‚Äúwholly irrelevant‚Äù to the uses of a for-profit company like LinkedIn.</p>



<p>‚ÄúIt‚Äôs unfortunate,‚Äù she said, ‚Äúbecause people don‚Äôt expect that their health information will be collected and used in this way.‚Äù</p>


<div>
	
	<p><a href="#the-linkedin-insight-tag">‚Ü©Ô∏é link</a></p><h2 id="the-linkedin-insight-tag">The LinkedIn Insight&nbsp;Tag</h2>
</div>


<p>The Markup and CalMatters in recent months scanned for trackers on hundreds of California state and county government websites that offer services for undocumented immigrants using <a href="https://themarkup.org/blacklight">Blacklight</a>, an automated tool developed by The Markup for auditing website trackers.&nbsp;</p>



<figure><blockquote><p>People don‚Äôt expect that their health information will be collected and used in this way.</p><cite>Sara Geoghegan, senior counsel at the Electronic Privacy Information Center</cite></blockquote></figure>



<p>The Markup found that Covered California had more than 60 trackers on its site. Out of more than 200 of the government sites, the average number of trackers on the sites was three. Covered California had dozens more than any other website we examined.&nbsp;</p>



<p>On coveredca.com, trackers from well-known social media firms like Meta collected information on visitor page views, while lesser-known analytics and media campaign companies like email marketing company LiveIntent also followed users across the site.&nbsp;</p>



<p>But by far the most sensitive information was transmitted to LinkedIn.&nbsp;</p>



<p>While some of the data sent to LinkedIn was relatively innocuous, such as what pages were visited, Covered California also sent the company detailed information when visitors selected doctors to see if they were covered by a plan, including their specialization. The site also told LinkedIn if someone searched for a specific hospital.</p>





<div>
	
	<ol>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-interface-360x443.png" alt="Screenshot of the interface for coveredca.com displaying a medical provider with their redacted first name and address">
</picture></div>
				</figure>
			</li>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/provider-tracker-360x443.png" alt="Screenshot highlighting the line of code where coveredca.com shares the respondent‚Äôs medical provider with LinkedIn">
</picture></div>
				</figure>
			</li>
		
	</ol>
	<p>
		When an individual selected a medical provider, the information was sent to LinkedIn via the Insight Tag.
	</p>
	<p><span>Credit:</span>
		
	</p>
</div>


<p>&nbsp;In addition to demographic information including gender, the site also shared details with LinkedIn when visitors selected their ethnicity and marital status, and when they told coveredca.com how often they saw doctors for surgery or outpatient treatment.&nbsp;</p>





<div>
	
	<ol>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface-360x442.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface-360x442.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-interface-360x442.png" alt="Screenshot of the interface for coveredca.com displaying a respondent‚Äôs selection for being of Hispanic, Latino or Spanish origin">
</picture></div>
				</figure>
			</li>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker-360x442.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker-360x442.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/race-ethnicity-tracker-360x442.png" alt="Screenshot highlighting the line of code where coveredca.com shares the respondent‚Äôs ethnicity with LinkedIn">
</picture></div>
				</figure>
			</li>
		
	</ol>
	<p>
		When an individual selected their ethnicity, the information was sent to LinkedIn via the Insight Tag.
	</p>
	<p><span>Credit:</span>
		
	</p>
</div>


<p>LinkedIn, like other large social media firms, offers a way for websites to easily transmit data on their visitors through a tracking tool that the sites can place on their pages. In LinkedIn‚Äôs case, this <a href="https://www.linkedin.com/help/lms/answer/a418880" target="_blank">tool is called the Insight Tag</a>. By using the tag, businesses and other organizations can <a href="https://www.linkedin.com/help/lms/answer/a427660" target="_blank">later target advertisements</a> on LinkedIn to consumers that have already shown interest in their products or services. For an e-commerce site, a tracker on a page might be able to note when someone added a product to their cart, and the business can then send ads for that product to the same person on their social media feeds.&nbsp;</p>



<p>A health care marketplace like Covered California might use the trackers to reach a group of people who might be interested in a reminder of a deadline for open health insurance enrollment, for example.</p>



<p>In its statement, Covered California noted the usefulness of these tools, saying the organization ‚Äúleverages LinkedIn‚Äôs advertising platform tools to understand consumer behavior and deliver tailored messages to help them make informed decisions about their health care options.‚Äù</p>





<div>
	
	<ol>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-interface-360x443.png" alt="Screenshot of the interface for coveredca.com displaying a respondent‚Äôs selection for being a victim of domestic abuse or spousal abandonment">
</picture></div>
				</figure>
			</li>
		
			<li>
				<figure>
					
					<div data-fullscreen-desktop="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker.png" data-fullscreen-mobile="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker.png"><picture>
  <source media="(min-width: 491px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker.png 2x">
  <source media="(max-width: 490px)" srcset="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker-360x443.png, https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker.png 3x">
  <img src="https://mrkp-static-production.themarkup.org/uploads/2025/04/domestic-abuse-tracker-360x443.png" alt="Screenshot highlighting the line of code where coveredca.com shares the respondent‚Äôs data about domestic abuse with LinkedIn">
</picture></div>
				</figure>
			</li>
		
	</ol>
	<p>
		When an individual indicated they were a victim of domestic abuse or spousal abandonment, the information was sent to LinkedIn via the Insight Tag.
	</p>
	<p><span>Credit:</span>
		
	</p>
</div>


<p>Trackers can also be valuable to the social media companies that offer them. In addition to driving ad sales, they provide an opportunity to gather information on visitors to websites other than their own.</p>



<p>On <a href="https://www.linkedin.com/help/lms/answer/a418880/add-the-linkedin-insight-tag-to-your-website?lang=en" target="_blank">its informational page</a> about the Insight Tag, LinkedIn places the burden on websites that employ the tag not to use it in risky situations. The tag ‚Äúshould not be installed on web pages that collect or contain Sensitive Data,‚Äù the page advises, including ‚Äúpages offering specific health-related or financial services or products to consumers.‚Äù</p>



<p>LinkedIn spokesperson Brionna Ruff said in an emailed statement, ‚ÄúOur Ads Agreement and documentation expressly prohibit customers from installing the Insight Tag on web pages that collect or contain sensitive data, including pages offering health-related services. We don‚Äôt allow advertisers to target ads based on sensitive data or categories.‚Äù</p>





<p>Collection of sensitive information by social media trackers has in previous instances led to removal of the trackers, lawsuits, and scrutiny by state and federal lawmakers.</p>





<p>For example, after The Markup in 2022 <a href="https://themarkup.org/pixel-hunt/2022/04/28/applied-for-student-aid-online-facebook-saw-you">revealed the Department of Education sent personal information to Facebook</a> when students applied for college financial aid online, the department turned off the sharing, faced <a href="https://themarkup.org/pixel-hunt/2022/05/11/lawmakers-question-education-department-about-facebook-student-aid-tracking-after-markup-investigation">questions</a> from two members of Congress, and was <a href="https://themarkup.org/pixel-hunt/2024/07/26/department-of-education-sued-following-markup-investigation-into-fafsa-data-shared-with-facebook">sued by two advocacy groups</a> who sought more information about the sharing. Other stories in the same <a href="https://themarkup.org/series/pixel-hunt">series about trackers, known as the Pixel Hunt</a>, also led to changes and blowback, including a <a href="https://themarkup.org/pixel-hunt/2024/04/19/ftc-cracks-down-on-telehealth-addiction-service-monument-for-sharing-health-data">crackdown by the Federal Trade Commission</a> on telehealth companies transmitting personal information to companies including Meta and Google without user consent and proposed class action lawsuits over information shared through trackers with <a href="https://themarkup.org/hello-world/2023/09/30/our-pixel-hunt-project-keeps-paying-dividends">drug stores, health providers</a>, and <a href="https://themarkup.org/pixel-hunt/2022/12/02/meta-sued-for-collecting-financial-information-through-tax-filing-websites">tax prep companies</a>.</p>



<p>LinkedIn is already facing multiple proposed class-action lawsuits related to the collection of medical information. In October, <a href="https://www.bankinfosecurity.com/lawsuits-accuse-linkedin-tracking-users-health-info-a-26668" target="_blank">three new lawsuits in California courts alleged</a> that LinkedIn violated users‚Äô privacy by collecting information on medical appointment sites, including for a fertility clinic.&nbsp;</p>



<p>Social media companies‚Äô tracking practices have underpinned the tremendous growth of the tech industry, but few web users are aware of how far the tracking goes. ‚ÄúThis absolutely contradicts the expectation of the average consumer,‚Äù Geoghegan said.&nbsp;</p>



<p>In California, a law called the California Confidentiality of Medical Information Act governs the privacy of medical information in the state. Under the act, consumers must give permission to some organizations before their medical information is disclosed to third parties. Companies have faced litigation under the law for using web tracking technologies, although those suits have <a href="https://www.hklaw.com/en/news/pressreleases/2024/08/holland-knight-wins-dismissal-of-meta-pixel-data-privacy-class-action" target="_blank">not always been successful</a>.&nbsp;</p>



<p>Geoghegan said current protections like these don‚Äôt go far enough in helping consumers protect their sensitive data.&nbsp;</p>



<p>‚ÄúThis is an exact example of why we need better protections,‚Äù she said of LinkedIn receiving the data. ‚ÄúThis is sensitive health information that consumers expect to be protected and a lack of regulations is failing us.‚Äù</p>



</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Tiny Boltzmann Machine (241 pts)]]></title>
            <link>https://eoinmurray.info/boltzmann-machine</link>
            <guid>43995005</guid>
            <pubDate>Thu, 15 May 2025 13:41:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eoinmurray.info/boltzmann-machine">https://eoinmurray.info/boltzmann-machine</a>, See on <a href="https://news.ycombinator.com/item?id=43995005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Boltzmann Machines</p><p>Here we introduce introduction to Boltzmann machines and
present a Tiny Restricted Boltzmann Machine that runs in the browser.</p><a href="#trainer"><span>Skip to Simulator</span></a></div><div><div><h2 id="boltzmann-machines-are-one-of-the-earliest-generative-ai-models-introduced-in-the-1980s">Boltzmann Machines are one of the earliest generative AI models, introduced in the 1980s.</h2><p>Boltzmann Machines are used for unsupervised learning, which means they can learn
from data without being told what to look for.</p><p>The can be used for generating new data that is similar to the data they were trained on, also known as generative AI.</p></div><p><img alt="Boltzmann Machine" loading="lazy" width="600" height="600" decoding="async" data-nimg="1" srcset="https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=640&amp;q=75 1x, https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=1200&amp;q=75 2x" src="https://eoinmurray.info/_next/image?url=%2Fboltzmann-machine%2Fpaper.png&amp;w=1200&amp;q=75"></p></div><div><div><h2 id="boltzmann-machine">Boltzmann Machine</h2><p>A Boltzmann Machine is a type of neural network that tries to learn patterns by mimicking
how energy works in physics.</p><p>Each neuron can be on or off, the machine is made up of many of these neurons connect to each other.</p><p>Some neurons are <span></span> visible (we can see them and even set their state), and some are <span></span> hidden (we can't see them).</p><p>The connections between neurons are called weights, and they can be <span></span> positive or <span></span> negative.</p></div><div><svg width="400" height="400"><g><circle cx="200" cy="360" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="145.27677706789302" cy="350.35081932574536" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="354.35081932574536" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="97.1539824501537" cy="322.5671108990365" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.1539824501537" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="61.43593539448983" cy="280.00000000000006" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.43593539448983" y="284.00000000000006" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="42.43075951804673" cy="227.78370842670884" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.43075951804673" y="231.78370842670884" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="42.4307595180467" cy="172.2162915732912" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.4307595180467" y="176.2162915732912" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="61.435935394489775" cy="120.00000000000004" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.435935394489775" y="124.00000000000004" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="97.15398245015368" cy="77.43288910096354" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.15398245015368" y="81.43288910096354" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="145.27677706789302" cy="49.64918067425464" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="53.64918067425464" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="199.99999999999997" cy="40" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="199.99999999999997" y="44" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="200" cy="40" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="44" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="254.723222932107" cy="49.649180674254666" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="53.649180674254666" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="302.8460175498463" cy="77.43288910096352" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.8460175498463" y="81.43288910096352" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="338.56406460551017" cy="120" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.56406460551017" y="124" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="357.5692404819533" cy="172.21629157329113" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="176.21629157329113" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="357.5692404819533" cy="227.78370842670887" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="231.78370842670887" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="338.5640646055102" cy="280" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.5640646055102" y="284" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="302.84601754984635" cy="322.5671108990365" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.84601754984635" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="254.723222932107" cy="350.3508193257453" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="354.3508193257453" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="200" cy="360" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>Hover over the neurons to highlight their connections.</p></div></div><div><div><p>General Boltzmann Machine</p><svg width="400" height="400"><g><circle cx="200" cy="360" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="145.27677706789302" cy="350.35081932574536" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="354.35081932574536" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="97.1539824501537" cy="322.5671108990365" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.1539824501537" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="61.43593539448983" cy="280.00000000000006" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.43593539448983" y="284.00000000000006" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="42.43075951804673" cy="227.78370842670884" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.43075951804673" y="231.78370842670884" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="42.4307595180467" cy="172.2162915732912" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="42.4307595180467" y="176.2162915732912" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="61.435935394489775" cy="120.00000000000004" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="61.435935394489775" y="124.00000000000004" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="97.15398245015368" cy="77.43288910096354" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="97.15398245015368" y="81.43288910096354" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="145.27677706789302" cy="49.64918067425464" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="145.27677706789302" y="53.64918067425464" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="199.99999999999997" cy="40" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="199.99999999999997" y="44" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="200" cy="40" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="44" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="254.723222932107" cy="49.649180674254666" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="53.649180674254666" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="302.8460175498463" cy="77.43288910096352" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.8460175498463" y="81.43288910096352" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="338.56406460551017" cy="120" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.56406460551017" y="124" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="357.5692404819533" cy="172.21629157329113" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="176.21629157329113" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="357.5692404819533" cy="227.78370842670887" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="357.5692404819533" y="231.78370842670887" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="338.5640646055102" cy="280" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="338.5640646055102" y="284" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="302.84601754984635" cy="322.5671108990365" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="302.84601754984635" y="326.5671108990365" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="254.723222932107" cy="350.3508193257453" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="254.723222932107" y="354.3508193257453" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="200" cy="360" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="200" y="364" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>Hover over the neurons to highlight their connections.</p><p>A General Boltzmann Machine has connections between all neurons. This makes it powerful, but its training involves calculating an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mn>2</mn><mi>n</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(2^n)</annotation></semantics></math></span></span> term.</p></div><div><p>Restricted Boltzmann Machine</p><svg width="400" height="400"><g><circle cx="50" cy="36.36363636363637" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="40.36363636363637" font-size="8" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="72.72727272727273" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="76.72727272727273" font-size="8" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="109.0909090909091" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="113.0909090909091" font-size="8" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="145.45454545454547" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="149.45454545454547" font-size="8" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="181.81818181818184" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="185.81818181818184" font-size="8" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="218.1818181818182" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="222.1818181818182" font-size="8" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="254.54545454545456" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="258.54545454545456" font-size="8" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="290.90909090909093" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="294.90909090909093" font-size="8" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="327.2727272727273" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="331.2727272727273" font-size="8" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="363.6363636363637" r="20" fill="rgb(255, 255, 255)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="50" y="367.6363636363637" font-size="8" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="36.36363636363637" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="40.36363636363637" font-size="8" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="72.72727272727273" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="76.72727272727273" font-size="8" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="109.0909090909091" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="113.0909090909091" font-size="8" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="145.45454545454547" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="149.45454545454547" font-size="8" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="181.81818181818184" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="185.81818181818184" font-size="8" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="218.1818181818182" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="222.1818181818182" font-size="8" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="254.54545454545456" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="258.54545454545456" font-size="8" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="290.90909090909093" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="294.90909090909093" font-size="8" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="327.2727272727273" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="331.2727272727273" font-size="8" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="363.6363636363637" r="20" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1" style="cursor:pointer"></circle><text x="350" y="367.6363636363637" font-size="8" fill="#000" text-anchor="middle">h9</text></g></svg><p>A Restricted Boltzmann Machine is a special case where the visible and hidden neurons are not connected to each other. This makes it faster to train and understand.</p></div></div><div><h2 id="a-boltzmann-machine-is-an-energy-based-model">A Boltzmann Machine is an energy based model.</h2><p>The energy of a configuration of the visible and hidden units is defined as:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>b</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>c</mi><mi>j</mi></msub><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">  E(v,h) = -\sum_{i=1}^{m} \sum_{j=1}^{n} w_{ij} v_i h_j - \sum_{i=1}^{m} b_i v_i - \sum_{j=1}^{n} c_j h_j</annotation></semantics></math></span></span></span></p><p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is the visible layer, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> is the hidden layer, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span></span> is the weight matrix, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> are the biases for the visible
and hidden layers, respectively.</p><p>The visualisation on the right randomises the weights, biases and activation values of a Boltzmann machine and calculates its energy.</p></div><div><div><h2 id="training-and-generation">Training and Generation</h2><p>During training it is given examples (e.g., images, text) and the machine adjusts its
weights to lower the energy of those samples.</p><p>It effectively learns <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v)</annotation></semantics></math></span></span>, the probability of visible units <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span>, which
is proportional to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>‚àí</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">e^{-E(v)}</annotation></semantics></math></span></span>.</p><p>After training, it can sample new data from the learned distribution using Gibbs sampling.</p><p>These samples are new, never-before-seen, but statistically similar to the training data.</p></div><div><p>Here is our training data.</p><p>We want the network to learn how to make similar samples to these.</p></div></div><div id="trainer"><div><p>Lets simulate one step at a time</p><p>A Restricted Boltzmann Machine (RBM) is trained using a process called Contrastive Divergence. The steps are as follows:</p><div><ol><li><span>Step 1:</span><span> <!-- -->Clamping visible units to data</span></li><li><span>Step 2:</span><span> Sampling hidden units</span></li><li><span>Step 3:</span><span> Sampling visible units</span></li><li><span>Step 4:</span><span> Sampling hidden units</span></li><li><span>Step 5:</span><span> Updating weights</span></li></ol></div><div><p>A more formal description of the steps above are given in the</p><!-- --> <p><a href="#appendix">Appendix</a>.</p></div></div><div><div><h2>Input Sample</h2></div><p><svg width="400" height="400"><rect x="25" y="47.72727272727272" width="350" height="344.5454545454545" stroke="black" stroke-width="2" fill="none" rx="10"></rect><g><circle cx="50" cy="72.72727272727272" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="105.45454545454545" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="138.1818181818182" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="170.9090909090909" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="203.63636363636363" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="236.36363636363637" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="269.0909090909091" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="301.8181818181818" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="334.54545454545456" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="367.27272727272725" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="72.72727272727272" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="105.45454545454545" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="138.1818181818182" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="170.9090909090909" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="203.63636363636363" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="236.36363636363637" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="269.0909090909091" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="301.8181818181818" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="334.54545454545456" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="367.27272727272725" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">h9</text></g></svg></p></div></div><div id="trainer"><div><p>Simulator</p><p>Press the "Run Simulation" button to start traininng the RBM. If you let the simulation run for a while, you will see the weights of the RBM converge to a stable state. The energy loss will also decrease over time.</p><p><strong>You can compare the input and output states of the RBM by pausing the simulation</strong>.</p><p>In the beginning, the input and output states will be dissimilar. As the simulation progresses, the input and output states will become more similar.</p></div><div><div><div><h2>Input Sample</h2></div><p><svg width="400" height="400"><g><circle cx="50" cy="72.72727272727272" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">v0</text></g><g><circle cx="50" cy="105.45454545454545" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">v1</text></g><g><circle cx="50" cy="138.1818181818182" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">v2</text></g><g><circle cx="50" cy="170.9090909090909" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">v3</text></g><g><circle cx="50" cy="203.63636363636363" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">v4</text></g><g><circle cx="50" cy="236.36363636363637" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">v5</text></g><g><circle cx="50" cy="269.0909090909091" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">v6</text></g><g><circle cx="50" cy="301.8181818181818" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">v7</text></g><g><circle cx="50" cy="334.54545454545456" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">v8</text></g><g><circle cx="50" cy="367.27272727272725" r="15" fill="rgb(200, 200, 200)" stroke="#000" stroke-width="1"></circle><text x="50" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">v9</text></g><g><circle cx="350" cy="72.72727272727272" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="76.72727272727272" font-size="12" fill="#000" text-anchor="middle">h0</text></g><g><circle cx="350" cy="105.45454545454545" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="109.45454545454545" font-size="12" fill="#000" text-anchor="middle">h1</text></g><g><circle cx="350" cy="138.1818181818182" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="142.1818181818182" font-size="12" fill="#000" text-anchor="middle">h2</text></g><g><circle cx="350" cy="170.9090909090909" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="174.9090909090909" font-size="12" fill="#000" text-anchor="middle">h3</text></g><g><circle cx="350" cy="203.63636363636363" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="207.63636363636363" font-size="12" fill="#000" text-anchor="middle">h4</text></g><g><circle cx="350" cy="236.36363636363637" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="240.36363636363637" font-size="12" fill="#000" text-anchor="middle">h5</text></g><g><circle cx="350" cy="269.0909090909091" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="273.0909090909091" font-size="12" fill="#000" text-anchor="middle">h6</text></g><g><circle cx="350" cy="301.8181818181818" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="305.8181818181818" font-size="12" fill="#000" text-anchor="middle">h7</text></g><g><circle cx="350" cy="334.54545454545456" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="338.54545454545456" font-size="12" fill="#000" text-anchor="middle">h8</text></g><g><circle cx="350" cy="367.27272727272725" r="15" fill="rgb(50, 50, 50)" stroke="#000" stroke-width="1"></circle><text x="350" y="371.27272727272725" font-size="12" fill="#000" text-anchor="middle">h9</text></g></svg></p><div><h2>Output</h2></div></div><div><div><div><h2>Reconstruction Accuracy</h2></div><div><h2>Energy</h2></div></div><div><h2>Weights</h2></div></div></div></div><div id="appendix"><div><p>Appendix: Contrastive Divergence</p><p>Starting with a Boltzmann machine as defined earlier, we
want to derivce the contrastive divergence algorithm for training.
The goal is to adjust the weights of the network to minimize the energy of the training data.</p><p>We have:</p><ul>
<li>A visisble layer <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> and a hidden layer <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span>.</li>
<li>A weight matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span></span> that connects the visible and hidden layers.</li>
<li>A bias vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span> for the visible layer and a bias vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> for the hidden layer.</li>
</ul><p>Energy function in matrix form:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><msub><mi>b</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>c</mi><mi>j</mi></msub><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">E(v,h) = -\sum_{i=1}^{m} \sum_{j=1}^{n} w_{ij} v_i h_j - \sum_{i=1}^{m} b_i v_i - \sum_{j=1}^{n} c_j h_j</annotation></semantics></math></span></span></span></p><p>Joint distribution:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>Z</mi></mfrac><msup><mi>e</mi><mrow><mo>‚àí</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">P(v,h) = \frac{1}{Z} e^{-E(v,h)}</annotation></semantics></math></span></span></span></p><p>Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span></span> is the partition function, which normalizes the distribution.</p></div><div><p>We train the RBM by maximizing the likelihood of the training data, i.e. maximizing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>log</mtext><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{log}(P(v))</annotation></semantics></math></span></span>.</p><p>The marginal likelihood of the visible layer is given by:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>‚àë</mo><mi>h</mi></munder><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v) = \sum_{h} P(v,h)</annotation></semantics></math></span></span></span></p><p>Then the log-likelihood is:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>log</mtext><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mtext>log</mtext><munder><mo>‚àë</mo><mi>h</mi></munder><mfrac><mn>1</mn><mi>Z</mi></mfrac><msup><mi>e</mi><mrow><mo>‚àí</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mtext>log</mtext><munder><mo>‚àë</mo><mi>h</mi></munder><msup><mi>e</mi><mrow><mo>‚àí</mo><mi>E</mi><mo stretchy="false">(</mo><mi>v</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow></msup><mo>‚àí</mo><mtext>log</mtext><mo stretchy="false">(</mo><mi>Z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{log}(P(v)) = \text{log}\sum_{h} \frac{1}{Z} e^{-E(v,h)} = \text{log}\sum_{h} e^{-E(v,h)} - \text{log}(Z)</annotation></semantics></math></span></span></span></p><p>Differentiating with respect to the weights <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span></span> gives:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi>log</mi><mo>‚Å°</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><munder><mo>‚àë</mo><mi mathvariant="bold">h</mi></munder><msup><mi>e</mi><mrow><mo>‚àí</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><munder><mo>‚àë</mo><mi mathvariant="bold">h</mi></munder><mrow><mo fence="true">(</mo><mo>‚àí</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo fence="true">)</mo></mrow><msup><mi>e</mi><mrow><mo>‚àí</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mo>‚àí</mo><mfrac><mn>1</mn><mi>Z</mi></mfrac><munder><mo>‚àë</mo><mrow><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi></mrow></munder><mrow><mo fence="true">(</mo><mo>‚àí</mo><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo fence="true">)</mo></mrow><msup><mi>e</mi><mrow><mo>‚àí</mo><mi>E</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mo separator="true">,</mo><mi mathvariant="bold">h</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\frac{\partial \log P(\mathbf{v})}{\partial w_{ij}} 
&amp;= \frac{1}{\sum_{\mathbf{h}} e^{-E(\mathbf{v}, \mathbf{h})}} 
\sum_{\mathbf{h}} \left( -\frac{\partial E(\mathbf{v}, \mathbf{h})}{\partial w_{ij}} \right) e^{-E(\mathbf{v}, \mathbf{h})} \\
&amp;\quad - \frac{1}{Z} \sum_{\mathbf{v}, \mathbf{h}} \left( -\frac{\partial E(\mathbf{v}, \mathbf{h})}{\partial w_{ij}} \right) e^{-E(\mathbf{v}, \mathbf{h})}
\end{align*}</annotation></semantics></math></span></span></span></p><p>Similar forms exist for the biases <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_j</annotation></semantics></math></span></span>.</p><p>Since we are performing gradient ascent, therefore.</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Œî</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚Üê</mo><mi mathvariant="normal">Œî</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>Œ∑</mi><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi>log</mi><mo>‚Å°</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\Delta w_{ij} \leftarrow \Delta w_{ij} + \eta \frac{\partial \log P(v)}{\partial w_{ij}}</annotation></semantics></math></span></span></span></p><p>Therefore we get our weight update rule:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Œî</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>Œ∑</mi><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mi>log</mi><mo>‚Å°</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><mi>Œ∑</mi><mrow><mo fence="true">(</mo><mo stretchy="false">‚ü®</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>‚àí</mo><mo stretchy="false">‚ü®</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta w_{ij} = \eta \frac{\partial \log P(v)}{\partial w_{ij}} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p></div><div><p>A similar process can be followed for the biases <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_j</annotation></semantics></math></span></span>.</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Œî</mi><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mi>Œ∑</mi><mrow><mo fence="true">(</mo><mo stretchy="false">‚ü®</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>‚àí</mo><mo stretchy="false">‚ü®</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta b_i = \eta \left( \langle v_i \rangle_{data} - \langle v_i \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Œî</mi><msub><mi>c</mi><mi>j</mi></msub><mo>=</mo><mi>Œ∑</mi><mrow><mo fence="true">(</mo><mo stretchy="false">‚ü®</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>‚àí</mo><mo stretchy="false">‚ü®</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta c_j = \eta \left( \langle h_j \rangle_{data} - \langle h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p><p>Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">‚ü®</mo><mo>‚ãÖ</mo><msub><mo stretchy="false">‚ü©</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\langle \cdot \rangle_{data}</annotation></semantics></math></span></span> is the expectation with respect to the training data and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">‚ü®</mo><mo>‚ãÖ</mo><msub><mo stretchy="false">‚ü©</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\langle \cdot \rangle_{model}</annotation></semantics></math></span></span> is the expectation with respect to the model distribution.</p><p>The next step is to approximate the model expectation using Gibbs sampling.</p><ol>
<li>Positive phase: Sample <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>‚âà</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">h</mi><mi mathvariant="normal">‚à£</mi><msup><mi mathvariant="bold">v</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mtext>data</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(0)} \approx P(\mathbf{h}|\mathbf{v}^{(0)} = \text{data})</annotation></semantics></math></span></span></li>
<li>Negative phase: Run k steps of Gibbs sampling:</li>
</ol><ul>
<li>Alternating between <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">v</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>‚âà</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">v</mi><mi mathvariant="normal">‚à£</mi><mo stretchy="false">(</mo><mi>h</mi><msup><mo stretchy="false">)</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{v}^{(t+1)} \approx P(\mathbf{v}|\mathbf(h)^{(t)})</annotation></semantics></math></span></span></li>
<li>and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>‚âà</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="bold">h</mi><mi mathvariant="normal">‚à£</mi><mo stretchy="false">(</mo><mi>v</mi><msup><mo stretchy="false">)</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{h}^{(t+1)} \approx P(\mathbf{h}|\mathbf(v)^{(t)})</annotation></semantics></math></span></span></li>
</ul><p>Once those steps are done we update the weights and biases according to:</p><p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Œî</mi><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>Œ∑</mi><mrow><mo fence="true">(</mo><mo stretchy="false">‚ü®</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>‚àí</mo><mo stretchy="false">‚ü®</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta w_{ij} = \eta \left( \langle v_i h_j \rangle_{data} - \langle v_i h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Œî</mi><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mi>Œ∑</mi><mrow><mo fence="true">(</mo><mo stretchy="false">‚ü®</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>‚àí</mo><mo stretchy="false">‚ü®</mo><msub><mi>v</mi><mi>i</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta b_i = \eta \left( \langle v_i \rangle_{data} - \langle v_i \rangle_{model} \right)</annotation></semantics></math></span></span></span><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Œî</mi><msub><mi>c</mi><mi>j</mi></msub><mo>=</mo><mi>Œ∑</mi><mrow><mo fence="true">(</mo><mo stretchy="false">‚ü®</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo>‚àí</mo><mo stretchy="false">‚ü®</mo><msub><mi>h</mi><mi>j</mi></msub><msub><mo stretchy="false">‚ü©</mo><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Delta c_j = \eta \left( \langle h_j \rangle_{data} - \langle h_j \rangle_{model} \right)</annotation></semantics></math></span></span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Min.js style compression of tech docs for LLM context (171 pts)]]></title>
            <link>https://github.com/marv1nnnnn/llm-min.txt</link>
            <guid>43994987</guid>
            <pubDate>Thu, 15 May 2025 13:40:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/marv1nnnnn/llm-min.txt">https://github.com/marv1nnnnn/llm-min.txt</a>, See on <a href="https://news.ycombinator.com/item?id=43994987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">llm-min.txt: Min.js Style Compression of Tech Docs for LLM Context ü§ñ</h2><a id="user-content-llm-mintxt-minjs-style-compression-of-tech-docs-for-llm-context-" aria-label="Permalink: llm-min.txt: Min.js Style Compression of Tech Docs for LLM Context ü§ñ" href="#llm-mintxt-minjs-style-compression-of-tech-docs-for-llm-context-"></a></p>
<p dir="auto"><a href="https://opensource.org/licenses/MIT" rel="nofollow"><img src="https://camo.githubusercontent.com/6cd0120cc4c5ac11d28b2c60f76033b52db98dac641de3b2644bb054b449d60c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667" alt="License: MIT" data-canonical-src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>
<a href="https://www.python.org/downloads/" rel="nofollow"><img src="https://camo.githubusercontent.com/afa6acb8b16dba41da2aaa15991ecc9e600d28c7f905f66c32b0030df20ce925/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e31302532422d626c7565" alt="Python Version" data-canonical-src="https://img.shields.io/badge/Python-3.10%2B-blue"></a>
<a href="https://console.cloud.google.com/apis/api/gemini.googleapis.com/overview?project=llm-min" rel="nofollow"><img src="https://camo.githubusercontent.com/8e6e26ccbea21fab3467d671fb3229f2104e8e0ba589f73a07e40804f3063bce/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47656d696e692d4150492d677265656e" alt="Gemini API" data-canonical-src="https://img.shields.io/badge/Gemini-API-green"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">üìú Table of Contents</h2><a id="user-content--table-of-contents" aria-label="Permalink: üìú Table of Contents" href="#-table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#llm-mintxt-minjs-style-compression-of-tech-docs-for-llm-context-">llm-min.txt: Min.js Style Compression of Tech Docs for LLM Context ü§ñ</a>
<ul dir="auto">
<li><a href="#-table-of-contents">üìú Table of Contents</a></li>
<li><a href="#what-is-llm-mintxt-and-why-is-it-important">What is <code>llm-min.txt</code> and Why is it Important?</a></li>
<li><a href="#understanding-llm-mintxt-a-machine-optimized-format-">Understanding <code>llm-min.txt</code>: A Machine-Optimized Format üß©</a></li>
<li><a href="#does-it-really-work-visualizing-the-impact">Does it Really Work? Visualizing the Impact</a></li>
<li><a href="#its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation">It's necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using <code>llm-min</code> will largely improve the success rate of code generation.</a></li>
<li><a href="#quick-start-">Quick Start üöÄ</a></li>
<li><a href="#output-directory-structure-">Output Directory Structure üìÇ</a></li>
<li><a href="#choosing-the-right-ai-model-why-gemini-">Choosing the Right AI Model (Why Gemini) üß†</a></li>
<li><a href="#how-it-works-a-look-inside-srcllm_min-%EF%B8%8F">How it Works: A Look Inside (src/llm_min) ‚öôÔ∏è</a></li>
<li><a href="#whats-next-future-plans-">What's Next? Future Plans üîÆ</a></li>
<li><a href="#common-questions-faq-">Common Questions (FAQ) ‚ùì</a></li>
<li><a href="#want-to-help-contributing-">Want to Help? Contributing ü§ù</a></li>
<li><a href="#license-">License üìú</a></li>
</ul>
</li>
</ul>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">What is <code>llm-min.txt</code> and Why is it Important?</h2><a id="user-content-what-is-llm-mintxt-and-why-is-it-important" aria-label="Permalink: What is llm-min.txt and Why is it Important?" href="#what-is-llm-mintxt-and-why-is-it-important"></a></p>
<p dir="auto">If you've ever used an AI coding assistant (like GitHub Copilot, Cursor, or others powered by Large Language Models - LLMs), you've likely encountered situations where they don't know about the latest updates to programming libraries. This knowledge gap exists because AI models have a "knowledge cutoff" ‚Äì a point beyond which they haven't learned new information. Since software evolves rapidly, this limitation can lead to outdated recommendations and broken code.</p>
<p dir="auto">Several innovative approaches have emerged to address this challenge:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://llmstxt.org/" rel="nofollow"><img src="https://camo.githubusercontent.com/cee972bc70b97903b0c5bfb4a5c07d2cf76521237ffae1166df120505060f09a/68747470733a2f2f6c6c6d737478742e6f72672f6c6f676f2e706e67" alt="llms.txt logo" width="60" data-canonical-src="https://llmstxt.org/logo.png"></a> <a href="https://llmstxt.org/" rel="nofollow">llms.txt</a>
A community-driven initiative where contributors create reference files (<code>llms.txt</code>) containing up-to-date library information specifically formatted for AI consumption.</p>
</li>
<li>
<p dir="auto"><a href="https://context7.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/48f83838d8c895d18f477e9d5fb7bb10f90cd920a763d5101a1e13e73aff4070/68747470733a2f2f656e637279707465642d74626e302e677374617469632e636f6d2f696d616765733f713d74626e3a414e6439476352625075774b4e647545414242443567415a4f5f4153397a30467955416d6c37326a33672673" alt="Context7 logo" width="60" data-canonical-src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRbPuwKNduEABBD5gAZO_AS9z0FyUAml72j3g&amp;s"></a> <a href="https://context7.com/" rel="nofollow">Context7</a>
A service that dynamically provides contextual information to AIs, often by intelligently summarizing documentation.</p>
</li>
</ul>
<p dir="auto">While these solutions are valuable, they face certain limitations:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>llms.txt</code> files can become extraordinarily large ‚Äì some exceeding <strong>800,000</strong> tokens (word fragments). This size can overwhelm many AI systems' context windows.</p>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/token.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/token.png" alt="Token comparison for llms.txt" width="500"></a>
<p dir="auto">Many shorter <code>llms.txt</code> variants simply contain links to official documentation, requiring the AI to fetch and process those documents separately. Even the comprehensive versions (<code>llms-full.txt</code>) often exceed what most AI assistants can process at once. Additionally, these files may not always reflect the absolute latest documentation.</p>
</li>
<li>
<p dir="auto"><code>Context7</code> operates somewhat as a "black box" ‚Äì while useful, its precise information selection methodology isn't fully transparent to users. It primarily works with GitHub code repositories or existing <code>llms.txt</code> files, rather than any arbitrary software package.</p>
</li>
</ul>
<p dir="auto"><strong><code>llm-min.txt</code> offers a fresh approach:</strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/icon.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/icon.png" alt="llm-min.txt icon" width="300"></a></p>
<p dir="auto">Inspired by <code>min.js</code> files in web development (JavaScript with unnecessary elements removed), <code>llm-min.txt</code> adopts a similar philosophy for technical documentation. Instead of feeding an AI a massive, verbose manual, we leverage another AI to distill that documentation into a super-condensed, highly structured summary. The resulting <code>llm-min.txt</code> file captures only the most essential information needed to understand a library's usage, packaged in a format optimized for AI assistants rather than human readers.</p>
<p dir="auto">Modern AI reasoning capabilities excel at this distillation process, creating remarkably efficient knowledge representations that deliver maximum value with minimal token consumption.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Understanding <code>llm-min.txt</code>: A Machine-Optimized Format üß©</h2><a id="user-content-understanding-llm-mintxt-a-machine-optimized-format-" aria-label="Permalink: Understanding llm-min.txt: A Machine-Optimized Format üß©" href="#understanding-llm-mintxt-a-machine-optimized-format-"></a></p>
<p dir="auto">The <code>llm-min.txt</code> file utilizes the <strong>Structured Knowledge Format (SKF)</strong> ‚Äì a compact, machine-optimized format designed for efficient AI parsing rather than human readability. This format organizes technical information into distinct, highly structured sections with precise relationships.</p>
<p dir="auto"><strong>Key Elements of the SKF Format:</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Header Metadata:</strong> Every file begins with essential contextual information:</p>
<ul dir="auto">
<li><code># IntegratedKnowledgeManifest_SKF</code>: Format identifier and version</li>
<li><code># SourceDocs: [...]</code>: Original documentation sources</li>
<li><code># GenerationTimestamp: ...</code>: Creation timestamp</li>
<li><code># PrimaryNamespace: ...</code>: Top-level package/namespace, critical for understanding import paths</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Three Core Structured Sections:</strong> The content is organized into distinct functional categories:</p>
<ul dir="auto">
<li>
<p dir="auto"><code># SECTION: DEFINITIONS (Prefix: D)</code>: Describes the static aspects of the library:</p>
<ul dir="auto">
<li>Canonical component definitions with unique global IDs (e.g., <code>D001:G001_MyClass</code>)</li>
<li>Namespace paths relative to <code>PrimaryNamespace</code></li>
<li>Method signatures with parameters and return types</li>
<li>Properties/fields with types and access modifiers</li>
<li>Static relationships like inheritance or interface implementation</li>
<li><strong>Important:</strong> This section effectively serves as the glossary for the file, as the traditional glossary (<code>G</code> section) is used during generation but deliberately omitted from the final output to save space.</li>
</ul>
</li>
<li>
<p dir="auto"><code># SECTION: INTERACTIONS (Prefix: I)</code>: Captures dynamic behaviors within the library:</p>
<ul dir="auto">
<li>Method invocations (<code>INVOKES</code>)</li>
<li>Component usage patterns (<code>USES_COMPONENT</code>)</li>
<li>Event production/consumption</li>
<li>Error raising and handling logic, with references to specific error types</li>
</ul>
</li>
<li>
<p dir="auto"><code># SECTION: USAGE_PATTERNS (Prefix: U)</code>: Provides concrete usage examples:</p>
<ul dir="auto">
<li>Common workflows for core functionality</li>
<li>Step-by-step sequences involving object creation, configuration, method invocation, and error handling</li>
<li>Each pattern has a descriptive name (e.g., <code>U_BasicCrawl</code>) with numbered steps (<code>U_BasicCrawl.1</code>, <code>U_BasicCrawl.2</code>)</li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Line-Based Structure:</strong> Each item appears on its own line following precise formatting conventions that enable reliable machine parsing.</p>
</li>
</ol>
<p dir="auto"><strong>Example SKF Format (Simplified):</strong></p>
<div data-snippet-clipboard-copy-content="# IntegratedKnowledgeManifest_SKF/1.4 LA
# SourceDocs: [example-lib-docs]
# GenerationTimestamp: 2024-05-28T12:00:00Z
# PrimaryNamespace: example_lib

# SECTION: DEFINITIONS (Prefix: D)
# Format_PrimaryDef: Dxxx:Gxxx_Entity [DEF_TYP] [NAMESPACE &quot;relative.path&quot;] [OPERATIONS {op1:RetT(p1N:p1T)}] [ATTRIBUTES {attr1:AttrT1}] (&quot;Note&quot;)
# ---
D001:G001_Greeter [CompDef] [NAMESPACE &quot;.&quot;] [OPERATIONS {greet:Str(name:Str)}] (&quot;A simple greeter class&quot;)
D002:G002_AppConfig [CompDef] [NAMESPACE &quot;config&quot;] [ATTRIBUTES {debug_mode:Bool(&quot;RO&quot;)}] (&quot;Application configuration&quot;)
# ---

# SECTION: INTERACTIONS (Prefix: I)
# Format: Ixxx:Source_Ref INT_VERB Target_Ref_Or_Literal (&quot;Note_Conditions_Error(Gxxx_ErrorType)&quot;)
# ---
I001:G001_Greeter.greet INVOKES G003_Logger.log (&quot;Logs greeting activity&quot;)
# ---

# SECTION: USAGE_PATTERNS (Prefix: U)
# Format: U_Name:PatternTitleKeyword
#         U_Name.N:[Actor_Or_Ref] ACTION_KEYWORD (Target_Or_Data_Involving_Ref) -> [Result_Or_State_Change_Involving_Ref]
# ---
U_BasicGreeting:Basic User Greeting
U_BasicGreeting.1:[User] CREATE (G001_Greeter) -> [greeter_instance]
U_BasicGreeting.2:[greeter_instance] INVOKE (greet name='Alice') -> [greeting_message]
# ---
# END_OF_MANIFEST"><pre lang="text"><code># IntegratedKnowledgeManifest_SKF/1.4 LA
# SourceDocs: [example-lib-docs]
# GenerationTimestamp: 2024-05-28T12:00:00Z
# PrimaryNamespace: example_lib

# SECTION: DEFINITIONS (Prefix: D)
# Format_PrimaryDef: Dxxx:Gxxx_Entity [DEF_TYP] [NAMESPACE "relative.path"] [OPERATIONS {op1:RetT(p1N:p1T)}] [ATTRIBUTES {attr1:AttrT1}] ("Note")
# ---
D001:G001_Greeter [CompDef] [NAMESPACE "."] [OPERATIONS {greet:Str(name:Str)}] ("A simple greeter class")
D002:G002_AppConfig [CompDef] [NAMESPACE "config"] [ATTRIBUTES {debug_mode:Bool("RO")}] ("Application configuration")
# ---

# SECTION: INTERACTIONS (Prefix: I)
# Format: Ixxx:Source_Ref INT_VERB Target_Ref_Or_Literal ("Note_Conditions_Error(Gxxx_ErrorType)")
# ---
I001:G001_Greeter.greet INVOKES G003_Logger.log ("Logs greeting activity")
# ---

# SECTION: USAGE_PATTERNS (Prefix: U)
# Format: U_Name:PatternTitleKeyword
#         U_Name.N:[Actor_Or_Ref] ACTION_KEYWORD (Target_Or_Data_Involving_Ref) -&gt; [Result_Or_State_Change_Involving_Ref]
# ---
U_BasicGreeting:Basic User Greeting
U_BasicGreeting.1:[User] CREATE (G001_Greeter) -&gt; [greeter_instance]
U_BasicGreeting.2:[greeter_instance] INVOKE (greet name='Alice') -&gt; [greeting_message]
# ---
# END_OF_MANIFEST
</code></pre></div>
<p dir="auto">The <code>llm-min-guideline.md</code> file (generated alongside <code>llm-min.txt</code>) provides detailed decoding instructions and schema definitions that enable an AI to correctly interpret the SKF format. It serves as the essential companion document explaining the notation, field meanings, and relationship types used throughout the file.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does it Really Work? Visualizing the Impact</h2><a id="user-content-does-it-really-work-visualizing-the-impact" aria-label="Permalink: Does it Really Work? Visualizing the Impact" href="#does-it-really-work-visualizing-the-impact"></a></p>
<p dir="auto"><code>llm-min.txt</code> achieves dramatic token reduction while preserving the essential knowledge needed by AI assistants. The chart below compares token counts between original library documentation (<code>llm-full.txt</code>) and the compressed <code>llm-min.txt</code> versions:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/comparison.png"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/comparison.png" alt="Token Compression Comparison"></a></p>
<p dir="auto">These results demonstrate token reductions typically ranging from 90-95%, with some cases exceeding 97%. This extreme compression, combined with the highly structured SKF format, enables AI tools to ingest and process library documentation far more efficiently than with raw text.</p>
<p dir="auto">In our samples directory, you can examine these impressive results firsthand:</p>
<ul dir="auto">
<li><code>sample/crawl4ai/llm-full.txt</code>: Original documentation (uncompressed)</li>
<li><code>sample/crawl4ai/llm-min.txt</code>: The compressed SKF representation</li>
<li><code>sample/crawl4ai/llm-min-guideline.md</code>: The format decoder companion file, also seen in <a href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/llm-min-guideline.md">llm-min-guideline.md</a></li>
</ul>
<p dir="auto">Most compressed files contain around 10,000 tokens ‚Äì well within the processing capacity of modern AI assistants.</p>
<p dir="auto"><strong>How to use it?</strong></p>
<p dir="auto">Simply reference the files in your AI-powered IDE's conversation, and watch your assistant immediately gain detailed knowledge of the library:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/marv1nnnnn/llm-min.txt/blob/main/assets/demo.gif"><img src="https://github.com/marv1nnnnn/llm-min.txt/raw/main/assets/demo.gif" alt="Demo" data-animated-image=""></a></p>
<p dir="auto"><strong>How does it perform?</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">It's necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using <code>llm-min</code> will largely improve the success rate of code generation.</h2><a id="user-content-its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation" aria-label="Permalink: It's necessary to make a benchmark but incredibly hard. LLM code generation is stochastic and the quality of the generated code depends on many factors. crawl4ai / google-genai / svelte are all packages current LLM failed to generate correct code for. Using llm-min will largely improve the success rate of code generation." href="#its-necessary-to-make-a-benchmark-but-incredibly-hard-llm-code-generation-is-stochastic-and-the-quality-of-the-generated-code-depends-on-many-factors-crawl4ai--google-genai--svelte-are-all-packages-current-llm-failed-to-generate-correct-code-for-using-llm-min-will-largely-improve-the-success-rate-of-code-generation"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start üöÄ</h2><a id="user-content-quick-start-" aria-label="Permalink: Quick Start üöÄ" href="#quick-start-"></a></p>
<p dir="auto">Getting started with <code>llm-min</code> is straightforward:</p>
<p dir="auto"><strong>1. Installation:</strong></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>For regular users (recommended):</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="pip install llm-min

# Install required browser automation tools
playwright install"><pre>pip install llm-min

<span><span>#</span> Install required browser automation tools</span>
playwright install</pre></div>
</li>
<li>
<p dir="auto"><strong>For contributors and developers:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Clone the repository (if not already done)
# git clone https://github.com/your-repo/llm-min.git
# cd llm-min

# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies with UV (faster than pip)
uv sync
uv pip install -e .

# Optional: Set up pre-commit hooks for code quality
# uv pip install pre-commit
# pre-commit install"><pre><span><span>#</span> Clone the repository (if not already done)</span>
<span><span>#</span> git clone https://github.com/your-repo/llm-min.git</span>
<span><span>#</span> cd llm-min</span>

<span><span>#</span> Create and activate a virtual environment</span>
python -m venv .venv
<span>source</span> .venv/bin/activate  <span><span>#</span> On Windows: .venv\Scripts\activate</span>

<span><span>#</span> Install dependencies with UV (faster than pip)</span>
uv sync
uv pip install -e <span>.</span>

<span><span>#</span> Optional: Set up pre-commit hooks for code quality</span>
<span><span>#</span> uv pip install pre-commit</span>
<span><span>#</span> pre-commit install</span></pre></div>
</li>
</ul>
<p dir="auto"><strong>2. Set Up Your Gemini API Key:</strong> üîë</p>
<p dir="auto"><code>llm-min</code> uses Google's Gemini AI to generate compressed documentation. You'll need a Gemini API key to proceed:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Best practice:</strong> Set an environment variable named <code>GEMINI_API_KEY</code> with your key value:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Linux/macOS
export GEMINI_API_KEY=your_api_key_here

# Windows (Command Prompt)
set GEMINI_API_KEY=your_api_key_here

# Windows (PowerShell)
$env:GEMINI_API_KEY=&quot;your_api_key_here&quot;"><pre><span><span>#</span> Linux/macOS</span>
<span>export</span> GEMINI_API_KEY=your_api_key_here

<span><span>#</span> Windows (Command Prompt)</span>
<span>set</span> GEMINI_API_KEY=your_api_key_here

<span><span>#</span> Windows (PowerShell)</span>
<span>$env</span>:GEMINI_API_KEY=<span><span>"</span>your_api_key_here<span>"</span></span></pre></div>
</li>
<li>
<p dir="auto"><strong>Alternative:</strong> Supply your key directly via the <code>--gemini-api-key</code> command-line option.</p>
</li>
</ul>
<p dir="auto">You can obtain a Gemini API key from the <a href="https://aistudio.google.com/app/apikey" rel="nofollow">Google AI Studio</a> or Google Cloud Console.</p>
<p dir="auto"><strong>3. Generate Your First <code>llm-min.txt</code> File:</strong> üíª</p>
<p dir="auto">Choose one of the following input sources:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Option</th>
<th>Short</th>
<th>Type</th>
<th>What it does</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--output-dir</code></td>
<td><code>-o</code></td>
<td><code>DIRECTORY</code></td>
<td>Where to save the generated files (default is a folder named <code>llm_min_docs</code>).</td>
</tr>
<tr>
<td><code>--output-name</code></td>
<td><code>-n</code></td>
<td><code>TEXT</code></td>
<td>Give a custom name for the subfolder inside <code>output-dir</code>.</td>
</tr>
<tr>
<td><code>--max-crawl-pages</code></td>
<td><code>-p</code></td>
<td><code>INTEGER</code></td>
<td>Max web pages to read (default: 200; 0 means no limit).</td>
</tr>
<tr>
<td><code>--max-crawl-depth</code></td>
<td><code>-D</code></td>
<td><code>INTEGER</code></td>
<td>How many links deep to follow on a website (default: 3).</td>
</tr>
<tr>
<td><code>--chunk-size</code></td>
<td><code>-c</code></td>
<td><code>INTEGER</code></td>
<td>How much text to give the AI at once (default: 600,000 characters).</td>
</tr>
<tr>
<td><code>--gemini-api-key</code></td>
<td><code>-k</code></td>
<td><code>TEXT</code></td>
<td>Your Gemini API Key (if not set as an environment variable).</td>
</tr>
<tr>
<td><code>--gemini-model</code></td>
<td><code>-m</code></td>
<td><code>TEXT</code></td>
<td>Which Gemini model to use (default: <code>gemini-2.5-flash-preview-04-17</code>).</td>
</tr>
<tr>
<td><code>--verbose</code></td>
<td><code>-v</code></td>
<td></td>
<td>Show more detailed messages while it's working.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Key Command-Line Options:</strong></p>
<ul dir="auto">
<li>Process the Python package <code>typer</code>, read up to 50 web pages, and save to a folder called <code>my_docs</code>:
<div dir="auto" data-snippet-clipboard-copy-content="llm-min -pkg &quot;typer&quot; -o my_docs -p 50 --gemini-api-key YOUR_API_KEY_HERE"><pre>llm-min -pkg <span><span>"</span>typer<span>"</span></span> -o my_docs -p 50 --gemini-api-key YOUR_API_KEY_HERE</pre></div>
</li>
</ul>
<p dir="auto"><strong>Example Commands:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Process the &quot;typer&quot; package, save to &quot;my_docs&quot; folder
llm-min -pkg &quot;typer&quot; -o my_docs -p 50

# Process the FastAPI documentation website
llm-min -u &quot;https://fastapi.tiangolo.com/&quot; -o my_docs -p 50

# Process documentation files in a local folder
llm-min -i &quot;./docs&quot; -o my_docs"><pre><span><span>#</span> Process the "typer" package, save to "my_docs" folder</span>
llm-min -pkg <span><span>"</span>typer<span>"</span></span> -o my_docs -p 50

<span><span>#</span> Process the FastAPI documentation website</span>
llm-min -u <span><span>"</span>https://fastapi.tiangolo.com/<span>"</span></span> -o my_docs -p 50

<span><span>#</span> Process documentation files in a local folder</span>
llm-min -i <span><span>"</span>./docs<span>"</span></span> -o my_docs</pre></div>
<p dir="auto"><strong>4. Programmatic Usage in Python:</strong> üêç</p>
<p dir="auto">You can also integrate <code>llm-min</code> directly into your Python applications:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from llm_min import LLMMinGenerator
import os

# Configuration for the AI processing
llm_config = {
    &quot;api_key&quot;: os.environ.get(&quot;GEMINI_API_KEY&quot;),  # Use environment variable
    &quot;model_name&quot;: &quot;gemini-2.5-flash-preview-04-17&quot;,  # Recommended model
    &quot;chunk_size&quot;: 600000,  # Characters per AI processing batch
    &quot;max_crawl_pages&quot;: 200,  # Maximum pages to crawl
    &quot;max_crawl_depth&quot;: 3,  # Link following depth
}

# Initialize the generator (output files will go to ./my_output_docs/[package_name]/)
generator = LLMMinGenerator(output_dir=&quot;./my_output_docs&quot;, llm_config=llm_config)

# Generate llm-min.txt for the 'requests' package
try:
    generator.generate_from_package(&quot;requests&quot;)
    print(&quot;‚úÖ Successfully created documentation for 'requests'!&quot;)
except Exception as e:
    print(f&quot;‚ùå Error processing 'requests': {e}&quot;)

# Generate llm-min.txt from a documentation URL
try:
    generator.generate_from_url(&quot;https://bun.sh/llms-full.txt&quot;)
    print(&quot;‚úÖ Successfully processed 'https://bun.sh/llms-full.txt'!&quot;)
except Exception as e:
    print(f&quot;‚ùå Error processing URL: {e}&quot;)"><pre><span>from</span> <span>llm_min</span> <span>import</span> <span>LLMMinGenerator</span>
<span>import</span> <span>os</span>

<span># Configuration for the AI processing</span>
<span>llm_config</span> <span>=</span> {
    <span>"api_key"</span>: <span>os</span>.<span>environ</span>.<span>get</span>(<span>"GEMINI_API_KEY"</span>),  <span># Use environment variable</span>
    <span>"model_name"</span>: <span>"gemini-2.5-flash-preview-04-17"</span>,  <span># Recommended model</span>
    <span>"chunk_size"</span>: <span>600000</span>,  <span># Characters per AI processing batch</span>
    <span>"max_crawl_pages"</span>: <span>200</span>,  <span># Maximum pages to crawl</span>
    <span>"max_crawl_depth"</span>: <span>3</span>,  <span># Link following depth</span>
}

<span># Initialize the generator (output files will go to ./my_output_docs/[package_name]/)</span>
<span>generator</span> <span>=</span> <span>LLMMinGenerator</span>(<span>output_dir</span><span>=</span><span>"./my_output_docs"</span>, <span>llm_config</span><span>=</span><span>llm_config</span>)

<span># Generate llm-min.txt for the 'requests' package</span>
<span>try</span>:
    <span>generator</span>.<span>generate_from_package</span>(<span>"requests"</span>)
    <span>print</span>(<span>"‚úÖ Successfully created documentation for 'requests'!"</span>)
<span>except</span> <span>Exception</span> <span>as</span> <span>e</span>:
    <span>print</span>(<span>f"‚ùå Error processing 'requests': <span><span>{</span><span>e</span><span>}</span></span>"</span>)

<span># Generate llm-min.txt from a documentation URL</span>
<span>try</span>:
    <span>generator</span>.<span>generate_from_url</span>(<span>"https://bun.sh/llms-full.txt"</span>)
    <span>print</span>(<span>"‚úÖ Successfully processed 'https://bun.sh/llms-full.txt'!"</span>)
<span>except</span> <span>Exception</span> <span>as</span> <span>e</span>:
    <span>print</span>(<span>f"‚ùå Error processing URL: <span><span>{</span><span>e</span><span>}</span></span>"</span>)</pre></div>
<p dir="auto">For a complete list of command-line options, run:</p>

<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Output Directory Structure üìÇ</h2><a id="user-content-output-directory-structure-" aria-label="Permalink: Output Directory Structure üìÇ" href="#output-directory-structure-"></a></p>
<p dir="auto">When <code>llm-min</code> completes its processing, it creates the following organized directory structure:</p>
<div data-snippet-clipboard-copy-content="your_chosen_output_dir/
‚îî‚îÄ‚îÄ name_of_package_or_website/
    ‚îú‚îÄ‚îÄ llm-full.txt             # Complete documentation text (original content)
    ‚îú‚îÄ‚îÄ llm-min.txt              # Compressed SKF/1.4 LA structured summary
    ‚îî‚îÄ‚îÄ llm-min-guideline.md     # Essential format decoder for AI interpretation"><pre lang="text"><code>your_chosen_output_dir/
‚îî‚îÄ‚îÄ name_of_package_or_website/
    ‚îú‚îÄ‚îÄ llm-full.txt             # Complete documentation text (original content)
    ‚îú‚îÄ‚îÄ llm-min.txt              # Compressed SKF/1.4 LA structured summary
    ‚îî‚îÄ‚îÄ llm-min-guideline.md     # Essential format decoder for AI interpretation
</code></pre></div>
<p dir="auto">For example, running <code>llm-min -pkg "requests" -o my_llm_docs</code> produces:</p>
<div data-snippet-clipboard-copy-content="my_llm_docs/
‚îî‚îÄ‚îÄ requests/
    ‚îú‚îÄ‚îÄ llm-full.txt             # Original documentation
    ‚îú‚îÄ‚îÄ llm-min.txt              # Compressed SKF format (D, I, U sections)
    ‚îî‚îÄ‚îÄ llm-min-guideline.md     # Format decoding instructions"><pre lang="text"><code>my_llm_docs/
‚îî‚îÄ‚îÄ requests/
    ‚îú‚îÄ‚îÄ llm-full.txt             # Original documentation
    ‚îú‚îÄ‚îÄ llm-min.txt              # Compressed SKF format (D, I, U sections)
    ‚îî‚îÄ‚îÄ llm-min-guideline.md     # Format decoding instructions
</code></pre></div>
<p dir="auto"><strong>Important:</strong> The <code>llm-min-guideline.md</code> file is a critical companion to <code>llm-min.txt</code>. It provides the detailed schema definitions and format explanations that an AI needs to correctly interpret the structured data. When using <code>llm-min.txt</code> with an AI assistant, always include this guideline file as well.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Choosing the Right AI Model (Why Gemini) üß†</h2><a id="user-content-choosing-the-right-ai-model-why-gemini-" aria-label="Permalink: Choosing the Right AI Model (Why Gemini) üß†" href="#choosing-the-right-ai-model-why-gemini-"></a></p>
<p dir="auto"><code>llm-min</code> utilizes Google's Gemini family of AI models for document processing. While you can select a specific Gemini model via the <code>--gemini-model</code> option, we strongly recommend using the default: <code>gemini-2.5-flash-preview-04-17</code>.</p>
<p dir="auto">This particular model offers an optimal combination of capabilities for documentation compression:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Advanced Reasoning:</strong> Excels at understanding complex technical documentation and extracting the essential structural relationships needed for the SKF format.</p>
</li>
<li>
<p dir="auto"><strong>Exceptional Context Window:</strong> With a 1-million token input capacity, it can process large documentation chunks at once, enabling more coherent and comprehensive analysis.</p>
</li>
<li>
<p dir="auto"><strong>Cost Efficiency:</strong> Provides an excellent balance of capability and affordability compared to other large-context models.</p>
</li>
</ol>
<p dir="auto">The default model has been carefully selected to deliver the best results for the <code>llm-min</code> compression process across a wide range of documentation styles and technical domains.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it Works: A Look Inside (src/llm_min) ‚öôÔ∏è</h2><a id="user-content-how-it-works-a-look-inside-srcllm_min-Ô∏è" aria-label="Permalink: How it Works: A Look Inside (src/llm_min) ‚öôÔ∏è" href="#how-it-works-a-look-inside-srcllm_min-Ô∏è"></a></p>
<p dir="auto">The <code>llm-min</code> tool employs a sophisticated multi-stage process to transform verbose documentation into a compact, machine-optimized SKF manifest:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Input Processing:</strong> Based on your command-line options (e.g., <code>--package "requests"</code>), <code>llm-min</code> gathers documentation from the appropriate source (PyPI, web crawling, or local files).</p>
</li>
<li>
<p dir="auto"><strong>Text Preparation:</strong> The collected documentation is cleaned and segmented into manageable chunks for processing. The original text is preserved as <code>llm-full.txt</code>.</p>
</li>
<li>
<p dir="auto"><strong>Three-Step AI Analysis Pipeline (Gemini):</strong> This is the heart of the SKF manifest generation, orchestrated by the <code>compact_content_to_structured_text</code> function in <code>compacter.py</code>:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Step 1: Global Glossary Generation (Internal Only):</strong></p>
<ul dir="auto">
<li>Each document chunk is analyzed using the <code>SKF_PROMPT_CALL1_GLOSSARY_TEMPLATE</code> prompt to identify key technical entities and generate a <em>chunk-local</em> glossary fragment with temporary <code>Gxxx</code> IDs.</li>
<li>These fragments are consolidated via the <code>SKF_PROMPT_CALL1_5_MERGE_GLOSSARY_TEMPLATE</code> prompt, which resolves duplicates and creates a unified entity list.</li>
<li>The <code>re_id_glossary_items</code> function then assigns globally sequential <code>Gxxx</code> IDs (G001, G002, etc.) to these consolidated entities.</li>
<li>This global glossary is maintained in memory throughout the process but is <strong>not included in the final <code>llm-min.txt</code> output</strong> to conserve space.</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Step 2: Definitions &amp; Interactions (D &amp; I) Generation:</strong></p>
<ul dir="auto">
<li>For the first document chunk (or if there's only one chunk), the AI uses the <code>SKF_PROMPT_CALL2_DETAILS_SINGLE_CHUNK_TEMPLATE</code> with the global glossary to generate initial D and I items.</li>
<li>For subsequent chunks, the <code>SKF_PROMPT_CALL2_DETAILS_ITERATIVE_TEMPLATE</code> is used, providing both the global glossary and previously generated D&amp;I items as context to avoid duplication.</li>
<li>As each chunk is processed, newly identified D and I items are accumulated and assigned sequential global IDs (D001, D002, etc. and I001, I002, etc.).</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Step 3: Usage Patterns (U) Generation:</strong></p>
<ul dir="auto">
<li>Similar to Step 2, the first chunk uses <code>SKF_PROMPT_CALL3_USAGE_SINGLE_CHUNK_TEMPLATE</code>, receiving the global glossary, all accumulated D&amp;I items, and the current chunk text.</li>
<li>Subsequent chunks use <code>SKF_PROMPT_CALL3_USAGE_ITERATIVE_TEMPLATE</code>, which additionally receives previously generated U-items to enable pattern continuation and avoid duplication.</li>
<li>Usage patterns are identified with descriptive names (e.g., <code>U_BasicNetworkFetch</code>) and contain numbered steps (e.g., <code>U_BasicNetworkFetch.1</code>, <code>U_BasicNetworkFetch.2</code>).</li>
</ul>
</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Final Assembly:</strong> The complete <code>llm-min.txt</code> file is created by combining:</p>
<ul dir="auto">
<li>The SKF manifest header (protocol version, source docs, timestamp, primary namespace)</li>
<li>The accumulated <code>DEFINITIONS</code> section</li>
<li>The accumulated <code>INTERACTIONS</code> section</li>
<li>The accumulated <code>USAGE_PATTERNS</code> section</li>
<li>A final <code># END_OF_MANIFEST</code> marker</li>
</ul>
</li>
</ol>
<p dir="auto"><strong>Conceptual Pipeline Overview:</strong></p>
<div data-snippet-clipboard-copy-content="User Input      ‚Üí  Doc Gathering   ‚Üí  Text Processing   ‚Üí  AI Step 1: Glossary   ‚Üí  In-Memory Global    ‚Üí  AI Step 2: D&amp;I     ‚Üí  Accumulated D&amp;I
(CLI/Python)       (Package/URL)      (Chunking)           (Extract + Merge)        Glossary (Gxxx)        (Per chunk)          (Dxxx, Ixxx)
                                                                                                                                     ‚Üì
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚Üì
           ‚Üì                                                                                                 ‚Üë                      ‚Üì
Final SKF Manifest   ‚Üê   Assembly   ‚Üê   Accumulated Usage   ‚Üê   AI Step 3: Usage   ‚Üê   Global Glossary + Accumulated D&amp;I
(llm-min.txt)            (D,I,U)        Patterns (U_Name.N)      (Per chunk)           (Required context for generating valid U-items)"><pre><code>User Input      ‚Üí  Doc Gathering   ‚Üí  Text Processing   ‚Üí  AI Step 1: Glossary   ‚Üí  In-Memory Global    ‚Üí  AI Step 2: D&amp;I     ‚Üí  Accumulated D&amp;I
(CLI/Python)       (Package/URL)      (Chunking)           (Extract + Merge)        Glossary (Gxxx)        (Per chunk)          (Dxxx, Ixxx)
                                                                                                                                     ‚Üì
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚Üì
           ‚Üì                                                                                                 ‚Üë                      ‚Üì
Final SKF Manifest   ‚Üê   Assembly   ‚Üê   Accumulated Usage   ‚Üê   AI Step 3: Usage   ‚Üê   Global Glossary + Accumulated D&amp;I
(llm-min.txt)            (D,I,U)        Patterns (U_Name.N)      (Per chunk)           (Required context for generating valid U-items)
</code></pre></div>
<p dir="auto">This multi-stage approach ensures that the SKF manifest is comprehensive, avoids duplication across chunks, and maintains consistent cross-references between entities, definitions, interactions, and usage patterns.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's Next? Future Plans üîÆ</h2><a id="user-content-whats-next-future-plans-" aria-label="Permalink: What's Next? Future Plans üîÆ" href="#whats-next-future-plans-"></a></p>
<p dir="auto">We're exploring several exciting directions to evolve <code>llm-min</code>:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Public Repository for Pre-Generated Files</strong> üåê
A central hub where the community could share and discover <code>llm-min.txt</code> files for popular libraries would be valuable. This would eliminate the need for individual users to generate these files repeatedly and ensure consistent, high-quality information. Key challenges include quality control, version management, and hosting infrastructure costs.</p>
</li>
<li>
<p dir="auto"><strong>Code-Based Documentation Inference</strong> üíª
An intriguing possibility is using source code analysis (via Abstract Syntax Trees) to automatically generate or augment documentation summaries. While initial experiments have shown this to be technically challenging, particularly for complex libraries with dynamic behaviors, it remains a promising research direction that could enable even more accurate documentation.</p>
</li>
<li>
<p dir="auto"><strong>Model Control Protocol Integration</strong> ü§î
While technically feasible, implementing <code>llm-min</code> as an MCP server doesn't fully align with our current design philosophy. The strength of <code>llm-min.txt</code> lies in providing reliable, static context ‚Äì a deterministic reference that reduces the uncertainty sometimes associated with dynamic AI integrations. We're monitoring user needs to determine if a server-based approach might deliver value in the future.</p>
</li>
</ul>
<p dir="auto">We welcome community input on these potential directions!</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Common Questions (FAQ) ‚ùì</h2><a id="user-content-common-questions-faq-" aria-label="Permalink: Common Questions (FAQ) ‚ùì" href="#common-questions-faq-"></a></p>
<p dir="auto"><strong>Q: Do I need a reasoning-capable model to generate an <code>llm-min.txt</code> file?</strong> üß†</p>
<p dir="auto">A: Yes, generating an <code>llm-min.txt</code> file requires a model with strong reasoning capabilities like Gemini. The process involves complex information extraction, entity relationship mapping, and structured knowledge representation. However, once generated, the <code>llm-min.txt</code> file can be effectively used by any competent coding model (e.g., Claude 3.5 Sonnet) to answer library-specific questions.</p>
<p dir="auto"><strong>Q: Does <code>llm-min.txt</code> preserve all information from the original documentation?</strong> üìö</p>
<p dir="auto">A: No, <code>llm-min.txt</code> is explicitly designed as a lossy compression format. It prioritizes programmatically relevant details (classes, methods, parameters, return types, core usage patterns) while deliberately omitting explanatory prose, conceptual discussions, and peripheral information. This selective preservation is what enables the dramatic token reduction while maintaining the essential technical reference information an AI assistant needs.</p>
<p dir="auto"><strong>Q: Why does generating an <code>llm-min.txt</code> file take time?</strong> ‚è±Ô∏è</p>
<p dir="auto">A: Creating an <code>llm-min.txt</code> file involves a sophisticated multi-stage AI pipeline:</p>
<ol dir="auto">
<li>Gathering and preprocessing documentation</li>
<li>Analyzing each chunk to identify entities (glossary generation)</li>
<li>Consolidating entities across chunks</li>
<li>Extracting detailed definitions and interactions from each chunk</li>
<li>Generating representative usage patterns</li>
</ol>
<p dir="auto">This intensive process can take several minutes, particularly for large libraries. However, once created, the resulting <code>llm-min.txt</code> file can be reused indefinitely, providing much faster reference information for AI assistants.</p>
<p dir="auto"><strong>Q: I received a "Gemini generation stopped due to MAX_TOKENS limit" error. What should I do?</strong> üõë</p>
<p dir="auto">A: This error indicates that the Gemini model reached its output limit while processing a particularly dense or complex documentation chunk. Try reducing the <code>--chunk-size</code> option (e.g., from 600,000 to 300,000 characters) to give the model smaller batches to process. While this might slightly increase API costs due to more separate calls, it often resolves token limit errors.</p>
<p dir="auto"><strong>Q: What's the typical cost for generating one <code>llm-min.txt</code> file?</strong> üí∞</p>
<p dir="auto">A: Processing costs vary based on documentation size and complexity, but for a moderate-sized library, expect to spend between <strong>$0.01 and $1.00 USD</strong> in Gemini API charges. Key factors affecting cost include:</p>
<ul dir="auto">
<li>Total documentation size</li>
<li>Number of chunks processed</li>
<li>Complexity of the library's structure</li>
<li>Selected Gemini model</li>
</ul>
<p dir="auto">For current pricing details, refer to the <a href="https://cloud.google.com/vertex-ai/pricing#gemini" rel="nofollow">Google Cloud AI pricing page</a>.</p>
<p dir="auto"><strong>Q: Did you vibe code this project</strong> ü§ñ</p>
<p dir="auto">A: Yes, definitely. This project was developed using <a href="https://roocode.com/" rel="nofollow">Roocode</a> with a custom configuration called <a href="https://github.com/marv1nnnnn/rooroo">Rooroo</a>.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Want to Help? Contributing ü§ù</h2><a id="user-content-want-to-help-contributing-" aria-label="Permalink: Want to Help? Contributing ü§ù" href="#want-to-help-contributing-"></a></p>
<p dir="auto">We welcome contributions to make <code>llm-min</code> even better! üéâ</p>
<p dir="auto">Whether you're reporting bugs, suggesting features, or submitting code changes via pull requests, your involvement helps improve this tool for everyone. Check our GitHub repository for contribution guidelines and open issues.</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">License üìú</h2><a id="user-content-license-" aria-label="Permalink: License üìú" href="#license-"></a></p>
<p dir="auto">This project is licensed under the MIT License. See the <code>LICENSE</code> file for complete details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Real-Time Gaussian Splatting (135 pts)]]></title>
            <link>https://github.com/axbycc/LiveSplat</link>
            <guid>43994827</guid>
            <pubDate>Thu, 15 May 2025 13:26:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/axbycc/LiveSplat">https://github.com/axbycc/LiveSplat</a>, See on <a href="https://news.ycombinator.com/item?id=43994827">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/155058764/443802899-9a97fcf9-33cd-4bea-b124-0233c5435f90.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDczNDEzMDEsIm5iZiI6MTc0NzM0MTAwMSwicGF0aCI6Ii8xNTUwNTg3NjQvNDQzODAyODk5LTlhOTdmY2Y5LTMzY2QtNGJlYS1iMTI0LTAyMzNjNTQzNWY5MC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNTE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUxNVQyMDMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lOGMwNThhN2NiNWI5YzdhOWE5ODJkN2U1OWYyNTdmZjcwMmQ2M2Y1ZGMyZTJlNDc4ODI2YWNmNzFlZmUxM2JlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.R__n_IkVSRLUJPuIiie4ldVmeLYLBvwlN9Pr3k_6hao"><img src="https://private-user-images.githubusercontent.com/155058764/443802899-9a97fcf9-33cd-4bea-b124-0233c5435f90.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDczNDEzMDEsIm5iZiI6MTc0NzM0MTAwMSwicGF0aCI6Ii8xNTUwNTg3NjQvNDQzODAyODk5LTlhOTdmY2Y5LTMzY2QtNGJlYS1iMTI0LTAyMzNjNTQzNWY5MC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNTE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUxNVQyMDMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1lOGMwNThhN2NiNWI5YzdhOWE5ODJkN2U1OWYyNTdmZjcwMmQ2M2Y1ZGMyZTJlNDc4ODI2YWNmNzFlZmUxM2JlJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.R__n_IkVSRLUJPuIiie4ldVmeLYLBvwlN9Pr3k_6hao" width="150"></a>  
<p dir="auto"><strong>LiveSplat</strong> is an algorithm for realtime Gaussian splatting using RGBD camera streams. Join our <a href="https://discord.gg/rCF5SXnc" rel="nofollow">discord</a> for discussion and help. Check out the demo video below to see the the type of output that LiveSplat produces.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description LiveSplat_Alexa.mp4">LiveSplat_Alexa.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/155058764/443907841-0e41f600-36e3-4482-866e-70b5c962c6f4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDczNDEzMDEsIm5iZiI6MTc0NzM0MTAwMSwicGF0aCI6Ii8xNTUwNTg3NjQvNDQzOTA3ODQxLTBlNDFmNjAwLTM2ZTMtNDQ4Mi04NjZlLTcwYjVjOTYyYzZmNC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNTE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUxNVQyMDMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01YTIxY2ZiNDg4OTE4MGYwNmU3YWY5YTgxM2Y5N2ZhMDNkMTUyZTA1MDlhMWMyZGVmZDU1NjFlNGMxMmQ4YTZjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.m_goHDqrvNcnUPYqAcWo5hyzqlZ0nuV3SFKIlEEl2Iw" data-canonical-src="https://private-user-images.githubusercontent.com/155058764/443907841-0e41f600-36e3-4482-866e-70b5c962c6f4.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDczNDEzMDEsIm5iZiI6MTc0NzM0MTAwMSwicGF0aCI6Ii8xNTUwNTg3NjQvNDQzOTA3ODQxLTBlNDFmNjAwLTM2ZTMtNDQ4Mi04NjZlLTcwYjVjOTYyYzZmNC5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwNTE1JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDUxNVQyMDMwMDFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT01YTIxY2ZiNDg4OTE4MGYwNmU3YWY5YTgxM2Y5N2ZhMDNkMTUyZTA1MDlhMWMyZGVmZDU1NjFlNGMxMmQ4YTZjJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.m_goHDqrvNcnUPYqAcWo5hyzqlZ0nuV3SFKIlEEl2Iw" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Message from the Author</h2><a id="user-content-message-from-the-author" aria-label="Permalink: Message from the Author" href="#message-from-the-author"></a></p>
<p dir="auto">LiveSplat was developed as a small part of a larger proprietary VR telerobotics system. I posted a video of the Gaussian splatting component of this system on Reddit and many people expressed an interest in experimenting with it themselves. So I spun it out and I'm making it publicly available as LiveSplat (see installation instructions below).</p>
<p dir="auto">LiveSplat should be considered alpha quality. I do not have the resources to test the installation on many different machines, so let me know if the application does not run on yours (assuming your machine meets the requirements).</p>
<p dir="auto">I've decided to keep LiveSplat closed source in order to explore potential business opportunities. If you are a business wanting to license/integrate this technology, please email <a href="mailto:mark@axby.cc">mark@axby.cc</a>.</p>
<p dir="auto">I hope you have fun with LiveSplat!</p>
<p dir="auto">‚Äî Mark Liu</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Python 3.12+</li>
<li>Windows or Ubuntu (other Linux distributions may work, but are untested)</li>
<li>x86_64 CPU</li>
<li>Nvidia graphics card</li>
<li>One or more (up to four) RGBD sensors</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">Note that the application is not open source, and is covered by this <a href="https://github.com/axbycc/LiveSplat/blob/main/LICENSE">license</a>.</p>
<p dir="auto"><strong>Ubuntu:</strong><br>
<code>pip3 install https://livesplat.s3.us-east-2.amazonaws.com/livesplat-0.1.0-cp312-cp312-manylinux_x86_64.whl</code></p>
<p dir="auto"><strong>Windows:</strong><br>
<code>pip install https://livesplat.s3.us-east-2.amazonaws.com/livesplat-0.1.0-cp312-cp312-win_amd64.whl</code></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Running</h2><a id="user-content-running" aria-label="Permalink: Running" href="#running"></a></p>
<p dir="auto">To run LiveSplat, you will have to create an integration script that feeds your RGBD streams to the LiveSplat viewer. This repo provides an integration script for Intel Realsense devices called <a href="https://github.com/axbycc/LiveSplat/blob/main/livesplat_realsense.py">livesplat_realsense.py</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Malicious compliance by booking an available meeting room (334 pts)]]></title>
            <link>https://www.clientserver.dev/p/malicious-compliance-by-booking-an</link>
            <guid>43994765</guid>
            <pubDate>Thu, 15 May 2025 13:20:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.clientserver.dev/p/malicious-compliance-by-booking-an">https://www.clientserver.dev/p/malicious-compliance-by-booking-an</a>, See on <a href="https://news.ycombinator.com/item?id=43994765">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Back in 2011, Larry Page became the CEO of Google in place of Eric Schmidt. This happened at a time when Google was feeling the growing pains of becoming a huge company. It had 30,000 employees and was growing rapidly. But you could really feel the weight; projects were getting more ambitious, taking longer, and often failing more spectacularly.</p><p>At the time, I remember an anecdote told by Larry Page. He said that companies like Yahoo! used to be a punchline at Google because it would take them weeks to get something onto their homepage. Google could accomplish the same thing in a few hours, or a few days at worst. But now he was the CEO of a company where it took weeks to get something onto the homepage, and he was sure that he was the butt of some startup‚Äôs jokes.</p><p><span>Anyways, all of this clearly bothered Larry Page. He wanted to fix it. One of his first actions was to shutter tons of projects that didn‚Äôt make tactical or strategic sense, and focus on fewer efforts. This came with the catch phrase ‚Äúmore wood behind fewer arrows.‚Äù For example, they shuttered </span><a href="https://en.wikipedia.org/wiki/Google_Buzz" rel="">Google Buzz</a><span> so that it wouldn‚Äôt distract from </span><a href="https://en.wikipedia.org/wiki/Google%2B" rel="">Google+</a><span>.</span></p><p><span>And second, Larry Page emailed the whole company </span><a href="https://www.businessinsider.com/this-is-how-larry-page-changed-meetings-at-google-after-taking-over-last-spring-2012-1" rel="">a ham-fisted attempt to revamp how meetings were done</a><span>.</span></p><ul><li><p>Every meeting needed a ‚Äúdecision-maker.‚Äù</p></li><li><p>Meetings should be capped at 10 people.</p></li><li><p>Everybody in a meeting should give input or they shouldn‚Äôt be in the meeting.</p></li><li><p>Hour-long meetings should be only 50 minutes to give the participants an opportunity to use the restroom between meetings.</p></li></ul><p>They later softened some of the language by saying that these were properties of ‚Äúdecision-oriented meetings,‚Äù implying there were other types of meetings that someone might need to attend. But you could never shake the feeling that Larry Page had to make decisions all day long and forgot that sometimes people meet for other reasons.</p><p><span>Anyways, let‚Äôs focus on the fact that Larry Page wanted hour-long meetings to only be 50 minutes. This is a good thing! It gives people a chance to stretch, go to the bathroom, grab a snack, etc. During a Q/A on the changes</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-163523555" href="https://www.clientserver.dev/p/malicious-compliance-by-booking-an#footnote-1-163523555" target="_self" rel="">1</a></span><span>, someone asked him whether Google Calendar should default to 25 and 50 minutes for meeting lengths instead of 30 and 60 minutes. Larry Page said ‚Äúyes.‚Äù And then someone on the Google Calendar team implemented this.</span></p><p>And then nothing changed. When 2:50 rolled around and your meeting was supposed to end, do you think people actually ended the meeting? Noooooo. Absolutely not! Meetings continue until the participants of the next meeting are clawing on your door like a pack of zombies.</p><p>At one point, one team in the NYC office noticed that their standups were about 10 minutes long. They didn‚Äôt want to compete with meetings that respected the half-hour boundaries. And why would they need to? Every meeting room had free slots at the last 10 minutes of every hour because people were now booking 50-minute meetings. So they did what any rational engineering team would do: they started booking their standup in the tiny 10-minute time slices that were free on the calendar of every meeting room.</p><p>I found this out when I saw them knock on the door to a meeting room by my desk. 2:50 rolls around and someone knocks on the door and says ‚ÄúI have the meeting room.‚Äù</p><p>The person in the room responds, ‚ÄúNo you don‚Äôt, it‚Äôs 2:50.‚Äù</p><p>‚ÄúLook again at the room‚Äôs calendar. You booked a 50-minute meeting, we have the room for the last 10 minutes of the hour for our standup.‚Äù</p><p>I could hear the muffled exasperation. ‚ÄúYou‚Äôve got to be joking me.‚Äù</p><p>‚ÄúWe have the room, sorry.‚Äù</p><p>Then everyone shuffled out of the room, looking vaguely pissed off. And who could blame them! Can you imagine if someone actually held you to this policy? You‚Äôre there stammering ‚Äúit‚Äôs the default, I meant for the room to be ours for an hour‚Äù and they counter with the fact that their names are listed as the active participant? I mean, I‚Äôd personally tell them that I wasn‚Äôt going to leave the room, but surely it worked a lot?</p><p>I wish I knew the identities of these brave meeting crashers. I saw them pull this stunt twice and then ride off into the sunset, and I never got to learn what team they were on. I wish I knew their motivations. Were they true believers in the 50-minute policy? Were they bored pedants? Were they wraiths, cursed to hunt the office for available meeting rooms? I‚Äôll never know for sure.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My Engineering Craft Regressed (119 pts)]]></title>
            <link>https://lemmy.ml/post/30100312</link>
            <guid>43994635</guid>
            <pubDate>Thu, 15 May 2025 13:07:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lemmy.ml/post/30100312">https://lemmy.ml/post/30100312</a>, See on <a href="https://news.ycombinator.com/item?id=43994635">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="postContent"><div><p dir="auto">5 years ago when I graduated University, I had a whole host of open source projects under my belt. I put my heart and soul into them - for thousands of hours. And users loved them. I still remember some of the faceless users whose messages gave me a smile.</p>
<p dir="auto"><span><img src="https://lemmy.ml/pictrs/image/518a9cb2-59d2-4412-a846-567377b8fd9e.png" alt=""></span></p>
<p dir="auto">When I went into the job market - which was much better back then - I had some incorrect assumption that recruiters would care about this work. Or at least technically inclined companies. Or at the very least, companies <em>I</em> would want to work for would care about this.</p>
<p dir="auto">But that never happened. My Indeed profile shows I applied to over 600 jobs back then. With 3 offers, I accepted the only one that didn‚Äôt treat me like a baby, and had a great time working for that company.</p>
<p dir="auto">During the day, I worked 8-9 hours for this startup, and until late at night I continued my open source contributions. Surely, they would take me somewhere.</p>
<p dir="auto"><span><img src="https://lemmy.ml/api/v3/image_proxy?url=https%3A%2F%2Fi.imgur.com%2F6vUNA8Y.png" alt=""></span></p>
<p dir="auto">I hopped to another startup and oversaw major projects - for pitiful pay - but enjoyed it. My skillset had never been so strong, and my impact was measurably through the roof. Surely if I kept this up, I would land a high paying gig anytime soon.</p>
<p dir="auto">But of course, that didn‚Äôt happen. Waking up at 6am to make some commits, reading documentation on the subway, and coding to dubstep at night wasn‚Äôt getting me anywhere. But I was happy.</p>
<p dir="auto">Eventually I came to face the music - Nobody gives a crap about real projects. The people who knew my value weren‚Äôt the people who could pay me. I peeled back, and started grinding Leetcode instead.</p>
<p dir="auto"><span><img src="https://lemmy.ml/api/v3/image_proxy?url=https%3A%2F%2Fi.imgur.com%2FcXM1lJd.png" alt=""></span></p>
<p dir="auto">My projects slowed to a crawl. The communities slowly got demotivated. It was sad to turn my back to, but it got me a 5x salary bump.</p>
<p dir="auto">When I joined, I was treated like a baby for having ‚Äú4 years of industry experience‚Äù. Whatever. I did work here and there, and apparently my impact exceeded expectations.</p>
<p dir="auto">But what about my skillset? Despite significantly regressing, now my email is filled with recruiters begging me to come to be ‚ÄúAmazon SDE II‚Äù, ‚ÄúTech lead at YC startup X‚Äù, ‚ÄúPart time job paying 150-290$/hr‚Äù. Pathetic. I was so much better (and happier) before, yet I‚Äôm only seen when I do the fake crap like update my LinkedIn to celebrate 1 year at $FAANG.</p>
<p dir="auto">I‚Äôll collect some money and retire in a couple years. Hopefully the open source world stays the same until then.</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pathfinding (128 pts)]]></title>
            <link>https://juhrjuhr.itch.io/deep-space-exploitation/devlog/945428/9-pathfinding</link>
            <guid>43994333</guid>
            <pubDate>Thu, 15 May 2025 12:32:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://juhrjuhr.itch.io/deep-space-exploitation/devlog/945428/9-pathfinding">https://juhrjuhr.itch.io/deep-space-exploitation/devlog/945428/9-pathfinding</a>, See on <a href="https://news.ycombinator.com/item?id=43994333">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
    </channel>
</rss>