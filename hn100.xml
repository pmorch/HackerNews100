<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 08 Feb 2025 15:30:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Generating Voronoi Diagrams Using Fortune's Algorithm (With Odin) (101 pts)]]></title>
            <link>https://redpenguin101.github.io/html/posts/2025_01_21_voronoi.html</link>
            <guid>42982015</guid>
            <pubDate>Sat, 08 Feb 2025 10:41:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://redpenguin101.github.io/html/posts/2025_01_21_voronoi.html">https://redpenguin101.github.io/html/posts/2025_01_21_voronoi.html</a>, See on <a href="https://news.ycombinator.com/item?id=42982015">Hacker News</a></p>
Couldn't get https://redpenguin101.github.io/html/posts/2025_01_21_voronoi.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Ghostwriter – use the reMarkable2 as an interface to vision-LLMs (135 pts)]]></title>
            <link>https://github.com/awwaiid/ghostwriter</link>
            <guid>42979986</guid>
            <pubDate>Sat, 08 Feb 2025 03:02:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/awwaiid/ghostwriter">https://github.com/awwaiid/ghostwriter</a>, See on <a href="https://news.ycombinator.com/item?id=42979986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto"><strong>MAIN IDEA</strong></h2><a id="user-content-main-idea" aria-label="Permalink: MAIN IDEA" href="#main-idea"></a></p>
<blockquote>
<p dir="auto">An experiment for the remarkable2 that watches what you write and, when prompted either with a gesture or some on-screen content, can write back to the screen. This is an exploration of various interacts through this handwriting+screen medium.</p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/awwaiid/ghostwriter/blob/main/docs/simple-chihuahua.jpg"><img src="https://github.com/awwaiid/ghostwriter/raw/main/docs/simple-chihuahua.jpg" width="300"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/awwaiid/ghostwriter/blob/main/docs/chihuahua-logo.png"><img src="https://github.com/awwaiid/ghostwriter/raw/main/docs/chihuahua-logo.png" width="300"></a></p>
<p dir="auto"><b><i>I wrote the handwritten prompt, GPT-4o drew the Chihuahua!!!</i></b></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/awwaiid/ghostwriter/blob/main/docs/example-kansas.gif"><img src="https://github.com/awwaiid/ghostwriter/raw/main/docs/example-kansas.gif" data-animated-image=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup/Installation</h2><a id="user-content-setupinstallation" aria-label="Permalink: Setup/Installation" href="#setupinstallation"></a></p>
<p dir="auto">You need an <code>OPENAI_API_KEY</code> (or similar for other models) environment variable set. I did this by adding it to my ~/.bashrc file on the remarkable:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# In the remarkable's ~/.bashrc or before you run ghostwriter, set one or more of your keys
export OPENAI_API_KEY=your-key-here
export ANTHROPIC_API_KEY=your-key-here
export GOOGLE_API_KEY=your-key-here"><pre><span><span>#</span> In the remarkable's ~/.bashrc or before you run ghostwriter, set one or more of your keys</span>
<span>export</span> OPENAI_API_KEY=your-key-here
<span>export</span> ANTHROPIC_API_KEY=your-key-here
<span>export</span> GOOGLE_API_KEY=your-key-here</pre></div>
<p dir="auto">Install by getting the binary to your remarkable. On your not-remarkable:</p>
<div dir="auto" data-snippet-clipboard-copy-content="wget https://github.com/awwaiid/ghostwriter/releases/latest/download/ghostwriter

# Replace this ip address with your remarkable ip address
scp ghostwriter root@192.168.1.117:"><pre>wget https://github.com/awwaiid/ghostwriter/releases/latest/download/ghostwriter

<span><span>#</span> Replace this ip address with your remarkable ip address</span>
scp ghostwriter root@192.168.1.117:</pre></div>
<p dir="auto">Then you have to ssh over and run it. Here is how to install and run (run these on the remarkable):</p>
<div dir="auto" data-snippet-clipboard-copy-content="# One itme -- make it executable after the initial copy
chmod +x ./ghostwriter

./ghostwriter --help # Get the options and see that it runs at all"><pre><span><span>#</span> One itme -- make it executable after the initial copy</span>
chmod +x ./ghostwriter

./ghostwriter --help <span><span>#</span> Get the options and see that it runs at all</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">First you need to start <code>ghostwriter</code> on the reMarkable. SSH into your remarkable and run:</p>
<div data-snippet-clipboard-copy-content="# Use the defaults, including claude-3-5-sonnet
./ghostwriter

# Use ChatGPT with the gpt-4o-mini model
./ghostwriter --model gpt-4o-mini"><pre><code># Use the defaults, including claude-3-5-sonnet
./ghostwriter

# Use ChatGPT with the gpt-4o-mini model
./ghostwriter --model gpt-4o-mini
</code></pre></div>
<p dir="auto">Draw some stuff on your screen, and then trigger the assistant by <em>touching/tapping the upper-right corner with your finger</em>. In the ssh session you'll see other touch-detections and there is a log of what happens while it is processing. You should see some dots drawn during processing and then a typewritten or drawn response!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Status / Journal</h2><a id="user-content-status--journal" aria-label="Permalink: Status / Journal" href="#status--journal"></a></p>
<ul dir="auto">
<li><strong>2024-10-06</strong> - Bootstrapping
<ul dir="auto">
<li>Basic proof of concept works!!!</li>
<li>Drawing back on the screen doesn't work super well; it takes the SVG output from ChatGPT and rasterizes it and then tries to draw lots of individual dots on the screen. The Remarkable flips out a bit ... and when the whole screen is a giant black square it really freaks out and doesn't complete</li>
<li>Things that worked at least once:
<ul dir="auto">
<li>Writing "Fill in the answer to this math problem... 3 + 7 ="</li>
<li>"Draw a picture of a chihuahua. Use simple line-art"</li>
</ul>
</li>
</ul>
</li>
<li><strong>2024-10-07</strong> - Loops are the stuff of souls
<ul dir="auto">
<li>I got a rudimentary gesture and status display!</li>
<li>So now you can touch in the upper-right and you get an "X" drawn. Then as the input is processed you get further crosses through the X. You have to erase it yourself though :)</li>
</ul>
</li>
<li><strong>2024-10-10</strong> - Initial virtual keyboard setup
<ul dir="auto">
<li>I've started to learn about using the Remarkable with a keyboard, something that I hadn't done before. It's surprisingly limited ... there is basicaly one large textarea for each page with some very basic formatting</li>
<li>To write in that I have to make a pretend keyboard, which we can do via rM-input-devices, and I've done basic validation that it works!</li>
<li>So now I want to introduce a mode where it always writes back to the text layer and recognizes that text comes from Machine and hadwriting from Human. Not sure that I'll like this mode</li>
</ul>
</li>
<li><strong>2024-10-20</strong> - Text output and other modes
<ul dir="auto">
<li>Slowly starting to rework the code to be less scratch-work, organized a bit</li>
<li>Now introduced <code>./ghostwriter text-assist</code> mode, uses a virtual keyboard to respond!</li>
</ul>
</li>
<li><strong>2024-10-21</strong> - Binary release build
<ul dir="auto">
<li>Got a github action all set to do binary builds</li>
</ul>
</li>
<li><strong>2024-10-23</strong> - Code shuffle
<ul dir="auto">
<li>Doing a bit of refactoring, grouping utilities into separate files</li>
<li>Yesterday a new Anthropic model came out (3.5-sonnet-new) which might be better at spacial awareness on the screen, so next up is to try that out in drawing-mode</li>
<li>In any case, next I want to set it up with <code>tools</code> so that it can contextually give back an SVG or text or start to trigger external scripts, like for TODO list management</li>
</ul>
</li>
<li><strong>2024-11-02</strong> - Tool Time
<ul dir="auto">
<li>Switch to providing some tools -- draw_text and draw_svg</li>
<li>This should make it more compatible with Anthropic?</li>
<li>More immediately, this means now there is the one overall assistant and it decides to draw back keyboard text or SVG drawing</li>
</ul>
</li>
<li><strong>2024-11-07</strong> - Claude! (Anthropic)
<ul dir="auto">
<li>More shuffling to start to isolate the API</li>
<li>... and now I added Claude/Anthropic!</li>
<li>It is able to use an almost identical tool-use setup, so I should be able to merge the two</li>
<li>So far it seems to like drawing a bit more, but it is not great at drawing and not much better at spacial awareness</li>
<li>Maybe next on the queue will be augmenting spacial awareness through some image pre-processing and result positioning. Like detect bounding boxes, segments, etc, feed that into the model, and have the model return an array of svgs and where they should be positioned. Maybe.</li>
</ul>
</li>
<li><strong>2024-11-22</strong> - Manual Evaluations
<ul dir="auto">
<li>Starting to sketch out how an evaluation might work</li>
<li>First I've added a bunch of parameters for recording input/output</li>
<li>Then I use that to record a sample input and output on the device</li>
<li>Then I added support to run ghostwriter on my laptop using the pre-captured input (build with <code>./build.sh local</code>)</li>
<li>Next I will build some tooling around iterating on examples given different prompts or pre-processing</li>
<li>And then if I can get enough examples maye I'll have to make an AI judge to scale :)</li>
<li>To help with that ... on idea is to make overlay the original input with the output but make the output a different color to make it differentiable by the judge</li>
<li>So far this technique is looking good for SVG output, but it'd be nice to somehow render keyboard output locally too. That is tricker since the keyboard input rendering is done by the reMarkable app</li>
</ul>
</li>
<li><strong>2024-12-02</strong> - Initial segmenter

</li>
<li><strong>2024-12-15</strong> - Engine Unification
<ul dir="auto">
<li>With the usual help from claude/copilot and some tutorials I extracted out some polymorphic engine layer for OpenAI and Anthropic backends</li>
<li>So now you can pass in engine and model</li>
<li>A lot of other codebases take a model and then do a map; maybe I'll do that based on the model name or something</li>
<li>I also got the prompt and tool definitions externalized (into a <code>prompts/</code> directory) and unified, so each engine does whatever it needs to adjust for its own API</li>
<li>In theory the <code>prompts/</code> files are both bundled in the executable AND overridable at runtime with a local directory, but I haven't verified that much</li>
</ul>
</li>
<li><strong>2024-12-18</strong> - System Upgrade Panic
<ul dir="auto">
<li>I auto-update my remarkable, usually fine</li>
<li>But I just got 3.16.2.3 and ... screenshots stopped working!</li>
<li>So I used <a href="https://github.com/Jayy001/codexctl">codexctl</a> to downgrade. It gave me a VERY scary "SystemError: Update failed!" and then the whole system locked up!</li>
<li>... but a reboot fixed it and the downgrade to 3.14.1.9 worked upon reboot</li>
<li>So... I'm keeping an eye out for other reports of issues on the new version</li>
<li>Oh yes. Now you can take prompts/general.json, rename it to <code>james.json</code> and go in and add "Your name is James" into the prompt. Then copy that to your reMarkable</li>
<li>Now run <code>./remarkable --prompt james.json</code> and it has a locally modified prompt!<br><a target="_blank" rel="noopener noreferrer" href="https://github.com/awwaiid/ghostwriter/blob/main/docs/james-name.png"><img src="https://github.com/awwaiid/ghostwriter/raw/main/docs/james-name.png" width="300"></a></li>
</ul>
</li>
<li><strong>2024-12-19</strong> -- Not Quite Local

</li>
<li><strong>2024-12-22</strong> -- Starting to Evaluate
<ul dir="auto">
<li>Starting to build out the evaluation system a bit more, including a <a href="https://github.com/awwaiid/ghostwriter/blob/main/run_eval.sh">basic script to kick it all off</a></li>
<li>Right now it is a hard-wired set of parameters which basically turn on/off segmentation and use either Claude 3.5 Sonnet or ChatGPT 4o-mini</li>
<li>See <a href="https://github.com/awwaiid/ghostwriter/blob/main/evaluation_results/2024-12-21_13-57-31/results.md">the initial evaluation report</a>!</li>
<li>I think markdown doesn't let me lay this out how I want, so will probably switch to html (maybe turn on github site hosting for it)</li>
<li>This is starting to get into the terratory where it can take some time and money to execute ... running this a bunch of times and I sent like $1. Not sure how long it took. but there were 48 executions in this final report</li>
<li>Oh -- I think it's rather important to run each set a few times assuming there is some temperature involved</li>
<li>To scale this even further we of course would want to bring in a JUDGE-BOT!</li>
<li>Then I could say things like "my new segmentation algorithm improved output quality by 17% per the JUDGE-BOT" etc</li>
</ul>
</li>
<li><strong>2024-12-25</strong> -- CLI simplify and expand
<ul dir="auto">
<li>Now you can pass just <code>-m gpt-4o-mini</code> and it will guess the engine is <code>openai</code></li>
<li>You can also pass <code>--engine-api-key</code> and <code>--engine-url-base</code></li>
<li>So now to use <a href="https://groq.com/" rel="nofollow">Groq</a>: <code>./ghostwriter -m llama-3.2-90b-vision-preview --engine-api-key $GROQ_API_KEY --engine openai --engine-base-url https://api.groq.com/openai</code></li>
<li>... but so far Llama 3.2 90b vision is still quite bad with this interface</li>
<li>I turned off a bunch of debugging. Now I'll need to go back and introduce log-level or something</li>
<li>BONUS: And now I've added Google Gemini! Try <code>-m gemini-2.0-flash-exp</code> with your <code>GOOGLE_API_KEY</code> set!<br><a target="_blank" rel="noopener noreferrer" href="https://github.com/awwaiid/ghostwriter/blob/main/docs/gemini_hello_chihuahua.png"><img src="https://github.com/awwaiid/ghostwriter/raw/main/docs/gemini_hello_chihuahua.png" width="200"></a></li>
</ul>
</li>
<li><strong>2024-12-28</strong> -- Usability
<ul dir="auto">
<li>I used a powered usb-hub to get an external keyboard plugged in, trying to see what sort of keyboard shortcuts we might have</li>
<li>That helped to get a further sense for where the keyboard input goes</li>
<li>So now I'm sending an extra touch-event in the bottom-center of the screen which will make the next keyboard input always go below the lowest element, which is what I wanted. Before it would go below the most recent typed text, so if you drew under that it would get confusing. Before, the answer to "what is your favorite color?" would have been placed directly below the first typed output; now it is nice and neatly put lower down! Also I guess this is a dream-bubble of a sheep?<br><a target="_blank" rel="noopener noreferrer" href="https://github.com/awwaiid/ghostwriter/blob/main/docs/sheep-dreams.png"><img src="https://github.com/awwaiid/ghostwriter/raw/main/docs/sheep-dreams.png" width="300"></a></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Ideas</h2><a id="user-content-ideas" aria-label="Permalink: Ideas" href="#ideas"></a></p>
<ul dir="auto">
<li>[DONE] Matt showed me his iOS super calc that just came out, take inspiration from that!
<ul dir="auto">
<li>This already kinda works, try writing an equation</li>
</ul>
</li>
<li>[DONE] A gesture or some content to trigger the request
<ul dir="auto">
<li>like an x in a certain place</li>
<li>or a hover circle -- doesn't need to be an actual touch event per se</li>
</ul>
</li>
<li>[DONE] Take a screenshot, feed it into a vision model, get some output, put the output back on the screen somehow</li>
<li>[DONE] Like with actual writing; or heck it can draw a million dots on the screen if it does it fast</li>
<li>[DONE] OK ... we can also send <em>keyboard</em> events! That means we can use the Remarkable text area. This is an awkward and weird text area that lives on a different layer from the drawing
<ul dir="auto">
<li>So maybe we can say drawing = human, text = machine</li>
<li>Probably a lot easier to erase too...</li>
</ul>
</li>
<li>[DONE] Basic Evaluation
<ul dir="auto">
<li>Create a set of screenshots for inputs</li>
<li>Represent different use-cases</li>
<li>Some of these, such as TODO-extraction, might have specific expectations for output or execution, but most of them won't</li>
<li>Run through the system to get example output -- text, svg, actions</li>
<li>Write a test suite to judge the results .... somewhat human powered? Separate VLM judge?</li>
</ul>
</li>
<li>[WIP] Prompt library
<ul dir="auto">
<li>There is already the start of this in <a href="https://github.com/awwaiid/ghostwriter/blob/main/prompts">prompts/</a></li>
<li>The idea is to give a set of tools (maybe actual llm "tools") that can be configured in the prompt</li>
<li>But also could put in there some other things ... like an external command that gets run for the tool</li>
<li>Example: a prompt that is good at my todo list management. It would look for "todo", extract that into a todo, and then run <code>add-todo.sh</code> or something
<ul dir="auto">
<li>(which would in turn ssh somewhere to add something to taskwarrior)</li>
</ul>
</li>
</ul>
</li>
<li>Initial config
<ul dir="auto">
<li>On first run, maybe create a config file</li>
<li>Could prompt for openai key and then write it into the file</li>
<li>Maybe an auto-start, auto-recovery?</li>
</ul>
</li>
<li>Generate Diagrams
<ul dir="auto">
<li>Let one of the outputs be plantuml and/or mermaid, and then turn that into an SVG/png that it then outputs to the screen</li>
</ul>
</li>
<li>External stuff
<ul dir="auto">
<li>Let it look things up</li>
<li>Let it send me stuff ... emails, slacks</li>
</ul>
</li>
<li>Conversation Mode
<ul dir="auto">
<li>On a single screen, keep track of each version of the screen betweent turns</li>
<li>So first send would be the screen</li>
<li>Second send would be the original screen and then the response screen (maybe with claude output in red) and then the new additions (maybe in green?)
<ul dir="auto">
<li>This could then be a whole chain for the page</li>
<li>Could have two separate buttons to trigger the VLM -- one for "new prompt" and one for "continue"</li>
</ul>
</li>
<li>OR we could make it so that every time it was the last three:
<ul dir="auto">
<li>Black: Original</li>
<li>Red: Claude response</li>
<li>Green: New input</li>
</ul>
</li>
<li>Or could use the same color structure but a whole chain of messages?</li>
<li>Might be weird when we go to a new blank page though. It'd look like the new input erased everything</li>
<li>In general this would also make it easier to handle scrolling maybe</li>
</ul>
</li>
<li>Run off of a network-local VLM (like ollama)
<ul dir="auto">
<li>First attempt at using the OpenAI-API compatible ollama failed; the ollama LLAMA 3.2 vision model doesn't support tools</li>
<li>Though Groq has a modified llama-3.2-vision that DOES have tools... but it isn't nearly as good as ChatGPT, Claude, or Gemini.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">References</h2><a id="user-content-references" aria-label="Permalink: References" href="#references"></a></p>
<ul dir="auto">
<li>Generally pulled resources from <a href="https://github.com/reHackable/awesome-reMarkable">Awesome reMarkable</a></li>
<li>Adapted screen capture from <a href="https://github.com/cloudsftp/reSnap">reSnap</a></li>
<li>Techniques for screen-drawing inspired from <a href="https://github.com/rmkit-dev/rmkit/blob/master/src/lamp/main.cpy">rmkit lamp</a></li>
<li>Super cool SVG-to-png done with <a href="https://github.com/RazrFalcon/resvg">resvg</a></li>
<li>Make the keyboard input device even without a keyboard via <a href="https://github.com/pl-semiotics/rM-input-devices">rM-input-devices</a></li>
<li>Not quite the same, but I recently found <a href="https://github.com/nickian/reMarkableAI">reMarkableAI</a> that does OCR→OpenAI→PDF→Device</li>
<li>Another reMarkable-LLM interface is <a href="https://github.com/StarNumber12046/rMAI">rMAI</a>. This one is a separate app (not trying to integrate in with simulated pen/keyboard input) and uses <a href="https://replicate.com/" rel="nofollow">replicate</a> as the model API service</li>
<li>I haven't adopted anything from it yet, but <a href="https://github.com/machinelevel/sp425-crazy-cow">Crazy Cow</a> is a cool/crazy tool that turns text into pen strokes for the reMarkable1</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Initial dependencies install (also ... rust, which I get via asdf)
rustup target add armv7-unknown-linux-gnueabihf
sudo apt-get install gcc-arm-linux-gnueabihf
cargo install cross

# Then to build
cross build --release --target=armv7-unknown-linux-gnueabihf

# And deploy by scp'ing the binary over and run it on the device!
scp target/armv7-unknown-linux-gnueabihf/release/ghostwriter remarkable:"><pre><span><span>#</span> Initial dependencies install (also ... rust, which I get via asdf)</span>
rustup target add armv7-unknown-linux-gnueabihf
sudo apt-get install gcc-arm-linux-gnueabihf
cargo install cross

<span><span>#</span> Then to build</span>
cross build --release --target=armv7-unknown-linux-gnueabihf

<span><span>#</span> And deploy by scp'ing the binary over and run it on the device!</span>
scp target/armv7-unknown-linux-gnueabihf/release/ghostwriter remarkable:</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Scratch Notes</h2><a id="user-content-scratch-notes" aria-label="Permalink: Scratch Notes" href="#scratch-notes"></a></p>
<div data-snippet-clipboard-copy-content="
# Record an evaluation on the device
./ghostwriter --output-file tmp/result.out --model-output-file tmp/result.json --save-screenshot tmp/input.png --no-draw-progress --save-bitmap tmp/result.png claude-assist

# On local, copy the evaluation to local and then put it into a folder
export evaluation_name=tic_tac_toe_1
rm tmp/*
scp -r remarkable:tmp/ ./
mkdir -p evaluations/$evaluation_name
mv tmp/* evaluations/$evaluation_name

# Run an evaluation
./target/release/ghostwriter --input-png evaluations/$evaluation_name/input.png --output-file tmp/result.out --model-output-file tmp/result.json --save-bitmap tmp/result.png --no-draw --no-draw-progress --no-loop --no-trigger claude-assist

# Layer the input and output
convert \( evaluations/$evaluation_name/input.png -colorspace RGB \) \( tmp/result.png -type truecolormatte -transparent white -fill red -colorize 100 \) -compose Over -composite tmp/merged-output.png"><pre><code>
# Record an evaluation on the device
./ghostwriter --output-file tmp/result.out --model-output-file tmp/result.json --save-screenshot tmp/input.png --no-draw-progress --save-bitmap tmp/result.png claude-assist

# On local, copy the evaluation to local and then put it into a folder
export evaluation_name=tic_tac_toe_1
rm tmp/*
scp -r remarkable:tmp/ ./
mkdir -p evaluations/$evaluation_name
mv tmp/* evaluations/$evaluation_name

# Run an evaluation
./target/release/ghostwriter --input-png evaluations/$evaluation_name/input.png --output-file tmp/result.out --model-output-file tmp/result.json --save-bitmap tmp/result.png --no-draw --no-draw-progress --no-loop --no-trigger claude-assist

# Layer the input and output
convert \( evaluations/$evaluation_name/input.png -colorspace RGB \) \( tmp/result.png -type truecolormatte -transparent white -fill red -colorize 100 \) -compose Over -composite tmp/merged-output.png
</code></pre></div>
<p dir="auto">Prompt / Tool ideas:</p>
<ul dir="auto">
<li>There are a few models for tools -- each tool can be re-usable and generalized or each tool could include things like extra-inputs for chain-of thought and hints for what goes into each parameter</li>
<li>The prompts should be plain JSON or YAML and should be normalized across V/LLM models</li>
<li>A general direction I'm thinking is to have top-level "modes" that each have a main prompt and a set of tools they can use</li>
<li>But maybe there can be a whole state-machine flow that the follow also?</li>
<li>So like ... a math-helper might have a different state-machine than a todo-helper</li>
<li>The states would be start, intermediate, and terminal</li>
<li>The terminal states should all have some output or effect, those are the ones that do something</li>
<li>The start state is the initial prompt</li>
<li>One intermediate state could be <code>thinking</code> where it can use the input of the tool as a place to write out thoughts, and the output of the tool is ignored</li>
<li>But overall what we're leading to here is a system where the prompts are easy to write, easy to copy/paste, easy to maintain</li>
<li>And then maybe we can have a set of evals or examples that are easy to use on top of a prompt mode</li>
<li>Increasingly, the reMarkable2 case might HAPPEN to be a specific prompt we set up in this system...</li>
<li>So the state machine chould be:</li>
</ul>
<section data-identity="6af76b40-5cb9-4682-aab3-e2f7f80c83d2" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;stateDiagram-v2\n    [*] --&amp;gt; Screenshot\n    Screenshot --&amp;gt; OutputScreen\n    Screenshot --&amp;gt; OutputKeyboardText\n&quot;}" data-plain="stateDiagram-v2
    [*] --> Screenshot
    Screenshot --> OutputScreen
    Screenshot --> OutputKeyboardText
">
      <pre lang="mermaid" aria-label="Raw mermaid code">stateDiagram-v2
    [*] --&gt; Screenshot
    Screenshot --&gt; OutputScreen
    Screenshot --&gt; OutputKeyboardText
</pre>
    </div>
  <span role="presentation">
    <span data-view-component="true">
      <span>Loading</span>
</span>
  </span>
</section>

<section data-identity="c37795ec-f89c-407a-b274-4c0d2f25e052" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;stateDiagram-v2\n    [*] --&amp;gt; WaitForTouch\n    WaitForTouch --&amp;gt; Screenshot\n    Screenshot --&amp;gt; OutputScreen\n    Screenshot --&amp;gt; OutputKeyboardText\n    OutputScreen --&amp;gt; [*]\n    OutputKeyboardText --&amp;gt; [*]\n&quot;}" data-plain="stateDiagram-v2
    [*] --> WaitForTouch
    WaitForTouch --> Screenshot
    Screenshot --> OutputScreen
    Screenshot --> OutputKeyboardText
    OutputScreen --> [*]
    OutputKeyboardText --> [*]
">
      <pre lang="mermaid" aria-label="Raw mermaid code">stateDiagram-v2
    [*] --&gt; WaitForTouch
    WaitForTouch --&gt; Screenshot
    Screenshot --&gt; OutputScreen
    Screenshot --&gt; OutputKeyboardText
    OutputScreen --&gt; [*]
    OutputKeyboardText --&gt; [*]
</pre>
    </div>
  <span role="presentation">
    <span data-view-component="true">
      <span>Loading</span>
</span>
  </span>
</section>

<section data-identity="9b785cb0-c2fd-40a6-aceb-97efa237742f" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;stateDiagram-v2\n    [*] --&amp;gt; WaitForTouch\n    WaitForTouch --&amp;gt; Screenshot\n    Screenshot --&amp;gt; Thinking\n    Thinking --&amp;gt; Thinking\n    Thinking --&amp;gt; OutputScreen\n    Thinking --&amp;gt; OutputKeyboardText\n    OutputScreen --&amp;gt; [*]\n    OutputKeyboardText --&amp;gt; [*]\n&quot;}" data-plain="stateDiagram-v2
    [*] --> WaitForTouch
    WaitForTouch --> Screenshot
    Screenshot --> Thinking
    Thinking --> Thinking
    Thinking --> OutputScreen
    Thinking --> OutputKeyboardText
    OutputScreen --> [*]
    OutputKeyboardText --> [*]
">
      <pre lang="mermaid" aria-label="Raw mermaid code">stateDiagram-v2
    [*] --&gt; WaitForTouch
    WaitForTouch --&gt; Screenshot
    Screenshot --&gt; Thinking
    Thinking --&gt; Thinking
    Thinking --&gt; OutputScreen
    Thinking --&gt; OutputKeyboardText
    OutputScreen --&gt; [*]
    OutputKeyboardText --&gt; [*]
</pre>
    </div>
  <span role="presentation">
    <span data-view-component="true">
      <span>Loading</span>
</span>
  </span>
</section>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VSCode's SSH Agent Is Bananas (513 pts)]]></title>
            <link>https://fly.io/blog/vscode-ssh-wtf/</link>
            <guid>42979467</guid>
            <pubDate>Sat, 08 Feb 2025 01:25:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/vscode-ssh-wtf/">https://fly.io/blog/vscode-ssh-wtf/</a>, See on <a href="https://news.ycombinator.com/item?id=42979467">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
         <dl>
             <dt>Author</dt>
             <dd>
                 <img alt="Thomas Ptacek" src="https://fly.io/static/images/thomas.webp">
               <dl>
                 <dt>Name</dt>
                 <dd>
                   Thomas Ptacek
                 </dd>
                  <dt>@tqbf</dt>
                  <dd>
                    <a href="https://twitter.com/tqbf" target="_blank">
                      @tqbf
                    </a>
                  </dd>
               </dl>
             </dd>
         </dl>

        <section>
          <p>We’re interested in getting integrated into the flow VSCode uses to do remote editing over SSH, because everybody is using VSCode now, and, in particular, they’re using forks of VSCode that generate code with LLMs. </p>
<p>”hallucination” is what we call it when LLMs get code wrong; “engineering” is what we call it when people do.</p>
<p>LLM-generated code is <a href="https://nicholas.carlini.com/writing/2024/how-i-use-ai.html" title="">useful in the general case</a> if you know what you’re doing. But it’s ultra-useful if you can close the loop between the LLM and the execution environment (with an “Agent” setup). There’s lots to say about this, but for the moment: it’s a semi-effective antidote to hallucination: the LLM generates the code, the agent scaffolding runs the code, the code generates errors, the agent feeds it back to the LLM, the process iterates. </p>

<p>So, obviously, the issue here is you don’t want this iterative development process happening on your development laptop, because LLMs have boundary issues, and they’ll iterate on your system configuration just as happily on the Git project you happen to be working in. A thing you’d really like to be able to do: run a closed-loop agent-y (“agentic”? is that what we say now) configuration for an LLM, on a clean-slate Linux instance that spins up instantly and that can’t screw you over in any way. You get where we’re going with this.</p>

<p>Anyways! I would like to register a concern.</p>

<p>Emacs hosts the spiritual forebearer of remote editing systems, a blob of hyper-useful Elisp called <a href="https://www.gnu.org/software/tramp/" title="">“Tramp”</a>. If you can hook Tramp up to any kind of interactive environment — usually, an SSH session — where it can run Bourne shell commands, it can extend Emacs to that environment.</p>

<p>So, VSCode has a feature like Tramp. Which, neat, right? You’d think, take Tramp, maybe simplify it a bit, switch out Elisp for Typescript.</p>

<p>You’d think wrong!</p>

<p>Unlike Tramp, which lives off the land on the remote connection, VSCode mounts a full-scale invasion: it runs a Bash snippet stager that downloads an agent, including a binary installation of Node. </p>

<p>I <em>think</em> this is <a href="https://github.com/microsoft/vscode/tree/c9e7e1b72f80b12ffc00e06153afcfedba9ec31f/src/vs/server/node" title="">the source code</a>?</p>

<p>The agent runs over port-forwarded SSH. It establishes a WebSockets connection back to your running VSCode front-end. The underlying protocol on that connection can:</p>

<ul>
<li>Wander around the filesystem
</li><li>Edit arbitrary files
</li><li>Launch its own shell PTY processes
</li><li>Persist itself
</li></ul>

<p>In security-world, there’s a name for tools that work this way. I won’t say it out loud, because that’s not fair to VSCode, but let’s just say the name is murid in nature.</p>

<p>I would be a little nervous about letting people VSCode-remote-edit stuff on dev servers, and apoplectic if that happened during an incident on something in production. </p>

<p>It turns out we don’t have to care about any of this to get a custom connection to a Fly Machine working in VSCode, so none of this matters in any kind of deep way, but: we’ve decided to just be a blog again, so: we had to learn this, and now you do too.</p>

          
        </section>
        <dl>
            <dt>
              Previous post  ↓
            </dt>
            <dd>
              <a href="https://fly.io/blog/ai-gpu-clusters-from-your-laptop-livebook/">
                AI GPU Clusters, From Your Laptop, With Livebook
              </a>
            </dd>
        </dl>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: ExpenseOwl – Simple, self-hosted expense tracker (173 pts)]]></title>
            <link>https://github.com/Tanq16/ExpenseOwl</link>
            <guid>42977388</guid>
            <pubDate>Fri, 07 Feb 2025 20:56:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Tanq16/ExpenseOwl">https://github.com/Tanq16/ExpenseOwl</a>, See on <a href="https://news.ycombinator.com/item?id=42977388">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/logo.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/logo.png" alt="ExpenseOwl Logo" width="250" height="250"></a><br>
</p><p dir="auto"><h2 tabindex="-1" dir="auto">ExpenseOwl</h2><a id="user-content-expenseowl" aria-label="Permalink: ExpenseOwl" href="#expenseowl"></a></p>
<p dir="auto">
<a href="https://github.com/tanq16/expenseowl/actions/workflows/release.yml"><img src="https://github.com/tanq16/expenseowl/actions/workflows/release.yml/badge.svg" alt="Release Build"></a>&nbsp;<a href="https://goreportcard.com/report/github.com/tanq16/expenseowl" rel="nofollow"><img src="https://camo.githubusercontent.com/e3f56fc7cf41ec08690d280b47ec938f61012e4d107f44ecc49709d99b6ec416/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f74616e7131362f657870656e73656f776c" alt="Go Report Card" data-canonical-src="https://goreportcard.com/badge/github.com/tanq16/expenseowl"></a><br>
<a href="https://github.com/Tanq16/expenseowl/releases"><img alt="GitHub Release" src="https://camo.githubusercontent.com/f33c9056d6576dd2a00d2c4837f87a84984267ad1b3d059d2a9a442e23fccab8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74616e7131362f657870656e73656f776c" data-canonical-src="https://img.shields.io/github/v/release/tanq16/expenseowl"></a>&nbsp;<a href="https://hub.docker.com/r/tanq16/expenseowl" rel="nofollow"><img alt="Docker Pulls" src="https://camo.githubusercontent.com/167ed4d5f1820e70a875a92e3ffac8358bcb4aa22b7e41a88d8618fb2997498f/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f74616e7131362f657870656e73656f776c" data-canonical-src="https://img.shields.io/docker/pulls/tanq16/expenseowl"></a>
</p>

<p dir="auto"><code>ExpenseOwl</code> is an extremely simple expense tracking system with a modern monthly pie-chart visualization. It tracks daily expenses, visualizes monthly spending patterns, and maintains an overview of financial habits.</p>
<hr>
<ul dir="auto">
<li><a href="#why-create-this">Why Create This</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#screenshots">Screenshots</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#technology-stack">Tech Stack</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Create This?</h2><a id="user-content-why-create-this" aria-label="Permalink: Why Create This?" href="#why-create-this"></a></p>
<p dir="auto">There are a ton of amazing projects for expense tracking across GitHub (<a href="https://github.com/actualbudget/actual">Actual</a>, <a href="https://github.com/firefly-iii/firefly-iii">Firefly III</a>, etc.). They're all incredible, but they aren't the <em>fastest</em> when trying to add expenses, and offer a ton of features which I don't use. Some use varying formats of data or complex APIs. <em>Don't get me wrong</em>, they're great when needed, but I wanted something dead simple that just gives me a pie chart per month and a tabular representation. NOTHING else!</p>
<p dir="auto">Hence, I created this project, which I use in my homelab to track my expenses. The data is just JSON, so I can do whatever I want with that, including using <code>jq</code> to convert to CSV. The UI is elegant and mobile-friendly.</p>
<p dir="auto">The intention of this app is to track spending across your categories in a simplistic manner. No complicated searching, no editing - just add, delete, and view! This intention is not going to change throughout the lifecycle of this project. This is not an app for budgeting, it's for tracking.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Functionality</h3><a id="user-content-core-functionality" aria-label="Permalink: Core Functionality" href="#core-functionality"></a></p>
<ul dir="auto">
<li>Simple expense tracking with essential details only (optional name, date without time, amount, and category)</li>
<li>UUID-based expense identification in the backend</li>
<li>Flat file storage system (<code>data/expenses.json</code>)</li>
<li>Multi-architecture Docker container with support for persistent storage</li>
<li>REST API for expense management</li>
<li>Single-user focused (mainly for a homelab deployment)</li>
<li>CLI for both server and client (if needed) operations</li>
<li>Custom categories via environment variable (<code>EXPENSE_CATEGORIES</code>) with sensible defaults</li>
<li>Custom currency symbol in the frontend via environment variable (<code>CURRENCY</code>)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Visualization</h3><a id="user-content-visualization" aria-label="Permalink: Visualization" href="#visualization"></a></p>
<ol dir="auto">
<li>Dashboard with expense category breakdown (pie chart)
<ul dir="auto">
<li>Click on a category to exclude it from the graph, click again to add it back</li>
<li>This helps visualize the breakdown without considering some categories like Rent</li>
<li>The legend shows a total expenditure of the month along with a total without the "Rent" category</li>
</ul>
</li>
<li>Table view for detailed expense listing
<ul dir="auto">
<li>This is where you can view individual expenses chronologically and delete them</li>
<li>You can use the browser's search to find a name if needed</li>
</ul>
</li>
<li>Month-by-month navigation</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Progressive Web App (PWA)</h3><a id="user-content-progressive-web-app-pwa" aria-label="Permalink: Progressive Web App (PWA)" href="#progressive-web-app-pwa"></a></p>
<p dir="auto">The frontend of ExpenseOwl can be installed as a Progressive Web App on desktop and mobile devices. To install:</p>
<ul dir="auto">
<li>Desktop: Click the install icon in your browser's address bar</li>
<li>iOS: Use Safari's "Add to Home Screen" option in the share menu</li>
<li>Android: Use Chrome's "Install" option in the menu</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Intention of Use</h3><a id="user-content-intention-of-use" aria-label="Permalink: Intention of Use" href="#intention-of-use"></a></p>
<p dir="auto">Reiterating that you should use this to just add expenses quickly. The default name for an expense is <code>unnamed</code> and the date is automatically set to current date. There's a default list of categories to choose from, which can be edited very easily.</p>
<p dir="auto">In the ideal case, <code>enter the amount and choose category</code> - that's it!</p>
<p dir="auto">For a bit more involved case, <code>enter the amount and name, choose the category, and select the date</code> - still very simple!</p>
<p dir="auto">The application only allows addition and deletion, there's no need for editing. There are no tags, no wallet info, no budgeting, nothing! Plain and simple for the win.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th></th>
<th>Desktop View</th>
<th>Mobile View</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dashboard Light</td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/dashboard-light.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/dashboard-light.png" alt="Dashboard Light"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/mobile-dashboard-light.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/mobile-dashboard-light.png" alt="Mobile Dashboard Light"></a></td>
</tr>
<tr>
<td>Dashboard Dark</td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/dashboard-dark.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/dashboard-dark.png" alt="Dashboard Dark"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/mobile-dashboard-dark.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/mobile-dashboard-dark.png" alt="Mobile Dashboard Dark"></a></td>
</tr>
<tr>
<td>Table Light</td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/table-light.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/table-light.png" alt="Table Light"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/mobile-table-light.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/mobile-table-light.png" alt="Mobile Table Light"></a></td>
</tr>
<tr>
<td>Table Dark</td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/table-dark.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/table-dark.png" alt="Table Dark"></a></td>
<td><a target="_blank" rel="noopener noreferrer" href="https://github.com/Tanq16/ExpenseOwl/blob/main/assets/mobile-table-dark.png"><img src="https://github.com/Tanq16/ExpenseOwl/raw/main/assets/mobile-table-dark.png" alt="Mobile Table Dark"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">The interface automatically adapts to system preferences for themes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Go Install</h3><a id="user-content-go-install" aria-label="Permalink: Go Install" href="#go-install"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="go install github.com/tanq16/expenseowl/cmd/expenseowl@latest"><pre>go install github.com/tanq16/expenseowl/cmd/expenseowl@latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker Installation</h3><a id="user-content-docker-installation" aria-label="Permalink: Docker Installation" href="#docker-installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="docker pull tanq16/expenseowl:main"><pre>docker pull tanq16/expenseowl:main</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -d \
--name expenseowl \
-p 8080:8080 \
-e EXPENSE_CATEGORIES=&quot;Rent,Food,Transport,Fun,Bills&quot; \
-e CURRENCY=jpy \
-v expenseowl_data:/app/data \
tanq16/expenseowl:main # EXPENSE_CATEGORIES, CURRENCY are optional configs"><pre>docker run -d \
--name expenseowl \
-p 8080:8080 \
-e EXPENSE_CATEGORIES=<span><span>"</span>Rent,Food,Transport,Fun,Bills<span>"</span></span> \
-e CURRENCY=jpy \
-v expenseowl_data:/app/data \
tanq16/expenseowl:main <span><span>#</span> EXPENSE_CATEGORIES, CURRENCY are optional configs</span></pre></div>
<p dir="auto">To use it with Docker compose or a container-management system like Portainer or Dockge, use this YAML definition:</p>
<div dir="auto" data-snippet-clipboard-copy-content="version: &quot;3.8&quot;
services:
  budgetlord:
    image: tanq16/expenseowl:main
    restart: unless-stopped
    ports:
      - 5006:8080
    volumes:
      - /home/tanq/expenseowl:/app/data # CHANGE DIR"><pre><span>version</span>: <span><span>"</span>3.8<span>"</span></span>
<span>services</span>:
  <span>budgetlord</span>:
    <span>image</span>: <span>tanq16/expenseowl:main</span>
    <span>restart</span>: <span>unless-stopped</span>
    <span>ports</span>:
      - <span>5006:8080</span>
    <span>volumes</span>:
      - <span>/home/tanq/expenseowl:/app/data </span><span><span>#</span> CHANGE DIR</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building from Source</h3><a id="user-content-building-from-source" aria-label="Permalink: Building from Source" href="#building-from-source"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/tanq16/expenseowl.git &amp;&amp; \
cd expenseowl"><pre>git clone https://github.com/tanq16/expenseowl.git <span>&amp;&amp;</span> \
<span>cd</span> expenseowl</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content="go build ./cmd/expenseowl"><pre>go build ./cmd/expenseowl</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Ideally, once deployed, just use the web interface and you're good to go. Access the web interface through your browser:</p>
<ul dir="auto">
<li>Dashboard: <code>http://localhost:8080/</code></li>
<li>Table View: <code>http://localhost:8080/table</code></li>
</ul>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">This app has no authentication, so deploy carefully. It works very well with a reverse proxy like Nginx Proxy Manager and is mainly intended for homelab use.</p>
</div>
<p dir="auto">If there are command-line automations that are required for use with the REST API, read on!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">CLI Mode</h3><a id="user-content-cli-mode" aria-label="Permalink: CLI Mode" href="#cli-mode"></a></p>
<p dir="auto">The application binary can run in either server or client mode:</p>
<p dir="auto">Server Mode (Default):</p>
<div dir="auto" data-snippet-clipboard-copy-content="./expenseowl
# or explicitly
./expenseowl --serve"><pre>./expenseowl
<span><span>#</span> or explicitly</span>
./expenseowl --serve</pre></div>
<p dir="auto">Client Mode:</p>
<div dir="auto" data-snippet-clipboard-copy-content="./expenseowl --client --addr localhost:8080"><pre>./expenseowl --client --addr localhost:8080</pre></div>
<p dir="auto">In client mode, you'll be prompted to enter the expense name, category (select from a list), amount, and date (in YYYY-MM-DD; optional, sets to current date when not provided).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">REST API</h3><a id="user-content-rest-api" aria-label="Permalink: REST API" href="#rest-api"></a></p>
<p dir="auto">Add Expense:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -X PUT http://localhost:8080/expense \
-H &quot;Content-Type: application/json&quot; \
-d '{
    &quot;name&quot;: &quot;Groceries&quot;,
    &quot;category&quot;: &quot;Food&quot;,
    &quot;amount&quot;: 75.50,
    &quot;date&quot;: &quot;2024-03-15T14:30:00Z&quot;
}'"><pre>curl -X PUT http://localhost:8080/expense \
-H <span><span>"</span>Content-Type: application/json<span>"</span></span> \
-d <span><span>'</span>{</span>
<span>    "name": "Groceries",</span>
<span>    "category": "Food",</span>
<span>    "amount": 75.50,</span>
<span>    "date": "2024-03-15T14:30:00Z"</span>
<span>}<span>'</span></span></pre></div>
<p dir="auto">Get All Expenses:</p>
<div dir="auto" data-snippet-clipboard-copy-content="curl http://localhost:8080/expenses"><pre>curl http://localhost:8080/expenses</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Config Options</h3><a id="user-content-config-options" aria-label="Permalink: Config Options" href="#config-options"></a></p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Currency Settings</h5><a id="user-content-currency-settings" aria-label="Permalink: Currency Settings" href="#currency-settings"></a></p>
<p dir="auto">ExpenseOwl supports multiple currencies through the CURRENCY environment variable. If not specified, it defaults to USD ($). Example, to run with Euro, use the following environment variable:</p>
<div dir="auto" data-snippet-clipboard-copy-content="CURRENCY=eur ./expenseowl"><pre>CURRENCY=eur ./expenseowl</pre></div>
<p dir="auto">Similarly, the environment variable can be set in a compose stack or using <code>-e</code> in the command line with a Docker command. The full list of currencies supported are present in <a href="https://github.com/Tanq16/ExpenseOwl/blob/main/internal/config/config.go#L27">this file</a>.</p>
<p dir="auto"><h5 tabindex="-1" dir="auto">Category Settings</h5><a id="user-content-category-settings" aria-label="Permalink: Category Settings" href="#category-settings"></a></p>
<p dir="auto">ExpenseOwl also supports custom categories, which can be specified through environment variables like so:</p>
<div dir="auto" data-snippet-clipboard-copy-content="EXPENSE_CATEGORIES=&quot;Rent,Food,Transport,Fun,Bills&quot; ./expenseowl"><pre>EXPENSE_CATEGORIES=<span><span>"</span>Rent,Food,Transport,Fun,Bills<span>"</span></span> ./expenseowl</pre></div>
<p dir="auto">Similarly, it can be specified in a Docker compose stack of a Docker CLI command with the <code>-e</code> flag. Refer to the examples shown above in the README.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Technology Stack</h2><a id="user-content-technology-stack" aria-label="Permalink: Technology Stack" href="#technology-stack"></a></p>
<ul dir="auto">
<li>Backend: Go</li>
<li>Storage: JSON file system</li>
<li>Frontend: Chart.js and vanialla web stack (HTML, JS, CSS)</li>
<li>Interface: CLI + Web UI</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A website that heatmaps your city based on your housing preferences (276 pts)]]></title>
            <link>https://theretowhere.com/</link>
            <guid>42975803</guid>
            <pubDate>Fri, 07 Feb 2025 18:23:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theretowhere.com/">https://theretowhere.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42975803">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[A Brief History of Code Signing at Mozilla (228 pts)]]></title>
            <link>https://hearsum.ca/posts/history-of-code-signing-at-mozilla/</link>
            <guid>42975436</guid>
            <pubDate>Fri, 07 Feb 2025 17:51:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hearsum.ca/posts/history-of-code-signing-at-mozilla/">https://hearsum.ca/posts/history-of-code-signing-at-mozilla/</a>, See on <a href="https://news.ycombinator.com/item?id=42975436">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody text">
    <p>Shipping large software to end-user devices is a complicated process. Shipping large software <em>securely</em> to end-user devices is even more complicated. Signing the things that ship to end-user devices is one of those complications, and it gets even more complicated when you sign thousands of artifacts per day.</p>

<p>Mozilla has been signing Firefox in some form beginning with Firefox 1.0. This began with detached GPG signatures for builds, and progressed to Authenticode signing for Windows installers in Firefox 1.0.1. Since then it has evolved over time to encompass other platforms, other types of files within our products, and other ways that we ship (such as our own update packages). This post will provide a overview of the what, when, why, and how of code signing at Mozilla over the past ~20 years.</p>

<h2>What, when, and why</h2>

<h3>Early GPG &amp; Authenticode Signing</h3>

<p>When we first began signing, it happened on a Windows machine. Late in the release process, after Windows installers had been built, we would download all of the release artifacts to this machine, sign the Windows installers, and generate detached GPG signatures for those before pushing the artifacts elsewhere.</p>

<p>At this time, the private keys and certificates were held on a USB stick that was kept removed from the machine at-rest. A Release Engineer needed to be physically present in Mountain View to perform this step. Once inserted, signing could be done via Remote Desktop rather than at the physical machine (but don't forget to remove the USB stick afterwards!).</p>

<p>GPG signing was done with the standard GPG tools, running in cygwin. Authenticode signing was also done with the standard (at the time) Microsoft 'signcode.exe' tool. An annoying fact about that tool, is that it only accepted the necessary passphrase from a GUI dialog. To work around this, we had an <a href="https://en.wikipedia.org/wiki/AutoIt">AutoIt</a> script running in the background that injected the passphrase into this dialog whenever it popped up. This interesting way of automating the process meant that mouse movements or keyboard interaction at the wrong time could interfere with the signing process.</p>

<p>This process was partly scripted, but there was still a series of ~15 commands someone had to run by hand (and not mess up) to get everything done. You can see these commands for yourself in our now-ancient <a href="https://wiki.mozilla.org/ReleaseEngineering/Unified_Release_Process#Sign_builds">Unified Release Process</a> documentation.</p>

<h3>Windows internal file signing</h3>

<p>Careful readers may have noted that early Authenticode signing only covered the Firefox installer itself, not the EXEs and DLLs inside of it. At some point (I haven't gone to the effort of tracking down exactly where...) we started signing these inner files as well. This process seems to have been <a href="https://wiki.mozilla.org/ReleaseEngineering/Unified_Release_Process#Signing_windows_files">lost to the sands of time</a>, but I seem to recall it worked very similarly to the installer signing process, but without the GPG parts.</p>

<h3>Improved signing on Windows</h3>

<p>The first notable improvement we had to this process was to automate most of the copy/pasting that was done from the wiki. This came in the form of <a href="https://github.com/mozilla/build-tools/blob/dba69406faad0c8e7a016150a3f5761ef83914d2/release/signing/Makefile">a Makefile that with a few mere inputs</a>, would download, sign, and re-upload the signed builds. The main benefit of this was reduced opportunity for human error.</p>

<p>Not long after that, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=470146">we had our first real game-changing improvements</a>. Chris AtLee <a href="https://atlee.ca/posts/blog20090804faster-signing/">wrote a nice post about it back in 2009</a>, and how his changes took signing time from 8 hours all the way down to sub-15 minutes. This was accomplished through a combination of faster hardware, parallelized signing, in-process compression &amp; decompression, and better caching. His changes also introduced <a href="https://github.com/mozilla/build-tools/blob/c2e8e6f048db8990d629f169072d2a06b42e4759/release/signing/sign-release.py">a hefty amount of python</a> into the signing process, which paved the way for the next big improvement...</p>

<h3>Automatic signing</h3>

<p>Despite the signing process now being very quick once it gets started, it could still sometimes take hours or longer to begin the process. Typically this would happen if our build and repack processes finished at a time when no Release Engineer was around to begin signing. This was solved with what we called "autosign". Rather than require a Release Engineer to be around at the right moment, we adjusted our scripts to allow them to <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=558464#c1">be started ahead of time</a>, and be smart enough to know when all of the files it needs to sign are ready. This work eliminated all wait time between builds being ready and signing running.</p>

<h3>Signing Windows builds...on Linux!</h3>

<p>In 2011, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=509158#c10">signing was rearchitected altogether</a>. In short, the idea was to move signing to a highly secured Linux server, and sign builds through an API as part of the build process. This allowed builds to be signed as they were produced, and reduced the number of times builds had to move from one server to another before they shipped.

An obvious question here is how we would manage to sign Windows binaries on Linux...as it turns out, the mono project had <a href="https://manpages.debian.org/testing/mono-devel/signcode.1.en.html">its own version of signcode that ran natively on Linux</a> that we were able to make use of.</p>

<h3>MAR Signing</h3>

<p>Shortly after (and perhaps even motivating - I'm not sure at this point) the aforementioned signing server work, we <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=699700">began signing our MAR (Mozilla ARchive) packages</a> that update users from an older version of Firefox to a newer version. Thanks to the earlier work, it was fairly trivial to use the same architecture to sign these files.</p>

<h3>macOS .app signing</h3>

<p>The idea of signing .app bundles for macOS was filed <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=400296#c0">all the way back in 2007</a>. There was <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=400296#c25">some initial work on this</a> in 2010, but we were unable to land it at that time. Around the same time that MAR signing was happening in 2012, we <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=400296#c68">picked up this work again</a> and managed to drive it home this time.</p>

<p>Unfortunately, there were no tools available at the time to sign macOS builds on anything except a fairly modern macOS machine. For this reason, we had to run additional copies of our signing server on macOS and sign those builds with them. (If you've ever had to run macOS as a server you'll know just how unfortunate this was...)</p>

<h3>Taskcluster/signingscript/iscript</h3>

<p>In 2018, Mozilla migrated its CI and Release automation from our aging Buildbot systems to <a href="https://taskcluster.net/">Taskcluster</a>. As part of this, signing tasks moved to specialized Taskcluster workers known as "signingscript" and "iscript", used for signing non-macOS and macOS builds respectively. These specialized workers continued to outsource the actual work of signing to the previously discussed signing servers.</p>

<p>An important part of this change is the introduction of <a href="https://scriptworker.readthedocs.io/en/latest/chain_of_trust.html">Chain of Trust</a>, a significant security enhancement that helps ensure that only authentic artifacts are signed to this day.</p>

<h3>Introducing...Autograph!</h3>

<p><a href="https://github.com/mozilla-services/autograph">Autograph</a> is Mozilla's modern code signing service. It was built specifically to provide a signing service that allowed us to keep private key material in Hardware Security Modules (HSMs). Migrating release signing to it was a huge improvement over the existing signing server where Release Engineers had direct access to such things. It was initially used for signing XPIs and APKs, but by the end of 2019 we had migrated all non-macOS signing to it and retired the old Linux signing servers.</p>

<p>In addition to the security enhancements it brought, we saw great performance wins with it as well, largely in thanks to it's support for <a href="https://github.com/mozilla-services/autograph/blob/main/docs/architecture.md#overview"> only requiring a hash of the bytes being signed to be sent over the wire</a>. (This requires that the client has some advanced knowledge of the file being signed, but it saves a <em>tremendous</em> amount of network traffic at our scale.)

</p>
<h3>Notarization with rcodesign</h3><p>

In 2023 we started making use of <a href="https://gregoryszorc.com/docs/apple-codesign/0.17.0/apple_codesign_rcodesign.html">rcodesign</a> to <a href="https://github.com/mozilla-releng/scriptworker-scripts/pull/714">notarize and staple our macOS builds</a>. While actual macOS code signing itself continues to happen on macOS machines, this allowed us to move at least some of our operations into the cloud and reduce our reliance on mac hardware.

</p><h2>Tools and tech</h2>

<p>I've mentioned a number of tools and technology that we use as part of signing, but I've purposely glossed over some details in the interest of brevity. The following section is a glossary of sorts, and introduces some more under the hood tools that we use as part of signing. If you're interested in the gory details, the links below should be enough to find them for yourself! Or you can stop by <a href="https://chat.mozilla.org/#/room/#firefox-ci:mozilla.org">#firefox-ci on Matrix to ask questions!</a></p>

<h3>osslsigncode</h3>
<p>osslsigncode is a <a href="https://github.com/mtrojnar/osslsigncode">tool that implements parts of Microsoft's signtool.exe</a>. In the past, we used it to directly sign PE files. These days, we use it's support for attaching the signatures that Autograph makes to them.</p>

<h3>winsign</h3>
<p>Winsign is a <a href="https://github.com/mozilla-releng/winsign">python library for signing and manipulating Authenticode signatures</a>. It relies on osslsigncode for writing signatures, and supports signing directly with a private key, or outsourcing the signing process to a passed in function. The latter is what we use, and it's how we inject a call to Autograph into the signing process.</p>

<h3>msix-packaging</h3>
<p>In 2021 we began shipping Firefox as an MSIX package. As part of this we discovered that osslsigncode does not support signing MSIX packages. Luckily for us, Microsoft's MSIX packaging tools are open source and run on Linux, and we found a <a href="https://github.com/microsoft/msix-packaging/issues/340#issuecomment-620797067">fork that contained most of what was needed</a> to support signing. With a few additional modifications, we were able to support signing these packages in our existing systems.</p>

<h3>apple-codesign</h3>
<p><a href="https://gregoryszorc.com/docs/apple-codesign/0.17.0/index.html">apple-codesign</a> is a very exciting project from Gregory Szorc which provides 3rd party tools capable of signing, notarizing, and stapling .app bundles and other Apple formats such as .pkg and .dmg. These tools run on Linux, and as noted above, we're already making use of them to notarize and staple our .app bundles.

We're extremely excited about this project, and grateful to Gregory Szorc for all the effort he's bit into it. In the future we're looking forward to migrating our actual code signing to these tools which would (finally) allow us to retire our dedicated macOS signing machines.</p>

<h3>mardor</h3>
<p><a href="https://github.com/mozilla-releng/build-mar">mardor</a> is a python tool to manage, and most importantly, sign, MAR files. In the days before Autograph it was used to directly sign MAR files. These days we only use it to inject signatures made by Autograph into the files, similar to our usage of osslsigncode.</p>

<h3>signingscript</h3>
<p><a href="https://github.com/mozilla-releng/scriptworker-scripts/tree/master/signingscript">signingscript</a> is the glue between our CI system (Taskcluster) and Autograph. Through a combination of the tools listed above, custom code in signingscript itself, and communication with Autograph it produces signed builds. It is additionally responsible for notarizing and stapling our macOS builds.</p>

<h3>iscript</h3>
<p><a href="https://github.com/mozilla-releng/scriptworker-scripts/tree/master/iscript">iscript</a> is essentially a pared down version of signingscript (in fact their code is both derived from our early signing server code), and is responsible for signing our macOS builds. iscript runs on a small cluster of mac minis, which are a huge pain in the butt to manage.

</p>
<h3>autograph</h3><p>
As noted in an earlier section <a href="https://github.com/mozilla-services/autograph">Autograph</a> is our modern code signing service. It has a simple HTTP API that accepts signing requests and returns signed data or files. In addition to signing various artifacts that we ship it also makes <a href="https://github.com/mozilla-services/autograph/blob/main/signer/contentsignaturepki/README.md">Content Signatures</a> on behalf of addons.mozilla.org, aus5.mozilla.org/Balrog (our update server), and some other backend services that Firefox communicates with, helping to ensure the security and integrity of requests made between Firefox and Mozilla-run services.

</p><h2>Conclusion</h2>
<p>What a ride it's been over the last 20 years! We've gone from signing nothing to signing nearly everything in some form. Signing started off as a very manual process, and now happens seamlessly thousands of times per day.</p>

<p>I don't think it would be possible to name everyone that contributed to this, but it took the ideas and efforts of tens, if not hundreds, of people to get to this point: release engineers, build system experts, security folks, and many others were all critical to getting us where we are today.</p>

<p>I've got this post as brief as possible, but if you're interested in more details on any parts here feel free to reach out!</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A German court rules: X must provide researchers access to data (264 pts)]]></title>
            <link>https://www.reuters.com/world/europe/german-civil-activists-claim-victory-case-against-musks-x-2025-02-07/</link>
            <guid>42975170</guid>
            <pubDate>Fri, 07 Feb 2025 17:27:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/world/europe/german-civil-activists-claim-victory-case-against-musks-x-2025-02-07/">https://www.reuters.com/world/europe/german-civil-activists-claim-victory-case-against-musks-x-2025-02-07/</a>, See on <a href="https://news.ycombinator.com/item?id=42975170">Hacker News</a></p>
Couldn't get https://www.reuters.com/world/europe/german-civil-activists-claim-victory-case-against-musks-x-2025-02-07/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Ketamine for Depression: How It Works (2024) [video] (101 pts)]]></title>
            <link>https://www.yalemedicine.org/news/ketamine-for-depression</link>
            <guid>42974882</guid>
            <pubDate>Fri, 07 Feb 2025 16:59:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.yalemedicine.org/news/ketamine-for-depression">https://www.yalemedicine.org/news/ketamine-for-depression</a>, See on <a href="https://news.ycombinator.com/item?id=42974882">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="News Content Sections"><p>When at least two trials of a standard anti-depression medication fail to alleviate symptoms, <a data-ym-type="news" href="https://www.yalemedicine.org/news/ketamine-depression">ketamine</a> treatment may be an option. </p> <p>When someone has depression, there are changes in the brain’s circuitry, including how neurons communicate with one another. Ketamine seems to allow for a regrowth of synapses (connections between neurons), explains <a data-ym-type="specialist" href="https://www.yalemedicine.org/specialists/john-krystal">John Krystal, MD</a>, chair of the Yale School of Medicine Department of Psychiatry. </p> <p>The drug targets a different system in the brain than typical antidepressants, and that may be why ketamine works so well, even for patients who have not had success with different standard antidepressants, says Yale’s <a data-ym-type="specialist" href="https://www.yalemedicine.org/specialists/r-katz">Rachel Katz, PA-C, MS</a>.</p> <p>There's evidence that <a data-ym-type="news" href="https://www.yalemedicine.org/news/ketamine-the-new-miracle-drug">ketamine</a> can have a very rapid onset of effects in neuroplasticity, says <a data-ym-type="specialist" href="https://www.yalemedicine.org/specialists/gerard-sanacora">Gerard Sanacora, MD, PhD</a>, director of the Yale Depression Research Center. </p> <p>“We think that this may open a critical period in which the brain is more plastic, both at the cellular level and possibly even at the functional level, where we can start to work to change people's reference of thinking or their ability to respond and adapt to new social or environmental stimuli,” Dr. Sanacora says.</p> <p>In the video above, Yale experts talk more about ketamine and how it can work with a comprehensive treatment plan that includes psychotherapy, such as cognitive behavioral therapy (CBT).</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The origins of 60-Hz as a power frequency (163 pts)]]></title>
            <link>https://ieeexplore.ieee.org/document/628099</link>
            <guid>42974809</guid>
            <pubDate>Fri, 07 Feb 2025 16:53:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ieeexplore.ieee.org/document/628099">https://ieeexplore.ieee.org/document/628099</a>, See on <a href="https://news.ycombinator.com/item?id=42974809">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="LayoutWrapper">
						











						
						






<meta name="cToken" content="eyJhbGciOiJIUzUxMiIsInppcCI6IkRFRiJ9.eNqqVkosKFCyUoooyMkvSlXSUcosLgZyK2Dc1AqgrKG5sYWluYGhgRFQPrEEKmBmbmJgVAsAAAD__w.6tGQAfnOFRU6v3-ikbAgQX2rcH1j1xshbtxMlFogkS3_srDDbbWAPqIzOPrIi0wXbbZ2IRlWr92-qH47WzLj2g">



<!-- XPL-21560-Added as part of Universal CASA-Dev -->



<!--- This is a picture popup embed for mobilew view.  -->












<div>
		<xpl-root>
			
		</xpl-root>
	</div>

						




<div id="xploreFooter">
		<div>
			<div>
				<h3>IEEE Account</h3>
				<ul>
					<li><a href="https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Change Username/Password</a></li>
					<li><a href="https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Update Address</a></li>
				</ul>
			</div>
			<div>
				<h3>Purchase Details</h3>
				<ul>
					<li><a href="https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Payment Options</a></li>
					<li><a href="https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Order History</a></li>
					<li><a href="https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp">View Purchased Documents</a></li>
				</ul>
			</div>
			<div>
				<h3>Profile Information</h3>
				<ul>
					<li><a href="https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Communications Preferences</a></li>
					<li><a href="https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Profession and Education</a></li>
					<li><a href="https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&amp;refSiteName=IEEE%20Xplore">Technical Interests</a></li>
				</ul>
			</div>
			<div>
				<h3>Need Help?</h3>
				<ul>
					<li><strong>US &amp; Canada:</strong> +1 800 678 4333</li>
					<li><strong>Worldwide: </strong> +1 732 981 0060<br>
					</li>
					<li><a href="https://ieeexplore.ieee.org/xpl/contact">Contact &amp; Support</a></li>
				</ul>
			</div>
		</div>
		<div>
						<ul>
							<li><a href="https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore">About IEEE <em>Xplore</em></a></li>
							<li><a href="https://ieeexplore.ieee.org/xpl/contact">Contact Us</a></li>
							<li><a href="https://ieeexplore.ieee.org/Xplorehelp" target="blank">Help</a></li>
							<li><a href="https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement" target="blank">Accessibility</a></li> 
							<li><a href="https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use" target="_blank">Terms of Use</a></li>
							<li><a href="http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html">Nondiscrimination Policy</a></li>
							<li><a href="https://ieeexplore.ieee.org/xpl/sitemap.jsp">Sitemap</a></li>
							<li><a href="http://www.ieee.org/about/help/security_privacy.html" target="blank">Privacy &amp; Opting Out of Cookies</a></li>
						</ul>
						<p>
							A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.<br>© Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.
						</p>
					</div>
	</div>

						<!-- BEGIN: tealium in v2/common/template.jsp. We need to include tealiumAnalytics.js here since Angular 2+ app load if you load after commnon.js then tealium value will not be available in angular 2+ app  -->
						






		<!-- BEGIN: TealiumAnalytics.jsp -->
		
		
		
		
		
		
		
		
		
		
		
		
			
				
			
			
			
			
		
		
		
		
		
		

			


			

			
			


		
 		
		<!-- END: TealiumAnalytics.jsp -->
			 

						<!-- END: tealium in v2/common/template.jsp -->
						






	
	






















	
	


<!-- START OF Angular bundle assets -->




<!-- END OF Angular bundle assets -->

<!-- Usabilla Combicode for IEEE-->
<!-- Begin Usabilla for Websites embed code -->

<!-- end usabilla live embed code -->




<!-- START: ZoomInfo JS beacon integration XPL-27850-->
	
<!-- END: ZoomInfo JS beacon integration-->



<!-- START: Hum JS beacon integration XPL-27979-->

<!-- END: Hum JS beacon integration-->





	









<g:compress>








		
		
			
				
					
					
								
					
								
			
				
					
						








 



					
					
								
			
				
					
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
				
					
					
								
					
								
			
		
	








</g:compress>








		
			
					
			
					
			
				
					








					
			
					
			
					
			
					
			
					
			
		
		
	




	<!--Begin Optional Configuration-->
	
	
	
	
	
	


	
	
	

	<!--End Optional Configuration-->


<!-- Removed due to network issues when loading in China -->
<!-- <script type="text/javascript" src="http://s7.addthis.com/js/250/addthis_widget.js#pubid=ra-5005a435228f9245" async="async"></script>-->

<!-- Load Mathjax and process the document for Mathjax characters -->


<!-- <script type="text/javascript" src="/xploreAssets/MathJax-274/MathJax.js?config=default"></script> -->






						







					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stop Using Zip Codes for Geospatial Analysis (2019) (170 pts)]]></title>
            <link>https://carto.com/blog/zip-codes-spatial-analysis</link>
            <guid>42974728</guid>
            <pubDate>Fri, 07 Feb 2025 16:46:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://carto.com/blog/zip-codes-spatial-analysis">https://carto.com/blog/zip-codes-spatial-analysis</a>, See on <a href="https://news.ycombinator.com/item?id=42974728">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The last time you used your <a href="https://www.gislounge.com/understanding-and-playing-with-zip-codes/">zip code</a>,&nbsp;you were most likely entering your address into a website to make a purchase,&nbsp;finding a store near your home or office,&nbsp;or filling out some other online form. You likely found the answer you were looking for and didn't stop to think further about that five-digit code you'd just typed out.</p><p>However,&nbsp;lots of companies,&nbsp;marketers&nbsp;and data analysts spend hours looking at zip codes. They are deciding how to use data tied to those zip codes to understand trends,&nbsp;run their businesses,&nbsp;and find new ways to reach you&nbsp;- all using that same five-digit code.</p><p>Even though there are different place associations that probably mean more to you as an individual,&nbsp;such as a neighborhood,&nbsp;street,&nbsp;or the block you live on,&nbsp;the zip code is,&nbsp;in many organizations,&nbsp;the geographic unit of choice. It is used to make major decisions for marketing,&nbsp;opening or closing stores,&nbsp;providing services,&nbsp;and making decisions that can have a massive financial impact.</p><p>The problem is that zip codes are not a good representation of real human behavior,&nbsp;and when used in data analysis,&nbsp;often mask real,&nbsp;underlying insights,&nbsp;and may ultimately lead to bad outcomes. To understand why this is,&nbsp;we first need to understand a little more about the zip code itself.</p><p>Looking to learn more about Spatial Data Science? Check out our range of <a href="https://spatial-data-science-conference.com/">Spatial Data Science events</a>!</p><h3>The Zip Code: A Brief History</h3><p>The predecessor to the zip code was the postal zone &nbsp;which represented a post office department for a specific city. For example:</p><p><strong>Mr. John Smith<br>3256 Epiphenomenal Avenue<br>Minneapolis 16 &nbsp;Minnesota</strong></p><p>"16" represents the postal zone in a Minneapolis. But with more and more mail being sent,&nbsp;in 1963 the Postal Service decided to roll out the <strong>Zone Improvement Plan</strong> &nbsp;which transformed addresses to look like the following:</p><p><strong>Mr. John Smith<br>3256 Epiphenomenal Avenue<br>Minneapolis &nbsp;MN 55416</strong></p><p>The five digit code represents a part of the country (5_ _ _ _ ) &nbsp;a sectional center facility ( _ 5 4 _ _ ) &nbsp;and the associate post office or delivery area (_ _ _ 1 6). </p><figure><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/66b9cffc5d14b6e5ca213d33_639c39c9c67f664c8539d8da_zip-first-digit.png" alt="First digit of every zip code" loading="lazy"></p></figure><p>The first digit for every zip code for the states in the contiguous United States</p><p>By 1967 ZIP codes were made mandatory for bulk mailers and continued to be adopted by almost anyone sending mail in the US. Over time,&nbsp;the ZIP+4 was added to add more granularity to the zip code to denote specific locations &nbsp;even buildings for postal workers to deliver. The Postal Service even created a character,&nbsp;<a href="https://en.wikipedia.org/wiki/Mr._ZIP">Mr. Zip</a>,&nbsp;to promote the use of ZIP codes,&nbsp;who was featured on stamps,&nbsp;<a href="https://www.youtube.com/watch?v=Tf71PHyOfR0">commercials</a> &nbsp;and <a href="https://www.youtube.com/watch?v=ojn6U2kP_pQ">songs</a>.</p><figure><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/639c39c9c67f66731639d8de_mr-zip.jpeg" alt="Mr. Zip" loading="lazy"></p></figure><p>ZIP codes themselves do not actually represent an area,&nbsp;rather a collection of routes:</p><p>Despite the geographic derivation of most ZIP Codes,&nbsp;the codes themselves do not represent geographic regions; in general,&nbsp;they correspond to address groups or delivery routes. As a consequence &nbsp;ZIP Code "areas" can overlap &nbsp;be subsets of each other,&nbsp;or be artificial constructs with no geographic area (such as 095 for mail to the Navy &nbsp;which is not geographically fixed). In similar fashion,&nbsp;in areas without regular postal routes (<a href="https://en.wikipedia.org/wiki/Rural_delivery_service">rural route</a> areas) or no mail delivery (undeveloped areas) &nbsp;ZIP Codes are not assigned or are based on sparse delivery routes &nbsp;and hence the boundary between ZIP Code areas is undefined.</p><p>The US Census provides data for <a href="https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html">ZIP Code Tabulation Areas</a>,&nbsp;or geographic files:</p><p>ZIP Code Tabulation Areas (ZCTAs) are generalized areal representations of United States Postal Service (USPS) ZIP Code service areas.The USPS ZIP Codes identify the individual post office or metropolitan area delivery station associated with mailing addresses. USPS ZIP Codes are not areal features but a collection of mail delivery routes.</p><p>Here we find our first problem with ZIP Codes,&nbsp;that they do not represent an actual area on a map,&nbsp;but rather a collection of routes that help postal workers effectively deliver mail. They aren't designed to measure sociodemographic trends &nbsp;as a business would generally want to do. You can actually look up <a href="https://www.melissa.com/v2/lookups/mapcartzip/zipcode?zipcode=10009">individual delivery routes</a> &nbsp;like the one below:</p><figure><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/639c39c9c67f66975339d8e4_zip-route-nyc.png" alt="One zip code route in New York's East Village" loading="lazy"></p></figure><p>One zip code route in New York's East Village</p><p>We are only scratching the surface of the issue here. Similar issues exist around the world,&nbsp;with postal codes representing strange boundaries</p><figure><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/639c39c9c67f66abda39d8dc_global-zip-shapes.png" alt="Postal Codes in London  Toronto  and Sydney" loading="lazy"></p></figure><p>Postal Codes in London,&nbsp;Toronto,&nbsp;and Sydney.</p><h3>Using ZIP codes for data analysis</h3><p>Fast forward to today,&nbsp;where many companies can easily look into their database and find a dataset with a zip_code column in it, which allows them to group and aggregate data to see trends and business performance metrics. As stated earlier,&nbsp;the problem with ZIP Codes is that:</p><ol role="list"><li>They don't represent real boundaries &nbsp;but rather routes</li><li>They don't represent how humans behave</li></ol><p>The later represents two specific issues in using spatial data: spatial scale of observations and spatial scale support (you can learn more about this in this lecture from UChicago's Luc Anselin, <a href="https://youtu.be/MmCYeJ27DsA?t=326">here</a>). The first is that humans don't behave based on <a href="https://carto.com/spatial-data-catalog/boundary-data">administrative units</a> such as zip codes,&nbsp;or even census units. Their behavior is influenced much more by their neighbors,&nbsp;or areas such as a neighborhoods or high activity areas (such as central business districts). The second is that spatial data is provided at multiple scales,&nbsp;and many times those boundaries are overlapping or nested within another boundary. </p><p>Let's look at an example of this in one specific area in Dallas.</p><p>In this map,&nbsp;we can see large white boundaries,&nbsp;which represent <a href="https://carto.com/spatial-data-catalog/browser/dataset/acs_sociodemogr_716c8567/">ZIP code boundaries</a>,&nbsp;and below them are boundaries for <a href="https://carto.com/spatial-data-catalog/browser/geography/tigr_blockgroup_688551c7/">US Census Block Groups</a>. The darker green represents higher income,&nbsp;as provided by the US Census. You can access both of these datasets - with associated sociodemographic data - via our <a href="https://carto.com/spatial-data-catalog/browser">Spatial Data Catalog</a>.</p><p>When we look at one specific ZIP Code we can see that income data in even more detail:</p><p>What we can see is that 12 month median household income in this single zip code (75206) ranges from $9,700 to $227,000 when we look at block groups that completely or partially fall within this single ZIP Code,&nbsp;which the Census lists as having a median household income of $63,392.</p><p>Median Income is one way to evaluate the range of values within a ZIP Code (keep in mind these are ZCTA boundaries) but we can likely see variance like this in population,&nbsp;employment,&nbsp;and other relevant metrics for data analysis. </p><p>Sticking with median household income,&nbsp;we decided to expand this analysis to the entire United States,&nbsp;to see which areas are the least and most in-equal when you look at ZIP Codes and the Census Block Groups that intersect with the ZCTA Boundaries.</p><p><strong>The most unequal zip code is 33139 in Miami Beach &nbsp;FL</strong></p><ol role="list"><li>33139: Miami &nbsp;FL ($241,344 Difference)</li><li>44120: Cleveland &nbsp;OH ($237,501 Difference)</li><li>10013: New York &nbsp;NY ($233,559 Difference)</li><li>10023: New York &nbsp;NY ($233,157 Difference)</li><li>11201: Brooklyn &nbsp;NY ($233,031 Difference)</li><li>10601: White Plains &nbsp;NY ($232,813 Difference)</li><li>33141: Miami &nbsp;FL ($232,633 Difference)</li><li>92648: Huntington Beach &nbsp;CA ($231,290 Difference)</li><li>98105: Seattle &nbsp;WA ($230,906 Difference)</li><li>33143: Miami &nbsp;FL ($230,626 Difference)</li></ol><p><strong>The most similar* zip code is in Chesapeake &nbsp;WV</strong></p><ol role="list"><li>25315: Chesapeake &nbsp;WV ($2 Difference)</li><li>79357: Cone &nbsp;TX ($4 Difference)</li><li>65052: Linn Creek &nbsp;MO ($9 Difference)</li><li>73093: Washington &nbsp;OK ($12 Difference)</li><li>68370: Hebron &nbsp;NE ($13 Difference)</li><li>19541: Mohrsville &nbsp;PA ($15 Difference)</li><li>05340: Bondville &nbsp;VT ($18 Difference)</li><li>12958: Mooers &nbsp;NY ($26 Difference)</li><li>19941: Ellendale &nbsp;DE ($37 Difference)</li><li>54896: Loretta &nbsp;WI ($38 Difference)</li></ol><p><em>* similar where the difference is greater than 0</em></p><p>We can quickly use <a href="https://carto.com/workflows">CARTO Workflows</a> to perform this analysis for all of the United States without code. If you'd like to have a go yourself, you can sign up for a free 14-day CARTO trial <a href="https://app.carto.com/signup/">here</a>. </p><p>In this workflow (see below), we used a <a href="https://docs.carto.com/carto-user-manual/workflows/components/joins#spatial-join">Spatial Join</a> to establish which census blocks intersect each Zip Code. Next, the <a href="https://docs.carto.com/carto-user-manual/workflows/components/joins#spatial-join">Group By</a> component was used to calculate the minimum and maximum income for each zip code, after which the Create Column component was used to calculate the difference between the two. </p><figure><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/66b9cffc5d14b6e5ca213d37_65609c448a3c55d4b787852c_zip_codes_workflow.png" loading="lazy" alt="A screenshot of CARTO Workflows being used to analyze zip codes "></p></figure><p>After creating this as a new table we can see that the majority of the most unequal zip codes tend to be in cities or larger metro areas and more equal zip codes tend to be in rural areas around the country.</p><p><a href="https://clausa.app.carto.com/map/29fd0873-64cb-42a6-a90d-c83a8840bbfe">Check out the full map.</a></p><p> <a href="https://carto.com/signup/" target="_blank">Sign me up for a FREE account</a></p><p>Learn more about how organisations are changing the way they perform spatial analysis with our FREE ebook <a href="https://go.carto.com/state-spatial-data-science-2024">The State of Spatial Data Science 2024</a>!</p><figure><a href="https://go.carto.com/state-spatial-data-science-2024" target="_blank"><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/66b9cffc5d14b6e5ca213d1b_66b9cfe8dd3bc64e0fcb225b_State%2520of%2520Spatial%2520Data%2520Science%2520202%2520-%2520Email%2520620x280.png" loading="lazy" alt="A promotional imagefor the report"></p></a></figure><h3>So why do we use ZIP Codes?</h3><p>In practice,&nbsp;it is easy to use zip codes as a geospatial unit. As we stated earlier,&nbsp;almost any e-commerce or delivery service,&nbsp;or app that uses location or needs to locate their users in any way will collect a zip code. Additionally &nbsp;everyone is familiar with zip codes &nbsp;as they are part of any address in most countries. </p><figure><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/639c39c9c67f669fdb39d8db_zip-lookup.png" alt="'Find A Store' on Target.com" loading="lazy"></p></figure><p>'Find A Store' on Target.com</p><p>Terms like Census Block Group or Tract are less familiar to those who don't work with geospatial data on a regular basis &nbsp;and they can be more difficult to find and harder to work with &nbsp;especially if you aren't familiar with terms like Shapefile, FTP,&nbsp;and ETL. Even then you have to go through the US Census FTP website,, download geographic files state by state &nbsp;and join those files to census measures.</p><p>Finally,&nbsp;most people know &nbsp;without looking at a map that zip codes represent a smaller area than a city,&nbsp;but larger than say a neighborhood. Conceptually,&nbsp;they feel small enough to get a very focused view of the world,&nbsp;and big enough to capture enough of a sample size of trends.</p><p><strong>The short answer is that zip codes are easy to find,&nbsp;familiar,&nbsp;and provide a granular enough view of the world (or so we thought).</strong></p><p>With that said there are real world problems that arise from using zip codes in geospatial analysis. One example is in real estate where,&nbsp;in many cities or areas,&nbsp;there are homes listed in a &nbsp;zip code &nbsp;even though as we know those boundaries are arbitrary. <a href="https://hbr.org/2019/04/research-when-airbnb-listings-in-a-city-increase-so-do-rent-prices">This article</a> from the Harvard Business Review also describes a similar phenomenon with Airbnb listings and rising rent prices.</p><p>In simple terms &nbsp;we argue that if a zip code is "touristy," meaning it has a lot of restaurants and bars,&nbsp;and if awareness of Airbnb increases &nbsp;which we measure using the Google search index for the keyword "Airbnb," then any jump in Airbnb supply in that zip code is likely driven by an increase in demand for short-term rentals through Airbnb &nbsp;rather than local economic conditions.</p><p>More importantly,&nbsp;using zip codes for analysis can mask serious conditions that are taking place at a different spatial scale. The Flint water crisis was one of these cases. This article by Richard Casey Sadler,&nbsp;an Assistant Professor at Michigan State University describes the problem in great detail and raises similar points about zip codes (the full article is well worth reading):</p><p>Dr. Tony Grubesic,&nbsp;an Arizona State University professor &nbsp;has called them "one of the <a href="http://www.sciencedirect.com/science/article/pii/S0038012106000516">quirkier 'geographies'</a> in the world." Dr. Nancy Krieger,&nbsp;a Harvard University professor &nbsp;and colleagues have called out their <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1447194/">unacceptability</a> for small-area analyses.</p><p>Ultimately the state used zip codes to analyze the blood lead statistics in aggregate,&nbsp;which effectively masked the actual issue. This is because:</p><p>In Flint's case,&nbsp;the state's error was introduced because "Flint ZIP codes" do not align well with the city of Flint or its water system. The city and water system are almost 100 percent coterminous – that is &nbsp;they share the same boundaries… &nbsp;In total,&nbsp;Flint ZIP codes used in the state's analysis blanket parts of eight different municipalities (seven townships and one city) surrounding Flint.</p><figure><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/639c39c9c67f66616a39d8e1_flint-zip-misallignment.jpeg" alt="'Map by Richard Casey Sadler" loading="lazy"></p></figure><p>Map by Richard Casey Sadler</p><p>In the case of Flint &nbsp;simply looking at a different spatial scale or analysis may have shown the problem more clearly. For fields like public health and critical services &nbsp;understanding and using appropriate spatial scale is critically important.</p><h3>What else can we do?</h3><p>So if you are compelled to do away with zip code analysis,&nbsp;the good news is that there are several different options available.</p><h4>Use Addresses</h4><p>The first and foremost recommendation would be to use real world addresses. When you know an address string,&nbsp;you can use a geocoder - or the service that Google or other map search engines use to take an address and transform it into latitude and longitude. Most every major service offers an API and wrappers for different languages to do this. Keep in mind there are generally some <a href="https://docs.mapbox.com/help/troubleshooting/address-geocoding-format-guide/">best practices</a> for cleaning your data &nbsp;and you will need valid address strings to do so. Other tools like <a href="https://github.com/openvenues/libpostal">Libpostal</a> can help you normalize and parse your address strings.</p><h4>Use Census Units</h4><p>You can also use Census units such as a Census Block Group or Tract. As I mentioned it has not always been easy to find and collect this data at scale &nbsp;but there are many new tools that are becoming available to use. <a href="https://github.com/cenpy-devs/cenpy">CenPy</a> is a Python library that allows you to connect and find Census data (good <a href="https://medium.com/@mswhitetoyou/scraping-us-census-data-via-cenpy-9aeab12c877e">tutorial here</a>) where you can find measures from the decennial census or American Community Survey. CARTO also provides Census and ACS through the <a href="https://carto.com/developers/data-observatory/">Data Observatory</a> &nbsp;which was used in the notebook for the full US analysis.</p><p>You can also find out Census geometry IDs for a specific address location using the <a href="https://www.census.gov/data/developers/data-sets/Geocoding-services.html">US Census Geocoder</a>. You can pass in a variety of parameters via the API or use it in Python with the <a href="https://pypi.org/project/censusgeocode/">censusgeocode</a> package.</p><p>As your gather address data,&nbsp;you can easily add in a Census Tract or Block Group ID to that entry &nbsp;and use that rather than the zip code field in your data. This will allow you to do the same aggregation you were doing &nbsp;except at a more appropriate geographic scale.</p><h4>Use your own Spatial Index</h4><p>Finally -&nbsp;and arguably the best option - is to use a Spatial Index.</p><p>Spatial Indexes like <a href="https://eng.uber.com/h3/">H3</a> and <a href="http://s2geometry.io/%3ES2%20cells%3C/a%3E%20%20you%20can%20use%20%3Ca%20href=">quadkey</a> are global, multi-resolution grid systems which are geolocated by a short reference ID, rather than a complex geometry. This makes them super lightweight to store and lightning-fast to analyze. The grid cells have the same shape (although their size varies by latitude), meaning data is evenly geographically distributed - avoiding boundary and visual bias which is often caused by irregular grids like zip code; read&nbsp;more <a href="https://carto.com/blog/hexagons-for-location-intelligence">here</a>. </p><p>‍</p><p>A key benefit here is the anonymization of sensitive data. Given that address data can be sensitive &nbsp;you can create a data pipeline that simply reads incoming addresses &nbsp;geocodes them,&nbsp;assigns a spatial index,&nbsp;then passes that indexed data into a separate table,&nbsp;then you can store or delete the address data as needed.</p><figure><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/639c39c9c67f66ae7839d8df_uber-h3-cells.png" alt="Uber H3 Cells" loading="lazy"></p></figure><p>Want to learn more about Spatial Indexes? Check out our ebook: <a href="https://go.carto.com/report-spatial-indexes-101">Spatial Indexes 101</a>! </p><figure><a href="https://go.carto.com/report-spatial-indexes-101" target="_blank"><p><img src="https://cdn.prod.website-files.com/63483ad423421bd16e7a7ae7/66b9cffc5d14b6e5ca213d93_63c91e55e5a6b9697ea829e3_Spatial%2520Indexes%2520101%2520-%2520Social%2520Asset%2520-%2520Twitter_Animated.gif" loading="lazy" alt=""></p></a></figure><p>Working with spatial data can be difficult &nbsp;but the availability of data and tools has made it more accessible. By using the correct spatial scale and discarding analysis at the zip code level &nbsp;you can improve the quality of your insights and create more meaningful outcomes and analysis.</p><p> Want to see how it might work with your data? </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radiation belts detected around Earth after solar storm (139 pts)]]></title>
            <link>https://www.sciencealert.com/mysterious-radiation-belts-detected-around-earth-after-epic-solar-storm</link>
            <guid>42974727</guid>
            <pubDate>Fri, 07 Feb 2025 16:46:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencealert.com/mysterious-radiation-belts-detected-around-earth-after-epic-solar-storm">https://www.sciencealert.com/mysterious-radiation-belts-detected-around-earth-after-epic-solar-storm</a>, See on <a href="https://news.ycombinator.com/item?id=42974727">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>In May 2024, an epic  <a href="https://www.sciencealert.com/geomagnetic-storms" data-linkid="73062" data-postid="151552" rel="nofollow" target="_self">solar storm</a> rattled Earth so powerfully that its effects were felt <a href="https://www.sciencealert.com/the-solar-storm-was-so-intense-we-felt-it-even-at-the-bottom-of-the-ocean">even at the bottom of the ocean</a>.</p><p>In the wake of a <a href="https://www.sciencealert.com/powerful-flares-just-erupted-on-the-sun-and-their-effects-are-lashing-earth">torrent of flare activity</a> on the Sun, our planet was buffeted by a powerful blast of solar particles that shook our magnetic field, and bathed our skies with a <a href="https://www.sciencealert.com/biggest-solar-storm-in-decades-triggers-intense-auroras-the-science-explained">panoply of shimmering colors</a> as auroras reached far lower latitudes than usual.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>But its effects were way more far-reaching, as scientists now reveal. In the months following the storm, Earth was girded by two new, temporary radiation belts of high-energy particles, trapped by the planet's magnetic field.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>While we've seen this phenomenon before – following powerful  <a href="https://www.sciencealert.com/geomagnetic-storms" data-linkid="73062" data-postid="151552" rel="nofollow" target="_self">geomagnetic storms</a> – the solar storm of May 2024 delivered something we had never detected: energetic protons in one of the new belts.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>"When we compared the data from before and after the storm, I said, 'Wow, this is something really new'," <a href="https://science.nasa.gov/science-research/heliophysics/nasa-cubesat-finds-new-radiation-belts-after-may-2024-solar-storm/">says physicist Xinlin Li</a> of the University of Colorado Boulder. "This is really stunning."</p><figure id="attachment_151554" aria-describedby="caption-attachment-151554"><img decoding="async" src="https://www.sciencealert.com/images/2025/02/aurora-vermont.jpg" alt="An Epic Solar Storm Rattled Earth's Cage. It Left Behind New Radiation Belts" width="642" height="427" srcset="https://www.sciencealert.com/images/2025/02/aurora-vermont.jpg 642w, https://www.sciencealert.com/images/2025/02/aurora-vermont-624x415.jpg 624w, https://www.sciencealert.com/images/2025/02/aurora-vermont-600x399.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-151554">The aurora borealis as seen over Vermont in May 2024. (<a href="https://flickr.com/photos/nickerwin/53718023941/in/photostream/">Nicholas Erwin/Flickr</a>, CC BY-NC-ND 2.0)</figcaption></figure><p>Radiation belts are a normal part of the architecture of a planet with a global magnetic field. Stars are constantly leaking particles, borne by a stellar wind; these stream out and, where they encounter planetary magnetic fields, become entrapped, forming vast belts in toroidal formation around the planet in the center.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>Earth has two permanent radiation belts known as the Van Allen belts, an inner belt closer to the planet, and an outer belt encircling both. This is a good thing; it's sort of like a planetary defense system that <a href="https://www.jpl.nasa.gov/nmp/st5/SCIENCE/magnetosphere.html">protects our planet from direct bombardment by solar particles</a>, and allows us to live here relatively unscathed.</p><figure id="attachment_151625" aria-describedby="caption-attachment-151625"><img decoding="async" src="https://www.sciencealert.com/images/2025/02/Van_Allen_radiation_belts.jpg" alt="Scientists Detect New Radiation Belts Around Earth After Epic Solar Storm" width="642" height="354" srcset="https://www.sciencealert.com/images/2025/02/Van_Allen_radiation_belts.jpg 642w, https://www.sciencealert.com/images/2025/02/Van_Allen_radiation_belts-600x331.jpg 600w" sizes="(max-width: 642px) 100vw, 642px" loading="lazy"><figcaption id="caption-attachment-151625">A diagram showing the arrangement of Van Allen radiation belts. (<a href="https://commons.wikimedia.org/wiki/File:Van_Allen_radiation_belt.svg">Booyabazooka/Wikimedia Commons/Public Domain</a>)</figcaption></figure><p>Since these belts are maintained and replenished by solar particles, it's unsurprising that an increase in solar particle output, as generated by the flares and coronal mass ejections of a solar storm, would augment what's already there.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>Even so, when the scientists investigated the effects of the solar storm of May 2024 based on data collected by the NASA's <a href="https://lasp.colorado.edu/cirbe/">Colorado Inner Radiation Belt Experiment CubeSat</a>, what they saw surprised them.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>There, sandwiched between the two Van Allen radiation belts, they found two new belts – one predominantly comprising electrons, as we've seen previously, and the other containing energetic protons, which has never been seen before.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>"These are really high-energy electrons and protons that have found their way into Earth's inner magnetic environment," <a href="https://science.nasa.gov/science-research/heliophysics/nasa-cubesat-finds-new-radiation-belts-after-may-2024-solar-storm/">says astronomer David Sibeck</a> of NASA's Goddard Space Flight Center, who was not involved with the research. "Some might stay in this place for a very long time."</p><p><iframe src="https://www.youtube.com/embed/CKUNT2Qshk4?feature=oembed" frameborder="0" allowfullscreen=""> frameborder="0″ allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p><p>In fact, the belts remained intact for much longer than previous temporary radiation belts generated by  <a href="https://www.sciencealert.com/geomagnetic-storms" data-linkid="73062" data-postid="151552" rel="nofollow" target="_self">solar storms</a>: three months, compared to the weeks we'd normally expect.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>Subsequent solar storms in June and August of 2024 knocked most of the particles out of orbit, significantly diminishing the density of the belts. A small amount, however, still remains up there, hanging out with Earth.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>What's more, the proton belt may remain intact for over a year. Ongoing measurements will help scientists measure its longevity and decay rate.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>This is important information to have: particles in Earth orbit can pose a hazard to satellites hanging out up there, so knowing the particle density and the effects solar storms can have thereon can help engineers design mitigation strategies to protect our technology.</p><!-- START single/mrec -->
<!-- END single/mrec --><p>At the moment, though, the hazard posed by the new radiation belts is unquantified. Future studies will be needed to determine the risks these, and future belts, might pose.</p><p>The research has been published in the <a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2024JA033504"><i>Journal of Geophysical Research: Space Physics</i></a>.</p>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A 16TB Mirror of Data.gov on Source.Coop (133 pts)]]></title>
            <link>https://source.coop/repositories/harvard-lil/gov-data/description</link>
            <guid>42974533</guid>
            <pubDate>Fri, 07 Feb 2025 16:29:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://source.coop/repositories/harvard-lil/gov-data/description">https://source.coop/repositories/harvard-lil/gov-data/description</a>, See on <a href="https://news.ycombinator.com/item?id=42974533">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>NOTE: This service is under active development. Certain features may not work or become unavailable at any time.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Three-nanite: Unreal Nanite in Three.js (212 pts)]]></title>
            <link>https://github.com/AIFanatic/three-nanite</link>
            <guid>42974461</guid>
            <pubDate>Fri, 07 Feb 2025 16:23:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/AIFanatic/three-nanite">https://github.com/AIFanatic/three-nanite</a>, See on <a href="https://news.ycombinator.com/item?id=42974461">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">THREE-Nanite</h2><a id="user-content-three-nanite" aria-label="Permalink: THREE-Nanite" href="#three-nanite"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Note: The code is a mess at the moment.</h3><a id="user-content-note-the-code-is-a-mess-at-the-moment" aria-label="Permalink: Note: The code is a mess at the moment." href="#note-the-code-is-a-mess-at-the-moment"></a></p>
<p dir="auto"><a href="https://aifanatic.github.io/three-nanite/dist/index.html" rel="nofollow">Live demo</a></p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AIFanatic/three-nanite/blob/master/screenshots/showcase.png"><img src="https://github.com/AIFanatic/three-nanite/raw/master/screenshots/showcase.png"></a>
</p>
<p dir="auto">Photo description:</p>
<ul dir="auto">
<li>1: Original object.</li>
<li>2: Meshletized/Clustered: Using <a href="https://github.com/KarypisLab/METIS">METIS</a>.</li>
<li>3: Grouped meshlets: Partitions adjacent meshlets into groups using <a href="https://github.com/KarypisLab/METIS">METIS</a>.</li>
<li>4: Merged grouped meshlets: Merges meshlets using <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a>.</li>
<li>5: Simplification: Simplify the mesh using <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> (Garland 1997).</li>
<li>6: Split: Similar to step 2.</li>
</ul>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AIFanatic/three-nanite/blob/master/screenshots/showcase2.png"><img src="https://github.com/AIFanatic/three-nanite/raw/master/screenshots/showcase2.png"></a>
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AIFanatic/three-nanite/blob/master/screenshots/showcase3.png"><img src="https://github.com/AIFanatic/three-nanite/raw/master/screenshots/showcase3.png"></a>
</p>
<p dir="auto">Photo description:
Meshlets shown are the red ones, as long as the edges of the DAG don't cross there should be no cracks and a dynamic LOD is achieved (1414628 and 8154144 are simplified versions of their children).</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AIFanatic/three-nanite/blob/master/screenshots/showcase4.gif"><img src="https://github.com/AIFanatic/three-nanite/raw/master/screenshots/showcase4.gif" data-animated-image=""></a>
</p>
<p dir="auto">
<a target="_blank" rel="noopener noreferrer" href="https://github.com/AIFanatic/three-nanite/blob/master/screenshots/showcase5.gif"><img src="https://github.com/AIFanatic/three-nanite/raw/master/screenshots/showcase5.gif" data-animated-image=""></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Description</h2><a id="user-content-description" aria-label="Permalink: Description" href="#description"></a></p>
<p dir="auto">An attempt at reproducing a dynamic LOD in threejs similarly to unreal's nanite.
Very far from it but nonetheless a start.
<br>
For now it clusters a mesh (meshlets), then groups adjacent clusters into a group, merges the mesh (shared vertices), performs mesh simplification to half the triangles in the mesh (max 128) and finally it splits it into 2 (should be N/2).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODO:</h2><a id="user-content-todo" aria-label="Permalink: TODO:" href="#todo"></a></p>
<ul dir="auto">
<li>More testing on LODS and DAG cut</li>
<li>Stream geometry to GPU</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">References</h2><a id="user-content-references" aria-label="Permalink: References" href="#references"></a></p>
<p dir="auto"><a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf" rel="nofollow">Nanite - A Deep Dive</a>
<br>
<a href="https://www.medien.ifi.lmu.de/lehre/ws2122/gp/slides/gp-ws2122-extra-nanite.pdf" rel="nofollow">The Nanite System in Unreal Engine 5</a>
<br>
<a href="https://vcg.isti.cnr.it/~ponchio/download/ponchio_phd.pdf" rel="nofollow">Multiresolution structures
for interactive visualization
of very large 3D datasets</a>
<br>
<a href="https://vcg.isti.cnr.it/Publications/2005/CGGMPS05/Slide_BatchedMT_Vis05.pdf" rel="nofollow">Batched Multi Triangulations</a>
<br>
<a href="https://cs418.cs.illinois.edu/website/text/nanite.html" rel="nofollow">CS 418 – Streaming, Level of Detail, and Occlusion
</a>
<br>
<a href="https://cdrdv2-public.intel.com/782066/real-time-ray-tracing-micro-poly-geometry.pdf" rel="nofollow">Real-Time Ray Tracing of Micro-Poly Geometry</a>
<br>
<a href="https://jglrxavpok.github.io/" rel="nofollow">CARROT - JGLRXAVPOK'S BLOG</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Transductive regular expressions for text editing (251 pts)]]></title>
            <link>https://github.com/c0stya/trre</link>
            <guid>42974378</guid>
            <pubDate>Fri, 07 Feb 2025 16:18:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/c0stya/trre">https://github.com/c0stya/trre</a>, See on <a href="https://news.ycombinator.com/item?id=42974378">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">TRRE: transductive regular expressions</h2><a id="user-content-trre-transductive-regular-expressions" aria-label="Permalink: TRRE: transductive regular expressions" href="#trre-transductive-regular-expressions"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">TLDR: It is an extension of the regular expressions for text editing and a <code>grep</code>-like command line tool.</h4><a id="user-content-tldr-it-is-an-extension-of-the-regular-expressions-for-text-editing-and-a-grep-like-command-line-tool" aria-label="Permalink: TLDR: It is an extension of the regular expressions for text editing and a grep-like command line tool." href="#tldr-it-is-an-extension-of-the-regular-expressions-for-text-editing-and-a-grep-like-command-line-tool"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">It is a PROTOTYPE. Do not use in production.</h4><a id="user-content-it-is-a-prototype-do-not-use-in-production" aria-label="Permalink: It is a PROTOTYPE. Do not use in production." href="#it-is-a-prototype-do-not-use-in-production"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro</h2><a id="user-content-intro" aria-label="Permalink: Intro" href="#intro"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/c0stya/trre/blob/main/docs/automata.png"><img src="https://github.com/c0stya/trre/raw/main/docs/automata.png" alt="automata"></a></p>
<p dir="auto">Regular expressions is a great tool for searching patterns in text. But I always found it unnatural for text editing. The <em>group</em> logic works as a post-processor and can be complicated. Here I propose an extension to the regular expression language for pattern matching and text modification. I call it <strong>transductive regular expressions</strong> or simply <strong><code>trre</code></strong>.</p>
<p dir="auto">It  introduces the <code>:</code> symbol to define transformations. The simplest form is <code>a:b</code>, which replaces a with b. I call this a <code>transductive pair</code> or <code>transduction</code>.</p>
<p dir="auto">To demonstrate the concept, I have created a command line tool <strong><code>trre</code></strong>. It feels similar to the <code>grep -E</code> command.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basics</h3><a id="user-content-basics" aria-label="Permalink: Basics" href="#basics"></a></p>
<p dir="auto">To change <code>cat</code> to <code>dog</code> we use the following expression:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'cat' | trre 'c:da:ot:g'

dog"><pre>$ <span>echo</span> <span><span>'</span>cat<span>'</span></span> <span>|</span> trre <span><span>'</span>c:da:ot:g<span>'</span></span>

dog</pre></div>
<p dir="auto">A more readable version:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'cat' | trre '(cat):(dog)'

dog"><pre>$ <span>echo</span> <span><span>'</span>cat<span>'</span></span> <span>|</span> trre <span><span>'</span>(cat):(dog)<span>'</span></span>

dog</pre></div>
<p dir="auto">It can be used like <code>sed</code> to replace all matches in a string:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'Mary had a little lamb.' | trre '(lamb):(cat)'

Mary had a little cat."><pre>$ <span>echo</span> <span><span>'</span>Mary had a little lamb.<span>'</span></span> <span>|</span> trre <span><span>'</span>(lamb):(cat)<span>'</span></span>

Mary had a little cat.</pre></div>
<p dir="auto"><strong>Deletion:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'xor' | trre '(x:)or'

or"><pre>$ <span>echo</span> <span><span>'</span>xor<span>'</span></span> <span>|</span> trre <span><span>'</span>(x:)or<span>'</span></span>

or</pre></div>
<p dir="auto">The expression <code>(x:)</code> could be interpreted as of translation of <code>x</code> to an empty string.</p>
<p dir="auto"><strong>Insertion:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'or' | trre '(:x)or'

xor"><pre>$ <span>echo</span> <span><span>'</span>or<span>'</span></span> <span>|</span> trre <span><span>'</span>(:x)or<span>'</span></span>

xor</pre></div>
<p dir="auto">We could think of the expression <code>(:x)</code> as of translation of an empty string into <code>x</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Regex over transductions</h3><a id="user-content-regex-over-transductions" aria-label="Permalink: Regex over transductions" href="#regex-over-transductions"></a></p>
<p dir="auto">As for normal regular expression we could use <strong>alternations</strong> with <code>|</code> symbol:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'cat dog' | trre 'c:bat|d:hog'

bat hog"><pre>$ <span>echo</span> <span><span>'</span>cat dog<span>'</span></span> <span>|</span> trre <span><span>'</span>c:bat|d:hog<span>'</span></span>

bat hog</pre></div>
<p dir="auto">Or use the <strong>star</strong> over <code>trre</code> to repeat the transformation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'catcatcat' | trre '((cat):(dog))*'

dogdogdog"><pre>$ <span>echo</span> <span><span>'</span>catcatcat<span>'</span></span> <span>|</span> trre <span><span>'</span>((cat):(dog))*<span>'</span></span>

dogdogdog</pre></div>
<p dir="auto">In the default <code>scan</code> mode, <strong>star</strong> can be omitted:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'catcatcat' | trre '(cat):(dog)'

dogdogdog"><pre>$ <span>echo</span> <span><span>'</span>catcatcat<span>'</span></span> <span>|</span> trre <span><span>'</span>(cat):(dog)<span>'</span></span>

dogdogdog</pre></div>
<p dir="auto">You can also use the star in the left part to "consume" a pattern infinitely:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'catcatcat' | trre '(cat)*:(dog)'

dog"><pre>$ <span>echo</span> <span><span>'</span>catcatcat<span>'</span></span> <span>|</span> trre <span><span>'</span>(cat)*:(dog)<span>'</span></span>

dog</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Danger zone</h4><a id="user-content-danger-zone" aria-label="Permalink: Danger zone" href="#danger-zone"></a></p>
<p dir="auto">Avoid using <code>*</code> or <code>+</code> in the right part, as it can cause infinite loops:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo '' | trre ':a*'      # <- do NOT do this

..."><pre>$ <span>echo</span> <span><span>'</span><span>'</span></span> <span>|</span> trre <span><span>'</span>:a*<span>'</span></span>      <span><span>#</span> &lt;- do NOT do this</span>

...</pre></div>
<p dir="auto">Instead, use finite iterations:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo '' | trre ':(repeat-10-times){10}'

repeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-times"><pre>$ <span>echo</span> <span><span>'</span><span>'</span></span> <span>|</span> trre <span><span>'</span>:(repeat-10-times){10}<span>'</span></span>

repeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-timesrepeat-10-times</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Range transformations</h3><a id="user-content-range-transformations" aria-label="Permalink: Range transformations" href="#range-transformations"></a></p>
<p dir="auto">Transform ranges of characters:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo &quot;regular expressions&quot; | trre  &quot;[a:A-z:Z]&quot;

REGULAR EXPRESSIONS"><pre>$ <span>echo</span> <span><span>"</span>regular expressions<span>"</span></span> <span>|</span> trre  <span><span>"</span>[a:A-z:Z]<span>"</span></span>

REGULAR EXPRESSIONS</pre></div>
<p dir="auto">As more complex example, lets create a toy cipher. Below is the Caesar cipher(1) implementation:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'caesar cipher' | trre '[a:b-y:zz:a]'

dbftbs djqifs"><pre>$ <span>echo</span> <span><span>'</span>caesar cipher<span>'</span></span> <span>|</span> trre <span><span>'</span>[a:b-y:zz:a]<span>'</span></span>

dbftbs djqifs</pre></div>
<p dir="auto">And decrypt it back:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo 'dbftbs djqifs' | trre '[a:zb:a-y:x]'

caesar cipher"><pre>$ <span>echo</span> <span><span>'</span>dbftbs djqifs<span>'</span></span> <span>|</span> trre <span><span>'</span>[a:zb:a-y:x]<span>'</span></span>

caesar cipher</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generators</h3><a id="user-content-generators" aria-label="Permalink: Generators" href="#generators"></a></p>
<p dir="auto"><strong><code>trre</code></strong> can generate multiple output strings for a single input. By default, it uses the first possible match. You can also generate all possible outputs.</p>
<p dir="auto"><strong>Binary sequences:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo '' | trre -g '(0|1){3}'

000
001
010
011
100
101
110
111"><pre>$ <span>echo</span> <span><span>'</span><span>'</span></span> <span>|</span> trre -g <span><span>'</span>(0|1){3}<span>'</span></span>

000
001
010
011
100
101
110
111</pre></div>
<p dir="auto"><strong>Subsets:</strong></p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo '' | trre -g ':(0|1){,3}?'


0
00
000
001
01
010
011
1
10
100
101
11
110
111"><pre>$ <span>echo</span> <span><span>'</span><span>'</span></span> <span>|</span> trre -g <span><span>'</span>:(0|1){,3}?<span>'</span></span>


0
00
000
001
01
010
011
1
10
100
101
11
110
111</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Language specification</h2><a id="user-content-language-specification" aria-label="Permalink: Language specification" href="#language-specification"></a></p>
<p dir="auto">Informally, we define a <strong><code>trre</code></strong> as a pair <code>pattern-to-match</code>:<code>pattern-to-generate</code>. The <code>pattern-to-match</code> can be a string or regexp. The <code>pattern-to-generate</code> normally is a string. But it can be a <code>regex</code> as well. Moreover, we can do normal regular expression over these pairs.</p>
<p dir="auto">We can specify a smiplified grammar of <strong><code>trre</code></strong> as following:</p>
<div data-snippet-clipboard-copy-content="TRRE    <- TRRE* TRRE|TRRE TRRE.TRRE
TRRE    <- REGEX REGEX:REGEX"><pre><code>TRRE    &lt;- TRRE* TRRE|TRRE TRRE.TRRE
TRRE    &lt;- REGEX REGEX:REGEX
</code></pre></div>
<p dir="auto">Where <code>REGEX</code> is a usual regular expression, and <code>.</code> stands for concatenation.</p>
<p dir="auto">For now I consider the operator <code>:</code> as non-associative and the expression <code>TRRE:TRRE</code> as unsyntactical. There is a natural meaning for this expression as a composition of relations defined by **<code>trre</code>**s. But it can make things too complex. Maybe I change this later.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why it works</h2><a id="user-content-why-it-works" aria-label="Permalink: Why it works" href="#why-it-works"></a></p>
<p dir="auto">Under the hood, <strong><code>trre</code></strong> constructs a special automaton called a <strong>Finite State Transducer (FST)</strong>, which is similar to a <strong>Finite State Automaton (FSA)</strong> used in normal regular expressions but handles input-output pairs instead of plain strings.</p>
<p dir="auto">Key differences:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong><code>trre</code></strong> defines a binary relation between two regular languages.</p>
</li>
<li>
<p dir="auto">It uses <strong>FST</strong>s instead of <strong>FSA</strong>s for inference.</p>
</li>
<li>
<p dir="auto">It supports on-the-fly determinization for performance (experimental and under construction!).</p>
</li>
</ul>
<p dir="auto">To justify the laguage of trunsductive regular expression we need to prove the correspondence between <strong><code>trre</code></strong> expressions and the corresponding <strong>FST</strong>s. There is my sketch of a the proof: <a href="https://github.com/c0stya/trre/blob/main/doc/theory.pdf">theory.pdf</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Design choices and open questions</h2><a id="user-content-design-choices-and-open-questions" aria-label="Permalink: Design choices and open questions" href="#design-choices-and-open-questions"></a></p>
<p dir="auto">There are tons of decisions:</p>
<ul dir="auto">
<li>
<p dir="auto">Associativity of <code>:</code>. The <code>:</code> symbol is non-associative, meaning a:b:c is invalid in the current version. There is a natural meaning of transducer composition but it could make things too complicated. Alternative syntaxes (e.g., &gt; and &lt;) could be explored.</p>
</li>
<li>
<p dir="auto">Precedence of <code>:</code>. The priority of : in expressions needs clarification.</p>
</li>
<li>
<p dir="auto">Implicit Epsilon: should there be an explicit symbol?</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Modes and greediness</h2><a id="user-content-modes-and-greediness" aria-label="Permalink: Modes and greediness" href="#modes-and-greediness"></a></p>
<p dir="auto"><strong><code>trre</code></strong> supports two modes:</p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Scan Mode (default)</strong>: Applies transformations sequentially.</p>
</li>
<li>
<p dir="auto"><strong>Match Mode</strong>: Checks the entire string against the expression (use <code>-m</code> flag).</p>
</li>
</ul>
<p dir="auto">Use <code>-a</code> to generate all possible outputs.</p>
<p dir="auto">The <code>? modifier makes </code>*<code>, </code>+<code>, and </code>{,}` operators non-greedy:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ echo '<cat><dog>' | trre '<(.:)*>'

<>

$ echo '<cat><dog>' | trre '<(.:)*?>'

<><>"><pre>$ <span>echo</span> <span><span>'</span>&lt;cat&gt;&lt;dog&gt;<span>'</span></span> <span>|</span> trre <span><span>'</span>&lt;(.:)*&gt;<span>'</span></span>

<span>&lt;&gt;</span>

$ <span>echo</span> <span><span>'</span>&lt;cat&gt;&lt;dog&gt;<span>'</span></span> <span>|</span> trre <span><span>'</span>&lt;(.:)*?&gt;<span>'</span></span>

&lt;&gt;&lt;&gt;</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Determinization</h2><a id="user-content-determinization" aria-label="Permalink: Determinization" href="#determinization"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/c0stya/trre/blob/main/docs/determinization.png"><img src="https://github.com/c0stya/trre/raw/main/docs/determinization.png" width="80%"></a></p>
<p dir="auto">The important part of the modern regex engines is determinization. This routine converts the non-deterministic automata to the deterministic one. Once converted it has linear time inference on the input string length. It is handy but the convertion is exponential in the worst case.</p>
<p dir="auto">For <strong><code>trre</code></strong> the similar approach is possible. The bad news is that not all the non-deterministic transducers (<strong>NFT</strong>) can be converted to a deterministic (<strong>DFT</strong>). In case of two "bad" cycles with same input labels the algorithm is trapped in the infinite loop of a state creation. There is a way to detect such loops but it is expensive (see more in <a href="https://cs.nyu.edu/~mohri/pub/twins.pdf" rel="nofollow">Allauzen, Mohri, Efficient Algorithms for testing the twins property</a>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">The NFT (non-deterministic) version is a bit slower then <code>sed</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ wget https://www.gutenberg.org/cache/epub/57333/pg57333.txt -O chekhov.txt

$ time cat chekhov.txt | trre '(vodka):(VODKA)' > /dev/null

real	0m0.046s
user	0m0.043s
sys	0m0.007s

$ time cat chekhov.txt | sed  's/vodka/VODKA/' > /dev/null

real	0m0.024s
user	0m0.020s
sys	0m0.010s
"><pre>$ wget https://www.gutenberg.org/cache/epub/57333/pg57333.txt -O chekhov.txt

$ <span>time</span> cat chekhov.txt <span>|</span> trre <span><span>'</span>(vodka):(VODKA)<span>'</span></span> <span>&gt;</span> /dev/null

real	0m0.046s
user	0m0.043s
sys	0m0.007s

$ <span>time</span> cat chekhov.txt <span>|</span> sed  <span><span>'</span>s/vodka/VODKA/<span>'</span></span> <span>&gt;</span> /dev/null

real	0m0.024s
user	0m0.020s
sys	0m0.010s
</pre></div>
<p dir="auto">For complex tasks, <strong><code>trre_dft</code></strong> (deterministic version) can outperform sed:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ time cat chekhov.txt | sed -e 's/\(.*\)/\U\1/' > /dev/null

real	0m0.508s
user	0m0.504s
sys	0m0.015s

$ time cat chekhov.txt | trre_dft '[a:A-z:Z]' > /dev/null

real	0m0.131s
user	0m0.127s
sys	0m0.009s"><pre>$ <span>time</span> cat chekhov.txt <span>|</span> sed -e <span><span>'</span>s/\(.*\)/\U\1/<span>'</span></span> <span>&gt;</span> /dev/null

real	0m0.508s
user	0m0.504s
sys	0m0.015s

$ <span>time</span> cat chekhov.txt <span>|</span> trre_dft <span><span>'</span>[a:A-z:Z]<span>'</span></span> <span>&gt;</span> /dev/null

real	0m0.131s
user	0m0.127s
sys	0m0.009s</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">No pre-built binaries are available yet. Clone the repository and compile:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:c0stya/trre.git trre
cd trre
make &amp;&amp; sh test.sh"><pre>git clone git@github.com:c0stya/trre.git trre
<span>cd</span> trre
make <span>&amp;&amp;</span> sh test.sh</pre></div>
<p dir="auto">Then move the binary to a directory in your <code>$PATH</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">TODO</h2><a id="user-content-todo" aria-label="Permalink: TODO" href="#todo"></a></p>
<ul dir="auto">
<li>Complete the ERE feature set:
<ul dir="auto">
<li>negation <code>^</code> within <code>[]</code></li>
<li>character classes</li>
<li>'$^' anchoring symbols</li>
</ul>
</li>
<li>Full unicode support</li>
<li>Efficient range processing</li>
<li>Stable DFT version</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">References</h2><a id="user-content-references" aria-label="Permalink: References" href="#references"></a></p>
<ul dir="auto">
<li>The approach is strongly inspired by Russ Cox article: <a href="https://swtch.com/~rsc/regexp/regexp1.html" rel="nofollow">Regular Expression Matching Can Be Simple And Fast</a></li>
<li>The idea of transducer determinization comes from this paper: <a href="https://research.google/pubs/finitely-subsequential-transducers-2/" rel="nofollow">Cyril Allauzen, Mehryar Mohri, "Finitely Subsequential Transducers"</a></li>
<li>Parsing approach comes from <a href="https://github.com/erikeidt/erikeidt.github.io/blob/master/The-Double-E-Method.md">Double-E algorithm</a> by Erik Eidt which is close to the classical <a href="https://en.wikipedia.org/wiki/Shunting_yard_algorithm" rel="nofollow">Shunting Yard algorithm</a>.</li>
</ul>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>