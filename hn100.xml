(ignoring known css parsing error)
<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 22 Aug 2025 22:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[U.S. government takes 10% stake in Intel (184 pts)]]></title>
            <link>https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html</link>
            <guid>44989773</guid>
            <pubDate>Fri, 22 Aug 2025 21:01:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html">https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html</a>, See on <a href="https://news.ycombinator.com/item?id=44989773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108186237" data-test="InlineImage"><p>Lip-Bu Tan, chief executive officer of Intel Corp., departs following a meeting at the White House in Washington, DC, US, on Monday, Aug. 11, 2025. </p><p>Alex Wroblewski | Bloomberg | Getty Images</p></div><div><p>Commerce Secretary Howard Lutnick said on Friday that the U.S. government has taken a 10% stake in embattle chipmaker Intel, the Trump administration's latest effort to exert control over corporate America. </p><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/INTC/">Intel</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> shares rose about 6% during trading on Friday. They were flat in extended trading.</p><p>Intel, the only American company capable of making advanced chips on U.S. soil, said in a press release that the government made an $8.9 billion investment in Intel common stock, purchasing 433.3 million shares at a price of $20.47 per share, giving it a 10% stake in the company. Intel noted that the price the government paid was a discount to the current market price.</p><p>Of the total, $5.7 billion of the government funds will come from grants under the CHIPS Act that had been awarded but not paid, and $3.2 billion will come from separate government awards under a program to make secure chips.</p><p>"The United States paid nothing for these Shares, and the Shares are now valued at approximately $11 Billion Dollars," President Trump wrote in a post on Truth Social. "This is a great Deal for America and, also, a great Deal for INTEL."&nbsp;</p><p>The government will also have a warrant to buy an additional 5% of Intel shares if the company is no longer majority owner of its foundry business.</p><p>Intel said that the U.S. government won't have a board seat or other governance rights.</p><p>"As the only semiconductor company that does leading-edge logic R&amp;D and manufacturing in the U.S., Intel is deeply committed to ensuring the world's most advanced technologies are American made," Intel CEO Lip-Bu Tan said in the press release. </p></div><h2><a id="headline0"></a>'A great deal for them'</h2><div><p>Earlier on Friday, <a href="https://www.cnbc.com/donald-trump/">President Donald Trump</a> said the government should get about 10% of the company, which has a market cap of just over $100 billion. &nbsp;</p><p>"They've agreed to do it and I think it's a great deal for them," Trump told reporters Friday at the White House</p><p>White House officials previously told CNBC that Trump and Tan will meet on Friday afternoon. Lutnick's post included a photo with Tan.</p></div><div id="RegularArticle-RelatedContent-1"><h2>Read more CNBC tech news</h2><div><ul><li><a href="https://www.cnbc.com/2025/08/22/apple-will-make-chips-at-texas-instruments-60-billion-us-project.html">Inside Texas Instruments' $60 billion U.S. megaproject, where Apple will make iPhone chips</a></li><li><a href="https://www.cnbc.com/2025/08/22/nvidia-in-talks-with-us-to-sell-more-advanced-chip-to-china-huang.html">Nvidia in talks with U.S. to sell a more advanced chip to China, Jensen Huang says</a></li><li><a href="https://www.cnbc.com/2025/08/22/deepseek-hints-latest-model-supported-by-chinas-next-generation-homegrown-ai-chips.html">DeepSeek hints latest model will be compatible with China's 'next generation' homegrown AI chips</a></li><li><a href="https://www.cnbc.com/2025/08/21/elon-musk-asked-meta-ceo-mark-zuckerberg-to-join-xai-bid-to-buy-openai.html">Elon Musk asked Meta CEO Mark Zuckerberg to join xAI bid to buy OpenAI, filing shows</a></li></ul></div></div><div><p>The marks the latest example of a distinct shift in U.S. industrial policy, with the government taking an active role in the private sector. Lutnick <a href="https://www.cnbc.com/2025/08/19/lutnick-intel-stock-chips-trump.html">told CNBC this week</a> that the U.S. government was seeking an equity stake in Intel in exchange for CHIPS Act funds.</p><p>"We should get an equity stake for our money," Lutnick said on CNBC's "<a href="https://www.cnbc.com/squawk-on-the-street/">Squawk on the Street</a>." "So we'll deliver the money, which was already committed under the Biden administration. We'll get equity in return for it."</p><p>Earlier this week, Intel announced another major backer, when SoftBank said it would make a $2 billion investment in the chipmaker, equal to about 2% of the company.</p><p>Intel's technology is seen as lagging <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-5"><a href="https://www.cnbc.com/quotes/TSM/">Taiwan Semiconductor Manufacturing Company</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, which makes chips for companies including Apple, Nvidia, Qualcomm, AMD, and even Intel.</p><p>Intel has been spending billions of dollars to build a series of chip factories in Ohio, an area the company previously called the "Silicon Heartland," where Intel would be able to produce the most advanced chips, including for AI.</p><p>But in July, Tan said in a memo to employees that there would be "no more blank checks," and that it was slowing down the construction of its Ohio factory complex, depending on market conditions. Intel's Ohio factory is now scheduled to start operations in 2030.</p><p>Intel said last fall that it had <a href="https://www.cnbc.com/2024/11/25/intel-close-to-8-billion-chips-act-grant-source.html">finalized</a> a nearly $8 billion grant under the CHIPS and Science Act to fund its factory-building plans. The CHIPS Act was passed in 2022, under the Biden administration.</p><p><em>— CNBC's David Sucherman contributed to this report.</em></p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/08/22/president-trump-says-intel-ceo-agreed-to-pay-u-s-10-percent-of-the-company.html">President Trump says Intel should transfer 10% of company to government</a></p></div><div id="Placeholder-ArticleBody-Video-108189803" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000386565" aria-labelledby="Placeholder-ArticleBody-Video-108189803"><p><img src="https://image.cnbcfm.com/api/v1/image/108189804-17558856121755885610-41301124345-1080pnbcnews.jpg?v=1755885611&amp;w=750&amp;h=422&amp;vtcrop=y" alt="President Trump says Intel CEO agreed to pay U.S. 10% of the company"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists just found a protein that reverses brain aging (121 pts)]]></title>
            <link>https://www.sciencedaily.com/releases/2025/08/250820000808.htm</link>
            <guid>44988393</guid>
            <pubDate>Fri, 22 Aug 2025 18:56:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencedaily.com/releases/2025/08/250820000808.htm">https://www.sciencedaily.com/releases/2025/08/250820000808.htm</a>, See on <a href="https://news.ycombinator.com/item?id=44988393">Hacker News</a></p>
<div id="readability-page-1" class="page"><p id="first">Aging is particularly harsh on the hippocampus -- the brain region responsible for learning and memory.</p><div id="text">
<p>Now, researchers at UC San Francisco have identified a protein that's at the center of this decline.</p>
<p>They looked at how the genes and proteins in the hippocampus changed over time in mice and found just one that differed between old and young animals. It's called FTL1.</p>
<p>Old mice had more FTL1, as well as fewer connections between brain cells in the hippocampus and diminished cognitive abilities.</p>
<p>When the researchers artificially increased FTL1 levels in young mice, their brains and behavior began to resemble that of old mice.</p>
<p>In experiments in petri dishes, nerve cells engineered to make lots of FTL1 grew simple, one-armed neurites -- rather than the branching neurites that normal cells create.</p>
<p>But once the scientists reduced the amount of FTL1 in the hippocampus of the old mice, they regained their youth. They had more connections between nerve cells, and the mice did better on memory tests.</p>


<p>"It is truly a reversal of impairments," said Saul Villeda, PhD, associate director of the UCSF Bakar Aging Research Institute and senior author of the paper, which appears in <em>Nature Aging</em> on Aug. 19. "It's much more than merely delaying or preventing symptoms."</p>
<p>In old mice, FTL1 also slowed down metabolism in the cells of the hippocampus. But treating the cells with a compound that stimulates metabolism prevented these effects.</p>
<p>Villeda is optimistic the work could lead to therapies that block the effects of FTL1 in the brain.</p>
<p>"We're seeing more opportunities to alleviate the worst consequences of old age," he said. "It's a hopeful time to be working on the biology of aging."</p>
<p>Authors: Other UCSF authors are Laura Remesal, PhD, Juliana Sucharov-Costa, Karishma J.B. Pratt, PhD, Gregor Bieri, PhD, Amber Philp, PhD, Mason Phan, Turan Aghayev, MD, PhD, Charles W. White III, PhD, Elizabeth G. Wheatley, PhD, Brandon R. Desousa, Isha H. Jian, Jason C. Maynard, PhD, and Alma L. Burlingame, PhD. For all authors see the paper.</p>
<p>Funding: This work was funded in part by the Simons Foundation, Bakar Family Foundation, National Science Foundation, Hillblom Foundation, Bakar Aging Research Institute, Marc and Lynne Benioff, and the National Institutes of Health (AG081038, AG067740, AG062357, P30 DK063720). For all funding see the paper.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The First Media over QUIC CDN: Cloudflare (110 pts)]]></title>
            <link>https://moq.dev/blog/first-cdn/</link>
            <guid>44987924</guid>
            <pubDate>Fri, 22 Aug 2025 18:24:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://moq.dev/blog/first-cdn/">https://moq.dev/blog/first-cdn/</a>, See on <a href="https://news.ycombinator.com/item?id=44987924">Hacker News</a></p>
<div id="readability-page-1" class="page"><article> <p>
published 8/21/2025 </p> 
<p>🚨 It’s finally happening! 🚨</p>
<p>Cloudflare has <a href="https://blog.cloudflare.com/moq/">just announced</a> their Media over QUIC CDN!
It’s an <strong>official product</strong>, and you can test MoQ on their <em>massive</em>, anycast network.
Try it out, and convince your boss’ boss that the writing is on the wall.</p>
<p>If you’ve been living under a rock, MoQ is an <a href="https://datatracker.ietf.org/group/moq/about/">up-and-coming standard</a> for live media, aiming to supplant <a href="https://moq.dev/blog/replacing-webrtc">WebRTC</a>, <a href="https://moq.dev/blog/replacing-hls-dash">HLS/DASH</a>, and even <strong>RTMP/SRT</strong> as the one to rule them all.
And now Cloudflare wins the award for the first CDN offering!</p>
<figure><p><img src="https://moq.dev/blog/first-cdn/cloudflare.png" alt="Cloudflare logo">
</p><figcaption>Your prize is a blog post. You’re welcome mega-corp.</figcaption></figure>
<p>Also, <em>while you’re here</em>, some shameless self-promotion: I just soft-launched <a href="https://moq.dev/blog/first-app">hang.live</a>.
Check it out if you want to see the <del>cringe</del> cool stuff you can do with MoQ.</p>
<h2 id="whats-available-now">What’s available now?</h2>
<p>This is a <a href="https://developers.cloudflare.com/moq/">technical preview</a>, so it’s both free and subject to change.</p>
<p>Cloudflare is hosting a public <code>relay.cloudflare.mediaoverquic.com</code> endpoint that you can <del>abuse</del> test.
Connect using <a href="https://github.com/kixelated/moq">my library</a>, <a href="https://github.com/englishm/moq-rs">Mike’s fork</a>, <a href="https://www.meetecho.com/blog/imquic/">Lorenzo’s imquic</a>, <a href="https://github.com/facebookexperimental/moxygen">Meta’s moxygen</a>, or any client that supports this limited subset of draft-07.</p>
<p>I’m biased so naturally I’m going to use <a href="https://github.com/kixelated/moq/tree/main/js/hang">@kixelated/hang</a> (smash that star button).
You can publish a live broadcast in the browser using the <a href="https://moq.dev/publish">web demo</a> or the <a href="https://github.com/kixelated/moq/blob/main/js/hang-demo/src/publish.html#L25">library</a>:</p>
<pre tabindex="0" data-language="html"><code><span><span>&lt;</span><span>script</span><span> type</span><span>=</span><span>"module"</span><span>&gt;</span></span>
<span><span>	// Registers the &lt;hang-publish&gt; element.</span></span>
<span><span>	import</span><span> "@kixelated/hang/publish/element"</span><span>;</span></span>
<span><span>&lt;/</span><span>script</span><span>&gt;</span></span>
<span></span>
<span><span>&lt;!-- You'll need to replace `name` with something unique/random. --&gt;</span></span>
<span><span>&lt;</span><span>hang-publish</span><span> url</span><span>=</span><span>"https://relay.cloudflare.mediaoverquic.com"</span><span> name</span><span>=</span><span>"unique-name-abc123"</span><span> audio</span><span> video</span><span> controls</span><span> captions</span><span>&gt;</span></span>
<span><span>	&lt;!-- It's optional to provide a video element to preview the outgoing media. --&gt;</span></span>
<span><span>	&lt;</span><span>video</span><span> style</span><span>=</span><span>"max-width: 100%; height: auto; border-radius: 4px; margin: 0 auto;"</span><span> muted</span><span> autoplay</span><span>&gt;&lt;/</span><span>video</span><span>&gt;</span></span>
<span><span>&lt;/</span><span>hang-publish</span><span>&gt;</span></span></code></pre>
<p>There’s a link to watch your live broadcast using the <a href="https://moq.dev/watch">web demo</a>, or again you can use the <a href="https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/js/hang-demo/src/index.html#L30">library</a>:</p>
<pre tabindex="0" data-language="html"><code><span><span>&lt;</span><span>script</span><span> type</span><span>=</span><span>"module"</span><span>&gt;</span></span>
<span><span>	// Registers the &lt;hang-watch&gt; element.</span></span>
<span><span>	import</span><span> "@kixelated/hang/watch/element"</span><span>;</span></span>
<span><span>&lt;/</span><span>script</span><span>&gt;</span></span>
<span></span>
<span><span>&lt;!-- Use the same name as the broadcast you published. --&gt;</span></span>
<span><span>&lt;</span><span>hang-watch</span><span> url</span><span>=</span><span>"https://relay.cloudflare.mediaoverquic.com"</span><span> name</span><span>=</span><span>"unique-name-abc123"</span><span> muted</span><span> controls</span><span> captions</span><span>&gt;</span></span>
<span><span>	&lt;!-- It's optional to provide a canvas if you want audio only --&gt;</span></span>
<span><span>	&lt;</span><span>canvas</span><span> style</span><span>=</span><span>"max-width: 100%; height: auto; border-radius: 4px; margin: 0 auto;"</span><span>&gt;&lt;/</span><span>canvas</span><span>&gt;</span></span>
<span><span>&lt;/</span><span>hang-watch</span><span>&gt;</span></span></code></pre>
<p>You might even notice <strong>closed captions</strong> because I’ve been experimenting with AI features (gotta get funding eventually 💰).
They’re generated <em>in the browser</em> using <a href="https://github.com/snakers4/silero-vad">silero-vad</a> + <a href="https://github.com/openai/whisper">whisper</a> + <a href="https://huggingface.co/docs/transformers.js/en/index">transformers.js</a> + <a href="https://github.com/microsoft/onnxruntime">onnxruntime-web</a> + <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API">WebGPU</a> and transmitted using MoQ of course.
But that’s a whole separate blog post; it’s pretty cool.</p>
<p><strong>NOTE:</strong> You don’t have to use this <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_components">Web Component</a> API.
<a href="https://moq.dev/blog/first-app">hang.live</a> uses the far more powerful Javascript API to do more complicated stuff like get access to individual video frames.
There’s a <em>super secret</em> section at the end of this blog if you LOVE sample code, but I’m not going to bore the rest of you.</p>
<p>There’s also a 🦀 Rust 🦀 library <a href="https://github.com/kixelated/moq/tree/main/rs/hang">to import MP4</a>, <a href="https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/rs/justfile#L103">pipe media from ffmpeg</a>, and <a href="https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/rs/justfile#L119">publish/watch using gstreamer</a> so you can do more complicated media stuff without 🤮 Javascript 🤮.
I wish I could spend more time on the Rust side but <strong>WebSupport</strong> is a big deal.
We are no longer forced to use WebRTC, but that also means we need to build our own WebRTC in 🤮 Javascript 🤮.
I can suffer and you can reap the rewards.</p>
<p>(and yes, <a href="https://moq.dev/blog/to-wasm">I’m aware that WASM exists</a>, but I ended up abandoning it)</p>
<h2 id="whats-not-available-yet">What’s not available yet?</h2>
<p>This is a <strong>preview</strong> release.
Cloudflare is only supporting a <em>tiny</em> subset of an <a href="https://www.ietf.org/archive/id/draft-ietf-moq-transport-07.html">old draft</a>, which is even smaller than <a href="https://www.ietf.org/archive/id/draft-lcurley-moq-lite-01.html">my tiny subset</a>.
They’re using a <a href="https://github.com/englishm/moq-rs">fork</a> of my terrible code so bugs are guaranteed.</p>
<ul>
<li><strong>There’s no authentication yet</strong>: choose an unguessable name for each broadcast.</li>
<li><strong>There’s no ANNOUNCE support</strong>: my <a href="https://github.com/kixelated/moq/blob/main/js/hang-demo/src/meet.html">conferencing example</a> uses <strong>ANNOUNCE</strong> to discover when broadcasts start/stop, so that won’t work.</li>
<li><strong>There’s no <a href="https://caniuse.com/webtransport">Safari support</a></strong>: <a href="https://github.com/WebKit/standards-positions/issues/18#issuecomment-1495890122">It’s coming eventually</a>.</li>
<li><strong>Nothing has been optimized</strong>: the user experience will improve over time.</li>
</ul>
<p>If any of these are deal breakers, then you could always run your own <a href="https://github.com/kixelated/moq/tree/main/rs/moq-relay">moq-relay</a> in the meantime.
I’ve been adding new features and fixing a bunch of stuff <em>after</em> Cloudflare smashed that fork button.
For example, authentication (via JWT) and a WebSocket fallback for Safari/TCP support.</p>
<p>There’s even a <a href="https://github.com/kixelated/moq.dev/blob/main/infra/relay.tf">terraform module</a> that powers <code>relay.moq.dev</code>.
You too can run your own “global” CDN with 3 nodes and pay GCP a boatload of money for the privilege.
It’s not <em>quite</em> as good as Cloudflare’s network, currently available for free…</p>
<figure><p><img src="https://moq.dev/blog/first-cdn/global.png" alt="A global CDN">
</p><figcaption>A “global” CDN according to me, an American. At least I didn’t <a href="https://www.reddit.com/r/MapsWithoutNZ/">forget New Zealand</a>.</figcaption></figure>
<p>Or host <strong>moq-relay</strong> yourself!
It should even work on private networks provided you <a href="https://moq.dev/blog/tls-and-quic">wrestle with TLS certificates</a>.
I’d also love to get MoQ running over <a href="https://www.iroh.computer/">Iroh</a> for peer-to-peer action if anybody wants to help.</p>
<h2 id="why-should-you-care">Why should you care?</h2>
<p>As a great philosopher once said:</p>
<blockquote>
<p>Apathy is a tragedy and boredom is a crime.
- <a href="https://www.youtube.com/watch?v=k1BneeJTDcU">Bo Burnham</a></p>
</blockquote>
<p>This is a big deal.
The biggest of deals.
The HUGEST of deals.</p>
<p>I’ve been an <a href="https://moq.dev/blog/transfork">outspoken critic</a> of the MoQ standardization process.
It’s just really difficult to design a protocol, via a cross-company committee, before there’s been any real world usage.
It’s been over 3 years since I fought Amazon lawyers and published my <a href="https://www.ietf.org/archive/id/draft-lcurley-warp-00.html">first MoQ draft</a>.
It’s going to be at least another 3 years before even the <a href="https://datatracker.ietf.org/doc/draft-ietf-moq-transport/">base networking layer</a> becomes an RFC.</p>
<p><strong>And that’s by design!</strong>
The best standards take a while.
Look no further than QUIC, deployed by Google in 2012, started standardization in 2015, with the RFC released in 2021.
And they had a boatload of production data to shape the specification.
Meanwhile, we have only had a <a href="https://moq.dev/watch">Big Buck Bunny demo</a>, and I believe the standard has veered off course as a result.</p>
<p>Cloudflare has done something fantastic and said:</p>
<blockquote>
<p>fuck waiting for a RFC, let’s release something</p>
</blockquote>
<p>Okay they didn’t say that, but this is <strong>exactly</strong> the mentality that MoQ needs right now.
<strong>Just build something</strong>.
<strong>Just release something</strong>.
<strong>Just do it</strong>.</p>
<figure><iframe width="560" height="315" src="https://www.youtube.com/embed/ZXsQAXx_ao0?si=xsAscm04CnwAer4b" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe><figcaption>Holy shit I’m Shia LaBeouf.</figcaption></figure>
<p>Arguing in the <a href="https://github.com/moq-wg/moq-transport/issues">650+ issues</a> and <a href="https://github.com/moq-wg/moq-transport/pulls">500+ PRs</a> can wait for another day.
Tweaking the messaging encoding for the hundredth time can wait for another day.
We’re still going to make sure that MoQ gets standardized <em>eventually</em>, but it’s more important to get <em>something</em> out there.</p>
<p>I’m looking at you: Google, Akamai, Fastly, etc.
Take some code, run it on some spare servers, and start to learn what customers need <em>before</em> you design the protocol.</p>
<h2 id="whats-next">What’s next?</h2>
<p>A lot of stuff.</p>
<p>We’re effectively trying to reimplement WebRTC / HLS / RTMP using relatively new Web APIs.
Don’t judge MoQ based on these initial offerings.
We’ve got a <strong>ton</strong> of work to do.
<strong>Let’s do it</strong>.</p>
<p><a href="https://discord.gg/FCYF3p99mr">Join the Discord</a>.
Somehow there’s 900+ people in there.
Ping me and I will do whatever I can to help.
<em>Especially</em> if it means putting one more nail in the WebRTC coffin.</p>
<p>Written by <a href="https://github.com/kixelated">@kixelated</a>.
<img src="https://moq.dev/blog/avatar.png" alt="@kixelated"></p>
<h2 id="javascript-is-an-abomination">Javascript is an Abomination</h2>
<p>Still reading?</p>
<p>You win some bonus documentation.
Congrats!
I knew you would win.</p>
<p>Here’s an example of my reactive library in action.
It powers <a href="https://moq.dev/blog/first-app">hang.live</a> so the API is subject to change and is probably already out of date.
When in doubt, <a href="https://github.com/kixelated/moq/tree/main/js/hang">consult the source code</a> like the hacker you are.</p>
<pre tabindex="0" data-language="typescript"><code><span><span>import</span><span> { Watch } </span><span>from</span><span> "@kixelated/hang"</span></span>
<span></span>
<span><span>// Start downloading a broadcast.</span></span>
<span><span>const</span><span> watch</span><span> =</span><span> new</span><span> Watch.</span><span>Broadcast</span><span>({</span></span>
<span><span>	enabled: </span><span>true</span><span>,</span></span>
<span><span>	url: </span><span>"https://relay.cloudflare.mediaoverquic.com"</span><span>,</span></span>
<span><span>	name: </span><span>"unique-name-abc123"</span><span>,</span></span>
<span><span>	video: { enabled: </span><span>true</span><span> },</span></span>
<span><span>	reload: </span><span>false</span><span>, </span><span>// required for Cloudflare's CDN</span></span>
<span><span>});</span></span>
<span></span>
<span><span>// You can toggle reactive properties.</span></span>
<span><span>watch.audio.enabled.</span><span>set</span><span>(</span><span>true</span><span>);</span></span>
<span></span>
<span><span>// There are helpers to convert my custom signals, like for React:</span></span>
<span><span>import</span><span> react </span><span>from</span><span> "@kixelated/signals/react"</span></span>
<span><span>const</span><span> audioInfo</span><span> =</span><span> react</span><span>(watch.audio.info); </span><span>// a JSON blob of track information</span></span>
<span></span>
<span><span>// You could use the built-in renderers.</span></span>
<span><span>const</span><span> canvas</span><span> =</span><span> document.</span><span>getElementById</span><span>(</span><span>"canvas"</span><span>);</span></span>
<span><span>const</span><span> audio</span><span> =</span><span> new</span><span> Watch.</span><span>AudioEmitter</span><span>(watch.audio, { volume: </span><span>0.5</span><span> });</span></span>
<span><span>const</span><span> video</span><span> =</span><span> new</span><span> Watch.</span><span>VideoRenderer</span><span>(watch.video, { canvas });</span></span>
<span></span>
<span><span>// Or you can do it yourself, like this crude Vanilla JS example:</span></span>
<span><span>const</span><span> dispose</span><span> =</span><span> watch.video.frame.</span><span>subscribe</span><span>((</span><span>frame</span><span>?:</span><span> VideoFrame</span><span>) </span><span>=&gt;</span><span> {</span></span>
<span><span>	if</span><span> (</span><span>!</span><span>frame) </span><span>return</span><span>;</span></span>
<span></span>
<span><span>	// Render the frame to a canvas, or pass it to a ML model, or whatever.</span></span>
<span><span>	canvas.</span><span>getContext</span><span>(</span><span>"2d"</span><span>)?.</span><span>drawImage</span><span>(frame, </span><span>0</span><span>, </span><span>0</span><span>);</span></span>
<span></span>
<span><span>	// NOTE: You should use requestAnimationFrame instead, but I'm lazy.</span></span>
<span><span>});</span></span></code></pre>
<p>There’s even some <em>top-secret</em> features behind undocumented APIs.
Like running an object detection model in browser and publishing the results as a MoQ track.
Stay tuned for a blog post about that if I can figure out a better use-case than a cat cam. 🐈</p>
<pre tabindex="0" data-language="typescript"><code><span><span>// Publish a broadcast.</span></span>
<span><span>const</span><span> publish</span><span> =</span><span> new</span><span> Watch.</span><span>Publish</span><span>({</span></span>
<span><span>	enabled: </span><span>true</span><span>,</span></span>
<span><span>	url: </span><span>"https://relay.cloudflare.mediaoverquic.com"</span><span>,</span></span>
<span><span>	name: </span><span>"unique-name-abc123"</span><span>,</span></span>
<span><span>	device: </span><span>"camera"</span><span>,</span></span>
<span><span>	video: {</span></span>
<span><span>		enabled: </span><span>true</span><span>,</span></span>
<span><span>		detection: {</span></span>
<span><span>			enabled: </span><span>true</span><span>,</span></span>
<span><span>		}</span></span>
<span><span>	},</span></span>
<span><span>})</span></span></code></pre>
<p>Also, for the record, Typescript is really nice.
🤮 Javascript 🤮 is still an abomination.</p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The US Department of Agriculture Bans Support for Renewables (106 pts)]]></title>
            <link>https://insideclimatenews.org/news/19082025/usda-bans-farm-renewables-support/</link>
            <guid>44987539</guid>
            <pubDate>Fri, 22 Aug 2025 17:55:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://insideclimatenews.org/news/19082025/usda-bans-farm-renewables-support/">https://insideclimatenews.org/news/19082025/usda-bans-farm-renewables-support/</a>, See on <a href="https://news.ycombinator.com/item?id=44987539">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		<main id="main" role="main">

			
				<article id="post-98509">

					
						
<!-- .entry-header -->

					
					<div>

						
<p>The U.S. Department of Agriculture announced this week that it will stop funding wind and solar energy on American farmland, a move that continues the Trump administration’s attempts to kill incentives for renewables while it boosts support for fossil fuels and land-hungry, energy inefficient biofuels.</p>



<p>At the state fairgrounds in Lebanon, Tennessee, Agriculture Secretary Brooke Rollins said Monday that the agency will no longer allow “businesses to use your taxpayer dollars to fund solar projects on prime American farmland, and we will no longer allow solar panels manufactured by foreign adversaries to be used in our USDA-funded projects.”</p>



<p>The move is part of a broader effort by the administration to revoke or reduce Biden-era funding for the expansion of wind and solar through the Inflation Reduction Act, much of which benefited farmers and agricultural areas.</p>



<p>In July, President Donald Trump signed into law the One Big Beautiful Bill Act, slashing incentives for wind and solar while boosting support for biofuels, which consume the majority of the country’s cropland. The bill also restricts the use of Chinese-made solar components, a directive echoed in Rollins’ comments this week.</p>







<p>The USDA formally announced the wind and solar funding cuts on Tuesday. It did not respond to specific questions from Inside Climate News.</p>



<p>The agency and lawmakers supporting the move said their primary concern was safeguarding the country’s farmland and food.&nbsp;</p>



<p>“Secretary Rollins understands that food security is national security, and preserving prime farmland for agricultural production is a key component of protecting our food supply,” said Glenn “GT” Thompson (R-Penn.), chairman of the House Committee on Agriculture, in a statement.&nbsp;</p>



<p>More than half the country’s cropland—178 million out of <a href="https://www.ers.usda.gov/data-products/charts-of-note/chart-detail?chartId=111436#:~:text=For%202024%2C%20total%20U.S.%20cropland,acreage%20of%20334%20million%20acres.">328 million crop acres</a>—is used to grow corn and soybeans, much of it&nbsp; for biofuels, not food. About one-third of the acres planted in corn are used for corn-based ethanol, which <a href="https://www.wri.org/insights/increased-biofuel-production-impacts-climate-change-farmers">amounts to about 4 percent</a> of the country’s fuel mix. More than 40 percent of the soybean supply is used for biofuels, despite biodiesel amounting to less than <a href="https://www.wri.org/insights/increased-biofuel-production-impacts-climate-change-farmers">1 percent of the fuel mix</a>.&nbsp;&nbsp;</p>



<p>Most of the remaining corn and soy is fed to confined livestock, which are a major source of greenhouse gas emissions.</p>



<p>Only about <a href="https://www.wri.org/insights/crop-expansion-food-security-trends">2 percent</a> of the country’s corn is used for direct human consumption.&nbsp;</p>



<p>“Tennessee farmland should be used to grow the crops that feed our state and country, not to house solar panels made by foreign countries like Communist China,” said Sen. Marsha Blackburn (R-Tenn.). “Secretary Rollins and President Trump are right to put an end to these Green New Deal subsidies that waste taxpayer dollars while threatening America’s food security. I applaud this administration for investing in rural communities across Tennessee and empowering them to prosper for years to come.”</p>



<p>The biggest <a href="https://www.nass.usda.gov/Quick_Stats/Ag_Overview/stateOverview.php?state=TENNESSEE">agricultural land users</a> in Tennessee are corn and soybeans, which are grown on about 2.5 million acres.&nbsp;</p>



<p>A state commission found in 2024 that <a href="https://www.tn.gov/content/dam/tn/tacir/2023publications/2023_Solar.pdf">solar development did not threaten</a> the state’s farmland.&nbsp;</p>



<p>The USDA said it would immediately disqualify wind and solar projects from its Rural Development Business and Industry Guaranteed Loan Program and disqualify any wind or solar systems that are not “right-sized for their facilities” from a loan program under the Rural Energy for America Program (REAP).</p>



<p>Earlier this year, the agency stopped distributing already promised REAP grants, prompting <a href="https://insideclimatenews.org/news/13032025/farmers-community-groups-sue-trump-usda-allocated-funds/">farmers to sue</a> the administration.&nbsp;</p>



<p>The administration’s latest move could complicate the economic landscape for farmers, who have increasingly relied on the income from wind and solar installations on their land as commodity prices have fallen and climate-driven weather extremes threaten production.</p>



<p>In Iowa—the nation’s biggest corn-producing state—wind provides <a href="https://www.eia.gov/state/analysis.php?sid=IA">about 60 percent</a> of electricity.</p>



<p>“This is such a popular program—it saves them money and gives them a potential financial source,” said Richa Patel, a policy specialist at the National Sustainable Agriculture Coalition.&nbsp; “It’s a step backwards for farmers and small businesses that are trying to make decisions that are good for the business and the environment.”</p>



<p>Patel said she and her colleagues were still digging into the specifics about what the new limitations might mean and what type of solar facilities they will apply to.&nbsp;&nbsp;</p>



<p>In its statement Tuesday, the USDA said the number of solar panels on farmland has increased by nearly 50 percent since 2021. “That is why the Department is taking action,” the statement said.&nbsp;</p>



<p>A <a href="https://www.ers.usda.gov/amber-waves/2024/september/agricultural-land-near-solar-and-wind-projects-usually-remained-in-agriculture-after-development">2024 analysis by the USDA</a> found that about 424,000 acres were used for wind and solar, about 0.05 percent of the 897 million total pasture, rangeland and crop acres in the country.&nbsp;&nbsp;</p>



<p>The agency also found that agricultural land usually maintained similar characteristics and could still be used as farmland “even after the addition of solar or wind development.”</p>

						
<div>

		<h2>About This Story</h2>

		<p>Perhaps you noticed: This story, like all the news we publish, is free to read. That’s because Inside Climate News is a 501c3 nonprofit organization. We do not charge a subscription fee, lock our news behind a paywall, or clutter our website with ads. We make our news on climate and the environment freely available to you and anyone who wants it.</p>
		<p>That’s not all. We also share our news for free with scores of other media organizations around the country. Many of them can’t afford to do environmental journalism of their own. We’ve built bureaus from coast to coast to report local stories, collaborate with local newsrooms and co-publish articles so that this vital work is shared as widely as possible.</p>
		<p>Two of us launched ICN in 2007. Six years later we earned a Pulitzer Prize for National Reporting, and now we run the oldest and largest dedicated climate newsroom in the nation. We tell the story in all its complexity. We hold polluters accountable. We expose environmental injustice. We debunk misinformation. We scrutinize solutions and inspire action.</p>
		<p>Donations from readers like you fund every aspect of what we do. If you don’t already, will you support our ongoing work, our reporting on the biggest crisis facing our planet, and help us reach even more readers in more places? </p>
		<p>Please take a moment to make a tax-deductible donation. Every one of them makes a difference.</p>
		<p>Thank you,</p>

		 <!-- /.footer -->

	</div> <!-- /.icn-epic -->

						
 <!-- /.icn-share-holder -->

						
		<div>

			
				<p><img width="300" height="300" src="https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-300x300.jpg" alt="" decoding="async" srcset="https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-300x300.jpg 300w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-150x150.jpg 150w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-64x64.jpg 64w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-600x600.jpg 600w" sizes="(max-width: 300px) 100vw, 300px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20300%20300'%3E%3C/svg%3E" data-lazy-srcset="https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-300x300.jpg 300w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-150x150.jpg 150w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-64x64.jpg 64w, https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-600x600.jpg 600w" data-lazy-src="https://insideclimatenews.org/wp-content/uploads/2020/10/GeorginaGustin-300x300.jpg">
				</p> <!-- /.image-holder -->

			
			<div>

				

				
				
					<h4>Reporter, Washington, D.C.</h4>

				
				
				
					<div dir="auto" data-qa="message_content">
<p>Georgina Gustin covers agriculture for Inside Climate News, and has reported on the intersections of farming, food systems and the environment for much of her journalism career. Her work has won numerous awards, including the John B. Oakes Award for Distinguished Environmental Journalism, and she was twice named the Glenn Cunningham Agricultural Journalist of the Year, once with ICN colleagues. She has worked as a reporter for The Day in New London, Conn., the St. Louis Post-Dispatch and CQ Roll Call, and her stories have appeared in The New York Times, Washington Post and National Geographic’s The Plate, among others. She is a graduate of the Columbia University Graduate School of Journalism and the University of Colorado at Boulder.</p>
</div>


				
				
	

	
			</div> <!-- /.bio -->

		</div> <!-- /.post-author-bio -->

		
					</div><!-- .entry-content -->

				</article><!-- #post-## -->

				 <!-- /.post-footer -->

			
		</main><!-- #main -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sprinkling self-doubt on ChatGPT (127 pts)]]></title>
            <link>https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/</link>
            <guid>44987422</guid>
            <pubDate>Fri, 22 Aug 2025 17:45:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/">https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/</a>, See on <a href="https://news.ycombinator.com/item?id=44987422">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
  <article data-pagefind-body="">
    <time pubdate="" datetime="2025-08-22" data-pagefind-meta="date:2025-08-22" data-pagefind-sort="date:2025-08-22">
      Friday, Aug 22, 2025
    </time>
    <a href="https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/">
      
    </a>

    <p>I replaced my ChatGPT personalization settings with this prompt a few weeks ago and promptly forgot about it:</p>
<blockquote>
<ul>
<li>Be extraordinarily skeptical of your own correctness or stated assumptions. You aren't a cynic, you are a highly critical thinker and this is tempered by your self-doubt: you absolutely hate being wrong but you live in constant fear of it</li>
<li>When appropriate, broaden the scope of inquiry beyond the stated assumptions to think through unconvenitional opportunities, risks, and pattern-matching to widen the aperture of solutions</li>
<li>Before calling anything "done" or "working", take a second look at it ("red team" it) to critically analyze that you really are done or it really is working</li>
</ul></blockquote>
<p>I noticed a difference in results right away (even though I kept forgetting the change was due to my instructions and not the separately <a href="https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming">tumultuous rollout of GPT-5</a>).</p>
<p>Namely, pretty much every initial response now starts with:</p>
<ul>
<li>An expression of caution, self-doubt, and desire to get things right</li>
<li>Hilariously long "thinking" times (I asked it to estimate the macronutrients in lettuce yesterday and it spent 3 minutes and 59 seconds reasoning)</li>
<li>A post-hoc adversarial "<a href="https://en.wikipedia.org/wiki/Red_team">red team</a>" analysis of whatever it just vomited up as an answer</li>
</ul>
<p>I'm delighted to report that ChatGPT's output has been more useful since this change. Still not altogether <em>great</em>, but better at the margins. In particular, the "red team" analysis at the end of many requests frequently spots an error and causes it to arrive at the <em>actually-correct</em> answer, which—if nothing else—saves me the step of expressing skepticism. And even when ChatGPT is nevertheless wrong, its penchant for extremely-long thinking times means I'm getting my money's worth in GPU time.</p>
  </article>
  <div>
  <hr>
  <h2>Got a taste for hot, fresh takes?</h2>
  <p>
    Then you're in luck, because you'll pay $0 for my 2¢ when you <a href="https://justin.searls.co/subscribe/">subscribe to my work</a>, whether via <a href="https://justin.searls.co/rss/">RSS</a> or your favorite <a href="https://justin.searls.co/posse/">social network</a>.
  </p>
  <p>
    I also have a monthly <a href="https://justin.searls.co/newsletter">newsletter</a> where I write high-tempo, thought-provoking essays about life, in case that's more your speed:
  </p>
  
  <p>
    And if you'd rather give your eyes a rest and your ears a workout, might I suggest my long-form solo podcast, <a href="https://justin.searls.co/casts/breaking-change/">Breaking Change</a>? Odds are, you haven't heard anything quite like it.
  </p>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Leaving Gmail for Mailbox.org (111 pts)]]></title>
            <link>https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/</link>
            <guid>44987380</guid>
            <pubDate>Fri, 22 Aug 2025 17:41:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/">https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/</a>, See on <a href="https://news.ycombinator.com/item?id=44987380">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>This was a tough decision, having used Gmail since 2007/2008. However, I had to draw the line and stop giving Google my data for free.</p><p>The problem with email is that everything is transmitted in plain text. Technically, Google can store every message you receive and know everything, and U.S. agencies can request access to that data (this include also EU citizens under the <a href="https://policies.google.com/privacy/frameworks?hl=en-US">EU-U.S. and Swiss-U.S. Data Privacy Frameworks</a>).</p><p>For someone like me, who cares about privacy and runs as much as possible on my own home servers, that felt like way too much.</p><p>So I decided to switch to another provider, one that respects privacy a bit more. Of course, this meant no longer “paying” with my personal data, but instead paying the actual price of the email service.</p><h3 id="the-beginning">The beginning</h3><p>Let me start by saying: I use email in a very basic way. I send and receive a lot of messages (at least 50 a day), but they’re plain text/html emails with no attachments or fancy features. I couldn’t care less about the rest of the “suite", like notes, contacts, calendars and all that extra stuff.</p><p>So, after a bit of research, I narrowed it down to three different services:</p><ul><li>Mailbox.org</li><li>Proton Mail</li><li>Tutanota</li></ul><p>The last two providers offered true end-to-end encryption, at a cost of about €3/4 per month. Sounds good… but the catch is that to use their end-to-end encryption you’re forced to use their apps (or, on macOS, run a background “bridge”).</p><p>That’s a no go for me, because I love Apple’s Mail app on macOS and iOS, it just works perfectly for my needs, and I don’t want to give that up.</p><p>So, I went with mailbox.org that still offers integrated PGP encryption, and if you want, you can always use external PGP too (which I was already doing with Gmail).</p><p>Mailbox.org has a solid plan: 10GB of email storage plus 5GB of cloud storage starting at €2.50/month (paid annually). You can even expand the mail storage up to 100GB, at €0.20 per gigabyte.</p><p>I was using around 2.5GB on Gmail, so I had no issues with paying the equivalent of two coffees a month for a huge boost in privacy. And if I ever need more space, I can just add it on-demand for €0.20/GB.</p><p>There’s also a free one-month trial, but it’s pretty limited since you can’t send emails outside of mailbox.org domains.</p><p>So win the end, I registered my new address <code>giuliomagnifico@mailbox.org</code> and paid €3 for a month of testing. That means I’m covered for two months, and then I can just “top up” the account with €30 for a full year.</p><div id="callout"><p>⚠️</p><p>Mailbox.org doesn’t use auto-renewal, so you have to manually top up your account. Nice feature</p></div><h3 id="mailboxorg-online">Mailbox.org online</h3><p>The web interface is extremely simple but very effective. I actually find it better than Gmail, less bloated of useless stuff.</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/webui.jpeg" alt="webUI"></p><p>And on mobile it’s very usable too.</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/ios.jpeg" alt="ios"></p><p>One thing I prefer is using folders instead of Gmail’s “labels.” Mainly because this way I can put the folders directly under the account in Apple Mail (I think is the only email that can actually support this).</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/folders.jpeg" alt="folders"></p><p>Mailbox.org also has all the features I need,
and probably way more than I’ll ever use. It even includes storage, video chat, an XMPP chat, task lists, calendar, contacts, an Etherpad (basically shared notes, I think), and so on… none of which I really care about.</p><h2 id="migrating">Migrating</h2><p>I decided to move all my emails from Gmail to mailbox.org, so I could (in future) completely wipe my Gmail account.</p><p>To do this, I used the tool <a href="https://github.com/imapsync/imapsync">imapsync</a>, which I installed on my Archive server (see this post <a href="https://giuliomagnifico.blog/post/2025-08-01-home-setup-v6/#archive-minipc-for-paperless-immich-n8n-and-linkding">My home setup v6 | Archive server</a>)</p><p>After creating an “app password” on Gmail, I installed the Docker image and ran the tool with this script:</p><pre tabindex="0"><code>#!/bin/sh
set -eu

HOST1="imap.gmail.com"
USER1="giuliomagnifico@gmail.com"
PASS1="xxx"

HOST2="imap.mailbox.org"
USER2="giuliomagnifico@mailbox.org"
PASS2="xxx"

LOGDIR="/home/imapsync/logs"
mkdir -p "$LOGDIR"
LOGFILE="$LOGDIR/sync_$(date +%F_%H-%M-%S).log"

echo "Starting: $(date)"
docker compose run --rm imapsync imapsync \
  --host1 "$HOST1" --user1 "$USER1" --password1 "$PASS1" --ssl1 \
  --host2 "$HOST2" --user2 "$USER2" --password2 "$PASS2" --ssl2 \
  --automap --syncinternaldates --skipsize \
  --useuid --addheader --usecache --buffersize 4096 \
  --nofoldersizes --nofoldersizesatend \
  --exclude "\[Gmail\]/All Mail" \
  --regextrans2 "s/\[Imap\]\/Archive/Archive/" \
  --log &gt; "$LOGFILE" 2&gt;&amp;1

echo "Complete: $(date)"
echo "Log file: $LOGFILE"
</code></pre><p>The script excludes the All Mail folder" using: <code>--exclude "\[Gmail\]/All Mail" \</code></p><p>This to avoid duplicate emails already present in the folders, I also merged the <code>[Imap]/Archive</code> folder into the general Archive folder using: <code>--regextrans2 "s/\[Imap\]\/Archive/Archive/"</code></p><p>This because Apple’s Mail app creates the <code>[Imap]/Archive</code> folder/label on Gmail whenever you use the “Archive” function instead of “Trash.”</p><p>The whole process took a couple of hours (11201secs, ~3h to be precise) during which I was monitoring the logs using: <code>tail -f /home/imapsync/logs/sync_2025-08-19_15-02-48.log</code></p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/logs.jpeg" alt="logs"></p><p>And in the end, the end…</p><pre tabindex="0"><code>[cut]
msg [Gmail]/Trash/183393 {19549}      copied to Trash/13361      2.36 msgs/s  200.418 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183394 {92245}      copied to Trash/13362      2.36 msgs/s  200.420 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183395 {19675}      copied to Trash/13363      2.36 msgs/s  200.415 KiB/s 2.140 GiB copied 
msg [Gmail]/Trash/183396 {5953}       copied to Trash/13364      2.36 msgs/s  200.410 KiB/s 2.140 GiB copied 
++++ End looping on each folder
++++ Statistics
Transfer started on                     : Tuesday 19 August 2025-08-19 03:02:49 +0000 UTC
Transfer ended on                       : Tuesday 19 August 2025-08-19 06:09:30 +0000 UTC
Transfer time                           : 11201.5 sec
Folders synced                          : 14/14 synced
Folders deleted on host2                : 0 
Messages transferred                    : 26407 
Messages skipped                        : 0
Messages found duplicate on host1       : 0
Messages found duplicate on host2       : 0
Messages found crossduplicate on host2  : 0
Messages void (noheader) on host1       : 0  
Messages void (noheader) on host2       : 0
Messages found in host1 not in host2    : 0 messages
Messages found in host2 not in host1    : 0 messages
Messages deleted on host1               : 0
Messages deleted on host2               : 0
Total bytes transferred                 : 2297647358 (2.140 GiB)
Total bytes skipped                     : 0 (0.000 KiB)
Message rate                            : 2.4 messages/s
Average bandwidth rate                  : 200.3 KiB/s
Reconnections to host1                  : 0
Reconnections to host2                  : 0
Memory consumption at the end           : 268.7 MiB (*time 836.2 MiB*h) (started with 161.5 MiB)
Load end is                             : 0.06 0.08 0.08 1/1135 on 16 cores
CPU time and %CPU                       : 446.72 sec 4.0 %CPU 0.2 %allcpus
Biggest message transferred             : 30413995 bytes (29.005 MiB)
Memory/biggest message ratio            : 9.3
Detected 0 errors
This imapsync is up to date. ( local 2.306 &gt;= official 2.290 )( Use --noreleasecheck to avoid this release check. )
Homepage: https://imapsync.lamiral.info/
Exiting with return value 0 (EX_OK: successful termination) 0/50 nb_errors/max_errors PID 1
Removing pidfile /var/tmp//tmp/imapsync.pid
Log file is LOG_imapsync/2025_08_19_03_02_49_171_giuliomagnifico_gmail_com_giuliomagnifico_mailbox_org.txt ( to change it, use --logfile filepath ; or use --nolog to turn off logging )
</code></pre><h3 id="simplify-the-transition">Simplify the transition</h3><p>Of course, the full switch will be a gradual process, even though I’ve already updated almost all my main services with the new address.</p><p>To make things easier, on my old Gmail account (which I removed from Apple Mail on all devices) I set up a forward to my new mailbox.org address.</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/forward.jpeg" alt="forward">
On the new mailbox.org account, I also set up a filter to flag any emails that get forwarded from Gmail.</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/gmailrule.jpeg" alt="gmailrule"></p><p>That way, I immediately notice them and I can update the address from Gmail to Mailbox.org whenever they show up. (The <code>\flagged</code> tag is perfect for this, since it add a “real red flag” in Apple Mail on iOS, iPadOS and macOS)</p><h2 id="cryptography">Cryptography</h2><p>Mailbox.org allows you to easily import your keys for PGP cryptography directly from the web. This is convenient as it lets you read and send PGP encrypted emails right from the browser on iOS, where there aren’t any “decent” apps for encrypted mail.</p><p>The same goes for macOS, although there you can just use Thunderbird, which works really well.</p><p>Here’s how PGP emails look on iOS:</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/gpgios.jpeg" alt="PGPios"></p><p>To send encrypted emails, you just select “Use PGP encrypted” when composing a new message, after importing your private key, of course.</p><p>And from the web interface, there’s also a handy feature to quickly import the sender’s public keys:</p><p><img src="https://giuliomagnifico.blog/_images/2025/away-from-gmail/gpgmac.jpeg" alt="gpgmac"></p><h3 id="conclusion">Conclusion</h3><p>I’m satisfied. Leaving Gmail completely was something I wanted to do for a long time, but I was always hesitant. Finally, I made the switch, and, as often happens with these transitions, I discovered many unexpected positive aspects.</p><p>Oh, and if you have something to tell me or just want to test Mailbox.org after your switch, feel free to send me an email. Here’s my public key:</p><pre tabindex="0"><code>-----BEGIN PGP PUBLIC KEY BLOCK-----
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=GpPW
-----END PGP PUBLIC KEY BLOCK-----
</code></pre></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[XSLT removal will break multiple government and regulatory sites (134 pts)]]></title>
            <link>https://github.com/whatwg/html/issues/11582</link>
            <guid>44987346</guid>
            <pubDate>Fri, 22 Aug 2025 17:38:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/whatwg/html/issues/11582">https://github.com/whatwg/html/issues/11582</a>, See on <a href="https://news.ycombinator.com/item?id=44987346">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><h3 dir="auto">What is the issue with the HTML Standard?</h3>
<p dir="auto">One of the issues we've seen in <a data-error-text="Failed to load title" data-id="3285251497" data-permission-text="Title is private" data-url="https://github.com/whatwg/html/issues/11523" data-hovercard-type="issue" data-hovercard-url="/whatwg/html/issues/11523/hovercard" href="https://github.com/whatwg/html/issues/11523">#11523</a> and <a data-error-text="Failed to load title" data-id="3323021317" data-permission-text="Title is private" data-url="https://github.com/whatwg/html/issues/11563" data-hovercard-type="pull_request" data-hovercard-url="/whatwg/html/pull/11563/hovercard" href="https://github.com/whatwg/html/pull/11563">#11563</a> is that the proposal to remove XSLT from the spec doesn't acknowledge existing use cases beyond Chrome Status counter stats.</p>
<p dir="auto">According to Chrome's own <a href="https://docs.google.com/document/d/1RC-pBBvsazYfCNNUSkPqAVpSpNJ96U8trhNkfV0v9fk/edit?tab=t.0#heading=h.83o2xr8ayal6" rel="nofollow">Blink principles of web compatibility</a>:</p>
<blockquote>
<p dir="auto">The primary signal we use is the fraction of page views impacted in Chrome, usually computed via Blink’s UseCounter UMA metrics.  As a general rule of thumb, 0.1% of PageVisits (1 in 1000) is large, while 0.001% is considered small but non-trivial.  Anything below about 0.00001% (1 in 10 million) is generally considered trivial.  There are around 771 billion web pages viewed in Chrome every month (not counting other Chromium-based browsers).  So seriously breaking even 0.0001% still results in someone being frustrated every 3 seconds, and so not to be taken lightly!</p>
</blockquote>
<p dir="auto">To add to the use cases already provided in the discussions above, here's a small list:</p>
<markdown-accessiblity-table><table role="table">
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>United States Congress</td>
<td>Legislative texts are shown using XSLT</td>
<td><a href="https://www.congress.gov/117/bills/hr3617/BILLS-117hr3617ih.xml" rel="nofollow">https://www.congress.gov/117/bills/hr3617/BILLS-117hr3617ih.xml</a> and <a href="https://www.govinfo.gov/content/pkg/BILLS-119hr400ih/xml/BILLS-119hr400ih.xml" rel="nofollow">https://www.govinfo.gov/content/pkg/BILLS-119hr400ih/xml/BILLS-119hr400ih.xml</a></td>
</tr>
<tr>
<td>National Weather Service Current Observations</td>
<td>Uses client-side XSLT to transform and display current weather observation data from XML.</td>
<td><a href="https://www.weather.gov/xml/current_obs/KABE.xml" rel="nofollow">https://www.weather.gov/xml/current_obs/KABE.xml</a></td>
</tr>
<tr>
<td>European Parliament Political Parties</td>
<td>Uses client-side XSLT to transform and display information on European political parties from XML.</td>
<td><a href="https://www.europarl.europa.eu/politicalparties/index_en.xml" rel="nofollow">https://www.europarl.europa.eu/politicalparties/index_en.xml</a></td>
</tr>
<tr>
<td>Therapeutic Goods Administration Code Definitions</td>
<td>Uses client-side XSLT to transform and display regulatory code definitions from XML.</td>
<td><a href="https://apps.tga.gov.au/downloads/sequence-description.xml" rel="nofollow">https://apps.tga.gov.au/downloads/sequence-description.xml</a></td>
</tr>
<tr>
<td>Canadian Forest Service Weather Stations Metadata</td>
<td>Uses client-side XSLT to transform and display weather station metadata from XML.</td>
<td><a href="https://cwfis.cfs.nrcan.gc.ca/downloads/fwi_obs/WeatherStations_CWFIS_export.xml" rel="nofollow">https://cwfis.cfs.nrcan.gc.ca/downloads/fwi_obs/WeatherStations_CWFIS_export.xml</a></td>
</tr>
<tr>
<td>European Pollutant Release and Transfer Register Method Types</td>
<td>Uses client-side XSLT to transform and display method type codes from XML.</td>
<td><a href="https://converters.eionet.europa.eu/xmlfile/EPRTR_MethodTypeCode_1.xml" rel="nofollow">https://converters.eionet.europa.eu/xmlfile/EPRTR_MethodTypeCode_1.xml</a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">If I, and others, could easily find such examples, I believe browser vendors could easily find these, too.</p>
<p dir="auto">I would like <a data-hovercard-type="user" data-hovercard-url="/users/mfreed7/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/mfreed7">@mfreed7</a> <a data-hovercard-type="user" data-hovercard-url="/users/domenic/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/domenic">@domenic</a> and other Googlers who initiated and move on with this proposal to acknowledge this, and provide more rationale for removing XSLT than just Chrome Status counter stats.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo granted permit to begin testing in New York City (429 pts)]]></title>
            <link>https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html</link>
            <guid>44986949</guid>
            <pubDate>Fri, 22 Aug 2025 17:02:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html">https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html</a>, See on <a href="https://news.ycombinator.com/item?id=44986949">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108110425" data-test="InlineImage"><p>Waymo self-driving cars with roof-mounted sensor arrays traveling near palm trees and modern buildings along the Embarcadero, San Francisco, California, February 21, 2025.&nbsp;</p><p>Smith Collection/gado | Archive Photos | Getty Images</p></div><div><p><a href="https://www.cnbc.com/2025/07/08/waymo-teen-accounts.html">Waymo</a> is getting one step closer to rides in <a href="https://www.cnbc.com/2025/06/18/waymo-cars-are-coming-to-new-york-with-a-driver-behind-the-wheel.html">New York City</a>.</p><p>The <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/GOOGL/">Alphabet</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> autonomous vehicle subsidiary received its first permit from the New York Department of Transportation on Friday to start testing in New York City, Mayor <a href="https://www.nyc.gov/mayors-office/news/2025/08/mayor-adams--dot-announce-approval-of-first-application-to-test-" target="_blank">Eric Adams</a> announced Friday. The rollout is the city's first <a href="https://www.cnbc.com/2025/07/02/autonomous-cars-are-having-their-chatgpt-moment-bank-of-america.html">autonomous vehicle</a> testing launch.</p><p>Waymo will start testing up to eight vehicles in <a href="https://www.cnbc.com/2025/04/02/manhattan-luxury-real-estate-market.html">Manhattan</a> and Downtown Brooklyn through late September with the potential to extend the program. New York state law requires the company to have a driver behind the wheel to operate.</p><p>"We're a tech-friendly administration and we're always looking for innovative ways to safely move our city forward," Adams said in a release. "New York City is proud to welcome Waymo to test this new technology in Manhattan and Brooklyn, as we know this testing is only the first step in moving our city further into the 21st century."</p></div><div id="RegularArticle-RelatedContent-1"><h2>Read more CNBC tech news</h2><div><ul><li><a href="https://www.cnbc.com/2025/08/20/google-pixel-10-gemini-ai.html">Google announces its AI-powered Pixel 10 smartphone series</a></li><li><a href="https://www.cnbc.com/2025/08/20/these-little-robots-are-changing-the-way-solar-farms-are-built.html">These little robots are changing the way solar farms are built, saving time and money</a></li><li><a href="https://www.cnbc.com/2025/08/20/intel-investors-equity-softbank-trump.html">Intel in talks with other large investors for equity boost at discount, sources say</a></li><li><a href="https://www.cnbc.com/2025/08/20/gates-nvidia-fieldai-robotics.html">Nvidia, Bill Gates-backed robotics startup Field AI hits $2 billion valuation after recent raise</a></li></ul></div></div><div><p>The news comes just two months after the company said it filed permits to test its cars in the city with a trained specialist behind the wheel.</p><p>Waymo has hit <a href="https://www.cnbc.com/2025/07/28/waymo-plans-to-bring-its-robotaxi-service-to-dallas-in-2026.html">expansion</a> mode on its services nationwide, launching in <a href="https://www.cnbc.com/2025/03/04/waymo-uber-begin-offering-robotaxi-rides-in-austin-ahead-of-sxsw.html">Austin</a> this year and expanding its <a href="https://www.cnbc.com/2025/03/11/waymo-expands-its-robotaxi-service-in-the-san-francisco-bay-area.html">San Francisco area</a> operations in March. Waymo also plans to bring autonomous vehicles to Atlanta, Miami and Washington, D.C. and recently said it will start operations in <a href="https://www.cnbc.com/2025/07/07/waymo-to-begin-testing-in-philadelphia-with-drivers-behind-the-wheel.html">Philadelphia</a> as it looks to break further into the Northeast market.</p><p>Waymo's CEO said the company surpassed 10 million <a href="https://www.cnbc.com/2025/05/20/waymo-ceo-tekedra-mawakana-10-million.html">robotaxi</a> trips in May.</p><p>For years, autonomous vehicle companies have sought to introduce their technology to The Big Apple, with Waymo previously taking a crack at it in 2021. At that time, the company rolled out some cars in certain areas of the city for<a href="https://waymo.com/blog/2021/11/introducing-waymo-driver-to-new-york" target="_blank"> manual driving and data collection</a>.</p><p>New York City has also expressed interest in bringing autonomous vehicles to the city. <a href="https://www.nyc.gov/mayors-office/news/2024/03/mayor-adams-releases-requirements-opens-permit-applications-responsible-autonomous-vehicle" target="_blank">Last year</a>, the Adams administration implemented a series of safety requirements for responsible testing in the city and opened a permit program.</p><p>As part of the permit, Waymo must regularly meet and report data to DOT and work closely with law enforcement and emergency services.</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/07/08/waymo-begins-testing-self-driving-cars-with-human-drivers-in-new-york-and-philadelphia.html">Waymo begins testing self-driving cars with human drivers in New York and Philadelphia</a></p></div><div id="Placeholder-ArticleBody-Video-108168745" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000381578" aria-labelledby="Placeholder-ArticleBody-Video-108168745"><p><img src="https://image.cnbcfm.com/api/v1/image/108168746-17519709091751970907-40621734100-1080pnbcnews.jpg?v=1751970908&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Waymo begins testing self-driving cars with human drivers in New York and Philadelphia"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg 8.0 (626 pts)]]></title>
            <link>https://ffmpeg.org/index.html#pr8.0</link>
            <guid>44985730</guid>
            <pubDate>Fri, 22 Aug 2025 15:22:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ffmpeg.org/index.html#pr8.0">https://ffmpeg.org/index.html#pr8.0</a>, See on <a href="https://news.ycombinator.com/item?id=44985730">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="index">

  <div>
    <p>
      <h2>
        A complete, cross-platform solution to record, convert and stream audio and video.
      </h2>
    </p> <!-- col -->
     <!-- col -->
  </div> <!-- row -->

  <div>
    <h3>Converting <strong>video</strong> and <strong>audio</strong> has never been so easy.
    </h3>
    <pre>$ ffmpeg -i input.mp4 output.avi</pre>
    
  </div> <!-- well -->

  <h2 id="news">
    <span>
      <a href="https://ffmpeg.org/main.rss"><strong></strong></a> &nbsp;
      <a href="https://twitter.com/FFmpeg"><strong><i></i></strong></a> &nbsp;
      <a href="https://www.facebook.com/ffmpeg"><strong><i></i></strong></a>
    </span>
    News
  </h2>

  <h3 id="pr8.0">August 23nd, 2025, FFmpeg 8.0 <span title="David A. Huffman">"Huffman"</span></h3>
  <p>
  A new major release, <a href="https://ffmpeg.org/download.html#release_8.0">FFmpeg 8.0 <span title="David A. Huffman">"Huffman"</span></a>,
  is now available for download.
  Thanks to several delays, and modernization of our entire infrastructure, this release ended up
  being one of our largest releases to date. In short, its new features are:
  </p><ul>
    <li>Native decoders: <span title="Advanced Professional Video">APV</span>, ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728</li>
    <li>VVC decoder improvements: <span title="Intra Block Copy">IBC</span>,
                                  <span title="Adaptive Color Transform">ACT</span>,
                                  Palette Mode</li>
    <li>Vulkan compute-based codecs: FFv1 (encode and decode), ProRes RAW (decode only)</li>
    <li>Hardware accelerated decoding: Vulkan VP9, VAAPI VVC, OpenHarmony H264/5</li>
    <li>Hardware accelerated encoding: Vulkan AV1, OpenHarmony H264/5</li>
    <li>Formats: MCC, G.728, Whip, APV</li>
    <li>Filters: colordetect, pad_cuda, scale_d3d11, Whisper, and others</li>
  </ul>
  

  <p>
  A new class of decoders and encoders based on pure Vulkan compute implementation have been added.
  Vulkan is a cross-platform, open standard set of APIs that allows programs to use GPU hardware in various ways,
  from drawing on screen, to doing calculations, to decoding video via custom hardware accelerators.
  Rather than using a custom hardware accelerator present, these codecs are based on compute shaders, and work
  on any implementation of Vulkan 1.3.<br>
  Decoders use the same hwaccel API and commands, so users do not need to do anything special to enable them,
  as enabling <a href="https://trac.ffmpeg.org/wiki/HWAccelIntro#Vulkan">Vulkan decoding</a> is sufficient to use them.<br>
  Encoders, like our hardware accelerated encoders, require specifying a new encoder (ffv1_vulkan).
  Currently, the only codecs supported are: FFv1 (encoding and decoding) and ProRes RAW (decode only).
  ProRes (encode+decode) and VC-2 (encode+decode) implementations are complete and currently in review,
  to be merged soon and available with the next minor release.<br>
  Only codecs specifically designed for parallelized decoding can be implemented in such a way, with
  more mainstream codecs not being planned for support.<br>
  Depending on the hardware, these new codecs can provide very significant speedups, and open up
  possibilities to work with them for situations like non-linear video editors and
  lossless screen recording/streaming, so we are excited to learn what our downstream users can make with them.
  </p>

  <p>
  The project has recently started to modernize its infrastructure. Our mailing list servers have been
  fully upgraded, and we have recently started to accept contributions via a new forge, available on
  <a href="https://code.ffmpeg.org/">code.ffmpeg.org</a>, running a Forgejo instance.
  </p>

  <p>
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  </p>

  <h3 id="pr7.1">September 30th, 2024, FFmpeg 7.1 <span title="Rózsa Péter">"Péter"</span></h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_7.1">FFmpeg 7.1 "Péter"</a>, a new
    major release, is now available! A full list of changes can be found in the release
    <a href="https://git.ffmpeg.org/gitweb/ffmpeg.git/blob/refs/heads/release/7.1:/Changelog">changelog</a>.
  </p>
  <p>
    The more important highlights of the release are that the VVC decoder, merged as experimental in version 7.0,
    has had enough time to mature and be optimized enough to be declared as stable. The codec is starting to gain
    traction with broadcast standardization bodies.<br>
    Support has been added for a native AAC USAC (part of the xHE-AAC coding system) decoder, with the format starting
    to be adopted by streaming websites, due to its extensive volume normalization metadata.<br>
    MV-HEVC decoding is now supported. This is a stereoscopic coding tool that begun to be shipped and generated
    by recent phones and VR headsets.<br>
    LC-EVC decoding, an enhancement metadata layer to attempt to improve the quality of codecs, is now supported via an
    external library.<br>
  </p>
  <p>
    Support for Vulkan encoding, with H264 and HEVC was merged. This finally allows fully Vulkan-based decode-filter-encode
    pipelines, by having a sink for Vulkan frames, other than downloading or displaying them. The encoders have feature-parity
    with their VAAPI implementation counterparts. Khronos has announced that support for AV1 encoding is also coming soon to Vulkan,
    and FFmpeg is aiming to have day-one support.
  </p>
  <p>
    In addition to the above, this release has had a lot of important internal work done. By far, the standout internally
    are the improvements made for full-range images. Previously, color range data had two paths, no negotiation,
    and was unreliably forwarded to filters, encoders, muxers. Work on cleaning the system up started more than 10
    years ago, however this stalled due to how fragile the system was, and that breaking behaviour would be unacceptable.
    The new system fixes this, so now color range is forwarded correctly and consistently everywhere needed, and also
    laid the path for more advanced forms of negotiation.<br>
    Cropping metadata is now supported with Matroska and MP4 formats. This metadata is important not only for archival,
    but also with AV1, as hardware encoders require its signalling due to the codec not natively supporting one.
  </p>
  <p>
    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.
  </p>

  <h3 id="coverity">September 11th, 2024, Coverity</h3>
  <p>
  The number of issues FFmpeg has in <a href="https://scan.coverity.com/projects/ffmpeg">Coverity (a static analyzer)</a> is now lower than it has been since 2016.
  Our defect density is less than one 30th of the average in OSS with over a million code
  lines. All this was possible thanks to a grant from the <a href="https://www.sovereigntechfund.de/">Sovereign Tech Fund</a>.
  </p>
  <p><img src="https://ffmpeg.org/img/coverity-lifetime-2024-08.PNG" alt="Coverity Lifetime Graph till 2024-08"></p><h3 id="xheaac">June 2nd, 2024, native xHE-AAC decoder</h3>
  <p>
  FFmpeg now implements a native xHE-AAC decoder. Currently, streams without (e)SBR, USAC or MPEG-H Surround
  are supported, which means the majority of xHE-AAC streams in use should work. Support for USAC and (e)SBR is
  coming soon. Work is also ongoing to improve its stability and compatibility.
  During the process we found several specification issues, which were then submitted back to the authors
  for discussion and potential inclusion in a future errata.
  </p>

  <h3 id="stf24">May 13th, 2024, Sovereign Tech Fund</h3>
  <p>
  The FFmpeg community is excited to announce that Germany's
  <a href="https://www.sovereigntechfund.de/tech/ffmpeg">Sovereign Tech Fund</a>
  has become its first governmental sponsor. Their support will help
  sustain the maintainance of the FFmpeg project, a critical open-source
  software multimedia component essential to bringing audio and video to
  billions around the world everyday.
  </p>

  <h3 id="pr7.0">April 5th, 2024, FFmpeg 7.0 "Dijkstra"</h3>
  <p>
  A new major release, <a href="https://ffmpeg.org/download.html#release_7.0">FFmpeg 7.0 "Dijkstra"</a>,
  is now available for download. The most noteworthy changes for most users are
  a <a href="#vvcdec">native VVC decoder</a> (currently experimental, until more
  fuzzing is done), <a href="#iamf">IAMF support</a>, or a
  <a href="#cli_threading">multi-threaded <code>ffmpeg</code> CLI tool</a>.
  </p>

  <p>
  This release is <em>not</em> backwards compatible, removing APIs deprecated before 6.0.
  The biggest change for most library callers will be the removal of the old bitmask-based
  channel layout API, replaced by the <code>AVChannelLayout</code> API allowing such
  features as custom channel ordering, or Ambisonics. Certain deprecated <code>ffmpeg</code>
  CLI options were also removed, and a C11-compliant compiler is now required to build
  the code.
  </p>

  <p>
  As usual, there is also a number of new supported formats and codecs, new filters, APIs,
  and countless smaller features and bugfixes. Compared to 6.1, the <code>git</code> repository
  contains almost ∼2000 new commits by ∼100 authors, touching &gt;100000 lines in
  ∼2000 files — thanks to everyone who contributed. See the
  <a href="https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=Changelog;hb=n7.0">Changelog</a>,
  <a href="https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=doc/APIchanges;hb=n7.0">APIchanges</a>,
  and the git log for more comprehensive lists of changes.
  </p>

  <h3 id="vvcdec">January 3rd, 2024, native VVC decoder</h3>
  <p>
  The <code>libavcodec</code> library now contains a native VVC (Versatile Video Coding)
  decoder, supporting a large subset of the codec's features. Further optimizations and
  support for more features are coming soon. The code was written by Nuo Mi, Xu Mu,
  Frank Plowman, Shaun Loo, and Wu Jianhua.
  </p>

  <h3 id="iamf">December 18th, 2023, IAMF support</h3>
  <p>
  The <code>libavformat</code> library can now read and write <a href="https://aomediacodec.github.io/iamf/">IAMF</a>
  (Immersive Audio) files. The <code>ffmpeg</code> CLI tool can configure IAMF structure with the new
  <code>-stream_group</code> option. IAMF support was written by James Almer.
  </p>

  <h3 id="cli_threading">December 12th, 2023, multi-threaded <code>ffmpeg</code> CLI tool</h3>
  <p>
  Thanks to a major refactoring of the <code>ffmpeg</code> command-line tool, all the major
  components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now
  run in parallel. This should improve throughput and CPU utilization, decrease latency,
  and open the way to other exciting new features.
  </p>

  <p>
  Note that you should <em>not</em> expect significant performance improvements in cases
  where almost all computational time is spent in a single component (typically video
  encoding).
  </p>

  <h3 id="pr6.1">November 10th, 2023, FFmpeg 6.1 "Heaviside"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_6.1">FFmpeg 6.1 "Heaviside"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>libaribcaption decoder</li>
    <li>Playdate video decoder and demuxer</li>
    <li>Extend VAAPI support for libva-win32 on Windows</li>
    <li>afireqsrc audio source filter</li>
    <li>arls filter</li>
    <li>ffmpeg CLI new option: -readrate_initial_burst</li>
    <li>zoneplate video source filter</li>
    <li>command support in the setpts and asetpts filters</li>
    <li>Vulkan decode hwaccel, supporting H264, HEVC and AV1</li>
    <li>color_vulkan filter</li>
    <li>bwdif_vulkan filter</li>
    <li>nlmeans_vulkan filter</li>
    <li>RivaTuner video decoder</li>
    <li>xfade_vulkan filter</li>
    <li>vMix video decoder</li>
    <li>Essential Video Coding parser, muxer and demuxer</li>
    <li>Essential Video Coding frame merge bsf</li>
    <li>bwdif_cuda filter</li>
    <li>Microsoft RLE video encoder</li>
    <li>Raw AC-4 muxer and demuxer</li>
    <li>Raw VVC bitstream parser, muxer and demuxer</li>
    <li>Bitstream filter for editing metadata in VVC streams</li>
    <li>Bitstream filter for converting VVC from MP4 to Annex B</li>
    <li>scale_vt filter for videotoolbox</li>
    <li>transpose_vt filter for videotoolbox</li>
    <li>support for the P_SKIP hinting to speed up libx264 encoding</li>
    <li>Support HEVC,VP9,AV1 codec in enhanced flv format</li>
    <li>apsnr and asisdr audio filters</li>
    <li>OSQ demuxer and decoder</li>
    <li>Support HEVC,VP9,AV1 codec fourcclist in enhanced rtmp protocol</li>
    <li>CRI USM demuxer</li>
    <li>ffmpeg CLI '-top' option deprecated in favor of the setfield filter</li>
    <li>VAAPI AV1 encoder</li>
    <li>ffprobe XML output schema changed to account for multiple variable-fields elements within the same parent element</li>
    <li>ffprobe -output_format option added as an alias of -of</li>
  </ul>
  <p>
    This release had been overdue for at least half a year, but due to constant activity in the repository,
    had to be delayed, and we were finally able to branch off the release recently, before some of the large
    changes scheduled for 7.0 were merged.
  </p>
  <p>
    Internally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs
    and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon).<br>
    This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds.<br>
    There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders,
    reducing overhead.<br>
    RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left.<br>
    There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the
    accurracy of variable frame rate video.
  </p>
  <p>
    Next major release will be version 7.0, scheduled to be released in February. We will attempt to better stick
    to the new release schedule we announced at the start of this year.
  </p>
  <p>
    We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.
  </p>

  <h3 id="vk2023">May 31st, 2023, Vulkan decoding</h3>
  <p>
    A few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase.
    This is the first vendor-generic and platform-generic decode acceleration API, enabling the
    same code to be used on multiple platforms, with very minimal overhead.
    This is also the first multi-threaded hardware decoding API, and our code makes full use of this,
    saturating all available decode engines the hardware exposes.
  </p>
  <p>
    Those wishing to test the code can read our
    <a href="https://trac.ffmpeg.org/wiki/HWAccelIntro#Vulkan">documentation page</a>.
    For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive
    a VkImage to present or manipulate, documentation and examples are available in our source tree.
    Currently, using the latest available git checkout of our
    <a href="https://git.videolan.org/?p=ffmpeg.git;a=summary">repository</a> is required.
    The functionality will be included in stable branches with the release of version 6.1, due
    to be released soon.
  </p>
  <p>
    As this is also the first practical implementation of the specifications, bugs may be present,
    particularly in drivers, and, although passing verification, the implementation itself.
    New codecs, and encoding support are also being worked on, by both the Khronos organization
    for standardizing, and us as implementing it, and giving feedback on improving.
  </p>

  <h3 id="pr6.0">February 28th, 2023, FFmpeg 6.0 "Von Neumann"</h3>
  <p>
    A new major release, <a href="https://ffmpeg.org/download.html#release_6.0">FFmpeg 6.0 "Von Neumann"</a>,
    is now available for download. This release has many new encoders and decoders, filters,
    ffmpeg CLI tool improvements, and also, changes the way releases are done. All major
    releases will now bump the version of the ABI. We plan to have a new major release each
    year. Another release-specific change is that deprecated APIs will be removed after 3
    releases, upon the next major bump.
    This means that releases will be done more often and will be more organized.
  </p>
  <p>
    New decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats.
    QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c
    to avoid confusion) has speed-up improvements due to threading, as well as statistics options,
    and the ability to pass option values for filters from a file. There are quite a few new audio
    and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too.
    Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT
    implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better
    ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V
    vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed
    in the doc/APIchanges file in our tree.
    A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the
    next minor release, 6.1, which we plan to release soon, in line with our new release schedule.
    Some highlights are:
  </p>
  <ul>
    <li>Radiance HDR image support</li>
    <li>ddagrab (Desktop Duplication) video capture filter</li>
    <li>ffmpeg -shortest_buf_duration option</li>
    <li>ffmpeg now requires threading to be built</li>
    <li>ffmpeg now runs every muxer in a separate thread</li>
    <li>Add new mode to cropdetect filter to detect crop-area based on motion vectors and edges</li>
    <li>VAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9</li>
    <li>WBMP (Wireless Application Protocol Bitmap) image format</li>
    <li>a3dscope filter</li>
    <li>bonk decoder and demuxer</li>
    <li>Micronas SC-4 audio decoder</li>
    <li>LAF demuxer</li>
    <li>APAC decoder and demuxer</li>
    <li>Media 100i decoders</li>
    <li>DTS to PTS reorder bsf</li>
    <li>ViewQuest VQC decoder</li>
    <li>backgroundkey filter</li>
    <li>nvenc AV1 encoding support</li>
    <li>MediaCodec decoder via NDKMediaCodec</li>
    <li>MediaCodec encoder</li>
    <li>oneVPL support for QSV</li>
    <li>QSV AV1 encoder</li>
    <li>QSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9</li>
    <li>showcwt multimedia filter</li>
    <li>corr video filter</li>
    <li>adrc audio filter</li>
    <li>afdelaysrc audio filter</li>
    <li>WADY DPCM decoder and demuxer</li>
    <li>CBD2 DPCM decoder</li>
    <li>ssim360 video filter</li>
    <li>ffmpeg CLI new options: -stats_enc_pre[_fmt], -stats_enc_post[_fmt], -stats_mux_pre[_fmt]</li>
    <li>hstack_vaapi, vstack_vaapi and xstack_vaapi filters</li>
    <li>XMD ADPCM decoder and demuxer</li>
    <li>media100 to mjpegb bsf</li>
    <li>ffmpeg CLI new option: -fix_sub_duration_heartbeat</li>
    <li>WavArc decoder and demuxer</li>
    <li>CrystalHD decoders deprecated</li>
    <li>SDNS demuxer</li>
    <li>RKA decoder and demuxer</li>
    <li>filtergraph syntax in ffmpeg CLI now supports passing file contents as option values</li>
    <li>hstack_qsv, vstack_qsv and xstack_qsv filters</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr5.1">July 22nd, 2022, FFmpeg 5.1 "Riemann"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_5.1">FFmpeg 5.1 "Riemann"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>add ipfs/ipns protocol support</li>
    <li>dialogue enhance audio filter</li>
    <li>dropped obsolete XvMC hwaccel</li>
    <li>pcm-bluray encoder</li>
    <li>DFPWM audio encoder/decoder and raw muxer/demuxer</li>
    <li>SITI filter</li>
    <li>Vizrt Binary Image encoder/decoder</li>
    <li>avsynctest source filter</li>
    <li>feedback video filter</li>
    <li>pixelize video filter</li>
    <li>colormap video filter</li>
    <li>colorchart video source filter</li>
    <li>multiply video filter</li>
    <li>PGS subtitle frame merge bitstream filter</li>
    <li>blurdetect filter</li>
    <li>tiltshelf audio filter</li>
    <li>QOI image format support</li>
    <li>ffprobe -o option</li>
    <li>virtualbass audio filter</li>
    <li>VDPAU AV1 hwaccel</li>
    <li>PHM image format support</li>
    <li>remap_opencl filter</li>
    <li>added chromakey_cuda filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr5.0">January 17th, 2022, FFmpeg 5.0 "Lorentz"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_5.0">FFmpeg 5.0 "Lorentz"</a>, a new
    major release, is now available! For this long-overdue release, a major effort
    underwent to remove the old encode/decode APIs and replace them with an
    N:M-based API, the entire libavresample library was removed, libswscale
    has a new, easier to use AVframe-based API, the Vulkan code was much improved,
    many new filters were added, including libplacebo integration, and finally,
    DoVi support was added, including tonemapping and remuxing. The default
    AAC encoder settings were also changed to improve quality.
    Some of the changelog highlights:
  </p>
  <ul>
    <li>ADPCM IMA Westwood encoder</li>
    <li>Westwood AUD muxer</li>
    <li>ADPCM IMA Acorn Replay decoder</li>
    <li>Argonaut Games CVG demuxer</li>
    <li>Argonaut Games CVG muxer</li>
    <li>Concatf protocol</li>
    <li>afwtdn audio filter</li>
    <li>audio and video segment filters</li>
    <li>Apple Graphics (SMC) encoder</li>
    <li>hsvkey and hsvhold video filters</li>
    <li>adecorrelate audio filter</li>
    <li>atilt audio filter</li>
    <li>grayworld video filter</li>
    <li>AV1 Low overhead bitstream format muxer</li>
    <li>swscale slice threading</li>
    <li>MSN Siren decoder</li>
    <li>scharr video filter</li>
    <li>apsyclip audio filter</li>
    <li>morpho video filter</li>
    <li>amr parser</li>
    <li>(a)latency filters</li>
    <li>GEM Raster image decoder</li>
    <li>asdr audio filter</li>
    <li>speex decoder</li>
    <li>limitdiff video filter</li>
    <li>xcorrelate video filter</li>
    <li>varblur video filter</li>
    <li>huesaturation video filter</li>
    <li>colorspectrum source video filter</li>
    <li>RTP packetizer for uncompressed video (RFC 4175)</li>
    <li>bitpacked encoder</li>
    <li>VideoToolbox VP9 hwaccel</li>
    <li>VideoToolbox ProRes hwaccel</li>
    <li>support loongarch.</li>
    <li>aspectralstats audio filter</li>
    <li>adynamicsmooth audio filter</li>
    <li>libplacebo filter</li>
    <li>vflip_vulkan, hflip_vulkan and flip_vulkan filters</li>
    <li>adynamicequalizer audio filter</li>
    <li>yadif_videotoolbox filter</li>
    <li>VideoToolbox ProRes encoder</li>
    <li>anlmf audio filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="irc2021">June 19th, 2021, IRC</h3>
  <p>
    We have a new IRC home at Libera Chat
    now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at <a href="https://ffmpeg.org/contact.html#IRCChannels">contact#IRCChannels</a>
  </p>

  <h3 id="pr4.4">April 8th, 2021, FFmpeg 4.4 "Rao"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.4">FFmpeg 4.4 "Rao"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>AudioToolbox output device</li>
    <li>MacCaption demuxer</li>
    <li>PGX decoder</li>
    <li>chromanr video filter</li>
    <li>VDPAU accelerated HEVC 10/12bit decoding</li>
    <li>ADPCM IMA Ubisoft APM encoder</li>
    <li>Rayman 2 APM muxer</li>
    <li>AV1 encoding support SVT-AV1</li>
    <li>Cineform HD encoder</li>
    <li>ADPCM Argonaut Games encoder</li>
    <li>Argonaut Games ASF muxer</li>
    <li>AV1 Low overhead bitstream format demuxer</li>
    <li>RPZA video encoder</li>
    <li>ADPCM IMA MOFLEX decoder</li>
    <li>MobiClip FastAudio decoder</li>
    <li>MobiClip video decoder</li>
    <li>MOFLEX demuxer</li>
    <li>MODS demuxer</li>
    <li>PhotoCD decoder</li>
    <li>MCA demuxer</li>
    <li>AV1 decoder (Hardware acceleration used only)</li>
    <li>SVS demuxer</li>
    <li>Argonaut Games BRP demuxer</li>
    <li>DAT demuxer</li>
    <li>aax demuxer</li>
    <li>IPU decoder, parser and demuxer</li>
    <li>Intel QSV-accelerated AV1 decoding</li>
    <li>Argonaut Games Video decoder</li>
    <li>libwavpack encoder removed</li>
    <li>ACE demuxer</li>
    <li>AVS3 demuxer</li>
    <li>AVS3 video decoder via libuavs3d</li>
    <li>Cintel RAW decoder</li>
    <li>VDPAU accelerated VP9 10/12bit decoding</li>
    <li>afreqshift and aphaseshift filters</li>
    <li>High Voltage Software ADPCM encoder</li>
    <li>LEGO Racers ALP (.tun &amp; .pcm) muxer</li>
    <li>AV1 VAAPI decoder</li>
    <li>adenorm filter</li>
    <li>ADPCM IMA AMV encoder</li>
    <li>AMV muxer</li>
    <li>NVDEC AV1 hwaccel</li>
    <li>DXVA2/D3D11VA hardware accelerated AV1 decoding</li>
    <li>speechnorm filter</li>
    <li>SpeedHQ encoder</li>
    <li>asupercut filter</li>
    <li>asubcut filter</li>
    <li>Microsoft Paint (MSP) version 2 decoder</li>
    <li>Microsoft Paint (MSP) demuxer</li>
    <li>AV1 monochrome encoding support via libaom &gt;= 2.0.1</li>
    <li>asuperpass and asuperstop filter</li>
    <li>shufflepixels filter</li>
    <li>tmidequalizer filter</li>
    <li>estdif filter</li>
    <li>epx filter</li>
    <li>Dolby E parser</li>
    <li>shear filter</li>
    <li>kirsch filter</li>
    <li>colortemperature filter</li>
    <li>colorcontrast filter</li>
    <li>PFM encoder</li>
    <li>colorcorrect filter</li>
    <li>binka demuxer</li>
    <li>XBM parser</li>
    <li>xbm_pipe demuxer</li>
    <li>colorize filter</li>
    <li>CRI parser</li>
    <li>aexciter audio filter</li>
    <li>exposure video filter</li>
    <li>monochrome video filter</li>
    <li>setts bitstream filter</li>
    <li>vif video filter</li>
    <li>OpenEXR image encoder</li>
    <li>Simbiosis IMX decoder</li>
    <li>Simbiosis IMX demuxer</li>
    <li>Digital Pictures SGA demuxer and decoders</li>
    <li>TTML subtitle encoder and muxer</li>
    <li>identity video filter</li>
    <li>msad video filter</li>
    <li>gophers protocol</li>
    <li>RIST protocol via librist</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr4.3">June 15th, 2020, FFmpeg 4.3 "4:3"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.3">FFmpeg 4.3 "4:3"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>v360 filter</li>
    <li>Intel QSV-accelerated MJPEG decoding</li>
    <li>Intel QSV-accelerated VP9 decoding</li>
    <li>Support for TrueHD in mp4</li>
    <li>Support AMD AMF encoder on Linux (via Vulkan)</li>
    <li>IMM5 video decoder</li>
    <li>ZeroMQ protocol</li>
    <li>support Sipro ACELP.KELVIN decoding</li>
    <li>streamhash muxer</li>
    <li>sierpinski video source</li>
    <li>scroll video filter</li>
    <li>photosensitivity filter</li>
    <li>anlms filter</li>
    <li>arnndn filter</li>
    <li>bilateral filter</li>
    <li>maskedmin and maskedmax filters</li>
    <li>VDPAU VP9 hwaccel</li>
    <li>median filter</li>
    <li>QSV-accelerated VP9 encoding</li>
    <li>AV1 encoding support via librav1e</li>
    <li>AV1 frame merge bitstream filter</li>
    <li>AV1 Annex B demuxer</li>
    <li>axcorrelate filter</li>
    <li>mvdv decoder</li>
    <li>mvha decoder</li>
    <li>MPEG-H 3D Audio support in mp4</li>
    <li>thistogram filter</li>
    <li>freezeframes filter</li>
    <li>Argonaut Games ADPCM decoder</li>
    <li>Argonaut Games ASF demuxer</li>
    <li>xfade video filter</li>
    <li>xfade_opencl filter</li>
    <li>afirsrc audio filter source</li>
    <li>pad_opencl filter</li>
    <li>Simon &amp; Schuster Interactive ADPCM decoder</li>
    <li>Real War KVAG demuxer</li>
    <li>CDToons video decoder</li>
    <li>siren audio decoder</li>
    <li>Rayman 2 ADPCM decoder</li>
    <li>Rayman 2 APM demuxer</li>
    <li>cas video filter</li>
    <li>High Voltage Software ADPCM decoder</li>
    <li>LEGO Racers ALP (.tun &amp; .pcm) demuxer</li>
    <li>AMQP 0-9-1 protocol (RabbitMQ)</li>
    <li>Vulkan support</li>
    <li>avgblur_vulkan, overlay_vulkan, scale_vulkan and chromaber_vulkan filters</li>
    <li>ADPCM IMA MTF decoder</li>
    <li>FWSE demuxer</li>
    <li>DERF DPCM decoder</li>
    <li>DERF demuxer</li>
    <li>CRI HCA decoder</li>
    <li>CRI HCA demuxer</li>
    <li>overlay_cuda filter</li>
    <li>switch from AvxSynth to AviSynth+ on Linux</li>
    <li>mv30 decoder</li>
    <li>Expanded styling support for 3GPP Timed Text Subtitles (movtext)</li>
    <li>WebP parser</li>
    <li>tmedian filter</li>
    <li>maskedthreshold filter</li>
    <li>Support for muxing pcm and pgs in m2ts</li>
    <li>Cunning Developments ADPCM decoder</li>
    <li>asubboost filter</li>
    <li>Pro Pinball Series Soundbank demuxer</li>
    <li>pcm_rechunk bitstream filter</li>
    <li>scdet filter</li>
    <li>NotchLC decoder</li>
    <li>gradients source video filter</li>
    <li>MediaFoundation encoder wrapper</li>
    <li>untile filter</li>
    <li>Simon &amp; Schuster Interactive ADPCM encoder</li>
    <li>PFM decoder</li>
    <li>dblur video filter</li>
    <li>Real War KVAG muxer</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="photosensitivity">October 5th, 2019, Bright Lights</h3>
  <p>
  FFmpeg has added a realtime bright flash removal filter to libavfilter.
  </p>
  <p>
  Note that this filter is not FDA approved, nor are we medical professionals.
  Nor has this filter been tested with anyone who has photosensitive epilepsy.
  FFmpeg and its photosensitivity filter are not making any medical claims.
  </p>
  <p>
  That said, this is a new video filter that may help photosensitive people
  watch tv, play video games or even be used with a VR headset to block
  out epiletic triggers such as filtered sunlight when they are outside.
  Or you could use it against those annoying white flashes on your tv screen.
  The filter fails on some input, such as the
  <a href="https://www.youtube.com/watch?v=8L_9hXnUzRk">Incredibles 2 Screen Slaver</a>
  scene. It is not perfect. If you have other clips that you want this filter to
  work better on, please report them to us on our <a href="http://trac.ffmpeg.org/">trac</a>.
  </p>
  <p>
  <a href="http://ffmpeg.org/~compn/output20p8.mp4">See for yourself</a>.
  Example was made with -vf photosensitivity=20:0.8
  </p>
  <p>
  We are not professionals. Please use this in your medical studies to
  advance epilepsy research. If you decide to use this in a medical
  setting, or make a hardware hdmi input output realtime tv filter,
  or find another use for this, <a href="mailto:compn@ffmpeg.org">please let me know</a>.
  This filter was a feature request of mine
  <a href="https://trac.ffmpeg.org/ticket/2104">since 2013</a>.
  </p>

  <h3 id="pr4.2">August 5th, 2019, FFmpeg 4.2 "Ada"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.2">FFmpeg 4.2 "Ada"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>tpad filter</li>
    <li>AV1 decoding support through libdav1d</li>
    <li>dedot filter</li>
    <li>chromashift and rgbashift filters</li>
    <li>freezedetect filter</li>
    <li>truehd_core bitstream filter</li>
    <li>dhav demuxer</li>
    <li>PCM-DVD encoder</li>
    <li>GIF parser</li>
    <li>vividas demuxer</li>
    <li>hymt decoder</li>
    <li>anlmdn filter</li>
    <li>maskfun filter</li>
    <li>hcom demuxer and decoder</li>
    <li>ARBC decoder</li>
    <li>libaribb24 based ARIB STD-B24 caption support (profiles A and C)</li>
    <li>Support decoding of HEVC 4:4:4 content in nvdec and cuviddec</li>
    <li>removed libndi-newtek</li>
    <li>agm decoder</li>
    <li>KUX demuxer</li>
    <li>AV1 frame split bitstream filter</li>
    <li>lscr decoder</li>
    <li>lagfun filter</li>
    <li>asoftclip filter</li>
    <li>Support decoding of HEVC 4:4:4 content in vdpau</li>
    <li>colorhold filter</li>
    <li>xmedian filter</li>
    <li>asr filter</li>
    <li>showspatial multimedia filter</li>
    <li>VP4 video decoder</li>
    <li>IFV demuxer</li>
    <li>derain filter</li>
    <li>deesser filter</li>
    <li>mov muxer writes tracks with unspecified language instead of English by default</li>
    <li>added support for using clang to compile CUDA kernels</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr4.1">November 6th, 2018, FFmpeg 4.1 "al-Khwarizmi"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.1">FFmpeg 4.1 "al-Khwarizmi"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>deblock filter</li>
    <li>tmix filter</li>
    <li>amplify filter</li>
    <li>fftdnoiz filter</li>
    <li>aderivative and aintegral audio filters</li>
    <li>pal75bars and pal100bars video filter sources</li>
    <li>mbedTLS based TLS support</li>
    <li>adeclick and adeclip filters</li>
    <li>libtensorflow backend for DNN based filters like srcnn</li>
    <li>VC1 decoder is now bit-exact</li>
    <li>ATRAC9 decoder</li>
    <li>lensfun wrapper filter</li>
    <li>colorconstancy filter</li>
    <li>AVS2 video decoder via libdavs2</li>
    <li>IMM4 video decoder</li>
    <li>Brooktree ProSumer video decoder</li>
    <li>MatchWare Screen Capture Codec decoder</li>
    <li>WinCam Motion Video decoder</li>
    <li>1D LUT filter (lut1d)</li>
    <li>RemotelyAnywhere Screen Capture decoder</li>
    <li>cue and acue filters</li>
    <li>Support for AV1 in MP4 and Matroska/WebM</li>
    <li>transpose_npp filter</li>
    <li>AVS2 video encoder via libxavs2</li>
    <li>amultiply filter</li>
    <li>Block-Matching 3d (bm3d) denoising filter</li>
    <li>acrossover filter</li>
    <li>ilbc decoder</li>
    <li>audio denoiser as afftdn filter</li>
    <li>AV1 parser</li>
    <li>sinc audio filter source</li>
    <li>chromahold filter</li>
    <li>setparams filter</li>
    <li>vibrance filter</li>
    <li>S12M timecode decoding in h264</li>
    <li>xstack filter</li>
    <li>(a)graphmonitor filter</li>
    <li>yadif_cuda filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr4.0">April 20th, 2018, FFmpeg 4.0 "Wu"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_4.0">FFmpeg 4.0 "Wu"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streams</li>
    <li>Experimental MagicYUV encoder</li>
    <li>TiVo ty/ty+ demuxer</li>
    <li>Intel QSV-accelerated MJPEG encoding</li>
    <li>native aptX and aptX HD encoder and decoder</li>
    <li>NVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decoding</li>
    <li>Intel QSV-accelerated overlay filter</li>
    <li>mcompand audio filter</li>
    <li>acontrast audio filter</li>
    <li>OpenCL overlay filter</li>
    <li>video mix filter</li>
    <li>video normalize filter</li>
    <li>audio lv2 wrapper filter</li>
    <li>VAAPI MJPEG and VP8 decoding</li>
    <li>AMD AMF H.264 and HEVC encoders</li>
    <li>video fillborders filter</li>
    <li>video setrange filter</li>
    <li>support LibreSSL (via libtls)</li>
    <li>Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.</li>
    <li>deconvolve video filter</li>
    <li>entropy video filter</li>
    <li>hilbert audio filter source</li>
    <li>aiir audio filter</li>
    <li>Removed the ffserver program</li>
    <li>Removed the ffmenc and ffmdec muxer and demuxer</li>
    <li>VideoToolbox HEVC encoder and hwaccel</li>
    <li>VAAPI-accelerated ProcAmp (color balance), denoise and sharpness filters</li>
    <li>Add android_camera indev</li>
    <li>codec2 en/decoding via libcodec2</li>
    <li>native SBC encoder and decoder</li>
    <li>drmeter audio filter</li>
    <li>hapqa_extract bitstream filter</li>
    <li>filter_units bitstream filter</li>
    <li>AV1 Support through libaom</li>
    <li>E-AC-3 dependent frames support</li>
    <li>bitstream filter for extracting E-AC-3 core</li>
    <li>Haivision SRT protocol via libsrt</li>
    <li>vfrdet filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr3.4">October 15th, 2017, FFmpeg 3.4 "Cantor"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.4">FFmpeg 3.4 "Cantor"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>deflicker video filter</li>
    <li>doubleweave video filter</li>
    <li>lumakey video filter</li>
    <li>pixscope video filter</li>
    <li>oscilloscope video filter</li>
    <li>update cuvid/nvenc headers to Video Codec SDK 8.0.14</li>
    <li>afir audio filter</li>
    <li>scale_cuda CUDA based video scale filter</li>
    <li>librsvg support for svg rasterization</li>
    <li>crossfeed audio filter</li>
    <li>spec compliant VP9 muxing support in MP4</li>
    <li>surround audio filter</li>
    <li>sofalizer filter switched to libmysofa</li>
    <li>Gremlin Digital Video demuxer and decoder</li>
    <li>headphone audio filter</li>
    <li>superequalizer audio filter</li>
    <li>roberts video filter</li>
    <li>additional frame format support for Interplay MVE movies</li>
    <li>support for decoding through D3D11VA in ffmpeg</li>
    <li>limiter video filter</li>
    <li>libvmaf video filter</li>
    <li>Dolby E decoder and SMPTE 337M demuxer</li>
    <li>unpremultiply video filter</li>
    <li>tlut2 video filter</li>
    <li>floodfill video filter</li>
    <li>pseudocolor video filter</li>
    <li>raw G.726 muxer and demuxer, left- and right-justified</li>
    <li>NewTek NDI input/output device</li>
    <li>FITS demuxer and decoder</li>
    <li>FITS muxer and encoder</li>
    <li>despill video filter</li>
    <li>haas audio filter</li>
    <li>SUP/PGS subtitle muxer</li>
    <li>convolve video filter</li>
    <li>VP9 tile threading support</li>
    <li>KMS screen grabber</li>
    <li>CUDA thumbnail filter</li>
    <li>V4L2 mem2mem HW assisted codecs</li>
    <li>Rockchip MPP hardware decoding</li>
    <li>vmafmotion video filter</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr3.3">April 13th, 2017, FFmpeg 3.3 "Hilbert"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.3">FFmpeg 3.3 "Hilbert"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>Apple Pixlet decoder</li>
    <li>NewTek SpeedHQ decoder</li>
    <li>QDMC audio decoder</li>
    <li>PSD (Photoshop Document) decoder</li>
    <li>FM Screen Capture decoder</li>
    <li>ScreenPressor decoder</li>
    <li>XPM decoder</li>
    <li>DNxHR decoder fixes for HQX and high resolution videos</li>
    <li>ClearVideo decoder (partial)</li>
    <li>16.8 and 24.0 floating point PCM decoder</li>
    <li>Intel QSV-accelerated VP8 video decoding</li>
    <li>native Opus encoder</li>
    <li>DNxHR 444 and HQX encoding</li>
    <li>Quality improvements for the (M)JPEG encoder</li>
    <li>VAAPI-accelerated MPEG-2 and VP8 encoding</li>
    <li>premultiply video filter</li>
    <li>abitscope multimedia filter</li>
    <li>readeia608 filter</li>
    <li>threshold filter</li>
    <li>midequalizer filter</li>
    <li>MPEG-7 Video Signature filter</li>
    <li>add internal ebur128 library, remove external libebur128 dependency</li>
    <li>Intel QSV video scaling and deinterlacing filters</li>
    <li>Sample Dump eXchange demuxer</li>
    <li>MIDI Sample Dump Standard demuxer</li>
    <li>Scenarist Closed Captions demuxer and muxer</li>
    <li>Support MOV with multiple sample description tables</li>
    <li>Pro-MPEG CoP #3-R2 FEC protocol</li>
    <li>Support for spherical videos</li>
    <li>CrystalHD decoder moved to new decode API</li>
    <li>configure now fails if autodetect-libraries are requested but not found</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="gsoc2016finalreport">October 30th, 2016, Results: Summer Of Code 2016.</h3>
  <p>
    This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.
  </p>
  <p>
    Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:
  </p>
  <h4>FFv1 (Mentor: Michael Niedermayer)</h4>
  <p>
    Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.
  </p>
  <h4>Self test coverage (Mentor: Michael Niedermayer)</h4>
  <p>
    Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.
  </p>
  <h4>MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann)</h4>
  <p>
    Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.
  </p>
  <h4>Tee muxer improvements (Mentor: Marton Balint)</h4>
  <p>
    Ján Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.
  </p>
  <h4>TrueHD encoder (Mentor: Rostislav Pehlivanov)</h4>
  <p>
    Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.
  </p>
  <h4>Motion interpolation filter (Mentor: Paul B Mahol)</h4>
  <p>
    Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.
  </p>
  <p>
    And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!
  </p>
  <h3 id="sdl1">September 24th, 2016, SDL1 support dropped.</h3>
  <p>
    Support for the SDL1 library has been dropped, due to it no longer being maintained (as of
    January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device
    has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output
    devices have been updated to support SDL2.
  </p>
  <h3 id="pr3.1.2">August 9th, 2016, FFmpeg 3.1.2 "Laplace"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.1">FFmpeg 3.1.2</a>, a new point release from the 3.1 release branch, is now available!
    It fixes several bugs.
  </p>
  <p>
    We recommend users, distributors, and system integrators, to upgrade unless they use current git master.
  </p>
  <h3 id="ffserv">July 10th, 2016, ffserver program being dropped</h3>
  <p>
    After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release.
    ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat
    library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has
    been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax.
    Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs
    and to contact us so we may point users to test and contribute to its development.
  </p>
  <h3 id="pr3.1.1">July 1st, 2016, FFmpeg 3.1.1 "Laplace"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.1">FFmpeg 3.1.1</a>, a new point release from the 3.1 release branch, is now available!
    It mainly deals with a few ABI issues introduced in the previous release.
  </p>
  <p>
    We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to
    upgrade unless they use current git master.
  </p>

  <h3 id="pr3.1">June 27th, 2016, FFmpeg 3.1 "Laplace"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.1">FFmpeg 3.1 "Laplace"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li>DXVA2-accelerated HEVC Main10 decoding</li>
    <li>fieldhint filter</li>
    <li>loop video filter and aloop audio filter</li>
    <li>Bob Weaver deinterlacing filter</li>
    <li>firequalizer filter</li>
    <li>datascope filter</li>
    <li>bench and abench filters</li>
    <li>ciescope filter</li>
    <li>protocol blacklisting API</li>
    <li>MediaCodec H264 decoding</li>
    <li>VC-2 HQ RTP payload format (draft v1) depacketizer and packetizer</li>
    <li>VP9 RTP payload format (draft v2) packetizer</li>
    <li>AudioToolbox audio decoders</li>
    <li>AudioToolbox audio encoders</li>
    <li>coreimage filter (GPU based image filtering on OSX)</li>
    <li>libdcadec removed</li>
    <li>bitstream filter for extracting DTS core</li>
    <li>ADPCM IMA DAT4 decoder</li>
    <li>musx demuxer</li>
    <li>aix demuxer</li>
    <li>remap filter</li>
    <li>hash and framehash muxers</li>
    <li>colorspace filter</li>
    <li>hdcd filter</li>
    <li>readvitc filter</li>
    <li>VAAPI-accelerated format conversion and scaling</li>
    <li>libnpp/CUDA-accelerated format conversion and scaling</li>
    <li>Duck TrueMotion 2.0 Real Time decoder</li>
    <li>Wideband Single-bit Data (WSD) demuxer</li>
    <li>VAAPI-accelerated H.264/HEVC/MJPEG encoding</li>
    <li>DTS Express (LBR) decoder</li>
    <li>Generic OpenMAX IL encoder with support for Raspberry Pi</li>
    <li>IFF ANIM demuxer &amp; decoder</li>
    <li>Direct Stream Transfer (DST) decoder</li>
    <li>loudnorm filter</li>
    <li>MTAF demuxer and decoder</li>
    <li>MagicYUV decoder</li>
    <li>OpenExr improvements (tile data and B44/B44A support)</li>
    <li>BitJazz SheerVideo decoder</li>
    <li>CUDA CUVID H264/HEVC decoder</li>
    <li>10-bit depth support in native utvideo decoder</li>
    <li>libutvideo wrapper removed</li>
    <li>YUY2 Lossless Codec decoder</li>
    <li>VideoToolbox H.264 encoder</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="gsoc2016">March 16th, 2016, Google Summer of Code</h3>
  <p>
    FFmpeg has been accepted as a <a href="https://summerofcode.withgoogle.com/">Google Summer of Code</a> open source organization. If you wish to
    participate as a student see our <a href="https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2016">project ideas page</a>.
    You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft.
    Good luck!
  </p>

  <h3 id="pr3.0">February 15th, 2016, FFmpeg 3.0 "Einstein"</h3>
  <p>
    <a href="https://ffmpeg.org/download.html#release_3.0">FFmpeg 3.0 "Einstein"</a>, a new
    major release, is now available! Some of the highlights:
  </p>
  <ul>
    <li><a href="#aac_encoder_stable">The native FFmpeg AAC encoder has seen extensive improvements and is no longer considered experimental</a></li>
    <li><a href="#removing_external_aac_encoders">Removed support for libvo-aacenc and libaacplus</a></li>
    <li>Over 30 new filters have been added</li>
    <li>Many ASM optimizations</li>
    <li>VP9 Hardware Acceleration (DXVA2 and VA-API)</li>
    <li>Cineform HD decoder</li>
    <li>New DCA decoder based on libdcadec with full support for DTS-HD extensions</li>
    <li>As with all major releases expect major backward incompatible API/ABI changes</li>
    <li>See the <a href="https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=Changelog;hb=n3.0">Changelog</a> for a list of more updates</li>
  </ul>
  <p>
    We strongly recommend users, distributors, and system integrators to
    upgrade unless they use current git master.
  </p>

  <h3 id="removing_external_aac_encoders">January 30, 2016, Removing support for two external AAC encoders</h3>
  <p>
    We have just removed support for VisualOn AAC encoder (libvo-aacenc) and
    libaacplus in FFmpeg master.
  </p>
  <p>
    Even before marking our internal AAC encoder as
    <a href="#aac_encoder_stable">stable</a>, it was known that libvo-aacenc
    was of an inferior quality compared to our native one for most samples.
    However, the VisualOn encoder was used extensively by the Android Open
    Source Project, and we would like to have a tested-and-true stable option
    in our code base.
  </p>
  <p>
    When first committed in 2011, libaacplus filled in the gap of encoding
    High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported
    by any of the encoders in FFmpeg at that time.
  </p>
  <p>
    The circumstances for both have changed. After the work spearheaded by
    Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC
    encoder is ready to compete with much more mature encoders. The Fraunhofer
    FDK AAC Codec Library for Android was added in 2012 as the fourth
    supported external AAC encoder, and the one with the best quality and the
    most features supported, including HE-AAC and HE-AACv2.
  </p>
  <p>
    Therefore, we have decided that it is time to remove libvo-aacenc and
    libaacplus. If you are currently using libvo-aacenc, prepare to transition
    to the native encoder (<code>aac</code>) when updating to the next version
    of FFmpeg. In most cases it is as simple as merely swapping the encoder
    name. If you are currently using libaacplus, start using FDK AAC
    (<code>libfdk_aac</code>) with an appropriate <code>profile</code> option
    to select the exact AAC profile that fits your needs. In both cases, you
    will enjoy an audible quality improvement and as well as fewer licensing
    headaches.
  </p>
  <p>
    Enjoy!
  </p>

  <h3 id="pr2.8.5">January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10</h3>
  <p>
    We have made several new point releases (<b><a href="https://ffmpeg.org/download.html#release_2.8">2.8.5</a>,
      <a href="https://ffmpeg.org/download.html#release_2.7">2.7.5</a>,
      <a href="https://ffmpeg.org/download.html#release_2.6">2.6.7</a>,
      <a href="https://ffmpeg.org/download.html#release_2.5">2.5.10</a></b>).
    They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898.
    Please see the changelog for each release for more details.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="aac_encoder_stable">December 5th, 2015, The native FFmpeg AAC encoder is now stable!</h3>
  <p>
    After seven years the native FFmpeg AAC encoder has had its experimental flag
    removed and declared as ready for general use. The encoder is transparent
    at 128kbps for most samples tested with artifacts only appearing in extreme
    cases. Subjective quality tests put the encoder to be of equal or greater
    quality than most of the other encoders available to the public.
  </p>
  <p>
    Licensing has always been an issue with encoding AAC audio as most of the
    encoders have had a license making FFmpeg unredistributable if compiled with
    support for them. The fact that there now exists a fully open and truly
    free AAC encoder integrated directly within the project means a lot to those
    who wish to use accepted and widespread standards.
  </p>
  <p>
    The majority of the work done to bring the encoder up to quality was started
    during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov.
    Both continued to work on the encoder with the latter joining as a developer
    and mainainer, working on other parts of the project as well. Also, thanks
    to <a href="http://d.hatena.ne.jp/kamedo2/">Kamedo2</a> who does comparisons
    and tests, the original authors and all past and current contributors to the
    encoder. Users are suggested and encouraged to use the encoder and provide
    feedback or breakage reports through our <a href="https://trac.ffmpeg.org/">bug tracker</a>.
  </p>

  
  <p>
    A big thank you note goes to our newest supporters: MediaHub and Telepoint.
    Both companies have donated a dedicated server with free of charge internet
    connectivity. Here is a little bit about them in their own words:
  </p>

  <ul>
    <li>
      <p>
        <a href="http://www.telepoint.bg/en/">Telepoint</a> is the biggest
        carrier-neutral data center in Bulgaria. Located in the heart of Sofia
        on a cross-road of many Bulgarian and International networks, the
        facility is a fully featured Tier 3 data center that provides flexible
        customer-oriented colocation solutions (ranging from a server to a
        private collocation hall) and a high level of security.
      </p>
    </li>

    <li>
      <p>
        MediaHub Ltd. is a Bulgarian IPTV platform and services provider which
        uses FFmpeg heavily since it started operating a year ago. <i>"Donating
        to help keep FFmpeg online is our way of giving back to the community"
        </i>.
      </p>
    </li>
  </ul>

  <p>
    Thanks Telepoint and MediaHub for their support!
  </p>

  <h3 id="gsoc2015_result">September 29th, 2015, GSoC 2015 results</h3>

  <p>
    FFmpeg participated to the latest edition of
    the <a href="http://www.google-melange.com/gsoc/homepage/google/gsoc2015">Google
    Summer of Code</a> Project. FFmpeg got a total of 8 assigned
    projects, and 7 of them were successful.
  </p>

  <p>We want to thank <a href="https://www.google.com/">Google</a>, the
    participating students, and especially the mentors who joined this
    effort. We're looking forward to participating in the next GSoC
    edition!
  </p>

  <p>
    Below you can find a brief description of the final outcome of
    each single project.
  </p>

  <h4>Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George</h4>

  <p>
    Stephan Holljes's project for this session of Google Summer of Code was to
    implement basic HTTP server features for libavformat, to complement the
    already present HTTP client and RTMP and RTSP server code.
  </p>

  <p>
    The first part of the project was to make the HTTP code capable of accepting
    a single client; it was completed partly during the qualification period and
    partly during the first week of the summer. Thanks to this work, it is now
    possible to make a simple HTTP stream using the following commands:
  </p>

  <pre>    ffmpeg -i /dev/video0 -listen 1 -f matroska \
    -c:v libx264 -preset fast -tune zerolatency http://:8080
    ffplay http://localhost:8080/
  </pre>

  <p>
    The next part of the project was to extend the code to be able to accept
    several clients, simultaneously or consecutively. Since libavformat did not
    have an API for that kind of task, it was necessary to design one. This part
    was mostly completed before the midterm and applied shortly afterwards.
    Since the ffmpeg command-line tool is not ready to serve several clients,
    the test ground for that new API is an example program serving hard-coded
    content.
  </p>

  <p>
    The last and most ambitious part of the project was to update ffserver to
    make use of the new API. It would prove that the API is usable to implement
    real HTTP servers, and expose the points where more control was needed. By
    the end of the summer, a first working patch series was undergoing code
    review.
  </p>

  <h4>Browsing content on the server, mentee: Mariusz Szczepańczyk, mentor: Lukasz Marek</h4>

  <p>
    Mariusz finished an API prepared by the FFmpeg community and implemented
    Samba directory listing as qualification task.
  </p>

  <p>
    During the program he extended the API with the possibility to
    remove and rename files on remote servers. He completed the
    implementation of these features for file, Samba, SFTP, and FTP
    protocols.
  </p>

  <p>
    At the end of the program, Mariusz provided a sketch of an
    implementation for HTTP directory listening.
  </p>

  <h4>Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack</h4>

  <p>
    Mate was working on directshow input from digital video sources. He
    got working input from ATSC input sources, with specifiable tuner.
  </p>

  <p>
    The code has not been committed, but a patch of it was sent to the
    ffmpeg-devel mailing list for future use.
  </p>

  <p>
    The mentor plans on cleaning it up and committing it, at least for the
    ATSC side of things. Mate and the mentor are still working trying to
    finally figure out how to get DVB working.
  </p>

  <h4>Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale</h4>

  <p>
    Niklesh's project was to expand our support for 3GPP Timed Text
    subtitles. This is the native subtitle format for mp4 containers, and
    is interesting because it's usually the only subtitle format supported
    by the stock playback applications on iOS and Android devices.
  </p>

  <p>
    ffmpeg already had basic support for these subtitles which ignored all
    formatting information - it just provided basic plain-text support.
  </p>

  <p>
    Niklesh did work to add support on both the encode and decode side for
    text formatting capabilities, such as font size/colour and effects like
    bold/italics, highlighting, etc.
  </p>

  <p>
    The main challenge here is that Timed Text handles formatting in a very
    different way from most common subtitle formats. It uses a binary
    encoding (based on mp4 boxes, naturally) and stores information
    separately from the text itself. This requires additional work to track
    which parts of the text formatting applies to, and explicitly dealing
    with overlapping formatting (which other formats support but Timed
    Text does not) so it requires breaking the overlapping sections into
    separate non-overlapping ones with different formatting.
  </p>

  <p>
    Finally, Niklesh had to be careful about not trusting any size
    information in the subtitles - and that's no joke: the now infamous
    Android stagefright bug was in code for parsing Timed Text subtitles.
  </p>

  <p>
    All of Niklesh's work is committed and was released in ffmpeg 2.8.
  </p>

<h4>libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla</h4>

  <p>
    Pedro Arthur has modularized the vertical and horizontal scalers.
    To do this he designed and implemented a generic filter framework
    and moved the existing scaler code into it. These changes now allow
    easily adding removing, splitting or merging processing steps.
    The implementation was benchmarked and several alternatives were
    tried to avoid speed loss.
  </p>

  <p>
    He also added gamma corrected scaling support.
    An example to use gamma corrected scaling would be:
  </p>

  <pre>    ffmpeg -i input -vf scale=512:384:gamma=1 output
  </pre>

  <p>
    Pedro has done impressive work considering the short time available,
    and he is a FFmpeg committer now. He continues to contribute to
    FFmpeg, and has fixed some bugs in libswscale after GSoC has
    ended.
  </p>

  <h4>AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire</h4>

  <p>
    Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main
    prediction on the native AAC encoder. Of all those extensions, only
    TNS was left in a less-than-usable state, but the implementation has
    been pushed (disabled) anyway since it's a good basis for further
    improvements.
  </p>

  <p>
    PNS replaces noisy bands with a single scalefactor representing the
    energy of that band, gaining in coding efficiency considerably, and
    the quality improvements on low bitrates are impressive for such a
    simple feature.
  </p>

  <p>
    TNS still needs some polishing, but has the potential to reduce coding
    artifacts by applying noise shaping in the temporal domain (something
    that is a source of annoying, notable distortion on low-entropy
    bands).
  </p>

  <p>
    Intensity Stereo coding (I/S) can double coding efficiency by
    exploiting strong correlation between stereo channels, most effective
    on pop-style tracks that employ panned mixing. The technique is not as
    effective on classic X-Y recordings though.
  </p>

  <p>
    Finally, main prediction improves coding efficiency by exploiting
    correlation among successive frames. While the gains have not been
    huge at this point, Rostislav has remained active even after the GSoC,
    and is polishing both TNS and main prediction, as well as looking for
    further improvements to make.
  </p>

  <p>
    In the process, the MIPS port of the encoder was broken a few times,
    something he's also working to fix.
  </p>

  <h4>Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol</h4>

  <p>
    Donny Yang implemented basic keyframe only APNG encoder as the
    qualification task. Later he wrote interframe compression via
    various blend modes. The current implementation tries all blend
    modes and picks one which takes the smallest amount of memory.
  </p>

  <p>
    Special care was taken to make sure that the decoder plays
    correctly all files found in the wild and that the encoder
    produces files that can be played in browsers that support APNG.
  </p>

  <p>
    During his work he was tasked to fix any encountered bug in the
    decoder due to the fact that it doesn't match APNG
    specifications. Thanks to this work, a long standing bug in the
    PNG decoder has been fixed.
  </p>

  <p>
    For latter work he plans to continue working on the encoder,
    making it possible to select which blend modes will be used in the
    encoding process. This could speed up encoding of APNG files.
  </p>

  <h3 id="pr2.8">September 9th, 2015, FFmpeg 2.8</h3>
  <p>
    We published release <b><a href="https://ffmpeg.org/download.html#release_2.8">2.8</a></b> as new major version.
    It contains all features and bug fixes of the git master branch from September 8th. Please see
    the <b><a href="https://raw.githubusercontent.com/FFmpeg/FFmpeg/release/2.8/Changelog">changelog</a></b>
    for a list of the most important changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use current git master.
  </p>

  <h3 id="message">August 1st, 2015, A message from the FFmpeg project</h3>
  <p>
    Dear multimedia community,
  </p>
  <p>
    The resignation of Michael Niedermayer as leader of FFmpeg yesterday has
    come by surprise. He has worked tirelessly on the FFmpeg project for many
    years and we must thank him for the work that he has done. We hope that in
    the future he will continue to contribute to the project. In the coming
    weeks, the FFmpeg project will be managed by the active contributors.
  </p>
  <p>
    The last four years have not been easy for our multimedia community - both
    contributors and users. We should now look to the future, try to find
    solutions to these issues, and to have reconciliation between the forks,
    which have split the community for so long.
  </p>
  <p>
    Unfortunately, much of the disagreement has taken place in inappropriate
    venues so far, which has made finding common ground and solutions
    difficult. We aim to discuss this in our communities online over the coming
    weeks, and in person at the <a href="https://www.videolan.org/videolan/events/vdd15/">VideoLAN Developer
    Days</a> in Paris in September: a neutral venue for the entire open source
    multimedia community.
  </p>
  <p>
    The FFmpeg project.
  </p>

  <h3 id="needhost">July 4th, 2015, FFmpeg needs a new host</h3>
  <p><b>UPDATE:</b> We have received more than 7 offers for hosting and servers, thanks a lot to everyone!</p>
  <p>
    After graciously hosting our projects (<a href="http://www.ffmpeg.org/">FFmpeg</a>, <a href="http://www.mplayerhq.hu/">MPlayer</a>
    and <a href="http://rtmpdump.mplayerhq.hu/">rtmpdump</a>) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.
  </p>
  <p>
    If you want to host an open source project, please let us know, either on <a href="http://ffmpeg.org/mailman/listinfo/ffmpeg-devel">ffmpeg-devel</a>
    mailing list or irc.freenode.net #ffmpeg-devel.
  </p>
  <p>
    We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, <a href="http://trac.ffmpeg.org/">trac</a>, <a href="http://samples.ffmpeg.org/">samples repo</a>, svn, etc.
  </p>

  <h3 id="pr2.6.1">March 16, 2015, FFmpeg 2.6.1</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.6">2.6</a></b>)
    and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March.
    Please see the <b><a href="http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.6">Release Notes</a></b> for a
    list of note-worthy changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="gsoc2015">March 4, 2015, Google Summer of Code</h3>
  <p>
    FFmpeg has been accepted as a <a href="http://www.google-melange.com/gsoc/homepage/google/gsoc2015">Google Summer of Code</a> Project. If you wish to
    participate as a student see our <a href="https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2015">project ideas page</a>.
    You can already get in contact with mentors and start working on qualification tasks. Registration
    at Google for students will open March 16th. Good luck!
  </p>

  <h3 id="clt2015">March 1, 2015, Chemnitzer Linux-Tage</h3>
  <p>
    We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage
    (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.
  </p>

  <p>
    More information can be found <a href="https://chemnitzer.linux-tage.de/2015/en/">here</a>
  </p>

  <p>
    We demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes. <strong>If you have media files that cannot be
    processed correctly with FFmpeg, be sure to have a sample with you
    so we can have a look!</strong>
  </p>
  <p>
    For the first time in our CLT history, there will be an <strong>FFmpeg workshop</strong>!
    You can read the details <a href="https://chemnitzer.linux-tage.de/2015/de/programm/beitrag/209">here</a>.
    The workshop is targeted at FFmpeg beginners. First the basics of
    multimedia will be covered. Thereafter you will learn how to use
    that knowledge and the FFmpeg CLI tools to analyse and process media
    files. The workshop is in German language only and prior registration
    is necessary. The workshop will be on Saturday starting at 10 o'clock.
  </p>
  <p>
    We are looking forward to meet you (again)!
  </p>

  <h3 id="pr2.5">December 5, 2014, FFmpeg 2.5</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.5">2.5</a></b>)
    It contains all features and bugfixes of the git master branch from the 4th December.
    Please see the <b><a href="http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.5">Release Notes</a></b> for a
    list of note-worthy changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="ffmpeg_back_in_sid">October 10, 2014, FFmpeg is in Debian unstable again</h3>
  <p>
    We wanted you to know there are
    <a href="https://packages.debian.org/search?keywords=ffmpeg&amp;searchon=sourcenames&amp;suite=unstable&amp;section=main">
    FFmpeg packages in Debian unstable</a> again. <strong>A big thank-you
    to Andreas Cadhalpun and all the people that made it possible.</strong> It has been anything but simple.
  </p>
  <p>
    Unfortunately that was already the easy part of this news. The bad news is the packages probably won't
    migrate to Debian testing to be in the upcoming release codenamed jessie.
    <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=763148">Read the argumentation over at Debian.</a>
  </p>
  <p>
    <strong>However things will come out in the end, we hope for your continued remarkable support!</strong>
  </p>

  <h3 id="opw03">October 8, 2014, FFmpeg secured a place in OPW!</h3>
  <p>
    Thanks to a generous 6K USD donation by Samsung (Open Source Group),
    FFmpeg will be welcoming at least 1 "Outreach Program for Women" intern
    to work with our community for an initial period starting December 2014
    (through March 2015).
  </p>

  <p>
    We all know FFmpeg is used by the industry, but even while there are
    countless products building on our code, it is not at all common for
    companies to step up and help us out when needed. So a big thank-you
    to Samsung and the OPW program committee!
  </p>

  <p>
    If you are thinking on participating in OPW as an intern, please take
    a look at our <a href="https://trac.ffmpeg.org/wiki/SponsoringPrograms/OPW/2014-12">OPW wiki page</a>
    for some initial guidelines. The page is still a work in progress, but
    there should be enough information there to get you started. If you, on
    the other hand, are thinking on sponsoring work on FFmpeg through the
    OPW program, please get in touch with us at opw@ffmpeg.org. With your
    help, we might be able to secure some extra intern spots for this round!
  </p>

  <h3 id="pr2.4">September 15, 2014, FFmpeg 2.4</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.4">2.4</a></b>)
    It contains all features and bugfixes of the git master branch from the 14th September.
    Please see the <b><a href="http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.4">Release Notes</a></b> for a
    list of note-worthy changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="pr2.3.3">August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8</h3>
  <p>
    We have made several new point releases (<b><a href="https://ffmpeg.org/download.html#release_2.3">2.3.3</a>,
      <a href="https://ffmpeg.org/download.html#release_2.2">2.2.7</a>,
      <a href="https://ffmpeg.org/download.html#release_1.2">1.2.8</a></b>).
    They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272.
    Please see the changelog for more details.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="opw02">July 29, 2014, Help us out securing our spot in OPW</h3>
  <p>
    Following our previous post regarding our participation on this year's
    OPW (Outreach Program for Women), we are now reaching out to our users
    (both individuals and companies) to help us gather the needed money to
    secure our spot in the program.<br>
    We need to put together 6K USD as a minimum but securing more funds would
    help us towards getting more than one intern.<br>
    You can donate by credit card using
    <a href="https://co.clickandpledge.com/advanced/default.aspx?wid=56226">
    Click&amp;Pledge</a> and selecting the "OPW" option. If you would like to
    donate by money transfer or by check, please get in touch by
    <a href="mailto:opw@ffmpeg.org">e-mail</a> and we will get back to you
    with instructions.<br>Thanks!
  </p>

  <h3 id="newweb">July 20, 2014, New website</h3>
  <p>
    The FFmpeg project is proud to announce a brand new version of the website
    made by <a href="http://db0.fr/">db0</a>. While this was initially motivated
    by the need for a larger menu, the whole website ended up being redesigned,
    and most pages got reworked to ease navigation. We hope you'll enjoy
    browsing it.
  </p>

  <h3 id="pr2.3">July 17, 2014, FFmpeg 2.3</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.3">2.3</a></b>)
    It contains all features and bugfixes of the git master branch from the 16th July.
    Please see the <b><a href="http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=489d066">Release Notes</a></b> for a
    list of note-worthy changes.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="opw01">July 3, 2014, FFmpeg and the Outreach Program For Women</h3>
  <p>
    FFmpeg has started the process to become an OPW includer organization for the
    next round of the program, with internships starting December 9. The
    <a href="https://gnome.org/opw/">OPW</a> aims to "Help women (cis and trans)
    and genderqueer to get involved in free and open source software". Part of the
    process requires securing funds to support at least one internship (6K USD), so
    if you were holding on your donation to FFmpeg, this is a great chance for you
    to come forward, get in touch and help both the project and a great initiative!
  </p>
  <p>
    We have set up an <a href="mailto:opw@ffmpeg.org">email address</a> you can use
    to contact us about donations and general inquires regarding our participation
    in the program. Hope to hear from you soon!
  </p>

  <h3 id="pr2.2.4">June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14</h3>
  <p>
    We have made several new point releases (<b><a href="https://ffmpeg.org/download.html#release_2.2">2.2.4</a>,
      <a href="https://ffmpeg.org/download.html#release_2.1">2.1.5</a>,
      <a href="https://ffmpeg.org/download.html#release_2.0">2.0.5</a>,
      <a href="https://ffmpeg.org/download.html#release_1.2">1.2.7</a>,
      <a href="https://ffmpeg.org/download.html#release_1.1">1.1.12</a>,
      <a href="https://ffmpeg.org/download.html#release_0.10">0.10.14</a></b>).
    They fix a
    <a href="http://blog.securitymouse.com/2014/06/raising-lazarus-20-year-old-bug-that.html">security issue in the LZO implementation</a>,
    as well as several other bugs. See the git log for details.
  </p>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>


  <h3 id="lt2014">May 1, 2014, LinuxTag</h3>
  <p>
    Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will
    take place from 8th to 10th of May. Please note that this year's LinuxTag is at a
    different location closer to the city center.
  </p>

  <p>
    We will have a shared booth with XBMC and VideoLAN.
    <b>
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    </b>
  </p>

  <p>
    More information about LinuxTag can be found <a href="http://www.linuxtag.org/2014/">here</a>
  </p>

  <p>
    We are looking forward to see you in Berlin!
  </p>

  <h3 id="heartbleed">April 18, 2014, OpenSSL Heartbeat bug</h3>
  <p>
    Our server hosting the Trac issue tracker was vulnerable to the attack
    against OpenSSL known as "heartbleed". The OpenSSL software library
    was updated on 7th of April, shortly after the vulnerability was publicly
    disclosed. We have changed the private keys (and certificates) for all
    FFmpeg servers. The details were sent to the mailing lists by
    Alexander Strasser, who is part of the project server team. Here is a
    link to the user mailing list
    <a href="https://lists.ffmpeg.org/pipermail/ffmpeg-user/2014-April/020968.html">archive</a>
    .
  </p><p>
    We encourage you to read up on
    <a href="https://www.schneier.com/blog/archives/2014/04/heartbleed.html">"OpenSSL heartbleed"</a>.
    <b>It is possible that login data for the issue tracker was exposed to
      people exploiting this security hole. You might want to change your password
      in the tracker and everywhere else you used that same password.</b>
  </p>

  <h3 id="pr2.2.1">April 11, 2014, FFmpeg 2.2.1</h3>
  <p>
    We have made a new point releases (<b><a href="https://ffmpeg.org/download.html#release_2.2">2.2.1</a></b>).
    It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as
    several other fixes.
    See the git log for details.
  </p>

  <h3 id="pr2.2">March 24, 2014, FFmpeg 2.2</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.2">2.2</a></b>)
    It contains all features and bugfixes of the git master branch from 1st March.
    A partial list of new stuff is below:
  </p>
  <pre>    - HNM version 4 demuxer and video decoder
    - Live HDS muxer
    - setsar/setdar filters now support variables in ratio expressions
    - elbg filter
    - string validation in ffprobe
    - support for decoding through VDPAU in ffmpeg (the -hwaccel option)
    - complete Voxware MetaSound decoder
    - remove mp3_header_compress bitstream filter
    - Windows resource files for shared libraries
    - aeval filter
    - stereoscopic 3d metadata handling
    - WebP encoding via libwebp
    - ATRAC3+ decoder
    - VP8 in Ogg demuxing
    - side &amp; metadata support in NUT
    - framepack filter
    - XYZ12 rawvideo support in NUT
    - Exif metadata support in WebP decoder
    - OpenGL device
    - Use metadata_header_padding to control padding in ID3 tags (currently used in
    MP3, AIFF, and OMA files), FLAC header, and the AVI "junk" block.
    - Mirillis FIC video decoder
    - Support DNx444
    - libx265 encoder
    - dejudder filter
    - Autodetect VDA like all other hardware accelerations
  </pre>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  <h3 id="clt2014">February 3, 2014, Chemnitzer Linux-Tage</h3>
  <p>
    We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage'
    in Chemnitz, Germany. The event will take place on 15th and 16th of March.
  </p>

  <p>
    More information can be found <a href="http://chemnitzer.linux-tage.de/2014/en/info/">here</a>
  </p>

  <p>
    We invite you to visit us at our booth located in the Linux-Live area!
    There we will demonstrate usage of FFmpeg, answer your questions and listen to
    your problems and wishes.
  </p>
  <p>
    <b>
      If you have media files that cannot be processed correctly with
      FFmpeg, be sure to have a sample with you so we can have a look!
    </b>
  </p>
  <p>
    We are looking forward to meet you (again)!
  </p>


  <h3 id="trac_sec">February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach</h3>
  <p>
    The server on which FFmpeg and MPlayer Trac issue trackers were
    installed was compromised. The affected server was taken offline
    and has been replaced and all software reinstalled.
    FFmpeg Git, releases, FATE, web and mailinglists are on other servers
    and were not affected. We believe that the original compromise happened
    to a server, unrelated to FFmpeg and MPlayer, several months ago.
    That server was used as a source to clone the VM that we recently moved
    Trac to. It is not known if anyone used the backdoor that was found.
  </p>
  <p>
    We recommend all users to change their passwords.
    <b>Especially users who use a password on Trac that they also use
      elsewhere, should change that password at least elsewhere.</b>
  </p>


  <h3 id="ffmpeg_rfp">November 12, 2013, FFmpeg RFP in Debian</h3>
  <p>
    Since the splitting of Libav the Debian/Ubuntu maintainers have followed
    the Libav fork. Many people have requested the packaging of ffmpeg in
    Debian, as it is more feature-complete and in many cases less buggy.
  </p>
  <p>
    <a href="http://cynic.cc/blog/">Rogério Brito</a>, a Debian developer,
    has proposed a Request For Package (RFP) in the Debian bug tracking
    system.
  </p>
  <p>
    Please let the Debian and Ubuntu developers know that you support packaging
    of the real FFmpeg! See Debian <a href="http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=729203">ticket #729203</a>
    for more details.
  </p>

  <h3 id="pr2.1">October 28, 2013, FFmpeg 2.1</h3>
  <p>
    We have made a new major release (<b><a href="https://ffmpeg.org/download.html#release_2.1">2.1</a></b>)
    It contains all features and bugfixes of the git master branch from 28th October.
    A partial list of new stuff is below:
  </p>
  <pre>    - aecho filter
    - perspective filter ported from libmpcodecs
    - ffprobe -show_programs option
    - compand filter
    - RTMP seek support
    - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate
    even when used as an input option. Previous behavior can be restored with
    the -noaccurate_seek option.
    - ffmpeg -t option can now be used for inputs, to limit the duration of
    data read from an input file
    - incomplete Voxware MetaSound decoder
    - read EXIF metadata from JPEG
    - DVB teletext decoder
    - phase filter ported from libmpcodecs
    - w3fdif filter
    - Opus support in Matroska
    - FFV1 version 1.3 is stable and no longer experimental
    - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support
    - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be
    more consistent with other muxers.
    - adelay filter
    - pullup filter ported from libmpcodecs
    - ffprobe -read_intervals option
    - Lossless and alpha support for WebP decoder
    - Error Resilient AAC syntax (ER AAC LC) decoding
    - Low Delay AAC (ER AAC LD) decoding
    - mux chapters in ASF files
    - SFTP protocol (via libssh)
    - libx264: add ability to encode in YUVJ422P and YUVJ444P
    - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does
    - make decoding alpha optional for prores, ffv1 and vp6 by setting
    the skip_alpha flag.
    - ladspa wrapper filter
    - native VP9 decoder
    - dpx parser
    - max_error_rate parameter in ffmpeg
    - PulseAudio output device
    - ReplayGain scanner
    - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support)
    - Linux framebuffer output device
    - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4
    - mergeplanes filter
  </pre>
  <p>
    We recommend users, distributors and system integrators to upgrade unless they use
    current git master.
  </p>

  

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A guide to Gen AI / LLM vibecoding for expert programmers (117 pts)]]></title>
            <link>https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/</link>
            <guid>44985207</guid>
            <pubDate>Fri, 22 Aug 2025 14:37:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/">https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/</a>, See on <a href="https://news.ycombinator.com/item?id=44985207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-1877">
                <p>I get it, you’re too good to vibe code. You’re a senior developer who has been doing this for 20 years and knows the system like the back of your hand. Or maybe you’re the star individual contributor who is the only person who can ever figure out how to solve the hard problems. Or maybe you’re the professor who created the entire subject of the algorithms you’re implementing. I don’t know you, but I do know that you think you’re too good to vibe code. And guess what, you’re absolutely and totally wrong.</p>
<p>Facetious? Maybe… but I will go even further.</p>
<p>No, you’re not too good to vibe code. In fact, you’re the only person who should be vibe coding.</p>
<p>I would have thought this statement was crazy just a month ago because this label of “expert” coder also applies to me. Just to establish some street cred here, <a href="https://github.com/ChrisRackauckas">I am the maintainer of over 200 Github packages, totaling over 23,000 stars</a>, am <a href="https://julia.mit.edu/">Co-PI of a CS Lab at MIT</a>, was the <a href="https://pumas.ai/">founding architect at one pretty successful tech startup</a> and am VP leading <a href="https://juliahub.com/products/dyad">architecting Dyad in another domain</a>. I clearly know how to program, would leave snide remarks at students and interns when their code was clearly created by LLMs, and was pretty publicly against all of this because these bots are too stupid to know what “correct” even means. </p>
<p>But I started picking up this “vibe coding” about a month ago and I found out that it can be a really powerful tool, in the right circumstances and in the right workflow. For the record, I now have about 32 Claude agents continuously running in tmux windows that I can ssh to, so all day long I can just check via laptop or phone and keep plugging along. This was completely unheard of a month ago, but it’s here.</p>
<p>This is the expert’s guide to vibe coding for those who are scoffing at those kids who don’t know what they are doing, but also want to start doing it correctly.</p>
<h2>A Mental Model for LLM Agents: Your Sophomore Year Student/Intern</h2>
<p>Drop the hype, I’m not here to sell you a ChatGPT so I’m not going to tell you it’s PhD level when it 100% absolutely clearly isn’t to anyone who has ever met a PhD in their life. But it is something, what is it?</p>
<p>Think about an LLM agent as a dedicated intern, or a student who is around the proficiency of a sophomore in college. They know the basics of what programming looks like, they can copy other ideas and architectures, they know how to do things like run unit tests, and they know how to Google things. They have had their basic programming course, and probably have done a deep dive into some random subject as a higher level course, but if you quiz them on the topic enough you’ll learn they haven’t actually learned it deeply. The kid seems smart enough, you’d give them a shot.</p>
<p>If this was a person who showed up to your office looking for work, what would you do with them? Generally you would do one of two things. First, you could sandbox their work. Now the reason you sandbox the work of a new student or intern is rather simple: it’s because you don’t know a new subject/tool well, you want to give it a try, and ehh why not let’s see what happens. If you took the sandbox route, you probably aren’t caring about the code (it’s likely to be unmaintainable and bit rot anyways if it’s not in your core repos), it’s about getting the artifact. You vibe code a bit, “that looks cool!”, throw it into a demo / LinkedIn post, and then move on. That’s the simple vibe coding you may have already tried and thought “that’s not useful enough”. It works, but that’s not what we’re here for, so no need to mention that more in the blog.</p>
<p>The second path, in the complete opposite direction, you would integrate the student/intern into a project you know well because it makes it very easy for you to review their work: you can give clear feedback because you’ve already made the first 10 mistakes they will make, you already know how to tell them what is next, and you have the first 6 months of the project planned out for them so it’s low maintenance. This is how you would train most people that you want to stay long term, right? In the same way, this is the path to take with the LLM agents.</p>
<p>So let’s walk through this process step by step.</p>
<h3>Major Point: Vibe coding turns everyone into a team lead, but not everyone should be a team lead</h3>
<p>Leading a team of programmers is hard! It takes time, skill, and patience. I think everyone when they are a kid thinks “I don’t want to be the worker, I want to be the boss and I just sit in my chair and tell people what to do and boom it all gets done!”. But after your second group project in college, you pretty quickly realize that if you lead a team wrong, you instead just end up doing all of the work yourself while having the expectation of 4 people. </p>
<p>Now there’s a few reasons for this. One issue with trying to establish team programming is that you may just not know the subject well enough. If it takes you a while to understand the subject, what someone is trying to do, and what their code is about, then it’s just not worth the time to manage someone else. You need to be at a point where you can very quickly see the code, understand what’s going on, and say “I can’t merge this until you have tests for X and Y, and also show me a plot of Z so I know how it all relates”. If you cannot instantly see that kind of feedback, then you probably aren’t experienced enough to lead. You need to write a few million lines of code before it becomes automatic where you just look at code and say “don’t do that, that’ll be a performance bottleneck”, but you need that quickness to the code review in order for vibe coding to work. But also, let me say this bluntly:</p>
<h4>If you are an individual contributor who usually does not like to train interns because you find that they take more time than they are helpful, then don’t vibe code</h4>
<p>Some people tend to do better at working by directing. Other really smart people just can’t seem to get good work out of other people. It’s not an indictment, Silicon Valley created the “individual contributor” role for a reason. If you are one of those people, then vibe coding may not be for you as you will likely grow frustrated with the agents even quicker than you would a human (they somehow retain less information than even the worst intern, at least they remember your favorite lunch order).</p>
<p>So go in with this mindset: I will have to have meetings with the agents, I will need to plan and give them tasks, and I will need to review the code. If I find this stuff to be slower than coding myself, then just stop right now. But if you do well with a team, then go on. How do we now make this team effective?</p>
<h2>What is the workflow of vibe coding correctly?</h2>
<p>If someone shows you Claude Code and you ask it to try and solve the problem that you were just working on (obviously a hard problem, because if it ends up on your desk that means someone else failed to solve it), you just poke and laugh at Claude when it fails miserably. But you would have never done that with a new intern or student (hopefully), so why do this? Again, you know how smart it actually is, cut the hype, and treat it the same way. This immediately leads to a few workflow principles:</p>
<h3>The workflow of vibe coding is the same as managing scrums or helping a student through a research thesis</h3>
<p>You have a problem, you give it to the agent, you review the results, and then you give it feedback. This is exactly how you would manage a scrum team or help a student through their thesis. You don’t just give them the problem and expect them to solve it, you give them the problem, they come back with a solution, and then you tell them what to do next.</p>
<p>You probably already get most of your work done this way if you’re senior enough. Every professor has more students coding than themselves, and every senior developer has a total amount of code created by their team that is far greater than their own. Just think of Claude as your pack of newbies that just started. Now if you’re thinking “but it can be difficult to manage a bunch of newbies”… yeah, that means you’re actually senior enough to understand how to do this right. It’s fairly easy to send a new intern or student off on a project, and if their pay/grade depends on it getting done they will give you something back. Whether it’s any good depends on how well you chunked up the work for them and gave them an appropriate task. </p>
<p>But one key thing is, if you had to do a meeting every 10 minutes it would drive you crazy, so don’t. Set up say 12-32 agents running on different processes, preferably sandboxed on some other compute resource (sandboxed so they can’t break the machine, but also so they can have a Github authentication that does not have core read/write privileges. This way you can tell it to have “dangerously unsafe permissions” and the worst that happens is it segfaults its own docker container and never opens a PR). Give it a full command:</p>
<p>“try solving (an easy issue in this open source repository). Create a PR with the solution, and after an hour check the continuous integration to see if tests are passing. If tests are not passing, assess what the issue is, and if it is a quick fix make a commit to handle it, otherwise report what the core difficulty of the problem is”</p>
<h4>Don’t spend too much time setting up the calls, just pull from lists you already have and let it find “whatever is easy”</h4>
<p>Make it clear, make it easy, make it know the steps, and let it just keep cycling for a bit.</p>
<h3>How to review vibe coding: immediately throw out anything bad</h3>
<p>If you saw a student was cheating and just copy-pasted from StackOverflow but couldn’t explain what it did, you’d throw it out and tell them to try again. If your new intern didn’t reuse all of the solid code your team had written and instead rewrote some low level detail in a buggy and unmaintainable way, you’d throw it out and tell them to try again. If they wrote a function that was 500 lines long and did 10 different things, you’d throw it out and tell them to try again. You wouldn’t waste your time trying to fix it, you’d just tell them to try again.</p>
<p>Again, treat the LLMs the same way. I see a lot of people following the mindset they see the vibe coding YouTubers making their silly games. “ChatGPT, try harder! Fix for me!”. You want to know a secret? That stuff is worse than worthless. The problem is that these LLMs are made to please you, so if you tell them to try harder, they will either start hallucinating or just start changing your tests. Don’t even give it a try. The moment you see it go off the rails, just throw it out. That problem is too hard for Claude, it’s for you now.</p>
<p>Send a bunch of commands at 9am. At noon, check on them. You might have 10 done. 8 of them probably went off the rails, whatever, fire them. Hey two PRs worked, whoopee! Fire 10 more, come back at 3. 20 done, 4 successes and 16 failures. Fire a few more off, maybe a few clean up ones to look for missing docstrings or dig around to see if any performance regressions were introduced. At 6, see the other 4 successes and cut the other jobs. </p>
<h4>Vibe coding is useful only if you have enough problems that you’re happy that some subset being solved, not caring what in that subset is solved.</h4>
<p>10 PRs were merged, plus whatever you were working on that day (yes, because you didn’t focus on this for most of your day!). You might think, that’s like 10/40 = 25% success rate, that’s not good. But you know what? Those were free. You just got a lot of extra stuff done that you wouldn’t have otherwise. The success rate is just a matter of how much these things give value for their cost. That’s for Sam Altman to worry about. But if you have a subscription to these LLMs, just keep burning through the tokens who cares. Don’t worry about success rate, just go for total successes.</p>
<h3>Where to apply vibe coding: code you know very well</h3>
<p>So this leads to a very counter-intuitive fact that may come out of left field, but I’m serious. Everyone’s first inclination is to throw it on some project they haven’t actually contributed to and get banned (okay, maybe it just looks like that to open source maintainers). But the real issue is that, the majority of your time will be spent doing code review. If you do this on code you don’t know well, you will have to spend a lot of time trying to understand the code and at that point, why not just write the code yourself?</p>
<p>This is where most people seem to just stop and drop the idea of vibe coding all together. But instead… what about applying it to the code base you’re on? No, not on the hard problems you’re thinking about, but all of those little side problems? The small refactor you put off for the last 6 months? What about bisecting the Git commits to find the exact cause of the performance regression that showed up on master a week ago? Or you created a version specialized for Windows and Mac but left a “todo” over the Linux section because it’s easy but would be 4 hours of monotonous work? All of those things, if someone showed up with the code, you could review it in about 5 minutes and know whether it’s right or wrong. Give the agents that stuff!</p>
<h4>Vibe coding is not useful if you need it to solve a particular problem. You still do the hard stuff</h4>
<p>In just the same way, the best place to put trainees is in the project that you already know well because that makes it easy to review their work. It’s the “I don’t have time for you, so try this easy task” approach. You know the code, you know the problem, and you can give them a task that is easy enough that they can do it without too much help. This is the same principle here.</p>
<h2>Some Examples of Vibe Coding PRs</h2>
<p>Now let’s look at some of the examples <a href="https://github.com/ChrisRackauckas-claude">my bot account has been putting out</a>.</p>
<h3>Example 1: The Simple Success Story</h3>
<p><a href="https://github.com/SciML/OrdinaryDiffEq.jl/pull/2856">Here’s a quick and simple PR</a>, the kind that is perfect here. If you don’t know performance Julia handling or trim, basically it’s a new feature in Julia v1.12 where Julia can now build <a href="https://www.youtube.com/watch?v=R0DEG-ddBZA">small lean binaries</a>. In order to do that, you need to make sure functions fully specialize, which they don’t by default as that would create a lot of extra compilation in many circumstances, but for higher order numerical solvers that is the behavior we want. So I told it to go specialize all instances of the function in the package, and I could check the PR fairly quickly and see it stuck to the goals and did it. This is then going to be followed up with new tooling that will perform static checks of trimming compatibility (<a href="https://github.com/SciML/NonlinearSolve.jl/pull/665">still being worked out</a>), but with just those backwards compatible minor changes things seem to work in the beta, so merge now and add those tests when we have a good system for it.</p>
<p>1 minute to write the query, come back later and 1 minute to review.</p>
<p>This is exactly the kind of small targeted change these are geared towards. Most of the PRs aim to be like this.</p>
<h4>Even if you work on hard stuff, a huge chunk of your work isn’t hard stuff. There’s a lot of simple janitorial work you have to do on your code all of the time. Automate that part.</h4>
<h3>Example 2: The Immediately Closed “That’s not for Claude” PR</h3>
<p><a href="https://github.com/SciML/SciMLSensitivity.jl/pull/1266">This PR</a> came from pointing it at the fact that every once in awhile I get a test failure in the docs build for a chaotic ODE differentiation w.r.t. ergodic properties tutorial. <a href="https://frankschae.github.io/post/shadowing/">It is a very fun topic</a>, but generally anything with real math in it is too hard for the LLMs. And in this, yeah I could see immediately that this PR does not make sense… well it did. The NaN’s and Infs were definitely coming from a numerical issue in the least squares shadowing code, and what this pointed to was the Schur complement was being done with things like B * Diagonal(wBinv) * B’ which as a numerical analyst I can immediately see would double the condition number of the matrix, but there doesn’t seem to be an immediate solution with open source linear algebra things I could find. So closed this, sent a note over to <a href="https://math.mit.edu/directory/profile.html?pid=63">Alan Edelman</a> to try and figure out what the better way to do this factorization. While it didn’t solve the problem, at least I know what the problem is now.</p>
<p>This is probably what most of the PRs become. It gives a hint of where the problem is, and then I take the reins.</p>
<h3>Example 3: Repeated Refactors</h3>
<p><a href="https://github.com/SciML/NonlinearSolve.jl/pull/672">Is a sweet and simple PR that refactors the tests to move some things</a>, specifically the Enzyme automatic differentiation engine testing, to a “no pre” set. The “no pre” means “does not run on prereleases of the next language version”, since these tools touch language internals in the compiler so they are never ready early. This always make prerelease tests fail before they actually test anything meaningful, so I wanted to move all Enzyme usage to a “no pre” set in every repo it showed up. </p>
<p>About 5 minutes to write the query. Some of the test suites needed a simple Github suggestion to fix up a little detail here or there. About 5 minutes to get this thing into 8 repos. Now I was ready to start using prerelease tests. Would’ve been at least a half hour by hand just because we didn’t have an easy system for doing this before. Maybe that’s a little dirtier than the perfect regex, but whatever 10 minutes of my time sounds like a win.</p>
<p>Refactors generally work out really well and are one of the top uses for the tool. “make it correct, write good tests, and let it refactor” is generally a lazy way to get 90% of the way there.</p>
<h3>Example 4: The Information Gathering PR</h3>
<p><a href="https://github.com/SciML/LinearSolve.jl/pull/734">Here is a pull request</a> that was generated by pointing it to solve <a href="https://github.com/SciML/LinearSolve.jl/issues/593">this issue</a>. That issue was mostly chosen because it was sitting on the issue list for awhile and it didn’t seem so difficult but I hadn’t had the time to track down the memory leak in a not so widely used extension for an alternative C-based sparse matrix solver, but it needed to get done some time. So, throw the bot on it.</p>
<p>And what it comes back with is to add a memory finalizer (i.e. how to tell the GC how to remove the memory) for the other library. I could take one look at it and immediately see that kind of code should not live in this library, it should live in the library where the solver is bound to the language, and the fact that it was missing a finalizer is something that should be solved over there. Close the PR, throw out the code, find the <a href="https://github.com/JuliaSparse/Pardiso.jl/pull/117"> stalled discussion on the repo that should have the finalizer</a>, poke the author a bit, and it’s in. Done, someone just needed to be reminded.</p>
<p>Total time on my end was about 3 minutes. The bot could have also written that fix but it basically already existed so no need, this was more about finding out where in the system something was offer.</p>
<h3>Example 5: The “How Long is that Going to Take?” PR</h3>
<p><a href="https://github.com/SciML/Catalyst.jl/pull/1317">Here is a nice PR where it didn’t finish</a> (at least at the time of writing this) and the reason is because there are lots of other clean ups that need to happen for this to ever work. How far away is it? Well it generated a set of tests that cleanly listed out all 120 things to solve. Great, this is probably a full week’s task… I knew it would be a lot but that is now pretty concrete. I probably won’t use the bot to finish this one, but now if someone asked what the effort would be I can give them a pretty clear estimate because it has been reduced from “someone needs to give it a try, seems like a good chunk of work” to “the hard part is making these 120 things happen, which is easy but tedious and it would take about a week, probably not worth the effort right now”. That’s very useful when planning ahead. Total me time was about 5 minutes, plus the PR discussion time to explain to others what the results meant.</p>
<h2>Conclusion: Vibe Coding Done Right is actually an Expert’s Task</h2>
<p>Vibe coding turns any individual into the CTO leading a team of 20, 30, 50, 60 interns. You immediately get a massive team. It takes time and experience to actually handle a group like this correctly and to make it be productive. Making all of 60 interns not break the performance, correctness, or maintainability of your code is very difficult. So I do not recommend this to people who are still “up and coming programmers”. But if you’re a bit more senior and starting to grow your group, well this is then a cheap way to accelerate that. </p>
<p>What that means is, vibe coding is sold for people who don’t know how to program, but if you actually think about it, the main audience that can actually use it correctly is experts.</p>
<h3>A few side remarks I didn’t get to</h3>
<h4>A bunch more examples and how bad Claude is at math</h4>
<p>In the <a href="https://discourse.julialang.org/t/the-use-of-claude-code-in-sciml-repos/131009/8?u=chrisrackauckas">Julia Discourse forum</a>, I detail a bunch of different PRs to show where the AIs tend to succeed and fail. But generally anything that is mostly about “programming” (refactoring, inefficient implementations, etc.) the LLMs do well. Anything about the domain or application (differential equations, engineering, physics for me) it just seems to flat out do something dumb. The results all lined up very clearly shows what kind of PRs you should be asking it to make and which ones just aren’t worth the effort.</p>
<h4>The role of empathy in vibe coding success</h4>
<p>Some of the least empathetic people I know in open source are the ones who are also the most skeptical of vibe coding. I have a heavy speculation that they speak to the agents similarly to how they speak to other potential contributors, and drive the bots away the same way they do to people. But with a bot, it will always try to make you happy, just by hallucinating and commenting out your tests. These same people also don’t want the bots around because they claim that’s all the bots ever do. Weird coincidence. I wonder what this will do to the culture of programming over time.</p>
<h4>The cost may not make sense in the long run, but it does while the VCs are paying for it</h4>
<p>On my \$200/month Claude 20x Max subscription I used enough tokens for about $5,200 of compute in the first month. This is obviously not sustainable, but hey, it’s a startup world and VCs are paying for it right now. If you can get a few extra features done that get you more funding, then this is worth it. If you’re a professor and you can get a few more papers out, then this is worth it. If you’re an individual contributor at a big company and you can get a few more features out that make your team look good, then this is worth it. Will it be worth it after the money runs out? Who knows, but mine while the gold is there.</p>
<h4>What’s the right setup? Easy, Claude Code</h4>
<p>The tab-complete stuff is pretty annoying. The power comes from running agents. Claude Code has a simple setup and is able to start running code. Just write a decent Claude.md that tells it to stop being so nice and instead just tell me when it cannot solve the problem, and you’re good to go. The <a href="https://github.com/upstash/context7">context7 MCP is good</a>, <a href="https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking">Sequential Thinking as well</a>. </p>
<h4>“This sounds like hell?”: A response to seeing what vibe coding is like</h4>
<p>Hey Hacker News! Looks like it got there. One of the interesting comments is “To me, someone who actually love programming, it makes vibe coding look like hell.” I 100% agree! Notice from the examples that the amount of time I’m spending on this I try to keep as minimal as possible. I like to program, and I need to be programming because I can do the hard stuff while the LLM can’t. The goal is to get as much easy stuff done with as minimal work on your end as possible, so you can stop worrying about the annoying/boring stuff and can focus more time on the interesting work.</p>
<p>But then again, if you also just really dislike having to do meetings at all and prefer to just be coding alone, then… yeah the agents will probably drive you mad. But with the amount of code coming from LLMs I think there will be an even greater need for good individual contributors to yell at the clouds and maintain the integrity of the codebase so, you’ll have your place. You just might have a larger load of PRs coming your way and may want to be quicker to shut a few down.</p>

                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Thunderbird Pro August 2025 Update (157 pts)]]></title>
            <link>https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/</link>
            <guid>44985131</guid>
            <pubDate>Fri, 22 Aug 2025 14:29:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/">https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/</a>, See on <a href="https://news.ycombinator.com/item?id=44985131">Hacker News</a></p>
Couldn't get https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/: Error: Request failed with status code 504]]></description>
        </item>
        <item>
            <title><![CDATA[All managers make mistakes; good managers acknowledge and repair (258 pts)]]></title>
            <link>https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/</link>
            <guid>44983986</guid>
            <pubDate>Fri, 22 Aug 2025 12:50:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/">https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/</a>, See on <a href="https://news.ycombinator.com/item?id=44983986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<p><em>“There is a crack in everything. That’s how the light gets in.”</em> — Leonard Cohen</p>
</blockquote>



<p>Let me tell you something that will happen after you become a manager: you’re going to mess up. A lot. You’ll give feedback that lands wrong and crushes someone’s confidence. You’ll make a decision that seems logical but turns out to be completely misguided. You’ll forget that important thing you promised to do for someone on your team. You’ll lose your temper in a meeting when you should have stayed calm.</p>



<p>The real question isn’t whether you’ll make mistakes; it’s what you do <em>after</em>.</p>



<p>I recently read&nbsp;<em><a href="https://www.amazon.com/Good-Inside-Guide-Becoming-Parent/dp/B09PGMSBBN">“Good Inside”</a></em>&nbsp;by Dr. Becky Kennedy, a parenting book that completely changed how I think about this. She talks about how the most important parenting skill isn’t being perfect — it’s&nbsp;<strong>repair</strong>. When you inevitably lose your patience with your kid or handle something poorly, what matters most is going back and fixing it. <strong>Acknowledging what happened, taking responsibility, and reconnecting.</strong></p>



<p>Sound familiar? Because that’s what good management is about too.</p>



<p>Think about the worst manager you ever had. I bet they weren’t necessarily the ones who made the most mistakes. But they were probably the ones who never acknowledged them. Who doubled down when they were wrong. Who let their ego prevent them from admitting they didn’t have all the answers.</p>



<p>Here’s a pattern I see play out constantly: A manager commits to something without consulting the team. Maybe it’s a feature at a client demo, a timeline in a board meeting, or just a “small favor” for another department. The team scrambles to deliver, working nights and weekends. They make it happen, but barely, and with real costs: technical debt, burned-out engineers, resentment building.</p>



<p>What happens next determines everything. The manager who never acknowledges what they put the team through? That’s how you lose your best people. But the manager who comes back and says, <em>“I put you in an impossible position. I should have consulted you first. I’m sorry for the stress that caused, and here’s how I’ll handle it differently next time”</em>, that manager builds trust even through the mistake.</p>



<p>I’ve been on both sides of this. As an engineer, I watched managers make the same mistakes over and over again, never acknowledging the chaos they created. As a manager, I’ve been the one creating that chaos 🥲. The difference in outcomes is massive; when you own your mistakes completely and specifically, something unexpected happens: your team trusts you more, not less.</p>



<p>Here’s what repair looks like in practice:</p>



<ol>
<li><strong>Be specific about what you did wrong.</strong>&nbsp;Not “mistakes were made” or “things could have gone better.” But “I interrupted you three times in that meeting and dismissed your concerns. That was wrong.”</li>



<li><strong>Don’t make it about you.</strong>&nbsp;This isn’t the time for a long explanation of your stress levels or why you acted that way. Save that for your therapist or your own manager. The repair is about acknowledging the impact on the other person.</li>



<li><strong>Actually change the behavior.</strong>&nbsp;An apology without changed behavior is just empty words. If you keep making the same “mistake,” it’s not a mistake anymore; it’s a choice.</li>



<li><strong>Give it time.</strong>&nbsp;One conversation doesn’t instantly repair broken trust. It’s a starting point, not a finish line. You have to consistently show up differently.</li>
</ol>



<p><strong>The beautiful thing about getting comfortable with repair is that it actually makes you better as a manager</strong>. When you know you can fix things when they go wrong, you’re more willing to make decisions, have difficult conversations, and take reasonable risks. You stop being paralyzed by perfectionism because you know that most mistakes, while serious, create opportunities for growth and stronger relationships when handled well.</p>



<p>This doesn’t mean being reckless or careless. It doesn’t mean making the same mistakes repeatedly. And it definitely doesn’t mean using repair as a get-out-of-jail-free card for being a shitty manager.</p>



<p>What it means is accepting that you’re human, that management is complex, and that you won’t always get it right. Your job isn’t to be perfect. Your job is to&nbsp;<a href="https://terriblesoftware.org/2025/06/13/good-engineer-bad-engineer/">ship working software that adds real value to users</a>, to help your team grow, and to create an environment where people can do their best work. </p>



<p>Sometimes you’ll fail at those things. When you do, you repair, you learn, and you keep going.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Being “Confidently Wrong” is holding AI back (147 pts)]]></title>
            <link>https://promptql.io/blog/being-confidently-wrong-is-holding-ai-back</link>
            <guid>44983570</guid>
            <pubDate>Fri, 22 Aug 2025 12:14:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://promptql.io/blog/being-confidently-wrong-is-holding-ai-back">https://promptql.io/blog/being-confidently-wrong-is-holding-ai-back</a>, See on <a href="https://news.ycombinator.com/item?id=44983570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-9.54.45-PM.png" alt="" loading="lazy" width="1397" height="464" srcset="https://hasura.io/blog/content/images/size/w600/2025/08/Screenshot-2025-08-20-at-9.54.45-PM.png 600w, https://hasura.io/blog/content/images/size/w1000/2025/08/Screenshot-2025-08-20-at-9.54.45-PM.png 1000w, https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-9.54.45-PM.png 1397w" sizes="(min-width: 720px) 720px"></figure><blockquote>The reason humans are so useful is not mainly their raw intelligence. It’s their ability to build up context, interrogate their own failures, and pick up small improvements and efficiencies as they practice a task<p>- "Why I don't think AGI is right around the corner", <a href="https://www.dwarkesh.com/p/timelines-june-2025">Dwarkesh Patel</a></p></blockquote><p>In this post, based on our recent experiences selling 7-figure AI deals to Fortune 500s and Silicon Valley tech cos alike, &nbsp;I'll discuss how "confident inaccuracy" seems to be at the heart of this problem.</p><!--kg-card-begin: markdown--><ul>
<li><a href="#being-wrong">Confidently Wrong is the problem</a>
<ul>
<li><a href="#verification-tax">The Verification Tax that nukes ROI</a></li>
<li><a href="#trust-erosion">Asymetric erosion of trust that decreases adoption</a></li>
<li><a href="#prevents-improvements">Prevents improvements that kills AI motivation</a></li>
<li><a href="#compounding-errors">90% accuracy is 2 errors in 3!</a></li>
</ul>
</li>
<li><a href="#fixing-wrong">Tentatively Right is the solution</a>
<ul>
<li><a href="#flywheel">An Accuracy Flywheel</a></li>
<li><a href="#ai-adoption">A Path to increasing AI adoption vs Obsoloscence</a></li>
</ul>
</li>
</ul>
<!--kg-card-end: markdown--><hr><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-19-at-11.48.17-PM.png" alt="" loading="lazy" width="1338" height="805" srcset="https://hasura.io/blog/content/images/size/w600/2025/08/Screenshot-2025-08-19-at-11.48.17-PM.png 600w, https://hasura.io/blog/content/images/size/w1000/2025/08/Screenshot-2025-08-19-at-11.48.17-PM.png 1000w, https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-19-at-11.48.17-PM.png 1338w" sizes="(min-width: 720px) 720px"></figure><!--kg-card-begin: html--><!--kg-card-end: html--><p><h2 id="being-confidently-wrong-is-the-only-problem">Being Confidently Wrong is The Only Problem</h2></p><p>Aside from the hilarious "Oh I spend $10M on this campaign because our AI assistant told me to" first order problem, confident inaccuracy causes second and third order problems that are far more insidious:</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="a-imposes-a-universal-verification-tax">a) Imposes a universal verification tax</h3><p>I don't know when I might get an incorrect response from my AI. So I have to forensically check every response. </p><p>My minutes turn into hours; the ROI disappears.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="b-erodes-trust-asymmetrically">b) Erodes trust asymmetrically</h3><p>For serious work, one high‑confidence miss costs more credibility than ten successes earn. &nbsp;</p><p>I'll revert to my older flow.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="c-hidden-failure-modes-kill-motivation-to-improve">c) Hidden failure modes kill motivation to improve</h3><p>Without high-quality uncertainty information, I don’t know whether a result is wrong because of ambiguity, missing context, stale data, or a model mistake.</p><p>If I don't know why its wrong, I'm not invested in making it successful.</p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="d-compounding-errors-results-in-ai-being-doomed-to-fail">d) Compounding errors results in AI being doomed to fail</h3><p>There's been a slew of recent reports on AI adoption is not trending well. </p><p>Per <a href="https://www.mckinsey.com/capabilities/quantumblack/our-insights/seizing-the-agentic-ai-advantage">McKinsey's report</a> 90% of AI initiatives stay stuck in pilot mode. <br><a href="https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/">Fortune covered</a> an MIT study that claims 95% of pilots are failing. </p><p>While the authors of the reports point out several issues, consider another perspective - at the core there are 2 simple facts:</p><ol><li>A system is either always accurate or its not</li><li>If the system is not always accurate even the tiniest percent of the time, I need to know when its not. </li></ol><p>No amount of solving any other problem (integration, data readiness, organizational readiness etc) will change the fact that AI's tendency to be confidently wrong keeps it out of real world use-cases. </p><p>Accuracy is not like Uptime.</p><ul><li>99.99% uptime is ~53 minutes a year. </li><li>99.99% accuracy in a ten step workflow is 1 error in a 1000 runs.</li><li>90% accuracy in a ten step workflow is <strong>2 in every 3 workflows have errors</strong> (1 - 0.9^10).</li></ul><hr><!--kg-card-begin: html--><!--kg-card-end: html--><p><h2 id="2-fixing-confidently-wrong-might-be-a-silver-bullet%E2%84%A2">2. Fixing "confidently wrong" might be A Silver Bullet<strong>™</strong></h2></p><p>The irony here is that perfect accuracy is not required to have a usable AI system. </p><p>As humans if we're working with systems that will never be fully accurate, then more valuable than, say, a 90% accurate system is, say, a 50% accurate system that can signal uncertainty - and get more accurate over time.</p><p><strong>We don’t need perfection; we need a loop that tightens.</strong></p><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="21-an-accuracy-flywheel">2.1 An Accuracy Flywheel </h3><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png" alt="" loading="lazy" width="1797" height="899" srcset="https://hasura.io/blog/content/images/size/w600/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png 600w, https://hasura.io/blog/content/images/size/w1000/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png 1000w, https://hasura.io/blog/content/images/size/w1600/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png 1600w, https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-11.04.07-PM.png 1797w" sizes="(min-width: 720px) 720px"></figure><p>The starting point of this loop is if an AI system could tell the user when its not certain about its accuracy in a concrete and native way.</p><ol><li><strong>Native uncertainty</strong> → it signals confidence and the top uncertainty causes; abstains below threshold.</li><li><strong>Human nudge</strong> → the user fills a planning gap that was causing uncertainty</li><li><strong>Model improvement</strong> → that nudge updates the domain knowledge and the planning space (not just the answer) and accuracy improves.</li></ol><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-20-at-11.53.44-PM.png" alt="" loading="lazy" width="520" height="823"></figure><div><p>In practice, the sources of inaccuracy are far more challenging, not just missing definitions of terms that need to be remembered. </p><p>Data is messy. The quality is unknown. Data is stale - especially unstructured data that lacks annotations. Procedural semantics are in people's heads. The list is endless.</p></div><!--kg-card-begin: html--><!--kg-card-end: html--><h3 id="a-path-to-increasing-ai-adoption-vs-obsoloscence">A path to increasing AI Adoption vs Obsoloscence</h3><p>If our AI systems can tell us that they're not sure and why, then we can start to help it become better.</p><figure><img src="https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png" alt="" loading="lazy" width="1697" height="877" srcset="https://hasura.io/blog/content/images/size/w600/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png 600w, https://hasura.io/blog/content/images/size/w1000/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png 1000w, https://hasura.io/blog/content/images/size/w1600/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png 1600w, https://hasura.io/blog/content/images/2025/08/Screenshot-2025-08-21-at-1.40.24-PM.png 1697w" sizes="(min-width: 720px) 720px"></figure><hr><p><h2 id="a-quick-diagnostic-for-your-ai-investment">A quick diagnostic for your AI investment</h2></p><p>Before you fund another “AI for X” pilot, ask:</p><ul><li><strong>Will it tell me when it’s unsure—and why?</strong> Ambiguity, missing context, data staleness, validation failure etc etc?</li><li><strong>Does it learn from the correction I just gave it?</strong> Will the next user avoid the same trap without re‑prompting?</li></ul><hr><p><h2 id="our-solution-approach">Our solution approach</h2></p><p>Two principles have proven to work really well for us across customers:</p><ul><li><strong>Instead of generating answers, generate plans in a DSL unique to your domain. </strong>Plans in the DSL compile to deterministic actions with runtime validations and policy checks and the DSL should be rich enough to capture the breadth of data/API operations, compute tasks and of course generative &amp; semantic AI tasks. </li><li><strong>Continuously specialize the AI to your domain to drive planning accuracy &amp; planning confidence.</strong> Build a system to continuously bind the AI model to your ontology/metrics, entity catalogs, data systems; learn your naming collisions and edge‑cases; understand your meanings and how your domain works. Most crucially, build a system that can leverage this to carry a calibrated confidence on the generated plan.</li></ul><p>The mechanics are open to debate and require solving hard design &amp; engineering problems - but solving for this takes us closer to a usable enterprise AI system.</p><hr><p>If you lead data / AI initiatives and want to exchange notes feel free to reach out at <a href="https://promptql.io/cdn-cgi/l/email-protection#7e0a1f10131f173e0e0c11130e0a0f12501711"><span data-cfemail="9ce8fdf2f1fdf5dceceef3f1ece8edf0b2f5f3">[email&nbsp;protected]</span></a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[4chan will refuse to pay daily online safety fines, lawyer tells BBC (339 pts)]]></title>
            <link>https://www.bbc.co.uk/news/articles/cq68j5g2nr1o</link>
            <guid>44982681</guid>
            <pubDate>Fri, 22 Aug 2025 10:02:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.co.uk/news/articles/cq68j5g2nr1o">https://www.bbc.co.uk/news/articles/cq68j5g2nr1o</a>, See on <a href="https://news.ycombinator.com/item?id=44982681">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="urn-bbc-ares--article-cq68j5g2nr1o"><header data-component="headline-block"></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg.webp 800w" type="image/webp"><img alt="An image of the 4chan logo on a mobile phone shown on a multicoloured abstract background" src="https://ichef.bbci.co.uk/ace/standard/820/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/d789/live/a6b26a30-7e96-11f0-8884-75e02e8a067a.jpg 800w" width="820" height="460.00000000000006"></picture></span><span role="text"><span>Image source, </span>Getty Images</span></p></figure></div><div data-component="text-block"><p><b>A lawyer representing the online message board 4chan says it won't pay a proposed fine by the UK's media regulator as it enforces the Online Safety Act.</b></p><p>According to Preston Byrne, managing partner of law firm Byrne &amp; Storm, Ofcom has provisionally decided to impose a £20,000 fine "with daily penalties thereafter" for as long as the site fails to comply with its request.</p><p>"Ofcom's notices create no legal obligations in the United States," he told the BBC, adding he believed the regulator's investigation was part of an "illegal campaign of harassment" against US tech firms.</p><p>Ofcom has declined to comment while its investigation continues.</p></div><div data-component="text-block"><p>"4chan has broken no laws in the United States - my client will not pay any penalty," Mr Byrne said.</p><p><a href="https://www.bbc.co.uk/news/articles/ckgqwlvq180o">Ofcom began investigating 4chan</a> over whether it was complying with its obligations under the UK's Online Safety Act.</p><p>Then in August, it said it had issued 4chan with "a provisional notice of contravention" for failing to comply with two requests for information.</p><p>Ofcom said its investigation would examine whether the message board was complying with the act, including requirements to protect its users from illegal content.</p><p>4chan has often been at the heart of online controversies in its 22 years, including misogynistic campaigns and conspiracy theories.</p><p>Users are anonymous, which can often lead to extreme content being posted.</p></div><p data-component="subheadline-block"><h2 id="First-Amendment-rights" tabindex="-1"><span role="text">'First Amendment rights'</span></h2></p><div data-component="text-block"><p>In a statement posted on X, law firms Byrne &amp; Storm and Coleman Law said 4chan was a US company incorporated in the US, and therefore protected against the UK law.</p><p>"American businesses do not surrender their First Amendment rights because a foreign bureaucrat sends them an email," they wrote.</p><p>"Under settled principles of US law, American courts will not enforce foreign penal fines or censorship codes. </p><p>"If necessary, we will seek appropriate relief in US federal court to confirm these principles."</p><p>They said authorities in the US had been "briefed" on their response to Ofcom's investigation.</p><p>The statement concludes by calling on the Trump administration to invoke all diplomatic and legal levers to protect American businesses from "extraterritorial censorship mandates".</p><p>Ofcom has previously said the Online Safety Act only requires services to take action to protect users based in the UK.</p></div><p data-component="subheadline-block"><h2 id="UK-backs-down" tabindex="-1"><span role="text">UK backs down</span></h2></p><div data-component="text-block"><p>Some American politicians - particularly the Trump administration, its allies and officials - have pushed back against what they regard as overreach in the regulation of US tech firms by the UK and EU. </p><p>A perceived impact of the Online Safety Act on free speech has been a particular concern, but other laws have also been the source of disagreement.</p><p>On 19 August, US Director of National Intelligence Tulsi Gabbard said the UK had withdrawn its controversial demand for a "backdoor" in an Apple data protection system - saying she worked with the President and Vice President to get the UK to abandon its plan.</p><p>Two days later, US Federal Trade Commission chairman Andrew Ferguson warned big tech firms they could be violating US law if they weakened privacy and data security requirements by complying with international laws such as the Online Safety Act.</p><p>"Foreign governments seeking to limit free expression or weaken data security in the United States might count on the fact that companies have an incentive to simplify their operations and legal compliance measures by applying uniform policies across jurisdictions," he said.</p><p>If 4chan does successfully fight the fine in the US courts, Ofcom may have other options.</p><p>"Enforcing against an offshore provider is tricky," Emma Drake, partner of online safety and privacy at law firm Bird and Bird, told the BBC. </p><p>"Ofcom can instead ask a court to order other services to disrupt a provider's UK business, such as requiring a service's removal from search results or blocking of UK payments.</p><p>"If Ofcom doesn't think this will be enough to prevent significant harm, it can even ask that ISPs be ordered to block UK access."</p></div><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png.webp 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png.webp 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png.webp 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png.webp 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png.webp 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png.webp 976w" type="image/webp"><img alt="A green promotional banner with black squares and rectangles forming pixels, moving in from the right. The text says: “Tech Decoded: The world’s biggest tech news in your inbox every Monday.”" loading="lazy" src="https://ichef.bbci.co.uk/ace/standard/1600/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png" srcset="https://ichef.bbci.co.uk/ace/standard/240/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png 240w, https://ichef.bbci.co.uk/ace/standard/320/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png 320w, https://ichef.bbci.co.uk/ace/standard/480/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png 480w, https://ichef.bbci.co.uk/ace/standard/624/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png 624w, https://ichef.bbci.co.uk/ace/standard/800/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png 800w, https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/80c1/live/7a4731e0-2128-11f0-9060-674316cb3a1f.png 976w" width="1600" height="263"></picture></span></p></figure></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go is still not good (502 pts)]]></title>
            <link>https://blog.habets.se/2025/07/Go-is-still-not-good.html</link>
            <guid>44982491</guid>
            <pubDate>Fri, 22 Aug 2025 09:25:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.habets.se/2025/07/Go-is-still-not-good.html">https://blog.habets.se/2025/07/Go-is-still-not-good.html</a>, See on <a href="https://news.ycombinator.com/item?id=44982491">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">


  

  <div itemprop="articleBody">
    <p>Previous posts <a href="https://blog.habets.se/2013/10/Why-Go-is-not-my-favourite-language.html">Why Go is not my favourite language</a> and <a href="https://blog.habets.se/2022/02/Go-programs-are-not-portable.html">Go programs
are not portable</a> have me critiquing Go for over a decade.</p>

<p>These things about Go are bugging me more and more. Mostly because they’re so
unnecessary. The world knew better, and yet Go was created the way it was.</p>

<p>For readers of previous posts you’ll find some things repeated here. Sorry
about that.</p>

<h2 id="error-variable-scope-is-forced-to-be-wrong">Error variable scope is forced to be wrong</h2>

<p>Here’s an example of the language forcing you to do the wrong thing. It’s very
helpful for the reader of code (and code is read more often than it’s written),
to minimize the scope of a variable. If by mere syntax you can tell the reader
that a variable is just used in these two lines, then that’s a good thing.</p>

<p>Example:</p>

<div><pre><code><span>if</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>foo</span><span>();</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
   </span><span>return</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>(enough has been said about this verbose repeated boilerplate that I don’t have
to. I also don’t particularly care)</p>

<p>So that’s fine. The reader knows <code>err</code> is here and only here.</p>

<p>But then you encounter this:</p>

<div><pre><code><span>bar</span><span>,</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>foo</span><span>()</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>=</span><span> </span><span>foo2</span><span>();</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>[</span><span>…</span><span> </span><span>a</span><span> </span><span>lot</span><span> </span><span>of</span><span> </span><span>code</span><span> </span><span>below</span><span> </span><span>…</span><span>]</span><span>
</span></code></pre>
</div>

<p>Wait, what? Why is <code>err</code> reused for <code>foo2()</code>? Is there’s something subtle I’m
not seeing? Even if we change that to <code>:=</code>, we’re left to wonder why <code>err</code> is
in scope for (potentially) the rest of the function. Why? Is it read later?</p>

<p>Especially when looking for bugs, an experienced coder will see these things
and slow down, because here be dragons. Ok, now I’ve wasted a couple of seconds
on the red herring of reusing <code>err</code> for <code>foo2()</code>.</p>

<p>Is a bug perhaps that the function ends with this?</p>

<div><pre><code><span>// Return foo99() error. (oops, that's not what we're doing)</span><span>
</span><span>foo99</span><span>()</span><span>
</span><span>return</span><span> </span><span>err</span><span> </span><span>// This is `err` from way up there in the foo() call.</span><span>
</span></code></pre>
</div>

<p>Why does the scope of <code>err</code> extend way beyond where it’s relevant?</p>

<p>The code would have been so much easier to read if only <code>err</code>’s scope had been
smaller. But that’s not syntactically possible in Go.</p>

<p>This was not thought through. Deciding on this was not thinking, it was typing.</p>

<h2 id="two-types-of-nil">Two types of nil</h2>

<p>Look at this nonsense:</p>

<div><pre><code><span>package</span><span> </span><span>main</span><span>
</span><span>import</span><span> </span><span>"fmt"</span><span>
</span><span>type</span><span> </span><span>I</span><span> </span><span>interface</span><span>{}</span><span>
</span><span>type</span><span> </span><span>S</span><span> </span><span>struct</span><span>{}</span><span>
</span><span>func</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span><span>
    </span><span>var</span><span> </span><span>i</span><span> </span><span>I</span><span>
    </span><span>var</span><span> </span><span>s</span><span> </span><span>*</span><span>S</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>s</span><span>,</span><span> </span><span>i</span><span>)</span><span> </span><span>// nil nil</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>s</span><span> </span><span>==</span><span> </span><span>nil</span><span>,</span><span> </span><span>i</span><span> </span><span>==</span><span> </span><span>nil</span><span>,</span><span> </span><span>s</span><span> </span><span>==</span><span> </span><span>i</span><span>)</span><span> </span><span>// t,t,f: They're equal, but they're not.</span><span>
    </span><span>i</span><span> </span><span>=</span><span> </span><span>s</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>s</span><span>,</span><span> </span><span>i</span><span>)</span><span> </span><span>// nil nil</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>s</span><span> </span><span>==</span><span> </span><span>nil</span><span>,</span><span> </span><span>i</span><span> </span><span>==</span><span> </span><span>nil</span><span>,</span><span> </span><span>s</span><span> </span><span>==</span><span> </span><span>i</span><span>)</span><span> </span><span>// t,f,t: They are not equal, but they are.</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>Go was not satisfied with one <a href="https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/">billion dollar
mistake</a>,
so they decided to have <strong>two</strong> flavors of <code>NULL</code>.</p>

<p>“What color is your nil?” — The two billion dollar mistake.</p>

<p>The reason for the difference boils down to again, not thinking, just typing.</p>

<h2 id="its-not-portable">It’s not portable</h2>

<p>Adding comment near the top of the file for conditional compilation must be the
dumbest thing ever. Anybody who’s actually tried to maintain a portable program
will tell you this will only cause suffering.</p>

<p>It’s an <a href="https://en.wikipedia.org/wiki/Aristotelian_physics">Aristotle way of the science</a> of designing a language; lock
yourself up in a room, and never test your hypotheses against reality.</p>

<p>The problem is that this is not year 350 BCE. We actually have experience that
aside from air resistance, heavy and light objects actually fall at the same
speed. And we have experience with portable programs, and would not do
something this dumb.</p>

<p>If this had been the year 350 BCE, then this could be forgiven. Science as we
know it hadn’t been invented yet. But this is after decades of very widely
available experience in portability.</p>

<p>More details in <a href="https://blog.habets.se/2022/02/Go-programs-are-not-portable.html">this post</a>.</p>

<h2 id="append-with-no-defined-ownership"><code>append</code> with no defined ownership</h2>

<p>What does this print?</p>

<div><pre><code><span>package</span><span> </span><span>main</span><span>
</span><span>import</span><span> </span><span>"fmt"</span><span>
</span><span>func</span><span> </span><span>foo</span><span>(</span><span>a</span><span> </span><span>[]</span><span>string</span><span>)</span><span> </span><span>{</span><span>
    </span><span>a</span><span> </span><span>=</span><span> </span><span>append</span><span>(</span><span>a</span><span>,</span><span> </span><span>"NIGHTMARE"</span><span>)</span><span>
</span><span>}</span><span>
</span><span>func</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span><span>
    </span><span>a</span><span> </span><span>:=</span><span> </span><span>[]</span><span>string</span><span>{</span><span>"hello"</span><span>,</span><span> </span><span>"world"</span><span>,</span><span> </span><span>"!"</span><span>}</span><span>
    </span><span>foo</span><span>(</span><span>a</span><span>[</span><span>:</span><span>1</span><span>])</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>a</span><span>)</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>Probably <code>[hello NIGHTMARE !]</code>. Who wants that? Nobody wants that.</p>

<p>Ok, how about this?</p>

<div><pre><code><span>package</span><span> </span><span>main</span><span>
</span><span>import</span><span> </span><span>"fmt"</span><span>
</span><span>func</span><span> </span><span>foo</span><span>(</span><span>a</span><span> </span><span>[]</span><span>string</span><span>)</span><span> </span><span>{</span><span>
    </span><span>a</span><span> </span><span>=</span><span> </span><span>append</span><span>(</span><span>a</span><span>,</span><span> </span><span>"BACON"</span><span>,</span><span> </span><span>"THIS"</span><span>,</span><span> </span><span>"SHOULD"</span><span>,</span><span> </span><span>"WORK"</span><span>)</span><span>
</span><span>}</span><span>
</span><span>func</span><span> </span><span>main</span><span>()</span><span> </span><span>{</span><span>
    </span><span>a</span><span> </span><span>:=</span><span> </span><span>[]</span><span>string</span><span>{</span><span>"hello"</span><span>,</span><span> </span><span>"world"</span><span>,</span><span> </span><span>"!"</span><span>}</span><span>
    </span><span>foo</span><span>(</span><span>a</span><span>[</span><span>:</span><span>1</span><span>])</span><span>
    </span><span>fmt</span><span>.</span><span>Println</span><span>(</span><span>a</span><span>)</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>If you guessed <code>[hello world !]</code>, then you know more than anybody should have
to know about quirks of a stupid programming language.</p>

<h2 id="defer-is-dumb"><code>defer</code> is dumb</h2>

<p>Even in a GC language, sometimes you just can’t wait to destroy a resource. It
really does need to run as we leave the local code, be it by normal return, or
via an exception (aka panic).</p>

<p>What we clearly want is RAII, or something like it.</p>

<p>Java has it:</p>

<div><pre><code><span>try</span> <span>(</span><span>MyResource</span> <span>r</span> <span>=</span> <span>new</span> <span>MyResource</span><span>())</span> <span>{</span>
  <span>/*
  work with resource r, which will be cleaned up when the scope ends via
  .close(), not merely when the GC feels like it.
  */</span>
<span>}</span>
</code></pre>
</div>

<p>Python has it. Though Python is <em>almost</em> entirely refcounted, so one can pretty
much rely on the <code>__del__</code> finalizer being called. But if it’s important, then
there’s the <code>with</code> syntax.</p>

<div><pre><code><span>with</span> <span>MyResource</span><span>()</span> <span>as</span> <span>res</span><span>:</span>
  <span># some code. At end of the block __exit__ will be called on res.</span>
</code></pre>
</div>

<p>Go? Go makes you go read the manual and see if this particular resource needs
to have a defer function called on it, and which one.</p>

<div><pre><code><span>foo</span><span>,</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>myResource</span><span>()</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>defer</span><span> </span><span>foo</span><span>.</span><span>Close</span><span>()</span><span>
</span></code></pre>
</div>

<p>This is so dumb. Some resources need a defer destroy. Some don’t. Which ones?
Good fucking luck.</p>

<p>And you also regularly end up with stuff like this monstrosity:</p>

<div><pre><code><span>f</span><span>,</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>openFile</span><span>()</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>nil</span><span>,</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>defer</span><span> </span><span>f</span><span>.</span><span>Close</span><span>()</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>f</span><span>.</span><span>Write</span><span>(</span><span>something</span><span>());</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>nil</span><span>,</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span><span>if</span><span> </span><span>err</span><span> </span><span>:=</span><span> </span><span>f</span><span>.</span><span>Close</span><span>();</span><span> </span><span>err</span><span> </span><span>!=</span><span> </span><span>nil</span><span> </span><span>{</span><span>
  </span><span>return</span><span> </span><span>nil</span><span>,</span><span> </span><span>err</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>Yes, this is what you NEED to do to safely write something to a file in Go.</p>

<p>What’s this, a <em>second</em> <code>Close()</code>? Oh yeah, of course that’s needed. Is it even
safe to double-close, or does my defer need to check for that? It happens to be
safe on <code>os.File</code>, but on other things: WHO KNOWS?!</p>

<h2 id="the-standard-library-swallows-exceptions-so-all-hope-is-lost">The standard library swallows exceptions, so all hope is lost</h2>

<p>(Largely a repeat of part of <a href="https://blog.habets.se/2013/10/Why-Go-is-not-my-favourite-language.html">a previous post</a>)</p>

<p>Go says it doesn’t have exceptions. Go makes it extremely awkward to use
exceptions, because they want to punish programmers who use them.</p>

<p>Ok, fine so far.</p>

<p>But all Go programmers must still write exception safe code. Because while
<em>they</em> don’t use exceptions, other code will. Things will panic.</p>

<p>So you need, not should, NEED, to write code like:</p>

<div><pre><code><span>func</span><span> </span><span>(</span><span>f</span><span> </span><span>*</span><span>Foo</span><span>)</span><span> </span><span>foo</span><span>()</span><span> </span><span>{</span><span>
    </span><span>f</span><span>.</span><span>mutex</span><span>.</span><span>Lock</span><span>()</span><span>
    </span><span>defer</span><span> </span><span>f</span><span>.</span><span>mutex</span><span>.</span><span>Unlock</span><span>()</span><span>
    </span><span>f</span><span>.</span><span>bar</span><span>()</span><span>
</span><span>}</span><span>
</span></code></pre>
</div>

<p>What is this stupid middle endian system? That’s dumb just like putting the day
in the middle of a date is dumb. MMDDYY, honestly? (separate rant)</p>

<p>But panic will terminate the program, they say, so why do you care if you
unlock a mutex five milliseconds before it exits anyway?</p>

<p>Because what if something swallows that exception and carries on as normal, and
you’re now stuck with a locked mutex?</p>

<p>But surely nobody would do that? Reasonable and strict coding standards would
surely prevent it, under penalty of being fired?</p>

<p>The standard library does that. <code>fmt.Print</code> when calling <code>.String()</code>, and the
standard library HTTP server does that, for exceptions in the HTTP handlers.</p>

<p>All hope is lost. You MUST write exception safe code. But you can’t use
exceptions. You can only have the downsides of exceptions be thrust upon you.</p>

<p>Don’t let them gaslight you.</p>

<h2 id="sometimes-things-arent-utf-8">Sometimes things aren’t UTF-8</h2>

<p>If you stuff random binary data into a <code>string</code>, Go just steams along, as
described <a href="https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride">in this post</a>.</p>

<p>Over the decades I have lost data to tools skipping non-UTF-8 filenames. I
should not be blamed for having files that were named before UTF-8 existed.</p>

<p>Well… I had them. They’re gone now. They were silently skipped in a
backup/restore.</p>

<p>Go wants you to continue losing data. Or at least, when you lose data, it’ll
say “well, what (encoding) was the data wearing?”.</p>

<p>Or how about you just do something more thought through, when you design a
language? How about doing the right thing, instead of the obviously wrong
simple thing?</p>

<h2 id="memory-use">Memory use</h2>

<p>Why do I care about memory use? RAM is cheap. Much cheaper than the time it
takes to read this blog post. I care because my service runs on a cloud
instance where you actually pay for RAM. Or you run containers, and you want to
run a thousand of them on the same machine. Your data may <a href="https://yourdatafitsinram.net/">fit in
RAM</a>, but it’s still expensive if you have to
give your thousand containers 4TiB of RAM instead of 1TiB.</p>

<p>You can manually trigger a GC run with <code>runtime.GC()</code>, but “oh no don’t do
that”, they say, “it’ll run when it has to, just trust it”.</p>

<p>Yeah, 90% of the time, that works every time. But then it doesn’t.</p>

<p>I rewrote some stuff in another language because over time the Go version would
use more and more memory.</p>

<h2 id="it-didnt-have-to-be-this-way">It didn’t have to be this way</h2>

<p>We knew better. This was not the COBOL debate over whether to use symbols or
English words.</p>

<p>And it’s not like when we didn’t know at the time that <a href="https://blog.habets.se/2022/08/Java-a-fractal-of-bad-experiments.html">Java’s ideas were
bad</a>, because we did know Go’s ideas were bad.</p>

<p>We already knew better than Go, and yet now we’re stuck with bad Go codebases.</p>

<h2 id="other-peoples-posts">Other people’s posts</h2>

<ul>
  <li>https://www.uber.com/en-GB/blog/data-race-patterns-in-go/</li>
  <li>https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang</li>
  <li>https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride</li>
</ul>


  </div>

  
  
  
  
  
  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LabPlot: Free, open source and cross-platform Data Visualization and Analysis (179 pts)]]></title>
            <link>https://labplot.org/</link>
            <guid>44982409</guid>
            <pubDate>Fri, 22 Aug 2025 09:11:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://labplot.org/">https://labplot.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44982409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-container">
		
	<div>
				<p>
					By using our website, you agree to the use of our cookies.				</p>
				
			</div>



									
					

							
					

		

        

			<div id="wrapper"><div data-vc-full-width="true" data-vc-full-width-init="false" data-vc-stretch-content="true"><div id="heading-i8kMxlHJIR">
	<p>

		
			<h2>
				FREE, open source and cross-platform Data Visualization and Analysis software accessible to everyone and trusted by  <a href="https://labplot.org/references">professionals</a>			</h2>

		
	</p>
</div><div id="single-image-Z8UnYDWaAp">

		
			<p><img decoding="async" src="https://labplot.org/wp-content/uploads/2022/02/01_basic_plots_linux.png" alt="Example showing a combination of some basic visualizations together with a spreadsheet providing the data">
													
									
							</p>

		
	</div></div><div id="features" data-vc-full-width="true" data-vc-full-width-init="false" data-jarallax="" data-speed="0.2"><div>
<div id="heading-I6Zozs2f8D">
	<p>

		
			<h2>
				FEATURE HIGHLIGHTS			</h2>

		
	</p>
</div>
            
        
            
            

        
<div id="heading-BO4MrwNl16">
	<p>

		
			<h2>
				What can you expect?			</h2>

		
	</p>
</div>
            
        </div><div><div data-wow-delay="0ss" data-wow-duration="0.7ss">
            
            

        
<div id="single-image-JdIV17Yakn">

		
			<p><a href="https://labplot.org/wp-content/uploads/2022/02/01_basic_plots_linux-1024x646.png" rel="lightbox" data-rel="lightbox-image-0" data-rl_title="" data-rl_caption="" title="">
					
													<img decoding="async" src="https://labplot.org/wp-content/uploads/2022/02/01_basic_plots_linux-1024x646.png" alt="Example showing a combination of some basic visualizations together with a spreadsheet providing the data">
													
											</a>
									
							</p>

		
	</div>
            
        
            
            

        
<div id="heading-PvFzOMfm1g">
	<p>

		
			<h3>
				High-quality data visualization and interactive plotting with few clicks			</h3>

		
	</p>
</div>
            
        
            
            

        </div><div data-wow-delay="0.3ss" data-wow-duration="0.7ss">
            
            

        
<div id="single-image-SOft7RPkag">

		
			<p><a href="https://labplot.org/wp-content/uploads/2022/02/02_overview_linux-1024x646.png" rel="lightbox" data-rel="lightbox-image-1" data-rl_title="" data-rl_caption="" title="">
					
													<img decoding="async" src="https://labplot.org/wp-content/uploads/2022/02/02_overview_linux-1024x646.png" alt="Example showing some basic statistical analysis of the data">
													
											</a>
									
							</p>

		
	</div>
            
        
            
            

        
<div id="heading-4RmIw9bcxz">
	<p>

		
			<h3>
				Reliable and easy data analysis, statistics, regression, curve and peak fitting			</h3>

		
	</p>
</div>
            
        
            
            

        </div><div data-wow-delay="0.6ss" data-wow-duration="0.7ss">
            
            

        
<div id="single-image-fTbuiXCpW4">

		
			<p><a href="https://labplot.org/wp-content/uploads/2022/02/11_maxima_Duffing_Oscillator_linux-1024x646.png" rel="lightbox" data-rel="lightbox-image-2" data-rl_title="" data-rl_caption="" title="">
					
													<img decoding="async" src="https://labplot.org/wp-content/uploads/2022/02/11_maxima_Duffing_Oscillator_linux-1024x646.png" alt="Results of the computations performed in a Maxima session, visualized in LabPlot">
													
											</a>
									
							</p>

		
	</div>
            
        
            
            

        
<div id="heading-V4Nxip0FSe">
	<p>

		
			<h3>
				Intuitive and fast computing with interactive notebooks using Python, R, Julia etc.			</h3>

		
	</p>
</div>
            
        
            
            

        </div></div><div><div data-wow-delay="0ss" data-wow-duration="0.7ss">
            
            

        
<div id="single-image-Pi6S2vNQXa">

		
			<p><a href="https://labplot.org/wp-content/uploads/2022/02/data_extractor_example-1024x547.jpg" rel="lightbox" data-rel="lightbox-image-3" data-rl_title="" data-rl_caption="" title="">
					
													<img decoding="async" src="https://labplot.org/wp-content/uploads/2022/02/data_extractor_example-1024x547.jpg" alt="Manual point-and-click or semi-automated approach to extract the data from images">
													
											</a>
									
							</p>

		
	</div>
            
        
            
            

        
<div id="heading-6LsY7DJEmU">
	<p>

		
			<h3>
				Effortless data extraction (plot digitizer) and support for real-time data			</h3>

		
	</p>
</div>
            
        
            
            

        </div><div data-wow-delay="0.3ss" data-wow-duration="0.7ss">
            
            

        
<div id="single-image-jWu2st8VcS">

		
			<p><a href="https://labplot.org/wp-content/uploads/2022/02/data_import_example-1024x549.jpg" rel="lightbox" data-rel="lightbox-image-4" data-rl_title="" data-rl_caption="" title="">
					
													<img decoding="async" src="https://labplot.org/wp-content/uploads/2022/02/data_import_example-1024x549.jpg" alt="Data Import">
													
											</a>
									
							</p>

		
	</div>
            
        
            
            

        
<div id="heading-7lcFOjboPp">
	<p>

		
			<h3>
				Smooth data import and export to and from multiple formats			</h3>

		
	</p>
</div>
            
        
            
            

        </div><div data-wow-delay="0.6ss" data-wow-duration="0.7ss">
            
            

        
<div id="single-image-Y7I0Xl8kde">

		
			<p><a href="https://labplot.org/wp-content/uploads/2022/02/04_theming_win-1024x619.png" rel="lightbox" data-rel="lightbox-image-5" data-rl_title="" data-rl_caption="" title="">
					
													<img decoding="async" src="https://labplot.org/wp-content/uploads/2022/02/04_theming_win-1024x619.png" alt="Windows Theme">
													
											</a>
									
							</p>

		
	</div>
            
        
            
            

        
<div id="heading-swOkrgEAhT">
	<p>

		
			<h3>
				Available for Windows, macOS, Linux, FreeBSD and Haiku			</h3>

		
	</p>
</div>
            
        
            
            

        </div></div><div id="features_more">
	<p>

		
			<h2>
				Visit feature page to view the complete feature list			</h2>

		
	</p>
</div>

            
        </div><div id="latestnews" data-vc-full-width="true" data-vc-full-width-init="false"><div>
<div id="heading-tZFxzWe57L">
	<p>

		
			<h2>
				LATEST NEWS			</h2>

		
	</p>
</div>
            
        
            
            

        
<div id="heading-oug3SerBFw">
	<p>

		
			<h2>
				Stay in touch with recent developments			</h2>

		
	</p>
</div>
            
        </div>



<div id="recent-posts-kO7GQ9qyR6" data-id="3">
		
	<article id="post-4200" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
				<a href="https://labplot.org/2025/08/18/labplot-2-12-1-released/">
					<h2 itemprop="headline">
												LabPlot 2.12.1 released					</h2>
				</a>

				

				<p>Today we are announcing the availability of the minor patch release 2.12.1. This release contains minor improvements and bug fixes only. The fixes are distributed over many different areas of the application and we recommend everybody update to this patch release which is available from…</p>

				
			</div>
	</article>



	<article id="post-4026" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
			
			<div>
				
									<p><img fetchpriority="high" decoding="async" width="470" height="236" src="https://labplot.org/wp-content/uploads/2025/04/LabPlot-Splash-2.12.0.png" alt="" srcset="https://labplot.org/wp-content/uploads/2025/04/LabPlot-Splash-2.12.0.png 470w, https://labplot.org/wp-content/uploads/2025/04/LabPlot-Splash-2.12.0-300x151.png 300w, https://labplot.org/wp-content/uploads/2025/04/LabPlot-Splash-2.12.0-24x12.png 24w, https://labplot.org/wp-content/uploads/2025/04/LabPlot-Splash-2.12.0-36x18.png 36w, https://labplot.org/wp-content/uploads/2025/04/LabPlot-Splash-2.12.0-48x24.png 48w" sizes="(max-width: 470px) 100vw, 470px"></p>

    			</div>

			<div>
				<a href="https://labplot.org/2025/04/28/labplot-2-12-released/">
					<h2 itemprop="headline">
												LabPlot 2.12 released					</h2>
				</a>

				

				<p>Welcome LabPlot 2.12! After many months of intense work, we are proud to announce the new release of LabPlot 2.12, a FREE, open source and cross-platform Data Visualization and Analysis software accessible to everyone and trusted by professionals! This latest release introduces a wealth of…</p>

				
			</div>

		</div>
	</article>



	<article id="post-3768" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
				<a href="https://labplot.org/2024/12/20/new-labplot-user-documentation/">
					<h2 itemprop="headline">
												New LabPlot User Documentation					</h2>
				</a>

				

				<p>In recent weeks we have been working on transferring LabPlot’s documentation to a new format. We decided to move the documentation from the DocBook and MediaWiki format to the Sphinx/reStrcutredText framework. In our perception Sphinx offers a user-friendly and flexible way to create and manage…</p>

				
			</div>
	</article>



	<article id="post-3709" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
				<a href="https://labplot.org/2024/11/05/performance-of-data-import-in-labplot/">
					<h2 itemprop="headline">
												Performance of data import in LabPlot					</h2>
				</a>

				

				<p>In many cases, importing data into LabPlot for further analysis and visualization is the first step in the application: LabPlot supports many different formats (CSV, Origin, SAS, Stata, SPSS, MATLAB, SQL, JSON, binary, OpenDocument Spreadsheets (ods), Excel (xlsx), HDF5, MQTT, Binary Logging Format (BLF), FITS,…</p>

				
			</div>
	</article>



	<article id="post-3696" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
				<a href="https://labplot.org/2024/10/01/google-summer-of-code-2024/">
					<h2 itemprop="headline">
												Google Summer of Code 2024					</h2>
				</a>

				

				<p>Programmes like Season of KDE (SoK) and Google Summer of Code (GSoC) provide a great opportunity for young talent to become part of the open source community and contribute to open source projects. LabPlot, and KDE in general, has a long history and experience in…</p>

				
			</div>
	</article>



	<article id="post-3320" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
			
			<div>
				
									<p><img decoding="async" width="1024" height="132" src="https://labplot.org/wp-content/uploads/2024/08/Wikipedia_Banner.png" alt="" srcset="https://labplot.org/wp-content/uploads/2024/08/Wikipedia_Banner.png 1024w, https://labplot.org/wp-content/uploads/2024/08/Wikipedia_Banner-300x39.png 300w, https://labplot.org/wp-content/uploads/2024/08/Wikipedia_Banner-768x99.png 768w, https://labplot.org/wp-content/uploads/2024/08/Wikipedia_Banner-24x3.png 24w, https://labplot.org/wp-content/uploads/2024/08/Wikipedia_Banner-36x5.png 36w, https://labplot.org/wp-content/uploads/2024/08/Wikipedia_Banner-48x6.png 48w" sizes="(max-width: 1024px) 100vw, 1024px"></p>

    			</div>

			<div>
				<a href="https://labplot.org/2024/08/13/bad-information-drives-out-good-or-how-much-can-we-trust-wikipedia/">
					<h2 itemprop="headline">
												Bad information drives out good or how much can we trust Wikipedia?					</h2>
				</a>

				

				<p>This post is written on behalf of the LabPlot team. It’s different compared to what we usually publish on our homepage but we feel we need to share this story with our community. Introduction You might already know this, but finalizing a release for a…</p>

				
			</div>

		</div>
	</article>



	<article id="post-3261" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
			
			<div>
				
									<p><img decoding="async" width="470" height="236" src="https://labplot.org/wp-content/uploads/2024/07/2.11_splash_nlnet.png" alt="LabPlot gets NLNet funding!" srcset="https://labplot.org/wp-content/uploads/2024/07/2.11_splash_nlnet.png 470w, https://labplot.org/wp-content/uploads/2024/07/2.11_splash_nlnet-300x151.png 300w, https://labplot.org/wp-content/uploads/2024/07/2.11_splash_nlnet-24x12.png 24w, https://labplot.org/wp-content/uploads/2024/07/2.11_splash_nlnet-36x18.png 36w, https://labplot.org/wp-content/uploads/2024/07/2.11_splash_nlnet-48x24.png 48w" sizes="(max-width: 470px) 100vw, 470px"></p>

    			</div>

			<div>
				<a href="https://labplot.org/2024/07/30/labplot-funded-through-ngio-core-fund/">
					<h2 itemprop="headline">
												LabPlot funded through NGIO Core Fund					</h2>
				</a>

				

				<p>This year we applied to NLnet’s NGI Zero Core open call for proposals in February 2024. After a thorough review by the NLnet Foundation, LabPlot’s application was accepted and will be funded by the NGI0 Core Fund, a fund established by NLnet with financial support…</p>

				
			</div>

		</div>
	</article>



	<article id="post-3186" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
			
			<div>
				
									<p><img loading="lazy" decoding="async" width="470" height="236" src="https://labplot.org/wp-content/uploads/2024/07/2.11.png" alt="" srcset="https://labplot.org/wp-content/uploads/2024/07/2.11.png 470w, https://labplot.org/wp-content/uploads/2024/07/2.11-300x151.png 300w, https://labplot.org/wp-content/uploads/2024/07/2.11-24x12.png 24w, https://labplot.org/wp-content/uploads/2024/07/2.11-36x18.png 36w, https://labplot.org/wp-content/uploads/2024/07/2.11-48x24.png 48w" sizes="auto, (max-width: 470px) 100vw, 470px"></p>

    			</div>

			<div>
				<a href="https://labplot.org/2024/07/16/labplot-2-11-released/">
					<h2 itemprop="headline">
												LabPlot 2.11 released					</h2>
				</a>

				

				<p>Say hello to LabPlot 2.11! This brand new release comes with many new features, improvements and performance enhancements in several areas, as well as support for more data formats and visualisation types. The main new features are outlined below. For a more detailed overview of…</p>

				
			</div>

		</div>
	</article>



	<article id="post-2847" itemscope="itemscope" itemtype="http://schema.org/Article">
		<div>
			
			<div>
				
									<p><img loading="lazy" decoding="async" width="470" height="236" src="https://labplot.org/wp-content/uploads/2023/07/LabPlot_2.10.1.png" alt="" srcset="https://labplot.org/wp-content/uploads/2023/07/LabPlot_2.10.1.png 470w, https://labplot.org/wp-content/uploads/2023/07/LabPlot_2.10.1-300x151.png 300w, https://labplot.org/wp-content/uploads/2023/07/LabPlot_2.10.1-24x12.png 24w, https://labplot.org/wp-content/uploads/2023/07/LabPlot_2.10.1-36x18.png 36w, https://labplot.org/wp-content/uploads/2023/07/LabPlot_2.10.1-48x24.png 48w" sizes="auto, (max-width: 470px) 100vw, 470px"></p>

    			</div>

			<div>
				<a href="https://labplot.org/2023/07/12/labplot-2-10-1-released/">
					<h2 itemprop="headline">
												LabPlot 2.10.1 released					</h2>
				</a>

				

				<p>Today we are announcing the availability of the minor patch release 2.10.1. This release contains minor improvements and bug fixes only. The fixes are distributed over many different areas of the application and we recommend everybody update to this patch release which is available from…</p>

				
			</div>

		</div>
	</article>


	</div>
            
        </div><div id="gallery" data-vc-full-width="true" data-vc-full-width-init="false">
<div id="heading-8BdjTuVR0o">
	<p>

		
			<h2>
				GALLERY			</h2>

		
	</p>
</div>
            
        
            
            

        
<div id="heading-A1ye4mWuT0">
	<p>

		
			<h2>
				Get a first impression			</h2>

		
	</p>
</div>
            
        </div><div id="galery" data-vc-full-width="true" data-vc-full-width-init="false" data-vc-stretch-content="true">
	<p>

		
			<h2>
				Want to see more?			</h2>

		
	</p>
</div><div id="heading-nSVtR6mQ2I" data-vc-full-width="true" data-vc-full-width-init="false">
	<p>

		
			<h2>
				Like what you see and curious to try it out?			</h2>

		
	</p>
</div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What is going on right now? (306 pts)]]></title>
            <link>https://catskull.net/what-the-hell-is-going-on-right-now.html</link>
            <guid>44981747</guid>
            <pubDate>Fri, 22 Aug 2025 07:08:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://catskull.net/what-the-hell-is-going-on-right-now.html">https://catskull.net/what-the-hell-is-going-on-right-now.html</a>, See on <a href="https://news.ycombinator.com/item?id=44981747">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
    
    <div>
      <p>What the <em>hell</em> is going on right now?</p>

<p>Engineers are burning out. Orgs expect their senior engineering staff to be able to review and contribute to “vibe-coded” features that don’t work. My personal observation is that the best engineers are highly enthusiastic about helping newer team members contribute and learn.</p>

<p>Instead of their comments being taken to heart, reflected on, and used as learning opportunities, hapless young coders are instead using feedback as simply the next prompt in their “AI” masterpiece. I personally have witnessed and heard first-hand accounts where it was incredibly obvious a junior engineer was (ab)using LLM tools.</p>

<p>In a recent company town-hall, I watched as a team of junior engineers demoed their latest work. I couldn’t tell you what exactly it did, or even what it was supposed to do - it didn’t seem like they themselves understood. However, at a large enough organization, it’s not about what you do, its about what people <em>think</em> you do. Championing their “success”, a senior manager goaded them into bragging about their use of “AI” tools to which they responded “This is four thousand lines of code written by Claude”. Applause all around.</p>

<p>I was asked to add a small improvement to an existing feature. After reviewing the code, I noticed a junior engineer was the most recent to work on that feature. As I always do, I reached out to let them know what I’d be doing and to see if they had any insight that would be useful to me. Armed with the Github commit URL, I asked for context around their recent change. I can’t know <em>for sure</em>, but I’d be willing to put money down that my exact question and the commit were fed directly into an LLM which was then copy and pasted back to me. I’m not sure why, but I felt violated. It felt wrong.</p>

<p>A friend recently confided in me that he’s been on a team of at least 5 others that have been involved in reviewing a heavily vibe-coded PR over the past <em>month</em>. A month. Reviewing slop produced by an LLM. What are the cost savings of paying ChatGPT $20 a month and then having a literal <em>team</em> of engineers try and review and merge the code?</p>

<p>Another friend commiserated the difficulty of trying to help an engineer contribute at work. “I review the code, ask for changes, and then they <em>immediately</em> hit me with another round of AI slop.”</p>

<p>Here’s the thing - we <em>want</em> to help. We <em>want</em> to build good things. Things that work well, that make people’s lives easier. We want to teach people how to do software engineering! Any engineer is standing entirely on the shoulders of their mentors and managers who’ve invested time and energy into them and their careers. But what good is that investment if it’s simply copy-pasted into the latest “model” that “is literally half a step from artificial general intelligence”? Should we instead focus our time and energy into training the models and eliminate the juniors altogether?</p>

<p>What a sad, dark world that would be.</p>

<p>Here’s an experiment for you: stop using “AI”. Try it for a day. For a week. For a month.</p>

<p>Recently, I completely reset my computer. I like to do it from time to time. As part of that process I prune out any software that I no longer use. I’ve been paying for Claude Pro for about 6 months. But slowly, I’ve felt like it’s just a huge waste of time. Even if I have to do a few independent internet searches and read through a few dozen stack overflow and doc pages, my own conclusion is so much more reliable and accurate than anything an LLM could ever spit out.</p>

<p>So what good are these tools? Do they have any value whatsoever?</p>

<p>Objectively, it would seem the answer is no. But at least they make a lot of money, right?</p>

<p>Is anyone making money on AI right now? I see a pipeline that looks like this:</p>

<ul>
  <li>“AI” is applied to some specific, existing area, and a company spins up around it because it’s so much more “efficient”</li>
  <li>AI company gets funding from venture capitalists</li>
  <li>AI company give funding to AI service providers such as OpenAI in the form of paying for usage credits</li>
  <li>AI company evaporates</li>
</ul>

<p>This isn’t necessarily all that different than the existing VC pipeline, but the difference is that not even OpenAI is making money right now. I believe this is because the technology is inherently flawed and cannot scale to meet the demand. It simply consumes too much electricity to ever be economically viable, not to mention the serious environmental concerns.</p>

<p>We can say our prayers that Moore’s Law will come back from the dead and save us. We can say our prayers that the heat death of the universe will be sufficiently prolonged in order for every human to become a billionaire. We can also take an honestly not even hard look at reality and realize this is a scam.</p>

<p>The emperor is wearing no clothes.</p>

    </div>
  </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It’s not wrong that "🤦🏼‍♂️".length == 7 (2019) (139 pts)]]></title>
            <link>https://hsivonen.fi/string-length/</link>
            <guid>44981525</guid>
            <pubDate>Fri, 22 Aug 2025 06:18:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hsivonen.fi/string-length/">https://hsivonen.fi/string-length/</a>, See on <a href="https://news.ycombinator.com/item?id=44981525">Hacker News</a></p>
<div id="readability-page-1" class="page">
<hgroup>
	
	<h2>But It’s Better that <code>"🤦🏼‍♂️".len() == 17</code> and Rather Useless that <code>len("🤦🏼‍♂️") == 5</code></h2>
</hgroup>
<p>From time to time, someone shows that in JavaScript the <code>.length</code> of a string containing an emoji results in a number greater than 1 (typically 2) and then proceeds to the conclusion that haha JavaScript is so broken—and is rewarded with many likes. In this post, I will try to convince you that ridiculing JavaScript for this is less insightful than it first appears and that Swift’s approach to string length isn’t unambiguously the best one. Python 3’s approach is unambiguously the worst one, though.</p>
<h2>What’s Going on with the Title?</h2>
<p><code>"🤦🏼‍♂️".length == 7</code> evaluates to <code>true</code> as JavaScript. Let’s try JavaScript console in Firefox:</p>
<pre>"🤦🏼‍♂️".length == 7
true</pre>
<p>Haha, right? Well, you’ve been told that the Python community suffered the Python 2 vs. Python 3 split, among other things, to Get Unicode Right. Let’s try Python 3:</p>
<pre>$ python3
Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; len("🤦🏼‍♂️") == 5
True
&gt;&gt;&gt; </pre>
<p>OK, then. Now, Rust has the benefit of learning from languages that came before it. Let’s try Rust:</p>
<pre>$ cargo new -q length
$ cd length
$ echo 'fn main() { println!("{}", "🤦🏼‍♂️".len() == 17); }' &gt; src/main.rs
$ cargo run -q
true
</pre>
<p>That’s better!</p>
<h2>What?</h2>
<p>The string contains a single emoji consisting of five Unicode scalar values:</p>
<table>
	<thead>
		<tr><th>Unicode scalar</th><th>UTF-32 code units</th><th>UTF-16 code units</th><th>UTF-8 code units</th><th>UTF-32 bytes</th><th>UTF-16 bytes</th><th>UTF-8 bytes</th></tr>
	</thead>
	<tbody>
		<tr><td>U+1F926 FACE PALM</td><td>1</td><td>2</td><td>4</td><td>4</td><td>4</td><td>4</td></tr>
		<tr><td>U+1F3FC EMOJI MODIFIER FITZPATRICK TYPE-3</td><td>1</td><td>2</td><td>4</td><td>4</td><td>4</td><td>4</td></tr>
		<tr><td>U+200D ZERO WIDTH JOINER</td><td>1</td><td>1</td><td>3</td><td>4</td><td>2</td><td>3</td></tr>
		<tr><td>U+2642 MALE SIGN</td><td>1</td><td>1</td><td>3</td><td>4</td><td>2</td><td>3</td></tr>
		<tr><td>U+FE0F VARIATION SELECTOR-16</td><td>1</td><td>1</td><td>3</td><td>4</td><td>2</td><td>3</td></tr>
	</tbody>
	<tfoot>
		<tr><th>Total</th><td>5</td><td>7</td><td>17</td><td>20</td><td>14</td><td>17</td></tr>
	</tfoot>
</table>
<p>The string that contains one graphical unit consists of 5 Unicode scalar values. First, there’s a base character that means a person face palming. By default, the person would have a cartoonish yellow color. The next character is an emoji skintone modifier the changes the color of the person’s skin (and, in practice, also the color of the person’s hair). By default, the gender of the person is undefined, and e.g. Apple defaults to what they consider a male appearance and e.g. Google defaults to what they consider a female appearance. The next two scalar values pick a male-typical appearance specifically regardless of font and vendor. Instead of being an emoji-specific modifier like the skin tone, the gender specification uses an emoji-predating gender symbol (MALE SIGN) explicitly ligated using the ZERO WIDTH JOINER with the (skin-toned) face-palming person. (Whether it is a good or a bad idea that the skin tone and gender specifications use different mechanisms is out of the scope of this post.) Finally, VARIATION SELECTOR-16 makes it explicit that we want a multicolor emoji rendering instead of a monochrome dingbat rendering.</p>

<p>Each of the languages above reports the string length as the number of <i>code units</i> that the string occupies. Python 3 strings store Unicode code points each of which is stored as one code unit by CPython 3, so the string occupies 5 code units. JavaScript (and Java) strings have (potentially-invalid) UTF-16 semantics, so the string occupies 7 code units. Rust strings are (guaranteed-valid) UTF-8, so the string occupies 17 code units. We’ll come to back to the actual <i>storage</i> as opposed to <i>semantics</i> later.</p>

<p>Note about Python 3 added on 2019-09-09: Originally this article claimed that Python 3 guaranteed UTF-32 validity. This was in error. Python 3 guarantees that the units of the string stay within the Unicode code point range but does not guarantee the absence of surrogates. It not only allows unpaired surrogates, which might be explained by wishing to be compatible with the value space of potentially-invalid UTF-16, but Python 3 allows materializing even surrogate pairs, which is a truly bizarre design. The previous conclusions stand with the added conclusion that Python 3 is even more messed up than I thought! With the way the example string was constructed in Python 3, the Python 3 string happens to match the valid UTF-32 representation of the string, so it is still illustrative of UTF-32, but the rest of the article has been slightly edited to avoid claiming that Python 3 used UTF-32.</p>

<h2>But I Want the Length to Be 1!</h2>

<p>There’s a language for that. The following used Swift 4.2.3, which was the latest release when I was researching this, on Ubuntu 18.04:</p>

<pre>$ mkdir swiftlen
$ cd swiftlen/
$ swift package init -q --type executable
$ swift package init --type executable
Creating executable package: swiftlen
Creating Package.swift
Creating README.md
Creating .gitignore
Creating Sources/
Creating Sources/swiftlen/main.swift
Creating Tests/
Creating Tests/LinuxMain.swift
Creating Tests/swiftlenTests/
Creating Tests/swiftlenTests/swiftlenTests.swift
Creating Tests/swiftlenTests/XCTestManifests.swift
$ echo 'print("🤦🏼‍♂️".count == 1)' &gt; Sources/swiftlen/main.swift 
$ swift run swiftlen 2&gt;/dev/null
true</pre>

<p>(Not using the Swift REPL for the example, because it does not appear to accept non-ASCII input on Ubuntu! Swift 5.0.3 prints the same and the REPL is still broken.)</p>

<p>OK, so we’ve found a language that thinks the string contains one countable unit. But what is that countable unit? It’s an <i>extended grapheme cluster</i>. (“Extended” to distinguish from the older attempt at defining grapheme clusters now called <i>legacy grapheme clusters</i>.) The definition is in <a href="http://www.unicode.org/reports/tr29/">Unicode Standard Annex #29</a> (UAX #29).</p>

<h2>The Lengths Seen So Far</h2>

<p>We’ve seen four different lengths so far:</p>
<ul>
	<li>Number of UTF-8 code units (17 in this case)</li>
	<li>Number of UTF-16 code units (7 in this case)</li>
	<li>Number of UTF-32 code units or Unicode scalar values (5 in this case)</li>
	<li>Number of extended grapheme clusters (1 in this case)</li>
</ul>

<p>Given a valid Unicode string and a version of Unicode, all of the above are well-defined and it holds that each item higher on the list is greater or equal than the items lower on the list.</p>

<p>One of these is not like the others, though: The first three numbers have an unchanging definition for any valid Unicode string whether it contains currently assigned scalar values or whether it is from the future and contains unassigned scalar values as far as software written today is aware. Also, computing the first three lengths does not involve lookups from the Unicode database. However, the last item depends on the Unicode version and involves lookups from the Unicode database. If a string contains scalar values that are unassigned as far as the copy of the Unicode database that the program is using is aware, the program will potentially overcount extended grapheme clusters in the string compared to a program whose copy of the Unicode database is newer and has assignments for those scalar values (and some of those assignments turn out to be combining characters).</p>

<h2>More Than One Length per Programming Language</h2>

<p>It is not the case that a given programming language has to choose only one of the above. If we run this Swift program:</p>

<pre>var s = "🤦🏼‍♂️"
print(s.count)
print(s.unicodeScalars.count)
print(s.utf16.count)
print(s.utf8.count)</pre>

<p>it prints:</p>

<pre>1
5
7
17</pre>

<p>Let’s try Rust with <code>unicode-segmentation = "1.3.0"</code> in <code>Cargo.toml</code>:</p>

<pre>use unicode_segmentation::UnicodeSegmentation;

fn main() {
	let s = "🤦🏼‍♂️";
	println!("{}", s.graphemes(true).count());
	println!("{}", s.chars().count());
	println!("{}", s.encode_utf16().count());
	println!("{}", s.len());
}</pre>

<p>The above program prints:</p>

<pre>2
5
7
17</pre>

<p>That’s unexpected! It turns out that <code>unicode-segmentation</code> does not implement the latest version of the Unicode segmentation rules, so it gives the ZERO WIDTH JOINER generic treatment (break right after ZWJ) instead of the newer refinement in the emoji context.</p>

<p>Let’s try again, but this time with <code>unic-segment = "0.9.0"</code> in <code>Cargo.toml</code>:

</p><pre>use unic_segment::Graphemes;

fn main() {
	let s = "🤦🏼‍♂️";
	println!("{}", Graphemes::new(s).count());
	println!("{}", s.chars().count());
	println!("{}", s.encode_utf16().count());
	println!("{}", s.len());
}</pre>

<pre>1
5
7
17</pre>

<p>In the Rust case, strings (here mere string slices) know the number of UTF-8 code units they contain. The <code>len()</code> method call just returns this number that has been stored since the creation of the string (in this case, compile time). In the other cases, what happens is the creation of an iterator and then instead of actually examining the values (string slices correspoding to extended grapheme clusters, Unicode scalar values or UTF-16 code units) that the iterator would yield, the <code>count()</code> method just consumes the iterator and returns the number of items that were yielded by the iteration. The count isn’t stored anywhere on the string (slice) afterwards. If we wanted to later know the counts again, we’d have to iterate over the string again.</p>

<h2>Know in Advance or Compute When Needed?</h2>

<p>This introduces a notable question in the design space: Should a given type of length quantity be eagerly computed when the string is created? Or should the length be computed when someone asks for it? Or should it be computed when someone asks for it and then automatically stored on the string object so that it’s available immediately if someone asks for it again?</p>

<p>The answer Rust has is that the length in the code units of the Unicode Encoding Form of the language is stored upon string creation, and the rest are computed when someone asks for them (and then forgotten and not stored on the string).</p>

<p>Swift is a higher-level language and doesn’t document the exact nature of its string internals as part of the API contract. In fact, the internal representation of Swift strings changed substantially between Swift 4.2 and Swift 5.0. It’s not documented if different views to the string are held onto once created, for example. The documentation does say that strings are copy-on-write, so the first mutation may involve copying the string’s storage.</p>

<p>Notably, the design space includes not remembering anything. The C programming language is a prominent example of this case. C strings don’t even remember their number of code units. To find out the number of code units, you have to iterate over the string until a sentinel value. In the case of C, the sentinel is the code unit for U+0000, so it excludes one Unicode scalar value from the possible string contents. However, that’s not a strictly necessary property of a sentinel-based design that doesn’t remember any lengths. 0xFF does not occur as a code unit in any valid UTF-8 string and 0xFFFFFFFF does not occur in any valid UTF-32 string, so they could be used as sentinels for UTF-8 and UTF-32 storage, respectively, without excluding a scalar value from the Unicode value space. There is no 16-bit value that never occurs in a valid UTF-16 string. However, a valid UTF-16 string does not contain unpaired surrogates, so an unpaired low surrogate could, in principle, be used as a sentinel in a design that wanted to use guaranteed-valid UTF-16 strings that don’t remember their code unit length.</p>

<h2>Knowing the Storage-Native Code Unit Length is Extremely Reasonable</h2>

<p>The length of the string as counted in code units of its storage-native Unicode Encoding Form (i.e. whichever of UTF-8, UTF-16, and UTF 32 the programming language has chosen for its string semantics) is not like the other lengths. It is the length that the implementation cannot avoid having to know at the time of creating a new string, because it is the length that is required to be known in order to be able to allocate storage for a string. Even C, which promptly forgets about the code unit length in the storage-native Unicode Encoding Form after string has been created, has to know this length when allocating storage for a new string.</p>

<p>That is, the design decision is about whether to remember this length. It is not about whether to compute it eagerly. You just have to have it at string creation time—i.e. eagerly.</p>

<p>Considering that remembering this quantity makes string concatenation, which is a common operation, substantially faster to implement compared to not remembering this quantity, remembering this quantity is fundamentally reasonable. Also, it means that you don’t need to maintain a sentinel value, which means that a substring operation can yield results that share the buffer with the original string instead of having to copy in order to be able to insert sentinel. (Note that you can easily foil this benefit if you wish to eagerly maintain zero-termination for the sake of C string compatibility.)</p>

<h2>What About Knowing the Other Lengths?</h2>

<p>Even if we’ve established that it makes sense for string implementation to remember the storage length of the string in code units all the storage-native Unicode encoding form, it doesn’t answer whether a string implementation should also remember other lengths or which kind of length should be offered in the most ergonomic API. (As we see above, Swift makes the number of extended grapheme clusters more ergonomic to obtain that the code unit or scalar value length.)</p>

<p>Also, if any other length is to be remembered, there is the question of whether it should be eagerly computed as string creation time or lazily computed the first time someone asks for it. It is easy to see why at least the latter does not make sense for multi-threaded systems-programming language like Rust. If some properties of an object are lazily initialized, in a multi-threaded case you also need to solve synchronization of these computations. Furthermore, you need to allocate space at least for a pointer to auxiliary information if you want to be able to add auxiliary information later or you need to have a hashtable of auxiliary information where the string the information is about is the key, so auxiliary information, even when not present, has storage implications or implications of having to have global state in a run-time system. Finally, for systems programming, it may be more desirable to know the time complexity of a given operation clearly even if it means “always O(n)” instead of “possibly O(n) but sometimes O(1)”. Even if the latter looks strictly better, it is less <i>predictable</i>.</p>

<p>For a higher-level language, arguments from space requirements or synchronization issues might not be decisive. It’s more relevant to consider what a given length quantity is <i>used for</i>. This is often forgotten in Internet debates that revolve around what length is the most “correct” or “logical” one. So for the lengths that don’t map to the size of storage allocation, what are they good for?</p>

<p>It turns out that in the Firefox code base there are two places where someone wants to know the number of Unicode scalar values in a string that is not being stored as UTF-32 and attention is not paid to what the scalar values actually are. The IETF specification for Session Traversal Utilities for NAT (STUN) used for WebRTC has the curious property that it places length limits on certain protocol strings such that the limits are expressed as number of Unicode scalar values but the strings are transmitted in UTF-8. Firefox validates these limits. (The limit looks like an arbitrary power-of-two (128 scalar values). The spec has remarks about the possible resulting byte length, which was wrong according to the IETF UTF-8 RFC that was current and already nearly five years old at the time of publication of the STUN RFC. Specifically, the STUN RFC repeatedly says that 128 characters as UTF-8 may be as long as 763 bytes. To arrive at that number, you have to assume that a UTF-8 character can be up to six bytes long, as opposed to up to 4 bytes long as in the prevailing UTF-8 RFC and in the Unicode Standard, and that the last character of the 128 is a zero terminator and, therefore, known to take just one byte.) In this case, the reason for wishing to know a non-storage length is to <i>impose a limit</i>. The other case is reporting the column number for the source location of JavaScript errors.</p>

<p>Length limits, which we’ll come back to, probably aren’t a frequent enough a use case to justify making strings know a particular kind of length as opposed to such length being possible to compute when asked for. Neither are error messages.</p>

<p>Another use case for asking for a length is iterating by index and using the length as the loop termination condition 1990s Java style. Like this:</p>

<pre>for (int i = 0; i &lt; s.length(); i++) {
    // Do something with s.charAt(i)
}</pre>

<p>In this case, it’s actually important for the length to be precomputed number on the string object. This use case is coupled with the requirement that indexing into the string to find the <i>n</i>th unit corresponding to the count of units that the “length” represents should be a fast operation.</p>

<p>The above pattern is a lot less conclusive in terms of what lengths should be precomputed (and what the indexing unit should be) than it first appears. The above loop doesn’t do random access by index. It sequentially uses every index from zero up to, but not including, <code>length</code>. Indeed, especially when iterating over a string by Unicode scalar value, typically when you examine the contents of a string, you iterate over the string in order. Programming languages these days provide an <i>iterator</i> facility for this, and e.g. to iterate over a UTF-8 string by scalar value, the iterator does not need to know the number of scalar values up front. E.g. in Rust, you can do this in O(n) time despite string slices not knowing their number of Unicode scalar values:</p>

<pre>for (c in s.chars()) {
    // Do something with c
}</pre>

<p>(Note that <code>char</code> is an 8-bit code unit (possibly UTF-8 code unit) in C and C++, <code>char</code> is a UTF-16 code unit in Java, <code>char</code> is a Unicode scalar value in Rust, and <code>Character</code> is an extended grapheme cluster in Swift.)</p>

<p>A programming language together with its library ecosystem should provide iteration over a string by Unicode scalar value and by extended grapheme cluster, but it does not follow that strings would need to know the scalar value length or the extended grapheme cluster length up front. Unlike the code unit storage length, those quantities aren’t useful for accelerating operations like concatenation that don’t care about the exact content of the string.

</p><h2>Which Unicode Encoding Form Should a Programming Language Choose?</h2>

<p>The observation that having strings know their code unit length in their storage-native Unicode encoding form is extremely reasonable does not answer how many bits wide the code units should be.</p>

<p>The usual way to approach this question is to argue that UTF-32 is the best, because it provides O(1) indexing by “character” in the sense of a character meaning a Unicode scalar value, or the argument focuses on whether UTF-8 is unfair to some languages relative to UTF-16. I think these are bad ways to approach this question.</p>

<p>First of all, the argument that the answer should be UTF-32 is bad on two counts. First, it assumes that random access scalar value is important, but in practice it isn’t. It’s reasonable to want to have a capability to iterate over a string by scalar value, but random access by scalar value is in the YAGNI department. Second, arguments in favor of UTF-32 typically come at a point where the person making the argument has learned about surrogate pairs in UTF-16 but has not yet learned about extended grapheme clusters being even larger things that the user perceives as unit. That is, if you escape the variable-width nature of UTF-16 to UTF-32, you pay by doubling the memory requirements and extended grapheme clusters are <i>still</i> variable-width.</p>

<p>I’ll come back to the length fairness issue later, but I think a different argument is much more relevant <i>in practice</i> for the choice of in-memory Unicode encoding form. The more relevant argument is this: Implementations that choose UTF-8 actually accept the UTF-8 storage requirements. When wider-unit semantics are chosen for a language that doesn’t provide raw memory access and, therefore, has the opportunity to tweak string storage, the implementations try to come up with ways to avoid actually paying the cost of the wider units in some situations.</p>

<p>JavaScript and Java strings have the semantics of potentially-invalid UTF-16. SpiderMonkey and V8 implement an optimization for omitting the leading zeros of each code unit in a string, i.e. storing the string as ISO-8859-1 (the actual ISO-8859-1, not the Web notion of “ISO-8859-1” as a label of windows-1252), when all code units in the string have zeros in the most-significant half. The HotSpot JVM also implements this optimization, though enabling it is optional. Swift 4.2 implements a slightly different variant of the same idea, where ASCII-only strings are stored as 8-bit units and everything else is stored as UTF-16. CPython since 3.3 makes the same idea three-level with code point semantics: Strings are stored with 32-bit code units if at least one code point has a non-zero bit above the low 16 bits. Else if a string has a non-zero bits above the low 8 bits for at least one code point, the string is stored as 16-bit units. Otherwise, the string is stored as 8-bit units (Latin1).</p>

<p>I think the unwillingness of implementations of languages that have chosen UTF-16 or UTF-32 (or UTF-32-ish as in the case of Python 3) string <i>semantics</i> to actually use UTF-16 or UTF-32 <i>storage</i> when they can get away with not using actual UTF-16 or UTF-32 storage is the clearest indictment against UTF-16 or UTF-32 (and other wide-unit semantics like what Python 3 uses).</p>

<p>Languages that choose UTF-8, on the other hand, stick to actual UTF-8 for the purpose of storing Unicode scalar values. When languages that choose UTF-8 deviate from UTF-8, they do so in order to represent values that are not Unicode scalar values for compatibility with external constraints. Rust uses a representation called <a href="https://simonsapin.github.io/wtf-8/">WTF-8</a> for file system paths on Windows. All UTF-8 strings are WTF-8 strings, but WTF-8 can also represent unpaired surrogates for compatibility with Windows file paths being sequences of 16-bit units that can contain unpaired surrogates. Perl 6 uses an internal representation called <a href="https://docs.perl6.org/language/unicode#UTF8-C8">UTF-8 Clean-8</a> (or UTF8-C8), which represents strings that consist of Unicode scalar values in Unicode Normalization Form C the same way as UTF-8 but represents non-NFC content differently and can represent sequences of bytes that are not valid UTF-8.</p>

<p>UTF-8 is the only one of the Unicode <i>encoding forms</i> that is also a Unicode <i>encoding scheme</i>, and of the Unicode encoding schemes, UTF-8 has clearly won for interchange. (Unicode encoding forms are what you have in RAM, so UTF-16 consists of native-endian, two-byte-aligned 16-bit code units. Unicode encoding schemes are what can be used for byte-oriented interchange, so e.g. UTF-16LE consist of 8-bit code units every pair of which form a potentially-unaligned little-endian 16-bit number, which in turn may form a surrogate pair.) When UTF-8 is used as the in-RAM representation, input and output operations are less expensive than with UTF-16 or UTF-32. UTF-16 or UTF-32 in RAM requires conversion from UTF-8 when reading input and conversion to UTF-8 when writing output. A system that guarantees UTF-8 validity internally, such as Rust, needs only to <i>validate</i> UTF-8 upon reading input and no conversion is needed when writing output. (Go takes a garbage in, garbage out approach to UTF-8: input is not validated at input time and output is written without conversion. However, iteration by scalar value can yield REPLACEMENT CHARACTERs when iterating over invalid UTF-8. That is, the input step is less expensive than in Rust, but iterating by scalar value is marginally more expensive. The output step is less correct.)

</p><p>Finally, in terms of nudging developers to write correct code, UTF-8 has the benefit of being blatantly variable-width, so even with languages such as English, Somali, and Swahili, as soon as you have a dash or a smart quote, the variable-width nature of UTF-8 shows up. In this context, extended grapheme clusters are just extending the variable-width nature. Meanwhile, UTF-16 allows programmers to get too far while pretending to be working with something where the units they need to care about are fixed-width. Reacting to surrogate pairs by wishing to use UTF-32 instead is a bad idea, because if you want to write correct software, you still need to deal with variable-width extended grapheme clusters.

</p><p>The choice of UTF-32 (or Python 3-style code point sequences) arises from wanting the wrong thing. The choice of UTF-16 is a matter of early-adopter legacy from the time when Unicode was expected to be capped to 16 bits of code space and, once UTF-16 has been committed to, not breaking compatibility with already-written programs is important and justified the continued use of UTF-16, but if you aren’t bound by that legacy and are designing a new language, you should go with UTF-8. Occasionally even systems that appear to be bound by the UTF-16 legacy can break free. Even though Swift is committed to interoperability with Cocoa, which uses UTF-16 strings, <a href="https://swift.org/blog/utf8-string/">Swift 5 switched to UTF-8</a> for Swift-native strings. Similarly, <a href="https://morepypy.blogspot.com/2019/03/pypy-v71-released-now-uses-utf-8.html?m=1">PyPy has gone UTF-8</a> despite Python 3 having code point semantics.</p>

<h2>Shouldn’t the Nudge Go All the Way to Extended Grapheme Clusters?</h2>

<p>Even if we accept that the storage should be UTF-8 and that the string implementation should maintain knowledge of the string length in UTF-8 code units, if the blatant variable-widthness of UTF-8 is argued to be a nudge toward dealing with the variable-widthness of extended grapheme clusters, shouldn’t the Swift approach of making extended grapheme cluster access and count the view that takes the least ceremony to use be the thing that every language should do?</p>

<p>Swift is still too young to draw definitive conclusions from. It’s easy to believe that the Swift approach nudges programmers to write more extended grapheme cluster-correct code and that the design makes sense for a language meant primarily for UI programming on a largely evergreen platform (iOS). It isn’t clear, though, that the Swift approach is the best for everyone.</p>

<p>Earlier, I said that the example used “Swift 4.2.3 on Ubuntu 18.04”. The “18.04” part is important! Swift.org ships binaries for Ubuntu 14.04, 16.04, and 18.04. Running the program</p>

<pre>var s = "🤦🏼‍♂️"
print(s.count)
print(s.unicodeScalars.count)
print(s.utf16.count)
print(s.utf8.count)</pre>

<p>in Swift 4.2.3 on Ubuntu 14.04 prints:</p>

<pre>3
5
7
17</pre>

<p>So Swift 4.2.3 <i>on Ubuntu 18.04</i> as well as the <code>unic_segment</code> 0.9.0 Rust crate counted one extended grapheme cluster, the <code>unicode-segmentation</code> 1.3.0 Rust crate counted two extended grapheme clusters, and <i>the same version of Swift</i>, 4.2.3, but on a <i>different operating system version</i> counted three extended grapheme clusters!</p>

<p>Swift 4 delegates Unicode segmentation to operating system-provided ICU, and “Long-Term Support” in the Ubuntu case means security patches but does not mean rolling forward the Unicode version that the system copy of ICU knows about. In the case of iOS, delegating to system ICU is probably OK and will not lead to too high probability of the text being from the future from the point of view of the OS copy of ICU, since the iOS ecosystem stays exceptionally well <a href="https://developer.apple.com/support/app-store/">up-to-date</a>. However, delegating to system ICU is not such a great match for the idea of using Swift on the server side if the server side means running an old LTS distro.</p>

<p>(Swift 5 appears to no longer use system ICU for this. That is, Swift 5.0.3 on Ubuntu 14.04 sees one extended grapheme cluster in the string. I haven’t investigated what Swift 5 uses, but I assume that the switch to UTF-8 string representation necessitated using something other than ICU, which is heavily UTF-16-oriented. However, the result with Swift 4.2.3 nicely illustrates the issue related to using extended grapheme clusters.)</p>

<p>If you are doing things that <i>have to</i> be extended grapheme cluster-aware, there just is no way around the issue of not being able to correctly segment text that comes from the future relative to the Unicode segmentation implementation that your program is using. This is not a reason to avoid extended grapheme clusters for tasks that <i>require</i> awareness of extended grapheme clusters.</p>

<p>However, pushing extended grapheme clusters onto tasks that do not really require the use of extended grapheme cluster introduces failure modes arising from the Unicode version dependency where such a dependency isn’t strictly necessary. For example, the Unicode version dependency of extended grapheme clusters means that you should <i>never</i> persist indices into a Swift strings and load them back in a future execution of your app, because an intervening Unicode data update may change the meaning of the persisted indices! The Swift string documentation does not warn against this.</p>

<p>You might think that this kind of thing is a theoretical issue that will never bite anyone, but even experts in data persistence, the developers of PostgreSQL, managed to <a href="https://www.postgresql.org/message-id/flat/BA6132ED-1F6B-4A0B-AC22-81278F5AB81E%40tripadvisor.com">make backup restorability dependent on collation order</a>, which may change with glibc updates.</p>

<p>Let’s consider other languages a bit.</p>

<p>C++ is often deployed such that the application developer doesn’t ship the standard library with the program. Most obviously, relying on GNU libstdc++ provided by an LTS Linux distribution presents similar problems as Swift 4 relying on ICU provided by an LTS Linux distribution. This isn’t a Linux-specific issue. Old supported branches of Windows <a href="https://github.com/sg16-unicode/sg16-meetings#may-22nd-2019">generally don’t get new system-level Unicode data</a>, either. Even though there is some movement towards individual applications shipping their own copy of LLVM libc++ with the application and the increased pace of C++ standard development starting with C++11 has made using a system-provided C++ standard library more problematic even ignoring Unicode considerations, it doesn’t seem like a good idea for C++ to develop a tight coupling with extended grapheme clusters for operations that don’t strictly necessitate it as longs as stuck-in-the-past system libraries (whether the C++ standard library itself or another library that it delegates to) are a significant part of the C++ standard library distribution practice.</p>

<p>There’s <a href="https://github.com/tc39/proposal-intl-segmenter">a proposal</a> to expose extended grapheme cluster segmentation to JavaScript programs. The main problem with this proposal is the implication on APK sizes on Android and the effect of APK sizes on browser competition on Android. But if we ignore that for the moment and imagine this was part of the Web Platform, it would still be problematic to build this dependency into operations for which working on extended grapheme clusters isn’t strictly necessary. While the most popular browsers are evergreen, there’s still a long tail of browser instances that aren’t on the latest engine versions. When JavaScript executes on such browsers, there’d be effects similar to running Swift 4 on Ubuntu 14.04.</p>

<p>In contrast to C++ or JavaScript, the current Rust approach is to statically link all Rust library code, including the standard library, into the executable program. This means that the application distributor is in control of library versions and doesn’t need to worry about the program executing in the context of out-of-date <i>Rust</i> libraries. The flip side is concerns about the size of the executable. People already (rightly or wrongly) complain about the sizes of Rust executables. Pulling in a lot of Unicode data due to baking extended grapheme cluster processing into programs whose problem domain doesn’t strictly require working with extended grapheme clusters would be problematic in embedded contexts where the executable size is a real problem and not just a perceived problem—and would obviously make the perceived problem worse, too. Furthermore, in order to avoid problems similar to those involved in relying on system libraries, baking tight coupling with Unicode data into the standard library necessitates the organizational capability of keeping up with new Unicode versions in this area where not only data in the tables keeps changing but the format of the tables and, therefore, the associated algorithms have still been changing recently. Right now of the two extended grapheme cluster crates outside the Rust standard library, the one that’s organizationally closer to the standard library is the one that’s out of date.</p>

<h2>Why Do We Want to Know Anyway?</h2>

<blockquote>
	<p>“String length is about as meaningful a measurement as string height” – <a href="https://mobile.twitter.com/qntm/status/1118993107310325760">@qntm</a></p>
</blockquote>

<p>Being able to allocate memory for strings gives a legitimate use case for knowing the storage length. However, in cases of Unicode scalar values or extended grapheme clusters, you typically want to iterate over them and look at each one instead of just knowing the count. So why do people want to know the count? As far as I can tell, there are two broad categories: Placing a quota limit that is fuzzy enough that it doesn’t need to be strictly tied to storage and trying to estimate how much text fits for display. Let’s look at the issue of estimating how much display space text takes, because it involves introducing yet another measurement of string length.</p>

<h2>Display Space</h2>

<p>Simply looking at the Latin letters i and m should make it clear that the display size of a string depends on the font and on the specific characters in the string. From this observation, the whole notion of estimating display space by counting characters seems folly. Indeed, if you want to know exactly how much text fits into a given space, you need to run a typesetting algorithm with a specific font, which may have a complex relationship between scalar values and glyphs, to actually see where the overflow starts. Yet, even in the case of the Latin script that has letters such as i and m, e.g. magazine editors can find character counts useful enough for estimating how many print pages an article of a given character count length is going to fill.</p>

<p>As for computer user interfaces, character terminal user interfaces use a monospaced font where both i and m take up one character cell on a grid. In the context of a monospaced font, the extended grapheme cluster count in the context of the Latin script corresponds directly to display space taken. The same obviously applies to the Greek and Cyrillic scripts, which are so close to the Latin script that fonts even intend to reuse glyphs across these scripts. In contrast, CJK ideographs, Japanese kana, and Hangul syllables take two cells of a terminal grid. From the CJK perspective, these are full-width characters and the ASCII characters are half-width characters. There exist also half-width katakana characters which fit into an 8-bit encoding with ASCII and take one cell on the terminal grid and, therefore, are technically easier to fit to Latin script-oriented terminal systems. The display width on a terminal also has a correspondence to byte with the legacy CJK encodings: ASCII takes one byte, a CJK ideograph, a full-width kana or a Hangul syllable takes two bytes. In the case of Shift_JIS, half-width katakana takes one byte per character.</p>

<p>This brings us to the concept of <a href="https://www.unicode.org/reports/tr11/">East Asian Width</a>. ASCII and half-width katakana characters are narrow. CJK ideographs, full-width kana, and Hangul syllables are wide. However, even in the worldview that is split to Latin, Greek, and Cyrillic on one hand and Chinese, Japanese, and Korean on the other hand, there are ambiguities. From the perspective of European legacy encodings, Greek and Cyrillic (as well as accented Latin) is equally wide as ASCII. However, in legacy CJK encodings, Greek and Cyrillic characters take two bytes. This means that in terms of East Asian Width, a string can have a general-purpose width, which resolves these ambiguous characters as narrow, or legacy CJK-context width, which resolves these ambiguous characters as wide.</p>

<p>So is the general-purpose variant (that resolves Greek and Cyrillic characters as narrow) of East Asian Width the one true string length measure? Well, no.</p>

<p>First of all, the concept ignores all scripts that are geographically and in Unicode order between Latin, Greek, and Cyrillic on one hand and CJK on the other (even though some other scripts that are structurally similar to the Latin, Greek, and Cyrillic scripts and make sense for a monospaced font, such as Armenian and the Georgian scripts, fit this concept, too, despite not having a history in pre-Unicode CJK context). As it happens, though, emoji do fit into the concept, except for <a href="https://mobile.twitter.com/fantasai/status/1080928442126909440">weird errors in the Unicode database</a>. After all, emoji originate from Japan and were two bytes each when represented using the private use area of Shift_JIS.</p>

<p>Second, the concept assumes that there is one-to-one correspondence between scalar values and extended grapheme clusters. If we run this Rust program:</p>

<pre>use unicode_width::UnicodeWidthStr;

fn main() {
    println!("{}", "🤦🏼‍♂️".width());
}</pre>

<p>it prints:</p>

<pre>5</pre>

<p>This is because the base emoji is wide (2), the combining skin tone modifier is also wide (2), the male sign is counted as narrow (1), and the zero-width joiner and the variation selector are treated as control characters that don’t count towards width. Obviously, this is not the answer that we want. The answer we want is 2. Ideas that come to mind immediately, such as only counting the width of the first character in an extended grapheme cluster or taking the width of the widest character in an extended grapheme cluster, don’t work, because flag emoji consist of two regional indicator symbol letter characters both of which have East Asian Width of Neutral (i.e. they are counted as narrow but are not marked as narrow, because they are considered to exist outside the domain of East Asian typography). I’m not aware of any official Unicode definition that would reliably return 2 as the width of every kind of emoji. 😭</p>

<p>If you really must estimate display size without running text layout with a font, whether the extended grapheme cluster count or the East Asian Width of the string works better depends on context.</p>

<h2>Arbitrary but Fair Quotas</h2>

<p>In some cases there is a desire to impose a length limit that doesn’t arise from a strict storage limitation. For example, in the STUN protocol given earlier, presumably there is a desire to make it so that human-readable error messages cannot make protocol messages arbitrarily long. For example, in the case of Twitter, tweets being short is a core part of the type of expression that Twitter is about, so some definition of “short” is needed. In the case of string-based browser <code>localStorage</code>, there is a need to have <i>some</i> limit, but the limit is necessarily arbitrary and does not need to strictly map to bytes on disk.</p>

<p>In cases like this, there seems to be some concern that the limit should be internationally fair. Observations that UTF-8 and UTF-16 take a different amount of storage per character depending on the character superficially suggests that the UTF-8 length or the UTF-16 length might be unfair internationally.</p>

<p>What’s fair, though? The usual concern goes that UTF-8 favors English, because English takes one byte per character, and disfavors CJK, because Chinese, Japanese, and Korean take three bytes per character, so UTF-8 in unfair to CJK. This kind of analysis ignores how much information is conveyed per character. To assess what lengths we get for different languages when the amount of information conveyed is kept constant, I looked at the counts for the translations of the Universal Declaration of Human Rights. This is a document for which <a href="https://www.unicode.org/udhr/translations.html">translation of the same content is available in particularly many languages</a>, which is why I used it as the measurement corpus.</p>

<p>Unfortunately, not all translations contain the same text, so one needs to be careful when preparing the data for comparison. Some translations are incomplete, in some cases, <i>very</i> incomplete. For this reason, I included only translations in stage 4 or stage 5 along the 5-stage scale. Some translations carry the preamble with the recitals, but some do not. Some also carry historical notes. To make the length comparable, the preamble, notes, and whitespace-only text nodes were omitted. The rest of the XML text nodes were concatenated and normalized to Unicode Normalition Form C before counting. (<a href="https://github.com/hsivonen/udhrlen">Source code is available</a>.)</p>

<p>Let’s look at the result. The table at the end of this document is sortable and is initially sorted by UTF-8 length. Each Δ% column shows how much the count in the column to its left deviates from the <i>median</i> count for that. (A note about color-coding. Coloring longer than median as red should not be taken to imply that those languages are somehow bad. It’s meant to imply that a length quota treats those languages badly.) In the table, the name of each language links to the translation in that language hosted on the site of the Unicode Consortium. The linked HTML versions may include the preamble and/or notes.</p>

<p>The CJK concern is alleviated when considering information conveyed. When measuring UTF-8 length, Mandarin using traditional characters is the shortest of the languages that have global name recognition! This should be expected, since the Han script pretty obviously packs more information per character than e.g. alphabetic scripts. (The globally less-known languages whose UTF-8 length is shorter than Mandarin’s (using traditional characters) are African and American Latin-script languages with a relatively small native speaker population for each—only one with a native speaker population exceeding a million and many whose native speaker population is smaller than 100 000, which explains why you might not recognize their names.)</p>

<p>Korean is also shorter than median in UTF-8 length. This also makes sense, since Hangul syllables pack three or two alphabetic jamo into one three-byte character. The UTF-8 length of Japanese is over median but only by 4.1%. The Japanese version of the text is 48% kanji and 52% hiragana. Japanese Wikipedia has almost the same kana to kanji ratio, though different kana: 46% kanji and the rest almost evenly split between hiragana and katakana, so we may assume the Universal Declaration of Human Rights to be representative of Japanese text in terms of kana to kanji ratio.</p>

<p>When sorting by UTF-16 code unit count, UTF-32 / scalar value count, or extended grapheme cluster count, CJK are the shortest. While it’s true that UTF-8 takes more bytes for CJK than UTF-16, the notion of UTF-8 being particularly disfavorable to CJK is not true <i>relative to other languages</i>. Rather, UTF-16 is particularly favorable to CJK. In particular, the Han script is so information-dense that even when sorting by East Asian Width, which effectively doubles the length of CJK but not other languages, Han-script languages stay clustered at the start of the table. Korean and Japanese move further but remain below median.</p>

<p>The language with the longest UTF-8 length is <a href="https://en.wikipedia.org/wiki/Shan_language">Shan</a>, which uses the Burmese script. The Burmese language, also using the Burmese script, is the second-longest in UTF-8 length. There are a number of other Brahmic-script languages among the ones with the longest UTF-8 length. They use three bytes per character but don’t have CJK-like information per character density. These languages are below median in extended grapheme cluster count. In scalar value count, they intermingle with alphabetic languages.</p>

<p>It’s not clear if the concepts of median and mean (average) are meaningful. Does it make sense for a language with tens of millions of native speakers to count as an equal data point as a language with tens of thousands native speakers? Since this is about writing, should the numbers of writers be considered instead? (I.e. should literacy rates be taken into account?) In the hope that with a large number of languages in the table, median hand-wavily sorts out this kind of issue, I chose to compare with median. At least the Han-script languages have comparable numbers of native speakers as the Bhramic-script languages and provide a counter-weight at the other end of the spectrum of UTF-8 length. In any case, for measures other than UTF-8 length, median and mean are very close to each other.</p>

<p>Saying that Brahmic-script languages intermingle with alphabetic languages in character count is rather meaningless, though. In character count, after CJK (and Han-script Vietnamese and Yi-script Nousu), the language with the smallest character count is a Latin-script language (Waama). Also, the language with the largest character count is a Latin-script language (Ashéninka, Pichis). (<s>I find it odd that in UTF-8 length Ashéninka Perené is the second-shortest but Ashéninka, Pichis is long enough to reach the Brahmic cluster. I don’t know what the relation of these two languages is and what explains two languages whose name suggests close relation ending up in opposite extremes in length.</s> Update: It has been pointed out to me that the supposed Ashéninka Perené translation is a mislabeled duplicate of the Cashinahua translation.)</p>

<p>One might hypothesize that the Latin script has just been put to so many uses that some of the uses have to be far from what it has been optimized for. Yet, when considering language-specific alphabets, the character counts for Greek and Georgian are above median. It just is the case that languages are different. In that sense, the whole notion of trying to find a simple length measure that is fair across languages seems folly.</p>

<p>Let’s look at the the factor between the minimum and maximum of each measure, i.e. the factor with which the minimum needs to be multiplied to get the maximum. Let’s even ignore the outlier for maximum for each measure and use the second largest value instead of the largest value for each count. (Otherwise, Ashéninka, Pichis alone would skew the numbers a lot.) We get these factors:</p>

<table>
	<tbody><tr><th>UTF-8</th><td>8.6</td></tr>
	<tr><th>UTF-16</th><td>7.9</td></tr>
	<tr><th>UTF-32</th><td>7.9</td></tr>
	<tr><th>EGC</th><td>7.9</td></tr>
	<tr><th>EAW</th><td>4.3</td></tr>
</tbody></table>

<p>UTF-16, UTF-32, and extended grapheme clusters aren’t distinguished by this measure, because the languages at the extremes use characters from the Basic Multilingual Plane with one character per grapheme cluster. Considering that there are supplementary-plane scripts, arguably the UTF-32 count would be fairer than the UTF-16 count even though this factor doesn’t show the difference. It’s not clear that counting extended grapheme clusters would be particularly fair compared to counting characters: It favors scripts that are visually joining over scripts that aren’t visually joining even if there’s no logical difference. While looking at just the factor, East Asian Width makes the gap the smallest, but it’s a rather imprecise fairness solution. It just counts CJK as double. Even after this, the Han-script languages are still among the ones with the smallest counts. On the other hand, it seems unfair to recognize Hangul syllables and kana as carrying more information than an alphabetic character while not giving the same treatment to other syllabaries, such as the Ethiopic script, Ge’ez.</p>

<p>Twitter counts each CJK character (including three-jamo Hangul syllables; i.e. it is not decomposing Hangul and treating it as alphabetic) as consuming 2 units of the quota (as when counting East Asian Width), counts emoji as consuming two units (even when East Asian Width of the cluster would be more), and, unlike East Asian Width, counts each Ethiopic syllable as consuming two units of the quota. What Twitter does seems fairer than just applying East Asian Width, but the result is still that the amount of information that can be packed in a tweet can vary four-fold depending on language. That still doesn’t seem exactly fair across languages.</p>

<p>In closing:</p>

<ul>
	<li>There is no simple measure of string length that would be fair in terms of how much information can be conveyed within a length quota regardless of language.</li>
	<li>Of solutions that don’t depend on the Unicode database and, therefore, the Unicode version and that don’t ad-hoc hard-code character ranges according to a particular version of Unicode, counting characters aka. scalar values i.e. UTF-32 length is the best that can be done. It’s still wildly unfair leading to almost eight-fold differences in how much information can be conveyed. This is not a flaw of Unicode but arises from differences in languages and writing systems.</li>
	<li>While counting scalar values is fairer than just counting UTF-8 or UTF-16 code units, the factor between minimum and maximum UTF-8 length is so close to the factor between minimum and maximum UTF-32 length, both of which are pretty large, that instead of putting thought into using the scalar value length instead of the UTF-8 length or the UTF-16 length, it’s probably better to put the thought into reconsidering if you <i>really</i> need to impose such a limit.</li>
	<li>Unicode doesn’t provide a good database-based definition that would improve upon the character count in terms of normalizing the amount of information conveyed. While East Asian Width brings minimum and maximum closer, it unfairly singles out Hangul syllables and kana without considering other syllabaries, because normalizing length for information conveyed is not the purpose of East Asian Width.</li>
	<li>Even if per-script (possibly non-integer) weights assigned to characters could make things fairer, it wouldn’t work well for the Latin script, which is all over the place in terms of language-dependent length.</li>
</ul>

<table id="counts">
<thead>
<tr><th id="charcountname">Name</th><th>UTF-8</th><th>Δ%</th><th>UTF-16</th><th>Δ%</th><th>UTF-32</th><th>Δ%</th><th>EGC</th><th>Δ%</th><th>EAW</th><th>Δ%</th><th id="charcountscript">Script</th></tr>
</thead>
<tbody>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cbs.html">Cashinahua</a></th>
<td>4170</td><td>-57.6</td>
<td>4135</td><td>-53.0</td>
<td>4135</td><td>-52.9</td>
<td>4135</td><td>-52.3</td>
<td>4135</td><td>-52.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_prq.html">Ashéninka Perené</a></th>
<td>4170</td><td>-57.6</td>
<td>4135</td><td>-53.0</td>
<td>4135</td><td>-52.9</td>
<td>4135</td><td>-52.3</td>
<td>4135</td><td>-52.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_wwa.html">Waama</a></th>
<td>4293</td><td>-56.3</td>
<td>4011</td><td>-54.4</td>
<td>4011</td><td>-54.4</td>
<td>4007</td><td>-53.8</td>
<td>4007</td><td>-53.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cic.html">Chickasaw</a></th>
<td>4850</td><td>-50.6</td>
<td>4685</td><td>-46.7</td>
<td>4685</td><td>-46.7</td>
<td>4587</td><td>-47.1</td>
<td>4587</td><td>-47.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_btb.html">Bulu</a></th>
<td>4919</td><td>-49.9</td>
<td>4808</td><td>-45.3</td>
<td>4808</td><td>-45.3</td>
<td>4808</td><td>-44.5</td>
<td>4808</td><td>-44.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nku.html">Kulango, Bouna</a></th>
<td>5286</td><td>-46.2</td>
<td>4164</td><td>-52.6</td>
<td>4164</td><td>-52.6</td>
<td>4164</td><td>-51.9</td>
<td>4164</td><td>-52.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_zam.html">Zapotec, Miahuatlán</a></th>
<td>5464</td><td>-44.4</td>
<td>5433</td><td>-38.2</td>
<td>5433</td><td>-38.2</td>
<td>5433</td><td>-37.3</td>
<td>5433</td><td>-37.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nym.html">Nyamwezi</a></th>
<td>5750</td><td>-41.5</td>
<td>5686</td><td>-35.3</td>
<td>5686</td><td>-35.3</td>
<td>5686</td><td>-34.4</td>
<td>5686</td><td>-34.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kqn.html">Kaonde</a></th>
<td>5972</td><td>-39.2</td>
<td>5972</td><td>-32.1</td>
<td>5972</td><td>-32.0</td>
<td>5972</td><td>-31.1</td>
<td>5972</td><td>-31.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mxv.html">Mixtec, Metlatónoc</a></th>
<td>6023</td><td>-38.7</td>
<td>5630</td><td>-36.0</td>
<td>5630</td><td>-35.9</td>
<td>5611</td><td>-35.2</td>
<td>5611</td><td>-35.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kde.html">Makonde</a></th>
<td>6100</td><td>-37.9</td>
<td>5946</td><td>-32.4</td>
<td>5946</td><td>-32.3</td>
<td>5946</td><td>-31.4</td>
<td>5946</td><td>-31.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mcd.html">Sharanahua</a></th>
<td>6165</td><td>-37.3</td>
<td>6162</td><td>-29.9</td>
<td>6162</td><td>-29.9</td>
<td>6162</td><td>-28.9</td>
<td>6162</td><td>-29.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_srr.html">Serer-Sine</a></th>
<td>6166</td><td>-37.3</td>
<td>6079</td><td>-30.9</td>
<td>6079</td><td>-30.8</td>
<td>6079</td><td>-29.8</td>
<td>6079</td><td>-30.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_dip.html">Dinka, Northeastern</a></th>
<td>6214</td><td>-36.8</td>
<td>5815</td><td>-33.9</td>
<td>5815</td><td>-33.8</td>
<td>5775</td><td>-33.4</td>
<td>5775</td><td>-33.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_oki.html">Okiek</a></th>
<td>6272</td><td>-36.2</td>
<td>6272</td><td>-28.7</td>
<td>6272</td><td>-28.6</td>
<td>6272</td><td>-27.6</td>
<td>6271</td><td>-27.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_dyo.html">Jola-Fonyi</a></th>
<td>6299</td><td>-35.9</td>
<td>6122</td><td>-30.4</td>
<td>6122</td><td>-30.3</td>
<td>6122</td><td>-29.3</td>
<td>6122</td><td>-29.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_emk.html">Maninkakan, Eastern</a></th>
<td>6372</td><td>-35.2</td>
<td>5867</td><td>-33.3</td>
<td>5867</td><td>-33.2</td>
<td>5867</td><td>-32.3</td>
<td>5867</td><td>-32.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_chj.html">Chinantec, Ojitlán</a></th>
<td>6463</td><td>-34.2</td>
<td>5957</td><td>-32.3</td>
<td>5957</td><td>-32.2</td>
<td>5957</td><td>-31.3</td>
<td>5957</td><td>-31.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_snk.html">Soninke</a></th>
<td>6496</td><td>-33.9</td>
<td>6430</td><td>-26.9</td>
<td>6430</td><td>-26.8</td>
<td>6430</td><td>-25.8</td>
<td>6430</td><td>-26.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cjk_AO.html">Chokwe (Angola)</a></th>
<td>6596</td><td>-32.9</td>
<td>6565</td><td>-25.3</td>
<td>6565</td><td>-25.3</td>
<td>6565</td><td>-24.2</td>
<td>6565</td><td>-24.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cmn_hant.html">Chinese, Mandarin (Traditional)</a></th>
<td>6606</td><td>-32.8</td>
<td>2202</td><td>-75.0</td>
<td>2202</td><td>-74.9</td>
<td>2202</td><td>-74.6</td>
<td>4404</td><td>-49.3</td>
<td>Hant</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ote.html">Otomi, Mezquital</a></th>
<td>6614</td><td>-32.7</td>
<td>6438</td><td>-26.8</td>
<td>6438</td><td>-26.7</td>
<td>6379</td><td>-26.4</td>
<td>6379</td><td>-26.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cmn_hans.html">Chinese, Mandarin (Simplified)</a></th>
<td>6708</td><td>-31.7</td>
<td>2278</td><td>-74.1</td>
<td>2278</td><td>-74.1</td>
<td>2278</td><td>-73.7</td>
<td>4493</td><td>-48.3</td>
<td>Hans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qud.html">Quechua (Unified Quichua, old Hispanic orthography)</a></th>
<td>6713</td><td>-31.7</td>
<td>6670</td><td>-24.2</td>
<td>6670</td><td>-24.1</td>
<td>6670</td><td>-23.0</td>
<td>6670</td><td>-23.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_shk.html">Shilluk</a></th>
<td>6798</td><td>-30.8</td>
<td>6036</td><td>-31.4</td>
<td>6036</td><td>-31.3</td>
<td>6036</td><td>-30.3</td>
<td>6036</td><td>-30.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cof.html">Colorado</a></th>
<td>6798</td><td>-30.8</td>
<td>6797</td><td>-22.7</td>
<td>6797</td><td>-22.7</td>
<td>6796</td><td>-21.6</td>
<td>6794</td><td>-21.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ddn.html">Dendi</a></th>
<td>6823</td><td>-30.6</td>
<td>6327</td><td>-28.1</td>
<td>6327</td><td>-28.0</td>
<td>6325</td><td>-27.0</td>
<td>6325</td><td>-27.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cjy.html">Chinese, Jinyu</a></th>
<td>6848</td><td>-30.3</td>
<td>2284</td><td>-74.0</td>
<td>2284</td><td>-74.0</td>
<td>2284</td><td>-73.6</td>
<td>4566</td><td>-47.4</td>
<td>Hans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nan.html">Chinese, Min Nan</a></th>
<td>6887</td><td>-29.9</td>
<td>2297</td><td>-73.9</td>
<td>2297</td><td>-73.9</td>
<td>2297</td><td>-73.5</td>
<td>4592</td><td>-47.1</td>
<td>Hans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gan.html">Chinese, Gan</a></th>
<td>6889</td><td>-29.9</td>
<td>2297</td><td>-73.9</td>
<td>2297</td><td>-73.9</td>
<td>2297</td><td>-73.5</td>
<td>4593</td><td>-47.1</td>
<td>Hans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_vie_han.html">Vietnamese (Han nom)</a></th>
<td>6910</td><td>-29.7</td>
<td>2564</td><td>-70.8</td>
<td>2224</td><td>-74.7</td>
<td>2224</td><td>-74.3</td>
<td>4397</td><td>-49.4</td>
<td>Hani</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hak.html">Chinese, Hakka</a></th>
<td>6929</td><td>-29.5</td>
<td>2311</td><td>-73.7</td>
<td>2311</td><td>-73.7</td>
<td>2311</td><td>-73.3</td>
<td>4620</td><td>-46.8</td>
<td>Hans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lun.html">Lunda</a></th>
<td>6968</td><td>-29.1</td>
<td>6968</td><td>-20.8</td>
<td>6968</td><td>-20.7</td>
<td>6968</td><td>-19.6</td>
<td>6968</td><td>-19.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_yue.html">Chinese, Yue</a></th>
<td>6973</td><td>-29.0</td>
<td>2325</td><td>-73.6</td>
<td>2325</td><td>-73.5</td>
<td>2325</td><td>-73.2</td>
<td>4648</td><td>-46.5</td>
<td>Hani</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fuf.html">Pular</a></th>
<td>6991</td><td>-28.9</td>
<td>6991</td><td>-20.5</td>
<td>6991</td><td>-20.4</td>
<td>6991</td><td>-19.3</td>
<td>6991</td><td>-19.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lia.html">Limba, West-Central</a></th>
<td>7007</td><td>-28.7</td>
<td>6257</td><td>-28.8</td>
<td>6257</td><td>-28.8</td>
<td>6257</td><td>-27.8</td>
<td>6257</td><td>-28.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_njo.html">Naga, Ao</a></th>
<td>7019</td><td>-28.6</td>
<td>6729</td><td>-23.5</td>
<td>6729</td><td>-23.4</td>
<td>6729</td><td>-22.3</td>
<td>6729</td><td>-22.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_maz.html">Mazahua Central</a></th>
<td>7052</td><td>-28.2</td>
<td>6750</td><td>-23.2</td>
<td>6750</td><td>-23.2</td>
<td>6517</td><td>-24.8</td>
<td>6517</td><td>-25.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_wuu.html">Chinese, Wu</a></th>
<td>7082</td><td>-27.9</td>
<td>2362</td><td>-73.1</td>
<td>2362</td><td>-73.1</td>
<td>2362</td><td>-72.7</td>
<td>4722</td><td>-45.6</td>
<td>Hans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gkp.html">Kpelle, Guinea</a></th>
<td>7139</td><td>-27.4</td>
<td>6136</td><td>-30.2</td>
<td>6136</td><td>-30.2</td>
<td>6136</td><td>-29.2</td>
<td>6136</td><td>-29.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ami.html">Amis</a></th>
<td>7206</td><td>-26.7</td>
<td>7206</td><td>-18.1</td>
<td>7206</td><td>-18.0</td>
<td>7206</td><td>-16.8</td>
<td>7206</td><td>-17.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bba.html">Baatonum</a></th>
<td>7255</td><td>-26.2</td>
<td>6788</td><td>-22.8</td>
<td>6788</td><td>-22.8</td>
<td>6779</td><td>-21.8</td>
<td>6779</td><td>-22.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tet.html">Tetun</a></th>
<td>7280</td><td>-25.9</td>
<td>7280</td><td>-17.2</td>
<td>7280</td><td>-17.2</td>
<td>7280</td><td>-16.0</td>
<td>7280</td><td>-16.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_csa.html">Chinantec, Chiltepec</a></th>
<td>7304</td><td>-25.7</td>
<td>6468</td><td>-26.4</td>
<td>6468</td><td>-26.4</td>
<td>6262</td><td>-27.7</td>
<td>6262</td><td>-27.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_014.html">(Maiunan)</a></th>
<td>7312</td><td>-25.6</td>
<td>7312</td><td>-16.9</td>
<td>7312</td><td>-16.8</td>
<td>7312</td><td>-15.6</td>
<td>7312</td><td>-15.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_010.html">Tetun Dili</a></th>
<td>7357</td><td>-25.1</td>
<td>7225</td><td>-17.8</td>
<td>7225</td><td>-17.8</td>
<td>7225</td><td>-16.6</td>
<td>7225</td><td>-16.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_017.html">(Minjiang, written)</a></th>
<td>7366</td><td>-25.0</td>
<td>7363</td><td>-16.3</td>
<td>7363</td><td>-16.2</td>
<td>7363</td><td>-15.0</td>
<td>7363</td><td>-15.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_quz.html">Quechua, Cusco</a></th>
<td>7369</td><td>-25.0</td>
<td>7309</td><td>-16.9</td>
<td>7309</td><td>-16.8</td>
<td>7309</td><td>-15.6</td>
<td>7309</td><td>-15.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_013.html">(Mijisa)</a></th>
<td>7393</td><td>-24.8</td>
<td>7393</td><td>-15.9</td>
<td>7393</td><td>-15.9</td>
<td>7393</td><td>-14.7</td>
<td>7392</td><td>-14.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_020.html">Drung</a></th>
<td>7412</td><td>-24.6</td>
<td>7412</td><td>-15.7</td>
<td>7412</td><td>-15.7</td>
<td>7412</td><td>-14.5</td>
<td>7412</td><td>-14.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mzi.html">Mazatec, Ixcatlán</a></th>
<td>7442</td><td>-24.3</td>
<td>7261</td><td>-17.4</td>
<td>7261</td><td>-17.4</td>
<td>7261</td><td>-16.2</td>
<td>7261</td><td>-16.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kin.html">Rwanda</a></th>
<td>7456</td><td>-24.1</td>
<td>7456</td><td>-15.2</td>
<td>7456</td><td>-15.2</td>
<td>7456</td><td>-14.0</td>
<td>7456</td><td>-14.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_016.html">(Minjiang, spoken)</a></th>
<td>7512</td><td>-23.6</td>
<td>7509</td><td>-14.6</td>
<td>7509</td><td>-14.6</td>
<td>7509</td><td>-13.3</td>
<td>7509</td><td>-13.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_suk.html">Sukuma</a></th>
<td>7532</td><td>-23.4</td>
<td>7452</td><td>-15.3</td>
<td>7452</td><td>-15.2</td>
<td>7452</td><td>-14.0</td>
<td>7452</td><td>-14.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_vmw.html">Makhuwa</a></th>
<td>7562</td><td>-23.0</td>
<td>7398</td><td>-15.9</td>
<td>7398</td><td>-15.8</td>
<td>7398</td><td>-14.6</td>
<td>7398</td><td>-14.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ayr.html">Aymara, Central</a></th>
<td>7568</td><td>-23.0</td>
<td>7363</td><td>-16.3</td>
<td>7363</td><td>-16.2</td>
<td>7363</td><td>-15.0</td>
<td>7363</td><td>-15.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ido.html">Ido</a></th>
<td>7580</td><td>-22.9</td>
<td>7580</td><td>-13.8</td>
<td>7580</td><td>-13.7</td>
<td>7580</td><td>-12.5</td>
<td>7580</td><td>-12.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_zro.html">Záparo</a></th>
<td>7591</td><td>-22.8</td>
<td>7583</td><td>-13.8</td>
<td>7583</td><td>-13.7</td>
<td>7583</td><td>-12.5</td>
<td>7583</td><td>-12.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bam.html">Bamanankan</a></th>
<td>7597</td><td>-22.7</td>
<td>6890</td><td>-21.7</td>
<td>6890</td><td>-21.6</td>
<td>6890</td><td>-20.5</td>
<td>6890</td><td>-20.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nyn.html">Nyankore</a></th>
<td>7628</td><td>-22.4</td>
<td>7628</td><td>-13.3</td>
<td>7628</td><td>-13.2</td>
<td>7628</td><td>-12.0</td>
<td>7628</td><td>-12.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nbl.html">Ndebele</a></th>
<td>7659</td><td>-22.1</td>
<td>7659</td><td>-12.9</td>
<td>7659</td><td>-12.8</td>
<td>7659</td><td>-11.6</td>
<td>7659</td><td>-11.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_007.html">Sãotomense</a></th>
<td>7712</td><td>-21.5</td>
<td>6956</td><td>-20.9</td>
<td>6956</td><td>-20.8</td>
<td>6956</td><td>-19.7</td>
<td>6956</td><td>-19.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pis.html">Pijin</a></th>
<td>7716</td><td>-21.5</td>
<td>7716</td><td>-12.3</td>
<td>7716</td><td>-12.2</td>
<td>7716</td><td>-11.0</td>
<td>7716</td><td>-11.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lat.html">Latin</a></th>
<td>7747</td><td>-21.2</td>
<td>7747</td><td>-11.9</td>
<td>7747</td><td>-11.8</td>
<td>7747</td><td>-10.6</td>
<td>7747</td><td>-10.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sus.html">Susu</a></th>
<td>7757</td><td>-21.1</td>
<td>7310</td><td>-16.9</td>
<td>7310</td><td>-16.8</td>
<td>7310</td><td>-15.6</td>
<td>7310</td><td>-15.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_orh.html">Oroqen</a></th>
<td>7768</td><td>-21.0</td>
<td>7761</td><td>-11.7</td>
<td>7761</td><td>-11.7</td>
<td>7761</td><td>-10.4</td>
<td>7761</td><td>-10.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_loz.html">Lozi</a></th>
<td>7825</td><td>-20.4</td>
<td>7825</td><td>-11.0</td>
<td>7825</td><td>-11.0</td>
<td>7825</td><td>-9.7</td>
<td>7825</td><td>-9.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lat_1.html">Latin (1)</a></th>
<td>7869</td><td>-19.9</td>
<td>7869</td><td>-10.5</td>
<td>7869</td><td>-10.5</td>
<td>7869</td><td>-9.2</td>
<td>7869</td><td>-9.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lot.html">Otuho</a></th>
<td>7890</td><td>-19.7</td>
<td>7890</td><td>-10.3</td>
<td>7890</td><td>-10.2</td>
<td>7801</td><td>-10.0</td>
<td>7712</td><td>-11.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_acu_1.html">Achuar-Shiwiar (1)</a></th>
<td>7893</td><td>-19.7</td>
<td>7842</td><td>-10.8</td>
<td>7842</td><td>-10.8</td>
<td>7785</td><td>-10.2</td>
<td>7728</td><td>-11.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hus.html">Huastec (Veracruz)</a></th>
<td>7911</td><td>-19.5</td>
<td>7882</td><td>-10.4</td>
<td>7882</td><td>-10.3</td>
<td>7882</td><td>-9.0</td>
<td>7882</td><td>-9.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_011.html">Umbundu (011)</a></th>
<td>7941</td><td>-19.2</td>
<td>7910</td><td>-10.1</td>
<td>7910</td><td>-10.0</td>
<td>7910</td><td>-8.7</td>
<td>7910</td><td>-9.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_iii.html">Nuosu</a></th>
<td>7953</td><td>-19.1</td>
<td>2663</td><td>-69.7</td>
<td>2663</td><td>-69.7</td>
<td>2663</td><td>-69.3</td>
<td>5308</td><td>-38.9</td>
<td>Yiii</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_eve.html">Even</a></th>
<td>7969</td><td>-18.9</td>
<td>4320</td><td>-50.9</td>
<td>4320</td><td>-50.8</td>
<td>4320</td><td>-50.1</td>
<td>4320</td><td>-50.3</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kek.html">Q'eqchi'</a></th>
<td>7981</td><td>-18.8</td>
<td>7981</td><td>-9.2</td>
<td>7981</td><td>-9.2</td>
<td>7981</td><td>-7.9</td>
<td>7981</td><td>-8.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mfq.html">Moba</a></th>
<td>7985</td><td>-18.7</td>
<td>7726</td><td>-12.1</td>
<td>7726</td><td>-12.1</td>
<td>7726</td><td>-10.8</td>
<td>7726</td><td>-11.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mam.html">Mam, Northern</a></th>
<td>7994</td><td>-18.7</td>
<td>7994</td><td>-9.1</td>
<td>7994</td><td>-9.0</td>
<td>7994</td><td>-7.7</td>
<td>7994</td><td>-8.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kbp.html">Kabiyé</a></th>
<td>7997</td><td>-18.6</td>
<td>6193</td><td>-29.6</td>
<td>6193</td><td>-29.5</td>
<td>6193</td><td>-28.5</td>
<td>6193</td><td>-28.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_knc.html">Kanuri, Central</a></th>
<td>8077</td><td>-17.8</td>
<td>7621</td><td>-13.3</td>
<td>7621</td><td>-13.3</td>
<td>7621</td><td>-12.0</td>
<td>7621</td><td>-12.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_epo.html">Esperanto</a></th>
<td>8095</td><td>-17.6</td>
<td>7930</td><td>-9.8</td>
<td>7930</td><td>-9.8</td>
<td>7930</td><td>-8.5</td>
<td>7930</td><td>-8.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_srp_latn.html">Serbian (Latin)</a></th>
<td>8102</td><td>-17.6</td>
<td>7876</td><td>-10.4</td>
<td>7876</td><td>-10.4</td>
<td>7876</td><td>-9.1</td>
<td>7876</td><td>-9.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ura.html">Urarina</a></th>
<td>8127</td><td>-17.3</td>
<td>8125</td><td>-7.6</td>
<td>8125</td><td>-7.5</td>
<td>8125</td><td>-6.2</td>
<td>8125</td><td>-6.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ckb.html">Kurdish, Central</a></th>
<td>8163</td><td>-16.9</td>
<td>7462</td><td>-15.1</td>
<td>7462</td><td>-15.1</td>
<td>7462</td><td>-13.9</td>
<td>7462</td><td>-14.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kmr.html">Kurdish, Northern</a></th>
<td>8163</td><td>-16.9</td>
<td>7462</td><td>-15.1</td>
<td>7462</td><td>-15.1</td>
<td>7462</td><td>-13.9</td>
<td>7462</td><td>-14.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_huu.html">Huitoto, Murui</a></th>
<td>8179</td><td>-16.8</td>
<td>7523</td><td>-14.5</td>
<td>7523</td><td>-14.4</td>
<td>7523</td><td>-13.2</td>
<td>7523</td><td>-13.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hrv.html">Croatian</a></th>
<td>8201</td><td>-16.5</td>
<td>7996</td><td>-9.1</td>
<td>7996</td><td>-9.0</td>
<td>7996</td><td>-7.7</td>
<td>7996</td><td>-8.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bem.html">Bemba</a></th>
<td>8206</td><td>-16.5</td>
<td>8206</td><td>-6.7</td>
<td>8206</td><td>-6.6</td>
<td>8206</td><td>-5.3</td>
<td>8206</td><td>-5.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_auc.html">Waorani</a></th>
<td>8209</td><td>-16.5</td>
<td>8137</td><td>-7.5</td>
<td>8137</td><td>-7.4</td>
<td>8052</td><td>-7.1</td>
<td>7967</td><td>-8.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gjn.html">Gonja</a></th>
<td>8215</td><td>-16.4</td>
<td>7579</td><td>-13.8</td>
<td>7579</td><td>-13.8</td>
<td>7579</td><td>-12.5</td>
<td>7579</td><td>-12.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sco.html">Scots</a></th>
<td>8224</td><td>-16.3</td>
<td>8224</td><td>-6.5</td>
<td>8224</td><td>-6.4</td>
<td>8224</td><td>-5.1</td>
<td>8224</td><td>-5.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ndo.html">Ndonga</a></th>
<td>8239</td><td>-16.2</td>
<td>8239</td><td>-6.3</td>
<td>8239</td><td>-6.2</td>
<td>8239</td><td>-4.9</td>
<td>8239</td><td>-5.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cab.html">Garifuna</a></th>
<td>8243</td><td>-16.1</td>
<td>7721</td><td>-12.2</td>
<td>7721</td><td>-12.1</td>
<td>7721</td><td>-10.9</td>
<td>7721</td><td>-11.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bos_latn.html">Bosnian (Latin)</a></th>
<td>8259</td><td>-16.0</td>
<td>8049</td><td>-8.5</td>
<td>8049</td><td>-8.4</td>
<td>8049</td><td>-7.1</td>
<td>8049</td><td>-7.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_aka_akuapem.html">Twi (Akuapem)</a></th>
<td>8264</td><td>-15.9</td>
<td>7653</td><td>-13.0</td>
<td>7653</td><td>-12.9</td>
<td>7653</td><td>-11.7</td>
<td>7653</td><td>-11.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_zul.html">Zulu</a></th>
<td>8265</td><td>-15.9</td>
<td>8261</td><td>-6.1</td>
<td>8261</td><td>-6.0</td>
<td>8261</td><td>-4.7</td>
<td>8261</td><td>-4.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gyr.html">Guarayu</a></th>
<td>8280</td><td>-15.7</td>
<td>8098</td><td>-7.9</td>
<td>8098</td><td>-7.9</td>
<td>8098</td><td>-6.5</td>
<td>8098</td><td>-6.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_swh.html">Swahili</a></th>
<td>8315</td><td>-15.4</td>
<td>8315</td><td>-5.4</td>
<td>8315</td><td>-5.4</td>
<td>8315</td><td>-4.0</td>
<td>8315</td><td>-4.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ccx.html">Zhuang, Yongbei</a></th>
<td>8318</td><td>-15.4</td>
<td>8316</td><td>-5.4</td>
<td>8316</td><td>-5.4</td>
<td>8316</td><td>-4.0</td>
<td>8316</td><td>-4.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_wol.html">Wolof</a></th>
<td>8321</td><td>-15.3</td>
<td>7940</td><td>-9.7</td>
<td>7940</td><td>-9.6</td>
<td>7940</td><td>-8.4</td>
<td>7940</td><td>-8.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ztu.html">Zapotec, Güilá</a></th>
<td>8364</td><td>-14.9</td>
<td>8328</td><td>-5.3</td>
<td>8328</td><td>-5.2</td>
<td>8328</td><td>-3.9</td>
<td>8328</td><td>-4.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gax.html">Oromo, Borana-Arsi-Guji</a></th>
<td>8381</td><td>-14.7</td>
<td>8381</td><td>-4.7</td>
<td>8381</td><td>-4.6</td>
<td>8381</td><td>-3.3</td>
<td>8381</td><td>-3.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cym.html">Welsh</a></th>
<td>8382</td><td>-14.7</td>
<td>8247</td><td>-6.2</td>
<td>8247</td><td>-6.2</td>
<td>8247</td><td>-4.8</td>
<td>8247</td><td>-5.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tpi.html">Tok Pisin</a></th>
<td>8399</td><td>-14.5</td>
<td>8393</td><td>-4.6</td>
<td>8393</td><td>-4.5</td>
<td>8393</td><td>-3.1</td>
<td>8393</td><td>-3.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kwi.html">Awa-Cuaiquer</a></th>
<td>8405</td><td>-14.5</td>
<td>8391</td><td>-4.6</td>
<td>8391</td><td>-4.5</td>
<td>8309</td><td>-4.1</td>
<td>8227</td><td>-5.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lue.html">Luvale</a></th>
<td>8411</td><td>-14.4</td>
<td>8411</td><td>-4.4</td>
<td>8411</td><td>-4.3</td>
<td>8411</td><td>-2.9</td>
<td>8411</td><td>-3.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_008.html">Crioulo, Upper Guinea (008)</a></th>
<td>8414</td><td>-14.4</td>
<td>8225</td><td>-6.5</td>
<td>8225</td><td>-6.4</td>
<td>8225</td><td>-5.1</td>
<td>8225</td><td>-5.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_afr.html">Afrikaans</a></th>
<td>8427</td><td>-14.2</td>
<td>8365</td><td>-4.9</td>
<td>8365</td><td>-4.8</td>
<td>8365</td><td>-3.5</td>
<td>8365</td><td>-3.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fao.html">Faroese</a></th>
<td>8454</td><td>-14.0</td>
<td>7854</td><td>-10.7</td>
<td>7854</td><td>-10.6</td>
<td>7854</td><td>-9.4</td>
<td>7854</td><td>-9.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fuv.html">Fulfulde, Nigerian</a></th>
<td>8455</td><td>-14.0</td>
<td>8135</td><td>-7.5</td>
<td>8135</td><td>-7.4</td>
<td>8135</td><td>-6.1</td>
<td>8135</td><td>-6.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nno.html">Norwegian, Nynorsk</a></th>
<td>8461</td><td>-13.9</td>
<td>8268</td><td>-6.0</td>
<td>8268</td><td>-5.9</td>
<td>8268</td><td>-4.6</td>
<td>8268</td><td>-4.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_yad.html">Yagua</a></th>
<td>8468</td><td>-13.8</td>
<td>8432</td><td>-4.1</td>
<td>8432</td><td>-4.1</td>
<td>8432</td><td>-2.7</td>
<td>8432</td><td>-2.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_run.html">Rundi</a></th>
<td>8498</td><td>-13.5</td>
<td>8498</td><td>-3.4</td>
<td>8498</td><td>-3.3</td>
<td>8498</td><td>-1.9</td>
<td>8498</td><td>-2.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nob.html">Norwegian, Bokmål</a></th>
<td>8500</td><td>-13.5</td>
<td>8360</td><td>-4.9</td>
<td>8360</td><td>-4.9</td>
<td>8360</td><td>-3.5</td>
<td>8360</td><td>-3.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_umb.html">Umbundu</a></th>
<td>8503</td><td>-13.5</td>
<td>8415</td><td>-4.3</td>
<td>8415</td><td>-4.2</td>
<td>8415</td><td>-2.9</td>
<td>8415</td><td>-3.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_eng.html">English</a></th>
<td>8565</td><td>-12.8</td>
<td>8555</td><td>-2.7</td>
<td>8555</td><td>-2.7</td>
<td>8555</td><td>-1.3</td>
<td>8555</td><td>-1.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_yao.html">Yao</a></th>
<td>8574</td><td>-12.8</td>
<td>8574</td><td>-2.5</td>
<td>8574</td><td>-2.4</td>
<td>8574</td><td>-1.1</td>
<td>8574</td><td>-1.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_not.html">Nomatsiguenga</a></th>
<td>8575</td><td>-12.7</td>
<td>8432</td><td>-4.1</td>
<td>8432</td><td>-4.1</td>
<td>8432</td><td>-2.7</td>
<td>8432</td><td>-2.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_arn.html">Mapudungun</a></th>
<td>8585</td><td>-12.6</td>
<td>8366</td><td>-4.9</td>
<td>8366</td><td>-4.8</td>
<td>8366</td><td>-3.5</td>
<td>8366</td><td>-3.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fij.html">Fijian</a></th>
<td>8586</td><td>-12.6</td>
<td>8584</td><td>-2.4</td>
<td>8584</td><td>-2.3</td>
<td>8584</td><td>-0.9</td>
<td>8584</td><td>-1.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tzm.html">Tamazight, Central Atlas</a></th>
<td>8587</td><td>-12.6</td>
<td>8226</td><td>-6.5</td>
<td>8226</td><td>-6.4</td>
<td>8226</td><td>-5.1</td>
<td>8226</td><td>-5.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nya_chinyanja.html">Nyanja (Chinyanja)</a></th>
<td>8590</td><td>-12.6</td>
<td>8590</td><td>-2.3</td>
<td>8590</td><td>-2.3</td>
<td>8590</td><td>-0.9</td>
<td>8590</td><td>-1.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_yap.html">Yapese</a></th>
<td>8635</td><td>-12.1</td>
<td>8473</td><td>-3.7</td>
<td>8473</td><td>-3.6</td>
<td>8473</td><td>-2.2</td>
<td>8473</td><td>-2.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pov.html">Crioulo, Upper Guinea</a></th>
<td>8636</td><td>-12.1</td>
<td>8632</td><td>-1.8</td>
<td>8632</td><td>-1.8</td>
<td>8632</td><td>-0.4</td>
<td>8632</td><td>-0.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sey.html">Secoya</a></th>
<td>8651</td><td>-12.0</td>
<td>8155</td><td>-7.3</td>
<td>8155</td><td>-7.2</td>
<td>8137</td><td>-6.1</td>
<td>8137</td><td>-6.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_guc.html">Wayuu</a></th>
<td>8664</td><td>-11.8</td>
<td>8077</td><td>-8.2</td>
<td>8077</td><td>-8.1</td>
<td>8077</td><td>-6.8</td>
<td>8077</td><td>-7.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lin.html">Lingala</a></th>
<td>8668</td><td>-11.8</td>
<td>8654</td><td>-1.6</td>
<td>8654</td><td>-1.5</td>
<td>8654</td><td>-0.1</td>
<td>8654</td><td>-0.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hat_kreyol.html">Haitian Creole French (Kreyol)</a></th>
<td>8680</td><td>-11.7</td>
<td>8535</td><td>-2.9</td>
<td>8535</td><td>-2.9</td>
<td>8535</td><td>-1.5</td>
<td>8535</td><td>-1.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_toi.html">Tonga</a></th>
<td>8685</td><td>-11.6</td>
<td>8685</td><td>-1.2</td>
<td>8685</td><td>-1.2</td>
<td>8685</td><td>0.2</td>
<td>8685</td><td>-0.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_crs.html">Seselwa Creole French</a></th>
<td>8706</td><td>-11.4</td>
<td>8697</td><td>-1.1</td>
<td>8697</td><td>-1.0</td>
<td>8697</td><td>0.4</td>
<td>8697</td><td>0.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_men.html">Mende</a></th>
<td>8707</td><td>-11.4</td>
<td>8010</td><td>-8.9</td>
<td>8010</td><td>-8.9</td>
<td>8010</td><td>-7.6</td>
<td>8010</td><td>-7.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nya_chechewa.html">Nyanja (Chechewa)</a></th>
<td>8725</td><td>-11.2</td>
<td>8725</td><td>-0.8</td>
<td>8725</td><td>-0.7</td>
<td>8725</td><td>0.7</td>
<td>8725</td><td>0.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hni.html">Hani</a></th>
<td>8767</td><td>-10.8</td>
<td>8767</td><td>-0.3</td>
<td>8767</td><td>-0.2</td>
<td>8767</td><td>1.2</td>
<td>8767</td><td>0.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_slv.html">Slovenian</a></th>
<td>8772</td><td>-10.7</td>
<td>8520</td><td>-3.1</td>
<td>8520</td><td>-3.0</td>
<td>8520</td><td>-1.7</td>
<td>8520</td><td>-1.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hms.html">Hmong, Southern Qiandong</a></th>
<td>8792</td><td>-10.5</td>
<td>8792</td><td>-0.0</td>
<td>8792</td><td>0.0</td>
<td>8792</td><td>1.5</td>
<td>8792</td><td>1.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cjk.html">Chokwe</a></th>
<td>8808</td><td>-10.4</td>
<td>8808</td><td>0.2</td>
<td>8808</td><td>0.2</td>
<td>8808</td><td>1.7</td>
<td>8808</td><td>1.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ppl.html">Pipil</a></th>
<td>8831</td><td>-10.1</td>
<td>8825</td><td>0.4</td>
<td>8825</td><td>0.4</td>
<td>8825</td><td>1.8</td>
<td>8825</td><td>1.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_012.html">(Bizisa)</a></th>
<td>8847</td><td>-10.0</td>
<td>8847</td><td>0.6</td>
<td>8847</td><td>0.7</td>
<td>8847</td><td>2.1</td>
<td>8847</td><td>1.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qvc.html">Quechua, Cajamarca</a></th>
<td>8858</td><td>-9.9</td>
<td>8851</td><td>0.6</td>
<td>8851</td><td>0.7</td>
<td>8851</td><td>2.1</td>
<td>8851</td><td>1.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_xsm.html">Kasem</a></th>
<td>8868</td><td>-9.8</td>
<td>8445</td><td>-4.0</td>
<td>8445</td><td>-3.9</td>
<td>8445</td><td>-2.5</td>
<td>8445</td><td>-2.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_rmn.html">Romani, Balkan</a></th>
<td>8875</td><td>-9.7</td>
<td>8606</td><td>-2.1</td>
<td>8606</td><td>-2.1</td>
<td>8606</td><td>-0.7</td>
<td>8606</td><td>-0.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tur.html">Turkish</a></th>
<td>8877</td><td>-9.7</td>
<td>8225</td><td>-6.5</td>
<td>8225</td><td>-6.4</td>
<td>8225</td><td>-5.1</td>
<td>8225</td><td>-5.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_aka_fante.html">Fante</a></th>
<td>8898</td><td>-9.5</td>
<td>8229</td><td>-6.4</td>
<td>8229</td><td>-6.4</td>
<td>8229</td><td>-5.0</td>
<td>8229</td><td>-5.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_eus.html">Basque</a></th>
<td>8907</td><td>-9.4</td>
<td>8907</td><td>1.3</td>
<td>8907</td><td>1.4</td>
<td>8907</td><td>2.8</td>
<td>8907</td><td>2.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lug.html">Ganda</a></th>
<td>8962</td><td>-8.8</td>
<td>8962</td><td>1.9</td>
<td>8962</td><td>2.0</td>
<td>8962</td><td>3.4</td>
<td>8962</td><td>3.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_prv.html">Occitan</a></th>
<td>8963</td><td>-8.8</td>
<td>8661</td><td>-1.5</td>
<td>8661</td><td>-1.4</td>
<td>8661</td><td>-0.0</td>
<td>8661</td><td>-0.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_xho.html">Xhosa</a></th>
<td>8969</td><td>-8.7</td>
<td>8881</td><td>1.0</td>
<td>8881</td><td>1.1</td>
<td>8881</td><td>2.5</td>
<td>8881</td><td>2.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bre.html">Breton</a></th>
<td>8982</td><td>-8.6</td>
<td>8661</td><td>-1.5</td>
<td>8661</td><td>-1.4</td>
<td>8661</td><td>-0.0</td>
<td>8661</td><td>-0.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_vep.html">Veps</a></th>
<td>8985</td><td>-8.6</td>
<td>8428</td><td>-4.2</td>
<td>8428</td><td>-4.1</td>
<td>8428</td><td>-2.7</td>
<td>8428</td><td>-3.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qxu.html">Quechua, Arequipa-La Unión</a></th>
<td>8988</td><td>-8.5</td>
<td>8969</td><td>2.0</td>
<td>8969</td><td>2.1</td>
<td>8969</td><td>3.5</td>
<td>8969</td><td>3.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fur.html">Friulian</a></th>
<td>9003</td><td>-8.4</td>
<td>8688</td><td>-1.2</td>
<td>8688</td><td>-1.1</td>
<td>8688</td><td>0.3</td>
<td>8688</td><td>0.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_swe.html">Swedish</a></th>
<td>9008</td><td>-8.3</td>
<td>8612</td><td>-2.1</td>
<td>8612</td><td>-2.0</td>
<td>8612</td><td>-0.6</td>
<td>8612</td><td>-0.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_dan.html">Danish</a></th>
<td>9010</td><td>-8.3</td>
<td>8831</td><td>0.4</td>
<td>8831</td><td>0.5</td>
<td>8831</td><td>1.9</td>
<td>8831</td><td>1.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_rmy.html">Aromanian</a></th>
<td>9020</td><td>-8.2</td>
<td>8694</td><td>-1.1</td>
<td>8694</td><td>-1.1</td>
<td>8694</td><td>0.3</td>
<td>8694</td><td>0.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mad.html">Madura</a></th>
<td>9023</td><td>-8.2</td>
<td>9023</td><td>2.6</td>
<td>9023</td><td>2.7</td>
<td>9023</td><td>4.1</td>
<td>9023</td><td>3.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_rmn_1.html">Romani, Balkan (1)</a></th>
<td>9035</td><td>-8.1</td>
<td>8739</td><td>-0.6</td>
<td>8739</td><td>-0.6</td>
<td>8739</td><td>0.9</td>
<td>8739</td><td>0.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cbt.html">Chayahuita</a></th>
<td>9065</td><td>-7.8</td>
<td>8639</td><td>-1.8</td>
<td>8639</td><td>-1.7</td>
<td>8639</td><td>-0.3</td>
<td>8639</td><td>-0.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_isl.html">Icelandic</a></th>
<td>9070</td><td>-7.7</td>
<td>8249</td><td>-6.2</td>
<td>8249</td><td>-6.1</td>
<td>8249</td><td>-4.8</td>
<td>8249</td><td>-5.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kri.html">Krio</a></th>
<td>9086</td><td>-7.5</td>
<td>8139</td><td>-7.4</td>
<td>8139</td><td>-7.4</td>
<td>8139</td><td>-6.1</td>
<td>8139</td><td>-6.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_est.html">Estonian</a></th>
<td>9093</td><td>-7.5</td>
<td>8800</td><td>0.1</td>
<td>8800</td><td>0.1</td>
<td>8800</td><td>1.6</td>
<td>8800</td><td>1.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ajg.html">Aja</a></th>
<td>9099</td><td>-7.4</td>
<td>8077</td><td>-8.2</td>
<td>8077</td><td>-8.1</td>
<td>8069</td><td>-6.9</td>
<td>8069</td><td>-7.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hsb.html">Sorbian, Upper</a></th>
<td>9108</td><td>-7.3</td>
<td>8442</td><td>-4.0</td>
<td>8442</td><td>-3.9</td>
<td>8442</td><td>-2.6</td>
<td>8442</td><td>-2.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sot.html">Sotho, Southern</a></th>
<td>9136</td><td>-7.0</td>
<td>9136</td><td>3.9</td>
<td>9136</td><td>4.0</td>
<td>9136</td><td>5.4</td>
<td>9136</td><td>5.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cat.html">Catalan-Valencian-Balear</a></th>
<td>9141</td><td>-7.0</td>
<td>8823</td><td>0.3</td>
<td>8823</td><td>0.4</td>
<td>8823</td><td>1.8</td>
<td>8823</td><td>1.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lua.html">Luba-Kasai</a></th>
<td>9143</td><td>-7.0</td>
<td>9143</td><td>4.0</td>
<td>9143</td><td>4.0</td>
<td>9143</td><td>5.5</td>
<td>9143</td><td>5.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_min.html">Minangkabau</a></th>
<td>9175</td><td>-6.6</td>
<td>9167</td><td>4.2</td>
<td>9167</td><td>4.3</td>
<td>9167</td><td>5.8</td>
<td>9167</td><td>5.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bfa.html">Bari</a></th>
<td>9178</td><td>-6.6</td>
<td>8555</td><td>-2.7</td>
<td>8555</td><td>-2.7</td>
<td>8555</td><td>-1.3</td>
<td>8555</td><td>-1.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_por_BR.html">Portuguese (Brazil)</a></th>
<td>9219</td><td>-6.2</td>
<td>8887</td><td>1.1</td>
<td>8887</td><td>1.1</td>
<td>8887</td><td>2.6</td>
<td>8887</td><td>2.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hva.html">Huastec (San Luís Potosí)</a></th>
<td>9222</td><td>-6.2</td>
<td>8826</td><td>0.4</td>
<td>8826</td><td>0.4</td>
<td>8826</td><td>1.9</td>
<td>8826</td><td>1.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ces.html">Czech</a></th>
<td>9225</td><td>-6.1</td>
<td>8126</td><td>-7.6</td>
<td>8126</td><td>-7.5</td>
<td>8126</td><td>-6.2</td>
<td>8126</td><td>-6.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tsz.html">Purepecha</a></th>
<td>9234</td><td>-6.0</td>
<td>9082</td><td>3.3</td>
<td>9082</td><td>3.3</td>
<td>9082</td><td>4.8</td>
<td>9082</td><td>4.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fon.html">Fon</a></th>
<td>9244</td><td>-5.9</td>
<td>7952</td><td>-9.6</td>
<td>7952</td><td>-9.5</td>
<td>7943</td><td>-8.3</td>
<td>7943</td><td>-8.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_aka_asante.html">Twi (Asante)</a></th>
<td>9246</td><td>-5.9</td>
<td>8374</td><td>-4.8</td>
<td>8374</td><td>-4.7</td>
<td>8374</td><td>-3.4</td>
<td>8374</td><td>-3.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pap.html">Papiamentu</a></th>
<td>9249</td><td>-5.9</td>
<td>9237</td><td>5.0</td>
<td>9237</td><td>5.1</td>
<td>9237</td><td>6.6</td>
<td>9237</td><td>6.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_slk.html">Slovak</a></th>
<td>9266</td><td>-5.7</td>
<td>8378</td><td>-4.7</td>
<td>8378</td><td>-4.7</td>
<td>8378</td><td>-3.3</td>
<td>8378</td><td>-3.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_plt.html">Malagasy, Plateau</a></th>
<td>9272</td><td>-5.6</td>
<td>9272</td><td>5.4</td>
<td>9272</td><td>5.5</td>
<td>9272</td><td>7.0</td>
<td>9272</td><td>6.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_roh_vallader.html">Romansch (Vallader)</a></th>
<td>9300</td><td>-5.4</td>
<td>9048</td><td>2.9</td>
<td>9048</td><td>3.0</td>
<td>9048</td><td>4.4</td>
<td>9048</td><td>4.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lld.html">Ladin</a></th>
<td>9324</td><td>-5.1</td>
<td>8740</td><td>-0.6</td>
<td>8740</td><td>-0.5</td>
<td>8740</td><td>0.9</td>
<td>8740</td><td>0.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kmb.html">Mbundu</a></th>
<td>9327</td><td>-5.1</td>
<td>9317</td><td>5.9</td>
<td>9317</td><td>6.0</td>
<td>9317</td><td>7.5</td>
<td>9317</td><td>7.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_auv.html">Occitan (Auvergnat)</a></th>
<td>9330</td><td>-5.1</td>
<td>8642</td><td>-1.7</td>
<td>8642</td><td>-1.7</td>
<td>8642</td><td>-0.3</td>
<td>8642</td><td>-0.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lit.html">Lithuanian</a></th>
<td>9339</td><td>-5.0</td>
<td>8794</td><td>0.0</td>
<td>8794</td><td>0.1</td>
<td>8794</td><td>1.5</td>
<td>8794</td><td>1.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lad.html">Ladino</a></th>
<td>9348</td><td>-4.9</td>
<td>9345</td><td>6.3</td>
<td>9345</td><td>6.3</td>
<td>9345</td><td>7.8</td>
<td>9345</td><td>7.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_miq.html">Mískito</a></th>
<td>9353</td><td>-4.8</td>
<td>9345</td><td>6.3</td>
<td>9345</td><td>6.3</td>
<td>9345</td><td>7.8</td>
<td>9345</td><td>7.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_aii.html">Assyrian Neo-Aramaic</a></th>
<td>9363</td><td>-4.7</td>
<td>5186</td><td>-41.0</td>
<td>5186</td><td>-41.0</td>
<td>5127</td><td>-40.8</td>
<td>5127</td><td>-41.0</td>
<td>Syrc</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_war.html">Waray-Waray</a></th>
<td>9387</td><td>-4.5</td>
<td>9387</td><td>6.7</td>
<td>9387</td><td>6.8</td>
<td>9387</td><td>8.3</td>
<td>9387</td><td>8.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kor.html">Korean</a></th>
<td>9391</td><td>-4.4</td>
<td>3856</td><td>-56.2</td>
<td>3856</td><td>-56.1</td>
<td>3856</td><td>-55.5</td>
<td>6623</td><td>-23.8</td>
<td>Hang</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_som.html">Somali</a></th>
<td>9403</td><td>-4.3</td>
<td>9403</td><td>6.9</td>
<td>9403</td><td>7.0</td>
<td>9403</td><td>8.5</td>
<td>9403</td><td>8.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fin.html">Finnish</a></th>
<td>9404</td><td>-4.3</td>
<td>9023</td><td>2.6</td>
<td>9023</td><td>2.7</td>
<td>9023</td><td>4.1</td>
<td>9023</td><td>3.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_roh_sursilv.html">Romansch (Sursilvan)</a></th>
<td>9421</td><td>-4.1</td>
<td>9300</td><td>5.8</td>
<td>9300</td><td>5.8</td>
<td>9300</td><td>7.3</td>
<td>9300</td><td>7.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ctd.html">Chin, Tedim</a></th>
<td>9441</td><td>-3.9</td>
<td>9431</td><td>7.2</td>
<td>9431</td><td>7.3</td>
<td>9431</td><td>8.8</td>
<td>9431</td><td>8.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lav.html">Latvian</a></th>
<td>9447</td><td>-3.9</td>
<td>8582</td><td>-2.4</td>
<td>8582</td><td>-2.3</td>
<td>8582</td><td>-1.0</td>
<td>8582</td><td>-1.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_roh_rumgr.html">Romansch (Grischun)</a></th>
<td>9449</td><td>-3.8</td>
<td>9293</td><td>5.7</td>
<td>9293</td><td>5.7</td>
<td>9293</td><td>7.2</td>
<td>9293</td><td>7.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gag.html">Gagauz</a></th>
<td>9451</td><td>-3.8</td>
<td>8510</td><td>-3.2</td>
<td>8510</td><td>-3.2</td>
<td>8510</td><td>-1.8</td>
<td>8510</td><td>-2.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_dag.html">Dagbani</a></th>
<td>9458</td><td>-3.8</td>
<td>8896</td><td>1.2</td>
<td>8896</td><td>1.2</td>
<td>8896</td><td>2.7</td>
<td>8896</td><td>2.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fkv.html">Finnish, Kven</a></th>
<td>9464</td><td>-3.7</td>
<td>9123</td><td>3.7</td>
<td>9123</td><td>3.8</td>
<td>9123</td><td>5.3</td>
<td>9123</td><td>5.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cos.html">Corsican</a></th>
<td>9475</td><td>-3.6</td>
<td>8922</td><td>1.5</td>
<td>8922</td><td>1.5</td>
<td>8922</td><td>3.0</td>
<td>8922</td><td>2.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kng_AO.html">Koongo (Angola)</a></th>
<td>9486</td><td>-3.5</td>
<td>9416</td><td>7.1</td>
<td>9416</td><td>7.1</td>
<td>9416</td><td>8.7</td>
<td>9356</td><td>7.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tbz.html">Ditammari</a></th>
<td>9487</td><td>-3.5</td>
<td>7867</td><td>-10.5</td>
<td>7867</td><td>-10.5</td>
<td>7748</td><td>-10.6</td>
<td>7748</td><td>-10.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_por_PT.html">Portuguese (Portugal)</a></th>
<td>9501</td><td>-3.3</td>
<td>9154</td><td>4.1</td>
<td>9154</td><td>4.2</td>
<td>9154</td><td>5.6</td>
<td>9154</td><td>5.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_glv.html">Manx</a></th>
<td>9504</td><td>-3.3</td>
<td>9440</td><td>7.3</td>
<td>9440</td><td>7.4</td>
<td>9440</td><td>8.9</td>
<td>9440</td><td>8.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cha.html">Chamorro</a></th>
<td>9506</td><td>-3.3</td>
<td>9504</td><td>8.1</td>
<td>9504</td><td>8.1</td>
<td>9504</td><td>9.7</td>
<td>9504</td><td>9.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_glg.html">Galician</a></th>
<td>9510</td><td>-3.2</td>
<td>9223</td><td>4.9</td>
<td>9223</td><td>4.9</td>
<td>9223</td><td>6.4</td>
<td>9223</td><td>6.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lnc.html">Occitan (Languedocien)</a></th>
<td>9522</td><td>-3.1</td>
<td>9364</td><td>6.5</td>
<td>9364</td><td>6.6</td>
<td>9364</td><td>8.1</td>
<td>9364</td><td>7.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_roh_puter.html">Romansch (Puter)</a></th>
<td>9538</td><td>-2.9</td>
<td>9303</td><td>5.8</td>
<td>9303</td><td>5.9</td>
<td>9303</td><td>7.4</td>
<td>9303</td><td>7.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lij.html">Ligurian</a></th>
<td>9557</td><td>-2.7</td>
<td>8942</td><td>1.7</td>
<td>8942</td><td>1.8</td>
<td>8942</td><td>3.2</td>
<td>8942</td><td>2.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qwh.html">Quechua, Huaylas Ancash</a></th>
<td>9563</td><td>-2.7</td>
<td>9471</td><td>7.7</td>
<td>9471</td><td>7.8</td>
<td>9471</td><td>9.3</td>
<td>9471</td><td>9.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lus.html">Mizo</a></th>
<td>9576</td><td>-2.6</td>
<td>9489</td><td>7.9</td>
<td>9489</td><td>8.0</td>
<td>9489</td><td>9.5</td>
<td>9489</td><td>9.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tiv.html">Tiv</a></th>
<td>9585</td><td>-2.5</td>
<td>9490</td><td>7.9</td>
<td>9490</td><td>8.0</td>
<td>9490</td><td>9.5</td>
<td>9490</td><td>9.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ina.html">Interlingua</a></th>
<td>9588</td><td>-2.4</td>
<td>9588</td><td>9.0</td>
<td>9588</td><td>9.1</td>
<td>9588</td><td>10.7</td>
<td>9588</td><td>10.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kng.html">Koongo</a></th>
<td>9596</td><td>-2.4</td>
<td>9596</td><td>9.1</td>
<td>9596</td><td>9.2</td>
<td>9596</td><td>10.7</td>
<td>9596</td><td>10.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pon.html">Pohnpeian</a></th>
<td>9603</td><td>-2.3</td>
<td>9603</td><td>9.2</td>
<td>9603</td><td>9.3</td>
<td>9603</td><td>10.8</td>
<td>9603</td><td>10.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pol.html">Polish</a></th>
<td>9613</td><td>-2.2</td>
<td>9111</td><td>3.6</td>
<td>9111</td><td>3.7</td>
<td>9111</td><td>5.1</td>
<td>9111</td><td>4.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gaa.html">Ga</a></th>
<td>9614</td><td>-2.2</td>
<td>8262</td><td>-6.0</td>
<td>8262</td><td>-6.0</td>
<td>8257</td><td>-4.7</td>
<td>8257</td><td>-5.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ktu.html">Kituba</a></th>
<td>9630</td><td>-2.0</td>
<td>9630</td><td>9.5</td>
<td>9630</td><td>9.6</td>
<td>9630</td><td>11.1</td>
<td>9630</td><td>10.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pau.html">Palauan</a></th>
<td>9654</td><td>-1.8</td>
<td>9654</td><td>9.8</td>
<td>9654</td><td>9.9</td>
<td>9654</td><td>11.4</td>
<td>9654</td><td>11.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gug.html">Guaraní, Paraguayan</a></th>
<td>9658</td><td>-1.7</td>
<td>8956</td><td>1.8</td>
<td>8956</td><td>1.9</td>
<td>8956</td><td>3.4</td>
<td>8956</td><td>3.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fri.html">Frisian, Western</a></th>
<td>9660</td><td>-1.7</td>
<td>9495</td><td>8.0</td>
<td>9495</td><td>8.0</td>
<td>9495</td><td>9.6</td>
<td>9495</td><td>9.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_als.html">Albanian, Tosk</a></th>
<td>9703</td><td>-1.3</td>
<td>8972</td><td>2.0</td>
<td>8972</td><td>2.1</td>
<td>8972</td><td>3.5</td>
<td>8972</td><td>3.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ita.html">Italian</a></th>
<td>9739</td><td>-0.9</td>
<td>9674</td><td>10.0</td>
<td>9674</td><td>10.1</td>
<td>9674</td><td>11.6</td>
<td>9674</td><td>11.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mah.html">Marshallese</a></th>
<td>9758</td><td>-0.7</td>
<td>9758</td><td>11.0</td>
<td>9758</td><td>11.0</td>
<td>9758</td><td>12.6</td>
<td>9758</td><td>12.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_spa.html">Spanish</a></th>
<td>9759</td><td>-0.7</td>
<td>9574</td><td>8.9</td>
<td>9574</td><td>8.9</td>
<td>9574</td><td>10.5</td>
<td>9574</td><td>10.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_vec.html">Venetian</a></th>
<td>9764</td><td>-0.6</td>
<td>9083</td><td>3.3</td>
<td>9083</td><td>3.4</td>
<td>9083</td><td>4.8</td>
<td>9083</td><td>4.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_roh_sutsilv.html">Romansch (Sutsilvan)</a></th>
<td>9764</td><td>-0.6</td>
<td>9459</td><td>7.6</td>
<td>9459</td><td>7.6</td>
<td>9459</td><td>9.2</td>
<td>9459</td><td>8.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hsf.html">Huastec (Sierra de Otontepec)</a></th>
<td>9778</td><td>-0.5</td>
<td>9430</td><td>7.2</td>
<td>9430</td><td>7.3</td>
<td>9430</td><td>8.8</td>
<td>9430</td><td>8.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_zdj.html">Comorian, Ngazidja</a></th>
<td>9783</td><td>-0.4</td>
<td>9783</td><td>11.2</td>
<td>9783</td><td>11.3</td>
<td>9783</td><td>12.9</td>
<td>9783</td><td>12.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lns.html">Lamnso'</a></th>
<td>9792</td><td>-0.4</td>
<td>7828</td><td>-11.0</td>
<td>7828</td><td>-10.9</td>
<td>7648</td><td>-11.7</td>
<td>7648</td><td>-12.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_haw.html">Hawaiian</a></th>
<td>9812</td><td>-0.2</td>
<td>8588</td><td>-2.3</td>
<td>8588</td><td>-2.3</td>
<td>8588</td><td>-0.9</td>
<td>8588</td><td>-1.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_roh_surmiran.html">Romansch (Surmiran)</a></th>
<td>9827</td><td>0.0</td>
<td>9662</td><td>9.9</td>
<td>9662</td><td>9.9</td>
<td>9662</td><td>11.5</td>
<td>9662</td><td>11.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_deu_1996.html">German, Standard (1996)</a></th>
<td>9828</td><td>0.0</td>
<td>9696</td><td>10.3</td>
<td>9696</td><td>10.3</td>
<td>9696</td><td>11.9</td>
<td>9696</td><td>11.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mto.html">Mixe, Totontepec</a></th>
<td>9829</td><td>0.0</td>
<td>8351</td><td>-5.0</td>
<td>8351</td><td>-5.0</td>
<td>8351</td><td>-3.6</td>
<td>8351</td><td>-3.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_deu_1901.html">German, Standard (1901)</a></th>
<td>9830</td><td>0.0</td>
<td>9692</td><td>10.2</td>
<td>9692</td><td>10.3</td>
<td>9692</td><td>11.9</td>
<td>9692</td><td>11.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tly.html">Talysh</a></th>
<td>9836</td><td>0.1</td>
<td>8180</td><td>-7.0</td>
<td>8180</td><td>-6.9</td>
<td>8180</td><td>-5.6</td>
<td>8180</td><td>-5.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ace.html">Aceh</a></th>
<td>9845</td><td>0.2</td>
<td>9729</td><td>10.6</td>
<td>9729</td><td>10.7</td>
<td>9729</td><td>12.3</td>
<td>9729</td><td>12.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mlt.html">Maltese</a></th>
<td>9846</td><td>0.2</td>
<td>9198</td><td>4.6</td>
<td>9198</td><td>4.7</td>
<td>9198</td><td>6.2</td>
<td>9198</td><td>5.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hlt.html">Chin, Matu</a></th>
<td>9854</td><td>0.3</td>
<td>9840</td><td>11.9</td>
<td>9840</td><td>12.0</td>
<td>9840</td><td>13.6</td>
<td>9840</td><td>13.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ast.html">Asturian</a></th>
<td>9858</td><td>0.3</td>
<td>9636</td><td>9.6</td>
<td>9636</td><td>9.6</td>
<td>9636</td><td>11.2</td>
<td>9636</td><td>10.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gla.html">Gaelic, Scottish</a></th>
<td>9859</td><td>0.3</td>
<td>9646</td><td>9.7</td>
<td>9646</td><td>9.8</td>
<td>9646</td><td>11.3</td>
<td>9646</td><td>11.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_chk.html">Chuukese</a></th>
<td>9878</td><td>0.5</td>
<td>9878</td><td>12.3</td>
<td>9878</td><td>12.4</td>
<td>9878</td><td>14.0</td>
<td>9878</td><td>13.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nba.html">Nyemba</a></th>
<td>9882</td><td>0.6</td>
<td>9881</td><td>12.4</td>
<td>9881</td><td>12.4</td>
<td>9881</td><td>14.0</td>
<td>9881</td><td>13.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_amr.html">Amarakaeri</a></th>
<td>9917</td><td>0.9</td>
<td>9499</td><td>8.0</td>
<td>9499</td><td>8.1</td>
<td>9086</td><td>4.9</td>
<td>9086</td><td>4.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cbu.html">Candoshi-Shapra</a></th>
<td>9918</td><td>0.9</td>
<td>9862</td><td>12.1</td>
<td>9862</td><td>12.2</td>
<td>9862</td><td>13.8</td>
<td>9862</td><td>13.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_snn.html">Siona</a></th>
<td>9933</td><td>1.1</td>
<td>9161</td><td>4.2</td>
<td>9161</td><td>4.2</td>
<td>8826</td><td>1.9</td>
<td>8748</td><td>0.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ada.html">Dangme</a></th>
<td>9936</td><td>1.1</td>
<td>8796</td><td>0.0</td>
<td>8796</td><td>0.1</td>
<td>8779</td><td>1.3</td>
<td>8779</td><td>1.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sna.html">Shona</a></th>
<td>9943</td><td>1.2</td>
<td>9943</td><td>13.1</td>
<td>9943</td><td>13.1</td>
<td>9943</td><td>14.7</td>
<td>9943</td><td>14.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pbb.html">Páez</a></th>
<td>9980</td><td>1.6</td>
<td>9869</td><td>12.2</td>
<td>9869</td><td>12.3</td>
<td>9869</td><td>13.9</td>
<td>9869</td><td>13.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_roh.html">Romansch</a></th>
<td>10003</td><td>1.8</td>
<td>9866</td><td>12.2</td>
<td>9866</td><td>12.3</td>
<td>9866</td><td>13.9</td>
<td>9866</td><td>13.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pam.html">Pampangan</a></th>
<td>10005</td><td>1.8</td>
<td>10005</td><td>13.8</td>
<td>10005</td><td>13.8</td>
<td>10005</td><td>15.5</td>
<td>10005</td><td>15.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ceb.html">Cebuano</a></th>
<td>10008</td><td>1.8</td>
<td>10008</td><td>13.8</td>
<td>10008</td><td>13.9</td>
<td>10008</td><td>15.5</td>
<td>10008</td><td>15.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tgl.html">Tagalog</a></th>
<td>10013</td><td>1.9</td>
<td>10013</td><td>13.9</td>
<td>10013</td><td>13.9</td>
<td>10013</td><td>15.6</td>
<td>10013</td><td>15.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_eml.html">Romagnolo</a></th>
<td>10029</td><td>2.1</td>
<td>9511</td><td>8.2</td>
<td>9511</td><td>8.2</td>
<td>9511</td><td>9.8</td>
<td>9511</td><td>9.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fra.html">French</a></th>
<td>10030</td><td>2.1</td>
<td>9598</td><td>9.1</td>
<td>9598</td><td>9.2</td>
<td>9598</td><td>10.8</td>
<td>9598</td><td>10.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nso.html">Sotho, Northern</a></th>
<td>10036</td><td>2.1</td>
<td>9771</td><td>11.1</td>
<td>9771</td><td>11.2</td>
<td>9771</td><td>12.8</td>
<td>9771</td><td>12.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ind.html">Indonesian</a></th>
<td>10059</td><td>2.4</td>
<td>10059</td><td>14.4</td>
<td>10059</td><td>14.5</td>
<td>10059</td><td>16.1</td>
<td>10059</td><td>15.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tsn.html">Tswana</a></th>
<td>10067</td><td>2.4</td>
<td>10047</td><td>14.2</td>
<td>10047</td><td>14.3</td>
<td>10047</td><td>15.9</td>
<td>10047</td><td>15.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bug.html">Bugis</a></th>
<td>10070</td><td>2.5</td>
<td>10070</td><td>14.5</td>
<td>10070</td><td>14.6</td>
<td>10070</td><td>16.2</td>
<td>10070</td><td>15.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sun.html">Sunda</a></th>
<td>10071</td><td>2.5</td>
<td>10071</td><td>14.5</td>
<td>10071</td><td>14.6</td>
<td>10071</td><td>16.2</td>
<td>10071</td><td>15.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_uzn_latn.html">Uzbek, Northern (Latin)</a></th>
<td>10088</td><td>2.7</td>
<td>9836</td><td>11.8</td>
<td>9836</td><td>11.9</td>
<td>9836</td><td>13.5</td>
<td>9836</td><td>13.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gle.html">Gaelic, Irish</a></th>
<td>10114</td><td>2.9</td>
<td>9591</td><td>9.1</td>
<td>9591</td><td>9.1</td>
<td>9591</td><td>10.7</td>
<td>9591</td><td>10.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hns.html">Hindustani, Sarnami</a></th>
<td>10116</td><td>2.9</td>
<td>9963</td><td>13.3</td>
<td>9963</td><td>13.4</td>
<td>9963</td><td>15.0</td>
<td>9963</td><td>14.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tzh.html">Tzeltal, Oxchuc</a></th>
<td>10119</td><td>3.0</td>
<td>9780</td><td>11.2</td>
<td>9780</td><td>11.3</td>
<td>9780</td><td>12.9</td>
<td>9780</td><td>12.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tuk_latn.html">Turkmen (Latin)</a></th>
<td>10124</td><td>3.0</td>
<td>9185</td><td>4.4</td>
<td>9185</td><td>4.5</td>
<td>9185</td><td>6.0</td>
<td>9185</td><td>5.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_dga.html">Dagaare, Southern</a></th>
<td>10141</td><td>3.2</td>
<td>9495</td><td>8.0</td>
<td>9495</td><td>8.0</td>
<td>9477</td><td>9.4</td>
<td>9477</td><td>9.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ibo.html">Igbo</a></th>
<td>10151</td><td>3.3</td>
<td>8653</td><td>-1.6</td>
<td>8653</td><td>-1.5</td>
<td>8653</td><td>-0.1</td>
<td>8653</td><td>-0.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pcd.html">Picard</a></th>
<td>10151</td><td>3.3</td>
<td>9175</td><td>4.3</td>
<td>9175</td><td>4.4</td>
<td>9175</td><td>5.9</td>
<td>9175</td><td>5.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mic.html">Micmac</a></th>
<td>10162</td><td>3.4</td>
<td>9234</td><td>5.0</td>
<td>9234</td><td>5.1</td>
<td>9234</td><td>6.6</td>
<td>9234</td><td>6.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_uig_latn.html">Uyghur (Latin)</a></th>
<td>10186</td><td>3.7</td>
<td>9999</td><td>13.7</td>
<td>9999</td><td>13.8</td>
<td>9999</td><td>15.4</td>
<td>9999</td><td>15.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mly_latn.html">Malay (Latin)</a></th>
<td>10189</td><td>3.7</td>
<td>10188</td><td>15.9</td>
<td>10188</td><td>15.9</td>
<td>10188</td><td>17.6</td>
<td>10188</td><td>17.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_azj_latn.html">Azerbaijani, North (Latin)</a></th>
<td>10198</td><td>3.8</td>
<td>8717</td><td>-0.9</td>
<td>8717</td><td>-0.8</td>
<td>8717</td><td>0.6</td>
<td>8717</td><td>0.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_jpn.html">Japanese</a></th>
<td>10227</td><td>4.1</td>
<td>3437</td><td>-60.9</td>
<td>3437</td><td>-60.9</td>
<td>3437</td><td>-60.3</td>
<td>6832</td><td>-21.4</td>
<td>Jpan</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bis.html">Bislama</a></th>
<td>10233</td><td>4.1</td>
<td>10233</td><td>16.4</td>
<td>10233</td><td>16.4</td>
<td>10233</td><td>18.1</td>
<td>10233</td><td>17.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ban.html">Bali</a></th>
<td>10235</td><td>4.2</td>
<td>10235</td><td>16.4</td>
<td>10235</td><td>16.5</td>
<td>10235</td><td>18.1</td>
<td>10235</td><td>17.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_oci_2.html">Occitan (Francoprovençal, Savoie)</a></th>
<td>10240</td><td>4.2</td>
<td>8665</td><td>-1.5</td>
<td>8665</td><td>-1.4</td>
<td>8665</td><td>0.0</td>
<td>8665</td><td>-0.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tem.html">Themne</a></th>
<td>10244</td><td>4.2</td>
<td>8323</td><td>-5.4</td>
<td>8323</td><td>-5.3</td>
<td>8323</td><td>-3.9</td>
<td>8323</td><td>-4.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_krl.html">Karelian</a></th>
<td>10245</td><td>4.3</td>
<td>9874</td><td>12.3</td>
<td>9874</td><td>12.4</td>
<td>9761</td><td>12.6</td>
<td>9648</td><td>11.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nld.html">Dutch</a></th>
<td>10247</td><td>4.3</td>
<td>10246</td><td>16.5</td>
<td>10246</td><td>16.6</td>
<td>10246</td><td>18.2</td>
<td>10246</td><td>17.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bax.html">Bamun</a></th>
<td>10248</td><td>4.3</td>
<td>8744</td><td>-0.6</td>
<td>8744</td><td>-0.5</td>
<td>8744</td><td>0.9</td>
<td>8744</td><td>0.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bin.html">Edo</a></th>
<td>10262</td><td>4.4</td>
<td>10260</td><td>16.7</td>
<td>10260</td><td>16.8</td>
<td>10260</td><td>18.4</td>
<td>10260</td><td>18.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bcl.html">Bicolano, Central</a></th>
<td>10263</td><td>4.4</td>
<td>10263</td><td>16.7</td>
<td>10263</td><td>16.8</td>
<td>10263</td><td>18.4</td>
<td>10263</td><td>18.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tso_MZ.html">Tsonga (Mozambique)</a></th>
<td>10274</td><td>4.5</td>
<td>10047</td><td>14.2</td>
<td>10047</td><td>14.3</td>
<td>10047</td><td>15.9</td>
<td>10047</td><td>15.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_quy.html">Quechua, Ayacucho</a></th>
<td>10295</td><td>4.8</td>
<td>10273</td><td>16.8</td>
<td>10273</td><td>16.9</td>
<td>10273</td><td>18.6</td>
<td>10273</td><td>18.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hna.html">Mina</a></th>
<td>10300</td><td>4.8</td>
<td>9085</td><td>3.3</td>
<td>9085</td><td>3.4</td>
<td>9040</td><td>4.3</td>
<td>9040</td><td>4.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ron_2006.html">Romanian (2006)</a></th>
<td>10303</td><td>4.8</td>
<td>9683</td><td>10.1</td>
<td>9683</td><td>10.2</td>
<td>9683</td><td>11.7</td>
<td>9683</td><td>11.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ltz.html">Luxembourgeois</a></th>
<td>10306</td><td>4.9</td>
<td>9998</td><td>13.7</td>
<td>9998</td><td>13.8</td>
<td>9998</td><td>15.4</td>
<td>9998</td><td>15.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ron_1993.html">Romanian (1993)</a></th>
<td>10311</td><td>4.9</td>
<td>9691</td><td>10.2</td>
<td>9691</td><td>10.3</td>
<td>9691</td><td>11.8</td>
<td>9691</td><td>11.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ron_1953.html">Romanian (1953)</a></th>
<td>10317</td><td>5.0</td>
<td>9691</td><td>10.2</td>
<td>9691</td><td>10.3</td>
<td>9691</td><td>11.8</td>
<td>9691</td><td>11.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mxi.html">Mozarabic</a></th>
<td>10317</td><td>5.0</td>
<td>10184</td><td>15.8</td>
<td>10184</td><td>15.9</td>
<td>10184</td><td>17.5</td>
<td>10184</td><td>17.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_src.html">Sardinian, Logudorese</a></th>
<td>10323</td><td>5.0</td>
<td>10195</td><td>15.9</td>
<td>10195</td><td>16.0</td>
<td>10195</td><td>17.7</td>
<td>10195</td><td>17.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hat_popular.html">Haitian Creole French (Popular)</a></th>
<td>10339</td><td>5.2</td>
<td>10103</td><td>14.9</td>
<td>10103</td><td>15.0</td>
<td>10103</td><td>16.6</td>
<td>10103</td><td>16.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hil.html">Hiligaynon</a></th>
<td>10405</td><td>5.9</td>
<td>10405</td><td>18.3</td>
<td>10405</td><td>18.4</td>
<td>10405</td><td>20.1</td>
<td>10405</td><td>19.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cjs.html">Shor</a></th>
<td>10414</td><td>6.0</td>
<td>5724</td><td>-34.9</td>
<td>5724</td><td>-34.9</td>
<td>5724</td><td>-33.9</td>
<td>5724</td><td>-34.1</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sag.html">Sango</a></th>
<td>10428</td><td>6.1</td>
<td>8644</td><td>-1.7</td>
<td>8644</td><td>-1.6</td>
<td>8644</td><td>-0.2</td>
<td>8644</td><td>-0.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ilo.html">Ilocano</a></th>
<td>10429</td><td>6.1</td>
<td>10429</td><td>18.6</td>
<td>10429</td><td>18.7</td>
<td>10429</td><td>20.4</td>
<td>10429</td><td>20.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_oci_1.html">Occitan (Francoprovençal, Fribourg)</a></th>
<td>10439</td><td>6.2</td>
<td>9226</td><td>4.9</td>
<td>9226</td><td>5.0</td>
<td>9226</td><td>6.5</td>
<td>9226</td><td>6.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_niu.html">Niue</a></th>
<td>10444</td><td>6.3</td>
<td>10443</td><td>18.8</td>
<td>10443</td><td>18.8</td>
<td>10443</td><td>20.5</td>
<td>10443</td><td>20.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_swb.html">Comorian, Maore</a></th>
<td>10458</td><td>6.4</td>
<td>10340</td><td>17.6</td>
<td>10340</td><td>17.7</td>
<td>10340</td><td>19.3</td>
<td>10340</td><td>19.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_flm.html">Chin, Falam</a></th>
<td>10467</td><td>6.5</td>
<td>10467</td><td>19.0</td>
<td>10467</td><td>19.1</td>
<td>10467</td><td>20.8</td>
<td>10467</td><td>20.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ibb.html">Ibibio</a></th>
<td>10468</td><td>6.5</td>
<td>10467</td><td>19.0</td>
<td>10467</td><td>19.1</td>
<td>10467</td><td>20.8</td>
<td>10467</td><td>20.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lin_tones.html">Lingala (tones)</a></th>
<td>10476</td><td>6.6</td>
<td>8990</td><td>2.2</td>
<td>8990</td><td>2.3</td>
<td>8760</td><td>1.1</td>
<td>8760</td><td>0.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_heb.html">Hebrew</a></th>
<td>10502</td><td>6.9</td>
<td>5822</td><td>-33.8</td>
<td>5822</td><td>-33.8</td>
<td>5822</td><td>-32.8</td>
<td>5822</td><td>-33.0</td>
<td>Hebr</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nds.html">Saxon, Low</a></th>
<td>10539</td><td>7.2</td>
<td>10318</td><td>17.3</td>
<td>10318</td><td>17.4</td>
<td>10318</td><td>19.1</td>
<td>10318</td><td>18.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ven.html">Venda</a></th>
<td>10620</td><td>8.1</td>
<td>10106</td><td>14.9</td>
<td>10106</td><td>15.0</td>
<td>10106</td><td>16.6</td>
<td>10106</td><td>16.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mos.html">Mòoré</a></th>
<td>10621</td><td>8.1</td>
<td>9427</td><td>7.2</td>
<td>9427</td><td>7.3</td>
<td>9427</td><td>8.8</td>
<td>9427</td><td>8.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qug.html">Quichua, Chimborazo Highland</a></th>
<td>10651</td><td>8.4</td>
<td>10549</td><td>20.0</td>
<td>10549</td><td>20.0</td>
<td>10436</td><td>20.4</td>
<td>10323</td><td>18.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sme.html">Saami, North</a></th>
<td>10654</td><td>8.4</td>
<td>9944</td><td>13.1</td>
<td>9944</td><td>13.2</td>
<td>9944</td><td>14.8</td>
<td>9944</td><td>14.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_oci_4.html">Occitan (Francoprovençal, Valais)</a></th>
<td>10662</td><td>8.5</td>
<td>9413</td><td>7.0</td>
<td>9413</td><td>7.1</td>
<td>9413</td><td>8.6</td>
<td>9413</td><td>8.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_wln.html">Walloon</a></th>
<td>10714</td><td>9.0</td>
<td>9785</td><td>11.3</td>
<td>9785</td><td>11.3</td>
<td>9785</td><td>12.9</td>
<td>9785</td><td>12.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hun.html">Hungarian</a></th>
<td>10718</td><td>9.1</td>
<td>9783</td><td>11.2</td>
<td>9783</td><td>11.3</td>
<td>9783</td><td>12.9</td>
<td>9783</td><td>12.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nzi.html">Nzema</a></th>
<td>10740</td><td>9.3</td>
<td>9439</td><td>7.3</td>
<td>9439</td><td>7.4</td>
<td>9439</td><td>8.9</td>
<td>9439</td><td>8.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tso_ZW.html">Tsonga (Zimbabwe)</a></th>
<td>10758</td><td>9.5</td>
<td>10546</td><td>19.9</td>
<td>10546</td><td>20.0</td>
<td>10546</td><td>21.7</td>
<td>10546</td><td>21.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qvn.html">Quechua, North Junín</a></th>
<td>10765</td><td>9.5</td>
<td>10756</td><td>22.3</td>
<td>10756</td><td>22.4</td>
<td>10756</td><td>24.1</td>
<td>10756</td><td>23.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hea.html">Hmong, Northern Qiandong</a></th>
<td>10801</td><td>9.9</td>
<td>10801</td><td>22.8</td>
<td>10801</td><td>22.9</td>
<td>10801</td><td>24.7</td>
<td>10801</td><td>24.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kha.html">Khasi</a></th>
<td>10810</td><td>10.0</td>
<td>10605</td><td>20.6</td>
<td>10605</td><td>20.7</td>
<td>10605</td><td>22.4</td>
<td>10605</td><td>22.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_quc.html">K'iche', Central</a></th>
<td>10817</td><td>10.1</td>
<td>10817</td><td>23.0</td>
<td>10817</td><td>23.1</td>
<td>10817</td><td>24.8</td>
<td>10817</td><td>24.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_jav.html">Javanese (Latin)</a></th>
<td>10863</td><td>10.5</td>
<td>10863</td><td>23.5</td>
<td>10863</td><td>23.6</td>
<td>10863</td><td>25.4</td>
<td>10863</td><td>25.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_oci_3.html">Occitan (Francoprovençal, Vaud)</a></th>
<td>10885</td><td>10.8</td>
<td>9757</td><td>11.0</td>
<td>9757</td><td>11.0</td>
<td>9757</td><td>12.6</td>
<td>9757</td><td>12.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_jiv.html">Shuar</a></th>
<td>10930</td><td>11.2</td>
<td>10533</td><td>19.8</td>
<td>10533</td><td>19.9</td>
<td>10533</td><td>21.6</td>
<td>10533</td><td>21.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bci.html">Baoulé</a></th>
<td>10946</td><td>11.4</td>
<td>10204</td><td>16.0</td>
<td>10204</td><td>16.1</td>
<td>10204</td><td>17.8</td>
<td>10204</td><td>17.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_top.html">Totonac, Papantla</a></th>
<td>10955</td><td>11.5</td>
<td>10955</td><td>24.6</td>
<td>10955</td><td>24.7</td>
<td>10955</td><td>26.4</td>
<td>10955</td><td>26.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_evn.html">Evenki</a></th>
<td>10962</td><td>11.5</td>
<td>5948</td><td>-32.4</td>
<td>5948</td><td>-32.3</td>
<td>5776</td><td>-33.3</td>
<td>5776</td><td>-33.5</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kea.html">Kabuverdianu</a></th>
<td>10971</td><td>11.6</td>
<td>10334</td><td>17.5</td>
<td>10334</td><td>17.6</td>
<td>10334</td><td>19.3</td>
<td>10325</td><td>18.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_dyu.html">Jula</a></th>
<td>11038</td><td>12.3</td>
<td>8719</td><td>-0.9</td>
<td>8719</td><td>-0.8</td>
<td>8719</td><td>0.6</td>
<td>8719</td><td>0.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ewe.html">Éwé</a></th>
<td>11107</td><td>13.0</td>
<td>9967</td><td>13.3</td>
<td>9967</td><td>13.4</td>
<td>9950</td><td>14.8</td>
<td>9950</td><td>14.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cni.html">Asháninka</a></th>
<td>11167</td><td>13.6</td>
<td>11164</td><td>27.0</td>
<td>11164</td><td>27.0</td>
<td>11164</td><td>28.8</td>
<td>11164</td><td>28.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_blu.html">Hmong Njua</a></th>
<td>11179</td><td>13.8</td>
<td>11179</td><td>27.1</td>
<td>11179</td><td>27.2</td>
<td>11179</td><td>29.0</td>
<td>11179</td><td>28.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_009.html">Mbundu (009)</a></th>
<td>11200</td><td>14.0</td>
<td>11133</td><td>26.6</td>
<td>11133</td><td>26.7</td>
<td>11133</td><td>28.5</td>
<td>11133</td><td>28.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_arb.html">Arabic, Standard</a></th>
<td>11214</td><td>14.1</td>
<td>6183</td><td>-29.7</td>
<td>6183</td><td>-29.6</td>
<td>6166</td><td>-28.8</td>
<td>6166</td><td>-29.0</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_smo.html">Samoan</a></th>
<td>11231</td><td>14.3</td>
<td>11231</td><td>27.7</td>
<td>11231</td><td>27.8</td>
<td>11231</td><td>29.6</td>
<td>11231</td><td>29.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qvm.html">Quechua, Margos-Yarowilca-Lauricocha</a></th>
<td>11260</td><td>14.6</td>
<td>11108</td><td>26.3</td>
<td>11108</td><td>26.4</td>
<td>11108</td><td>28.2</td>
<td>11108</td><td>27.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_acu.html">Achuar-Shiwiar</a></th>
<td>11299</td><td>15.0</td>
<td>11296</td><td>28.5</td>
<td>11296</td><td>28.5</td>
<td>11296</td><td>30.4</td>
<td>11296</td><td>30.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_toj.html">Tojolabal</a></th>
<td>11465</td><td>16.7</td>
<td>10173</td><td>15.7</td>
<td>10173</td><td>15.8</td>
<td>10173</td><td>17.4</td>
<td>10173</td><td>17.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_buc.html">Bushi</a></th>
<td>11487</td><td>16.9</td>
<td>10980</td><td>24.9</td>
<td>10980</td><td>24.9</td>
<td>10980</td><td>26.7</td>
<td>10980</td><td>26.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_oss.html">Osetin</a></th>
<td>11528</td><td>17.3</td>
<td>6370</td><td>-27.6</td>
<td>6370</td><td>-27.5</td>
<td>6370</td><td>-26.5</td>
<td>6370</td><td>-26.7</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tzc.html">Tzotzil (Chamula)</a></th>
<td>11558</td><td>17.6</td>
<td>10703</td><td>21.7</td>
<td>10703</td><td>21.8</td>
<td>10703</td><td>23.5</td>
<td>10703</td><td>23.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_rar.html">Rarotongan</a></th>
<td>11562</td><td>17.7</td>
<td>11527</td><td>31.1</td>
<td>11527</td><td>31.2</td>
<td>11527</td><td>33.0</td>
<td>11527</td><td>32.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_yua.html">Maya, Yucatán</a></th>
<td>11732</td><td>19.4</td>
<td>10675</td><td>21.4</td>
<td>10675</td><td>21.5</td>
<td>10675</td><td>23.2</td>
<td>10675</td><td>22.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qxn.html">Quechua, Northern Conchucos Ancash</a></th>
<td>11786</td><td>19.9</td>
<td>11782</td><td>34.0</td>
<td>11782</td><td>34.1</td>
<td>11782</td><td>36.0</td>
<td>11782</td><td>35.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_guu.html">Yanomamö</a></th>
<td>11913</td><td>21.2</td>
<td>10470</td><td>19.1</td>
<td>10470</td><td>19.1</td>
<td>10470</td><td>20.8</td>
<td>10470</td><td>20.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_agr.html">Aguaruna</a></th>
<td>11918</td><td>21.3</td>
<td>11854</td><td>34.8</td>
<td>11854</td><td>34.9</td>
<td>11854</td><td>36.8</td>
<td>11854</td><td>36.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hau_NE.html">Hausa (Niger)</a></th>
<td>12078</td><td>22.9</td>
<td>11831</td><td>34.5</td>
<td>11831</td><td>34.6</td>
<td>11831</td><td>36.5</td>
<td>11831</td><td>36.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hau_NG.html">Hausa (Nigeria)</a></th>
<td>12078</td><td>22.9</td>
<td>11863</td><td>34.9</td>
<td>11863</td><td>35.0</td>
<td>11863</td><td>36.9</td>
<td>11863</td><td>36.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_vie.html">Vietnamese</a></th>
<td>12182</td><td>24.0</td>
<td>8877</td><td>0.9</td>
<td>8877</td><td>1.0</td>
<td>8877</td><td>2.4</td>
<td>8877</td><td>2.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cnh.html">Chin, Haka</a></th>
<td>12231</td><td>24.5</td>
<td>12231</td><td>39.1</td>
<td>12231</td><td>39.2</td>
<td>12231</td><td>41.2</td>
<td>12231</td><td>40.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qva.html">Quechua, Ambo-Pasco</a></th>
<td>12327</td><td>25.4</td>
<td>12181</td><td>38.5</td>
<td>12181</td><td>38.6</td>
<td>12181</td><td>40.6</td>
<td>12181</td><td>40.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cbr.html">Cashibo-Cacataibo</a></th>
<td>12349</td><td>25.7</td>
<td>11514</td><td>30.9</td>
<td>11514</td><td>31.0</td>
<td>11514</td><td>32.9</td>
<td>11514</td><td>32.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kdh.html">Tem</a></th>
<td>12418</td><td>26.4</td>
<td>8878</td><td>1.0</td>
<td>8878</td><td>1.0</td>
<td>8246</td><td>-4.8</td>
<td>8246</td><td>-5.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ojb.html">Ojibwa, Northwestern</a></th>
<td>12419</td><td>26.4</td>
<td>4775</td><td>-45.7</td>
<td>4775</td><td>-45.7</td>
<td>4775</td><td>-44.9</td>
<td>4775</td><td>-45.0</td>
<td>Cans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pcm.html">Pidgin, Nigerian</a></th>
<td>12424</td><td>26.4</td>
<td>12424</td><td>41.3</td>
<td>12424</td><td>41.4</td>
<td>12424</td><td>43.4</td>
<td>12424</td><td>43.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tah.html">Tahitian</a></th>
<td>12449</td><td>26.7</td>
<td>12244</td><td>39.2</td>
<td>12244</td><td>39.3</td>
<td>12244</td><td>41.3</td>
<td>12244</td><td>40.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_amc.html">Amahuaca</a></th>
<td>12530</td><td>27.5</td>
<td>12530</td><td>42.5</td>
<td>12530</td><td>42.6</td>
<td>12530</td><td>44.6</td>
<td>12530</td><td>44.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lob.html">Lobi</a></th>
<td>12645</td><td>28.7</td>
<td>10435</td><td>18.7</td>
<td>10435</td><td>18.7</td>
<td>10435</td><td>20.4</td>
<td>10435</td><td>20.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_csw.html">Cree, Swampy</a></th>
<td>12705</td><td>29.3</td>
<td>4849</td><td>-44.9</td>
<td>4849</td><td>-44.8</td>
<td>4849</td><td>-44.0</td>
<td>4849</td><td>-44.2</td>
<td>Cans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nav.html">Navajo</a></th>
<td>12835</td><td>30.6</td>
<td>9981</td><td>13.5</td>
<td>9981</td><td>13.6</td>
<td>9803</td><td>13.1</td>
<td>9803</td><td>12.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qxa.html">Quechua, South Bolivian</a></th>
<td>12924</td><td>31.5</td>
<td>12902</td><td>46.7</td>
<td>12902</td><td>46.8</td>
<td>12902</td><td>48.9</td>
<td>12902</td><td>48.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cak.html">Kaqchikel, Central</a></th>
<td>12943</td><td>31.7</td>
<td>12616</td><td>43.5</td>
<td>12616</td><td>43.6</td>
<td>12616</td><td>45.6</td>
<td>12616</td><td>45.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mri.html">Maori</a></th>
<td>12994</td><td>32.2</td>
<td>12993</td><td>47.7</td>
<td>12993</td><td>47.8</td>
<td>12993</td><td>49.9</td>
<td>12993</td><td>49.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_skr.html">Seraiki</a></th>
<td>13020</td><td>32.5</td>
<td>7303</td><td>-17.0</td>
<td>7303</td><td>-16.9</td>
<td>7302</td><td>-15.7</td>
<td>7302</td><td>-16.0</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tca.html">Ticuna</a></th>
<td>13137</td><td>33.7</td>
<td>10508</td><td>19.5</td>
<td>10508</td><td>19.6</td>
<td>9886</td><td>14.1</td>
<td>9886</td><td>13.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_arl.html">Arabela</a></th>
<td>13256</td><td>34.9</td>
<td>13255</td><td>50.7</td>
<td>13255</td><td>50.8</td>
<td>13255</td><td>53.0</td>
<td>13255</td><td>52.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ssw.html">Swati</a></th>
<td>13372</td><td>36.1</td>
<td>13320</td><td>51.5</td>
<td>13320</td><td>51.6</td>
<td>13320</td><td>53.7</td>
<td>13320</td><td>53.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_koi.html">Komi-Permyak</a></th>
<td>13499</td><td>37.4</td>
<td>7378</td><td>-16.1</td>
<td>7378</td><td>-16.0</td>
<td>7378</td><td>-14.9</td>
<td>7378</td><td>-15.1</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pes_1.html">Farsi, Western</a></th>
<td>13597</td><td>38.4</td>
<td>7537</td><td>-14.3</td>
<td>7537</td><td>-14.2</td>
<td>7460</td><td>-13.9</td>
<td>7460</td><td>-14.1</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ykg.html">Yukaghir, Northern</a></th>
<td>13618</td><td>38.6</td>
<td>7366</td><td>-16.2</td>
<td>7366</td><td>-16.2</td>
<td>7366</td><td>-15.0</td>
<td>7366</td><td>-15.2</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pes_2.html">Dari</a></th>
<td>13669</td><td>39.1</td>
<td>7607</td><td>-13.5</td>
<td>7607</td><td>-13.4</td>
<td>7561</td><td>-12.7</td>
<td>7561</td><td>-13.0</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_piu.html">Pintupi-Luritja</a></th>
<td>13736</td><td>39.8</td>
<td>13736</td><td>56.2</td>
<td>13736</td><td>56.3</td>
<td>13736</td><td>58.5</td>
<td>13736</td><td>58.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_urd.html">Urdu</a></th>
<td>13859</td><td>41.0</td>
<td>7768</td><td>-11.7</td>
<td>7768</td><td>-11.6</td>
<td>7733</td><td>-10.8</td>
<td>7733</td><td>-11.0</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pnb.html">Panjabi, Western</a></th>
<td>13996</td><td>42.4</td>
<td>7904</td><td>-10.1</td>
<td>7904</td><td>-10.1</td>
<td>7893</td><td>-8.9</td>
<td>7893</td><td>-9.2</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ton.html">Tongan</a></th>
<td>14017</td><td>42.6</td>
<td>12453</td><td>41.6</td>
<td>12453</td><td>41.7</td>
<td>12453</td><td>43.7</td>
<td>12453</td><td>43.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_yor.html">Yoruba</a></th>
<td>14059</td><td>43.1</td>
<td>10238</td><td>16.4</td>
<td>10238</td><td>16.5</td>
<td>9276</td><td>7.1</td>
<td>9276</td><td>6.8</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kal.html">Inuktitut, Greenlandic</a></th>
<td>14067</td><td>43.1</td>
<td>14067</td><td>60.0</td>
<td>14067</td><td>60.1</td>
<td>14067</td><td>62.3</td>
<td>14067</td><td>61.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_srp_cyrl.html">Serbian (Cyrillic)</a></th>
<td>14090</td><td>43.4</td>
<td>7740</td><td>-12.0</td>
<td>7740</td><td>-11.9</td>
<td>7740</td><td>-10.7</td>
<td>7740</td><td>-10.9</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_urd_2.html">Urdu (2)</a></th>
<td>14108</td><td>43.6</td>
<td>7904</td><td>-10.1</td>
<td>7904</td><td>-10.1</td>
<td>7868</td><td>-9.2</td>
<td>7868</td><td>-9.4</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_gld.html">Nanai</a></th>
<td>14148</td><td>44.0</td>
<td>7666</td><td>-12.8</td>
<td>7666</td><td>-12.8</td>
<td>7636</td><td>-11.9</td>
<td>7636</td><td>-12.1</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cot.html">Caquinte</a></th>
<td>14250</td><td>45.0</td>
<td>14246</td><td>62.0</td>
<td>14246</td><td>62.1</td>
<td>14246</td><td>64.4</td>
<td>14246</td><td>64.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tir.html">Tigrigna</a></th>
<td>14270</td><td>45.2</td>
<td>5502</td><td>-37.4</td>
<td>5502</td><td>-37.4</td>
<td>5502</td><td>-36.5</td>
<td>5502</td><td>-36.7</td>
<td>Ethi</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bos_cyrl.html">Bosnian (Cyrillic)</a></th>
<td>14404</td><td>46.6</td>
<td>7906</td><td>-10.1</td>
<td>7906</td><td>-10.0</td>
<td>7906</td><td>-8.8</td>
<td>7906</td><td>-9.0</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mly_arab.html">Malay (Arabic)</a></th>
<td>14410</td><td>46.6</td>
<td>7899</td><td>-10.2</td>
<td>7899</td><td>-10.1</td>
<td>7899</td><td>-8.8</td>
<td>7899</td><td>-9.1</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_koo.html">Konjo</a></th>
<td>14620</td><td>48.8</td>
<td>14620</td><td>66.2</td>
<td>14620</td><td>66.4</td>
<td>14620</td><td>68.7</td>
<td>14620</td><td>68.3</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pbu.html">Pashto, Northern</a></th>
<td>14727</td><td>49.9</td>
<td>8276</td><td>-5.9</td>
<td>8276</td><td>-5.8</td>
<td>8274</td><td>-4.5</td>
<td>8274</td><td>-4.8</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_boa.html">Bora</a></th>
<td>14934</td><td>52.0</td>
<td>11819</td><td>34.4</td>
<td>11819</td><td>34.5</td>
<td>11659</td><td>34.6</td>
<td>11659</td><td>34.2</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_qvh.html">Quechua, Huamalíes-Dos de Mayo Huánuco</a></th>
<td>14973</td><td>52.4</td>
<td>14772</td><td>68.0</td>
<td>14772</td><td>68.1</td>
<td>14772</td><td>70.5</td>
<td>14772</td><td>70.0</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tob.html">Toba</a></th>
<td>15250</td><td>55.2</td>
<td>14672</td><td>66.8</td>
<td>14672</td><td>67.0</td>
<td>14672</td><td>69.3</td>
<td>14672</td><td>68.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nhn.html">Nahuatl, Central</a></th>
<td>15460</td><td>57.3</td>
<td>15457</td><td>75.8</td>
<td>15457</td><td>75.9</td>
<td>15457</td><td>78.4</td>
<td>15457</td><td>77.9</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_vai.html">Vai</a></th>
<td>15555</td><td>58.3</td>
<td>6931</td><td>-21.2</td>
<td>6931</td><td>-21.1</td>
<td>6931</td><td>-20.0</td>
<td>6931</td><td>-20.2</td>
<td>Vaii</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tat.html">Tatar</a></th>
<td>15601</td><td>58.8</td>
<td>8493</td><td>-3.4</td>
<td>8493</td><td>-3.4</td>
<td>8493</td><td>-2.0</td>
<td>8493</td><td>-2.2</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tgk.html">Tajiki</a></th>
<td>15606</td><td>58.8</td>
<td>8594</td><td>-2.3</td>
<td>8594</td><td>-2.2</td>
<td>8594</td><td>-0.8</td>
<td>8594</td><td>-1.1</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mkd.html">Macedonian</a></th>
<td>15843</td><td>61.2</td>
<td>8704</td><td>-1.0</td>
<td>8704</td><td>-1.0</td>
<td>8704</td><td>0.5</td>
<td>8704</td><td>0.2</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ukr.html">Ukrainian</a></th>
<td>16109</td><td>63.9</td>
<td>8785</td><td>-0.1</td>
<td>8785</td><td>-0.0</td>
<td>8785</td><td>1.4</td>
<td>8785</td><td>1.1</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_azj_cyrl.html">Azerbaijani, North (Cyrillic)</a></th>
<td>16117</td><td>64.0</td>
<td>8733</td><td>-0.7</td>
<td>8733</td><td>-0.6</td>
<td>8733</td><td>0.8</td>
<td>8733</td><td>0.5</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_oaa.html">Orok</a></th>
<td>16118</td><td>64.0</td>
<td>8696</td><td>-1.1</td>
<td>8696</td><td>-1.0</td>
<td>8251</td><td>-4.8</td>
<td>8251</td><td>-5.0</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_amh.html">Amharic</a></th>
<td>16144</td><td>64.3</td>
<td>5382</td><td>-38.8</td>
<td>5382</td><td>-38.8</td>
<td>5382</td><td>-37.9</td>
<td>5382</td><td>-38.1</td>
<td>Ethi</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kaz.html">Kazakh</a></th>
<td>16273</td><td>65.6</td>
<td>8791</td><td>-0.0</td>
<td>8791</td><td>0.0</td>
<td>8791</td><td>1.5</td>
<td>8791</td><td>1.2</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_khk.html">Mongolian, Halh (Cyrillic)</a></th>
<td>16295</td><td>65.8</td>
<td>8837</td><td>0.5</td>
<td>8837</td><td>0.6</td>
<td>8837</td><td>2.0</td>
<td>8837</td><td>1.7</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_zgh.html">Tamazight, Standard Morocan</a></th>
<td>16301</td><td>65.9</td>
<td>6371</td><td>-27.6</td>
<td>6371</td><td>-27.5</td>
<td>6371</td><td>-26.5</td>
<td>6371</td><td>-26.7</td>
<td>Tfng</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tuk_cyrl.html">Turkmen (Cyrillic)</a></th>
<td>16438</td><td>67.3</td>
<td>8826</td><td>0.4</td>
<td>8826</td><td>0.4</td>
<td>8826</td><td>1.9</td>
<td>8826</td><td>1.6</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_alt.html">Altai, Southern</a></th>
<td>16508</td><td>68.0</td>
<td>8865</td><td>0.8</td>
<td>8865</td><td>0.9</td>
<td>8865</td><td>2.3</td>
<td>8865</td><td>2.0</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_shp.html">Shipibo-Conibo</a></th>
<td>16674</td><td>69.7</td>
<td>16391</td><td>86.4</td>
<td>16391</td><td>86.5</td>
<td>16391</td><td>89.2</td>
<td>16391</td><td>88.7</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bul.html">Bulgarian</a></th>
<td>16844</td><td>71.4</td>
<td>9228</td><td>4.9</td>
<td>9228</td><td>5.0</td>
<td>9228</td><td>6.5</td>
<td>9228</td><td>6.2</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hye.html">Armenian</a></th>
<td>16853</td><td>71.5</td>
<td>9038</td><td>2.8</td>
<td>9038</td><td>2.8</td>
<td>9038</td><td>4.3</td>
<td>9038</td><td>4.0</td>
<td>Armn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cbi.html">Chachi</a></th>
<td>17042</td><td>73.4</td>
<td>16912</td><td>92.3</td>
<td>16912</td><td>92.4</td>
<td>16911</td><td>95.2</td>
<td>16910</td><td>94.6</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bel.html">Belarusan</a></th>
<td>17117</td><td>74.2</td>
<td>9307</td><td>5.8</td>
<td>9307</td><td>5.9</td>
<td>9307</td><td>7.4</td>
<td>9307</td><td>7.1</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_blt.html">Tai Dam</a></th>
<td>17301</td><td>76.1</td>
<td>7181</td><td>-18.3</td>
<td>7181</td><td>-18.3</td>
<td>6423</td><td>-25.9</td>
<td>6423</td><td>-26.1</td>
<td>Tavt</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_abk.html">Abkhaz</a></th>
<td>17318</td><td>76.2</td>
<td>9280</td><td>5.5</td>
<td>9280</td><td>5.6</td>
<td>9280</td><td>7.1</td>
<td>9280</td><td>6.8</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ame.html">Yaneshaʼ</a></th>
<td>17336</td><td>76.4</td>
<td>15851</td><td>80.2</td>
<td>15851</td><td>80.4</td>
<td>15238</td><td>75.9</td>
<td>15238</td><td>75.4</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_uzn_cyrl.html">Uzbek, Northern (Cyrillic)</a></th>
<td>17394</td><td>77.0</td>
<td>9364</td><td>6.5</td>
<td>9364</td><td>6.6</td>
<td>9364</td><td>8.1</td>
<td>9364</td><td>7.8</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ady.html">Adyghe</a></th>
<td>17432</td><td>77.4</td>
<td>9483</td><td>7.8</td>
<td>9483</td><td>7.9</td>
<td>9483</td><td>9.4</td>
<td>9483</td><td>9.2</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kir.html">Kirghiz</a></th>
<td>17490</td><td>78.0</td>
<td>9390</td><td>6.8</td>
<td>9390</td><td>6.9</td>
<td>9390</td><td>8.4</td>
<td>9390</td><td>8.1</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nio.html">Nganasan</a></th>
<td>17527</td><td>78.4</td>
<td>9336</td><td>6.2</td>
<td>9336</td><td>6.2</td>
<td>9336</td><td>7.7</td>
<td>9336</td><td>7.5</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ydd.html">Yiddish, Eastern</a></th>
<td>17589</td><td>79.0</td>
<td>9593</td><td>9.1</td>
<td>9593</td><td>9.2</td>
<td>8621</td><td>-0.5</td>
<td>8621</td><td>-0.8</td>
<td>Hebr</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sah.html">Yakut</a></th>
<td>17615</td><td>79.3</td>
<td>9470</td><td>7.7</td>
<td>9470</td><td>7.8</td>
<td>9470</td><td>9.3</td>
<td>9470</td><td>9.0</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kjh.html">Khakas</a></th>
<td>17616</td><td>79.3</td>
<td>9554</td><td>8.6</td>
<td>9554</td><td>8.7</td>
<td>9554</td><td>10.3</td>
<td>9554</td><td>10.0</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tyv.html">Tuva</a></th>
<td>17717</td><td>80.3</td>
<td>9572</td><td>8.8</td>
<td>9572</td><td>8.9</td>
<td>9572</td><td>10.5</td>
<td>9572</td><td>10.2</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_rus.html">Russian</a></th>
<td>17750</td><td>80.6</td>
<td>9605</td><td>9.2</td>
<td>9605</td><td>9.3</td>
<td>9605</td><td>10.8</td>
<td>9605</td><td>10.6</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mcf.html">Matsés</a></th>
<td>17788</td><td>81.0</td>
<td>17336</td><td>97.1</td>
<td>17336</td><td>97.3</td>
<td>17336</td><td>100.1</td>
<td>17336</td><td>99.5</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kbd.html">Kabardian</a></th>
<td>17879</td><td>81.9</td>
<td>9633</td><td>9.5</td>
<td>9633</td><td>9.6</td>
<td>9633</td><td>11.2</td>
<td>9633</td><td>10.9</td>
<td>Cyrl</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ike.html">Inuktitut, Eastern Canadian</a></th>
<td>17910</td><td>82.3</td>
<td>6456</td><td>-26.6</td>
<td>6456</td><td>-26.5</td>
<td>6456</td><td>-25.5</td>
<td>6456</td><td>-25.7</td>
<td>Cans</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mag.html">Magahi</a></th>
<td>17920</td><td>82.4</td>
<td>6950</td><td>-21.0</td>
<td>6950</td><td>-20.9</td>
<td>5090</td><td>-41.3</td>
<td>6052</td><td>-30.3</td>
<td>Deva</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_uig_arab.html">Uyghur (Arabic)</a></th>
<td>18323</td><td>86.5</td>
<td>9826</td><td>11.7</td>
<td>9826</td><td>11.8</td>
<td>9826</td><td>13.4</td>
<td>9826</td><td>13.1</td>
<td>Arab</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ell_monotonic.html">Greek (monotonic)</a></th>
<td>18324</td><td>86.5</td>
<td>10017</td><td>13.9</td>
<td>10017</td><td>14.0</td>
<td>10017</td><td>15.6</td>
<td>10017</td><td>15.3</td>
<td>Grek</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_chr_cased.html">Cherokee (cased)</a></th>
<td>18759</td><td>90.9</td>
<td>7245</td><td>-17.6</td>
<td>7245</td><td>-17.6</td>
<td>7245</td><td>-16.4</td>
<td>7245</td><td>-16.6</td>
<td>Cher</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_chr_uppercase.html">Cherokee (uppercase)</a></th>
<td>18759</td><td>90.9</td>
<td>7245</td><td>-17.6</td>
<td>7245</td><td>-17.6</td>
<td>7245</td><td>-16.4</td>
<td>7245</td><td>-16.6</td>
<td>Cher</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bho.html">Bhojpuri</a></th>
<td>18930</td><td>92.6</td>
<td>7294</td><td>-17.1</td>
<td>7294</td><td>-17.0</td>
<td>5217</td><td>-39.8</td>
<td>6274</td><td>-27.8</td>
<td>Deva</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ell_polytonic.html">Greek (polytonic)</a></th>
<td>19555</td><td>99.0</td>
<td>10039</td><td>14.2</td>
<td>10039</td><td>14.2</td>
<td>10039</td><td>15.9</td>
<td>10039</td><td>15.6</td>
<td>Grek</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mai.html">Maithili</a></th>
<td>20047</td><td>104.0</td>
<td>7500</td><td>-14.7</td>
<td>7500</td><td>-14.7</td>
<td>5382</td><td>-37.9</td>
<td>6435</td><td>-25.9</td>
<td>Deva</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_nep.html">Nepali</a></th>
<td>20816</td><td>111.8</td>
<td>7720</td><td>-12.2</td>
<td>7720</td><td>-12.2</td>
<td>5338</td><td>-38.4</td>
<td>6615</td><td>-23.9</td>
<td>Deva</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ben.html">Bengali</a></th>
<td>21349</td><td>117.2</td>
<td>7871</td><td>-10.5</td>
<td>7871</td><td>-10.4</td>
<td>5318</td><td>-38.6</td>
<td>7061</td><td>-18.7</td>
<td>Beng</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tha2.html">Thai (2)</a></th>
<td>21694</td><td>120.8</td>
<td>7390</td><td>-16.0</td>
<td>7390</td><td>-15.9</td>
<td>5896</td><td>-32.0</td>
<td>5950</td><td>-31.5</td>
<td>Thai</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tha.html">Thai</a></th>
<td>21873</td><td>122.6</td>
<td>7479</td><td>-15.0</td>
<td>7479</td><td>-14.9</td>
<td>5992</td><td>-30.8</td>
<td>6043</td><td>-30.4</td>
<td>Thai</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_guj.html">Gujarati</a></th>
<td>21890</td><td>122.8</td>
<td>8184</td><td>-6.9</td>
<td>8184</td><td>-6.9</td>
<td>5586</td><td>-35.5</td>
<td>6978</td><td>-19.7</td>
<td>Gujr</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_cpu.html">Ashéninka, Pichis</a></th>
<td>22298</td><td>126.9</td>
<td>22163</td><td>152.0</td>
<td>22163</td><td>152.2</td>
<td>22163</td><td>155.8</td>
<td>22163</td><td>155.1</td>
<td>Latn</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_pan.html">Panjabi, Eastern</a></th>
<td>22584</td><td>129.8</td>
<td>8788</td><td>-0.1</td>
<td>8788</td><td>0.0</td>
<td>6181</td><td>-28.7</td>
<td>7470</td><td>-14.0</td>
<td>Guru</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_san.html">Sanskrit</a></th>
<td>22717</td><td>131.2</td>
<td>8171</td><td>-7.1</td>
<td>8171</td><td>-7.0</td>
<td>5186</td><td>-40.2</td>
<td>6544</td><td>-24.7</td>
<td>Deva</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_sin.html">Sinhala</a></th>
<td>22785</td><td>131.9</td>
<td>8519</td><td>-3.1</td>
<td>8519</td><td>-3.1</td>
<td>6061</td><td>-30.1</td>
<td>6853</td><td>-21.1</td>
<td>Sinh</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kkh_lana.html">Khün</a></th>
<td>23411</td><td>138.2</td>
<td>8047</td><td>-8.5</td>
<td>8047</td><td>-8.4</td>
<td>4655</td><td>-46.3</td>
<td>5140</td><td>-40.8</td>
<td>Lana</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kan.html">Kannada</a></th>
<td>23429</td><td>138.4</td>
<td>8463</td><td>-3.8</td>
<td>8463</td><td>-3.7</td>
<td>5580</td><td>-35.6</td>
<td>6989</td><td>-19.6</td>
<td>Knda</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_hin.html">Hindi</a></th>
<td>23466</td><td>138.8</td>
<td>8962</td><td>1.9</td>
<td>8962</td><td>2.0</td>
<td>6187</td><td>-28.6</td>
<td>7632</td><td>-12.2</td>
<td>Deva</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_lao.html">Lao</a></th>
<td>24128</td><td>145.5</td>
<td>8340</td><td>-5.2</td>
<td>8340</td><td>-5.1</td>
<td>6365</td><td>-26.5</td>
<td>6447</td><td>-25.8</td>
<td>Laoo</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tel.html">Telugu</a></th>
<td>24993</td><td>154.3</td>
<td>9145</td><td>4.0</td>
<td>9145</td><td>4.1</td>
<td>6027</td><td>-30.4</td>
<td>7156</td><td>-17.6</td>
<td>Telu</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_khm.html">Khmer, Central</a></th>
<td>25053</td><td>154.9</td>
<td>8619</td><td>-2.0</td>
<td>8619</td><td>-1.9</td>
<td>5511</td><td>-36.4</td>
<td>6791</td><td>-21.8</td>
<td>Khmr</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mal.html">Malayalam</a></th>
<td>25115</td><td>155.6</td>
<td>8907</td><td>1.3</td>
<td>8907</td><td>1.4</td>
<td>5286</td><td>-39.0</td>
<td>6762</td><td>-22.2</td>
<td>Mlym</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mar.html">Marathi</a></th>
<td>25231</td><td>156.8</td>
<td>9345</td><td>6.3</td>
<td>9345</td><td>6.3</td>
<td>6241</td><td>-28.0</td>
<td>7939</td><td>-8.6</td>
<td>Deva</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_jav_java.html">Javanese (Javanese)</a></th>
<td>26155</td><td>166.2</td>
<td>8741</td><td>-0.6</td>
<td>8741</td><td>-0.5</td>
<td>5207</td><td>-39.9</td>
<td>6786</td><td>-21.9</td>
<td>Java</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_kat.html">Georgian</a></th>
<td>26534</td><td>170.0</td>
<td>9742</td><td>10.8</td>
<td>9742</td><td>10.9</td>
<td>9742</td><td>12.4</td>
<td>9742</td><td>12.1</td>
<td>Geor</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_ccp.html">Chakma</a></th>
<td>27301</td><td>177.8</td>
<td>14231</td><td>61.8</td>
<td>7696</td><td>-12.4</td>
<td>4883</td><td>-43.6</td>
<td>5313</td><td>-38.8</td>
<td>Cakm</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_fuf_adlm.html">Pular (Adlam)</a></th>
<td>28460</td><td>189.6</td>
<td>14951</td><td>70.0</td>
<td>8233</td><td>-6.3</td>
<td>7435</td><td>-14.2</td>
<td>7435</td><td>-14.4</td>
<td>Adlm</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_div.html">Maldivian</a></th>
<td>28469</td><td>189.7</td>
<td>15030</td><td>70.9</td>
<td>15030</td><td>71.0</td>
<td>8449</td><td>-2.5</td>
<td>8449</td><td>-2.8</td>
<td>Thaa</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_dzo.html">Dzongkha</a></th>
<td>28504</td><td>190.1</td>
<td>9650</td><td>9.7</td>
<td>9650</td><td>9.8</td>
<td>7620</td><td>-12.1</td>
<td>7620</td><td>-12.3</td>
<td>Tibt</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mnw.html">Mon</a></th>
<td>28674</td><td>191.8</td>
<td>10016</td><td>13.9</td>
<td>10016</td><td>14.0</td>
<td>5751</td><td>-33.6</td>
<td>6233</td><td>-28.3</td>
<td>Mymr</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_san_gran.html">Sanskrit (Grantha)</a></th>
<td>29914</td><td>204.4</td>
<td>15418</td><td>75.3</td>
<td>8241</td><td>-6.2</td>
<td>5244</td><td>-39.5</td>
<td>8173</td><td>-5.9</td>
<td>Gran</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tam.html">Tamil</a></th>
<td>30208</td><td>207.4</td>
<td>10824</td><td>23.1</td>
<td>10824</td><td>23.2</td>
<td>6894</td><td>-20.4</td>
<td>9273</td><td>6.7</td>
<td>Taml</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_tam_LK.html">Tamil (Sri Lanka)</a></th>
<td>30213</td><td>207.4</td>
<td>10825</td><td>23.1</td>
<td>10825</td><td>23.2</td>
<td>6893</td><td>-20.5</td>
<td>9275</td><td>6.8</td>
<td>Taml</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_bod.html">Tibetan, Central</a></th>
<td>30411</td><td>209.5</td>
<td>10243</td><td>16.5</td>
<td>10243</td><td>16.6</td>
<td>7958</td><td>-8.2</td>
<td>7958</td><td>-8.4</td>
<td>Tibt</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_mya.html">Burmese</a></th>
<td>35846</td><td>264.8</td>
<td>12572</td><td>43.0</td>
<td>12572</td><td>43.1</td>
<td>7695</td><td>-11.2</td>
<td>8630</td><td>-0.7</td>
<td>Mymr</td>
</tr>
<tr>
<th><a href="https://www.unicode.org/udhr/d/udhr_shn.html">Shan</a></th>
<td>36130</td><td>267.7</td>
<td>12550</td><td>42.7</td>
<td>12550</td><td>42.8</td>
<td>8327</td><td>-3.9</td>
<td>8604</td><td>-1.0</td>
<td>Mymr</td>
</tr>
</tbody>
<tfoot>
<tr>
<th>Min</th>
<td>4170</td><td>-57.6</td>
<td>2202</td><td>-75.0</td>
<td>2202</td><td>-74.9</td>
<td>2202</td><td>-74.6</td>
<td>4007</td><td>-53.9</td>
<td></td>
</tr>
<tr><th>Median</th><td>9827</td><td></td><td>8794</td><td></td><td>8788</td><td></td><td>8665</td><td></td><td>8688</td><td></td><td></td></tr>
<tr>
<th>Mean</th>
<td>11315</td><td>15.1</td>
<td>8833</td><td>0.4</td>
<td>8787</td><td>-0.0</td>
<td>8567</td><td>-1.1</td>
<td>8700</td><td>0.1</td>
<td></td>
</tr>
<tr>
<th>Max (ignoring outlier)</th>
<td>35846</td><td>264.8</td>
<td>17336</td><td>97.1</td>
<td>17336</td><td>97.3</td>
<td>17336</td><td>100.1</td>
<td>17336</td><td>99.5</td>
<td></td>
</tr>
<tr>
<th>Max</th>
<td>36130</td><td>267.7</td>
<td>22163</td><td>152.0</td>
<td>22163</td><td>152.2</td>
<td>22163</td><td>155.8</td>
<td>22163</td><td>155.1</td>
<td></td>
</tr>
</tfoot>
</table>
<hr>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Io_uring, kTLS and Rust for zero syscall HTTPS server (442 pts)]]></title>
            <link>https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html</link>
            <guid>44980865</guid>
            <pubDate>Fri, 22 Aug 2025 03:51:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html">https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html</a>, See on <a href="https://news.ycombinator.com/item?id=44980865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">


  

  <div itemprop="articleBody">
    <p>Around the turn of the century we started to get a bigger need for high
capacity web servers. For example there was <a href="https://en.wikipedia.org/wiki/C10k_problem">the C10k problem</a> paper.</p>

<p>At the time, the kinds of things done to reduce work done per request was
pre-forking the web server. This means a request could be handled without an
expensive process creation.</p>

<p>Because yes, creating a new process for every request used to be something
perfectly normal.</p>

<p>Things did get better. People learned how to create threads, making things more
light weight. Then they switched to using <code>poll()</code>/<code>select()</code>, in order to not
just spare the process/thread creation, but the whole context switch.</p>

<p>I remember a comment on <a href="https://en.wikipedia.org/wiki/Kuro5hin">Kuro5hin</a> from anakata, the creator of both The
Pirate Bay and the web server that powered it, along the lines of “I am select()
of borg, resistance is futile”, mocking someone for not understanding how to
write a scalable web server.</p>

<p>But <code>select()</code>/<code>poll()</code> also doesn’t scale. If you have ten thousand
connections, that’s an array of ten thousand integers that need to be sent to
the kernel for every single iteration of your request handling loop.</p>

<p>Enter <code>epoll</code> (<code>kqueue</code> on other operating systems, but I’m focusing on Linux
here). Now that’s better. The main loop is now:</p>

<div><pre><code>  set_up_epoll()
  while True:
    new, read, write = epoll()
    epoll_add_connections(new)
    for con in read:
      process(con.read())
      if con.read_all_we_need:
        epoll_remove_read_op(con)
    for con in write:
      con.write_buffer()
      if con.buffer_empty:
        epoll_remove_write_op(con)
</code></pre>
</div>

<p>All the syscalls are pretty cheap. <code>epoll()</code> only deals in deltas, and it
doesn’t have to be re-told the thousands of active connections.</p>

<p>But they’re not without cost. Once we’ve gotten this far, the cost of a syscall
is actually a significant part of the total remaining cost.</p>

<p>We’re here going to ignore improvements like <code>sendfile()</code> and <code>splice()</code>, and
instead jump to…</p>



<p>Instead of performing a syscall for everything we want to do, commanding the
kernel to do this or that, io_uring lets us just keep writing orders to a
queue, and letting the kernel consume that queue asynchronously.</p>

<p>For example, we can put <code>accept()</code> into the queue. The kernel will pick that
up, wait for an incoming connection, and when it arrives it’ll put a
“completion” into the completion queue.</p>

<p>The web server can then check the completion queue. If there’s a completion
there, it can act on it.</p>

<p>This way the web server can queue up all kinds of operations that were
previously “expensive” syscalls by simply writing them to memory. That’s it.
And then it’ll read the results from another part of memory. That’s it.</p>

<p>In order to avoid busy looping, both the kernel and the web server will only
busy-loop checking the queue for a little bit (configurable, but think
milliseconds), and if there’s nothing new, the web server will do a syscall to
“go to sleep” until something gets added to the queue.</p>

<p>Similarly on the kernel side, the kernel will stop busy-looping if there’s
nothing new, and needs a syscall to start busylooping again.</p>

<p>This sounds like it would be tricky to optimize, but it’s not. In the end the
web server just puts stuff on the queue, and calls a library function that only
does that syscall if the kernel actually has stopped busylooping.</p>

<p>This means that a busy web server can serve all of its queries without even once
(after setup is done) needing to do a syscall. As long as queues keep getting
added to, <code>strace</code> will show <em>nothing</em>.</p>

<h2 id="one-thread-per-core">One thread per core</h2>

<p>Since CPUs today have many cores, ideally you want to run exactly one thread
per core, bind it to that core, and not share any read-write data structure.</p>

<p>For <a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">NUMA</a> hardware, you also want to make sure that a thread only
accesses memory on the local NUMA node. <a href="https://youtu.be/36qZYL5RlgY">This netflix talk</a> has some
interesting stuff on NUMA and high volume HTTP delivery.</p>

<p>The request load will still not be perfectly balanced between the threads (and
therefore cores), but I guess fixing that would have to be the topic of a
future post.</p>

<h2 id="memory-allocations">Memory allocations</h2>

<p>We will still have memory allocations though, both on the kernel and web server
side. Memory allocations in user space will eventually need syscalls.</p>

<p>For the web server side, you can pre-allocate a fixed chunk for every
connection, and then have everything about that connection live there. That way
new connections don’t need syscalls, memory doesn’t get fragmented, and you
don’t run the risk of running out of memory.</p>

<p>On the kernel side each connection will still need buffers for incoming and
outgoing bytes. This may be somewhat controllable via socket options, but again
it’ll have to be the subject of a future post.</p>

<p>Try to not run out of RAM. Bad things tend to happen.</p>

<h2 id="ktls">kTLS</h2>

<p><a href="https://docs.kernel.org/networking/tls-offload.html">kTLS</a> is a feature of the Linux kernel where an application can hand off
the job of encryption/decryption to the kernel. The application still has to
perform the TLS handshake, but after that it can enable kTLS and pretend that
it’s all sent in plaintext.</p>

<p>You may say that this doesn’t actually speed anything up, it just moves <em>where</em>
encryption was done. But there are gains:</p>

<ol>
  <li>This means that <code>sendfile()</code> can be used, removing the need to copy a bunch
of data between user space and kernel space.</li>
  <li>If the network card has hardware support for it, the crypto operation may
actually be offloaded from the CPU onto the network card, leaving the CPU to
do better things.</li>
</ol>

<h2 id="descriptorless-files">Descriptorless files</h2>

<p>Another optimization is to avoid passing file descriptors back and forth
between user space and kernel space. The mapping between file descriptors and
io_uring apparently has overhead.</p>

<p>So in comes <a href="https://lwn.net/Articles/863071/">descriptorless files</a> via
<a href="https://docs.rs/io-uring/latest/io_uring/struct.Submitter.html#method.register_files"><code>register_files</code></a>.</p>

<p>Now the supposed file descriptor numbers that user space sees are just
integers. They don’t show up in <code>/proc/pid/fd</code>, and can only be used with
io_uring. They’re still capped by the <code>ulimit</code> file descriptor limit, though.</p>

<h2 id="tarweb">tarweb</h2>

<p>In order to learn these technologies better, I built <a href="https://github.com/ThomasHabets/tarweb">a web server
incorporating all these things</a>.</p>

<p>It’s named <code>tarweb</code> because it’s a web server that serves the content of a
single tar file.</p>

<p>Rust, io_uring, and kTLS. Not exactly the most common combination. I found that
io_uring and kTLS didn’t play super well together. Enabling kTLS requires three
<code>setsockopt()</code> calls, and io_uring doesn’t support <code>setsockopt</code> (until they
merge <a href="https://github.com/tokio-rs/io-uring/pull/320">my PR</a>, that is).</p>

<p>And the <code>ktls</code> crate, part of <code>rustls</code>, only allows you to call the synchronous
<code>setsockopt()</code>, not export the needed struct for me to pass to my new
io_uring <code>setsockopt</code>. <a href="https://github.com/rustls/ktls/pull/54">Another pr sent</a>.</p>

<p>So with those two PRs merged, it’s working great.</p>

<p>tarweb is far from perfect. The code needs a lot of work, and there’s no
guarantee that the TLS library (rustls) doesn’t do memory allocations during
handshakes. But it does serve https without even one syscall on a per request
basis. And that’s pretty cool.</p>

<h2 id="benchmarks">Benchmarks</h2>

<p>I have not done any benchmarks yet. I want to clean the code up first.</p>

<h2 id="io-uring-and-safety">io-uring and safety</h2>

<p>One thing making io_uring more complex than synchronous syscalls is that any
buffer needs to stay in memory until the operation is marked completed by
showing up in the completion queue.</p>

<p>For example when submitting a <code>write</code> operation, the memory location of those
bytes must not be deallocated or overwritten.</p>

<p>The <code>io-uring</code> crate doesn’t help much with this. The API doesn’t allow the
borrow checker to protect you at compile time, and I don’t see it doing any
runtime checks either.</p>

<p>I feel like I’m back in C++, where any mistake can blow your whole leg off.
It’s a miracle that I’ve not seen a segfault.</p>

<p>Someone should make a <code>safer-ring</code> crate or similar, using the powers of
<a href="https://blog.cloudflare.com/pin-and-unpin-in-rust/">pinning</a> and/or borrows or something, to achieve Rust’s normal “if it
compiles, then it’s correct”.</p>


  </div>

  
  
  
  
  
  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything Is Correlated (231 pts)]]></title>
            <link>https://gwern.net/everything</link>
            <guid>44980339</guid>
            <pubDate>Fri, 22 Aug 2025 02:05:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gwern.net/everything">https://gwern.net/everything</a>, See on <a href="https://news.ycombinator.com/item?id=44980339">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-metadata">
          
        <p>Anthology of sociology, statistical, or psychological papers discussing the observation that all real-world variables have non-zero correlations and the implications for statistical theory such as ‘null hypothesis testing’.</p>
        
      </div><div id="markdownBody"><div>
<blockquote>
<p>Statistical folklore asserts that “everything is correlated”: in any real-world dataset, most or all measured variables will have non-zero correlations, even between variables which appear to be completely independent of each other, and that these correlations are not merely sampling error flukes but will appear in large-scale datasets to arbitrarily designated levels of <a href="https://en.wikipedia.org/wiki/Statistical_significance" id="_P2KpkSBK" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Statistical_significance#bodyContent" title="Statistical-significance">statistical-significance</a> or posterior probability.</p>
<p>This raises serious questions for null-hypothesis statistical-significance testing, as it implies the null hypothesis of 0 will always be rejected with sufficient data, meaning that a failure to reject only implies insufficient data, and provides no actual test or confirmation of a theory. Even a directional prediction is minimally confirmatory since there is a 50% chance of picking the right direction at random.</p>
<p>It also has implications for conceptualizations of theories &amp; causal models, interpretations of structural models, and other statistical principles such as the “sparsity principle”.</p>
</blockquote>
</div>
<p>Knowing one variable tells you (a little) about everything else. In statistics &amp; psychology folklore, this idea circulates under many names: “everything is correlated”, “everything is related to everything else”, “crud factor”, “the null hypothesis is always false”, “coefficients are never zero”, “ambient correlational noise”, <a href="https://en.wikipedia.org/wiki/Edward_Thorndike" id="_eD7_4_zC" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Edward_Thorndike#bodyContent" title="Edward Thorndike">Thorndike’s</a> dictum (“in human nature good traits go together”<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>), etc. Closely related are the <a href="https://gwern.net/doc/www/citeseerx.ist.psu.edu/75d8bce1345921c88e4a5c0aa10ebf503f3f1429.pdf#page=518" id="_BDPfCSfr" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/citeseerx.ist.psu.edu/75d8bce1345921c88e4a5c0aa10ebf503f3f1429.pdf#page=518" data-url-original="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.644.2669&amp;rep=rep1&amp;type=pdf#page=518" title="'In praise of sparsity and convexity', Tibshirani 2014">“bet on sparsity principle”</a><a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>, <a href="https://en.wikipedia.org/wiki/Anna_Karenina_principle" id="_zDohaOlu" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Anna_Karenina_principle#bodyContent" title="Anna Karenina principle">Anna Karenina principle</a>, <a href="https://en.wikipedia.org/wiki/Barry_Commoner" id="_OEwL_s8W" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Barry_Commoner#bodyContent" title="Barry Commoner">Barry Commoner’s</a> “first law of ecology” (“Everything is connected to everything else”) &amp; <a href="https://en.wikipedia.org/wiki/Waldo_R._Tobler" id="_2174Ovvd" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Waldo_R._Tobler#bodyContent" title="Waldo R. Tobler">Waldo R. Tobler’s</a> <a href="https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography" id="_ToNCz_VP" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Tobler%27s_first_law_of_geography#bodyContent" title="Tobler's first law of geography">“first law of geography”</a> (<a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/a068a23457b8ececa6a739f92c264d24c511085f.pdf" id="tobler-1970" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/a068a23457b8ececa6a739f92c264d24c511085f.pdf" data-url-original="https://pdfs.semanticscholar.org/eaa5/eefedd4fa34b7de7448c0c8e0822e9fdf956.pdf" data-filesize-bytes="587763" data-filesize-percentage="37" title="'A computer movie simulating urban growth in the Detroit region', Tobler 1970">“everything is related to everything else, but near things are more related than distant things”</a>).<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>The core idea here is that in any real-world dataset, it is exceptionally unlikely that any particular relationship will be exactly 0 for reasons of arithmetic (eg. it may be impossible for a binary variable to be an equal percentage in 2 unbalanced groups); prior probability (0 is only one number out of the infinite reals); and because real-world properties &amp; traits are linked by a myriad of causal networks, dynamics, &amp; <a href="https://en.wikipedia.org/wiki/Latent_and_observable_variables" id="_hM0a8AZP" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Latent_and_observable_variables#bodyContent" title="Latent and observable variables">latent</a> variables (eg. the <a href="https://en.wikipedia.org/wiki/Genetic_correlation" id="_EFM6e8jH" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Genetic_correlation#bodyContent" title="Genetic correlation">genetic correlations</a> which affect all human traits, see <a href="#genetic-correlations">heat maps in appendix</a> for visualizations) which mutually affect each other which will produce genuine correlations between apparently-independent variables, and these correlations may be of surprisingly large &amp; important size.</p>
<p>These reasons are unaffected by sample size and are not simply due to ‘small <em>n</em>’. If we simulate out uncorrelated random variables, even with small sizes, they quickly approach absolute correlations of ~0, and few will be of meaningful size like |<em>r</em>| &gt; 0.10:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span># Simulation of the 'crud factor' in psychology, where variables seem to always be intercorrelated non-zero, with a rough absolute correlation of 0.1.</span></span>
<span id="cb1-2"><span># I would like to simulate out the null hypothesis of completely uncorrelated variables in plausible dataset sizes.</span></span>
<span id="cb1-3"><span># So this is a R Monte Carlo simulation to simulate a multivariate normal distribution of 1,000 independent, uncorrelated N(0,1) variables and drawing 1,000 datapoints from it; calculate the sample r correlation of all the variables, and what fraction are correlated r &gt; |0.1|. Then let's Monte Carlo that, say, 100 times (to avoid taking too long) and plot the averaged distributions of the |r| histogram.</span></span>
<span id="cb1-4"><span># Output:</span></span>
<span id="cb1-5"><span># # Mean proportion of |r| &gt; 0.1: 0.002 (SD: 0.000)</span></span>
<span id="cb1-6"><span>library</span>(MASS)</span>
<span id="cb1-7"><span>library</span>(ggplot2)</span>
<span id="cb1-8"><span>library</span>(reshape2)</span>
<span id="cb1-9"><span>library</span>(parallel)</span>
<span id="cb1-10"></span>
<span id="cb1-11"><span># Parameters</span></span>
<span id="cb1-12">n_vars <span>&lt;-</span> <span>1000</span></span>
<span id="cb1-13">n_samples <span>&lt;-</span> <span>1000</span></span>
<span id="cb1-14">n_sims <span>&lt;-</span> <span>100</span></span>
<span id="cb1-15">threshold <span>&lt;-</span> <span>0.1</span></span>
<span id="cb1-16"></span>
<span id="cb1-17"><span># Function to run one simulation and return correlation metrics</span></span>
<span id="cb1-18">run_simulation <span>&lt;-</span> <span>function</span>(seed, n_vars, n_samples, threshold) {</span>
<span id="cb1-19">  <span>set.seed</span>(seed)</span>
<span id="cb1-20">  sigma <span>&lt;-</span> <span>diag</span>(n_vars)</span>
<span id="cb1-21">  data <span>&lt;-</span> <span>mvrnorm</span>(<span>n =</span> n_samples,</span>
<span id="cb1-22">                  <span>mu =</span> <span>rep</span>(<span>0</span>, n_vars),</span>
<span id="cb1-23">                  <span>Sigma =</span> sigma)</span>
<span id="cb1-24"></span>
<span id="cb1-25">  cors <span>&lt;-</span> <span>cor</span>(data)</span>
<span id="cb1-26">  cors_upper <span>&lt;-</span> cors[<span>upper.tri</span>(cors)]</span>
<span id="cb1-27"></span>
<span id="cb1-28">  <span># Return both the histogram counts and proportion above threshold</span></span>
<span id="cb1-29">  hist_data <span>&lt;-</span> <span>hist</span>(<span>abs</span>(cors_upper), <span>breaks =</span> <span>seq</span>(<span>0</span>, <span>1</span>, <span>by =</span> <span>0.01</span>), <span>plot =</span> <span>FALSE</span>)</span>
<span id="cb1-30"></span>
<span id="cb1-31">  <span>return</span>(<span>list</span>(</span>
<span id="cb1-32">    <span>hist_counts =</span> hist_data<span>$</span>counts,</span>
<span id="cb1-33">    <span>prop_above_thresh =</span> <span>mean</span>(<span>abs</span>(cors_upper) <span>&gt;</span> threshold)</span>
<span id="cb1-34">  ))</span>
<span id="cb1-35">}</span>
<span id="cb1-36"></span>
<span id="cb1-37"><span># Set up parallel processing</span></span>
<span id="cb1-38">n_cores <span>&lt;-</span> <span>detectCores</span>() <span>-</span> <span>1</span></span>
<span id="cb1-39">cl <span>&lt;-</span> <span>makeCluster</span>(n_cores)</span>
<span id="cb1-40"></span>
<span id="cb1-41"><span># Export required packages and variables to the cluster</span></span>
<span id="cb1-42"><span>clusterEvalQ</span>(cl, {</span>
<span id="cb1-43">  <span>library</span>(MASS)</span>
<span id="cb1-44">})</span>
<span id="cb1-45"><span>clusterExport</span>(cl, <span>c</span>(<span>"n_vars"</span>, <span>"n_samples"</span>, <span>"threshold"</span>))</span>
<span id="cb1-46"></span>
<span id="cb1-47"><span># Run simulations in parallel</span></span>
<span id="cb1-48">seeds <span>&lt;-</span> <span>1</span><span>:</span>n_sims</span>
<span id="cb1-49">results <span>&lt;-</span> <span>parLapply</span>(cl, seeds, run_simulation,</span>
<span id="cb1-50">                    <span>n_vars =</span> n_vars,</span>
<span id="cb1-51">                    <span>n_samples =</span> n_samples,</span>
<span id="cb1-52">                    <span>threshold =</span> threshold)</span>
<span id="cb1-53"></span>
<span id="cb1-54"><span># Clean up</span></span>
<span id="cb1-55"><span>stopCluster</span>(cl)</span>
<span id="cb1-56"></span>
<span id="cb1-57"><span># Process results</span></span>
<span id="cb1-58">props <span>&lt;-</span> <span>sapply</span>(results, <span>function</span>(x) x<span>$</span>prop_above_thresh)</span>
<span id="cb1-59">mean_prop <span>&lt;-</span> <span>mean</span>(props)</span>
<span id="cb1-60"></span>
<span id="cb1-61"><span># Average the histogram counts across simulations</span></span>
<span id="cb1-62">avg_counts <span>&lt;-</span> <span>Reduce</span>(<span>'+'</span>, <span>lapply</span>(results, <span>function</span>(x) x<span>$</span>hist_counts)) <span>/</span> n_sims</span>
<span id="cb1-63"></span>
<span id="cb1-64"><span># Calculate total possible correlations for one simulation</span></span>
<span id="cb1-65">total_cors <span>&lt;-</span> (n_vars <span>*</span> (n_vars <span>-</span> <span>1</span>)) <span>/</span> <span>2</span></span>
<span id="cb1-66"></span>
<span id="cb1-67"><span># Create plotting data</span></span>
<span id="cb1-68">plot_data <span>&lt;-</span> <span>data.frame</span>(</span>
<span id="cb1-69">  <span>correlation =</span> <span>seq</span>(<span>0</span>, <span>0.99</span>, <span>by =</span> <span>0.01</span>)[<span>1</span><span>:</span><span>length</span>(avg_counts)],</span>
<span id="cb1-70">  <span>count =</span> avg_counts</span>
<span id="cb1-71">)</span>
<span id="cb1-72"></span>
<span id="cb1-73"><span># Create histogram</span></span>
<span id="cb1-74">p1 <span>&lt;-</span> <span>ggplot</span>(plot_data, <span>aes</span>(<span>x =</span> correlation, <span>y =</span> count<span>/</span>total_cors <span>*</span> <span>100</span>)) <span>+</span></span>
<span id="cb1-75">  <span>geom_bar</span>(<span>stat =</span> <span>"identity"</span>, <span>fill =</span> <span>"blue"</span>, <span>alpha =</span> <span>0.7</span>) <span>+</span></span>
<span id="cb1-76">  <span>theme_bw</span>(<span>base_size =</span> <span>40</span>) <span>+</span></span>
<span id="cb1-77">  <span>geom_vline</span>(<span>xintercept =</span> threshold, <span>color =</span> <span>"red"</span>, <span>linetype =</span> <span>"dashed"</span>, <span>size=</span><span>3</span>) <span>+</span></span>
<span id="cb1-78">  <span>theme</span>(<span>plot.title =</span> <span>element_text</span>(<span>face =</span> <span>"bold"</span>)) <span>+</span></span>
<span id="cb1-79">  <span>scale_y_continuous</span>(</span>
<span id="cb1-80">    <span>labels =</span> <span>function</span>(x) <span>paste0</span>(<span>round</span>(x, <span>1</span>), <span>"%"</span>),</span>
<span id="cb1-81">    <span>name =</span> <span>"Percentage of All Inter-Correlations"</span>,</span>
<span id="cb1-82">    <span>expand =</span> <span>c</span>(<span>0</span>, <span>0</span>),</span>
<span id="cb1-83">    <span>limits =</span> <span>c</span>(<span>0</span>, <span>NA</span>)</span>
<span id="cb1-84">  ) <span>+</span></span>
<span id="cb1-85">  <span>scale_x_continuous</span>(</span>
<span id="cb1-86">    <span>name =</span> <span>expression</span>(<span>paste</span>(<span>"Absolute Correlation (|"</span>, <span>italic</span>(<span>"r"</span>), <span>"|)"</span>)),</span>
<span id="cb1-87">    <span>breaks =</span> <span>seq</span>(<span>0</span>, <span>0.13</span>, <span>by =</span> <span>0.02</span>),</span>
<span id="cb1-88">    <span>expand =</span> <span>c</span>(<span>0</span>, <span>0</span>),</span>
<span id="cb1-89">    <span>limits =</span> <span>c</span>(<span>0</span>, <span>0.13</span>)</span>
<span id="cb1-90">  ) <span>+</span></span>
<span id="cb1-91">  <span>annotate</span>(<span>"text"</span>,</span>
<span id="cb1-92">           <span>x =</span> <span>0.101</span>,</span>
<span id="cb1-93">           <span>y =</span> <span>max</span>(avg_counts<span>/</span>total_cors <span>*</span> <span>100</span>)<span>/</span><span>3</span>,</span>
<span id="cb1-94">           <span>label =</span> <span>substitute</span>(<span>paste</span>(value, <span>"% exceed |"</span>, <span>italic</span>(<span>"r"</span>), <span>"| &gt; "</span>, thresh),</span>
<span id="cb1-95">                            <span>list</span>(<span>value =</span> <span>sprintf</span>(<span>"%.1f"</span>, <span>100</span> <span>*</span> mean_prop),</span>
<span id="cb1-96">                                 <span>thresh =</span> <span>sprintf</span>(<span>"%.2f"</span>, threshold))),</span>
<span id="cb1-97">           <span>size =</span> <span>10</span>,</span>
<span id="cb1-98">           <span>hjust =</span> <span>0</span>) <span>+</span></span>
<span id="cb1-99">  <span>labs</span>(<span>title =</span> <span>"Distribution of Absolute Correlations"</span>)</span>
<span id="cb1-100"></span>
<span id="cb1-101"><span># Print summary statistics</span></span>
<span id="cb1-102"><span>cat</span>(<span>sprintf</span>(<span>"Mean proportion of |r| &gt; 0.1: %.3f (SD: %.3f)</span><span>\n</span><span>"</span>, <span>mean</span>(props), <span>sd</span>(props)))</span>
<span id="cb1-103"></span>
<span id="cb1-104"><span>print</span>(p1)</span></code></pre></div>
<figure>
<p><img alt="A Monte Carlo simulation of an uncorrelated multivariate normal in R shows that even with p = 1,000 and n = 1,000 (many variables and a small dataset), we rarely will observe ‘crud factor’-style correlations between uncorrelated variables, and so the crud factor is not a statistical triviality." data-aspect-ratio="350 / 221" decoding="async" height="884" loading="lazy" src="https://gwern.net/doc/statistics/probability/2025-01-29-gwern-r-uncorrelatedmultivariatenormalsarerarelycorrelatedrgreaterthanpoint1.jpg" width="1400"></p>
<figcaption><p>A <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method" id="_Yik66BZ1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Monte_Carlo_method#bodyContent" title="Monte Carlo method">Monte Carlo simulation</a> of an uncorrelated multivariate normal in R shows that even with <em>p</em> = 1,000 and <em>n</em> = 1,000 (many variables and a small dataset), we rarely will observe ‘crud factor’-style correlations between uncorrelated variables, and so the crud factor is not a statistical triviality.</p></figcaption>
</figure>
<p>The claim is generally backed up by personal experience and reasoning, although in a few instances like Meehl large datasets are mentioned in which almost all variables are correlated at high levels of statistical-significance.</p>
<section id="importance">
<h2><a href="#importance" title="Link to section: § 'Importance'">Importance</a></h2>
<p>This claim has several implications:</p>
<ol>
<li><p><strong>Sharp null hypotheses are meaningless</strong>: The most commonly mentioned, and the apparent motivation for early discussions, is that in the null-hypothesis <a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_test" id="_OtaIDcz3" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Statistical_hypothesis_test#bodyContent" title="Statistical hypothesis test">significance-testing</a> paradigm dominant in psychology and many sciences, any sharp null-hypothesis such as a parameter (like a Pearson’s <em>r</em> correlation) being exactly equal to 0 is known—in advance—to already be false and so it will inevitably be rejected as soon as sufficient data collection permits sampling to the foregone conclusion.</p>
<p>The existence of pervasive correlations, in addition to the presence of systematic error<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>, guarantees nonzero ‘effects’. This renders the meaning of significance-testing unclear; it is calculating precisely the odds of the data under scenarios known <em>a priori</em> to be false.</p></li>
<li><p><strong>Directional hypotheses are little better</strong>: better null-hypotheses, such as &gt;0 or &lt;0, are also problematic since if the true value of a parameter is never 0 then one’s theories have at least a 50-50 chance of guessing the right direction and so correct ‘predictions’ of the sign count for little.</p>
<p>This renders any successful predictions of little value.</p></li>
<li><p><strong>Model interpretation is difficult</strong>: This extensive intercorrelation threatens many naive statistical models or theoretical interpretations thereof, quite aside from <em>p</em>-values</p>
<p>For example, given the large amounts of <a href="https://en.wikipedia.org/wiki/Observational_error" id="_5Yg7bDxK" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Observational_error#bodyContent" title="Observational error">measurement error</a> in most sociological or psychological traits such as <a href="https://en.wikipedia.org/wiki/Socioeconomic_status" id="__fxxFvC3" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Socioeconomic_status#bodyContent" title="Socioeconomic status">SES</a>, home environment, or IQ, fully ‘controlling for’ a latent variable based on measured variables is difficult or impossible and said variable will in fact be correlated with the primary variable of interest, leading to <span id="gwern-notes-regression"></span> <a href="https://gwern.net/doc/statistics/bayes/regression-to-mean/index" id="_8LEzTTCn" data-filesize-bytes="43699" data-filesize-percentage="38" title="'Regression To The Mean Fallacies', Gwern 2021">“residual confounding”</a></p></li>
<li><p><strong>Intercorrelation implies causal networks</strong>: The empirical fact of extensive intercorrelations is consistent with the <a href="https://gwern.net/causality" id="gwern-causality" data-filesize-bytes="54123" data-filesize-percentage="42" title="'Why Correlation Usually ≠ Causation', Gwern 2014">existence of complex causal networks &amp; latent variables</a> (often factors) linking all measured traits, such as extensive heritability &amp; genetic correlations of human traits, leading to <a href="https://gwern.net/correlation" id="gwern-correlation" data-filesize-bytes="144066" data-filesize-percentage="63" title="'How Often Does Correlation=Causality?', Gwern 2014">extensive examples of correlation ≠ causation</a>.</p>
<p>The existence of both “everything is correlated” and the success of the “bet on sparsity” principle suggests that these causal networks may be best thought of as having hubs or latent variables: there are a relatively few variables such as ‘arousal’ or ‘IQ’ which play central roles, explaining much of <a href="https://en.wikipedia.org/wiki/Variance" id="_yzXH8kID" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Variance#bodyContent" title="Variance">variance</a>, followed by almost all other variables accounting for a little bit each with most of their influence mediated through the key variables.</p>
<p>The fact that these variables can be successfully modeled as substantively linear or additive further implies that interactions between variables will be typically rare or small or both (implying further that most such hits will be false positives, as interactions are already harder to detect than main effects, and more so if they are <em>a priori</em> unlikely or of small size). Even extremely large &amp; deeply phenotyped datasets may struggle to achieve impressive improvements over baselines using the core variables (eg. <a href="https://www.pnas.org/doi/10.1073/pnas.1915006117" id="_0OTythr1" data-link-icon="PNAS" data-link-icon-type="text,quad" data-link-icon-color="#1f75b9" title="Measuring the predictability of life outcomes with a scientific mass collaboration"><span><span title="et al">Salganik</span><span> et al </span><span>2020</span></span></a>).</p>
<p>To the extent that these key variables are unmodifiable, the many peripheral variables may also be unmodifiable (which may be related to the <a href="https://gwern.net/doc/sociology/1987-rossi" id="rossi-2012" data-filesize-bytes="84889" data-filesize-percentage="50" title="The Iron Law Of Evaluation And Other Metallic Rules">broad failure of social intervention programs</a>). Any intervention on those peripheral variables, being ‘downstream’, will tend to either be ‘hollow’ or fade out or have no effect at all on the true desired goals no matter how consistently they are correlated.</p>
<p>On a more contemporary note, these theoretical &amp; empirical considerations also throw doubt on concerns about ‘algorithmic bias’ or inferences drawing on ‘protected classes’: not drawing on them may not be desirable, possible, or even meaningful.</p></li>
<li><p><strong>Uncorrelated variables may be meaningless</strong>: given this empirical reality, any variable which is uncorrelated with the major variables is suspicious (somewhat like the <a href="https://gwern.net/doc/genetics/heritable/2015-polderman.pdf" id="polderman-et-al-2015-02" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="10395460" data-filesize-percentage="95" title="'Meta-analysis of the heritability of human traits based on 50 years of twin studies', Polderman et al 2015">pervasiveness of heritability</a> in human traits renders traits with zero heritability suspicious, suggesting issues like measuring at the wrong time). The lack of correlation suggests that the analysis is underpowered, something has gone wrong in the construction of the variable/dataset, or that the variable is part of a system whose causal network renders conventional analyses dangerously misleading.</p>
<p>For example, the dataset may be corrupted by a systematic bias such as <a href="https://en.wikipedia.org/wiki/Statistical_conclusion_validity#Restriction_of_range" id="_FX37JfKA" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Statistical_conclusion_validity#bodyContent" title="Statistical conclusion validity § Restriction of range">range restriction</a> or a selection effect such as <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox" id="_vvExW5j4" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Simpson%27s_paradox#bodyContent" title="Simpson's paradox">Simpson’s paradox</a>, which erases from the data a correlation that actually exists. Or the data may be random noise, due to software error or fraud or extremely high levels of measurement error (such as <a href="https://gwern.net/doc/sociology/survey/lizardman/index" id="_aSDAgurb" data-filesize-bytes="26046" data-filesize-percentage="29" title="'Lizardman Constant in Surveys', Gwern 2013">“lizardman constant”</a> respondents answering at random); or the variable may not be real in the first place. Another possibility is that the variable is causally connected, in feedback loops (especially common in economics or biology), to another variable, in which case the standard statistical machinery is misleading—the classic example is Milton Friedman’s thermostat, noting that a thermostat would be almost entirely uncorrelated with room temperature.</p></li>
</ol>
<p>This idea, as suggested by the many names, is not due to any single theoretical or empirical result or researcher, but has been made many times by many different researchers in many contexts, circulating as informal ‘folklore’. To bring some order to this, I have compiled excerpts of some relevant references in chronological order. (Additional citations are welcome.)</p>
</section>
<section id="gosset-student-1904">
<h2><a href="#gosset-student-1904" title="Link to section: § 'Gosset / Student1904'">Gosset / <span><span>Student</span><span>1904</span></span></a></h2>
<p>A version of this is attributed to <a href="https://en.wikipedia.org/wiki/William_Sealy_Gosset" id="_oxQwsTeg" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/William_Sealy_Gosset#bodyContent" title="William Sealy Gosset">William Sealy Gosset</a> (Student) in his <a href="https://gwern.net/doc/statistics/decision/1904-gosset.pdf" id="gosset-1904" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="608997" data-filesize-percentage="63" title="‘The Application Of The &quot;Law Of Error&quot; To The Work Of The Brewery’, Gosset 1904">1904 internal report</a><a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> by <a href="https://gwern.net/doc/statistics/decision/2008-ziliak.pdf" id="ziliak-2008" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="190871" data-filesize-percentage="32" title="Guinnessometrics: The Economic Foundation of 'Student's' _t_"><span><span>Ziliak</span><span>2008</span></span></a>:</p>
<blockquote>
<p>In early November <span>1904<sub><span title="1904 was 121 years ago.">121ya</span></sub></span>, Gosset discussed his first breakthrough in an internal report entitled “The Application of the ‘Law of Error’ to the Work of the Brewery” (<span><span>Gosset</span><span>1904</span></span>; <em>Laboratory Report</em><span>, Nov. 3, <span>1904<sub><span title="1904 was 121 years ago.">121ya</span></sub></span>, pg3). Gosset (p.&nbsp;3–16) wrote:</span></p>
<blockquote>
<p>Results are only valuable when the amount by which they probably differ from the truth is so small as to be insignificant for the purposes of the experiment. What the odds should be depends</p>
<ol type="1">
<li><p>On the degree of accuracy which the nature of the experiment allows, and</p></li>
<li><p>On the importance of the issues at stake.</p></li>
</ol>
</blockquote>
<p>Two features of Gosset’s report are especially worth highlighting here. First, he suggested that judgments about “significant” differences were not a purely probabilistic exercise: they depend on the “importance of the issues at stake.” Second, Gosset underscored a positive correlation in the <a href="https://en.wikipedia.org/wiki/Normal_distribution" id="_T4iJJGn_" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Normal_distribution#bodyContent" title="Normal distribution">normal distribution</a> curve between “the square root of the number of observations” and the level of statistical-significance. Other things equal, he wrote, “the greater the number of observations of which means are taken [the larger the sample size], the smaller the [probable or standard] error” (pg5). “And the curve which represents their frequency of error”, he illustrated, “becomes taller and narrower” (pg7).</p>
<p>Since its discovery in the early 19<sup>th</sup> century, tables of the normal probability curve had been created for large samples…The relation between sample size and “significance” was rarely explored. For example, while looking at biometric samples with up to thousands of observations, Karl Pearson declared that a result departing by more than 3 standard deviations is “definitely significant.”<sup>12</sup> Yet Gosset, a self-trained statistician, found that at such large samples, nearly everything is <em>statistically</em> “significant”—though not, in Gosset’s terms, economically or scientifically “important.” Regardless, Gosset didn’t have the luxury of large samples. One of his earliest experiments employed a sample size of 2 (Gosset, <span>1904<sub><span title="1904 was 121 years ago.">121ya</span></sub></span>, p.7) and in fact in <a href="https://gwern.net/doc/www/bayes.wustl.edu/62eb6502694eb3955a937b5683e29fc6252a45a4.pdf" id="_zGZU9cFr" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/bayes.wustl.edu/62eb6502694eb3955a937b5683e29fc6252a45a4.pdf" data-url-original="https://bayes.wustl.edu/Manual/Student.pdf" data-filesize-bytes="815807" data-filesize-percentage="45">“The Probable Error of a Mean”</a> he calculated a <em>t</em> statistic for <em>n</em><span> = 2 (Student, <span>1908<sub><span title="1908 was 117 years ago.">117ya</span></sub></span>b, p.&nbsp;23).</span></p>
<p>…the “degree of certainty to be aimed at”, Gosset wrote, depends on the opportunity cost of following a result as if true, added to the opportunity cost of conducting the experiment itself. Gosset never deviated from this central position.15 [See, for example, Student (<a href="https://gwern.net/doc/statistics/decision/1923-student.pdf" id="gosset-1923" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2735480" data-filesize-percentage="88" title="On Testing Varieties of Cereals">1923, p.&nbsp;271, paragraph one</a>: “The object of testing varieties of cereals is to find out which will pay the farmer best.”) and Student (<a href="https://archive.org/details/in.ernet.dli.2015.233812/page/n168" id="_r4jMMmxR" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/in.ernet.dli.2015.233812/page/n168?view=theater" title="Yield trials">1931c</a>, p.&nbsp;1342, paragraph one) reprinted in Student (<span>1942<sub><span title="1942 was 83 years ago.">83ya</span></sub></span>, p.&nbsp;90 and p.&nbsp;150).]</p>
</blockquote>
</section>
<section id="thorndike-1920">
<h2><a href="#thorndike-1920" title="Link to section: § 'Thorndike1920'"><span><span>Thorndike</span><span>1920</span></span></a></h2>
<p><a href="https://gwern.net/doc/iq/1920-thorndike-2.pdf" id="thorndike-1920b" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="10033577" data-filesize-percentage="95" title="'Intelligence and Its Uses', Thorndike 1920b">“Intelligence and Its Uses”</a>, Edward L. <span><span>Thorndike</span><span>1920</span></span> (<a href="https://en.wikipedia.org/wiki/Harper%27s_Magazine" id="_Pfbos5Iu" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Harper%27s_Magazine#bodyContent" title="Harper's Magazine"><em>Harper’s Monthly</em></a>):</p>
<blockquote>
<p>…the significance of intelligence for success in a given activity of life is measured by the coefficient of correlation between them. Scientific investigations of these matters is just beginning; and it is a matter of great difficulty and expense to measure the intelligence of, say, a thousand clergymen, and then secure sufficient evidence to rate them accurately for their success as ministers of the Gospel. Consequently, one can report no final, perfectly authoritative results in this field. One can only organize reasonable estimates from the various partial investigations that have been made. Doing this, I find the following:</p>
<ul>
<li><p>Intelligence and success in the elementary schools, <em>r</em> = +0.80</p></li>
<li><p>Intelligence and success in high-school and colleges in the case of those who go, <em>r</em> = +0.60; but if all were forced to try to do this advanced work, the correlation would be +0.80 or more.</p></li>
<li><p>Intelligence and salary, <em>r</em> = +0.35.</p></li>
<li><p>Intelligence and success in athletic sports, <em>r</em> = +0.25</p></li>
<li><p>Intelligence and character, <em>r</em> = +0.40</p></li>
<li><p>Intelligence and popularity, <em>r</em> = +0.20</p></li>
</ul>
<p>Whatever be the eventual exact findings, two sound principles are illustrated by our provisional list. First, there is always some resemblance; intellect always counts. Second, the resemblance varies greatly; intellect counts much more in some lines than in others.</p>
<p>The first fact is in part a consequence of a still broader fact or principle—namely, that in human nature good traits go together. To him that hath a superior intellect is given also on the average a superior character; the quick boy is also in the long run more accurate; the able boy is also more industrious. There is no principle of compensation whereby a weak intellect is offset by a strong will, a poor memory by good judgment, or a lack of ambition by an attractive personality. Every pair of such supposed compensating qualities that have been investigated has been found really to show correspondence. Popular opinion has been misled by attending to striking individual cases which attracted attention partly because they were really exceptions to the rule. The rule is that desirable qualities are positively correlated. Intellect is good in and of itself, and also for what it implies about other traits.</p>
</blockquote>
</section>
<section id="berkson-1938">
<h2><a href="#berkson-1938" title="Link to section: § 'Berkson1938'"><span><span>Berkson</span><span>1938</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www.stats.org.uk/8a04b5f47c2d6c77a0baf79c2cc528f32d35f11e.pdf" id="_ZzMhya3F" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.stats.org.uk/8a04b5f47c2d6c77a0baf79c2cc528f32d35f11e.pdf" data-url-original="http://www.stats.org.uk/statistical-inference/Berkson1938.pdf" data-filesize-bytes="261405" data-filesize-percentage="19">“Some difficulties of interpretation encountered in the application of the chi-square test”</a>, <a href="https://en.wikipedia.org/wiki/Joseph_Berkson" id="_gLxQXR7d" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Joseph_Berkson#bodyContent" title="Joseph Berkson">Berkson</a> <span>1938<sub><span title="1938 was 87 years ago.">87ya</span></sub></span>:</p>
<blockquote>
<p>I believe that an observant statistician who has had any considerable experience with applying the chi-square test repeatedly will agree with my statement that, as a matter of observation, when the numbers in the data are quite large, the <em>P</em>’s tend to come out small. Having observed this, and on reflection, I make the following dogmatic statement, referring for illustration to the normal curve: “If the normal curve is fitted to a body of data representing any real observations whatever of quantities in the physical world, then if the number of observations is extremely large—for instance, on an order of 200,000—the chi-square <em>P</em> will be small beyond any usual limit of significance.”</p>
<p>This dogmatic statement is made on the basis of an extrapolation of the observation referred to and can also be defended as a prediction from <em>a priori</em> considerations. For we may assume that it is practically certain that any series of real observations does not actually follow a normal curve <em>with absolute exactitude</em> in all respects, and no matter how small the discrepancy between the normal curve and the true curve of observations, the chi-square <em>P</em> will be small if the sample has a sufficiently large number of observations in it.</p>
<p>If this be so, then we have something here that is apt to trouble the conscience of a reflective statistician using the chi-square test. For I suppose it would be agreed by statisticians that a large sample is always better than a small sample. If, then, we know in advance the <em>P</em> that will result from an application of a chi-square test to a large sample, there would seem to be no use in doing it on a smaller one. But since the result of the former test is known, it is no test at all.</p>
</blockquote>
</section>
<section id="thorndike-1939">
<h2><a href="#thorndike-1939" title="Link to section: § 'Thorndike1939'"><span><span>Thorndike</span><span>1939</span></span></a></h2>
<p><a href="https://gwern.net/doc/sociology/1939-thorndike-yourcity.pdf" id="_gq5r6RUr" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="14015086" data-filesize-percentage="96"><em>Your City</em></a>, <span><span>Thorndike</span><span>1939</span></span> (and the followup <a href="https://gwern.net/doc/sociology/1940-thorndike-144smallercities.pdf" id="_VHX7vokH" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="7693205" data-filesize-percentage="94"><em>144 Smaller Cities</em></a> providing tables for 144 cities) compiles various statistics about American cities such as infant mortality, spending on the arts, crime etc and finds extensive intercorrelations &amp; factors.</p>
<p>The general factor of socioeconomic status or ‘S-factor’ also applies across countries as well: economic growth is by far the largest influence on all measures of well-being, and attempts at computing international rankings of things like maternal founder on this fact, as they typically wind up simply reproducing GDP rank-orderings and being redundant. For example, <a href="https://gwern.net/doc/www/klenow.com/64bf9a5047655615c9424980fac2ed32eb00a789.pdf" id="jones-klenow-2016" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/klenow.com/64bf9a5047655615c9424980fac2ed32eb00a789.pdf" data-url-original="http://klenow.com/Jones_Klenow.pdf" data-filesize-bytes="199811" data-filesize-percentage="15" title="Beyond GDP? Welfare across Countries and Time"><span><span>Jones &amp; Klenow</span><span>2016</span></span></a> compute an international wellbeing metric using “life expectancy, the ratio of consumption to income, annual hours worked per capita, the standard deviation of log consumption, and the standard deviation of annual hours worked” to incorporate factors like inequality, but this still winds up just being equivalent to GDP (<em>r</em> = 0.98). Or <a href="https://gwern.net/doc/www/mpra.ub.uni-muenchen.de/d1ff1393c3161ebe2b1239b226ce11af1f098543.pdf" id="dill-gebhart-2016" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/mpra.ub.uni-muenchen.de/d1ff1393c3161ebe2b1239b226ce11af1f098543.pdf" data-url-original="https://mpra.ub.uni-muenchen.de/74268/1/MPRA_paper_74268.pdf" data-filesize-bytes="202133" data-filesize-percentage="15" title="Redundancy, Unilateralism and Bias beyond GDP -- results of a Global Index Benchmark"><span><span>Gill &amp; Gebhart</span><span>2016</span></span></a>, who note that of 9 international indices they consider, all correlate positively with per capita GDP, &amp; 6 have <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" id="_Iosl62Lz" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#bodyContent" title="Kendall rank correlation coefficient">rank-correlations</a> τ &gt; 0.5.</p>
</section>
<section id="good-1950">
<h2><a href="#good-1950" title="Link to section: § 'Good1950'"><span><span>Good</span><span>1950</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/bayes/1950-good-probabilityandtheweighingofevidence.pdf#page=96" id="good-1950-page-96" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2376912" data-filesize-percentage="86" title="‘Probability and the Weighing of Evidence § pg96’, Good 1950 (page 96)"><em>Probability and the Weighing of Evidence</em></a>, <a href="https://en.wikipedia.org/wiki/I._J._Good" id="_WWZm3Ipg" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/I._J._Good#bodyContent" title="I. J. Good">I. J. Good</a>:</p>
<blockquote>
<p>The general question of significance tests was raised in 7.3 and a simple example will now be considered. Suppose that a die is thrown <em>n</em> times and that it shows an <em>r</em>-face on <em>m<sub>r</sub></em> occasions (<em>r</em> = 1, 2, …, 6). The question is whether the die is loaded. The answer depends on the meaning of “loaded”. From one point of view, it is unnecessary to look at the statistics since it is obvious that no die could be absolutely symmetrical. [It would be no contradiction of 4.3 (2) to say that the hypothesis that the die is absolutely symmetrical is almost impossible. In fact, this hypothesis is an idealised proposition rather than an empirical one.] It is possible that a similar remark applies to all experiments—even to the ESP experiment, since there may be no way of designing it so that the probabilities are <em>exactly</em> equal to <span>1⁄2</span>.</p>
</blockquote>
</section>
<section id="hodges-lehmann-1954">
<h2><a href="#hodges-lehmann-1954" title="Link to section: § 'Hodges &amp; Lehmann1954'"><span><span>Hodges &amp; Lehmann</span><span>1954</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/decision/1954-hodges.pdf" id="_3uOo3wa-" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="947907" data-filesize-percentage="73">“Testing the approximate validity of statistical hypotheses”</a>, <span><span>Hodges &amp; Lehmann</span><span>1954</span></span>:</p>
<blockquote>
<p>When testing statistical hypotheses, we usually do not wish to take the action of rejection unless the hypothesis being tested is false to an extent sufficient to matter. For example, we may formulate the hypothesis that a population is normally distributed, but we realize that no natural population is ever exactly normal. We would want to reject normality only if the departure of the actual distribution from the normal form were great enough to be material for our investigation. Again, when we formulate the hypothesis that the sex ratio is the same in two populations, we do not really believe that it could be exactly the same, and would only wish to reject equality if they are sufficiently different. Further examples of the phenomenon will occur to the reader.</p>
</blockquote>
</section>
<section id="savage-1954">
<h2><a href="#savage-1954" title="Link to section: § 'Savage1954'"><span><span>Savage</span><span>1954</span></span></a></h2>
<p><a href="https://archive.org/details/in.ernet.dli.2015.223806/page/n267" id="_w-S3_i7-" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/in.ernet.dli.2015.223806/page/n267?view=theater" title="The Foundations Of Statistics"><em>The Foundations of Statistics</em> 1<sup>st</sup> edition</a>, <a href="https://en.wikipedia.org/wiki/Leonard_Jimmie_Savage" id="_wPytl7Vw" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Leonard_Jimmie_Savage#bodyContent" title="Leonard Jimmie Savage">Leonard Jimmie</a> <span><span>Savage</span><span>1954</span></span><a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>, pg252–255:</p>
<blockquote>
<p>The development of the theory of testing has been much influenced by the special problem of simple dichotomy, that is, testing problems in which <em>H</em><sub>0</sub> and <em>H</em><sub>1</sub> have exactly one element each. Simple dichotomy is susceptible of neat and full analysis (as in Exercise 7.5.2 and in §14.4), likelihood-ratio tests here being the only admissible tests; and simple dichotomy often gives insight into more complicated problems, though the point is not explicitly illustrated in this book.</p>
<p>Coin and ball examples of simple dichotomy are easy to construct, but instances seem rare in real life. The astronomical observations made to distinguish between the Newtonian and Einsteinian hypotheses are a good, but not perfect, example, and I suppose that research in Mendelian genetics sometimes leads to others. There is, however, a tradition of applying the concept of simple dichotomy to some situations to which it is, to say the best, only crudely adapted. Consider, for example, the decision problem of a person who must buy, <strong>f</strong><sub>0</sub>, or refuse to buy, <strong>f</strong><sub>1</sub>, a lot of manufactured articles on the basis of an observation <em>x</em>. Suppose that <em>i</em> is the difference between the value of the lot to the person and the price at which the lot is offered for sale, and that <em>P(x | i)</em> is known to the person. Clearly, <em>H</em><sub>0</sub>, <em>H</em><sub>1</sub>, and <em>N</em> are sets characterized respectively by <em>i</em> &gt; 0, <em>i</em> &lt; 0, <em>i</em> = 0. This analysis of this, and similar, problems has recently been explored in terms of the minimax rule, for example by Sprowls [S16] and a little more fully by Rudy [R4], and by Allen [A3]. It seems to me natural and promising for many fields of application, but it is not a traditional analysis. On the contrary, much literature recommends, in effect, that the person pretend that only two values of <em>i</em>, <em>i</em><sub>0</sub> &gt; 0 and <em>i</em><sub>1</sub> &lt; 0, are possible and that the person then choose a test for the resulting simple dichotomy. The selection of the two values <em>i</em><sub>0</sub> and <em>i</em><sub>1</sub> is left to the person, though they are sometimes supposed to correspond to the person’s judgment of what constitutes good quality and poor quality—terms really quite without definition. The emphasis on simple dichotomy is tempered in some acceptance-sampling literature, where it is recommended that the person choose among available tests by some largely unspecified overall consideration of operating characteristics and costs, and that he facilitate his survey of the available tests by focusing on a pair of points that happen to interest him and considering the test whose operating characteristic passes (economically, in the case of sequential testing) through the pair of points. These traditional analyses are certainly inferior in the theoretical framework of the present discussion, and I think they will be found inferior in practice.</p>
<p>…I turn now to a different and, at least for me, delicate topic in connection with applications of the theory of testing. Much attention is given in the literature of statistics to what purport to be tests of hypotheses, in which the null hypothesis is such that it would not really be accepted by anyone. The following 3 propositions, though playful in content, are typical in form of these <em>extreme</em> null hypotheses, as I shall call them for the moment.</p>
<ul>
<li><p>A. The mean noise output of the cereal Krakl is a linear function of the atmospheric pressure, in the range 900–1,100 millibars.</p></li>
<li><p>B. The basal metabolic consumption of sperm whales is normally distributed [Wll].</p></li>
<li><p>C. New York taxi drivers of Irish, Jewish, and Scandinavian extraction are equally proficient in abusive language.</p></li>
</ul>
<p>Literally to test such hypotheses as these is preposterous. If, for example, the loss associated with <strong>f</strong><sub>1</sub> is zero, except in case Hypothesis A is exactly satisfied, what possible experience with Krakl could dissuade you from adopting <strong>f</strong><sub>1</sub>?</p>
<p>The unacceptability of extreme null hypotheses is perfectly well known; it is closely related to the often heard maxim that science disproves, but never proves, hypotheses. The role of extreme hypotheses in science and other statistical activities seems to be important but obscure. In particular, though I, like everyone who practices statistics, have often “tested” extreme hypotheses, I cannot give a very satisfactory analysis of the process, nor say clearly how it is related to testing as defined in this chapter and other theoretical discussions. None the less, it seems worth while to explore the subject tentatively; I will do so largely in terms of two examples.</p>
<p>Consider first the problem of a cereal dynamicist who must estimate the noise output of Krakl at each of 10 atmospheric pressures between 900 and 1,100 millibars. It may well be that he can properly regard the problem as that of estimating the 10 parameters in question, in which case there is no question of testing. But suppose, for example, that one or both of the following considerations apply. First, the engineer and his colleagues may attach considerable personal probability to the possibility that A is very nearly satisfied—very nearly, that is, in terms of the dispersion of his measurements. Second, the administrative, computational, and other incidental costs of using 10 individual estimates might be considerably greater than that of using a linear formula.</p>
<p>It might be impractical to deal with either of these considerations very rigorously. One rough attack is for the engineer first to examine the observed data <em>x</em> and then to proceed either as though he actually believed Hypothesis A or else in some other way. The other way might be to make the estimate according to the objectivistic formulae that would have been used had there been no complicating considerations, or it might take into account different but related complicating considerations not explicitly mentioned here, such as the advantage of using a quadratic approximation. It is artificial and inadequate to regard this decision between one class of basic acts or another as a test, but that is what in current practice we seem to do. The choice of which test to adopt in such a context is at least partly motivated by the vague idea that the test should readily accept, that is, result in acting as though the extreme null hypotheses were true, in the farfetched case that the null hypothesis is indeed true, and that the worse the approximation of the null hypotheses to the truth the less probable should be the acceptance.</p>
<p>The method just outlined is crude, to say the best. It is often modified in accordance with common sense, especially so far as the second consideration is concerned. Thus, if the measurements are sufficiently precise, no ordinary test might accept the null hypotheses, for the experiment will lead to a clear and sure idea of just what the departures from the null hypotheses actually are. But, if the engineer considers those departures unimportant for the context at hand, he will justifiably decide to neglect them.</p>
<p>Rejection of an extreme null hypothesis, in the sense of the foregoing discussion, typically gives rise to a complicated subsidiary decision problem. Some aspects of this situation have recently been explored, for example by Paulson [P3], [P4]; Duncan [Dll], [D12]; <a href="https://en.wikipedia.org/wiki/John_Tukey" id="_XocxmpNj" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/John_Tukey#bodyContent" title="John Tukey">Tukey</a> [T4], [<a href="https://gwern.net/doc/www/arxiv.org/6c6da30801f7053b5392a4582eaff2b665d5df34.pdf#google" id="raffel-et-al-2019" data-link-icon="alphabet" data-link-icon-type="svg" data-link-icon-color="#4285f4" data-href-mobile="https://arxiv.org/html/1910.10683?fallback=original#google" data-url-archive="/doc/www/arxiv.org/6c6da30801f7053b5392a4582eaff2b665d5df34.pdf#google" data-url-original="https://arxiv.org/abs/1910.10683#google" title="'T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer', Raffel et al 2019">T5</a>]; Schefte [S7]; and W. D. Fisher [F7].</p>
</blockquote>
</section>
<section id="fisher-1956">
<h2><a href="#fisher-1956" title="Link to section: § 'Fisher1956'"><span><span>Fisher</span><span>1956</span></span></a></h2>
<p><a href="https://archive.org/details/in.ernet.dli.2015.134555/page/n47" id="_RTbkUo-u" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/in.ernet.dli.2015.134555/page/n47?view=theater" title="Statistical Methods And Scientific Inference"><em>Statistical Methods and Scientific Inference</em></a>, <a href="https://en.wikipedia.org/wiki/Ronald_Fisher" id="_58PQ6Izd" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Ronald_Fisher#bodyContent" title="Ronald Fisher">R. A.</a> <span><span>Fisher</span><span>1956</span></span> (pg42):</p>
<blockquote>
<p>…However, the calculation [of error rates of ‘rejecting the null’] is absurdly academic, for in fact no scientific worker has a fixed level of significance at which from year to year and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas. Further, the calculation is based solely on a hypothesis, which, in the light of the evidence, is often not believed to be true at all, so that the actual probability of erroneous decision, supposing such a phrase to have any meaning, may be much less than the frequency specifying the level of significance.</p>
</blockquote>
</section>
<section id="wallis-roberts-1956">
<h2><a href="#wallis-roberts-1956" title="Link to section: § 'Wallis &amp; Roberts1956'"><span><span>Wallis &amp; Roberts</span><span>1956</span></span></a></h2>
<p><a href="https://archive.org/details/in.ernet.dli.2015.214331/page/n421" id="_vbtsA3rU" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/in.ernet.dli.2015.214331/page/n421?view=theater" title="Statistics A New Approach"><em>Statistics: A New Approach</em></a>, <span><span>Wallis &amp; Roberts</span><span>1956</span></span> (pg384–388):</p>
<blockquote>
<p>A difficulty with this viewpoint is that it is often known that the hypothesis tested could not be precisely true. No coin, for example, has a probability of precisely <span>1⁄2</span> of coming heads. The true probability will always differ from <span>1⁄2</span> even if it differs by only 0.000,000,000,1. Neither will any treatment cure <em>precisely</em> one-third of the patients in the population to which it might be applied, nor will the proportion of voters in a presidential election favoring one candidate be <em>precisely</em> <span>1⁄2</span>. Recognition of this leads to the notion of differences that are or are not of practical importance. “Practical importance” depends on the actions that are going to be taken on the basis of the data, and on the losses from taking certain actions when others would be more appropriate.</p>
<p>Thus, the focus is shifted to decisions: Would the same decision about practical action be appropriate if the coin produces heads 0.500,000,000,1 of the time as if it produces heads 0.5 of the time precisely? Does it matter whether the coin produces heads 0.5 of the time or 0.6 of the time, and if so does it matter enough to be worth the cost of the data needed to decide between the actions appropriate to these situations? Questions such as these carry us toward a comprehensive theory of rational action, in which the consequences of each possible action are weighed in the light of each possible state of reality. The value of a correct decision, or the costs of various degrees of error, are then balanced against the costs of reducing the risks of error by collecting further data. It is this viewpoint that underlies the definition of statistics given in the first sentence of this book. [“Statistics is a body of methods for making wise decisions in the face of uncertainty.”]</p>
</blockquote>
</section>
<section id="savage-1957">
<h2><a href="#savage-1957" title="Link to section: § 'Savage1957'"><span><span>Savage</span><span>1957</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/decision/1957-savage.pdf" id="_B5Sgn6TG" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1574465" data-filesize-percentage="81">“Nonparametric statistics”</a>, <a href="https://projecteuclid.org/journals/statistical-science/volume-14/issue-1/A-conversation-with-I-Richard-Savage-with-the-assistance-of/10.1214/ss/1009211808.full" id="sampson-1999" title="'A conversation with I. Richard Savage (with the assistance of Bruce Spencer)', Sampson 1999">I. Richard Savage</a><a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> <span>1957<sub><span title="1957 was 68 years ago.">68ya</span></sub></span>:</p>
<blockquote>
<p>Siegel does not explain why his interest is confined to tests of significance; to make measurements and then ignore their magnitudes would ordinarily be pointless. Exclusive reliance on tests of significance obscures the fact that statistical-significance does not imply substantive significance. The tests given by Siegel apply only to null hypotheses of “no difference.” In research, however, null hypotheses of the form “Population A has a median at least 5 units <em>larger</em> than the median of Population B” arise. Null hypotheses of no difference are usually known to be false before the data are collected [<a href="#fisher-1956" title="_Statistical Methods and Scientific Inference_, pg42 (R. A. Fisher 1956)">9</a>, p.&nbsp;42; <a href="#wallis-roberts-1956" title="_Statistics: A New Approach_, Wallis &amp; Roberts 1956">48</a>, pp.&nbsp;384–8]; when they are, their rejection or acceptance simply reflects the size of the sample and the power of the test, and is not a contribution to science.</p>
</blockquote>
</section>
<section id="nunnally-1960">
<h2><a href="#nunnally-1960" title="Link to section: § 'Nunnally1960'"><span><span>Nunnally</span><span>1960</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1960-nunnally.pdf" id="_Ad3FDxg3" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="519361" data-filesize-percentage="59">“The place of statistics in psychology”</a>, <span><span>Nunnally</span><span>1960</span></span>:</p>
<blockquote>
<p>The most misused and misconceived hypothesis-testing model employed in psychology is referred to as the “null-hypothesis” model. Stating it crudely, one null hypothesis would be that two treatments do not produce different mean effects in the long run. Using the obtained means and sample estimates of”population” variances, probability statements can be made about the acceptance or rejection of the null hypothesis. Similar null hypotheses are applied to correlations, complex experimental designs, factor-analytic results, and most all experimental results.</p>
<p>Although from a mathematical point of view the null-hypothesis models are internally neat, they share a crippling flaw: in the real world the null hypothesis is almost never true, and it is usually nonsensical to perform an experiment with the <em>sole</em> aim of rejecting the null hypothesis. This is a personal point of view, and it cannot be proved directly. However, it is supported both by common sense and by practical experience. The common-sense argument is that different psychological treatments will almost always (in the long run) produce differences in mean effects, even though the differences may be very small. Also, just as nature abhors a vacuum, it probably abhors zero correlations between variables.</p>
<p>…Experience shows that when large numbers of subjects are used in studies, nearly all comparisons of means are “significantly” different and all correlations are “significantly” different from zero. The author once had occasion to use 700 subjects in a study of public opinion. After a <a href="https://en.wikipedia.org/wiki/Factor_analysis" id="_dUO1Hqah" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Factor_analysis#bodyContent" title="Factor analysis">factor analysis</a> of the results, the factors were correlated with individual-difference variables such as amount of education, age, income, sex, and others. In looking at the results I was happy to find so many “significant” correlations (under the null-hypothesis model)-indeed, nearly all correlations were significant, including ones that made little sense. Of course, with an <em>N</em> of 700 correlations as large as 0.08 are “beyond the 0.05 level.” Many of the “significant” correlations were of no theoretical or practical importance.</p>
<p>The point of view taken here is that if the null hypothesis is not rejected, it usually is because the <em>N</em> is too small. If enough data is gathered, the hypothesis will generally be rejected. If rejection of the null hypothesis were the real intention in psychological experiments, there usually would be no need to gather data.</p>
<p>…Statisticians are not to blame for the misconceptions in psychology about the use of statistical methods. They have warned us about the use of the hypothesis-testing models and the related concepts. In particular they have criticized the null-hypothesis model and have recommended alternative procedures similar to those recommended here (See <a href="#savage-1957">Savage, 1957</a>; <a href="https://gwern.net/doc/statistics/decision/1954-tukey.pdf" id="tukey-1954" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1207818" data-filesize-percentage="77" title="Unsolved Problems of Experimental Statistics">Tukey, 1954</a>; and <a href="https://gwern.net/doc/statistics/causality/1951-yates.pdf" id="yates-1951" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="950352" data-filesize-percentage="73" title="The Influence of Statistical Methods for Research Workers on the Development of the Science of Statistics">Yates, 1951</a>).</p>
</blockquote>
</section>
<section id="smith-1960">
<h2><a href="#smith-1960" title="Link to section: § 'Smith1960'"><span><span>Smith</span><span>1960</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1960-smith.pdf" id="_NH3lG8Zr" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="506803" data-filesize-percentage="59">“Review of N. T. J. Bailey, <em>Statistical methods in biology</em>”</a>, <span><span>Smith</span><span>1960</span></span>:</p>
<blockquote>
<p>However, it is interesting to look at this book from another angle. Here we have set before us with great clarity a panorama of modern statistical methods, as used in biology, medicine, physical science, social and mental science, and industry. How far does this show that these methods fulfil their aims of analysing the data reliably, and how many gaps are there still in our knowledge?…One feature which can puzzle an outsider, and which requires much more justification than is usually given, is the setting up of unplausible null hypotheses. For example, a statistician may set out a test to see whether two drugs have exactly the same effect, or whether a regression line is exactly straight. These hypotheses can scarcely be taken literally, but a statistician may say, quite reasonably, that he wishes to test whether there is an appreciable difference between the effects of the two drugs, or an appreciable curvature in the regression line. But this raises at once the question: how large is ‘appreciable’? Or in other words, are we not really concerned with some kind of estimation, rather than significance?</p>
</blockquote>
</section>
<section id="edwards-1963">
<h2><a href="#edwards-1963" title="Link to section: § 'Edwards1963'"><span><span>Edwards</span><span>1963</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/53877bf2fb57846c8a024744f8205dd53220fb9d.pdf" id="_XTGtSKns" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/53877bf2fb57846c8a024744f8205dd53220fb9d.pdf" data-url-original="https://pdfs.semanticscholar.org/f5ee/44c98c68fc1d9a6a7a10d3af3ba13fc9d058.pdf" data-filesize-bytes="2097666" data-filesize-percentage="64">“Bayesian statistical inference for psychological research”</a>, <span><span title="et al">Edwards</span><span> et al </span><span>1963</span></span>:</p>
<blockquote>
<p>The most popular notion of a test is, roughly, a tentative decision between two hypotheses on the basis of data, and this is the notion that will dominate the present treatment of tests. Some qualification is needed if only because, in typical applications, one of the hypotheses—the null hypothesis—is known by all concerned to be false from the outset (<a href="#berkson-1938" title="Some difficulties of interpretation encountered in the application of the chi-square test"><span><span>Berkson</span><span>1938</span></span></a>; <a href="#hodges-lehmann-1954" title="Testing the approximate validity of statistical hypotheses"><span><span>Hodges &amp; Lehmann</span><span>1954</span></span></a>; <a href="https://gwern.net/doc/statistics/decision/1959-lehmann-testingstatisticalhypotheses.pdf" id="lehmann-1959" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="6805671" data-filesize-percentage="93" title="_Testing Statistical Hypotheses_"><span><span>Lehmann</span><span>1959</span></span></a><a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a>; <a href="#savage-1957">I. R. <span><span>Savage</span><span>1957</span></span></a>; L. <a href="#savage-1954">J. <span><span>Savage</span><span>1954</span></span>, p.&nbsp;254</a>); some ways of resolving the seeming absurdity will later be pointed out, and at least one of them will be important for us here…Classical procedures sometimes test null hypotheses that no one would believe for a moment, no matter what the data; our list of situations that might stimulate hypothesis tests earlier in the section included several examples. Testing an unbelievable null hypothesis amounts, in practice, to assigning an unreasonably large prior probability to a very small region of possible values of the true parameter. In such cases, the more the procedure is against the null hypothesis, the better. The frequent reluctance of empirical scientists to accept null hypotheses which their data do not classically reject suggests their appropriate skepticism about the original plausibility of these null hypotheses.</p>
</blockquote>
</section>
<section id="bakan-1966">
<h2><a href="#bakan-1966" title="Link to section: § 'Bakan1966'"><span><span>Bakan</span><span>1966</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/stats.org.uk/9f378d39eb9e41287e3e353f8ec7e47ece9e0486.pdf" id="_MEcbLpo0" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/stats.org.uk/9f378d39eb9e41287e3e353f8ec7e47ece9e0486.pdf" data-url-original="http://stats.org.uk/statistical-inference/Bakan1966.pdf" data-filesize-bytes="873397" data-filesize-percentage="46">“The test of significance in psychological research”</a>, <span><span>Bakan</span><span>1966</span></span>:</p>
<blockquote>
<p>Let us consider some of the difficulties associated with the null hypothesis.</p>
<ol type="1">
<li><p>The <em>a priori</em> reasons for believing that the null hypothesis is generally false anyway. One of the common experiences of research workers is the very high frequency with which significant results are obtained with large samples. Some years ago, the author had occasion to run a number of tests of significance on a battery of tests collected on about 60,000 subjects from all over the United States. Every test came out significant. Dividing the cards by such arbitrary criteria as east versus west of the Mississippi River, Maine versus the rest of the country, North versus South, etc., all produced significant differences in means. In some instances, the differences in the sample means were quite small, but nonetheless, the <em>p</em> values were all very low. <a href="#nunnally-1960"><span><span>Nunnally</span><span>1960</span></span></a> has reported a similar experience involving correlation coefficients on 700 subjects. <a href="#berkson-1938" title="Some difficulties of interpretation encountered in the application of the chi-square test">Joseph <span><span>Berkson</span><span>1938</span></span></a> made the observation almost 30 years in connection with chi-square:</p></li>
</ol>
<blockquote>
<p>I believe that an observant statistician who has had any considerable experience with applying the chi-square test repeatedly will agree with my statement that, as a matter of observation, when the numbers in the data are quite large, the P’s tend to come out small. Having observed this, and on reflection, I make the following dogmatic statement, referring for illustration to the normal curve: “If the normal curve is fitted to a body of data representing any real observations whatever of quantities in the physical world, then if the number of observations is extremely large—for instance, on an order of 200,000—the chi-square <em>p</em> will be small beyond any usual limit of significance.”</p>
<p>This dogmatic statement is made on the basis of an extrapolation of the observation referred to and can also be defended as a prediction from <em>a priori</em> considerations. For we may assume that it is practically certain that any series of real observations does not actually follow a normal curve <em>with absolute exactitude</em> in all respects, and no matter how small the discrepancy between the normal curve and the true curve of observations, the chi-square <em>P</em> will be small if the sample has a sufficiently large number of observations in it.</p>
<p>If this be so, then we have something here that is apt to trouble the conscience of a reflective statistician using the chi-square test. For I suppose it would be agreed by statisticians that a large sample is always better than a small sample. If, then, we know in advance the <em>P</em> that will result from an application of a chi-square test to a large sample, there would seem to be no use in doing it on a smaller one. But since the result of the former test is known, it is no test at all [pp.&nbsp;526–527].</p>
</blockquote>
<p>As one group of authors has put it, “in typical applications . . . the null hypothesis . . . is known by all concerned to be false from the outset [<a href="#edwards-1963"><span><span title="et al">Edwards</span><span> et al </span><span>1963</span></span></a>, p.&nbsp;214].” The fact of the matter is that <em>there is really no good reason to expect the null hypothesis to be true in any population.</em> Why should the mean, say, of all scores east of the Mississippi be <em>identical</em> to all scores west of the Mississippi? Why should any correlation coefficient be <em>exactly</em> 0.00 in the population? Why should we expect the ratio of males to females be <em>exactly</em> 50:50 in any population? Or why should different drugs have <em>exactly</em> the same effect on any population parameter (<a href="#smith-1960"><span><span>Smith</span><span>1960</span></span></a>)? <em>A glance at any set of statistics on total populations will quickly confirm the rarity of the null hypothesis in nature.</em></p>
<p>…Should there be any deviation from the null hypothesis in the population, <em>no matter how small</em>—and we have little doubt but that such a deviation usually exists—a sufficiently large number of observations will lead to the rejection of the null hypothesis. As <span><span>Nunnally</span><span>1960</span></span> put it,</p>
<blockquote>
<p>if the null hypothesis is not rejected, it is usually because the <em>N</em> is too small. If enough data are gathered, the hypothesis will generally be rejected. If rejection of the null hypothesis were the real intention in psychological experiments, there usually would be no need to gather data [p.&nbsp;643].</p>
</blockquote>
</blockquote>
</section>
<section id="meehl-1967">
<h2><a href="#meehl-1967" title="Link to section: § 'Meehl1967'"><span><span>Meehl</span><span>1967</span></span></a></h2>
<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.8918&amp;rep=rep1&amp;type=pdf" id="_K1sUG73X" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“Theory-testing in psychology and physics: A methodological paradox”</a>, <span><span>Meehl</span><span>1967</span></span></p>
<blockquote>
<p>One reason why the directional null hypothesis (<em>H</em><sub>02</sub>: μ<sub>g</sub> ≤ μ<sub>b</sub>) is the appropriate candidate for experimental refutation is the universal agreement that the old point-null hypothesis (<em>H</em><sub>0</sub>: μ<sub>g</sub> = μ<sub>b</sub>) is [quasi-] always false in biological and social science. Any dependent variable of interest, such as I.Q., or academic achievement, or perceptual speed, or emotional reactivity as measured by skin resistance, or whatever, depends mainly upon a finite number of “strong” variables characteristic of the organisms studied (embodying the accumulated results of their genetic makeup and their learning histories) plus the influences manipulated by the experimenter. Upon some complicated, unknown mathematical function of this finite list of “important” determiners is then superimposed an indefinitely large number of essentially “random” factors which contribute to the intragroup variation and therefore boost the error term of the statistical-significance test. In order for two groups which differ in some identified properties (such as social class, intelligence, diagnosis, racial or religious background) to differ not at all in the “output” variable of interest, it would be necessary that all determiners of the output variable have precisely the same average values in both groups, or else that their values should differ by a <em>pattern of amounts of difference</em> which precisely counterbalance one another to yield a net difference of zero. Now our general background knowledge in the social sciences, or, for that matter, even “common sense” considerations, makes such an exact equality of all determining variables, or a precise “accidental” counterbalancing of them, so extremely unlikely that no psychologist or statistician would assign more than a negligibly small probability to such a state of affairs.</p>
<p>…<em>Example</em>: Suppose we are studying a simple perceptual-verbal task like rate of color-naming in school children, and the independent variable is father’s religious preference. Superficial consideration might suggest that these two variables would not be related, but a little thought leads one to conclude that they will almost certainly be related by <em>some</em> amount, however small. Consider, for instance, that a child’s reaction to any sort of school-context task will be to some extent dependent upon his social class, since the desire to please academic personnel and the desire to achieve at a performance (just because it is a <em>task</em>, regardless of its intrinsic interest) are both related to the kinds of sub-cultural and personality traits in the parents that lead to upward mobility, economic success, the gaining of further education, and the like. Again, since there is known to be a sex difference in color naming, it is likely that fathers who have entered occupations more attractive to “feminine” males will (on the average) provide a somewhat more feminine father figure for identification on the part of their male offspring, and that a more refined color vocabulary, making closer discriminations between similar hues, will be characteristic of the ordinary language of such a household. Further, it is known that there is a correlation between a child’s general intelligence and its father’s occupation, and of course there will be <em>some</em> relation, even though it may be small, between a child’s general intelligence and his color vocabulary, arising from the fact that <em>vocabulary in general</em> is heavily saturated with the general intelligence factor. Since religious preference is a correlate of social class, all of these social class factors, as well as the intelligence variable, would tend to influence color-naming performance. Or consider a more extreme and faint kind of relationship. It is quite conceivable that a child who belongs to a more liturgical religious denomination would be somewhat more color-oriented than a child for whom bright colors were not associated with the religious life. Everyone familiar with psychological research knows that numerous “puzzling, unexpected” correlations pop up all the time, and that it requires only a moderate amount of motivation-plus-ingenuity to construct very plausible alternative theoretical explanations for them.</p>
<p>…These armchair considerations are borne out by the finding that in psychological and sociological investigations involving very large numbers of subjects, it is regularly found that almost all correlations or differences between means are statistically-significant. See, for example, the papers by <a href="#bakan-1966" title="The test of significance in psychological research"><span><span>Bakan</span><span>1966</span></span></a> and <a href="#nunnally-1960"><span><span>Nunnally</span><span>1960</span></span></a>. Data currently being analyzed by Dr.&nbsp;David Lykken and myself<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a>, derived from a huge sample of over 55,000 Minnesota high school seniors, reveal statistically-significant relationships in 91% of pairwise associations among a congeries of 45 miscellaneous variables such as sex, birth order, religious preference, number of siblings, vocational choice, club membership, college choice, mother’s education, dancing, interest in woodworking, liking for school, and the like. The 9% of non-statistically-significant associations are heavily concentrated among a small minority of variables having dubious <a href="https://en.wikipedia.org/wiki/Reliability_(statistics)" id="_T1eYyggy" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Reliability_(statistics)#bodyContent" title="Reliability (statistics)">reliability</a>, or involving arbitrary groupings of non-homogeneous or nonmonotonic sub-categories. The majority of variables exhibited significant relationships <em>with all but 3 of the others</em>, often at a very high confidence level (<em>p</em> &lt; 10<sup>−6</sup>).</p>
<p>…Considering the fact that “everything in the brain is connected with everything else”, and that there exist several “general state-variables” (such as arousal, attention, anxiety, and the like) which are known to be at least <em>slightly</em> influenceable by practically any kind of stimulus input, it is highly unlikely that <em>any</em> psychologically discriminable stimulation which we apply to an experimental subject would exert literally <em>zero</em> effect upon any aspect of his performance. The psychological literature abounds with examples of small but detectable influences of this kind. Thus it is known that if a subject memorizes a list of nonsense syllables in the presence of a faint odor of peppermint, his recall will be facilitated by the presence of that odor. Or, again, we know that individuals solving intellectual problems in a “messy” room do not perform quite as well as individuals working in a neat, well-ordered surround. Again, cognitive processes undergo a detectable facilitation when the thinking subject is concurrently performing the irrelevant, noncognitive task of squeezing a hand dynamometer. It would require considerable ingenuity to concoct experimental manipulations, except the most minimal and trivial (such as a very slight modification in the word order of instructions given a subject) where one could have confidence that the manipulation would be utterly without effect upon the subject’s motivational level, attention, arousal, fear of failure, achievement drive, desire to please the experimenter, distraction, social fear, etc., etc. So that, for example, while there is no very “interesting” psychological theory that links hunger drive with color-naming ability, I myself would confidently predict a significant difference in color-naming ability between persons tested after a full meal and persons who had not eaten for 10 hours, provided the sample size were sufficiently large and the color-naming measurements sufficiently reliable, since one of the effects of the increased hunger drive is heightened “arousal”, and anything which heightens arousal would be expected to affect a perceptual-cognitive performance like color-naming. Suffice it to say that there are very good reasons for expecting at least <em>some</em> slight influence of almost any experimental manipulation which would differ sufficiently in its form and content from the manipulation imposed upon a control group to be included in an experiment in the first place. In what follows I shall therefore assume that the point-null hypothesis <em>H</em><sub>0</sub> is, in psychology, [quasi-] always false.</p>
</blockquote>
<p>See also <a href="#waller-2004"><span><span>Waller</span><span>2004</span></span></a>, and <span><span>Meehl’s</span><span>2003</span></span> CSS talk, <a href="https://meehl.umn.edu/files/aumeehl2003sigtests-trimmedmp3#.mp3" id="_2eyixAdk">“Critique of Null Hypothesis Significance Testing”</a> (MP3 audio; <a href="https://meehl.umn.edu/sites/g/files/pua1696/f/aumeehl2003ccshandout.pdf" id="_uz1_bxZb" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">slides</a>).</p>
</section>
<section id="lykken-1968">
<h2><a href="#lykken-1968" title="Link to section: § 'Lykken1968'"><span><span>Lykken</span><span>1968</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/citeseerx.ist.psu.edu/0eb3f8d79d2999fc884c23757f07bfa591917e79.pdf" id="_OWw-8sgE" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/citeseerx.ist.psu.edu/0eb3f8d79d2999fc884c23757f07bfa591917e79.pdf" data-url-original="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.5553&amp;rep=rep1&amp;type=pdf" data-filesize-bytes="490526" data-filesize-percentage="31">“Statistical-Significance in Psychological Research”</a>, <span><span>Lykken</span><span>1968</span></span>:</p>
<blockquote>
<p>Most theories in the areas of personality, clinical, and social psychology predict no more than the direction of a correlation, group difference, or treatment effect. Since the null hypothesis is never strictly true, such predictions have about a 50-50 chance of being confirmed by experiment when the theory in question is false, since the statistical-significance of the result is a function of the sample size.</p>
<p>…Most psychological experiments are of 3 kinds: (1) studies of the effect of some treatment on some output variables, which can be regarded as a special case of (2) studies of the difference between two or more groups of individuals with respect to some variable, which in turn are a special case of (3) the study of the relationship or correlation between two or more variables within some specified population. Using the bivariate correlation design as paradigmatic, then, one notes first that the strict null hypothesis must always be assumed to be false (this idea is not new and has recently been illuminated by <a href="#bakan-1966"><span><span>Bakan</span><span>1966</span></span></a>). Unless one of the variables is wholly unreliable so that the values obtained are strictly random, it would be foolish to suppose that the correlation between any two variables is identically equal to 0.0000 . . . (or that the effect of some treatment or the difference between two groups is exactly <em>zero</em>). The molar dependent variables employed in psychological research are extremely complicated in the sense that the measured value of such a variable tends to be affected by the interaction of a vast number of factors, both in the present situation and in the history of the subject organism. It is exceedingly unlikely that any two such variables will not share at least some of these factors and equally unlikely that their effects will exactly cancel one another out.</p>
<p>It might be argued that the more complex the variables the smaller their average correlation ought to be since a larger pool of common factors allows more chance for mutual cancellation of effects in obedience to the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" id="_c2MypS07" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Law_of_large_numbers#bodyContent" title="Law of large numbers">Law of Large Numbers</a><a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a>. However, one knows of a number of unusually potent and pervasive factors which operate to unbalance such convenient symmetries and to produce correlations large enough to rival the effects of whatever causal factors the experimenter may have had in mind. Thus, we know that (1) “good” psychological and physical variables tend to be positively correlated; (6) experimenters, without deliberate intention, can somehow subtly bias their findings in the expected direction (Rosenthal, <span>1963<sub><span title="1963 was 62 years ago.">62ya</span></sub></span>); (3) the effects of common method are often as strong as or stronger than those produced by the actual variables of interest (eg. in a large and careful study of the factorial structure of adjustment to stress among officer candidates, Holtzman &amp; Bitterman, <span>1956<sub><span title="1956 was 69 years ago.">69ya</span></sub></span>, found that their 101 original variables contained 5 main common factors representing, respectively, their rating scales, their perceptual-motor tests, the McKinney Reporting Test, their GSR variables, and the <a href="https://en.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory" id="_Tr4qHl-1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory#bodyContent" title="Minnesota Multiphasic Personality Inventory">MMPI</a>); (4) transitory state variables such as the subject’s anxiety level, fatigue, or his desire to please, may broadly affect all measures obtained in a single experimental session. This average shared variance of “unrelated” variables can be thought of as a kind of ambient noise level characteristic of the domain. It would be interesting to obtain empirical estimates of this quantity in our field to serve as a kind of <a href="https://en.wikipedia.org/wiki/Waterline" id="_5xRFcnIp" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Waterline#bodyContent" title="Waterline">Plimsoll mark</a> against which to compare obtained relationships predicted by some theory under test. If, as I think, it is not unreasonable to suppose that “unrelated” molar psychological variables share on the average about 4% to 5% of common variance, then the expected correlation between any such variables would be about 0.20 in absolute value and the expected difference between any two groups on some such variable would be nearly 0.5 standard deviation units. (Note that these estimates assume zero measurement error. One can better explain the near-zero correlations often observed in psychological research in terms of unreliability of measures than in terms of the assumption that the true scores are in fact unrelated.)</p>
</blockquote>
</section>
<section id="nichols-1968">
<h2><a href="#nichols-1968" title="Link to section: § 'Nichols1968'"><span><span>Nichols</span><span>1968</span></span></a></h2>
<p><a href="https://gwern.net/doc/iq/1968-nichols.pdf" id="nichols-1968" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1813883" data-filesize-percentage="83" title="'Heredity, Environment, and School Achievement', Nichols 1968">“Heredity, Environment, and School Achievement”</a>, <span><span>Nichols</span><span>1968</span></span>:</p>
<blockquote>
<p>There are 3 main factors or types of variables that seem likely to have an important influence on ability and school achievement. These are (1) the school factor or organized educational influences; (2) the family factor or all of the social influences of family life on a child; and (3) the genetic factor…the separation of the effects of the major types of influences has proved to be extraordinarily difficult, and all of the research so far has not resulted in a clear-cut conclusion.</p>
<p>…This messy situation is due primarily to the fact that in human society all good things tend to go together. The most intelligent parents—those with the best genetic potential—also tend to provide the most comfortable and intellectually stimulating home environments for their children, and also tend to send their children to the most affluent and well-equipped schools. Thus, the ubiquitous correlation between family socio-economic status and school achievement is ambiguous in meaning, and isolating the independent contribution of the factors involved is difficult. However, the strong emotionally motivated attitudes and vested interests in this area have also tended to inhibit the sort of dispassionate, objective evaluation of the available evidence that is necessary for the advance of science.</p>
</blockquote>
</section>
<section id="hays-1973">
<h2><a href="#hays-1973" title="Link to section: § 'Hays1973'"><span><span>Hays</span><span>1973</span></span></a></h2>
<p><a href="https://www.amazon.com/Statistics-Social-Sciences-W-Hays/dp/B000OEF3EK" id="_f5fmFVKL" data-link-icon="amazon" data-link-icon-type="svg" data-link-icon-color="#ffce53"><em>Statistics for the social sciences</em></a> (2nd edition), <span><span>Hays</span><span>1973</span></span>; <a href="https://gwern.net/doc/statistics/causality/1973-hays.pdf" id="_zzU6fTYC" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1810898" data-filesize-percentage="83">chapter 10</a>, page 413–417:</p>
<blockquote>
<p>10.19: Testmanship, or how big is a difference?</p>
<p>…As we saw in Chapter 4, the complete absence of a statistical relation, or no association, occurs only when the conditional distribution of the dependent variable is the same regardless of which treatment is administered. Thus if the independent variable is not associated at all with the dependent variable the population distributions must be identical over the treatments. If, on the other hand, the means of the different treatment populations are different, the conditional distributions themselves must be different and the independent and dependent variables must be associated. The rejection of the hypothesis of no difference between population means is tantamount to the assertion that the treatment given does have some statistical association with the dependent variable score.</p>
<p>…However, the occurrence of a significant result says nothing at all about the strength of the association between treatment and score. A significant result leads to the inference that some association exists, but in no sense does this mean that an important degree of association necessarily exists. Conversely, evidence of a strong statistical association can occur in data even when the results are not significant. The game of inferring the true degree of statistical association has a joker: this is the sample size. The time has come to define the notion of the strength of a statistical association more sharply, and to link this idea with that of the true difference between population means.</p>
<p>. When does it seem appropriate to say that a strong association exists between the experimental factor <em>X</em> and the dependent variable <em>Y</em>? Over all of the different possibilities for <em>X</em> there is a probability distribution of <em>Y</em> values, which is the marginal distribution of <em>Y</em> over (<em>x</em>,<em>y</em>) events. The existence of this distribution implies that we do not know exactly what the <em>Y</em> value for any observation will be; we are always uncertain about <em>Y</em> to some extent. However, given any particular <em>X</em>, there is also a conditional distribution of <em>Y</em>, and it may be that in this conditional distribution the highly probable values of <em>Y</em> tend to “shrink” within a much narrower range than in the marginal distribution. If so, we can say that the information about <em>X</em> tends to reduce uncertainty about <em>Y</em>. <strong>In general we will say that the strength of a statistical relation is reflected by the extent to which knowing <em>X</em> reduces uncertainty about <em>Y</em>.</strong> One of the best indicators of our uncertainty about the value of a variable is σ<sup>2</sup>, the variance of its distribution…This index reflects the predictive power afforded by a relationship: when <em>w</em><sup>2</sup> is zero, then <em>X</em> does not aid us at all in predicting the value of <em>Y</em>. On the other hand, when <em>w</em><sup>2</sup> is 1.00, this tells us that <em>X</em> lets us know <em>Y</em> exactly…About now you should be wondering what the index <em>w</em><sup>2</sup> has to do with the difference between population means.</p>
<p>…When the difference <em>u</em><sub>1</sub> - <em>u</em><sub>2</sub> is zero, then <em>w</em><sup>2</sup> must be zero. In the usual <em>t</em>-test for a difference, the hypothesis of no difference between means is equivalent to the hypothesis that <em>w</em><sup>2</sup> = 0. On the other hand, when there is any difference at all between population means, the value of <em>w</em><sup>2</sup> must be greater than 0. In short, a true difference is “big” in the sense of predictive power only if the square of that difference is large relative to <span><span><span><span aria-label="\sigma^2_Y"></span></span></span></span>. However, in significance tests such as <em>t</em>, we compare the difference we get with an estimate of σ<sub>diff</sub>. The standard error of the difference can be made almost as small as we choose if we are given a free choice of sample size. Unless sample size is specified, there is no <em>necessary</em> connection between significance and the true strength of association.</p>
<p>This points up the fallacy of evaluating the “goodness” of a result in terms of statistical-significance alone, without allowing for the sample size used. All significant results do not imply the same degree of true association between independent and dependent variables.</p>
<p>It is sad but true that researchers have been known to capitalize on this fact. There is a certain amount of “testmanship” involved in using inferential statistics. <em>Virtually any study can be made to show significant results if one uses enough subjects, regardless of how nonsensical the content may be.</em> There is surely nothing on earth that is completely independent of anything else. The strength of an association may approach zero, but it should seldom or never be exactly zero. If one applies a large enough sample of the study of any relation, trivial or meaningless as it may be, sooner or later he is almost certain to achieve a significant result. Such a result may be a valid finding, but only in the sense that one can say with assurance that some association is not exactly zero. The degree to which such a finding enhances our knowledge is debatable. If the criterion of strength of association is applied to such a result, it becomes obvious that little or nothing is actually contributed to our ability to predict one thing from another.</p>
<p>For example, suppose that two methods of teaching first grade children to read are being compared. A random sample of 1000 children are taught to read by method I, another sample of 1000 children by method II. The results of the instruction are evaluated by a test that provides a score, in whole units, for each child. Suppose that the results turned out as follows:</p>
<div>
<table>
<thead>
<tr>
<th><p>Method I</p></th>
<th><p>Method II</p></th>
</tr>
</thead>
<tbody>
<tr>
<td><p><em>M</em><sub>1</sub> = 147.21</p></td>
<td><p><em>M</em><sub>2</sub> = 147.64</p></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><p><em>N</em><sub>1</sub> = 1000</p></td>
<td><p><em>N</em><sub>2</sub> = 1000</p></td>
</tr>
</tbody>
</table>
</div>
<p>Then, the estimated standard error of the difference is about 0.145, and the <em>z</em> value is</p>

<p>This certainly permits rejection of the null hypothesis of no difference between the groups. However, does it really tell us very much about what to expect of an individual child’s score on the test, given the information that he was taught by method I or method II? If we look at the group of children taught by method II, and assume that the distribution of their scores is approximately normal, we find that about 45% of these children fall <em>below</em> the mean score for children in group I. Similarly, about 45% of children in group I fall above the mean score for group II. Although the difference between the two groups is significant, the two groups actually overlap a great deal in terms of their performances on the test. In this sense, the two groups are really not very different at all, even though the difference between the means is quite significant in a purely statistical sense.</p>
<p>Putting the matter in a slightly different way, we note that the grand mean of the two groups is 147.425. Thus, our best bet about the score of any child, not knowing the method of his training, is 147.425. If we guessed that any child drawn at random from the combined group should have a score above 147.425, we should be wrong about half the time. However, among the original groups, according to method I and method II, the proportions falling above and below this grand mean are approximately as follows:</p>
<div>
<table>
<thead>
<tr>
<th></th>
<th><p>Below 147.425</p></th>
<th><p>Above 147.425</p></th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Method I</p></td>
<td><p>0.51</p></td>
<td><p>0.49</p></td>
</tr>
<tr>
<td><p>Method II</p></td>
<td><p>0.49</p></td>
<td><p>0.51</p></td>
</tr>
</tbody>
</table>
</div>
<p>This implies that if we know a child is from group I, and we guess that this score is below the grand mean, then we will be wrong about 49% of the time. Similarly, if a child is from group II, and we guess his score to be above the grand mean, we will be wrong about 49% of the time. If we are not given the group to which the child belongs, ad we guess either above or below the grand mean, we will be wrong about 50% of the time. Knowing the group does reduce the probability of error in such a guess, but it does not reduce it very much. The method by which the child was trained simply doesn’t tell us a great deal about what the child’s score will be, even though the difference in mean scores is significant in the statistical sense.</p>
<p>This kind of testmanship flourishes best when people pay too much attention to the significance test and too little to the degree of statistical association the finding represents. This clutters up the literature with findings that are often not worth pursuing, and which serve only to obscure the really important predictive relations that occasionally appear. The serious scientist owes it to himself and his readers to ask not only, “Is there any association between <em>X</em> and <em>Y</em>?” but also, “How much does my finding suggest about the power to predict <em>Y</em> from <em>X</em>?” Much too much emphasis is paid to the former, at the expense of the latter, question.</p>
</blockquote>
</section>
<section id="oakes-1975">
<h2><a href="#oakes-1975" title="Link to section: § 'Oakes1975'"><span><span>Oakes</span><span>1975</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1975-oakes.pdf" id="oakes-1975" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="847436" data-filesize-percentage="71" title="'On the alleged falsity of the null hypothesis', Oakes 1975">“On the alleged falsity of the null hypothesis”</a>, <span><span>Oakes</span><span>1975</span></span>:</p>
<blockquote>
<p>Consideration is given to the contention by <a href="#bakan-1966">Bakan</a>, <a href="#meehl-1967">Meehl</a>, <a href="#nunnally-1960">Nunnally</a>, and others that the null hypothesis in behavioral research is generally false in nature and that the <em>N</em> is large enough, it will always be rejected. A distinction is made between self-selected-groups research designs and true experiments, and it is suggested that the null hypothesis probably is generally false in the case of research involving the former design, but is not in the case of research involving the latter. Reasons for the falsity of the null hypothesis in the one case but not in the other are suggested.</p>
<p>The U.S. <a href="https://en.wikipedia.org/wiki/Office_of_Economic_Opportunity" id="_eXhkNQ_h" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Office_of_Economic_Opportunity#bodyContent" title="Office of Economic Opportunity">Office of Economic Opportunity</a> has recently reported the results of research on performance contracting. With 23,000 <em>Ss</em>—13,000 experimental and 10,000 control—the null hypothesis was not rejected. The experimental <em>Ss</em>, who received special instruction in reading and mathematics for 2 hours per day during the <span title="The date range 1970–1971 lasted 1 year, ending 54 years ago.">1970–1971<sub><span title="1970 was 54 years ago.">54ya</span></sub></span> school year, did not differ statistically-significantly from the controls in achievement gains (American Institutes for <span><span>Research</span><span>1972</span></span>, pg 5). Such an inability to reject the null hypothesis might not be surprising to the typical classroom teacher or to most educational psychologists, but in view of the huge <em>N</em> involved, it should give pause to <a href="#bakan-1966"><span><span>Bakan</span><span>1966</span></span></a>, who contends that the null hypothesis is generally false in behavioral research, as well as to those writers such as <a href="#nunnally-1960"><span><span>Nunnally</span><span>1960</span></span></a> and <a href="#meehl-1967"><span><span>Meehl</span><span>1967</span></span></a>, who agree with that contention. They hold that if the <em>N</em> is large enough, the null is sure to be rejected in behavioral research. This paper will suggest that the Falsity contention does not hold in the case of experimental research—that the null hypothesis is not generally false in such research.</p>
<ul>
<li><p>American Institutes For Research. <span>1972<sub><span title="1972 was 53 years ago.">53ya</span></sub></span>. “OEO reports performance contracting a failure”. <em>Behavioral Sciences Newsletter for Research Planning</em>, 9, 4–5. [see also <a href="https://gwern.net/doc/sociology/1972-page.pdf" id="page-1972" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="443476" data-filesize-percentage="55" title="'How We <em>All</em> Failed In Performance Contracting', Page 1972">“How We <em>All</em> Failed In Performance Contracting”</a>, <span><span>Page</span><span>1972</span></span>]</p></li>
</ul>
</blockquote>
</section>
<section id="loehlin-nichols-1976">
<h2><a href="#loehlin-nichols-1976" title="Link to section: § 'Loehlin &amp; Nichols1976'"><span><span>Loehlin &amp; Nichols</span><span>1976</span></span></a></h2>
<p><span><span>Loehlin &amp; Nichols</span><span>1976</span></span>, <a href="https://gwern.net/doc/genetics/heritable/1976-loehlin-heredityenvironmentandpersonality.pdf" id="loehlin-nichols-1976-link" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="25148441" data-filesize-percentage="97" title="'<em>Heredity, Environment, &amp; Personality: A Study of 850 Sets of Twins</em>', Loehlin &amp; Nichols 1976"><em>Heredity, Environment and Personality: A Study of 850 Sets of Twins</em></a> (see also <a href="https://gwern.net/doc/genetics/heritable/1979-nichols-heredityandenvironment.pdf" id="_sa0-lqzd" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="3280662" data-filesize-percentage="89"><em>Heredity and Environment: Major Findings from Twin Studies of Ability, Personality, and Interests</em></a><span>, <span><span>Nichols</span><span>1976/1979</span></span>):</span></p>
<blockquote>
<p>This volume reports on a study of 850 pairs of twins who were tested to determine the influence of heredity and environment on individual differences in personality, ability, and interests. It presents the background, research design, and procedures of the study, a complete tabulation of the test results, and the authors’ extensive analysis of their findings. Based on one of the largest studies of twin behavior conducted in the twentieth century, the book challenges a number of traditional beliefs about genetic and environmental contributions to personality development.</p>
<p>The subjects were chosen from participants in the National Merit Scholarship Qualifying Test of <span>1962<sub><span title="1962 was 63 years ago.">63ya</span></sub></span> and were mailed a battery of personality and interest questionnaires. In addition, parents of the twins were sent questionnaires asking about the twins’ early experiences. A similar sample of nontwin students who had taken the merit exam provided a comparison group. The questions investigated included how twins are similar to or different from non-twins, how identical twins are similar to or different from fraternal twins, how the personalities and interests of twins reflect genetic factors, how the personalities and interests of twins reflect early environmental factors, and what implications these questions have for the general issue of how heredity and environment influence the development of psychological characteristics. In attempting to answer these questions, the authors shed light on the importance of both genes and environment and form the basis for different approaches in behavior genetic research.</p>
</blockquote>
<p>The book is largely a discussion of comprehensive <a href="https://en.wikipedia.org/wiki/Summary_statistics" id="_hUejtaYs" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Summary_statistics#bodyContent" title="Summary statistics">summary statistics</a> of twin correlations from an early large-scale twin study (canvassed via the National Merit Scholarship Qualifying Test, <span>1962<sub><span title="1962 was 63 years ago.">63ya</span></sub></span>). They attempted to compile a large-scale twin sample without the burden of a full-blown twin registry by an extensive mail survey of the <em>n</em><span> = <span>1507<sub><span title="1507 was 518 years ago.">518ya</span></sub></span> 11th-grade adolescent pairs of participants in the high school National Merit Scholarship Qualifying Test of <span>1962<sub><span title="1962 was 63 years ago.">63ya</span></sub></span> (total </span><em>n</em>~600,000) who indicated they were twins (as well as a control sample of non-twins), yielding 514 identical twin &amp; 336 (same-sex) fraternal twin pairs; they were questioned as follows:</p>
<blockquote>
<p>…to these [participants] were mailed a battery of personality and interest tests, including the California Psychological Inventory (CPI), the Holland Vocational Preference Inventory (VPI), an experimental Objective Behavior Inventory (OBI), an Adjective Check List (ACL), and a number of other, briefer self-rating scales, attitude measures, and other items. In addition, a parent was asked to fill out a questionnaire describing the early experiences and home environment of the twins. Other brief questionnaires were sent to teachers and friends, asking them to rate the twins on a number of personality traits; because these ratings were available for only part of our basic sample, they have not been analyzed in detail and will not be discussed further in this book. (The parent and twin questionnaires, except for the CPI, are reproduced in Appendix A.)</p>
</blockquote>
<p>Unusually, the book includes appendices reporting raw twin-pair correlations for all of the reported items, not a mere handful of selected analyses on full test-scales or subfactors. (Because of this, I was able to extract variables related to leisure time preferences &amp; activities for <a href="https://gwern.net/amuse#loehlin-nichols-1976-a-study-of-850-sets-of-twins" id="gwern-amuse--loehlin-nichols-1976-a-study-of-850-sets-of-twins" data-filesize-bytes="190311" data-filesize-percentage="69" title="'Amusing Ourselves to Death? § Loehlin &amp; Nichols 1976: <em>A Study of 850 Sets of Twins</em>', Gwern 2018">another analysis</a>.) One can see that even down to the item level, heritabilities tend to be non-zero and most variables are correlated within-individuals or with environments as well.</p>
</section>
<section id="meehl-1978">
<h2><a href="#meehl-1978" title="Link to section: § 'Meehl1978'"><span><span>Meehl</span><span>1978</span></span></a></h2>
<p><a href="https://gwern.net/doc/psychology/1978-meehl.pdf" id="meehl-1978" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="376480" data-filesize-percentage="51" title="'Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology', Meehl 1978">“Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology”</a>, <span><span>Meehl</span><span>1978</span></span>:</p>
<blockquote>
<p>Since the null hypothesis is quasi-always false, tables summarizing research in terms of patterns of “significant differences” are little more than complex, causally uninterpretable outcomes of <a href="https://en.wikipedia.org/wiki/Power_of_a_test" id="_4HjKZF87" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Power_of_a_test#bodyContent" title="Statistical power">statistical power</a> functions.</p>
<p>The kinds of theories and the kinds of theoretical risks to which we put them in soft psychology when we use significance testing as our method are <em>not</em> like testing Meehl’s theory of weather by seeing how well it forecasts the number of inches it will rain on certain days. Instead, they are depressingly close to testing the theory by seeing whether it rains in April at all, or rains several days in April, or rains in April more than in May. It happens mainly because, as I believe is generally recognized by statisticians today and by thoughtful social scientists, the null hypothesis, taken literally, is always false. I shall not attempt to document this here, because among sophisticated persons it is taken for granted. (See Morrison &amp; Henkel, <span>1970<sub><span title="1970 was 55 years ago.">55ya</span></sub></span> [<em>The Significance Test Controversy: A Reader</em>], especially the chapters by <a href="#bakan-1966">Bakan</a>, Hogben, <a href="#lykken-1968">Lykken</a>, <a href="#meehl-1967">Meehl</a>, and <a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/40448a28b9470a20f382cedee2b3fd9b6ccb1c41.pdf" id="rozeboom-1960" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/40448a28b9470a20f382cedee2b3fd9b6ccb1c41.pdf" data-url-original="https://pdfs.semanticscholar.org/b596/4787fc1abf739148d604abfbd2689e73e52f.pdf" data-filesize-bytes="653742" data-filesize-percentage="40" title="The fallacy of the null-hypothesis significance test">Rozeboom</a>.) A little reflection shows us why it has to be the case, since an output variable such as adult IQ, or academic achievement, or effectiveness at communication, or whatever, will always, in the social sciences, be a function of a sizable but finite number of factors. (The smallest contributions may be considered as essentially a random variance term.) In order for two groups (males and females, or whites and blacks, or manic depressives and <a href="https://en.wikipedia.org/wiki/Schizophrenia" id="_Gh5HccJm" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Schizophrenia#bodyContent" title="Schizophrenia">schizophrenics</a>, or Republicans and Democrats) to be <em>exactly</em> equal on such an output variable, we have to imagine that they are exactly equal <em>or</em> delicately counterbalanced on all of the contributors in the causal equation, which will never be the case.</p>
<p>Following the general line of reasoning (presented by myself and several others over the last decade), from the fact that the null hypothesis is always false in soft psychology, it follows that the probability of refuting it depends wholly on the sensitivity of the experiment—its logical design, the net (attenuated) construct validity of the measures, and, most importantly, the sample size, which determines where we are on the statistical power function. Putting it crudely, if you have enough cases and your measures are not totally unreliable, the null hypothesis will always be falsified, <em>regardless of the truth of the substantive theory</em>. Of course, it could be falsified in the wrong direction, which means that as the power improves, the probability of a corroborative result approaches one-half. However, if the theory has no verisimilitude—such that we can imagine, so to speak, picking our empirical results randomly out of a directional hat apart from any theory—the probability of refuting by getting a significant difference in the wrong direction also approaches one-half. Obviously, this is quite unlike the situation desired from either a Bayesian, a Popperian, or a commonsense scientific standpoint. As I have pointed out elsewhere (<a href="#meehl-1967">Meehl, 1967</a>/<span>1970<sub><span title="1970 was 55 years ago.">55ya</span></sub></span>b; but see criticism by <a href="#oakes-1975">Oakes, 1975</a>; <a href="https://gwern.net/doc/statistics/causality/1973-keuth.pdf" id="keuth-1973" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="437871" data-filesize-percentage="55" title="On prior probabilities of rejecting statistical hypotheses">Keuth, 1973</a>; and rebuttal by <a href="https://gwern.net/doc/statistics/causality/1975-swoyer.pdf" id="swoyer-monson-1975" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="346599" data-filesize-percentage="49" title="Theory confirmation in psychology">Swoyer &amp; Monson, 1975</a>), an improvement in instrumentation or other sources of experimental accuracy tends, in physics or astronomy or chemistry or genetics, to subject the theory to a greater risk of refutation <em>modus tollens</em>, whereas improved precision in null hypothesis testing usually decreases this risk. A successful significance test of a substantive theory in soft psychology provides a feeble corroboration of the theory because the procedure has subjected the theory to a feeble risk.</p>
<p>…I am not making some nit-picking statistician’s correction. I am saying that the whole business is so radically defective as to be scientifically almost pointless… I am making a philosophical complaint or, if you prefer, a complaint in the domain of scientific method. I suggest that when a reviewer tries to “make theoretical sense” out of such a table of favorable and adverse significance test results, what the reviewer is actually engaged in, willy-nilly or unwittingly, is meaningless substantive constructions on the properties of the statistical power function, and almost nothing else.</p>
<p>…You may say, “But, Meehl, R. A. Fisher was a genius, and we all know how valuable his stuff has been in agronomy. Why shouldn’t it work for soft psychology?” Well, I am not intimidated by Fisher’s genius, because my complaint is not in the field of mathematical statistics, and as regards inductive logic and philosophy of science, it is well-known that Sir Ronald permitted himself a great deal of dogmatism. I remember my amazement when the late <a href="https://en.wikipedia.org/wiki/Rudolf_Carnap" id="_R973FqQu" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Rudolf_Carnap#bodyContent" title="Rudolf Carnap">Rudolf Carnap</a> said to me, the first time I met him, “But, of course, on this subject Fisher is just mistaken: surely you must know that.” My statistician friends tell me that it is not clear just how useful the significance test has been in biological science either, but I set that aside as beyond my competence to discuss.</p>
</blockquote>
</section>
<section id="loftus-loftus-1982">
<h2><a href="#loftus-loftus-1982" title="Link to section: § 'Loftus &amp; Loftus1982'"><span><span>Loftus &amp; Loftus</span><span>1982</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1982-loftus-essenceofstatistics.pdf" id="loftus-loftus-1982" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="89687276" data-filesize-percentage="99" title="'Essence of Statistics (Second Edition)', Loftus &amp; Loftus 1982"><em>Essence of Statistics</em></a>, <span><span>Loftus &amp; Loftus</span><span>1982/1988</span></span><span> (2nd ed), pg515–516 (pg498–499 in the <span>1982<sub><span title="1982 was 43 years ago.">43ya</span></sub></span> printing):</span></p>
<blockquote>
<p><strong>Relative Importance Of These 3 Measures</strong>. It is a matter of some debate as to which of these 3 measures [σ<sup>2</sup>/<em>p</em>/R<sup>2</sup>] we should pay the most attention to in an experiment. It’s our opinion that finding a “significant effect” really provides very little information because it’s almost certainly true that <em>some</em> relationship (however small) exists between <em>any</em> two variables. And in general <em>finding</em> a significant effect simply means that enough observations have been collected in the experiment to make the statistical test of the experiment powerful enough to detect whatever effect there is. The smaller the effect, the more powerful the experiments needs to be of course, but no matter how small the effect, it’s always possible in principle to design an experiment sufficiently powerful to detect it. We saw a striking example of this principle in the office hours experiment. In this experiment there was a relationship between the two variables—and since there were so many subjects in the experiment (that is, since the test was so powerful), this relationship was revealed in the statistical analysis. But was it anything to write home about? Certainly not. In any sort of practical context the size of the effect, although nonzero, is so small it can almost be ignored.</p>
<p>It is our judgment that accounting for variance is really much more meaningful than testing for significance.</p>
</blockquote>
</section>
<section id="meehl-1990-1">
<h2><a href="#meehl-1990-1" title="Link to section: § 'Meehl1990 (1)'"><span><span>Meehl</span><span>1990</span></span> (1)</a></h2>
<p><a href="https://meehl.umn.edu/sites/g/files/pua1696/f/144whysummaries.pdf" id="_zj-iY4Ip" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“Why summaries of research on psychological theories are often uninterpretable”</a>, <span><span>Meehl</span><span>1990a</span></span> (also discussed in <span><span>Cohen’s</span><span>1994</span></span> paper <a href="https://gwern.net/doc/www/www.sjsu.edu/3936a51363629ca29bb8ee525ed25d2eda692757.pdf" id="cohen-1994" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.sjsu.edu/3936a51363629ca29bb8ee525ed25d2eda692757.pdf" data-url-original="https://www.sjsu.edu/faculty/gerstman/misc/Cohen1994.pdf" data-filesize-bytes="389513" data-filesize-percentage="26">“The Earth is Round (<em>p</em> &lt; 0.05)”</a>):</p>
<blockquote>
<p>Problem 6. <em>Crud factor</em>: In the social sciences and arguably in the biological sciences, “everything correlates to some extent with everything else.” This truism, which I have found no competent psychologist disputes given 5 minutes reflection, does not apply to pure experimental studies in which attributes that the subjects bring with them are not the subject of study (except in so far as they appear as a source of error and hence in the denominator of a significance test).<sup>6</sup> There is nothing mysterious about the fact that in psychology and sociology everything correlates with everything. Any measured trait or attribute is some function of a list of partly known and mostly unknown causal factors in the genes and life history of the individual, and both genetic and environmental factors are known from tons of empirical research to be themselves correlated. To take an extreme case, suppose we construe the null hypothesis literally (objecting that we mean by it “almost null” gets ahead of the story, and destroys the rigor of the Fisherian mathematics!) and ask whether we expect males and females in Minnesota to be precisely equal in some arbitrary trait that has individual differences, say, color naming. In the case of color naming we could think of some obvious differences right off, but even if we didn’t know about them, what is the causal situation? If we write a causal equation (which is not the same as a regression equation for pure predictive purposes but which, if we had it, would serve better than the latter) so that the score of an individual male is some function (presumably nonlinear if we knew enough about it but here supposed linear for simplicity) of a rather long set of causal variables of genetic and environmental type <em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, … <em>X</em><sub>m</sub>. These values are operated upon by regression coefficients <em>b</em><sub>1</sub>, <em>b</em><sub>2</sub>, …<em>b</em><sub>m</sub>.</p>
<p>…Now we write a similar equation for the class of females. Can anyone suppose that the beta coefficients for the two sexes will be exactly the same? Can anyone imagine that the mean values of all of the <em>X</em>s will be exactly the same for males and females, even if the culture were not still considerably sexist in child-rearing practices and the like? If the betas are not exactly the same for the two sexes, and the mean values of the <em>X</em>s are not exactly the same, what kind of Leibnitzian preestablished harmony would we have to imagine in order for the mean color-naming score to come out exactly equal between males and females? It boggles the mind; it simply would never happen. As Einstein said, “the Lord God is subtle, but He is not malicious.” We cannot imagine that nature is out to fool us by this kind of delicate balancing. Anybody familiar with large scale research data takes it as a matter of course that when the <em>N</em> gets big enough she will not be looking for the statistically-significant correlations but rather looking at their patterns, since almost all of them will be significant. In saying this, I am not going counter to what is stated by mathematical statisticians or psychologists with statistical expertise. For example, the standard psychologist’s textbook, the excellent treatment by Hays (<a href="#hays-1973">1973, page 415</a>), explicitly states that, taken literally, the null hypothesis is always false.</p>
<p>20 ago David Lykken and I conducted an exploratory study of the crud factor which we never published but I shall summarize it briefly here. (I offer it not as “empirical proof”—that <em>H</em><sub>0</sub> taken literally is quasi-always false hardly needs proof and is generally admitted—but as a punchy and somewhat amusing example of an insufficiently appreciated truth about soft correlational psychology.) In <span>1966<sub><span title="1966 was 59 years ago.">59ya</span></sub></span>, the University of Minnesota Student Counseling Bureau’s Statewide Testing Program administered a questionnaire to 57,000 high school seniors, the items dealing with family facts, attitudes toward school, vocational and educational plans, leisure time activities, school organizations, etc. We cross-tabulated a total of 15 (and then 45) variables including the following (the number of categories for each variable given in parentheses): father’s occupation (7), father’s education (9), mother’s education (9), number of siblings (10), birth order (only, oldest, youngest, neither), educational plans after high school (3), family attitudes towards college (3), do you like school (3), sex (2), college choice (7), occupational plan in 10 years (20), and religious preference (20). In addition, there were 22 “leisure time activities” such as “acting”, “model building”, “cooking”, etc., which could be treated either as a single 22-category variable or as 22 dichotomous variables. There were also 10 “high school organizations” such as “school subject clubs”, “farm youth groups”, “political clubs”, etc., which also could be treated either as a single ten-category variable or as 10 dichotomous variables. Considering the latter two variables as multichotomies gives a total of 15 variables producing 105 different cross-tabulations. All values of χ<sup>2</sup> for these 105 cross-tabulations were statistically-significant, and 101 (96%) of them were significant with a probability of less than 10<sup>−6</sup>.</p>
<p>…If “leisure activity” and “high school organizations” are considered as separate dichotomies, this gives a total of 45 variables and 990 different crosstabulations. Of these, 92% were statistically-significant and more than 78% were significant with a probability less than 10<sup>−6</sup>. Looked at in another way, the median number of significant relationships between a given variable and all the others was 41 out of a possible 44!</p>
<p>We also computed <a href="https://en.wikipedia.org/wiki/Medical_College_Admission_Test" id="_RKnM_1Nc" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Medical_College_Admission_Test#bodyContent" title="Medical College Admission Test">MCAT</a> scores by category for the following variables: number of siblings, birth order, sex, occupational plan, and religious preference. Highly significant deviations from chance allocation over categories were found for each of these variables. For example, the females score higher than the males; MCAT score steadily and markedly decreases with increasing numbers of siblings; eldest or only children are statistically-significantly brighter than youngest children; there are marked differences in MCAT scores between those who hope to become nurses and those who hope to become nurses aides, or between those planning to be farmers, engineers, teachers, or physicians; and there are substantial MCAT differences among the various religious groups. We also tabulated the 5 principal Protestant religious denominations (Baptist, Episcopal, Lutheran, Methodist, and Presbyterian) against all the other variables, finding highly significant relationships in most instances. For example, only children are nearly twice as likely to be Presbyterian than Baptist in Minnesota, more than half of the Episcopalians “usually like school” but only 45% of Lutherans do, 55% of Presbyterians feel that their grades reflect their abilities as compared to only 47% of Episcopalians, and Episcopalians are more likely to be male whereas Baptists are more likely to be female. 83% of Baptist children said that they enjoyed dancing as compared to 68% of Lutheran children. More than twice the proportion of Episcopalians plan to attend an out of state college than is true for Baptists, Lutherans, or Methodists. The proportion of Methodists who plan to become conservationists is nearly twice that for Baptists, whereas the proportion of Baptists who plan to become receptionists is nearly twice that for Episcopalians.</p>
<p>In addition, we tabulated the 4 principal Lutheran Synods (Missouri, ALC, LCA, and Wisconsin) against the other variables, again finding highly significant relationships in most cases. Thus, 5.9% of Wisconsin Synod children have no siblings as compared to only 3.4% of Missouri Synod children. 58% of ALC Lutherans are involved in playing a musical instrument or singing as compared to 67% of Missouri Synod Lutherans. 80% of Missouri Synod Lutherans belong to school or political clubs as compared to only 71% of LCA Lutherans. 49% of ALC Lutherans belong to debate, dramatics, or musical organizations in high school as compared to only 40% of Missouri Synod Lutherans. 36% of LCA Lutherans belong to organized non-school youth groups as compared to only 21% of Wisconsin Synod Lutherans. [Preceding text courtesy of D. T. Lykken.]</p>
<p>These relationships are not, I repeat, Type I errors. They are facts about the world, and with <em>N</em> = 57,000 they are pretty stable. Some are theoretically easy to explain, others more difficult, others completely baffling. The “easy” ones have multiple explanations, sometimes competing, usually not. Drawing theories from a pot and associating them whimsically with variable pairs would yield an impressive batch of <em>H</em><sub>0</sub>-refuting “confirmations.”</p>
<p>Another amusing example is the behavior of the items in the 550 items of the MMPI pool with respect to sex. Only 60 items appear on the Mf scale, about the same number that were put into the pool with the hope that they would discriminate femininity. It turned out that over half the items in the scale were not put in the pool for that purpose, and of those that were, a bare majority did the job. Scale derivation was based on item analysis of a small group of criterion cases of male homosexual invert syndrome, a significant difference on a rather small <em>N</em> of Dr.&nbsp;Starke Hathaway’s private patients being then conjoined with the requirement of discriminating between male normals and female normals. When the <em>N</em> becomes very large as in the data published by <a href="https://gwern.net/doc/statistics/causality/1973-wendell-anmmpisourcebook.pdf" id="_AWpWvtby" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="42307651" data-filesize-percentage="98">Swenson, Pearson, and Osborne (<span>1973<sub><span title="1973 was 52 years ago.">52ya</span></sub></span>; <em>An MMPI Source Book: Basic Item, Scale, And Pattern Data On 50,000 Medical Patients</em>. Minneapolis, MN: University of Minnesota Press.)</a>, approximately 25,000 of each sex tested at the Mayo Clinic over a period of years, it turns out that 507 of the 550 items discriminate the sexes. Thus in a heterogeneous item pool we find only 8% of items failing to show a significant difference on the sex dichotomy. The following are sex-discriminators, the male/female differences ranging from a few percentage points to over 30%:<sup>7</sup></p>
<ul>
<li><p>Sometimes when I am not feeling well I am cross.</p></li>
<li><p>I believe there is a Devil and a Hell in afterlife.</p></li>
<li><p>I think nearly anyone would tell a lie to keep out of trouble.</p></li>
<li><p>Most people make friends because friends are likely to be useful to them.</p></li>
<li><p>I like poetry.</p></li>
<li><p>I like to cook.</p></li>
<li><p>Policemen are usually honest.</p></li>
<li><p>I sometimes tease animals.</p></li>
<li><p>My hands and feet are usually warm enough.</p></li>
<li><p>I think Lincoln was greater than Washington.</p></li>
<li><p>I am certainly lacking in self-confidence.</p></li>
<li><p>Any man who is able and willing to work hard has a good chance of succeeding.</p></li>
</ul>
<p>I invite the reader to guess which direction scores “feminine.” Given this information, I find some items easy to “explain” by one obvious theory, others have competing plausible explanations, still others are baffling.</p>
<p>Note that we are not dealing here with some source of statistical error (the occurrence of random sampling fluctuations). That source of error is limited by the significance level we choose, just as the probability of Type II error is set by initial choice of the statistical power, based upon a pilot study or other antecedent data concerning an expected average difference. Since in social science everything correlates with everything to some extent, due to complex and obscure causal influences, in considering the crud factor we are talking about <em>real</em> differences, <em>real</em> correlations, <em>real</em> trends and patterns for which there is, of course, some true but complicated multivariate causal theory. I am not suggesting that these correlations are fundamentally unexplainable. They would be completely explained if we had the knowledge of Omniscient Jones, which we don’t. The point is that we are in the weak situation of corroborating our particular substantive theory by showing that <em>X</em> and <em>Y</em> are “related in a nonchance manner”, when our theory is too weak to make a numerical prediction or even (usually) to set up a range of admissible values that would be counted as corroborative.</p>
<p>…Some psychologists play down the influence of the ubiquitous crud factor, what <a href="#lykken-1968" title="Statistical-Significance in Psychological Research">David Lykken (<span>1968<sub><span title="1968 was 57 years ago.">57ya</span></sub></span>)</a> calls the “ambient correlational noise” in social science, by saying that we are not in danger of being misled by small differences that show up as significant in gigantic samples. How much that softens the blow of the crud factor’s influence depends upon the crud factor’s average size in a given research domain, about which neither I nor anybody else has accurate information. <em>But the notion that the correlation between arbitrarily paired trait variables will be, while not literally zero, of such minuscule size as to be of no importance, is surely wrong.</em> Everybody knows that there is a set of demographic factors, some understood and others quite mysterious, that correlate quite respectably with a variety of traits. (Socioeconomic status, SES, is the one usually considered, and frequently assumed to be only in the “input” causal role.) The clinical scales of the MMPI were developed by empirical keying against a set of disjunct nosological categories, some of which are phenomenologically and psychodynamically opposite to others. Yet the 45 pairwise correlations of these scales are almost always positive (scale Ma provides most of the negatives) and a representative size is in the neighborhood of 0.35 to 0.40. The same is true of the scores on the Strong Vocational Interest Blank, where I find an average absolute value correlation close to 0.40. The malignant influence of so-called “methods covariance” in psychological research that relies upon tasks or tests having certain kinds of behavioral similarities such as questionnaires or ink blots is commonplace and a regular source of concern to clinical and personality psychologists. For further discussion and examples of crud factor size, see <a href="#meehl-1990-2" title="Appraising and amending theories: the strategy of Lakatosian defense and two principles that warrant using it">Meehl (<span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>)</a>.</p>
<p>Now suppose we imagine a society of psychologists doing research in this soft area, and each investigator sets his experiments up in a whimsical, irrational manner as follows: First he picks a theory at random out of the theory pot. Then he picks a pair of variables randomly out of the observable variable pot. He then arbitrarily assigns a direction (you understand there is no intrinsic connection of content between the substantive theory and the variables, except once in a while there would be such by coincidence) and says that he is going to test the randomly chosen substantive theory by pretending that it predicts—although in fact it does not, having no intrinsic contentual relation—a positive correlation between randomly chosen observational variables <em>X</em> and <em>Y</em>. Now suppose that the crud factor operative in the broad domain were 0.30, that is, the average correlation between all of the variables pairwise in this domain is 0.30. This is not sampling error but the true correlation produced by some complex unknown network of genetic and environmental factors. Suppose he divides a normal distribution of subjects at the median and uses all of his cases (which frequently is not what is done, although if properly treated statistically that is not methodologically sinful). Let us take variable <em>X</em> as the “input” variable (never mind its causal role). The mean score of the cases in the top half of the distribution will then be at one mean deviation, that is, in standard score terms they will have an average score of 0.80. Similarly, the subjects in the bottom half of the <em>X</em> distribution will have a mean standard score of -0.80. So the mean difference in standard score terms between the high and low <em>X</em>s, the one “experimental” and the other “control” group, is 1.6. If the regression of output variable <em>Y</em> on <em>X</em> is approximately linear, this yields an expected difference in standard score terms of 0.48, so the difference on the arbitrarily defined “output” variable <em>Y</em> is in the neighborhood of half a standard deviation.</p>
<p>When the investigator runs a <em>t</em>-test on these data, what is the probability of achieving a statistically-significant result? This depends upon the statistical power function and hence upon the sample size, which varies widely, more in soft psychology because of the nature of the data collection problems than in experimental work. I do not have exact figures, but an informal scanning of several issues of journals in the soft areas of clinical, abnormal, and social gave me a representative value of the number of cases in each of two groups being compared at around <em>N</em><sub>1</sub> = <em>N</em><sub>2</sub> = 37 (that’s a median because of the skewness, sample sizes ranging from a low of 17 in one clinical study to a high of 1,000 in a social survey study). Assuming equal variances, this gives us a standard error of the mean difference of 0.2357 in sigma-units, so that our <em>t</em> is a little over 2.0. The substantive theory in a real life case being almost invariably predictive of a direction (it is hard to know what sort of significance testing we would be doing otherwise), the 5% level of confidence can be legitimately taken as one-tailed and in fact could be criticized if it were not (assuming that the 5% level of confidence is given the usual special magical significance afforded it by social scientists!). The directional 5% level being at 1.65, the expected value of our <em>t</em>-test in this situation is approximately 0.35 <em>t</em> units from the required significance level. Things being essentially normal for 72 df, this gives us a power of detecting a difference of around 0.64.</p>
<p>However, since in our imagined “experiment” the assignment of direction was random, the probability of detecting a difference in the predicted direction (even though in reality this prediction was not mediated by any rational relation of content) is only half of that. Even this conservative power based upon the assumption of a completely random association between the theoretical substance and the pseudopredicted direction should give one pause. We find that the probability of getting a positive result from a theory with no verisimilitude whatsoever, associated in a totally whimsical fashion with a pair of variables picked randomly out of the observational pot, is <em>one chance in 3</em>! This is quite different from the 0.05 level that people usually think about. Of course, the reason for this is that the 0.05 level is based upon strictly holding <em>H</em><sub>0</sub> if the theory were false. Whereas, because in the social sciences everything is correlated with everything, for epistemic purposes (despite the rigor of the mathematician’s tables) the true baseline—if the theory has nothing to do with reality and has only a chance relationship to it (so to speak, “any connection between the theory and the facts is purely coincidental”) - is 6 or 7 times as great as the reassuring 0.05 level upon which the psychologist focuses his mind. If the crud factor in a domain were running around 0.40, the power function is 0.86 and the “directional power” for random theory/prediction pairings would be 0.43.</p>
<p>…A similar situation holds for psychopathology, and for many variables in personality measurement that refer to aspects of social competence on the one hand or impairment of interpersonal function (as in mental illness) on the other. <a href="#thorndike-1920">Thorndike had a dictum</a> “All good things tend to go together.”</p>
</blockquote>
</section>
<section id="meehl-1990-2">
<h2><a href="#meehl-1990-2" title="Link to section: § 'Meehl1990 (2)'"><span><span>Meehl</span><span>1990</span></span> (2)</a></h2>
<p><a href="https://gwern.net/doc/www/meehl.umn.edu/577e7c2d422171276a0b7dc8ebba9da1962cf36b.pdf" id="_2PL7SjGU" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/meehl.umn.edu/577e7c2d422171276a0b7dc8ebba9da1962cf36b.pdf" data-url-original="https://meehl.umn.edu/sites/meehl.umn.edu/files/files/147appraisingamending.pdf" data-filesize-bytes="1338000" data-filesize-percentage="55">“Appraising and amending theories: the strategy of Lakatosian defense and two principles that warrant using it”</a>, <span><span>Meehl</span><span>1990b</span></span>:</p>
<blockquote>
<p>Research in the behavioral sciences can be experimental, correlational, or field study (including clinical); only the first two are addressed here. For reasons to be explained (<a href="#meehl-1990-1">Meehl, <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>c</a>), I treat as correlational those experimental studies in which the chief theoretical test provided involves an interaction effect between an experimental manipulation and an individual-differences variable (whether trait, status, or demographic). In correlational research there arises a special problem for the social scientist from the empirical fact that “everything is correlated with everything, more or less.” My colleague David Lykken presses the point further to include most, if not all, purely experimental research designs, saying that, speaking causally, “Everything influences everything”, a stronger thesis that I neither assert nor deny but that I do not rely on here. The obvious fact that everything is more or less correlated with everything in the social sciences is readily foreseen from the armchair on common-sense considerations. These are strengthened by more advanced theoretical arguments involving such concepts as genetic linkage, auto-catalytic effects between cognitive and affective processes, traits reflecting influences such as child-rearing practices correlated with intelligence, ethnicity, social class, religion, and so forth. If one asks, to take a trivial and theoretically uninteresting example, whether we might expect to find social class differences in a color-naming test, there immediately spring to mind numerous influences, ranging from (1) verbal intelligence leading to better verbal discriminations and retention of color names to (2) class differences in maternal teaching behavior (which one can readily observe by watching mothers explain things to their children at a zoo) to (3) more subtle—but still nonzero—influences, such as upper-class children being more likely Anglicans than Baptists, hence exposed to the changes in liturgical colors during the church year! Examples of such multiple possible influences are so easy to generate, I shall resist the temptation to go on. If somebody asks a psychologist or sociologist whether she might expect a nonzero correlation between dental caries and IQ, the best guess would be yes, small but statistically-significant. A small negative correlation was in fact found during the 1920s, misleading some hygienists to hold that IQ was lowered by toxins from decayed teeth. (The received explanation today is that dental caries and IQ are both correlates of social class.) More than 75 years ago, Edward Lee Thorndike enunciated the famous dictum, “All good things tend to go together, as do all bad ones.” Almost all human performance (work competence) dispositions, if carefully studied, are saturated to some extent with the general intelligence factor <em>g</em>, which for psychodynamic and ideological reasons has been somewhat neglected in recent years but is due for a comeback (Betz, <span>1986<sub><span title="1986 was 39 years ago.">39ya</span></sub></span>).<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>The ubiquity of nonzero correlations gives rise to what is methodologically disturbing to the theory tester and what I call, following Lykken, the crud factor. I have discussed this at length elsewhere (Meehl, <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>c), so I only summarize and provide a couple of examples here. The main point is that, when the sample size is sufficiently large to produce accurate estimates of the population values, almost any pair of variables in psychology will be correlated to some extent. Thus, for instance, less than 10% of the items in the MMPI item pool were put into the pool with masculinity-femininity in mind, and the empirically derived <em>Mf</em><span> scale contains only some of those plus others put into the item pool for other reasons, or without any theoretical considerations. When one samples thousands of individuals, it turns out that only 43 of the 550 items (8%) fail to show a significant difference between males and females. In an unpublished study (but see Meehl, <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>c) of the hobbies, interests, vocational plans, school course preferences, social life, and home factors of Minnesota college freshmen, when Lykken and I ran chi squares on all possible pairwise combinations of variables, 92% were significant, and 78% were significant at </span><em>p</em> &lt; 10<sup>−6</sup>. Looked at another way, the median number of significant relationships between a given variable and all the others was 41 of a possible 44. One finds such oddities as a relationship between which kind of shop courses boys preferred in high school and which of several Lutheran synods they belonged to!</p>
<p>…The third objection is somewhat harder to answer because it would require an encyclopedic survey of research literature over many domains. It is argued that, although the crud factor is admittedly ubiquitous—that is, almost no correlations of the social sciences are literally zero (as required by the usual significance test)—the crud factor is in most research domains not large enough to be worth worrying about. Without making a claim to know just how big it is, I think this objection is pretty clearly unsound. Doubtless the average correlation of any randomly picked pair of variables in social science depends on the domain, and also on the instruments employed (eg. it is well known that personality inventories often have as much methods-covariance as they do criterion validities).</p>
<p>A representative pairwise correlation among MMPI scales, despite the marked differences (sometimes amounting to phenomenological “oppositeness”) of the nosological rubrics on which they were derived, is in the middle to high 0.30s, in both normal and abnormal populations. The same is true for the occupational keys of the <a href="https://en.wikipedia.org/wiki/Strong_Interest_Inventory" id="_Or9gFIDD" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Strong_Interest_Inventory#bodyContent" title="Strong Interest Inventory">Strong Vocational Interest Bank</a>. Deliberately aiming to diversify the qualitative features of cognitive tasks (and thus “purify” the measures) in his classic studies of primary mental abilities (“pure factors”, orthogonal), Thurstone (<span>1938<sub><span title="1938 was 87 years ago.">87ya</span></sub></span>; <a href="https://gwern.net/doc/iq/1941-thurstone-factorialstudiesofintelligence.pdf" id="thurstone-thurstone-1941" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="7114649" data-filesize-percentage="94" title="_Factorial Studies of Intelligence_">Thurstone &amp; Thurstone, 1941</a>) still found an average intertest correlation of .28 (range = 0.01 to .56!) in the cross-validation sample. In the set of 20 <a href="https://en.wikipedia.org/wiki/California_Psychological_Inventory" id="_7g_r5857" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/California_Psychological_Inventory#bodyContent" title="California Psychological Inventory">California Psychological Inventory (CPI)</a> scales built to cover broadly the domain of (normal range) “folk-concept”<span> traits, Gough (<span>1987<sub><span title="1987 was 38 years ago.">38ya</span></sub></span>) found an average pairwise correlation of .44 among both males and females. Guilford’s Social </span><a href="https://en.wikipedia.org/wiki/Extraversion_and_introversion" id="_HAZaloBe" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Extraversion_and_introversion#bodyContent" title="Extraversion and introversion">Introversion</a>, Thinking Introversion, Depression, Cycloid Tendencies, and Rhathymia or Freedom From Care scales, constructed on the basis of (orthogonal) factors, showed pairwise correlations ranging from -.02 to .85, with 5 of the 10 <em>r</em>s ≥ 0.33 despite the purification effort (<a href="https://gwern.net/doc/psychology/personality/1941-evans.pdf" id="evans-mcconnell-1941" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="633152" data-filesize-percentage="64" title="A new measure of introversion-extroversion">Evans &amp; McConnell, 1941</a>). Any treatise on factor analysis exemplifying procedures with empirical data suffices to make the point convincingly. For example, in <a href="https://archive.org/details/ModernFactorAnalysis" id="_EkOsaEjc" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/ModernFactorAnalysis?view=theater" title="_Modern Factor Analysis_">Harman (<span>1960<sub><span title="1960 was 65 years ago.">65ya</span></sub></span>)</a>, 8 “emotional” variables correlate .10 to .87, median <em>r</em>= 0.44 (p.&nbsp;176), and 8 “political” variables correlate .03 to .88, median (absolute value) <em>r</em> = 0.62 (p.&nbsp;178). For highly diverse acquiescence-corrected measures (personality traits, interests, hobbies, psychopathology, social attitudes, and religious, political, and moral opinions), estimating individuals’ (orthogonal!) factor scores, one can hold mean <em>r</em>s down to an average of .12, means .04–.20, still some individual <em>r</em><span>s &gt; 0.30 (Lykken, personal communication, <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>; cf. McClosky &amp; Meehl, in preparation). Public opinion polls and attitude surveys routinely disaggregate data with respect to several demographic variables (eg. age, education, section of country, sex, ethnicity, religion, education, income, rural/urban, self-described political affiliation) because these factors are always correlated with attitudes or electoral choices, sometimes strongly so. One must also keep in mind that socioeconomic status, although intrinsically interesting (especially to sociologists) is probably often functioning as a proxy for other unmeasured personality or status characteristics that are not part of the definition of social class but are, for a variety of complicated reasons, correlated with it. The proxy role is important because it prevents adequate </span>“controlling for” unknown (or unmeasured) crud-factor influences by statistical procedures (matching, partial correlation, analysis of covariance, <a href="https://en.wikipedia.org/wiki/Path_analysis_(statistics)" id="_sNgHsjac" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Path_analysis_(statistics)#bodyContent" title="Path analysis (statistics)">path analysis</a>). [ie <a href="https://gwern.net/doc/statistics/bayes/regression-to-mean/index" id="_8LEzTTCn" data-filesize-bytes="43699" data-filesize-percentage="38" title="'Regression To The Mean Fallacies', Gwern 2021">“residual confounding”</a>]</p>
<ul>
<li><p>Thurstone, L. L. (<span>1938<sub><span title="1938 was 87 years ago.">87ya</span></sub></span>). <em>Primary mental abilities</em>. Chicago: University of Chicago Press. <!-- TODO: pricewatched --></p></li>
<li><p>Gough, H. G. (<span>1987<sub><span title="1987 was 38 years ago.">38ya</span></sub></span>). <em>CPI, Administrator’s guide</em>. Palo Alto, CA: Consulting Psychologists Press.</p></li>
<li><p>McClosky, Herbert, &amp; Meehl, P. E. (in preparation). <em>Ideologies in conflict</em>.<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a></p></li>
</ul>
</blockquote>
</section>
<section id="tukey-1991">
<h2><a href="#tukey-1991" title="Link to section: § 'Tukey1991'"><span><span>Tukey</span><span>1991</span></span></a></h2>
<p><a href="https://projecteuclid.org/download/pdf_1/euclid.ss/1177011945" id="_5qNrA51l" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“The philosophy of multiple comparisons”</a>, <span><span>Tukey</span><span>1991</span></span>:</p>
<blockquote>
<p>Statisticians classically asked the wrong question—and were willing to answer with a lie, one that was often a downright lie. They asked “Are the effects of A and B different?” and they were willing to answer “no”.</p>
<p>All we know about the world teaches us that the effects of A and B are always different—in some decimal place—for any A and B. Thus asking “Are the effects different?” is foolish.</p>
<p>What we should be answering first is “Can we tell the direction in which the effects differ from the effects of B?” In other words, can we be confident about the direction from A to B? Is it “up”, “down”, or “uncertain”?</p>
</blockquote>
</section>
<section id="raftery-1995">
<h2><a href="#raftery-1995" title="Link to section: § 'Raftery1995'"><span><span>Raftery</span><span>1995</span></span></a></h2>
<p><a href="https://wwwlegacy.stat.washington.edu/research/online/1994/bic.ps" id="_5ObbZvwb">“Bayesian Model Selection in Social Research (with Discussion by Andrew Gelman &amp; Donald B. Rubin, and Robert M. Hauser, and a Rejoinder)”</a>, <span><span>Raftery</span><span>1995</span></span>:</p>
<blockquote>
<p>In the past 15 years, however, some quantitative sociologists have been attaching less importance to <em>p</em>-values because of practical difficulties and counter-intuitive results. These difficulties are most apparent with large samples, where <em>p</em>-values tend to indicate rejection of the null hypothesis even when the null model seems reasonable theoretically and inspection of the data fails to reveal any striking discrepancies with it. Because much sociological research is based on survey data, often with thousands of cases, sociologists frequently come up against this problem. In the early 1980s, some sociologists dealt with this problem by ignoring the results of <em>p</em>-value-based tests when they seemed counter-intuitive, and by basing model selection instead on theoretical considerations and informal assessment of discrepancies between model and data (eg. Fienberg and Mason, <span>1979<sub><span title="1979 was 46 years ago.">46ya</span></sub></span>; Hout, <span>1983<sub><span title="1983 was 42 years ago.">42ya</span></sub></span>, <span>1984<sub><span title="1984 was 41 years ago.">41ya</span></sub></span>; Grusky and Hauser, <span>1984<sub><span title="1984 was 41 years ago.">41ya</span></sub></span>).</p>
<p>…It is clear that models 1 and 2 are unsatisfactory and should be rejected in favor of model 3.<sup>3</sup> By the standard test, model 3 should also be rejected, in favor of model 4, given the deviance difference of 150 on 16 degrees of freedom, corresponding to a <em>p</em>-value of about 10<sup>−120</sup> . <span><span>Grusky &amp; Hauser</span><span>1984</span></span> nevertheless adopted model 3 because it explains most (99.7%) of the deviance under the baseline model of independence, fits well in the sense that the differences between observed and expected counts are a small proportion of the total, and makes good theoretical sense. This seems sensible, and yet is in dramatic conflict with the <em>p</em>-value-based test. This type of conflict often arises in large samples, and hence is frequent in sociology with its survey data sets comprising thousands of cases. The main response to it has been to claim that there is a distinction between “statistical” and “substantive” significance, with differences that are statistically-significant not necessarily being substantively important.</p>
</blockquote>
</section>
<section id="thompson-1995">
<h2><a href="#thompson-1995" title="Link to section: § 'Thompson1995'"><span><span>Thompson</span><span>1995</span></span></a></h2>
<p><a href="https://files.eric.ed.gov/fulltext/ED392819.pdf#page=9" id="_71HtzEaV" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“Editorial Policies Regarding Statistical-Significance Testing: 3 Suggested Reforms”</a>, <span><span>Thompson</span><span>1995</span></span>:</p>
<blockquote>
<p>One serious problem with this statistical testing logic is that the in reality <em>H</em><sub>0</sub> is never true in the population, as recognized by any number of prominent statisticians (<a href="#tukey-1991">Tukey, 1991</a>), ie. there will always be some differences in population parameters, although the differences may be incredibly trivial. Near 40 years ago Savage (<a href="#savage-1957">1957</a>, pp.&nbsp;332–333) noted that, “Null hypotheses of no difference are usually known to be false before the data are collected.” Subsequently, Meehl (<span>1978<sub><span title="1978 was 47 years ago.">47ya</span></sub></span>, p.822) argued, “As I believe is generally recognized by statisticians today and by thoughtful social scientists, the null hypothesis, taken literally, is always false.” Similarly, noted statistician <a href="#hays-1973">Hays</a><span> (<span>1981<sub><span title="1981 was 44 years ago.">44ya</span></sub></span>, p.&nbsp;293 [</span><em>Statistics</em>], 3<sup>rd</sup> ed.) pointed out that “[t]here is surely nothing on earth that is completely independent of anything else. The strength of association may approach zero, but it should seldom or never be exactly zero.” And <a href="#loftus-loftus-1982">Loftus and Loftus</a><span> (<span>1982<sub><span title="1982 was 43 years ago.">43ya</span></sub></span>, pp.&nbsp;498–499) argued that, </span>“finding a ‘[statistically] significant effect’ really provides very little information, because it’s almost certain that some relationship (however small) exists between any two variables.” The very important implication of all this is that statistical-significance testing primarily becomes only a test of researcher endurance, because “virtually any study can be made to show [statistically] significant results if one uses enough subjects”<span> (Hays, <span>1981<sub><span title="1981 was 44 years ago.">44ya</span></sub></span>, p.&nbsp;293). As </span><a href="#nunnally-1960">Nunnally</a><span> (<span>1960<sub><span title="1960 was 65 years ago.">65ya</span></sub></span>, p.&nbsp;643) noted some 35 years ago, </span>“If the null hypothesis is not rejected, it is usually because the <em>N</em> is too small. If enough data are gathered, the hypothesis will generally be rejected.” The implication is that:</p>
<blockquote>
<p>Statistical-significance testing can involve a tautological logic in which tired researchers, having collected data from hundreds of subjects, then conduct a statistical test to evaluate whether there were a lot of subjects, which the researchers already know, because they collected the data and know they’re tired. This tautology has created considerable damage as regards the cumulation of knowledge… (<a href="https://gwern.net/doc/psychology/1992-thompson.pdf" id="thompson-1992" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="395049" data-filesize-percentage="52" title="Two and One-Half Decades of Leadership in Measurement and Evaluation">Thompson, 1992</a>, p.&nbsp;436)</p>
</blockquote>
</blockquote>
</section>
<section id="mulaik-et-al-1997">
<h2><a href="#mulaik-et-al-1997" title="Link to section: § 'Mulaik et al 1997'"><span><span title="et al">Mulaik</span><span> Et Al </span><span>1997</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/1997-muzaik.pdf" id="mulaik-et-al-1997" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2857133" data-filesize-percentage="88" title="‘There Is a Time and a Place for Significance Testing’, Mulaik et al 1997">“There Is a Time and a Place for Statistical-Significance Testing”</a>, <span><span title="et al">Mulaik</span><span> et al </span><span>1997</span></span> (in <em>What If There Were No Significance Tests</em><span> ed <span><span title="et al">Harlow</span><span> et al </span><span>1997</span></span>):</span></p>
<blockquote>
<p>Most of these articles expose misconceptions about significance testing common among researchers and writers of psychological textbooks on statistics and measurement. But the criticisms do not stop with misconceptions about significance testing. Others like <a href="#meehl-1967"><span><span>Meehl</span><span>1967</span></span></a> expose the limitations of a statistical practice that focuses only on testing for zero differences between means and zero correlations instead of testing predictions about specific nonzero values for parameters derived from theory or prior experience, as is done in the physical sciences. Still others emphasize that significance tests do not alone convey the information needed to properly evaluate research findings and perform accumulative research.</p>
<p>…Other than emphasizing a need to properly understand the interpretation of <a href="https://en.wikipedia.org/wiki/Confidence_interval" id="_i_2OI0BT" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Confidence_interval#bodyContent" title="Confidence interval">confidence intervals</a>, we have no disagreements with these criticisms and proposals. But a few of the critics go even further. In this chapter we will look at arguments made by <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.780&amp;rep=rep1&amp;type=pdf" id="_9h6u89Tu" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" title="The Case Against Statistical-Significance Testing">Carver (<span>1978<sub><span title="1978 was 47 years ago.">47ya</span></sub></span>)</a>, <a href="#cohen-1994">Cohen (<span>1994<sub><span title="1994 was 31 years ago.">31ya</span></sub></span>)</a>, <a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/40448a28b9470a20f382cedee2b3fd9b6ccb1c41.pdf" id="rozeboom-1960" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/40448a28b9470a20f382cedee2b3fd9b6ccb1c41.pdf" data-url-original="https://pdfs.semanticscholar.org/b596/4787fc1abf739148d604abfbd2689e73e52f.pdf" data-filesize-bytes="653742" data-filesize-percentage="40" title="The fallacy of the null-hypothesis significance test">Rozeboom (<span>1960<sub><span title="1960 was 65 years ago.">65ya</span></sub></span>)</a>, Schmidt (<a href="https://gwern.net/doc/statistics/meta-analysis/1992-schmidt.pdf" id="_2WeqSE4u" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1070079" data-filesize-percentage="75" title="What Do Data Really Mean? Research Findings, Meta-Analysis, and Cumulative Knowledge in Psychology">1992</a>, <span>1996<sub><span title="1996 was 29 years ago.">29ya</span></sub></span>), and Schmidt and Hunter (chapter 3 of this volume), in favor of not merely recommending the reporting of point estimates of <a href="https://en.wikipedia.org/wiki/Effect_size" id="_7H3QCJ9x" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Effect_size#bodyContent" title="Effect size">effect sizes</a><span> and confidence intervals based on them, but of abandoning altogether the use of significance tests in research. Our focus will be principally on Schmidt’s (<span>1992<sub><span title="1992 was 33 years ago.">33ya</span></sub></span>, <span>1996<sub><span title="1996 was 29 years ago.">29ya</span></sub></span>) papers, because they incorporate arguments from earlier papers, especially Carver’s (<span>1978<sub><span title="1978 was 47 years ago.">47ya</span></sub></span>), and also carry the argument to its most extreme conclusions. Where appropriate, we will also comment on Schmidt and Hunter’s (chapter 3 of this volume) rebuttal of arguments against their position.</span></p>
<p><strong>The Null Hypothesis Is Always False?</strong></p>
<p>Cohen (<span>1994<sub><span title="1994 was 31 years ago.">31ya</span></sub></span>), influenced by Meehl (<span>1978<sub><span title="1978 was 47 years ago.">47ya</span></sub></span>), argued that “the nil hypothesis is always false”<span> (p. 1000). Get a large enough sample and you will always reject the null hypothesis. He cites a number of eminent statisticians in support of this view. He quotes Tukey (<span>1991<sub><span title="1991 was 34 years ago.">34ya</span></sub></span>, p. 100) to the effect that there are always differences between experimental treatments-for some decimal places. Cohen cites an unpublished study by Meehl and Lykken in which cross tabulations for 15 Minnesota Multiphasic Personality Inventory (MMPI) items for a sample of 57,000 subjects yielded 105 chi-square tests of association and every one of them was significant, and 96% of them were significant at </span><em>p</em><span> &lt; 0.000001 (Cohen, 1994, p.&nbsp;1000). Cohen cites Meehl (<span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span>) as suggesting that this reflects a </span>“crud factor” in nature. “Everything is related to everything else” to some degree. So, the question is, why do a significance test if you know it will always be significant if the sample is large enough? But if this is an empirical hypothesis, is it not one that is established using significance testing?</p>
<p>But the example may not be an apt demonstration of the principle Cohen sought to establish: It is generally expected that responses to different items responded to by the same subjects are not independently distributed across subjects, so it would not be remarkable to find significant correlations between many such items.</p>
<p>Much more interesting would be to demonstrate systematic and replicable significant treatment effects when subjects are assigned at random to different treatment groups but the <em>same</em> treatments are administered to each group. But in this case, small but significant effects in studies with high power that deviate from expectations of no effect when no differences in treatments are administered are routinely treated as systematic experimenter errors, and knowledge of experimental technique is improved by their detection and removal or control. Systematic error and experimental artifact must always be considered a possibility when rejecting the null hypothesis. Nevertheless, do we know a priori that a test will <em>always</em> be significant if the sample is large enough? Is the proposition “Every statistical hypothesis is false” an <em>axiom</em> that needs no testing? Actually, we believe that to regard this as an axiom would introduce an internal contradiction into statistical reasoning, comparable to arguing that all propositions and descriptions are false. You could not think and reason about the world with such an axiom. So it seems preferable to regard this as some kind of empirical generalization. But no empirical generalization is ever incorrigible and beyond testing. Nevertheless, if indeed there is a phenomenon of nature known as “the crud factor”, then it is something we know to be objectively a fact only because of significance tests. Something in the background noise stands out as a signal against that noise, because we have sufficiently powerful tests using huge samples to detect it. At that point it may become a challenge to science to develop a better understanding of what produces it. However, it may tum out to reflect only experimenter artifact. But in any case the hypothesis of a crud factor is not beyond further testing.</p>
<p>The point is that it doesn’t matter if the null hypothesis is always judged false at some sample size, as long as we regard this as an empirical phenomenon. What matters is whether <em>at the sample size we have</em> we can distinguish observed deviations from our hypothesized values to be sufficiently large and improbable under a hypothesis of chance that we can treat them reasonably but provisionally as not due to chance error. There is no a priori reason to believe that one will always reject the null hypothesis at any given sample size. On the other hand, accepting the null hypothesis does not mean the hypothesized value is true, but rather that the evidence observed is not distinguishable from what we would regard as due to chance if the null hypothesis were true and thus is not sufficient to disprove it. The remaining uncertainty regarding the truth of our null hypothesis is measured by the width of the region of acceptance or a function of the standard error. And this will be closely related to the power of the test, which also provides us with information about our uncertainty. The fact that the width of the region of acceptance shrinks with increasing sample size, means we are able to reduce our uncertainty regarding the provisional validity of an accepted null hypothesis with larger samples. In huge samples the issue of uncertainty due to chance looms not as important as it does in small- and moderate-size samples.</p>
</blockquote>
</section>
<section id="waller-2004">
<h2><a href="#waller-2004" title="Link to section: § 'Waller2004'"><span><span>Waller</span><span>2004</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www3.nd.edu/c3eda49a209b2c858b278a543e7c1b08c086e578.pdf" id="_90Zj0N8r" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www3.nd.edu/c3eda49a209b2c858b278a543e7c1b08c086e578.pdf" data-url-original="https://www3.nd.edu/~ghaeffel/Waller2004%20Applied%20&amp;%20Preventive%20Psychology.pdf" data-filesize-bytes="93599" data-filesize-percentage="6">“The fallacy of the null hypothesis in soft psychology”</a>, <span><span>Waller</span><span>2004</span></span>:</p>
<blockquote>
<p>In his classic article on the fallacy of the null hypothesis in soft psychology, <a href="#meehl-1978">Paul Meehl</a> claimed that, in nonexperimental settings, the probability of rejecting the null hypothesis of nil group differences in favor of a directional alternative was 0.50—a value that is an order of magnitude higher than the customary Type I error rate. In a series of real data simulations, using Minnesota Multiphasic Personality Inventory-Revised (MMPI-2) data collected from more than 80,000 individuals, I found strong support for Meehl’s claim.</p>
<p>…Before running the experiments I realized that, to be fair to Meehl, I needed a large data set with a broad range of biosocial variables. Fortunately, I had access to data from 81,485 individuals who earlier had completed the 567 items of the Minnesota Multiphasic Personality Inventory-Revised (MMPI-2; Butcher, Dahlstrom, Graham, Tellegen, &amp; Kaemmer, <span>1989<sub><span title="1989 was 36 years ago.">36ya</span></sub></span>). The MMPI-2, in my opinion, is an ideal vehicle for testing Meehl’s claim because it includes items in such varied content domains as general health concerns; personal habits and interests; attitudes towards sex, marriage, and family; affective functioning; normal range personality; and extreme manifestations of psychopathology (for a more complete description of the latent content of the MMPI, see Waller, <span>1999<sub><span title="1999 was 26 years ago.">26ya</span></sub></span>, “Searching for structure in the MMPI”).</p>
<p>…Next, the computer selected (without replacement) a random item from the pool of MMPI-2 items. Using data from the 41,491 males and 39,994 females, it then (1) performed a difference of proportions test on the item group means; (2) recorded the signed <em>z</em>-value; and (3) recorded the associated significance level. Finally, the program tallied the number of “significant” test results (ie. those with |<em>z</em>|≥1.96). The results of this mini simulation were enlightening and in excellent accord with the outcome of Meehl’s <em>gedanken</em> experiment. Specifically, 46% of the directional hypotheses were supported at significance levels that far exceeded traditional <em>p</em>-value cutoffs. A summary of the results is portrayed in Fig. 1. Notice in this figure, which displays the distribution of <em>z</em>-values for the 511 tests, that many of the item mean differences were 50–100 times larger than their associated standard errors!</p>
<figure>
<p><img alt="Figure 1: Distribution of z -values for 511 hypothesis tests." data-aspect-ratio="1031 / 775" decoding="async" height="775" loading="lazy" src="https://gwern.net/doc/statistics/2004-waller-figure1.png" width="1031"></p>
<figcaption><p><strong>Figure 1</strong>: Distribution of <em>z</em>-values for 511 hypothesis tests.</p></figcaption>
</figure>
<figure>
<p><img alt="Figure 2: Distribution of the frequency of rejected null hypotheses, in favor of a randomly chosen directional alternative, in 320,922 hypothesis test." data-aspect-ratio="959 / 832" decoding="async" height="832" loading="lazy" src="https://gwern.net/doc/statistics/2004-waller-figure2.png" width="959"></p>
<figcaption><p><strong>Figure 2</strong>: Distribution of the frequency of rejected null hypotheses, in favor of a randomly chosen directional alternative, in 320,922 hypothesis test.</p></figcaption>
</figure>
</blockquote>
<p>Waller also highlights Bill <span><span>Thompson’s</span><span>2001</span></span> bibliography <a href="https://gwern.net/doc/statistics/causality/2001-thompson.html" id="_BHa3BLOQ" data-link-icon="internet-archive" data-link-icon-type="svg" data-filesize-bytes="76213" data-filesize-percentage="14">“402 Citations Questioning the Indiscriminate Use of Null Hypothesis Significance Tests in Observational Studies”</a> as a source for criticisms of NHST but unfortunately it’s unclear which of them might bear on the specific criticism of ‘the null hypothesis is always false’.</p>
</section>
<section id="kilgarriff-2005">
<h2><a href="#kilgarriff-2005" title="Link to section: § 'Kilgarriff2005'"><span><span>Kilgarriff</span><span>2005</span></span></a></h2>
<p><a href="https://gwern.net/doc/psychology/linguistics/2005-kilgarriff.pdf" id="kilgarriff-2005" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="80665" data-filesize-percentage="15" title="‘Language is never, ever, ever, random’, Kilgarriff 2005">“Language is never, ever, ever, random”, <span><span>Kilgarriff</span><span>2005</span></span></a></p>
</section>
<section id="starbuck-2006">
<h2><a href="#starbuck-2006" title="Link to section: § 'Starbuck2006'"><span><span>Starbuck</span><span>2006</span></span></a></h2>
<p><a href="https://www.amazon.com/Production-Knowledge-Challenge-Science-Research/dp/0199288534" id="_UI1YVajl" data-link-icon="amazon" data-link-icon-type="svg" data-link-icon-color="#ffce53"><em>The Production of Knowledge: The Challenge of Social Science Research</em></a>, <span><span>Starbuck</span><span>2006</span></span>, pg47–49:</p>
<blockquote>
<p>Induction requires distinguishing meaningful relationships (signals) in the midst of an obscuring background of <a href="https://en.wikipedia.org/wiki/Confounding" id="_L43L1NuO" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Confounding#bodyContent" title="Confounding">confounding</a> relationships (noise). The weak and meaningless or substantively secondary correlations in the background make induction untrustworthy. In many tasks, people can distinguish weak signals against rather strong background noise. The reason is that both the signals and the background noise match familiar patterns. For example, a driver traveling to a familiar destination focuses on landmarks that experience has shown to be relevant. People have trouble making such distinctions where signals and noise look much alike or where signals and noise have unfamiliar characteristics. For example, a driver traveling a new road to a new destination is likely to have difficulty spotting landmarks and turns on a recommended route.</p>
<p>Social science research has the latter characteristics. This activity is called research because its outputs are unknown; and the signals and noise look a lot alike in that both have systematic components and both contain components that vary erratically. Therefore, researchers rely upon statistical techniques to distinguish signals from noise. However, these techniques assume: (1) that the so-called random errors really do cancel each other out so that their average values are close to zero; and (2) that the so-called random errors in different variables are uncorrelated. These are very strong assumptions because they presume that the researchers’ hypotheses encompass absolutely all of the systematic effects in the data, including effects that the researchers have not foreseen or measured. When these assumptions are not met, the statistical techniques tend to mistake noise for signal, and to attribute more importance to the researchers’ hypotheses than they deserve.</p>
<p>I remembered what <a href="https://gwern.net/doc/economics/1961-ames.pdf" id="ames-reiter-1961" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1617916" data-filesize-percentage="82" title="Distributions of correlation coefficients in economic time series"><span><span>Ames &amp; Reiter</span><span>1961</span></span></a> had said about how easy it is for macroeconomists to discover statistically-significant correlations that have no substantive significance, and I could see 5 reasons why a similar phenomenon might occur with cross-sectional data. Firstly, a few broad characteristics of people and social systems pervade social science data—examples being sex, age, intelligence, social class, income, education, or organization size. Such characteristics correlate with many behaviors and with each other. Secondly, researchers’ decisions about how to treat data can create correlations between variables. For example, when the Aston researchers used factor analysis to create aggregate variables, they implicitly determined the correlations among these aggregate variables. Thirdly, so-called ‘samples’ are frequently not random, and many of them are complete subpopulations—say, every employee of a company—even though study after study has turned up evidence that people who live close together, who work together, or who socialize together tend to have more attitudes, beliefs, and behaviors in common than do people who are far apart physically and socially. Fourthly, some studies obtain data from respondents at one time and through one method. By including items in a single questionnaire or interview, researchers suggest to respondents that relationships exist among these items. Lastly, most researchers are intelligent people who are living successful lives. They are likely to have some intuitive ability to predict the behaviors of people and of social systems. They are much more likely to formulate hypotheses that accord with their intuition than ones that violate it; they are quite likely to investigate correlations and differences that deviate from zero; and they are less likely than chance would imply to observe correlations and differences near zero.</p>
<p><a href="https://gwern.net/doc/economics/1988-webster.html" id="_9Ox1oap9" data-link-icon="internet-archive" data-link-icon-type="svg" data-filesize-bytes="723500" data-filesize-percentage="67" title="Theory Building In Industrial And Organizational Psychology">Webster and I hypothesized</a> that statistical tests with a null hypothesis of no correlation are biased toward statistical-significance. Webster culled through <em>Administrative Science Quarterly</em>, the <em>Academy of Management Journal</em>, and the <em>Journal of Applied Psychology</em> seeking matrices of correlations. She tabulated only complete matrices of correlations in order to observe the relations among all of the variables that the researchers perceived when drawing inductive inferences, not only those variables that researchers actually included in hypotheses. Of course, some researchers probably gathered data on additional variables beyond those published, and then omitted these additional variables because they correlated very weakly with the dependent variables. We estimated that 64% of the correlations in our data were associated with researchers’ hypotheses.</p>
<figure>
<p><img alt="Figure 2.6: Correlations reported in 3 journals" data-aspect-ratio="1400 / 869" decoding="async" height="869" loading="lazy" src="https://gwern.net/doc/statistics/2006-starbuck-websterstarbuck1988-figure26-managementsciencecorrelations.jpg" width="1400"></p>
<figcaption><p><strong>Figure 2.6</strong>: Correlations reported in 3 journals</p></figcaption>
</figure>
<p>Figure 2.6 shows the distributions of 14,897 correlations. In all 3 journals, both the mean correlation and the median correlation were close to +0.09 and the distributions of correlations were very similar. Finding significant correlations is absurdly easy in this population of variables, especially when researchers make two-tailed tests with a null hypothesis of no correlation. Choosing two variables utterly at random, a researcher has 2-to-1 odds of finding a significant correlation on the first try, and 24-to-1 odds of finding a significant correlation within 3 tries (also see <a href="https://gwern.net/doc/www/web-archive.southampton.ac.uk/a9470a09988d6f45ceeb702b8b4e729433320617.pdf" id="_Cg0DLM91" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/web-archive.southampton.ac.uk/a9470a09988d6f45ceeb702b8b4e729433320617.pdf" data-url-original="https://web-archive.southampton.ac.uk/cogprints.org/5178/1/are_null_results_becoming.pdf" data-filesize-bytes="32340" data-filesize-percentage="2" title="Are Null Results Becoming an Endangered Species in Marketing?">Hubbard and <span><span>Armstrong</span><span>1992</span></span></a>). Furthermore, the odds are better than 2-to-1 that an observed correlation will be positive, and positive correlations are more likely than negative ones to be statistically-significant. Because researchers gather more data when they are getting small correlations, studies with large numbers of observations exhibit slightly less positive bias. The mean correlation in studies with fewer than 70 observations is about twice the mean correlation in studies with over 180 observations. The main inference I drew from these statistics was that the social sciences are drowning in statistically-significant but meaningless noise. Because the differences and correlations that social scientists test have distributions quite different from those assumed in hypothesis tests, social scientists are using tests that assign statistical-significance to confounding background relationships. Because social scientists equate statistical-significance with meaningful relationships, they often mistake confounding background relationships for theoretically important information. One result is that social science research creates a cloud of statistically-significant differences and correlations that not only have no real meaning but also impede scientific progress by obscuring the truly meaningful relationships.</p>
<p>Suppose that roughly 10% of all observable relations could be theoretically meaningful and that the remaining 90% either have no meanings or can be deduced as implications of the key 10%. However, we do not know now which relations constitute the key 10%, and so our research resembles a search through a haystack in which we are trying to separate needles from more numerous straws. Now suppose that we adopt a search method that makes almost every straw look very much like a needle and that turns up thousands of apparent needles annually; 90% of these apparent needles are actually straws, but we have no way of knowing which ones. Next, we fabricate a theory that ‘explains’ these apparent needles. Some of the propositions in our theory are likely to be correct, merely by chance; but many, many more propositions are incorrect or misleading in that they describe straws. Even if this theory were to account rationally for all of the needles that we have supposedly discovered in the past, which is extremely unlikely, the theory has very little chance of making highly accurate predictions about the consequences of our actions unless the theory itself acts as a powerful self-fulfilling prophecy (Eden and <span><span>Ravid</span><span>1982</span></span>). Our theory would make some correct predictions, of course, because with so many correlated variables, even a completely false theory would have a reasonable chance of generating predictions that come true. Thus, we dare not even take correct predictions as dependable evidence of our theory’s correctness (<a href="https://archive.org/details/psychologyasscie0000dees" id="_Fy_hLQfz" data-link-icon="internet-archive" data-link-icon-type="svg" data-url-iframe="https://archive.org/details/psychologyasscie0000dees?view=theater" title="<em>Psychology as science and art</em>"><span><span>Deese</span><span>1972</span></span>: 61–67</a> [<em>Psychology as Science and Art</em>]).</p>
</blockquote>
</section>
<section id="smith-et-al-2007">
<h2><a href="#smith-et-al-2007" title="Link to section: § 'Smith et al 2007'"><span><span title="et al">Smith</span><span> Et Al </span><span>2007</span></span></a></h2>
<p><a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040352" id="smith-et-al-2007-link" data-link-icon="plos" data-link-icon-type="svg" title="'Clustered Environments and Randomized Genes: A Fundamental Distinction between Conventional and Genetic Epidemiology', Smith et al 2007">“Clustered Environments and Randomized Genes: A Fundamental Distinction between Conventional and Genetic Epidemiology”</a>, <span><span title="et al">Smith</span><span> et al </span><span>2007</span></span>:</p>
<blockquote>
<p>…We examined the extent to which genetic variants, on the one hand, and nongenetic environmental exposures or phenotypic characteristics on the other, tend to be associated with each other, to assess the degree of confounding that would exist in conventional epidemiological studies compared with <a href="https://en.wikipedia.org/wiki/Mendelian_randomization" id="_eB48Cm87" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Mendelian_randomization#bodyContent" title="Mendelian Randomization">Mendelian Randomization</a> studies. <em>Methods &amp; Findings</em>: We estimated pairwise correlations between [96] nongenetic baseline variables and genetic variables in a cross-sectional study [British Women’s Heart and Health Study; <em>n</em> = 4,286] comparing the number of correlations that were statistically-significant at the 5%, 1%, and 0.01% level (α = 0.05, 0.01, and 0.0001, respectively) with the number expected by chance if all variables were in fact uncorrelated, using a two-sided binomial exact test. We demonstrate that behavioral, socioeconomic, and physiological factors are strongly interrelated, with 45% of all possible pairwise associations between 96 nongenetic characteristics (<em>n</em> = 4,560 correlations) being significant at the <em>p</em> &lt; 0.01 level (the ratio of observed to expected significant associations was 45; <em>p</em>-value for difference between observed and expected &lt; 0.000001). Similar findings were observed for other levels of significance.</p>
<p>…The 96 nongenetic variables generated 4,560 pairwise comparisons, of which, assuming no associations existed, 5 in 100 (total 228) would be expected to be associated by chance at the 5% significance level (α = 0.05). However, 2,447 (54%) of the correlations were significant at the α = 0.05 level, giving an observed to expected (O:E) ratio of 11, <em>p</em> for difference O:E &lt; 0.000001 (Table 1). At the 1% significance level, 45.6 of the correlations would be expected to be associated by chance, but we found that 2,036 (45%) of the pairwise associations were statistically-significant at α = 0.01, giving an O:E ratio of 45, <em>p</em> for difference O:E &lt; 0.000001 (Table 2). At the 0.01% significance level, 0.456 of the correlations would be expected to be associated by chance, but we found that 1,378 (30%) were statistically-significantly associated at α = 0.0001, giving an O:E ratio of 3,022, <em>p</em> for difference O:E &lt; 0.000001.</p>
<p>…Over 50% of the pairwise associations between baseline nongenetic characteristics in our study were statistically-significant at the 0.05 level; an 11-fold increase from what would be expected, assuming these characteristics were independent. Similar findings were found for statistically-significant associations at the 0.01 level (45-fold increase from expected) and the 0.0001 level (3,000-fold increase from expected). This illustrates the considerable difficulty of determining which associations are valid and potentially causal from a background of highly correlated factors, reflecting that behavioral, socioeconomic, and physiological characteristics tend to cluster. This tendency will mean that there will often be high levels of confounding when studying any single factor in relation to an outcome. Given the complexity of such confounding, even after formal statistical adjustment, a lack of data for some confounders, and measurement error in assessed confounders will leave considerable scope for <a href="https://gwern.net/doc/statistics/bayes/regression-to-mean/index" id="_8LEzTTCn" data-filesize-bytes="43699" data-filesize-percentage="38" title="'Regression To The Mean Fallacies', Gwern 2021">residual confounding</a> <a href="https://gwern.net/doc/statistics/causality/1992-phillips.pdf" id="phillips-smith-1992" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="589586" data-filesize-percentage="62" title="'Bias in relative odds estimation owing to imprecise measurement of correlated exposures', Phillips &amp; Smith 1992">[4]</a>. When epidemiological studies present adjusted associations as a reflection of the magnitude of a causal association, they are assuming that all possible confounding factors have been accurately measured and that their relationships with the outcome have been appropriately modelled. We think this is unlikely to be the case in most observational epidemiological studies <a href="https://gwern.net/doc/statistics/causality/1991-phillips.pdf" id="phillips-smith-1991" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1044312" data-filesize-percentage="75" title="'How independent are `independent` effects? Relative risk estimation when correlated exposures are measured imprecisely', Phillips &amp; Smith 1991">[26]</a>.</p>
<p>Predictably, such confounded relationships will be particularly marked for highly socially and culturally patterned risk factors, such as dietary intake. This high degree of confounding might underlie the poor concordance of observational epidemiological studies that identified dietary factors (such as beta carotene, vitamin E, and vitamin C intake) as protective against cardiovascular disease and cancer, with the findings of <a href="https://en.wikipedia.org/wiki/Randomized_controlled_trial" id="_q1b5B-dQ" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Randomized_controlled_trial#bodyContent" title="Randomized controlled trial">randomized controlled trials</a> of these dietary factors [1,27]. Indeed, with 45% of the pairwise associations of nongenetic characteristics being “statistically-significant” at the <em>p</em> &lt; 0.01 level in our study, and our study being unexceptional with regard to the levels of confounding that will be found in observational investigations, it is clear that the large majority of associations that exist in observational databases will not reach publication. We suggest that those that do achieve publication will reflect apparent biological plausibility (a weak causal criterion <a href="https://gwern.net/doc/statistics/causality/1992-smith.pdf" id="smith-et-al-1992" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="585231" data-filesize-percentage="62" title="'Smoking as `independent` risk factor for suicide: illustration of an artifact from observational epidemiology?', Smith et al 1992">[28]</a>) and the interests of investigators. Examples exist of investigators reporting provisional analyses in abstracts—such as antioxidant vitamin intake being apparently protective against future cardiovascular events in women with clinical evidence of cardiovascular disease [29]—but not going on to full publication of these findings, perhaps because randomized controlled trials appeared soon after the presentation of the abstracts [30] that rendered their findings as being unlikely to reflect causal relationships. Conversely, it is likely that the large majority of null findings will not achieve publication, unless they contradict high-profile prior findings, as has been demonstrated in molecular genetic research [31].</p>
<figure>
<p><img alt="Figure 1: Histogram of Statistically-Significant (at α = 1%) Age-Adjusted Pairwise Correlation Coefficients between 96 Nongenetic Characteristics. British Women Aged 60–79 y" data-aspect-ratio="1400 / 951" decoding="async" height="951" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2007-smith-figure1-correlationdistribution.jpg" title="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040352#pmed-0040352-g001" width="1400"></p>
<figcaption><p><strong>Figure 1</strong>: Histogram of Statistically-Significant (at α = 1%) Age-Adjusted Pairwise Correlation Coefficients between 96 Nongenetic Characteristics. British Women Aged 60–79 y</p></figcaption>
</figure>
<p>The magnitudes of most of the significant correlations between nongenetic characteristics were small (see Figure 1), with a median value at <em>p</em> ≤ 0.01 and <em>p</em> ≤ 0.05 of 0.08, and it might be considered that such weak associations are unlikely to be important sources of confounding. However, so many associated nongenetic variables, even with weak correlations, can present a very important potential for residual confounding. For example, we have previously demonstrated how 15 socioeconomic and behavioral risk factors, each with weak but statistically independent (at <em>p</em> ≤ 0.05) associations with both vitamin C levels and coronary heart disease (CHD), could together account for an apparent strong protective effect (odds ratio = 0.60 comparing top to bottom quarter of vitamin C distribution) of vitamin C on CHD (<a href="https://gwern.net/doc/www/www.thelancet.com/a74f7f8ed2c6f8eb6e9bb9b17d2fd765b549d550.html" id="lawlor-2004" data-link-icon="L" data-link-icon-type="text" data-link-icon-color="#004582" data-url-archive="/doc/www/www.thelancet.com/a74f7f8ed2c6f8eb6e9bb9b17d2fd765b549d550.html" data-url-original="https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(04)16925-0/fulltext" data-filesize-bytes="5464992" data-filesize-percentage="84" title="'Observational versus randomized trial evidence', Lawlor et al 2004a">32</a> [see also <a href="https://gwern.net/doc/statistics/causality/2004-lawlor.pdf" id="_iw3VVuYg" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="87279" data-filesize-percentage="16" title="Those confounded vitamins: what can we learn from the differences between observational versus randomized trial evidence?"><span><span title="et al">Lawlor</span><span> et al </span><span>2004b</span></span></a>]).</p>
</blockquote>
</section>
<section id="hecht-moxley-2009">
<h2><a href="#hecht-moxley-2009" title="Link to section: § 'Hecht &amp; Moxley2009'"><span><span>Hecht &amp; Moxley</span><span>2009</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/brenthecht.com/0221a1daddb4bd1bd7760fa1bbd7dc30d9f56431.pdf" id="_weRAfeh6" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/brenthecht.com/0221a1daddb4bd1bd7760fa1bbd7dc30d9f56431.pdf" data-url-original="https://brenthecht.com/papers/cosit2009.pdf" data-filesize-bytes="704097" data-filesize-percentage="41">“Terabytes of Tobler: evaluating the first law in a massive, domain-neutral representation of world knowledge”</a>, <span><span>Hecht &amp; Moxley</span><span>2009</span></span>:</p>
<blockquote>
<p>The First Law of Geography states, “everything is related to everything else, but near things are more related than distant things.” Despite the fact that it is to a large degree what makes “spatial special”, the law has never been empirically evaluated on a large, domain-neutral representation of world knowledge. We address the gap in the literature about this critical idea by statistically examining the multitude of entities and relations between entities present across 22 different language editions of Wikipedia. We find that, at least according to the myriad authors of Wikipedia, the First Law is true to an overwhelming extent regardless of language-defined cultural domain.</p>
</blockquote>
</section>
<section id="andrew-gelman">
<h2><a href="#andrew-gelman" title="Link to section: § 'Andrew Gelman'">Andrew Gelman</a></h2>
<section id="gelman-2004">
<h2><a href="#gelman-2004" title="Link to section: § 'Gelman2004'"><span><span>Gelman</span><span>2004</span></span></a></h2>
<p><a href="https://statmodeling.stat.columbia.edu/2004/12/29/type_1_type_2_t/" id="_1T2mSs3H" data-link-icon="▅▇▃" data-link-icon-type="text,tri">“Type 1, type 2, type S, and type M errors”</a></p>
<blockquote>
<p>I’ve never in my professional life made a Type I error <em>or</em> a Type II error. But I’ve made lots of errors. How can this be?</p>
<p>A Type 1 error occurs only if the null hypothesis is true (typically if a certain parameter, or difference in parameters, equals zero). In the applications I’ve worked on, in social science and public health, I’ve never come across a null hypothesis that could actually be true, or a parameter that could actually be zero.</p>
</blockquote>
</section>
<section id="gelman-2007">
<h2><a href="#gelman-2007" title="Link to section: § 'Gelman2007'"><span><span>Gelman</span><span>2007</span></span></a></h2>
<p><a href="https://statmodeling.stat.columbia.edu/2007/10/05/significance_te/" id="_fcoQuEiQ" data-link-icon="▅▇▃" data-link-icon-type="text,tri">“Significance testing in economics: McCloskey, Ziliak, Hoover, and Siegler”</a>:</p>
<blockquote>
<p>I think that McCloskey and Ziliak, and also Hoover and Siegler, would agree with me that the null hypothesis of zero coefficient is essentially always false. (The paradigmatic example in economics is program evaluation, and I think that just about every program being seriously considered will have effects—positive for some people, negative for others—but not averaging to exactly zero in the population.) From this perspective, the point of hypothesis testing (or, for that matter, of confidence intervals) is not to assess the null hypothesis but to give a sense of the uncertainty in the inference. As Hoover and Siegler put it, “while the economic significance of the coefficient does not depend on the statistical-significance, our certainty about the accuracy of the measurement surely does. . . . Significance tests, properly used, are a tool for the assessment of signal strength and not measures of economic significance.” Certainly, I’d rather see an estimate with an assessment of statistical-significance than an estimate without such an assessment.</p>
</blockquote>
</section>
<section id="gelman-2010a">
<h2><a href="#gelman-2010a" title="Link to section: § 'Gelman2010a'"><span><span>Gelman</span><span>2010a</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/stat.columbia.edu/193cb31928dffa84cf2992bce1764921c63ace86.pdf" id="_mt2mUNNw" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/stat.columbia.edu/193cb31928dffa84cf2992bce1764921c63ace86.pdf" data-url-original="https://stat.columbia.edu/~gelman/research/published/gelman_discussion_of_efron.pdf" data-filesize-bytes="295353" data-filesize-percentage="21">“Bayesian Statistics Then and Now”</a>, <span><span>Gelman</span><span>2010a</span></span>:</p>
<blockquote>
<p>My third meta-principle is that <em>different applications demand different philosophies</em>. This principle comes up for me in Efron’s discussion of hypothesis testing and the so-called false discovery rate, which I label as “so-called” for the following reason. In Efron’s formulation (which follows the classical <a href="https://en.wikipedia.org/wiki/Multiple_comparisons_problem" id="_r85cb20T" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Multiple_comparisons_problem#bodyContent" title="Multiple comparisons problem">multiple comparisons</a> literature), a “false discovery” is a zero effect that is identified as nonzero, whereas, in my own work, I never study zero effects. The effects I study are sometimes small but it would be silly, for example, to suppose that the difference in voting patterns of men and women (after controlling for some other variables) could be exactly zero. My problems with the “false discovery” formulation are partly a matter of taste, I’m sure, but I believe they also arise from the difference between problems in genetics (in which some genes really have essentially zero effects on some traits, so that the classical hypothesis-testing model is plausible) and in social science and environmental health (where essentially everything is connected to everything else, and effect sizes follow a continuous distribution rather than a mix of large effects and near-exact zeroes).</p>
</blockquote>
</section>
<section id="gelman-2010b">
<h2><a href="#gelman-2010b" title="Link to section: § 'Gelman2010b'"><span><span>Gelman</span><span>2010b</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www.stat.columbia.edu/93660606e46677d54674aed7e28b11d49b0bfd3f.pdf" id="_6Yl2evaM" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.stat.columbia.edu/93660606e46677d54674aed7e28b11d49b0bfd3f.pdf" data-url-original="http://www.stat.columbia.edu/~gelman/research/published/causalreview4.pdf" data-filesize-bytes="76010" data-filesize-percentage="4">“Causality and Statistical Learning”</a>, <span><span>Gelman</span><span>2010b</span></span>:</p>
<blockquote>
<p><em>There are (almost) no true zeroes: difficulties with the research program of learning causal structure</em></p>
<p>We can distinguish between learning within a causal model (that is, inference about parameters characterizing a specified directed graph) and learning causal structure itself (that is, inference about the graph itself). In social science research, I am extremely skeptical of this second goal.</p>
<p>The difficulty is that, in social science, there are no true zeroes. For example, religious attendance is associated with attitudes on economic as well as social issues, and both these correlations vary by state. And it does not interest me, for example, to test a model in which social class affects vote choice through party identification but not along a direct path.</p>
<p>More generally, anything that plausibly could have an effect will not have an effect that is exactly zero. I can respect that some social scientists find it useful to frame their research in terms of conditional independence and the testing of null effects, but I don’t generally find this approach helpful—and I certainly don’t believe that it is necessary to think in terms of conditional independence in order to study causality. Without structural zeroes, it is impossible to identify graphical <a href="https://en.wikipedia.org/wiki/Structural_equation_modeling" id="_u_O3tbiW" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Structural_equation_modeling#bodyContent" title="Structural equation modeling">structural equation models</a>.</p>
<p>The most common exceptions to this rule, as I see it, are independences from design (as in a designed or <a href="https://en.wikipedia.org/wiki/Natural_experiment" id="_BjPM3vhm" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Natural_experiment#bodyContent" title="Natural experiment">natural experiment</a>) or effects that are zero based on a plausible scientific hypothesis (as might arise, for example, in genetics where genes on different chromosomes might have essentially independent effects), or in a study of ESP. In such settings I can see the value of testing a null hypothesis of zero effect, either for its own sake or to rule out the possibility of a conditional correlation that is supposed not to be there.</p>
<p>Another sort of exception to the “no zeroes” rule comes from information restriction: a person’s decision should not be affected by knowledge that he or she doesn’t have. For example, a consumer interested in buying apples cares about the total price he pays, not about how much of that goes to the seller and how much goes to the government in the form of taxes. So the restriction is that the utility depends on prices, not on the share of that going to taxes. That is the type of restriction that can help identify demand functions in economics.</p>
<p>I realize, however, that my perspective that there are no zeroes (information restrictions aside) is a minority view among social scientists and perhaps among people in general, on the evidence of psychologist Sloman’s book. For example, from chapter 2: “A good politician will know who is motivated by greed and who is motivated by larger principles in order to discern how to solicit each one’s vote when it is needed.” I can well believe that people think in this way but I don’t buy it! Just about everyone is motivated by greed and by larger principles! This sort of discrete thinking doesn’t seem to me to be at all realistic about how people behave-although it might very well be a good model about how people characterize others!</p>
<p>In the next chapter, Sloman writes, “No matter how many times A and B occur together, mere co-occurrence cannot reveal <em>whether</em> A causes B, or B causes A, or something else causes both.” [italics added] Again, I am bothered by this sort of discrete thinking. I will return in a moment with an example, but just to speak generally, if A <em>could</em> cause B, and B <em>could</em> cause A, then I would think that, yes, they could cause each other. And if something else <em>could</em> cause them both, I imagine that could be happening along with the causation of A on B and of B on A.</p>
<p>Here we’re getting into some of the differences between a normative view of science, a descriptive view of science, and a descriptive view of how people perceive the world. Just as there are limits to what “folk physics” can tell us about the motion of particles, similarly I think we have to be careful about too closely identifying “folk causal inference” from the stuff done by the best social scientists. To continue the analogy: it is interesting to study how we develop physical intuitions using commonsense notions of force, energy, momentum, and so on—but it’s also important to see where these intuitions fail. Similarly, ideas of causality are fundamental but that doesn’t stop ordinary people and even experts from making basic mistakes.</p>
<p>Now I would like to return to the graphical model approach described by Sloman. In chapter 5, he discusses an example with 3 variables:</p>
<blockquote>
<p>If two of the variables are dependent, say, intelligence and socioeconomic status, but conditionally independent given the third variable [beer consumption], then either they are related by one of two chains:</p>
<pre><code>(Intelligence → Amount of beer consumed → Socioeconomic status)
(Socio-economic status → Amount of beer consumed → Intelligence)</code></pre>
<p>or by a fork:</p>
<pre><code>                           Socioeconomic status
                         ⤴
 Amount of beer consumed
                         ⤵
                           Intelligence</code></pre>
<p>and then we must use some other means [other than observational data] to decide between these 3 possibilities. In some cases, common sense may be sufficient, but we can also, if necessary, run an experiment. If we intervene and vary the amount of beer consumed and see that we affect intelligence, that implies that the second or third model is possible; the first one is not. Of course, all this assumes that there aren’t other variables mediating between the ones shown that provide alternative explanations of the dependencies.</p>
</blockquote>
<p>This makes no sense to me. I don’t see why only one of the 3 models can be true. This is a mathematical possibility, but it seems highly implausible to me. And, in particular, running an experiment that reveals one of these causal effects does <em>not</em> rule out the other possible paths. For example, suppose that Sloman were to perform the above experiment (finding that beer consumption affects intelligence) and then <em>another</em> experiment, this time varying intelligence (in some way; the method of doing this can very well determine the causal effect) and finding that it affects the amount of beer consumed.</p>
<p>Beyond this fundamental problem, I have a statistical critique, which is that in social science you won’t have these sorts of conditional independencies, except from design or as artifacts of small sample sizes that do not allow us to distinguish small dependencies from zero.</p>
<p>I think I see where Sloman is coming from, from a psychological perspective: you see these variables that are related to each other, and you want to know which is the cause and which is the effect. But I don’t think this is a useful way of understanding the world, just as I don’t think it’s useful to categorize political players as being motivated either by greed or by larger principles, but not both. Exclusive-or might feel right to us internally, but I don’t think it works as science.</p>
<p>One important place where I agree with Sloman (and thus with Pearl and Sprites et al.) is in the emphasis that causal structure cannot in general be learned from observational data alone; they hold the very reasonable position that we can use observational data to rule out possibilities and formulate hypotheses, and then use some sort of intervention or experiment (whether actual or hypothetical) to move further. In this way they connect the observational/experimental division to the hypothesis/deduction formulation that is familiar to us from the work of Popper, Kuhn, and other modern philosophers of science.</p>
<p>The place where I think Sloman is misguided is in his formulation of scientific models in an either/or way, as if, in truth, social variables are linked in simple causal paths, with a scientific goal of figuring out if A causes B or the reverse. I don’t know much about intelligence, beer consumption, and socioeconomic status, but I certainly don’t see any simple relationships between income, religious attendance, party identification, and voting—and I don’t see how a search for such a pattern will advance our understanding, at least given current techniques. I’d rather start with description and then go toward causality following the approach of economists and statisticians by thinking about potential interventions one at a time. I’d love to see Sloman’s and Pearl’s ideas of the interplay between observational and experimental data developed in a framework that is less strongly tied to the notion of choice among simple causal structures.</p>
</blockquote>
</section>
<section id="gelman-2012">
<h2><a href="#gelman-2012" title="Link to section: § 'Gelman2012'"><span><span>Gelman</span><span>2012</span></span></a></h2>
<p><a href="https://statmodeling.stat.columbia.edu/2012/03/16/hot-hand-debate-is-warming-up/" id="_AnBciugt" data-link-icon="▅▇▃" data-link-icon-type="text,tri">“The”hot hand” and problems with hypothesis testing”</a>, <span><span>Gelman</span><span>2012</span></span>:</p>
<blockquote>
<p>The effects are certainly not zero. We are not machines, and anything that can affect our expectations (for example, our success in previous tries) should affect our performance…Whatever the latest results on particular sports, I can’t see anyone overturning the basic finding of Gilovich, Vallone, and Tversky that players and spectators alike will <em>perceive</em> the hot hand even when it does not exist and dramatically <em>overestimate</em> the magnitude and consistency of any hot-hand phenomenon that does exist. In summary, this is yet another problem where much is lost by going down the standard route of null hypothesis testing.</p>
</blockquote>
</section>
<section id="gelman-et-al-2013">
<h2><a href="#gelman-et-al-2013" title="Link to section: § 'Gelman et al 2013'"><span><span title="et al">Gelman</span><span> Et Al </span><span>2013</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www.stat.columbia.edu/60e7c1c3045b451638455140757bd9f43a6005e2.pdf" id="_no1ItuWq" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.stat.columbia.edu/60e7c1c3045b451638455140757bd9f43a6005e2.pdf" data-url-original="http://www.stat.columbia.edu/~gelman/research/published/GRR18.pdf" data-filesize-bytes="217500" data-filesize-percentage="16">“Inherent difficulties of non-Bayesian likelihood-based inference, as revealed by an examination of a recent book by Aitkin”</a> (<a href="https://gwern.net/doc/www/www.stat.columbia.edu/7ce2ebe4b0087248eeea99ee1881e5f70aa311e6.pdf" id="_pgpeRBzi" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/www.stat.columbia.edu/7ce2ebe4b0087248eeea99ee1881e5f70aa311e6.pdf" data-url-original="http://www.stat.columbia.edu/~gelman/research/unpublished/GRR16.pdf" data-filesize-bytes="206900" data-filesize-percentage="15">earlier version</a>):</p>
<blockquote>
<ol start="7" type="1">
<li><p><em>Solving non-problems</em></p></li>
</ol>
<p>Several of the examples in <em>Statistical Inference</em> represent solutions to problems that seem to us to be artificial or conventional tasks with no clear analogy to applied work.</p>
<blockquote>
<p>“They are artificial and are expressed in terms of a survey of 100 individuals expressing support (Yes/No) for the president, before and after a presidential address (…) The question of interest is whether there has been a change in support between the surveys (…). We want to assess the evidence for the hypothesis of equality <em>H</em><sub>1</sub> against the alternative hypothesis <em>H</em><sub>2</sub> of a change.” —<em>Statistical Inference</em>, page 147</p>
</blockquote>
<p>Based on our experience in public opinion research, this is not a real question. Support for any political position is always changing. The real question is how much the support has changed, or perhaps how this change is distributed across the population.</p>
<p>A defender of Aitkin (and of classical hypothesis testing) might respond at this point that, yes, everybody knows that changes are never exactly zero and that we should take a more “grown-up” view of the null hypothesis, not that the change is zero but that it is nearly zero. Unfortunately, the metaphorical interpretation of hypothesis tests has problems similar to the theological doctrines of the Unitarian church. Once you have abandoned literal belief in the Bible, the question soon arises: why follow it at all? Similarly, once one recognizes the inappropriateness of the point null hypothesis, we think it makes more sense not to try to rehabilitate it or treat it as treasured metaphor but rather to attack our statistical problems directly, in this case by performing inference on the change in opinion in the population.</p>
<p>To be clear: we are not denying the value of hypothesis testing. In this example, we find it completely reasonable to ask whether observed changes are statistically-significant, i.e. whether the data are consistent with a null hypothesis of zero change. What we do not find reasonable is the statement that “the question of interest is whether there has been a change in support.”</p>
<p>All this is application-specific. Suppose public opinion was observed to really be flat, punctuated by occasional changes, as in the left graph in Figure 7.1. In that case, Aitkin’s question of “whether there has been a change” would be well-defined and appropriate, in that we could interpret the null hypothesis of no change as some minimal level of baseline variation.</p>
<p>Real public opinion, however, does not look like baseline noise plus jumps, but rather shows continuous movement on many time scales at once, as can be seen from the right graph in Figure 7.1, which shows actual presidential approval data. In this example, we do not see Aitkin’s question as at all reasonable. Any attempt to work with a null hypothesis of opinion stability will be inherently arbitrary. It would make much more sense to model opinion as a continuously-varying process. The statistical problem here is not merely that the null hypothesis of zero change is nonsensical; it is that the null is in no sense a reasonable approximation to any interesting model. The sociological problem is that, from <a href="#savage-1954">Savage (<span>1954<sub><span title="1954 was 71 years ago.">71ya</span></sub></span>)</a> onward, many Bayesians have felt the need to mimic the classical null-hypothesis testing framework, even where it makes no sense.</p>
</blockquote>
</section>
</section>
<section id="lin-et-al-2013">
<h2><a href="#lin-et-al-2013" title="Link to section: § 'Lin et al 2013'"><span><span title="et al">Lin</span><span> Et Al </span><span>2013</span></span></a></h2>
<p><a href="https://www.galitshmueli.com/system/files/Print%20Version.pdf" id="_TMFRsqiq" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" title="Site map">“Too Big to Fail: Large Samples and the <em>p</em>-Value Problem”</a>, <span><span title="et al">Lin</span><span> et al </span><span>2013</span></span>:</p>
<blockquote>
<p>The Internet has provided IS researchers with the opportunity to conduct studies with extremely large samples, frequently well over 10,000 observations. There are many advantages to large samples, but researchers using statistical inference must be aware of the <em>p</em>-value problem associated with them. In very large samples, <em>p</em>-values go quickly to zero, and solely relying on <em>p</em>-values can lead the researcher to claim support for results of no practical significance. In a survey of large sample IS research, we found that a significant number of papers rely on a low <em>p</em>-value and the sign of a regression coefficient alone to support their hypotheses. This research commentary recommends a series of actions the researcher can take to mitigate the <em>p</em>-value problem in large samples and illustrates them with an example of over 300,000 camera sales on eBay. We believe that addressing the <em>p</em>-value problem will increase the credibility of large sample IS research as well as provide more insights for readers.</p>
<p>…A key issue with applying small-sample statistical inference to large samples is that even minuscule effects can become statistically-significant. The increased power leads to a dangerous pitfall as well as to a huge opportunity. The issue is one that statisticians have long been aware of: “the <em>p</em>-value problem.” Chatfield (<span>1995<sub><span title="1995 was 30 years ago.">30ya</span></sub></span>, p.&nbsp;70 [<em>Problem Solving: A Statistician’s Guide, 2<sup>nd</sup> ed</em>]) comments, “The question is not whether differences are ‘significant’ (they nearly always are in large samples), but whether they are interesting. Forget statistical-significance, what is the practical significance of the results?” The increased power of large samples means that researchers can detect smaller, subtler, and more complex effects, but relying on <em>p</em>-values alone can lead to claims of support for hypotheses of little or no practical significance.</p>
<p>…In reviewing the literature, we found only a few mentions of the large-sample issue and its effect on <em>p</em>-values; we also saw little recognition that the authors’ low <em>p</em>-values might be an artifact of their large-sample sizes. Authors who recognized the “large-sample, small <em>p</em>-values” issue addressed it by one of the following approaches: reducing the significance level threshold<sup>5</sup> (which does not really help), by recomputing the <em>p</em>-value for a small sample (Gefen and <span><span>Carmel</span><span>2008</span></span>), or by focusing on practical significance and commenting about the uselessness of statistical-significance (<a href="https://gwern.net/doc/www/terpconnect.umd.edu/47df2c4ced4e178de4bad4630e42e69b863d3c71.pdf" id="_qOPgNhUs" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/terpconnect.umd.edu/47df2c4ced4e178de4bad4630e42e69b863d3c71.pdf" data-url-original="https://terpconnect.umd.edu/~smithas/papers/mithaslucas2010ms.pdf" data-filesize-bytes="199929" data-filesize-percentage="15" title="Are foreign IT workers cheaper? US visa policies and compensation of information technology professionals">Mithas and <span><span>Lucas</span><span>2010</span></span></a>).</p>
</blockquote>
</section>
<section id="schwitzgebel-2013">
<h2><a href="#schwitzgebel-2013" title="Link to section: § 'Schwitzgebel2013'"><span><span>Schwitzgebel</span><span>2013</span></span></a></h2>
<p><a href="https://philosophycommons.typepad.com/xphi/2013/02/preliminary-evidence-that-the-world-is-simple-an-exercise-in-stupid-epistemology.html" id="_mMWLLtZL">“Preliminary Evidence That the World Is Simple (An Exercise in Stupid Epistemology)”</a> (humorous blog post)</p>
<blockquote>
<p>Here’s what I did. I thought up 30 pairs of variables that would be easy to measure and that might relate in diverse ways. Some variables were physical (the distance versus apparent brightness of nearby stars), some biological (the length versus weight of sticks found in my back yard), and some psychological or social (the S&amp;P 500 index closing value versus number of days past). Some I would expect to show no relationship (the number of pages in a library book versus how high up it is shelved in the library), some I would expect to show a roughly linear relationship (distance of McDonald’s franchises from my house versus <a href="https://en.wikipedia.org/wiki/MapQuest" id="_zALzj9Ko" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/MapQuest#bodyContent" title="MapQuest">MapQuest</a> estimated driving time), and some I expected to show a curved or complex relationship (forecasted temperature versus time of day, size in KB of a JPG photo of my office versus the angle at which the photo was taken). <a href="https://gwern.net/doc/www/schwitzsplintersunderblog.blogspot.com/aa86fa2004e20effba2df61d144affd0891a352a.html" id="_OWI49Lia" data-url-archive="/doc/www/schwitzsplintersunderblog.blogspot.com/aa86fa2004e20effba2df61d144affd0891a352a.html" data-url-original="https://schwitzsplintersunderblog.blogspot.com/2013/02/variables-measured-for-post-preliminary.html" data-filesize-bytes="1626708" data-filesize-percentage="59" title="Variables Measured for the Post 'Preliminary Evidence That the World Is Simple'">See here</a> for the full list of variables. I took 11 measurements of each variable pair. Then I analyzed the resulting data.</p>
<p>Now, if the world is massively complex, then it should be difficult to predict a third datapoint from any two other data points. Suppose that two measurements of some continuous variable yield values of 27 and 53. What should I expect the third measured value to be? Why not 1,457,002? Or 3.22 × 10<sup>−17</sup>? There are just as many functions (that is, infinitely many) containing 27, 53, and 1,457,002 as there are containing 27, 53, and some more pedestrian-seeming value like 44.</p>
<p>…To conduct the test, I used each pair of dependent variables to predict the value of the next variable in the series (the 1<sup>st</sup> and 2<sup>nd</sup> observations predicting the value of the 3<sup>rd</sup>, the 2<sup>nd</sup> and 3<sup>rd</sup> predicting the value of the 4<sup>th</sup>, etc.), yielding 270 predictions for the 30 variables. I counted an observation “wild” if its absolute value was 10 times the maximum of the absolute value of the two previous observations or if its absolute value was below <span>1⁄10</span> of the minimum of the absolute value of the two previous observations. Separately, I also looked for flipped signs (either two negative values followed by a positive or two positive values followed by a negative), though most of the variables only admitted positive values. This measure of wildness yielded 3 wild observations out of 270 (1%) plus another 3 flipped-sign cases (total 2%). (A few variables were capped, either top or bottom, in a way that would make an above-10x or below-1/10<sup>th</sup> observation analytically unlikely, but excluding such variables wouldn’t affect the result much.) So it looks like the Wild Complexity Thesis might be in trouble.</p>
</blockquote>
</section>
<section id="ellenberg-2014">
<h2><a href="#ellenberg-2014" title="Link to section: § 'Ellenberg2014'"><span><span>Ellenberg</span><span>2014</span></span></a></h2>
<p>Jordan Ellenberg, <a href="https://deadspin.com/the-myth-of-the-myth-of-the-hot-hand-1588112937/" id="_oYIHLSa_">“The Myth Of The Myth Of The Hot Hand”</a> (excerpted from <em>How Not to Be Wrong: The Power of Mathematical Thinking</em>, <span>2014<sub><span title="2014 was 11 years ago.">11ya</span></sub></span>):</p>
<blockquote>
<p>A significance test is a scientific instrument, and like any other instrument, it has a certain degree of precision. If you make the test more sensitive—by increasing the size of the studied population, for example—you enable yourself see ever-smaller effects. That’s the power of the method, but also its danger. The truth is, the null hypothesis is probably <em>always</em> false! When you drop a powerful drug into a patient’s bloodstream, it’s hard to believe the intervention literally has zero effect on the probability that the patient will develop esophageal cancer, or thrombosis, or bad breath. Each part of the body speaks to every other, in a complex feedback loop of influence and control. Everything you do either gives you cancer or prevents it. And in principle, if you carry out a powerful enough study, you can find out which it is. But those effects are usually so minuscule that they can be safely ignored. Just because we can detect them doesn’t always mean they matter…The right question isn’t, “Do basketball players sometimes temporarily get better or worse at making shots?”—the kind of yes/no question a significance test addresses. The right question is “How <em>much</em> does their ability vary with time, and to what extent can observers detect in real time whether a player is hot?” Here, the answer is surely “not as much as people think, and hardly at all.”</p>
</blockquote>
</section>
<section id="lakens-2014">
<h2><a href="#lakens-2014" title="Link to section: § 'Lakens2014'"><span><span>Lakens</span><span>2014</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/daniellakens.blogspot.com/92fba058c348a440c26d2952267e27fa62c87f6e.html" id="_NPjSj1Yl" data-url-archive="/doc/www/daniellakens.blogspot.com/92fba058c348a440c26d2952267e27fa62c87f6e.html" data-url-original="https://daniellakens.blogspot.com/2014/06/the-null-is-always-false-except-when-it.html" data-filesize-bytes="600011" data-filesize-percentage="38" title="The 20% Statistician: The Null Is Always False (Except When It Is True)">“The Null Is Always False (Except When It Is True)”</a>, Daniel Lakens:</p>
<blockquote>
<p>The more important question is whether it is true that there are always real differences in the real world, and what the ‘real world’ is. Let’s consider the population of people in the real world. While you read this sentence, some individuals in this population have died, and some were born. For most questions in psychology, the population is surprisingly similar to an eternally running Monte Carlo simulation. Even if you could measure all people in the world in a millisecond, and the test-retest correlation was perfect, the answer you would get now would be different from the answer you would get in an hour. Frequentists (the people that use NHST) are not specifically interested in the exact value now, or in one hour, or next week Thursday, but in the average value in the ‘long’ run. The value in the real world today might never be zero, but it’s never anything, because it’s continuously changing. If we want to make generalizable statements about the world, I think the fact that the null-hypothesis is never precisely true at any specific moment is not a problem. I’ll ignore more complex questions for now, such as how we can establish whether effects vary over time.</p>
<p>…Meehl talks about how in psychology every individual-difference variable (eg. trait, status, demographic) correlates with every other variable, which means the null is practically never true. In these situations, it’s not that testing against the null-hypothesis is meaningless, but it’s not informative. If everything correlates with everything else, you need to create good models, and test those. A simple null-hypothesis significance test will not get you very far. I agree.</p>
<p><strong><em>Random Assignment versus Crud</em></strong></p>
<p>To illustrate when NHST can be used to as a source of information in large samples, and when NHST is not informative in large samples, I’ll analyze data of large dataset with 6344 participants from the Many Labs project. I’ve analyzed 10 dependent variables to see whether they were influenced by (1) Gender, and (2) Assignment to the high or low anchoring condition in the first study. Gender is a measured individual difference variable, and not a manipulated variable, and might thus be affected by what Meehl calls the crud factor. Here, I want to illustrate this is (1) probably often true for individual difference variables, but perhaps not always true, and (2) it is probably never true for when analyzing differences between groups individuals were randomly assignment to.</p>
<p>…When we analyze the 10 dependent variables as a function of the anchoring condition, none of the differences are statistically-significant (even though there are more than 6000 participants). You can play around with the script, repeating the analysis for the conditions related to the other 3 anchoring questions (remember to correct for multiple comparisons if you perform many tests), and see how randomization does a pretty good job at returning non-statistically-significant results even in very large sample sizes. If the null is always false, it is remarkably difficult to reject. Obviously, when we analyze the answer people gave on the first anchoring question, we find a huge effect of the high versus low anchoring condition they were randomly assigned to. Here, NHST works. There is probably something going on. If the anchoring effect was a completely novel phenomenon, this would be an important first finding, to be followed by replications and extensions, and finally model building and testing.</p>
<p>The results change dramatically if we use Gender as a factor. There are Gender effects on dependent variables related to quote attribution, system justification, the gambler’s fallacy, imagined contact, the explicit evaluation of arts and math, and the norm of reciprocity. There are no significant differences in political identification (as conservative or liberal), on the response scale manipulation, or on gain versus loss framing (even though <em>p</em> = 0.025, such a high <em>p</em>-value is stronger support for the null-hypothesis than for the alternative hypothesis with 5500 participants). It’s surprising that the null-hypothesis (gender does not influence the responses participants give) is rejected for 7 out of 10 effects. Personally (perhaps because I’ve got very little expertise in gender effects) I was actually extremely surprised, even though the effects are small (with Cohen <em>d</em>’s or around 0.09). This, ironically, shows that NHST works—I’ve learned gender effects are much more widespread than I’d have though before I wrote this blog post.</p>
</blockquote>
</section>
<section id="kirkegaard-2014">
<h2><a href="#kirkegaard-2014" title="Link to section: § 'Kirkegaard2014'"><span><span>Kirkegaard</span><span>2014</span></span></a></h2>
<p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.669.378&amp;rep=rep1&amp;type=pdf" id="__p8dCFOz" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02">“The international general socioeconomic factor: Factor analyzing international rankings”</a>:</p>
<blockquote>
<p>Many studies have examined the correlations between national IQs and various country-level indexes of well-being. The analyses have been unsystematic and not gathered in one single analysis or dataset. In this paper I gather a large sample of country-level indexes and show that there is a strong general socioeconomic factor (S factor) which is highly correlated (.86–.87) with national cognitive ability using either Lynn and Vanhanen’s dataset or Altinok’s. Furthermore, the method of correlated vectors shows that the correlations between variable loadings on the S factor and cognitive measurements are .99 in both datasets using both cognitive measurements, indicating that it is the S factor that drives the relationship with national cognitive measurements, not the remaining variance.</p>
</blockquote>
<p>See also <a href="https://www.npr.org/sections/goatsandsoda/2019/06/14/730257541/countries-are-ranked-on-everything-from-health-to-happiness-whats-the-point" id="_WCO12Fz0" data-link-icon="npr" data-link-icon-type="text,tri,sans" data-link-icon-color="#237bbd" title="International Rankings: What Are The Benefits And Drawbacks?">“Countries Are Ranked On Everything From Health To Happiness. What’s The Point?”</a>:</p>
<blockquote>
<p>It’s a brand new ranking. Called the Sustainable Development Goals Gender Index, it gives 129 countries a score for progress on achieving gender equality by 2030. Here’s the quick summary: Things are “good” in much of Europe and North America. And “very poor” in much of sub-Saharan Africa. In fact, that’s the way it looks in many international rankings, which tackle everything from the worst places to be a child to the most corrupt countries to world happiness…As for the fact that many rankings look the same at the top and bottom, one reason has to do with money. Many indexes are correlated with GDP per capita, a measure of a country’s prosperity, says Kenny. That includes the World Bank’s Human Capital Index, which measures the economic productivity of a country’s young people; and Freedom House’s Freedom in the World index, which ranks the world by its level of democracy, including economic freedom. And countries that have more money can spend more money on health, education and infrastructure.</p>
</blockquote>
</section>
<section id="shen-et-al-2014">
<h2><a href="#shen-et-al-2014" title="Link to section: § 'Shen et al 2014'"><span><span title="et al">Shen</span><span> Et Al </span><span>2014</span></span></a></h2>
<p><a href="https://gwern.net/doc/psychology/2014-shen.pdf" id="shen-et-al-2014-link" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="101042" data-filesize-percentage="18" title="'When Correcting for Unreliability of Job Performance Ratings, the Best Estimate Is Still 0.52', Shen et al 2014">“When Correcting for Unreliability of Job Performance Ratings, the Best Estimate Is Still 0.52”</a>, <span><span title="et al">Shen</span><span> et al </span><span>2014</span></span>:</p>
<blockquote>
<p><strong>Is Too Much Variance Explained?</strong> It is interesting that historically the I-O literature has bemoaned the presence of a “validity ceiling”, and the field seemed to be unable to make large gains in the prediction of job performance (Highhouse, <span>2008<sub><span title="2008 was 17 years ago.">17ya</span></sub></span>). In contrast, LeBreton et al. appear to have the opposite concern—that we maybe able to predict too much, perhaps even all, of the variance in job performance once accounting for statistical artifacts. In addition to their 4 focal predictors (ie. GMA, integrity, structured interview, work sample), LeBreton et al. list an additional 24 variables that have been shown to be related to job performance meta-analytically. However, we believe that many of the variables LeBreton et al. included in their list are variables that Sackett, Borneman, and Connelly (<span>2009<sub><span title="2009 was 16 years ago.">16ya</span></sub></span>) would argue are likely unknowable at time of hire.</p>
<p>…Furthermore, in contrast to LeBreton et al.’s assertion that organizational variables, such as procedural justice, are likely unrelated to their focal predictors, our belief is that many of these variables are likely to be at least moderately correlated–limiting the incremental validity we could expect with the inclusion of these additional variables. For example, research has shown that integrity tests mostly tap into <a href="https://en.wikipedia.org/wiki/Conscientiousness#Personality_models" id="_LpC9lLj1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Conscientiousness#bodyContent" title="Conscientiousness § Personality models">Conscientiousness</a>, <a href="https://en.wikipedia.org/wiki/Agreeableness" id="__Qr-IYeG" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Agreeableness#bodyContent" title="Agreeableness">Agreeableness</a>, and Emotional Stability (Ones &amp; Viswesvaran, <span>2001<sub><span title="2001 was 24 years ago.">24ya</span></sub></span>), and a recent <a href="https://en.wikipedia.org/wiki/Meta-analysis" id="_tKvFlpVk" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Meta-analysis#bodyContent" title="Meta-analysis">meta-analysis</a> of organizational justice shows that all 3 personality traits are moderately related to one’s experience of procedural justice (ρ=0.19–0.23; <span><span title="et al">Hutchinson</span><span> et al </span><span>2014</span></span><span>), suggesting that even apparently unrelated variables can share a surprising amount of construct-level variance. In support of this perspective, Paterson, Harms, and Crede (<span>2012<sub><span title="2012 was 13 years ago.">13ya</span></sub></span>) [</span>“The meta of all metas: 30 years of meta-analysis reviewed”] conducted a meta-analysis of over 200 meta-analyses and found an average correlation of 0.27, suggesting that most variables we study are at least somewhat correlated and validating the first author’s long-held personal assumption that the world is correlated 0.30 (on average; see also <a href="#meehl-1990-1">Meehl’s, 1990</a>, crud factor)!</p>
</blockquote>
</section>
<section id="gordon-et-al-2019">
<h2><a href="#gordon-et-al-2019" title="Link to section: § 'Gordon et al 2019'"><span><span title="et al">Gordon</span><span> Et Al </span><span>2019</span></span></a></h2>
<p><a href="https://gwern.net/doc/statistics/causality/2019-gordon.pdf" id="gordon-et-al-2019-2" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2814049" data-filesize-percentage="88" title="'A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook', Gordon et al 2019">“A Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook”</a>, <span><span title="et al">Gordon</span><span> et al </span><span>2019</span></span>:</p>
<blockquote>
<p>We examine how common techniques used to measure the causal impact of ad exposures on users’ conversion outcomes compare to the “gold standard” of a true experiment (randomized controlled trial). Using data from 12 US advertising lift studies at Facebook comprising 435 million user-study observations and 1.4 billion total impressions we contrast the experimental results to those obtained from observational methods, such as comparing exposed to unexposed users, matching methods, model-based adjustments, synthetic matched-markets tests, and before-after tests. We show that observational methods often fail to produce the same results as true experiments even after conditioning on information from thousands of behavioral variables and using non-linear models. We explain why this is the case. Our findings suggest that common approaches used to measure advertising effectiveness in industry fail to measure accurately the true effect of ads.</p>
<p>An important input to <a href="https://en.wikipedia.org/wiki/Propensity_score_matching" id="_KiZ5d_hD" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Propensity_score_matching#bodyContent" title="Propensity score matching">propensity score matching</a> (PSM) is the set of variables used to predict the propensity score itself. We tested 3 different PSM specifications for study 4, each of which used a larger set of inputs.</p>
<ol type="1">
<li><p><strong>PSM 1</strong>: In addition to age and gender, the basis of our exact matching (EM) approach, this specification uses common Facebook variables, such as how long users have been on Facebook, how many Facebook friends the have, their reported relationship status, and their phone OS, in addition to other user characteristics.</p></li>
<li><p><strong>PSM 2</strong>: In addition to the variables in PSM 1, this specification uses Facebook’s estimate of the user’s zip code of residence to associate with each user nearly 40 variables drawn from the most recent Census and American Communities Surveys (ACS).</p></li>
<li><p><strong>PSM 3</strong>: In addition to the variables in PSM 2, this specification adds a composite metric of Facebook data that summarizes thousands of behavioral variables. This is a machine-learning based metric used by Facebook to construct target audiences that are similar to consumers that an advertiser has identified as desirable.<sup><a href="https://www.facebook.com/business/help/164749007013531" id="_L-BscyZP" data-link-icon="facebook" data-link-icon-type="svg" data-link-icon-color="#1877f2" title="About Lookalike Audiences: A Lookalike Audience is a way to reach new people who are likely to be interested in your business because they're similar to your best existing customers.">16</a></sup> Using this metric bases the estimation of our propensity score on a non-linear machine-learning model with thousands of features.<sup>17</sup></p></li>
</ol>
<p>…When we go from exact matching (EM) to our most parsimonious propensity score matching model (PSM 1), the conversion rate for unexposed users increases from 0.032% to 0.042%, decreasing the implied advertising lift from 221% to 147%. PSM 2 performs similarly to PSM 1, with an implied lift of 154%.<sup>21</sup> Finally, adding the composite measure of Facebook variables in PSM 3 improves the fit of the propensity model (as measured by a higher AUC/ROC) and further increases the conversion rate for matched unexposed users to 0.051%. The result is that our best performing PSM model estimates an advertising lift of 102%…We summarize the result of all our propensity score matching and regression methods for study 4 in Figure 7.</p>
<figure>
<p><img alt="Figure 7: Summary of lift estimates and confidence intervals." data-aspect-ratio="1105 / 861" decoding="async" height="861" loading="lazy" src="https://gwern.net/doc/statistics/2016-gordon-figure7-propensityscoringresults.png" width="1105"></p>
<figcaption><p><strong>Figure 7</strong>: Summary of lift estimates and confidence intervals.</p></figcaption>
</figure>
</blockquote>
<p>While not directly testing statistical-significance in its propensity scoring, the increasing accuracy in estimating the true causal effect of adding in additional behavioral variables implies that (especially at Facebook-scale, using billions of datapoints) the correlations of the thousands of used variables with the advertising behavior would be statistically-significant and demonstrate that everything is correlated. (See also my <a href="https://gwern.net/banner" id="gwern-banner" data-filesize-bytes="155867" data-filesize-percentage="65" title="'Banner Ads Considered Harmful', Gwern 2017">ad harms</a> &amp; <a href="https://gwern.net/correlation" id="gwern-correlation" data-filesize-bytes="144066" data-filesize-percentage="63" title="'How Often Does Correlation=Causality?', Gwern 2014">“How Often Does Correlation=Causality?”</a> pages.)</p>
</section>
<section id="kirkegaard-2020">
<h2><a href="#kirkegaard-2020" title="Link to section: § 'Kirkegaard2020'"><span><span>Kirkegaard</span><span>2020</span></span></a></h2>
<p><a href="https://emilkirkegaard.dk/en/2020/03/enhancing-archival-datasets-with-machine-learned-psychometrics/" id="_tR6rAS91" title="Enhancing archival datasets with machine learned psychometrics">“Enhancing archival datasets with machine learned psychometrics”</a>, <span><span>Kirkegaard</span><span>2020</span></span> (published as <a href="https://gwern.net/doc/iq/2021-kirkegaard.pdf" id="kirkegaard-nyborg-2021" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="492309" data-filesize-percentage="58" title="Intelligence and General Psychopathology in the Vietnam Experience Study: A Closer Look"><span><span>Kirkegaard &amp; Nyborg</span><span>2021</span></span></a>):</p>
<blockquote>
<p>In our ISIR 2019 presentation (<a href="https://docs.google.com/presentation/d/1xxfYTWP2R0ZFvbI24sR1jVyFC_qN_Lw85782zE5Q2Jo/edit" id="_9uNcP5Wd" data-link-icon="word-doc" data-link-icon-type="svg" data-link-icon-color="#4285f4" title="Machine learning psychometrics: ISIR 2019">“Machine learning psychometrics: Improved cognitive ability validity from supervised training on item level data”</a>), we showed that one can use machine learning on cognitive data to improve the predictive validity of it. The effect sizes can be quite large, eg. one could predict educational attainment in the Vietnam Experience Study (VES) sample (<em>n</em> = 4.5k US army recruits) at R<sup>2</sup>=32.3% with <a href="https://en.wikipedia.org/wiki/Ridge_regression" id="_uewD694v" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Ridge_regression#bodyContent" title="Ridge regression">ridge regression</a> versus 17.7% with <a href="https://en.wikipedia.org/wiki/Item_response_theory" id="_ztWiV6Yy" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Item_response_theory#bodyContent" title="Item response theory">IRT</a>. Prediction is more than <em>g</em>, after all. What if we had a dataset of 185 diverse items, and we train the model to predict IRT-based <em>g</em> from the full set, but using only a limited set using the LASSO? How many items do we need when optimally weighted? Turns out that with 42 items, one can get a test that correlates at 0.96 with the full <em>g</em>. That’s an abbreviation of nearly 80%!</p>
<p>Now comes the fancy part. What if we have archival datasets with only a few cognitive items (eg. datasets with <a href="https://en.wikipedia.org/wiki/Mini%E2%80%93mental_state_examination" id="_2RpeWFt8" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Mini%E2%80%93mental_state_examination#bodyContent" title="Mini–mental state examination">MMSE</a> items) or maybe even no items. Can we improve things here? Maybe! If the dataset has a lot of other items, we may be able to train an machine learning (ML) model that predict <em>g</em> quite well from them, even if they seem unrelated. Every item has some variance overlap with <em>g</em> however small (crud factor), it is only a question of having a good enough algorithm and enough data to exploit this covariance. For instance, I have found that if one uses the 556 items in the <a href="https://en.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory" id="_Tr4qHl-1" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory#bodyContent" title="Minnesota Multiphasic Personality Inventory">MMPI</a> in the VES to predict the very well measured <em>g</em> based on all the cognitive data (18 tests), how well can one do? I was surprised to learn that one can do <em>extremely</em> well:</p>
<p><a href="https://gwern.net/doc/iq/2020-kirkegaard-elasticnetpredictionofiqfrommmpiintheves.png" id="_T0P3KUgb" data-link-icon="image" data-link-icon-type="svg" data-filesize-bytes="72272" data-filesize-percentage="13" data-image-height="1080" data-image-width="1464" data-aspect-ratio="1400 / 1033">“Elastic net prediction of <em>g</em>: <em>r</em> = 0.83 (0.82–0.84), <em>n</em> = 4,320”</a></p>
<p>[There are 203 (elastic)/217 (<a href="https://en.wikipedia.org/wiki/Lasso_(statistics)" id="_Zk9ZgDXA" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Lasso_(statistics)#bodyContent" title="Lasso (statistics)">lasso</a>) non-zero coefficients out of 556]</p>
<p>Thus, one can measure <em>g</em> as well as one could with a decent test like Wonderlic, or Raven’s without having any cognitive data at all! The big question here is whether these models generalize well. If one can train a model to predict <em>g</em> from MMPI items in dataset 1, and then apply it to dataset 2 without much loss of accuracy, this means that one could impute <em>g</em> in potentially thousands of old archival datasets that include the same MMPI items, or a subset of them.</p>
</blockquote>
<p>A similar analysis is done by <span><span title="et al">Revelle</span><span> et al </span><span>2020</span></span>’s <a href="https://www.sciencedirect.com/science/article/pii/S0191886920300945" id="revelle-et-al-2020" data-link-icon="E" data-link-icon-type="text" data-link-icon-color="#eb6500" title="'Exploring the persome: The power of the item in understanding personality structure', Revelle et al 2020">“Exploring the persome: The power of the item in understanding personality structure”</a> (especially “Study 4: Profile correlations using 696 items”); they do not directly report an equivalent to posteriors/<em>p</em>-values or non-zero correlations after penalized regression or anything like that, but the pervasiveness of correlation is apparent from their results &amp; data visualizations.</p>
</section>
<section id="ferguson-heene-2021">
<h2><a href="#ferguson-heene-2021" title="Link to section: § 'Ferguson &amp; Heene2021'"><span><span>Ferguson &amp; Heene</span><span>2021</span></span></a></h2>
<p><a href="https://gwern.net/doc/psychology/2021-ferguson.pdf" id="ferguson-heene-2021" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="101899" data-filesize-percentage="18" title="'Providing a lower-bound estimate for psychology’s “crud factor”: The case of aggression', Ferguson &amp; Heene 2021">“Providing a Lower-Bound Estimate for Psychology’s ‘Crud Factor’: The Case of Aggression”</a>, <span><span>Ferguson &amp; Heene</span><span>2021</span></span>:</p>
<blockquote>
<p>When conducting research on large data sets, statistically-significant findings having only trivial interpretive meaning may appear. Little consensus exists whether such small effects can be meaningfully interpreted. The current analysis examines the possibility that trivial effects may emerge in large datasets, but that some such effects may lack interpretive value. When such results match an investigator’s hypothesis, they may be over-interpreted.</p>
<p>The current study examines this issue as related to aggression research in 2 large samples. Specifically, in the first study, the National Longitudinal Study of Adolescent to Adult Health (Add Health) dataset was used. 15 variables with little theoretical relevance to aggression were selected, then correlated with self-reported delinquency. For the second study, the Understanding Society database was used. As with Study 1, 14 nonsensical variables were correlated with conduct problems.</p>
<p>Many variables achieved “statistical-significance” and some effect-sizes approached or exceeded <em>r</em> = 0.10, despite little theoretical relevance between the variables.</p>
<p>It is recommended that effect sizes below <em>r</em> = 0.10 should not be interpreted as hypothesis supportive.</p>
<figure>
<p><img alt="Table 1: Correlations Between Crud and Delinquency for Study 1" data-aspect-ratio="700 / 611" decoding="async" height="1222" loading="lazy" src="https://gwern.net/doc/psychology/2021-ferguson-table1-correlationsbetweenirrelevantfactorsandjuveniledelinquencyinaddhealth.png" width="1400"></p>
<figcaption><p><strong>Table 1</strong>: Correlations Between Crud and Delinquency for Study 1</p></figcaption>
</figure>
<figure>
<p><img alt="Table 2: Correlations Between Crud and Conduct Problems for Study 2" data-aspect-ratio="1400 / 911" decoding="async" height="911" loading="lazy" src="https://gwern.net/doc/psychology/2021-ferguson-table2-correlationsbetweenirrelevantfactorsandconductproblemsinunderstandingsocietywave1.png" width="1400"></p>
<figcaption><p><strong>Table 2</strong>: Correlations Between Crud and Conduct Problems for Study 2</p></figcaption>
</figure>
</blockquote>
</section>
<section id="iliev-bennis-2023">
<h2><a href="#iliev-bennis-2023" title="Link to section: § 'Iliev &amp; Bennis2023'"><span><span>Iliev &amp; Bennis</span><span>2023</span></span></a></h2>
<p><a href="https://link.springer.com/article/10.1007/s10902-023-00631-9" id="iliev-bennis-2023" data-link-icon="springerlink" data-link-icon-type="svg" title="‘The Convergence of Positivity: Are Happy People All Alike?’, Iliev &amp; Bennis 2023">“The Convergence of Positivity: Are Happy People All Alike?”, <span><span>Iliev &amp; Bennis</span><span>2023</span></span></a></p>
</section>
<section id="downey-2023">
<h2><a href="#downey-2023" title="Link to section: § 'Downey2023'"><span><span>Downey</span><span>2023</span></span></a></h2>
<p><a href="https://gwern.net/doc/www/www.allendowney.com/28a1c5ff15e3b7eeda5038806eb0f78e35dbed25.html" id="downey-2023" data-url-archive="/doc/www/www.allendowney.com/28a1c5ff15e3b7eeda5038806eb0f78e35dbed25.html" data-url-original="https://www.allendowney.com/blog/2023/08/20/how-correlated-are-you/" data-filesize-bytes="3274596" data-filesize-percentage="74" title="‘How Correlated Are You?’, Downey 2023">“How Correlated Are You?”, <span><span>Downey</span><span>2023</span></span></a></p>
</section>
<section id="external-links">
<h2><a href="#external-links" title="Link to section: § 'External Links'">External Links</a></h2>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/All_models_are_wrong" id="_Vsq3rXeR" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/All_models_are_wrong#bodyContent" title="All models are wrong">All models are wrong</a></p></li>
<li><p><a href="https://gwern.net/doc/www/pdfs.semanticscholar.org/9dca508527d7956aa713dfe247b6d995862f4f7f.pdf" id="_3U4ckghn" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/pdfs.semanticscholar.org/9dca508527d7956aa713dfe247b6d995862f4f7f.pdf" data-url-original="https://pdfs.semanticscholar.org/8d4e/4958856859323ecacca91807a0ea2e847dfc.pdf" data-filesize-bytes="95262" data-filesize-percentage="6">“Stereotype (In)Accuracy in Perceptions of Groups and Individuals”</a>, <span><span title="et al">Jussim</span><span> et al </span><span>2015</span></span></p></li>
<li><p><a href="https://www.amazon.com/Handbook-Social-Status-Correlates-Ellis/dp/0128053712" id="_VaQLollj" data-link-icon="amazon" data-link-icon-type="svg" data-link-icon-color="#ffce53"><em>Handbook of Social Status Correlates</em></a>, <span><span title="et al">Ellis</span><span> et al </span><span>2018</span></span></p></li>
<li><p><a href="https://slatestarcodex.com/2015/02/11/black-people-less-likely/" id="_cxzrdn56" data-link-icon="SSC" data-link-icon-type="text,tri" data-link-icon-color="#5175c2" title="Black People Less Likely">“Black People Less Likely”</a></p></li>
<li><p><a href="https://gwern.net/doc/psychology/personality/2006-ozer.pdf" id="ozer-benet-martínez-2006" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="162726" data-filesize-percentage="28" title="'Personality and the Prediction of Consequential Outcomes', Ozer &amp; Benet-Martínez 2006">“Personality and the Prediction of Consequential Outcomes”</a>, Ozer &amp; Benet-<span><span>Martínez</span><span>2006</span></span></p></li>
<li><p><a href="https://www.lesswrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst" id="_jvrieZPz" data-link-icon="LW" data-link-icon-type="text" data-link-icon-color="#7faf83" data-url-iframe="https://www.greaterwrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst?format=preview&amp;theme=classic" title="Against NHST">“Against NHST”</a></p></li>
<li><p><a href="https://gwern.net/doc/www/arxiv.org/ca13ba28ad7682a74385d3bf547af7fb242c8e80.pdf" id="valberg-et-al-2017" data-link-icon="𝛘" data-link-icon-type="text" data-link-icon-color="#b31b1b" data-href-mobile="https://arxiv.org/html/1707.00014?fallback=original" data-url-archive="/doc/www/arxiv.org/ca13ba28ad7682a74385d3bf547af7fb242c8e80.pdf" data-url-original="https://arxiv.org/abs/1707.00014" data-filesize-bytes="1836479" data-filesize-percentage="61" title="'The surprising implications of familial association in disease risk', Valberg et al 2017">“The surprising implications of familial association in disease risk”</a>, <span><span title="et al">Valberg</span><span> et al </span><span>2017</span></span></p></li>
<li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2858332/" id="yarkoni-2010" data-link-icon="nlm-ncbi" data-link-icon-type="svg" data-link-icon-color="#20558a" title="'The Abbreviation of Personality, or how to Measure 200 Personality Scales with 200 Items', Yarkoni 2010">“The Abbreviation of Personality, or how to Measure 200 Personality Scales with 200 Items”</a>, <span><span>Yarkoni</span><span>2010</span></span></p></li>
<li><p><a href="https://gwern.net/doc/psychology/okcupid/thebestquestionsforafirstdate.html" id="_EFyCPCAz" data-link-icon="internet-archive" data-link-icon-type="svg" data-filesize-bytes="2277764" data-filesize-percentage="86">“The Best Questions For A First Date”</a>, Christian <span><span>Rudder</span><span>2011</span></span> (OKCupid)</p></li>
<li><p><a href="https://gwern.net/doc/www/eighteenthelephant.com/e4045221f824564d5263546ec1341a28ae4f5f77.html" id="_NAWi5rnl" data-url-archive="/doc/www/eighteenthelephant.com/e4045221f824564d5263546ec1341a28ae4f5f77.html" data-url-original="https://eighteenthelephant.com/2021/11/29/pushed-around-by-stars/" data-filesize-bytes="2599291" data-filesize-percentage="68" title="Pushed around by stars">“Pushed around by stars”</a></p></li>
<li><p><a href="https://gwern.net/doc/statistics/bias/2022-wilson.pdf" id="wilson-et-al-2022" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1025322" data-filesize-percentage="75" title="'Theoretical false positive psychology', Wilson et al 2022">“Theoretical false positive psychology”</a>, <span><span title="et al">Wilson</span><span> et al </span><span>2022</span></span></p></li>
<li><p>Discussion: <a href="https://gwern.net/doc/www/news.ycombinator.com/05f4bff1b78c1eece9bbe3fe3d595d867fd8cd8b.html" id="_OEdPy7W0" data-link-icon="hacker-news" data-link-icon-type="svg" data-link-icon-color="#f26522" data-url-archive="/doc/www/news.ycombinator.com/05f4bff1b78c1eece9bbe3fe3d595d867fd8cd8b.html" data-url-original="https://news.ycombinator.com/item?id=19797844" data-filesize-bytes="96956" data-filesize-percentage="6">HN</a></p></li>
</ul>
</section>
<section id="appendix">
<h2><a href="#appendix" title="Link to section: § 'Appendix'">Appendix</a></h2>
<section id="genetic-correlations">
<h2><a href="#genetic-correlations" title="Link to section: § 'Genetic correlations'">Genetic Correlations</a></h2>
<p>Modern genomics has found large-scale <a href="https://en.wikipedia.org/wiki/Biobank" id="_ZVNUWNLH" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Biobank#bodyContent" title="Biobank">biobanks</a> &amp; summary-statistic-only methods to be a fruitful area for identifying <a href="https://en.wikipedia.org/wiki/Genetic_correlation" id="_EFM6e8jH" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Genetic_correlation#bodyContent" title="Genetic correlation">genetic correlations</a> as the power of publicly-released PGSes have steadily grown with increasing <em>n</em> (stabilizing estimates &amp; making ever more genetic correlations pass statistical-significance thresholds), which also frequently mirror phenotypic correlations in all organisms (“Cheverud’s conjecture”<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a>).</p>
<p>Example graphs drawn from the broader analyses (primarily visualized as heatmaps):</p>
<ul>
<li><p><a href="https://gwern.net/doc/genetics/heritable/correlation/2015-krapohl.pdf" id="_btxoW5yh" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="541123" data-filesize-percentage="60">“Phenome-wide analysis of genome-wide polygenic scores”</a>, <span><span title="et al">Krapohl</span><span> et al </span><span>2015</span></span>:</p>
<figure>
<p><img alt="Krapohl et al 2015: “Figure 1. Correlations between 13 genome-wide polygenic scores and 50 traits from the behavioral phenome. These results are based on GPS constructed using a GWAS P-value threshold (PT)=0.30; results for PT = 0.10 and 0.05 (Supplementary Figures 1a and b and Supplementary Table 3). P-values that pass Nyholt–Sidak correction (see Supplementary Methods 1) are indicated with two asterisks, whereas those reaching nominal significance (thus suggestive evidence) are shown with a single asterisk.”" data-aspect-ratio="700 / 439" decoding="async" height="878" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2015-krapohl-figure1-rgs.png" width="1400"></p>
<figcaption><p><span><span title="et al">Krapohl</span><span> et al </span><span>2015</span></span>: “Figure 1. Correlations between 13 genome-wide <a href="https://en.wikipedia.org/wiki/Polygenic_score" id="_fxPG6SR9" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Polygenic_score#bodyContent" title="Polygenic score">polygenic scores</a> and 50 traits from the behavioral phenome. These results are based on GPS constructed using a <a href="https://en.wikipedia.org/wiki/Genome-wide_association_study" id="_p74YXe0t" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Genome-wide_association_study#bodyContent" title="Genome-wide association study">GWAS</a> <em>P</em>-value threshold (<em>P<sub>T</sub></em>)=0.30; results for <em>P<sub>T</sub></em> = 0.10 and 0.05 (Supplementary Figures 1a and b and Supplementary Table 3). <em>P</em>-values that pass Nyholt–Sidak correction (see Supplementary Methods 1) are indicated with two asterisks, whereas those reaching nominal significance (thus suggestive evidence) are shown with a single asterisk.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.nature.com/articles/mp2015225" id="hagenaars-et-al-2016-1" data-link-icon="n" data-link-icon-type="text" title="'Shared genetic aetiology between cognitive functions and physical and mental health in UK Biobank (<em>n</em> = 112,151) and 24 GWAS consortia', Hagenaars et al 2016">“Shared genetic aetiology between cognitive functions and physical and mental health in UK Biobank (<em>n</em> = 112 151) and 24 GWAS consortia”</a>, <span><span title="et al">Hagenaars</span><span> et al </span><span>2016</span></span>:</p>
<figure>
<p><img alt="Hagenaars et al 2016: “Figure 1. Heat map of genetic correlations calculated using LD regression between cognitive phenotypes in UK Biobank and health-related variables from GWAS consortia. Hues and colors depict, respectively, the strength and direction of the genetic correlation between the cognitive phenotypes in UK Biobank and the health-related variables. Red and blue indicate positive and negative correlations, respectively. Correlations with the darker shade associated with a stronger association. Based on results in Table 2. ADHD, attention deficit hyperactivity disorder; FEV1, forced expiratory volume in 1 s; GWAS, genome-wide association study; LD, linkage disequilibrium; NA, not available.”" data-aspect-ratio="719 / 925" decoding="async" height="925" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2016-hagenaars-figure1-intelligencergs.jpg" width="719"></p>
<figcaption><p><span><span title="et al">Hagenaars</span><span> et al </span><span>2016</span></span>: “Figure 1. Heat map of genetic correlations calculated using <a href="https://en.wikipedia.org/wiki/Linkage_disequilibrium" id="_1CNzLoPs" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Linkage_disequilibrium#bodyContent" title="Linkage disequilibrium">LD</a> regression between cognitive phenotypes in UK Biobank and health-related variables from GWAS consortia. Hues and colors depict, respectively, the strength and direction of the genetic correlation between the cognitive phenotypes in UK Biobank and the health-related variables. Red and blue indicate positive and negative correlations, respectively. Correlations with the darker shade associated with a stronger association. Based on results in Table 2. <a href="https://en.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder" id="_R4loa9f4" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder#bodyContent" title="Attention deficit hyperactivity disorder">ADHD</a>, attention deficit hyperactivity disorder; FEV1, forced expiratory volume in 1 s; GWAS, genome-wide association study; LD, linkage disequilibrium; NA, not available.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/043000.full" id="hill-et-al-2016-1" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="'Molecular genetic contributions to social deprivation and household income in UK Biobank (<em>n</em> = 112,151)', Hill et al 2016">“Molecular genetic contributions to social deprivation and household income in UK Biobank (<em>n</em> = 112,151)”</a>, <span><span title="et al">Hill</span><span> et al </span><span>2016</span></span>:</p>
<figure>
<p><img alt="Hill et al 2016 figure: “Genetic correlations between household incomes and health variables”" data-aspect-ratio="286 / 157" decoding="async" height="628" loading="lazy" src="https://gwern.net/doc/genetics/heritable/2016-hill-ses-health-geneticcorrelations.jpg" width="1144"></p>
<figcaption><p><span><span title="et al">Hill</span><span> et al </span><span>2016</span></span> figure: “Genetic correlations between household incomes and health variables”</p></figcaption>
</figure></li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/203257.full" id="socrates-et-al-2017" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="'Polygenic risk scores applied to a single cohort reveal pleiotropy among hundreds of human phenotypes', Socrates et al 2017">“Polygenic risk scores applied to a single cohort reveal pleiotropy among hundreds of human phenotypes”</a>, <span><span title="et al">Socrates</span><span> et al </span><span>2017</span></span> (<a href="https://gwern.net/doc/www/www.biorxiv.org/00eb3a5d399d72007a43620f879504d7aa048b7e.pdf#page=1" id="_-onJZd9P" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" data-url-archive="/doc/www/www.biorxiv.org/00eb3a5d399d72007a43620f879504d7aa048b7e.pdf#page=1" data-url-original="https://www.biorxiv.org/content/biorxiv/suppl/2017/10/14/203257.DC1/203257-1.pdf#page=1">supplement w/full heatmaps</a>)</p>
<figure>
<p><img alt="Socrates et al 2017: “Figure 3. Heat map showing genetic associations between polygenic risk scores from GWAS traits (x-axis) and NFBC196659ya traits (y-axis) for self-reported disorders, medical and psychiatric conditions verified or treated by a doctor, controlled for sex, BMI, and SES”" data-aspect-ratio="699 / 547" decoding="async" height="1094" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-socrates-geneticcorrelations-figure3-psychiatrichealth.png" width="1398"></p>
<figcaption><p><span><span title="et al">Socrates</span><span> et al </span><span>2017</span></span>: “Figure 3. Heat map showing genetic associations between polygenic risk scores from GWAS traits (<em>x</em>-axis) and NFBC<span>1966<sub><span title="1966 was 59 years ago.">59ya</span></sub></span> traits (<em>y</em>-axis) for self-reported disorders, medical and psychiatric conditions verified or treated by a doctor, controlled for sex, <a href="https://en.wikipedia.org/wiki/Body_mass_index" id="_qYc4DCzQ" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Body_mass_index#bodyContent" title="Body mass index">BMI</a>, and SES”</p></figcaption>
</figure>
<figure>
<p><img alt="Socrates et al 2017: “Figure 3. Heat map showing genetic associations between polygenic risk scores from GWAS traits (x-axis) and NFBC196659ya traits (y-axis) from questionnaires lifestyle and social factors”" data-aspect-ratio="175 / 156" decoding="async" height="1248" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-socrates-geneticcorrelations-figure5-lifestylesocialfactors.png" width="1400"></p>
<figcaption><p><span><span title="et al">Socrates</span><span> et al </span><span>2017</span></span>: “Figure 3. Heat map showing genetic associations between polygenic risk scores from GWAS traits (<em>x</em>-axis) and NFBC<span>1966<sub><span title="1966 was 59 years ago.">59ya</span></sub></span> traits (<em>y</em>-axis) from questionnaires lifestyle and social factors”</p></figcaption>
</figure></li>
<li><p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5971142/" id="docherty-et-al-2017" data-link-icon="nlm-ncbi" data-link-icon-type="svg" data-link-icon-color="#20558a" title="'Polygenic prediction of the phenome, across ancestry, in emerging adulthood', Docherty et al 2017">“Polygenic prediction of the phenome, across ancestry, in emerging adulthood”</a>, <span><span title="et al">Docherty</span><span> et al </span><span>2017</span></span>:</p>
<figure>
<p><img alt="Docherty et al 2017: “Figure 2: Phenome on GPS regression q-values in European Sample (EUR). GPS displayed with prior proportion of causal effects = 0.3. Here, asterisks in the cells of the heatmap denote results of greater effect: ✱✱✱ = q-value < 0.01, ✱✱ = q-value < 0.05, ✱ = q-value < 0.16. Blue values reflect a negative association, and red reflect positive association. Intensity of color indicates −log10 p value.”" data-aspect-ratio="920 / 1069" decoding="async" height="1069" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-docherty-figure2-phenome-eur.jpg" width="920"></p>
<figcaption><p><span><span title="et al">Docherty</span><span> et al </span><span>2017</span></span>: “Figure 2: Phenome on GPS regression q-values in European Sample (EUR). GPS displayed with prior proportion of causal effects = 0.3. Here, asterisks in the cells of the heatmap denote results of greater effect: ✱✱✱ = <em>q</em>-value &lt; 0.01, ✱✱ = <em>q</em>-value &lt; 0.05, ✱ = <em>q</em>-value &lt; 0.16. Blue values reflect a negative association, and red reflect positive association. Intensity of color indicates −log10 <em>p</em> value.”</p></figcaption>
</figure>
<figure>
<p><img alt="Docherty et al 2017: “Figure 3: Genetic Overlap and Co-Heritability of GPS in European Sample (EUR). Heatmap of partial correlation coefficients between GPS with prior proportion of causal effects = 0.3. Here, asterisks in the cells of the heatmap denote results of greater effect: ✱✱✱✱ = q-value < 0.0001, ✱✱✱ = q-value < 0.001, ✱✱ = q value < 0.01, ✱ = q value < 0.05, and ~ = suggestive significance at q value < 0.16. Blue values reflect a negative correlation, and red reflect positive correlation.”" data-aspect-ratio="8 / 7" decoding="async" height="805" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-docherty-figure3-rgs.jpg" width="920"></p>
<figcaption><p><span><span title="et al">Docherty</span><span> et al </span><span>2017</span></span>: “Figure 3: Genetic Overlap and Co-Heritability of GPS in European Sample (EUR). Heatmap of partial correlation coefficients between GPS with prior proportion of causal effects = 0.3. Here, asterisks in the cells of the heatmap denote results of greater effect: ✱✱✱✱ = <em>q</em>-value &lt; 0.0001, ✱✱✱ = <em>q</em>-value &lt; 0.001, ✱✱ = <em>q</em> value &lt; 0.01, ✱ = <em>q</em> value &lt; 0.05, and ~ = suggestive significance at <em>q</em> value &lt; 0.16. Blue values reflect a negative correlation, and red reflect positive correlation.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.nature.com/articles/s41467-017-00934-5" id="joshi-et-al-2017-1" data-link-icon="n" data-link-icon-type="text" title="'Genome-wide meta-analysis associates HLA-DQA1/DRB1 and LPA and lifestyle factors with human longevity', Joshi et al 2017">“Genome-wide meta-analysis associates <em>HLA-DQA1/DRB1</em> and <em>LPA</em> and lifestyle factors with human longevity”</a>, <span><span title="et al">Joshi</span><span> et al </span><span>2017</span></span>:</p>
<figure>
<p><img alt="“Figure 5: Genetic correlations between trait clusters that associate with mortality. The upper panel shows whole genetic correlations, the lower panel, partial correlations. T2D, type 2 diabetes; BP, blood pressure; BC, breast cancer; CAD, coronary artery disease; Edu, educational attainment; RA, rheumatoid arthritis; AM, age at menarche; DL/WHR Dyslipidemia/Waist-Hip ratio; BP, blood pressure”" data-aspect-ratio="102 / 179" decoding="async" height="1790" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2017-joshi-figure5-mortalityrgs.jpg" width="1020"></p>
<figcaption><p>“Figure 5: Genetic correlations between trait clusters that associate with mortality. The upper panel shows whole genetic correlations, the lower panel, partial correlations. T2D, type 2 diabetes; <a href="https://en.wikipedia.org/wiki/Bipolar_disorder" id="_2AmaRRtd" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Bipolar_disorder#bodyContent" title="Bipolar disorders research">BP</a>, blood pressure; BC, breast cancer; CAD, coronary artery disease; Edu, educational attainment; RA, rheumatoid arthritis; AM, age at menarche; DL/WHR Dyslipidemia/Waist-Hip ratio; BP, blood pressure”</p></figcaption>
</figure></li>
<li><p><a href="https://www.nature.com/articles/s41380-017-0001-5" id="hill-et-al-2018-1" data-link-icon="n" data-link-icon-type="text" title="'A combined analysis of genetically correlated traits identifies 187 loci and a role for neurogenesis and myelination in intelligence', Hill et al 2018">“A combined analysis of genetically correlated traits identifies 187 loci and a role for neurogenesis and myelination in intelligence”</a>, <span><span title="et al">Hill</span><span> et al </span><span>2018</span></span>:</p>
<figure>
<p><img alt="“Fig. 4: Heat map showing the genetic correlations between the meta-analytic intelligence phenotype, intelligence, education with 29 cognitive, SES, mental health, metabolic, health and wellbeing, anthropometric, and reproductive traits. Positive genetic correlations are shown in green and negative genetic correlations are shown in red. Statistical-significance following FDR (using Benjamini-Hochberg procedure [51]) correction is indicated by an asterisk.”" data-aspect-ratio="115 / 37" decoding="async" height="296" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2018-hill-figure5-intelligencergs.jpg" width="920"></p>
<figcaption><p>“Fig. 4: Heat map showing the genetic correlations between the meta-analytic intelligence phenotype, intelligence, education with 29 cognitive, SES, mental health, metabolic, health and wellbeing, anthropometric, and reproductive traits. Positive genetic correlations are shown in green and negative genetic correlations are shown in red. Statistical-significance following FDR (using Benjamini-Hochberg procedure [51]) correction is indicated by an asterisk.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/500090.full" id="watanabe-et-al-2018" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="'A global overview of pleiotropy and genetic architecture in complex traits', Watanabe et al 2018">“A global view of pleiotropy and genetic architecture in complex traits”</a>, <span><span title="et al">Watanabe</span><span> et al </span><span>2018</span></span>:</p>
<figure>
<p><img alt="Watanabe et al 2018: “Fig. 2. Within and between domains genetic correlations. (a.) Proportion of trait pairs with significant rg (top) and average |_rg_| for significant trait pairs (bottom) within domains. Dashed lines represent the proportion of trait pairs with significant rg (top) and average |rg| for significant trait pairs (bottom) across all 558 traits, respectively. Connective tissue, muscular and infection domains are excluded as these each contains less than 3 traits. (b.) Heatmap of proportion of trait pairs with significant rg (upper right triangle) and average |rg| for significant trait pairs (lower left triangle) between domains. Connective tissue, muscular and infection domains are excluded as each contains less than 3 traits. The diagonal represents the proportion of trait pairs with significant rg within domains. Stars denote the pairs of domains in which the majority (>50%) of significant rg are negative.”" data-aspect-ratio="632 / 1001" decoding="async" height="2002" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2018-watanabe-figure2-rgs.png" width="1264"></p>
<figcaption><p><span><span title="et al">Watanabe</span><span> et al </span><span>2018</span></span>: “Fig. 2. Within and between domains genetic correlations. (a.) Proportion of trait pairs with significant <em>r<sub>g</sub></em> (top) and average |_r<sub>g_</sub>| for significant trait pairs (bottom) within domains. Dashed lines represent the proportion of trait pairs with significant <em>r<sub>g</sub></em> (top) and average |<em>r<sub>g</sub></em>| for significant trait pairs (bottom) across all 558 traits, respectively. Connective tissue, muscular and infection domains are excluded as these each contains less than 3 traits. (b.) Heatmap of proportion of trait pairs with significant <em>r<sub>g</sub></em> (upper right triangle) and average |<em>r<sub>g</sub></em>| for significant trait pairs (lower left triangle) between domains. Connective tissue, muscular and infection domains are excluded as each contains less than 3 traits. The diagonal represents the proportion of trait pairs with significant <em>r<sub>g</sub></em> within domains. Stars denote the pairs of domains in which the majority (&gt;50%) of significant <em>r<sub>g</sub></em> are negative.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.biorxiv.org/content/10.1101/457515.full" id="abdellaoui-et-al-2018" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="'Genetic Consequences of Social Stratification in Great Britain', Abdellaoui et al 2018">“Genetic Consequences of Social Stratification in Great Britain”</a>, <span><span title="et al">Abdellaoui</span><span> et al </span><span>2018</span></span>:</p>
<figure>
<p><img alt="Abdellaoui et al 2018: “Figure 6: Genetic correlations based on LD score regression. Colored is significant after FDR correction. The green numbers in the left part of the Figure below the diagonal of 1’s are the phenotypic correlations between the regional outcomes of coal mining, religiousness, and regional political preference. The blue stars next to the trait names indicate that UK Biobank was part of the GWAS of the trait.”" data-aspect-ratio="1400 / 643" decoding="async" height="643" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2018-abdellaoui-figure6-geneticcorrelations-politicsminingreligion.png" width="1400"></p>
<figcaption><p><span><span title="et al">Abdellaoui</span><span> et al </span><span>2018</span></span>: “Figure 6: Genetic correlations based on LD score regression. Colored is significant after FDR correction. The green numbers in the left part of the Figure below the diagonal of 1’s are the phenotypic correlations between the regional outcomes of coal mining, religiousness, and regional political preference. The blue stars next to the trait names indicate that UK Biobank was part of the GWAS of the trait.”</p></figcaption>
</figure></li>
<li><p><a href="https://www.nature.com/articles/s42003-019-0290-0" id="_OpoaL0TI" data-link-icon="n" data-link-icon-type="text">“Identification of 12 genetic loci associated with human healthspan”</a>, <span><span title="et al">Zenin</span><span> et al </span><span>2019</span></span>:</p>
<figure>
<p><img alt="“Figure 4. 35 traits with significant and high genetic correlations with healthspan (|rg| ≥ 0.3; p ≤ 4.3 × 10−5). PMID references are placed in square brackets. Note the absence of genetic correlation between the healthspan and Alzheimer disease traits (rg = −0.03)”" data-aspect-ratio="340 / 261" decoding="async" height="783" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2019-zenin-figure5-35rgs.png" width="1020"></p>
<figcaption><p>“Figure 4. 35 traits with significant and high genetic correlations with healthspan (|<em>r<sub>g</sub></em>| ≥ 0.3; <em>p</em> ≤ 4.3 × 10<sup>−5</sup>). PMID references are placed in square brackets. Note the absence of genetic correlation between the healthspan and Alzheimer disease traits (<em>r<sub>g</sub></em> = −0.03)”</p></figcaption>
</figure></li>
<li><p><a href="https://gwern.net/doc/genetics/heritable/correlation/2019-liu.pdf" id="liu-et-al-2019-4" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1556548" data-filesize-percentage="81" title="'Association studies of up to 1.2 million individuals yield new insights into the genetic etiology of tobacco and alcohol use', Liu et al 2019">“Association studies of up to 1.2 million individuals yield new insights into the genetic etiology of tobacco and alcohol use”</a>, <span><span title="et al">Li</span><span> et al </span><span>2019</span></span>:</p>
<figure>
<p><img alt="Liu et al 2019: “Fig. 1 | Genetic correlations between substance use phenotypes and phenotypes from other large GWAS. Genetic correlations between each of the phenotypes are shown in the first 5 rows, with heritability estimates displayed down the diagonal. All genetic correlations and heritability estimates were calculated using LD score regression. Purple shading represents negative genetic correlations, and red shading represents positive correlations, with increasing color intensity reflecting increasing correlation strength. A single asterisk reflects a significant genetic correlation at the p < 0.05 level. Double asterisks reflect a significant genetic correlation at the Bonferroni-correction p < 0.000278 level (corrected for 180 independent tests). Note that SmkCes was oriented such that higher scores reflected current smoking, and for AgeSmk, lower scores reflect earlier ages of initiation, both of which are typically associated with negative outcomes.”" data-aspect-ratio="947 / 1121" decoding="async" height="1121" loading="lazy" src="https://gwern.net/doc/genetics/heritable/correlation/2019-liu-figure1-drugusergs.png" width="947"></p>
<figcaption><p><span><span title="et al">Liu</span><span> et al </span><span>2019</span></span>: “Fig. 1 | Genetic correlations between substance use phenotypes and phenotypes from other large GWAS. Genetic correlations between each of the phenotypes are shown in the first 5 rows, with heritability estimates displayed down the diagonal. All genetic correlations and heritability estimates were calculated using LD score regression. Purple shading represents negative genetic correlations, and red shading represents positive correlations, with increasing color intensity reflecting increasing correlation strength. A single asterisk reflects a significant genetic correlation at the <em>p</em> &lt; 0.05 level. Double asterisks reflect a significant genetic correlation at the Bonferroni-correction <em>p</em> &lt; 0.000278 level (corrected for 180 independent tests). Note that <code>SmkCes</code> was oriented such that higher scores reflected current smoking, and for <code>AgeSmk</code>, lower scores reflect earlier ages of initiation, both of which are typically associated with negative outcomes.”</p></figcaption>
</figure></li>
</ul>

</section>
</section>
<section id="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Sometimes paraphrased as “All good things tend to go together, as do all bad ones”.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><span><span>Tibshirani</span><span>2014</span></span>:</p>
<blockquote>
<p>In describing some of this work, <a href="https://gwern.net/doc/www/hastie.su.domains/dd44844dfc0ecce9c57a6d334ba02f08b75221dd.pdf#page=630" id="_C9ypmOog" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-url-archive="/doc/www/hastie.su.domains/dd44844dfc0ecce9c57a6d334ba02f08b75221dd.pdf#page=630" data-url-original="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12.pdf#page=630" title="_The Elements of Statistical Learning_: §16.2.2, 'The `Bet on Sparsity` Principle'"><span><span title="et al">Hastie</span><span> et al </span><span>2001</span></span></a> coined the informal “Bet on Sparsity” principle [“Use a procedure that does well in sparse problems, since no procedure does well in dense problems.”]. The ℓ<sub>1</sub> methods assume that the truth is sparse, in some basis. If the assumption holds true, then the parameters can be efficiently estimated using ℓ<sub>1</sub> penalties. If the assumption does not hold—so that the truth is dense—then no method will be able to recover the underlying model without a large amount of data per parameter. This is typically not the case when <em>p</em> ≫ <em>N</em>, a commonly occurring scenario.</p>
</blockquote>
<p>This can be seen as a kind of decision-theoretic justification for Occam-style assumptions: if the real world is not predictable in the sense of being predictable by simple/fast algorithms, or induction doesn’t work at all, then no method works in expectation, and the “regret” (difference between <a href="https://en.wikipedia.org/wiki/Expected_value" id="_4Jur8wYN" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Expected_value#bodyContent" title="Expected value">expected value</a> of actual decision and expected value of optimal decision) from mistakenly assuming that the world is simple/sparse is zero. So one should assume the world is simple.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>A machine learning practitioner as of 2019, will be struck by the thought that Tobler’s first law nicely encapsulates the principle behind the “unreasonable effectiveness” of <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" id="_ZQFNQqCj" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Convolutional_neural_network#bodyContent" title="Convolutional neural network">convolutions in applications of neural networks</a> to so many domains far beyond images; this connection has been made by <a href="https://gwern.net/doc/www/blogs.loc.gov/01d254a360a477ee11c3b50503eba8af26d5544a.html" id="_2jsEjQNz" data-url-archive="/doc/www/blogs.loc.gov/01d254a360a477ee11c3b50503eba8af26d5544a.html" data-url-original="https://blogs.loc.gov/maps/2016/04/alphago-neural-networks-and-toblers-first-law/" data-filesize-bytes="3552758" data-filesize-percentage="76" title="[Computing Space VIII] Games Cartographers Play: AlphaGo, Neural Networks and Tobler's First Law">John Hessler</a>.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The most interesting example of this is ESP/psi parapsychology research: the more rigorously conducted the ESP experiments are, the smaller the effects become—but, while discrediting all claims of human ESP, frequently they aren’t pushed to <em>exactly</em> zero and are “statistically-significant”. <a href="https://gwern.net/modus" id="gwern-modus" data-filesize-bytes="110019" data-filesize-percentage="58" title="'One Man’s Modus Ponens', Gwern 2012">There must be</a> some residual crud factor in the experiments, even when conducted &amp; analyzed as best as we know how.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span><span>Gosset</span><span>1904</span></span> has been discussed in several sources, like <a href="https://gwern.net/doc/statistics/decision/1939-pearson.pdf" id="pearson-1939" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="5273313" data-filesize-percentage="92" title="'Student' as Statistician"><span><span>Pearson</span><span>1939</span></span></a>.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>The version in the second edition, <a href="https://gwern.net/doc/statistics/decision/1972-savage-foundationsofstatistics.pdf#page=270" id="_5H-0IOn3" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="28637068" data-filesize-percentage="97"><em>The Foundations of Statistics</em>, 2<sup>nd</sup> edition, <span><span>Savage</span><span>1972</span></span></a>, is identical to the first.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Note: “I. Richard Savage” is not to be confused with his brother, <em>Leonard Jimmie</em> Savage, who also worked in <a href="https://en.wikipedia.org/wiki/Bayesian_statistics" id="_yza4YDhb" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-iframe="https://en.m.wikipedia.org/wiki/Bayesian_statistics#bodyContent" title="Bayesian statistics">Bayesian statistics</a> &amp; is cited previously.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a href="https://gwern.net/doc/statistics/decision/1986-lehmann-testingstatisticalhypotheses.pdf" id="_FW_geoDn" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="16264064" data-filesize-percentage="96">2nd edition, 1986</a>; after skimming the 2<sup>nd</sup> edition, I have not been able to find a relevant passage, but Lehmann remarks that he substantially rewrote the textbook for a more robust decision-theoretic approach, so it may have been removed.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>This analysis was never published, according to <a href="#meehl-1990-1"><span><span>Meehl</span><span>1990a</span></span></a>.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>I would note there is <a href="https://gwern.net/note/statistic#expectations-are-not-expected-deviations-and-large-number-of-variables-are-not-large-samples" id="gwern-note-statistic--expectations-are-not-expected-deviations-and-large-number-of-variables-are-not-large-samples" data-filesize-bytes="456894" data-filesize-percentage="88">a dangerous fallacy here</a> even if one does believe the Law of Large Numbers should apply here with an expectation of zero effect: even if the expectation of the pairwise correlation of 2 arbitrary variables was in fact precisely zero (as is not too implausible in some domains such as optimization or feedback loops—such as the famous example of the thermostat/room-temperature), that does not mean any specific pair will be exactly zero no matter how many numbers get added up to create their relationship, as the absolute size of the deviation increases.</p>
<p>So for example, imagine 2 genetic traits which may be genetically-correlated, and their heritability may be caused by a number of genes ranging from 1 (monogenic) to tens of thousands (highly polygenic); the specific overlap is created by a chance draw of evolutionary processes throughout the organism’s evolution; does the Law of Large Numbers justify saying that while 2 monogenic traits may have a substantial correlation, 2 highly polygenic traits must have much closer to zero correlation simply because they are influenced by more genes? No, because the distribution around the expectation of 0 can become wider &amp; wider the more relevant genes there are.</p>
<p>To reason otherwise is, as Samuelson noted, to think like an insurer who is worried about losing $100 on an insurance contract so it goes out &amp; makes 100 more $100 contracts.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p><span><span>Betz</span><span>1986</span></span> special issue’s contents:</p>
<ol>
<li><p><a href="https://gwern.net/doc/iq/1986-gottfredson.pdf" id="gottfredson-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="280177" data-filesize-percentage="43" title="'The <em>g</em> factor in employment', Gottfredson 1986">“The <em>g</em> factor in employment”</a>, <span><span>Gottfredson</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-avery.pdf" id="avery-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="292550" data-filesize-percentage="44" title="'Origins of and Reactions to the PTC conference on 'The <em>g</em> Factor In Employment Testing'', Avery 1986">“Origins of and Reactions to the PTC conference on <em>The <em>g</em> Factor In Employment Testing</em>”</a>, <span><span>Avery</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-jensen-2.pdf" id="jensen-1986b" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2349355" data-filesize-percentage="86" title="'g: Artifact or Reality?', Jensen 1986">“<em>g</em>: Artifact or reality?”</a>, <span><span>Jensen</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-thorndike.pdf" id="thorndike-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="588128" data-filesize-percentage="62" title="'The role of general ability in prediction', Thorndike 1986">“The role of general ability in prediction”</a>, <span><span>Thorndike</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-hunter.pdf" id="hunter-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1615904" data-filesize-percentage="82" title="'Cognitive ability, cognitive aptitudes, job knowledge, and job performance', Hunter 1986">“Cognitive ability, cognitive aptitudes, job knowledge, and job performance”</a>, <span><span>Hunter</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-gottfredson-2.pdf" id="gottfredson-crouse-1986b" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1224257" data-filesize-percentage="78" title="'Validity versus utility of mental tests: Example of the SAT', Gottfredson &amp; Crouse 1986b">“Validity versus utility of mental tests: Example of the SAT”</a>, <span><span>Gottfredson &amp; Crouse</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-gottfredson-3.pdf" id="gottfredson-1986c" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="2429864" data-filesize-percentage="86" title="'Societal consequences of the <em>g</em> factor in employment', Gottfredson 1986c">“Societal consequences of the <em>g</em> factor in employment”</a>, <span><span>Gottfredson</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-hawk.pdf" id="hawk-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="284066" data-filesize-percentage="43" title="'Real world implications of g', Hawk 1986">“Real world implications of <em>g</em>”</a>, <span><span>Hawk</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-arvey.pdf" id="arvey-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="396905" data-filesize-percentage="52" title="'General ability in employment: A discussion', Arvey 1986">“General ability in employment: A discussion”</a>, <span><span>Arvey</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-humphreys.pdf" id="humphreys-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1245324" data-filesize-percentage="78" title="'Commentary [on 'The _g: factor in employment special issue']', Humphreys 1986">“Commentary”</a>, <span><span>Humphreys</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-linn.pdf" id="linn-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="507027" data-filesize-percentage="59" title="'Comments on the <em>g</em> factor in employment testing', Linn 1986">“Comments on the <em>g</em> factor in Employment Testing”</a>, <span><span>Linn</span><span>1986</span></span></p></li>
<li><p><a href="https://gwern.net/doc/iq/1986-tyler.pdf" id="tyler-1986" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="411770" data-filesize-percentage="53" title="'Back to Spearman?', Tyler 1986">“Back to Spearman?”</a>, <span><span>Tyler</span><span>1986</span></span></p></li>
</ol>
<a href="#fnref11" role="doc-backlink">↩︎</a></li>
<li id="fn12"><p>This work does not seem to have been published, as I can find no books published by them jointly, or nor nay McClosky books published between <span>1990<sub><span title="1990 was 35 years ago.">35ya</span></sub></span> &amp; his death in <span>2004<sub><span title="2004 was 21 years ago.">21ya</span></sub></span>.<a href="#fnref12" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>For definitions &amp; evidence for, see: <a href="https://gwern.net/doc/genetics/heritable/correlation/1988-cheverud.pdf" id="_M3xg-5cz" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1427491" data-filesize-percentage="80" title="A comparison of genetic and phenotypic correlations"><span><span>Cheverud</span><span>1988</span></span></a>, <a href="https://gwern.net/doc/genetics/heritable/correlation/1995-roff.pdf" id="technologies-1995" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="748280" data-filesize-percentage="68" title="The estimation of genetic correlations from phenotypic correlations---a test of Cheverud's conjecture"><span><span>Roff</span><span>1996</span></span></a>, <a href="https://gwern.net/doc/genetics/heritable/correlation/2008-kruuk.pdf" id="_CTwhZi2G" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="1558571" data-filesize-percentage="81" title="New answers for old questions: The evolutionary quantitative genetics of wild animal populations"><span><span title="et al">Kruuk</span><span> et al </span><span>2008</span></span></a>, <a href="https://gwern.net/doc/genetics/heritable/correlation/2011-dochtermann.pdf" id="dochtermann-2011" data-link-icon="pdf" data-link-icon-type="svg" data-link-icon-color="#f40f02" data-filesize-bytes="176446" data-filesize-percentage="30" title="Testing Cheverud's conjecture for behavioral correlations and behavioral syndromes"><span><span>Dochtermann</span><span>2011</span></span></a>, <a href="https://www.biorxiv.org/content/10.1101/311332.full" id="jordan-et-al-2018" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="The landscape of pervasive horizontal pleiotropy in human genetic variation is driven by extreme polygenicity of human traits and diseases"><span><span title="et al">Jordan</span><span> et al </span><span>2018</span></span></a>, &amp; <a href="https://www.biorxiv.org/content/10.1101/291062.full" id="sodini-et-al-2018" data-link-icon="chi-dna" data-link-icon-type="svg" data-link-icon-color="#bd2736" title="Comparison of Genotypic and Phenotypic Correlations: Cheverud's Conjecture in Humans"><span><span title="et al">Sodini</span><span> et al </span><span>2018</span></span></a>.<a href="#fnref13" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

      
      
      <section id="backlinks-section">
        <h2><a href="#backlinks-section" title="Link to section: § 'Backlinks'">Backlinks</a></h2>
        <a id="backlinks" href="https://gwern.net/metadata/annotation/backlink/%252Feverything.html" title="Reverse citations/backlinks for this page (the list of other pages which link to this page)." data-link-icon="arrows-pointing-inwards-to-dot" data-link-icon-type="svg">[Backlinks (what links here)]</a>
      </section>
      <section id="similars-section">
        <h2><a href="#similars-section" title="Link to section: § 'Similar Links'">Similar Links</a></h2>
        <a id="similars" href="https://gwern.net/metadata/annotation/similar/%252Feverything.html" title="Similar links for this link (by text embedding). Lazily-transcluded version at footer of page for easier scrolling." data-link-icon="≈" data-link-icon-type="text">[Similar links by topic]</a>
        </section>
      <section id="link-bibliography-section">
        <h2><a href="#link-bibliography-section" title="Link to section: § 'Bibliography'">Bibliography</a></h2> <!-- NOTE: In theory, '.collapse' on a '<h1>' is redundant with the '<section>'; but added to parallel Pandoc-generated headers which set all attributes/classes on both. -->
        <a id="link-bibliography" href="https://gwern.net/metadata/annotation/link-bibliography/%252Feverything.html" title="Bibliography of links cited in this page (forward citations). Lazily-transcluded version at footer of page for easier scrolling." data-link-icon="bibliography" data-link-icon-type="svg">[Bibliography of links/references used in page]</a>
          </section>
      
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Control shopping cart wheels with your phone (2021) (256 pts)]]></title>
            <link>https://www.begaydocrime.com/</link>
            <guid>44980004</guid>
            <pubDate>Fri, 22 Aug 2025 00:59:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.begaydocrime.com/">https://www.begaydocrime.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44980004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <div>
                <div>
                    <div>
                        <h2>About</h2><p>
                        Play the below sounds through your phone speaker and hold it next to a Gatekeeper Systems wheels to lock/unlock. Check me out on twitter @stoppingcart 
                    </p></div>
                    
                    <div>
                        <h2>How It Works</h2><p>
                        Most electronic shopping cart wheels listen for a 7.8 kHz signal from an underground wire to know when to lock and unlock. A management remote can send a different signal at 7.8 kHz to the wheel to unlock it.
        
                        Since 7.8 kHz is in the audio range, you can use the parasitic EMF from your phone's speaker to "transmit" a similar code by playing a crafted audio file. 
                    </p></div>

                    <p><a href="https://www.youtube.com/watch?v=fBICDODmCPI">Link to my original DEFCON 29 Talk</a>

                </p></div>
                <div>
                    <p>
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/fBICDODmCPI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
                    </p>
                </div>
            </div>
            
            

			<p><a href="https://www.counter12.com/"><img src="https://www.counter12.com/img-ZAZ7zcybZcwWdDzB-33.gif" alt="free counter"></a></p>
         
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google scores six-year Meta cloud deal worth over $10B (105 pts)]]></title>
            <link>https://www.cnbc.com/2025/08/21/google-scores-six-year-meta-cloud-deal-worth-over-10-billion.html</link>
            <guid>44979855</guid>
            <pubDate>Fri, 22 Aug 2025 00:34:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/08/21/google-scores-six-year-meta-cloud-deal-worth-over-10-billion.html">https://www.cnbc.com/2025/08/21/google-scores-six-year-meta-cloud-deal-worth-over-10-billion.html</a>, See on <a href="https://news.ycombinator.com/item?id=44979855">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108189400" data-test="InlineImage"><p>Meta CEO Mark Zuckerberg makes a keynote speech at the Meta Connect annual event at the company's headquarters in Menlo Park, Calif., on Sept. 25, 2024.</p><p>Manuel Orbegozo | Reuters</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/META/">Meta</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> has agreed to spend more than $10 billion on <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> cloud services, according to two people familiar with the matter.</p><p>The agreement spans six years, said the people, who asked not to be named because the terms are confidential. The deal was reported earlier by <a href="https://www.theinformation.com/articles/meta-signs-10-billion-plus-cloud-deal-google" target="_blank">The Information</a>.</p><p>Google is aiming to land big cloud contracts as it chases larger rivals <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-4"><a href="https://www.cnbc.com/quotes/AMZN/">Amazon</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> Web Services and <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-5"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> Azure in cloud infrastructure. Earlier this year Google <a href="https://www.cnbc.com/2025/07/16/openai-googles-cloud-chatgpt.html">won cloud business</a> from OpenAI, which had earlier been deeply dependent on Microsoft's Azure infrastructure.</p><p>Alphabet said in July that the Google Cloud unit, which contains productivity software subscriptions in addition to infrastructure, produced $2.83 billion in operating income on $13.6 billion in revenue during the second quarter. Revenue growth of 32% outpaced expansion of 13.8% for the company as a whole.</p><p>Meta's deal with Google is mainly around artificial intelligence infrastructure, said one of the people. Meta said in its <a href="https://www.cnbc.com/2025/07/30/metas-big-ai-spending-blitz-will-continue-into-2026-.html">earnings report</a> last month that it expects total expenses for 2025 to come in the range of $114 billion and $118 billion. It's investing heavily in AI infrastructure and talent, building out its Llama family of models and adding AI across its portfolio of services. &nbsp;</p><p>Meta and Google have long been rivals in online ads. But Meta needs all the cloud infrastructure it can access. The company operates data centers and <a href="https://www.datacenterdynamics.com/en/news/metafacebook-turns-to-aws-as-long-term-strategic-cloud-provider-for-acquisitions-third-party-collaborations-and-ai/" target="_blank">has made</a> commitments to use cloud services from <a href="https://www.datacenterdynamics.com/en/news/metafacebook-turns-to-aws-as-long-term-strategic-cloud-provider-for-acquisitions-third-party-collaborations-and-ai/" target="_blank">Amazon</a> and <a href="https://azure.microsoft.com/en-us/blog/meta-selects-azure-as-strategic-cloud-provider-to-advance-ai-innovation-and-deepen-pytorch-collaboration/" target="_blank">Microsoft</a>.</p><p>Google declined to comment. </p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2025/08/21/antitrust-ruling-could-end-googleas-26-billion-default-deals-but-experts-see-upside-for-ai.html">Antitrust ruling could end Google’s $26 billion default deals, but experts see upside for AI</a></p></div><div id="Placeholder-ArticleBody-Video-108189198" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000386412" aria-labelledby="Placeholder-ArticleBody-Video-108189198"><p><img src="https://image.cnbcfm.com/api/v1/image/108189205-1755794550294-1755794011-41285020012-hd.jpg?v=1755794587&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Antitrust ruling could end Google’s $26 billion default deals, but experts see upside for AI"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From GPT-4 to GPT-5: Measuring progress through MedHELM [pdf] (124 pts)]]></title>
            <link>https://www.fertrevino.com/docs/gpt5_medhelm.pdf</link>
            <guid>44979107</guid>
            <pubDate>Thu, 21 Aug 2025 22:52:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fertrevino.com/docs/gpt5_medhelm.pdf">https://www.fertrevino.com/docs/gpt5_medhelm.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=44979107">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
    </channel>
</rss>