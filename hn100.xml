<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 06 Aug 2025 16:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Constitution of the United States Website has removed sections (web.archive.org) (191 pts)]]></title>
            <link>https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/</link>
            <guid>44811733</guid>
            <pubDate>Wed, 06 Aug 2025 13:30:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/">https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/</a>, See on <a href="https://news.ycombinator.com/item?id=44811733">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main"><section><a href="https://old.reddit.com/login"><h2>Welcome to Reddit,</h2><p>the front page of the internet.</p><div><p><span>Become a Redditor</span></p><p>and join one of thousands of communities.</p></div></a><a href="#" title="close">×</a></section><div id="siteTable" onclick="click_thing(this)" data-fullname="t3_1mj3ttx" data-type="link" data-gildings="0" data-whitelist-status="all_ads" data-is-gallery="false" data-author="LithelyJaine" data-author-fullname="t2_eihbi3p7" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-timestamp="1754484700000" data-url="https://web.archive.org/web/diff/20250601021212/20250806023110/https://constitution.congress.gov/constitution/" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/" data-domain="web.archive.org" data-rank="" data-comments-count="1461" data-score="21545" data-promoted="false" data-nsfw="false" data-spoiler="false" data-oc="false" data-num-crossposts="17" data-context="comments"><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/" data-event-action="comments" rel="nofollow">1461 comments</a></li><li><a href="#">save</a></li><li></li><li>report</li></ul></div><div><div id="siteTable_t3_1mj3ttx"><div id="thing_t1_n785ves" onclick="click_thing(this)" data-fullname="t1_n785ves" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="Alive_Education_3785" data-author-fullname="t2_1kclkurbrv" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n785ves/"><p>[–]<a href="https://old.reddit.com/user/rif011412">rif011412</a><span></span> <span title="21">21 points</span><span title="22">22 points</span><span title="23">23 points</span> <time title="Wed Aug 6 13:56:39 2025 UTC" datetime="2025-08-06T13:56:39+00:00">33 minutes ago</time>&nbsp;(7 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n78dw4rjyl"><div><p>Conservatism is just political speak for humans natural inclination to dominate and feel superior than others. &nbsp;Its very natural state is to ignore or cause the hurt in others for personal gain. &nbsp;</p>

<p>The only time a conservative can be a good thing, is when people push to be superior off of merit and good character, and typically showing humility is still required. &nbsp;Unfortunately for everyone else, its so much easier to be superior using bad behaviors than good ones. &nbsp;Hence dictatorships, war and conflict exist in spades. Forcing others to be lower is a conservative cornerstone. &nbsp;</p>

<p>Confederacy, Nazis, Most Monarchies, Oligarchies, Zionists, Religious fanatics… conservatism without morals, superiority attained without humility and empathy.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n78dw4r/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n7882iw" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_n78396j" onclick="click_thing(this)" data-fullname="t1_n78396j" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="sane_sober61" data-author-fullname="t2_1hcd68khdy" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n78396j/"><p>[–]<a href="https://old.reddit.com/user/CircleBird12">CircleBird12</a><span></span> <span title="6">6 points</span><span title="7">7 points</span><span title="8">8 points</span> <time title="Wed Aug 6 13:43:47 2025 UTC" datetime="2025-08-06T13:43:47+00:00">46 minutes ago</time>&nbsp;(2 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n78bg9gw1u"><div><p>I detect the El Supremo<br>
From the room at the top of the stairs<br>
Well I've been around the world<br>
And I've been in the Washington Zoo<br>
And in all my travels as the facts unravel<br>
I've found this to be true  </p>

<p>They got the Steely Dan T-Shirt<br>
And for the coup de grace<br>
They're outrageous   </p>

<p>Show business kids making movies of themselves<br>
You know they don't give a fuck about anybody else</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n78bg9g/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n786tfm" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_n782bnl" onclick="click_thing(this)" data-fullname="t1_n782bnl" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="LithelyJaine" data-author-fullname="t2_eihbi3p7" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n782bnl/"><div id="thing_t1_n783rr0" onclick="click_thing(this)" data-fullname="t1_n783rr0" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="Andovars_Ghost" data-author-fullname="t2_185x3uep0r" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n783rr0/"><div id="thing_t1_n789iyk" onclick="click_thing(this)" data-fullname="t1_n789iyk" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="Biscuits4u2" data-author-fullname="t2_73776moa" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n789iyk/"><div><p>[–]<a href="https://old.reddit.com/user/MadeByTango">MadeByTango</a><span></span> <span title="1325">1325 points</span><span title="1326">1326 points</span><span title="1327">1327 points</span> <time title="Wed Aug 6 13:38:17 2025 UTC" datetime="2025-08-06T13:38:17+00:00">51 minutes ago</time>&nbsp;(66 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n78aepzm3m"><div><blockquote>
<p>This is some childish bullshit.</p>
</blockquote>

<p>Not it’s not, it’s <strong>criminal</strong>. They’re intentionally misleading citizens about their rights in a way that removes their rights. People with limited education will rely on this information as unimpeachable. <strong>And websites like this is where AIs scrape and source their data.</strong></p>

<p>They’re literally trying to rewrite history.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n78aepz/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n789iyk" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="siteTable_t1_n78aepz" onclick="click_thing(this)" data-fullname="t1_n78boy0" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="Jx31234" data-author-fullname="t2_h2t3r" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n78boy0/"><p>[–]<a href="https://old.reddit.com/user/boringestnickname">boringestnickname</a><span></span> <span title="3">3 points</span><span title="4">4 points</span><span title="5">5 points</span> <time title="Wed Aug 6 14:13:38 2025 UTC" datetime="2025-08-06T14:13:38+00:00">16 minutes ago</time>&nbsp;(0 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n78h9xnlfn"><div><p>Yeah, I don't understand why people are so hung up on whether or not LLMs are capable of programming (they're not, in any real sense, by the way) – the big issue here is that this is a second revolution in antidemocratic control over the dissemination of information.</p>

<p>People are literally being pushed into dependence on LLMs right now. In many cases, it's already a done deal. LLMs are owned and controlled.</p>

<p>You thought Murdock was a big deal?</p>

<p>This is a hundredfold worse.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n78h9xn/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n78cmgk" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_n78bdea" onclick="click_thing(this)" data-fullname="t1_n78bdea" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="CeeJayDK" data-author-fullname="t2_40srl" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n78bdea/"><p>[–]<a href="https://old.reddit.com/user/CeeJayDK">CeeJayDK</a><span></span> <span title="38">38 points</span><span title="39">39 points</span><span title="40">40 points</span> <time title="Wed Aug 6 13:43:22 2025 UTC" datetime="2025-08-06T13:43:22+00:00">46 minutes ago</time>&nbsp;(1 child)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n78bdeabdh"><div><p>Also congress did not approve the gift of a luxury jet from Qatar to Trump to serve as the new Air Force One.</p>

<p>"No Title of Nobility shall be granted by the United States: And no Person holding any Office of Profit or Trust under them, shall, without the Consent of the Congress, accept of any present, Emolument, Office, or Title, of any kind whatever, from any King, Prince, or foreign State."</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n78bdea/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n783rr0" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_n783s4p" onclick="click_thing(this)" data-fullname="t1_n783s4p" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="Curious_Document_956" data-author-fullname="t2_1qfii1xrjy" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n783s4p/"><div id="thing_t1_n7849d2" onclick="click_thing(this)" data-fullname="t1_n7849d2" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="LithelyJaine" data-author-fullname="t2_eihbi3p7" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n7849d2/"><div id="thing_t1_n788qzz" onclick="click_thing(this)" data-fullname="t1_n788qzz" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="Environmental-Day862" data-author-fullname="t2_8tkicjui" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n788qzz/"><p>[–]<a href="https://old.reddit.com/user/BitterFuture">BitterFuture</a><span></span> <span title="8">8 points</span><span title="9">9 points</span><span title="10">10 points</span> <time title="Wed Aug 6 13:33:26 2025 UTC" datetime="2025-08-06T13:33:26+00:00">56 minutes ago</time>&nbsp;(2 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n789hnj2yt"><div><blockquote>
<p>I don't know how this attack on our Democracy is acceptable to so many people.</p>
</blockquote>

<p>It turns out that many millions of people have always hated our democracy.</p>

<blockquote>
<p>I'm not a Trumper, but I feel like - and would like to think - if my guy or gal was in office, and doing these things, I'd cease my support for their undemocratic efforts to undermine the foundation upon which this country was built.</p>
</blockquote>

<p>Yeah, but if you supported him, you would by definition support undemocratic efforts to undermine the foundation upon which this country was built - and eventually destroy it.</p>

<p>We really are different. We just made ourselves blind to it for a long, long time.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n789hnj/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n788qzz" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div><div id="thing_t1_n787yt5" onclick="click_thing(this)" data-fullname="t1_n787yt5" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="WowImOldAF" data-author-fullname="t2_16s5zdb2uj" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n787yt5/"><p>[–]<a href="https://old.reddit.com/user/WowImOldAF">WowImOldAF</a><span></span> <span title="44">44 points</span><span title="45">45 points</span><span title="46">46 points</span> <time title="Wed Aug 6 13:25:19 2025 UTC" datetime="2025-08-06T13:25:19+00:00">1 hour ago</time>&nbsp;(25 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n787yt59iz"><div><p>I don't wanna get sent to a prison camp because I said something bad about the shitty administration on the internet... but it looks like that's where we're all headed in the next 1-3 years.</p>

<p>I actually deleted a bunch of my stuff with shreddit a little while back because I just didn't want to be targeted... maybe I was more worried than I should be, but it honestly wouldn't surprise me.</p>

<p>If u say something bad about dear leader in North Korea, Russia, etc you end up gone... I believe that's where we're headed atm.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n787yt5/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n784h4b" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_n787t6k" onclick="click_thing(this)" data-fullname="t1_n787t6k" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="Think-Airport-8933" data-author-fullname="t2_1d6bt1e888" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n787t6k/"><p>[–]<a href="https://old.reddit.com/user/Think-Airport-8933">Think-Airport-8933</a><span></span> <span title="25">25 points</span><span title="26">26 points</span><span title="27">27 points</span> <time title="Wed Aug 6 13:24:28 2025 UTC" datetime="2025-08-06T13:24:28+00:00">1 hour ago</time><time title="last edited 52 minutes ago" datetime="2025-08-06T13:37:08+00:00">*</time>&nbsp;(4 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n787t6kfii"><div><p>Sure, but remember, it was a few months before they decided to take down the Senate Intelligence Report on Russia before they started to pretend it didn’t exist. Now, with the goal of distracting from Epstein they had to move up the timeline on their plan for that in the future,  but if they follow the same pattern here this is because they plan on saying or doing something against what was removed.</p>

<p>To be honest I do not have enough faith in MAGA for them to be told this never existed and not believe it. They believe almost anything they are told to believe.</p>

<p>EDIT: now that I think about it more, would you really put it past this regime to pull up some archaic law from 1790 that interprets something as not being legal because it isn’t in on public display and then say that being “online“ is what public display is now? I mean, that’s really what their M.O. has been; carry out what you want to do through EO instead of legal channels,  slow down the judiciary trying to stop you from breaking the law, tell your cult this is judicial overreach when they inevitably rule against you and then just do what you want anyway until you create a crisis. Then if you do get forced back in line do it quietly  and tell your propaganda machine not to mention it so the cult still believes you are the God King they want you to be.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n787t6k/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n783s4p" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_n787t5j" onclick="click_thing(this)" data-fullname="t1_n787t5j" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="BlatantConservative" data-author-fullname="t2_9e2mv" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n787t5j/"><p>[–]<a href="https://old.reddit.com/user/BlatantConservative">BlatantConservative</a><span></span> <span title="18">18 points</span><span title="19">19 points</span><span title="20">20 points</span> <time title="Wed Aug 6 13:24:28 2025 UTC" datetime="2025-08-06T13:24:28+00:00">1 hour ago</time>&nbsp;(6 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n787t5jk90"><div><p>Hey OP just for my own factchecking sake, what specific website are you talking about? </p>

<p>The archives.gov and senate.gov and whitehouse.gov sites look normal to me. But the Constitution is on a lot of .gov sites so there could be something I missed. </p>

<p>This is important because it very much matters <em>who</em> did this.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n787t5j/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n782bnl" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div><div id="thing_t1_n7843pb" onclick="click_thing(this)" data-fullname="t1_n7843pb" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="Equivalent-Excuse-80" data-author-fullname="t2_84l9nplb" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n7843pb/"><div><p>[–]<a href="https://old.reddit.com/user/Equivalent-Excuse-80">Equivalent-Excuse-80</a><span></span> <span title="112">112 points</span><span title="113">113 points</span><span title="114">114 points</span> <time title="Wed Aug 6 13:04:12 2025 UTC" datetime="2025-08-06T13:04:12+00:00">1 hour ago</time>&nbsp;(45 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n7843pbaz5"><div><p>Do they think they’ve successfully changed the constitution?</p>

<p>Panic over this is wasteful. </p>

<p>The type of people who would fall for this are too illiterate to read the constitution in the first place. </p>

<p>Remember during Obama when the tea party was marching around with their pocket constitutions? They didn’t read those either and they had them in their pockets for months.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n7843pb/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li>report</li><li>reply</li></ul></div><div id="siteTable_t1_n7843pb" onclick="click_thing(this)" data-fullname="t1_n786yl0" data-type="comment" data-gildings="0" data-subreddit="law" data-subreddit-prefixed="r/law" data-subreddit-fullname="t5_2qh9k" data-subreddit-type="public" data-author="wycliffslim" data-author-fullname="t2_dag0n" data-replies="0" data-permalink="/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n786yl0/"><p>[–]<a href="https://old.reddit.com/user/wycliffslim">wycliffslim</a><span></span> <span title="15">15 points</span><span title="16">16 points</span><span title="17">17 points</span> <time title="Wed Aug 6 13:19:56 2025 UTC" datetime="2025-08-06T13:19:56+00:00">1 hour ago</time>&nbsp;(5 children)</p><form action="#" onsubmit="return post_form(this, 'editusertext')" id="form-t1_n786yl011p"><div><p>It's not wasteful to be concerned when the government is intentionally removing and limiting easy access to the founding laws of the country. </p>

<p>Or, to ask another way. WHY would this be done. What possible explanation would exist that would explain this. Like you said, it doesn't change the laws... so why would it be done? </p>

<p>Laws don't enforce themselves.</p>
</div></form><ul><li><a href="https://old.reddit.com/r/law/comments/1mj3ttx/constitution_of_the_united_states_website_has/n786yl0/" data-event-action="permalink" rel="nofollow">permalink</a></li><li>embed</li><li>save</li><li><a href="#n7843pb" data-event-action="parent" rel="nofollow">parent</a></li><li>report</li><li>reply</li></ul></div></div></div></div></div><p><span>π</span>&nbsp;<span>Rendered by PID 37012 on reddit-service-r2-loggedout-757fcb5c99-ppsnn at 2025-08-06 14:30:02.559599+00:00 running 6723ca3 country code: DK.</span></p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude Code IDE Integration for Emacs (252 pts)]]></title>
            <link>https://github.com/manzaltu/claude-code-ide.el</link>
            <guid>44811567</guid>
            <pubDate>Wed, 06 Aug 2025 13:17:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/manzaltu/claude-code-ide.el">https://github.com/manzaltu/claude-code-ide.el</a>, See on <a href="https://news.ycombinator.com/item?id=44811567">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Claude Code IDE for Emacs</h2><a id="user-content-claude-code-ide-for-emacs" aria-label="Permalink: Claude Code IDE for Emacs" href="#claude-code-ide-for-emacs"></a></p>
<p dir="auto"><a href="https://github.com/manzaltu/claude-code-ide.el/actions/workflows/test.yml"><img src="https://github.com/manzaltu/claude-code-ide.el/workflows/CI/badge.svg" alt="https://github.com/manzaltu/claude-code-ide.el/workflows/CI/badge.svg"></a>
  <a href="https://www.gnu.org/software/emacs/" rel="nofollow"><img src="https://camo.githubusercontent.com/e5e2c0cdd7eb562ab27df3a8f5a564f5d96dce41857e6712bef14d9e5b49bf4a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f474e55253230456d6163732d32382d2d33302d626c756576696f6c65742e737667" alt="https://img.shields.io/badge/GNU%20Emacs-28--30-blueviolet.svg" data-canonical-src="https://img.shields.io/badge/GNU%20Emacs-28--30-blueviolet.svg"></a>
  <a href="https://www.gnu.org/licenses/gpl-3.0" rel="nofollow"><img src="https://camo.githubusercontent.com/93ca07ea63e4649ae72b87de86d82eedeab50e744fa6e07ee95e5344779fb930/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d626c75652e737667" alt="https://img.shields.io/badge/License-GPL%20v3-blue.svg" data-canonical-src="https://img.shields.io/badge/License-GPL%20v3-blue.svg"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Claude Code IDE for Emacs provides native integration with Claude Code CLI through the Model Context Protocol (MCP). Unlike simple terminal wrappers, this package creates a bidirectional bridge between Claude and Emacs, enabling Claude to understand and leverage Emacs’ powerful features—from LSP and project management to custom Elisp functions. This transforms Claude into a true Emacs-aware AI assistant that works within your existing workflow and can interact with your entire Emacs ecosystem.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
  <li>Automatic project detection and session management</li>
  <li>Terminal integration with full color support using <code>vterm</code> or <code>eat</code></li>
  <li>MCP protocol implementation for IDE integration</li>
  <li>Tool support for file operations, editor state, and workspace info</li>
  <li>Extensible MCP tools server for accessing Emacs commands (xrefs, tree-sitter, project info, e.g.)</li>
  <li>Diagnostic integration with Flycheck and Flymake</li>
  <li>Advanced diff view with ediff integration</li>
  <li>Tab-bar support for proper context switching</li>
  <li>Selection and buffer tracking for better context awareness</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Emacs Tool Integration</h2><a id="user-content-emacs-tool-integration" aria-label="Permalink: Emacs Tool Integration" href="#emacs-tool-integration"></a></p>
<p dir="auto">This package enables Claude Code to leverage the full power of Emacs through MCP tools integration. Claude can directly access and utilize Emacs capabilities including:</p>
<ul dir="auto">
  <li><b>Language Server Protocol (LSP)</b> integration through xref commands for intelligent code navigation (eglot, lsp-mode and others)</li>
  <li><b>Tree-sitter</b> for syntax tree analysis and understanding code structure at the AST level</li>
  <li><b>Imenu</b> for structured symbol listing and navigation within files</li>
  <li><b>Project</b> integration for project-aware operations</li>
  <li><b>Any Emacs command or function</b> can be exposed as an MCP tool, allowing Claude to:
    <ul dir="auto">
      <li>Perform project-wide searches and refactoring</li>
      <li>Access specialized modes and their features</li>
      <li>Execute custom Elisp functions tailored to your workflow</li>
    </ul>
  </li>
</ul>
<p dir="auto">This deep integration means Claude Code understands your project context and can leverage Emacs’ extensive ecosystem to provide more intelligent and context-aware assistance.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Active File Awareness</h3><a id="user-content-active-file-awareness" aria-label="Permalink: Active File Awareness" href="#active-file-awareness"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/manzaltu/claude-code-ide.el/blob/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/file.png"><img src="https://github.com/manzaltu/claude-code-ide.el/raw/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/file.png"></a></p>
<p dir="auto"><i>Claude Code automatically knows which file you’re currently viewing in Emacs</i></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Code Selection Context</h3><a id="user-content-code-selection-context" aria-label="Permalink: Code Selection Context" href="#code-selection-context"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/manzaltu/claude-code-ide.el/blob/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/selection.png"><img src="https://github.com/manzaltu/claude-code-ide.el/raw/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/selection.png"></a></p>
<p dir="auto"><i>Claude Code can access and work with selected text in your buffers</i></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Advanced Diff View with Diagnostics</h3><a id="user-content-advanced-diff-view-with-diagnostics" aria-label="Permalink: Advanced Diff View with Diagnostics" href="#advanced-diff-view-with-diagnostics"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/manzaltu/claude-code-ide.el/blob/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/ediff_diag.png"><img src="https://github.com/manzaltu/claude-code-ide.el/raw/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/ediff_diag.png"></a></p>
<p dir="auto"><i>Integrated ediff view for code changes, with Claude Code able to directly access diagnostic data (errors, warnings, etc.) from opened files</i></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Automatic Text Mentions</h3><a id="user-content-automatic-text-mentions" aria-label="Permalink: Automatic Text Mentions" href="#automatic-text-mentions"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/manzaltu/claude-code-ide.el/blob/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/mentions.png"><img src="https://github.com/manzaltu/claude-code-ide.el/raw/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/mentions.png"></a></p>
<p dir="auto"><i>Automatically mention and reference selected text in Claude conversations</i></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Session Restoration</h3><a id="user-content-session-restoration" aria-label="Permalink: Session Restoration" href="#session-restoration"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/manzaltu/claude-code-ide.el/blob/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/restore.png"><img src="https://github.com/manzaltu/claude-code-ide.el/raw/25053b5f1b8123eed5c3f00e8b3e9687ee33391d/screenshots/restore.png"></a></p>
<p dir="auto"><i>Resume previous Claude Code conversations with the –resume flag</i></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<ul dir="auto">
  <li>Emacs 28.1 or higher</li>
  <li>Claude Code CLI installed and available in PATH</li>
  <li><code>vterm</code> or <code>eat</code> package (for terminal support)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing Claude Code CLI</h2><a id="user-content-installing-claude-code-cli" aria-label="Permalink: Installing Claude Code CLI" href="#installing-claude-code-cli"></a></p>
<p dir="auto">Follow the installation instructions at <a href="https://docs.anthropic.com/en/docs/claude-code" rel="nofollow">Claude Code Documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installing the Emacs Package</h2><a id="user-content-installing-the-emacs-package" aria-label="Permalink: Installing the Emacs Package" href="#installing-the-emacs-package"></a></p>
<p dir="auto">Currently, this package is in early development. To install using <code>use-package</code> and <a href="https://github.com/raxod502/straight.el">straight.el</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(use-package claude-code-ide
  :straight (:type git :host github :repo &quot;manzaltu/claude-code-ide.el&quot;)
  :bind (&quot;C-c C-'&quot; . claude-code-ide-menu) ; Set your favorite keybinding
  :config
  (claude-code-ide-emacs-tools-setup)) ; Optionally enable Emacs MCP tools"><pre>(<span>use-package</span> claude-code-ide
  <span>:straight</span> (<span>:type</span> git <span>:host</span> github <span>:repo</span> <span><span>"</span>manzaltu/claude-code-ide.el<span>"</span></span>)
  <span>:bind</span> (<span><span>"</span>C-c C-'<span>"</span></span> <span>.</span> claude-code-ide-menu) <span><span>;</span> Set your favorite keybinding</span>
  <span>:config</span>
  (claude-code-ide-emacs-tools-setup)) <span><span>;</span> Optionally enable Emacs MCP tools</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Basic Commands</h2><a id="user-content-basic-commands" aria-label="Permalink: Basic Commands" href="#basic-commands"></a></p>
<p dir="auto">The easiest way to interact with Claude Code IDE is through the transient menu interface, which provides visual access to all available commands. Simply run <code>M-x claude-code-ide-menu</code> to open the interactive menu.</p>
<markdown-accessiblity-table><table>
  <tbody><tr><th>Command</th><th>Description</th></tr>
  <tr><td><code>M-x claude-code-ide-menu</code></td><td>Open transient menu with all Claude Code commands</td></tr>
  <tr><td><code>M-x claude-code-ide-emacs-tools-setup</code></td><td>Set up built-in MCP tools (e.g. xref, project)</td></tr>
  <tr><td><code>M-x claude-code-ide</code></td><td>Start Claude Code for the current project</td></tr>
  <tr><td><code>M-x claude-code-ide-send-prompt</code></td><td>Send prompt to Claude from minibuffer input</td></tr>
  <tr><td><code>M-x claude-code-ide-continue</code></td><td>Continue most recent conversation in directory</td></tr>
  <tr><td><code>M-x claude-code-ide-resume</code></td><td>Resume Claude Code with previous conversation</td></tr>
  <tr><td><code>M-x claude-code-ide-stop</code></td><td>Stop Claude Code for the current project</td></tr>
  <tr><td><code>M-x claude-code-ide-switch-to-buffer</code></td><td>Switch to project’s Claude buffer</td></tr>
  <tr><td><code>M-x claude-code-ide-list-sessions</code></td><td>List all active Claude Code sessions and switch</td></tr>
  <tr><td><code>M-x claude-code-ide-check-status</code></td><td>Check if Claude Code CLI is installed and working</td></tr>
  <tr><td><code>M-x claude-code-ide-insert-at-mentioned</code></td><td>Send selected text to Claude prompt</td></tr>
  <tr><td><code>M-x claude-code-ide-send-escape</code></td><td>Send escape key to Claude terminal</td></tr>
  <tr><td><code>M-x claude-code-ide-insert-newline</code></td><td>Insert newline in Claude prompt (sends \ + Enter)</td></tr>
  <tr><td><code>M-x claude-code-ide-toggle</code></td><td>Toggle visibility of Claude Code window</td></tr>
  <tr><td><code>M-x claude-code-ide-show-debug</code></td><td>Show the debug buffer with WebSocket messages</td></tr>
  <tr><td><code>M-x claude-code-ide-clear-debug</code></td><td>Clear the debug buffer</td></tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Multi-Project Support</h2><a id="user-content-multi-project-support" aria-label="Permalink: Multi-Project Support" href="#multi-project-support"></a></p>
<p dir="auto">Claude Code IDE automatically detects your project using Emacs’ built-in <code>project.el</code>. Each project gets its own Claude Code instance with a unique buffer name like <code>*claude-code[project-name]*</code>.</p>
<p dir="auto">You can run multiple Claude Code instances simultaneously for different projects. Use <code>claude-code-ide-list-sessions</code> to see all active sessions and switch between them.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Window Management</h2><a id="user-content-window-management" aria-label="Permalink: Window Management" href="#window-management"></a></p>
<ul dir="auto">
  <li>Running <code>claude-code-ide</code> when a session is already active will toggle the window visibility</li>
  <li>The window can be closed with standard Emacs window commands (<code>C-x 0</code>) without stopping Claude</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration Variables</h3><a id="user-content-configuration-variables" aria-label="Permalink: Configuration Variables" href="#configuration-variables"></a></p>
<markdown-accessiblity-table><table>
  <tbody><tr><th>Variable</th><th>Description</th><th>Default</th></tr>
  <tr><td><code>claude-code-ide-cli-path</code></td><td>Path to Claude Code CLI</td><td><code>"claude"</code></td></tr>
  <tr><td><code>claude-code-ide-buffer-name-function</code></td><td>Function for buffer naming</td><td><code>claude-code-ide--default-buffer-name</code></td></tr>
  <tr><td><code>claude-code-ide-cli-debug</code></td><td>Enable CLI debug mode (-d flag)</td><td><code>nil</code></td></tr>
  <tr><td><code>claude-code-ide-cli-extra-flags</code></td><td>Additional CLI flags (e.g. “–model”)</td><td><code>""</code></td></tr>
  <tr><td><code>claude-code-ide-debug</code></td><td>Enable debug logging</td><td><code>nil</code></td></tr>
  <tr><td><code>claude-code-ide-terminal-backend</code></td><td>Terminal backend (vterm or eat)</td><td><code>'vterm</code></td></tr>
  <tr><td><code>claude-code-ide-log-with-context</code></td><td>Include session context in log messages</td><td><code>t</code></td></tr>
  <tr><td><code>claude-code-ide-debug-buffer</code></td><td>Buffer name for debug output</td><td><code>"*claude-code-ide-debug*"</code></td></tr>
  <tr><td><code>claude-code-ide-use-side-window</code></td><td>Use side window vs regular buffer</td><td><code>t</code></td></tr>
  <tr><td><code>claude-code-ide-window-side</code></td><td>Side for Claude window</td><td><code>'right</code></td></tr>
  <tr><td><code>claude-code-ide-window-width</code></td><td>Width for side windows (left/right)</td><td><code>90</code></td></tr>
  <tr><td><code>claude-code-ide-window-height</code></td><td>Height for side windows (top/bottom)</td><td><code>20</code></td></tr>
  <tr><td><code>claude-code-ide-focus-on-open</code></td><td>Focus Claude window when opened</td><td><code>t</code></td></tr>
  <tr><td><code>claude-code-ide-focus-claude-after-ediff</code></td><td>Focus Claude window after opening ediff</td><td><code>t</code></td></tr>
  <tr><td><code>claude-code-ide-show-claude-window-in-ediff</code></td><td>Show Claude window during ediff</td><td><code>t</code></td></tr>
  <tr><td><code>claude-code-ide-system-prompt</code></td><td>Custom system prompt to append</td><td><code>nil</code></td></tr>
  <tr><td><code>claude-code-ide-enable-mcp-server</code></td><td>Enable MCP tools server</td><td><code>nil</code></td></tr>
  <tr><td><code>claude-code-ide-mcp-server-port</code></td><td>Port for MCP tools server</td><td><code>nil</code> (auto-select)</td></tr>
  <tr><td><code>claude-code-ide-mcp-server-tools</code></td><td>Alist of exposed Emacs functions</td><td><code>nil</code></td></tr>
  <tr><td><code>claude-code-ide-diagnostics-backend</code></td><td>Diagnostics backend (auto/flycheck/flymake)</td><td><code>'auto</code></td></tr>
  <tr><td><code>claude-code-ide-prevent-reflow-glitch</code></td><td>Prevent terminal reflow glitch (bug #1422)</td><td><code>t</code></td></tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Side Window Configuration</h3><a id="user-content-side-window-configuration" aria-label="Permalink: Side Window Configuration" href="#side-window-configuration"></a></p>
<p dir="auto">Claude Code buffers open in a side window by default. You can customize the placement:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Open Claude on the left side
(setq claude-code-ide-window-side 'left)

;; Open Claude at the bottom with custom height
(setq claude-code-ide-window-side 'bottom
      claude-code-ide-window-height 30)

;; Open Claude on the right with custom width
(setq claude-code-ide-window-side 'right
      claude-code-ide-window-width 100)

;; Don't automatically focus the Claude window
(setq claude-code-ide-focus-on-open nil)

;; Keep focus on ediff control window when opening diffs
(setq claude-code-ide-focus-claude-after-ediff nil)

;; Hide Claude window during ediff for more screen space
(setq claude-code-ide-show-claude-window-in-ediff nil)"><pre><span><span>;</span>; Open Claude on the left side</span>
(<span>setq</span> claude-code-ide-window-side <span>'left</span>)

<span><span>;</span>; Open Claude at the bottom with custom height</span>
(<span>setq</span> claude-code-ide-window-side <span>'bottom</span>
      claude-code-ide-window-height <span>30</span>)

<span><span>;</span>; Open Claude on the right with custom width</span>
(<span>setq</span> claude-code-ide-window-side <span>'right</span>
      claude-code-ide-window-width <span>100</span>)

<span><span>;</span>; Don't automatically focus the Claude window</span>
(<span>setq</span> claude-code-ide-focus-on-open <span>nil</span>)

<span><span>;</span>; Keep focus on ediff control window when opening diffs</span>
(<span>setq</span> claude-code-ide-focus-claude-after-ediff <span>nil</span>)

<span><span>;</span>; Hide Claude window during ediff for more screen space</span>
(<span>setq</span> claude-code-ide-show-claude-window-in-ediff <span>nil</span>)</pre></div>
<p dir="auto">Or, if you’d prefer to use a regular window:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Use regular window instead of side window
(setq claude-code-ide-use-side-window nil)"><pre><span><span>;</span>; Use regular window instead of side window</span>
(<span>setq</span> claude-code-ide-use-side-window <span>nil</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Terminal Backend Configuration</h3><a id="user-content-terminal-backend-configuration" aria-label="Permalink: Terminal Backend Configuration" href="#terminal-backend-configuration"></a></p>
<p dir="auto">Claude Code IDE supports both <code>vterm</code> and <code>eat</code> as terminal backends. By default, it uses <code>vterm</code>, but you can switch to <code>eat</code> if preferred:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Use eat instead of vterm
(setq claude-code-ide-terminal-backend 'eat)

;; Or switch back to vterm (default)
(setq claude-code-ide-terminal-backend 'vterm)"><pre><span><span>;</span>; Use eat instead of vterm</span>
(<span>setq</span> claude-code-ide-terminal-backend <span>'eat</span>)

<span><span>;</span>; Or switch back to vterm (default)</span>
(<span>setq</span> claude-code-ide-terminal-backend <span>'vterm</span>)</pre></div>
<p dir="auto">The <code>eat</code> backend is a pure Elisp terminal emulator that may work better in some environments where <code>vterm</code> compilation is problematic. Both backends provide full terminal functionality including color support and special key handling.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Terminal Keybindings</h4><a id="user-content-terminal-keybindings" aria-label="Permalink: Terminal Keybindings" href="#terminal-keybindings"></a></p>
<p dir="auto">Claude Code IDE adds custom keybindings to the terminal for easier interaction:</p>
<markdown-accessiblity-table><table>
  <tbody><tr><th>Keybinding</th><th>Command</th><th>Description</th></tr>
  <tr><td><code>M-RET</code></td><td><code>claude-code-ide-insert-newline</code></td><td>Insert a newline in the prompt</td></tr>
  <tr><td><code>C-&lt;escape&gt;</code></td><td><code>claude-code-ide-send-escape</code></td><td>Send escape key to cancel operations</td></tr>
</tbody></table></markdown-accessiblity-table>
<p dir="auto">These keybindings are automatically set up for both <code>vterm</code> and <code>eat</code> backends and only apply within Claude Code terminal buffers.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Terminal Reflow Glitch Prevention (Temporary)</h4><a id="user-content-terminal-reflow-glitch-prevention-temporary" aria-label="Permalink: Terminal Reflow Glitch Prevention (Temporary)" href="#terminal-reflow-glitch-prevention-temporary"></a></p>
<p dir="auto">Claude Code IDE includes a temporary workaround for a known Claude Code bug (<a href="https://github.com/anthropics/claude-code/issues/1422" data-hovercard-type="issue" data-hovercard-url="/anthropics/claude-code/issues/1422/hovercard">#1422</a>) where terminal reflows during window resizes can cause uncontrollable scrolling. This workaround is enabled by default but can be disabled if needed:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Disable the terminal reflow glitch prevention (not recommended until bug is fixed)
(setq claude-code-ide-prevent-reflow-glitch nil)"><pre><span><span>;</span>; Disable the terminal reflow glitch prevention (not recommended until bug is fixed)</span>
(<span>setq</span> claude-code-ide-prevent-reflow-glitch <span>nil</span>)</pre></div>
<p dir="auto">The workaround will be removed once the upstream bug is fixed.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Diagnostics Configuration</h3><a id="user-content-diagnostics-configuration" aria-label="Permalink: Diagnostics Configuration" href="#diagnostics-configuration"></a></p>
<p dir="auto">Claude Code IDE supports both Flycheck and Flymake for code diagnostics. By default, it will automatically detect which one is active:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Let Claude Code automatically detect the active diagnostics backend
(setq claude-code-ide-diagnostics-backend 'auto) ; default

;; Or force a specific backend
(setq claude-code-ide-diagnostics-backend 'flycheck)
(setq claude-code-ide-diagnostics-backend 'flymake)"><pre><span><span>;</span>; Let Claude Code automatically detect the active diagnostics backend</span>
(<span>setq</span> claude-code-ide-diagnostics-backend <span>'auto</span>) <span><span>;</span> default</span>

<span><span>;</span>; Or force a specific backend</span>
(<span>setq</span> claude-code-ide-diagnostics-backend <span>'flycheck</span>)
(<span>setq</span> claude-code-ide-diagnostics-backend <span>'flymake</span>)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom Buffer Naming</h3><a id="user-content-custom-buffer-naming" aria-label="Permalink: Custom Buffer Naming" href="#custom-buffer-naming"></a></p>
<p dir="auto">You can customize how Claude Code buffers are named:</p>
<div dir="auto" data-snippet-clipboard-copy-content="(setq claude-code-ide-buffer-name-function
      (lambda (directory)
        (if directory
            (format &quot;*Claude:%s*&quot; (file-name-nondirectory (directory-file-name directory)))
          &quot;*Claude:Global*&quot;)))"><pre>(<span>setq</span> claude-code-ide-buffer-name-function
      (<span>lambda</span> (<span>directory</span>)
        (<span>if</span> directory
            (<span>format</span> <span>"</span><span>*Claude:<span>%s</span>*</span><span>"</span> (<span>file-name-nondirectory</span> (<span>directory-file-name</span> directory)))
          <span><span>"</span>*Claude:Global*<span>"</span></span>)))</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom CLI Flags</h3><a id="user-content-custom-cli-flags" aria-label="Permalink: Custom CLI Flags" href="#custom-cli-flags"></a></p>
<p dir="auto">You can pass additional flags to the Claude Code CLI:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Use a specific model
(setq claude-code-ide-cli-extra-flags &quot;--model opus&quot;)

;; Pass multiple flags
(setq claude-code-ide-cli-extra-flags &quot;--model opus --no-cache&quot;)

;; Flags are added to all Claude Code sessions"><pre><span><span>;</span>; Use a specific model</span>
(<span>setq</span> claude-code-ide-cli-extra-flags <span><span>"</span>--model opus<span>"</span></span>)

<span><span>;</span>; Pass multiple flags</span>
(<span>setq</span> claude-code-ide-cli-extra-flags <span><span>"</span>--model opus --no-cache<span>"</span></span>)

<span><span>;</span>; Flags are added to all Claude Code sessions</span></pre></div>
<p dir="auto">Note: These flags are appended to the Claude command after any built-in flags like <code>-d</code> (debug) or <code>-r</code> (resume).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom System Prompt</h3><a id="user-content-custom-system-prompt" aria-label="Permalink: Custom System Prompt" href="#custom-system-prompt"></a></p>
<p dir="auto">You can append a custom system prompt to Claude’s default prompt, allowing you to customize Claude’s behavior for specific projects or contexts:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Set a custom system prompt
(setq claude-code-ide-system-prompt &quot;You are an expert in Elisp and Emacs development.&quot;)

;; Or configure it per-project using dir-locals.el
;; In .dir-locals.el:
((nil . ((claude-code-ide-system-prompt . &quot;Focus on functional programming patterns and avoid mutations.&quot;))))

;; Set via the transient menu: M-x claude-code-ide-menu → Configuration → Set system prompt"><pre><span><span>;</span>; Set a custom system prompt</span>
(<span>setq</span> claude-code-ide-system-prompt <span><span>"</span>You are an expert in Elisp and Emacs development.<span>"</span></span>)

<span><span>;</span>; Or configure it per-project using dir-locals.el</span>
<span><span>;</span>; In .dir-locals.el:</span>
((<span>nil</span> <span>.</span> ((claude-code-ide-system-prompt <span>.</span> <span><span>"</span>Focus on functional programming patterns and avoid mutations.<span>"</span></span>))))

<span><span>;</span>; Set via the transient menu: M-x claude-code-ide-menu → Configuration → Set system prompt</span></pre></div>
<p dir="auto">When set, this adds the <code>--append-system-prompt</code> flag to the Claude command. Set to <code>nil</code> to disable (default).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Debugging</h3><a id="user-content-debugging" aria-label="Permalink: Debugging" href="#debugging"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Claude CLI Debug Mode</h4><a id="user-content-claude-cli-debug-mode" aria-label="Permalink: Claude CLI Debug Mode" href="#claude-cli-debug-mode"></a></p>
<p dir="auto">To enable debug mode for Claude Code CLI (passes the <code>-d</code> flag):</p>
<div dir="auto" data-snippet-clipboard-copy-content="(setq claude-code-ide-cli-debug t)"><pre>(<span>setq</span> claude-code-ide-cli-debug <span>t</span>)</pre></div>
<p dir="auto"><h4 tabindex="-1" dir="auto">Emacs Debug Logging</h4><a id="user-content-emacs-debug-logging" aria-label="Permalink: Emacs Debug Logging" href="#emacs-debug-logging"></a></p>
<p dir="auto">To enable debug logging within Emacs (logs WebSocket messages and JSON-RPC communication):</p>
<div dir="auto" data-snippet-clipboard-copy-content="(setq claude-code-ide-debug t)"><pre>(<span>setq</span> claude-code-ide-debug <span>t</span>)</pre></div>
<p dir="auto">Then view debug logs with:</p>
<ul dir="auto">
  <li><code>M-x claude-code-ide-show-debug</code> - Show the debug buffer</li>
  <li><code>M-x claude-code-ide-clear-debug</code> - Clear the debug buffer</li>
</ul>
<p dir="auto">The debug buffer shows:</p>
<ul dir="auto">
  <li>WebSocket connection events</li>
  <li>All JSON-RPC messages (requests/responses)</li>
  <li>Error messages and diagnostics</li>
  <li>General debug information with session context</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Multiple Claude Code Instances on One Project</h2><a id="user-content-multiple-claude-code-instances-on-one-project" aria-label="Permalink: Multiple Claude Code Instances on One Project" href="#multiple-claude-code-instances-on-one-project"></a></p>
<p dir="auto">Using git worktrees is the recommended way for running multiple Claude Code instances on different branches of the same project. This allows you to develop features or fix bugs in parallel:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Create a new worktree for a feature branch
git worktree add ../myproject-worktree feature-branch"><pre><span><span>#</span> Create a new worktree for a feature branch</span>
git worktree add ../myproject-worktree feature-branch</pre></div>
<div dir="auto" data-snippet-clipboard-copy-content=";; Start Claude Code in the main project
find-file /path/to/myproject
M-x claude-code-ide

;; Start another Claude Code instance in the worktree
find-file /path/to/myproject-worktree
M-x claude-code-ide"><pre><span><span>;</span>; Start Claude Code in the main project</span>
<span>find-file</span> /path/to/myproject
M-x claude-code-ide

<span><span>;</span>; Start another Claude Code instance in the worktree</span>
<span>find-file</span> /path/to/myproject-worktree
M-x claude-code-ide</pre></div>
<p dir="auto">Each worktree is treated as a separate project by <code>project.el</code>, allowing you to have independent Claude Code sessions with their own buffers (e.g., <code>*claude-code[myproject]*</code> and <code>*claude-code[myproject-worktree]*</code>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Emacs MCP Tools</h2><a id="user-content-emacs-mcp-tools" aria-label="Permalink: Emacs MCP Tools" href="#emacs-mcp-tools"></a></p>
<p dir="auto">Claude Code IDE includes built-in MCP tools that expose Emacs functionality to Claude, enabling powerful code navigation and analysis capabilities:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Built-in Tools</h3><a id="user-content-built-in-tools" aria-label="Permalink: Built-in Tools" href="#built-in-tools"></a></p>
<ul dir="auto">
  <li><code>xref-find-references</code> - Find all references to a symbol throughout the project</li>
  <li><code>xref-find-apropos</code> - Find symbols matching a pattern across the entire project</li>
  <li><code>treesit-info</code> - Get tree-sitter syntax tree information for deep code structure analysis</li>
  <li><code>imenu-list-symbols</code> - List all symbols (functions, classes, variables) in a file using imenu</li>
  <li><code>project-info</code> - Get information about the current project (directory, files, etc.)</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Enabling MCP Tools</h3><a id="user-content-enabling-mcp-tools" aria-label="Permalink: Enabling MCP Tools" href="#enabling-mcp-tools"></a></p>
<p dir="auto">To enable these tools, add to your configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content=";; Set up the built-in Emacs tools
(claude-code-ide-emacs-tools-setup)"><pre><span><span>;</span>; Set up the built-in Emacs tools</span>
(claude-code-ide-emacs-tools-setup)</pre></div>
<p dir="auto">Once enabled, Claude can use these tools to navigate your codebase. For example:</p>
<ul dir="auto">
  <li>“Find the definition of function foo”</li>
  <li>“Show me all places where this variable is used”</li>
  <li>“What type of AST node is under the cursor?”</li>
  <li>“Analyze the parse tree of this entire file”</li>
  <li>“List all functions and variables in this file”</li>
  <li>“How many files are in this project?”</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Creating Custom MCP Tools</h2><a id="user-content-creating-custom-mcp-tools" aria-label="Permalink: Creating Custom MCP Tools" href="#creating-custom-mcp-tools"></a></p>
<p dir="auto">You can expose your own Emacs functions to Claude through the MCP tools system. This allows Claude to interact with specialized Emacs features, custom commands, or domain-specific functionality.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tool Definition Format</h3><a id="user-content-tool-definition-format" aria-label="Permalink: Tool Definition Format" href="#tool-definition-format"></a></p>
<p dir="auto">Each tool definition in <code>claude-code-ide-mcp-server-tools</code> follows this format:</p>
<div dir="auto" data-snippet-clipboard-copy-content="'(function-name
  :description &quot;Human-readable description for Claude&quot;
  :parameters ((:name &quot;param1&quot;
                :type &quot;string&quot;
                :required t
                :description &quot;What this parameter does&quot;)
               (:name &quot;param2&quot;
                :type &quot;number&quot;
                :required nil
                :description &quot;Optional parameter&quot;)))"><pre>'(function-name
  <span>:description</span> <span><span>"</span>Human-readable description for Claude<span>"</span></span>
  <span>:parameters</span> ((<span>:name</span> <span><span>"</span>param1<span>"</span></span>
                <span>:type</span> <span><span>"</span>string<span>"</span></span>
                <span>:required</span> <span>t</span>
                <span>:description</span> <span><span>"</span>What this parameter does<span>"</span></span>)
               (<span>:name</span> <span><span>"</span>param2<span>"</span></span>
                <span>:type</span> <span><span>"</span>number<span>"</span></span>
                <span>:required</span> <span>nil</span>
                <span>:description</span> <span><span>"</span>Optional parameter<span>"</span></span>)))</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Context-Aware Tool Example</h3><a id="user-content-context-aware-tool-example" aria-label="Permalink: Context-Aware Tool Example" href="#context-aware-tool-example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="(defun my-project-grep (pattern)
  &quot;Search for PATTERN in the current session's project.&quot;
  (claude-code-ide-mcp-server-with-session-context nil
    ;; This executes with the session's project directory as default-directory
    (let* ((project-dir default-directory)
           (results (shell-command-to-string
                    (format &quot;rg -n '%s' %s&quot; pattern project-dir))))
      results)))

(add-to-list 'claude-code-ide-mcp-server-tools
             '(my-project-grep
               :description &quot;Search for pattern in project files&quot;
               :parameters ((:name &quot;pattern&quot;
                            :type &quot;string&quot;
                            :required t
                            :description &quot;Pattern to search for&quot;))))"><pre>(<span>defun</span> <span>my-project-grep</span> (<span>pattern</span>)
  <span><span>"</span>Search for PATTERN in the current session's project.<span>"</span></span>
  (claude-code-ide-mcp-server-with-session-context <span>nil</span>
    <span><span>;</span>; This executes with the session's project directory as default-directory</span>
    (<span>let*</span> ((project-dir default-directory)
           (results (<span>shell-command-to-string</span>
                    (<span>format</span> <span>"</span><span>rg -n '<span>%s</span>' <span>%s</span></span><span>"</span> pattern project-dir))))
      results)))

(<span>add-to-list</span> <span>'claude-code-ide-mcp-server-tools</span>
             '(my-project-grep
               <span>:description</span> <span><span>"</span>Search for pattern in project files<span>"</span></span>
               <span>:parameters</span> ((<span>:name</span> <span><span>"</span>pattern<span>"</span></span>
                            <span>:type</span> <span><span>"</span>string<span>"</span></span>
                            <span>:required</span> <span>t</span>
                            <span>:description</span> <span><span>"</span>Pattern to search for<span>"</span></span>))))</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the GNU General Public License v3.0 or later. See the LICENSE file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Related Projects</h2><a id="user-content-related-projects" aria-label="Permalink: Related Projects" href="#related-projects"></a></p>
<ul dir="auto">
  <li><a href="https://docs.anthropic.com/en/docs/claude-code" rel="nofollow">Claude Code CLI</a></li>
  <li><a href="https://github.com/anthropics/claude-code">Claude Code VS Code Extension</a></li>
  <li><a href="https://github.com/coder/claudecode.nvim">claudecode.nvim</a> - Neovim integration</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU proposal to scan all private messages gains momentum (157 pts)]]></title>
            <link>https://cointelegraph.com/news/eu-chat-control-plan-gains-support-threatens-encryption</link>
            <guid>44811564</guid>
            <pubDate>Wed, 06 Aug 2025 13:17:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cointelegraph.com/news/eu-chat-control-plan-gains-support-threatens-encryption">https://cointelegraph.com/news/eu-chat-control-plan-gains-support-threatens-encryption</a>, See on <a href="https://news.ycombinator.com/item?id=44811564">Hacker News</a></p>
Couldn't get https://cointelegraph.com/news/eu-chat-control-plan-gains-support-threatens-encryption: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Wired Called Our AirGradient Monitor 'Not Recommended' over a Broken Display (114 pts)]]></title>
            <link>https://www.airgradient.com/blog/wired-review-of-airgradient-one-not-recommended/</link>
            <guid>44810574</guid>
            <pubDate>Wed, 06 Aug 2025 11:27:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.airgradient.com/blog/wired-review-of-airgradient-one-not-recommended/">https://www.airgradient.com/blog/wired-review-of-airgradient-one-not-recommended/</a>, See on <a href="https://news.ycombinator.com/item?id=44810574">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Two weeks ago, I had what I can only describe as a <strong>punch-to-the-stomach moment</strong> (which luckily doesn’t happen very often). The AirGradient ONE - our monitor that was recognized in one of the world’s most rigorous scientific evaluations - suddenly became <strong>“Not Recommended”</strong> by WIRED magazine in their <a rel="nofollow" target="_blank" href="https://www.wired.com/gallery/best-indoor-air-quality-monitors/">The Best Indoor Air Quality Monitors</a> review.</p><p>Yes, this is the same monitor that got two awards from the <a href="https://airlab.solutions/en/actualites/resultats-du-challenge-airlab-microcapteurs-2023-203" target="_blank">AirLab micro sensor challenge</a> , one of the most rigorous sensor testing programs, <strong>beating more than 30 other well known brands</strong>. And yes, this is also the same monitor that the University of Cambridge chose -after rigorous testing- for the largest study on classroom air quality in the world. And yes, it’s the same monitor that is loved by thousands of Home Assistant users for its easy and local integration.</p><p>Today, I want to share why we feel that this is a <strong>flawed review</strong> and the larger picture of how influential publications’ recommendations affect manufacturers as well as consumers.</p><p>As the founder of AirGradient, many of you know that <strong>transparency is how we operate</strong>. This is why we are open-source and why we openly talk about our successes as well as our failures. We work closely with scientists and communities to improve air quality.</p><p>We’re often more critical of ourselves than others are, constantly raising our own bar. So when something like this happens, it feels <strong>deeply unfair</strong>, like when a teacher gives you an F because your pencil broke during the exam, while giving As to students who didn’t even submit their work. But let’s not get ahead of ourselves; we will explain the details below.</p><p>We’re a small team building open-source monitors, competing against companies with <strong>massive marketing budgets and PR machines</strong>. We started as a volunteer project in Thailand, putting impact before profit.</p><p>What we lack in marketing power, we make up for with genuine care, transparency, and authenticity. Over <strong>50,000 users</strong> trust our open-source approach because we believe in giving people control over their data and the ability to repair their own devices. Our monitors offer much more value at a lower price, and our competitors are probably not very happy about this.</p><p>Many companies would try to sweep this “Not Recommended” review under the rug or send lawyers. But I feel I have a responsibility here to <strong>address this head-on</strong>, because our community deserves to know the details, and hopefully, this will trigger a larger discussion around the integrity of tech journalism.</p><p>Now, at this point, I would really recommend that you read the <a rel="nofollow" target="_blank" href="https://www.wired.com/gallery/best-indoor-air-quality-monitors/">review</a> so that you can <strong>form your own opinion</strong> about it.</p><h2 id="why-did-the-airgradient-one-get-a-not-recommended">Why did the AirGradient One get a “Not Recommended”?</h2><p>The <strong>primary reason</strong> cited for the “Not Recommended” label was a <strong>failing display</strong> on the review unit. Let me be clear: this was a legitimate hardware failure, and we take full responsibility for it. As soon as we learned about the issue, we immediately sent replacement parts and a new unit, including repair instructions, as repairability is one of our core differentiators.</p><p>However, the reviewers logic is difficult to follow when you compare it across products:</p><ul><li><strong>Our monitor</strong>: Downgraded due to a faulty display (a warranty-covered hardware issue)</li><li><strong>Another Monitor</strong>: Recommended, despite having <strong>no display at all</strong></li><li><strong>Another Monitor</strong>: Also recommended, despite <strong>lacking a CO2 sensor</strong>—one of the most critical metrics for assessing indoor air quality and ventilation</li></ul><p>How can a product be penalized for a failing display when another recommended product has no display? How can an indoor monitor without CO2 sensing - essential for understanding indoor air quality - be recommended over one that includes this crucial measurement?</p><p>If the author had reached out regarding the failing display (a single unit from any brand can fail - even those known for high quality) and noted that their first unit failed in the final review, we would understand. But to fail the whole unit on what we know is an isolated issue feels very unfair.</p><div><p>However, what’s <strong>most shocking</strong> for me is that this review is labelled as “The Best Indoor Air Quality Monitors”, yet it has <strong>basically no testing methodology at all</strong> and is pretty much purely based on the personal preferences of the author.</p></div><p>The icing on the cake is that I actually corresponded with the author about the issues and sent detailed explanations of our methodology concerns. The responses I got back were limited to brief acknowledgements like “Received. I’m on deadline for three other stories and cannot give you a timeline.”</p><p>This response encapsulates many things wrong with the current state of tech journalism: recommendations that affect <strong>livelihoods and purchasing decisions</strong> are treated as just deadlines to meet, not as professional evaluations that deserve rigorous methodology and meaningful dialogue.</p><p>I wonder how much the author actually cares about the quality and correctness of her reviews.</p><h2 id="when-reviews-fail-everyone-pays">When Reviews Fail, Everyone Pays</h2><p>WIRED’s recommendations don’t just influence individual purchases; they have the power to <strong>shape entire market perceptions</strong>. When they say “Not Recommended,” small companies like ours feel a real reputational and financial impact.</p><p>Now, if this were based on a clear methodology and evaluation, I would probably be the first one who would jump into gear and improve our product (which I have done in the past, e.g. when we had issues with the <a href="https://www.airgradient.com/blog/update-on-pms5003-calibration/">calibration of our PM sensors</a>). But getting ‘Not Recommended’ based on <strong>basically non-existent methodology</strong> leaves me frustrated. How do you argue against personal preference?</p><p><strong>BUT it’s not only us who lose, ultimately it’s the consumer!</strong></p><p>When a major publication abandons objective methodology in favor of subjective impressions, readers get recommendations based on <strong>one person’s preferences</strong> rather than a fair and comprehensive evaluation. They miss out on products that might better serve their actual needs, whether that’s due to repairability, accuracy, connectivity or comprehensive sensor coverage.</p><p>Here’s what any credible product review should include—especially when the publication claims to identify “the best” products:</p><ul><li><strong>Clear Evaluation Criteria</strong>: Define what makes a product good or bad upfront. Is it accuracy? Usability? Value? Readers deserve to know the standards being applied, not discover them through inconsistent conclusions.</li><li><strong>Consistent Methodology</strong>: Apply the same criteria to every product. If display quality matters for one device, it should matter for all devices. If a feature is praised in one product, it shouldn’t be ignored in another.</li><li><strong>Objective Testing</strong>: Use measurable standards wherever possible. For technical products, this means controlled conditions, standardized protocols, and leveraging existing independent testing data rather than relying solely on personal impressions.</li><li><strong>Multiple Data Points</strong>: Test more than one unit when possible, especially if hardware failures occur. A single faulty device shouldn’t define an entire product line’s quality.</li><li><strong>Feature-Based Analysis</strong>: Evaluate products based on their intended use and core capabilities. What problems are they designed to solve? How well do they solve them compared to alternatives?</li><li><strong>Transparent Process</strong>: Explain how conclusions were reached. What was tested? Under what conditions? What weights were given to different criteria?</li></ul><p>Without these foundations, “best of” guides become opinion pieces masquerading as authoritative recommendations—and that’s not fair to either manufacturers or consumers trying to make informed decisions.</p><p>We know that testing air quality monitors is not easy. This is why we’ve outlined proper testing methodology in our guide: <a href="https://www.airgradient.com/blog/how-to-test-an-air-quality-monitor/">How to Test an Air Quality Monitor</a>. From a publication as influential as WIRED, can we not expect a <strong>proper evaluation</strong>?</p><p>Who is the <strong>ultimate loser</strong>? <strong>The consumer</strong>. When accuracy matters for health decisions—understanding air quality affects everything from asthma management to sleep quality—flawed methodology means people miss products that might better serve their actual needs.</p><div><p><img src="https://www.airgradient.com/images/blog/posts/wired-review/wired-one.webp" alt="Forest in New Zealand." loading="lazy" decoding="async"></p><div><p>Many thanks to our designer Cid for this nice illustration!</p></div></div><h2 id="why-were-embracing-not-recommended">Why We’re Embracing “Not Recommended”</h2><p>To WIRED: don’t worry, we’re not hiring lawyers or reputation management firms to try burying this review. That’s not who we are. Instead, we’re doing something different—<strong>we’re amplifying it</strong>. We want people to see exactly what passes for “comprehensive” evaluation at your publication.</p><p>Here’s what we stand for: prioritizing sensor accuracy over marketing budgets, building products people can actually repair in a throwaway world, and being transparent about our failures while working to fix them. These are the principles that built our community of 50,000+ users.</p><p>Most companies would be celebrating a “Best Overall” rating and staying quiet about the methodology. But here’s the thing: <strong>even if we’d won, I’d still be writing this article</strong>. Poor methodology doesn’t just hurt the companies that get unfair ratings—it hurts consumers making health decisions based on flawed recommendations, and it hurts the entire industry when subjective preferences masquerade as expert evaluation.</p><blockquote><div><p>This is <strong>bigger than AirGradient</strong>. When publications with millions of readers abandon rigorous standards, they’re not just affecting one company’s sales and reputation —they’re shaping market perceptions, influencing purchasing decisions that affect people’s health, and setting a precedent that opinion journalism is acceptable where technical expertise should rule.</p></div></blockquote><p>We remain committed to what we’ve always done: building accurate, repairable, and affordable monitors through independent scientific testing and open-source transparency. We’ll keep standing by our products with comprehensive support, because that’s what our community deserves.</p><p>We’ve escalated our concerns to WIRED’s editorial leadership—not seeking special treatment or demanding apologies, but asking for the <strong>consistent, professional evaluation</strong> that consumers deserve when making decisions about their health and indoor environment. We believe in independent journalism, but independence means nothing without professional standards.</p><p>So yes, we’re <strong>embracing our “Not Recommended” rating</strong>. It’s become a badge of honor—proof that we prioritize substance over relationships, transparency over marketing polish, and community trust over media approval.</p><h2 id="what-this-means-for-all-of-us">What This Means for All of Us</h2><p>To our community: The whole AirGradient team and I appreciate your continued support through this. You’ve stood by us not because of what magazines say, but because you’ve experienced our commitment to accuracy, repairability, and transparency firsthand. That means everything.</p><p>But this experience reinforces something that goes beyond AirGradient: when influential publications abandon rigorous methodology, it creates a <strong>broken ecosystem</strong>. Manufacturers get incentivized to invest in PR relationships instead of product quality. Consumers lose access to reliable information exactly when they need it most—when making decisions about their health and indoor environment. And the companies actually doing the hard work of innovation get drowned out by those with bigger marketing budgets.</p><p>The air quality monitoring space is already confusing enough. People are trying to protect their families from pollution, manage asthma, improve sleep quality, and make informed decisions about their indoor environments. They deserve better than subjective opinion pieces dressed up as authoritative guides.</p><p>Here’s what I’m curious about: <strong>Is this the norm now?</strong> Are you seeing this same pattern across other product categories? When you’re researching purchases—whether it’s air quality monitors, smart home devices, or any technical product—what sources do you actually trust?</p><p>And specifically for situations like this: <strong>How would you want us to handle it?</strong> Should companies stay quiet when review methodology breaks down? Should we be more aggressive in calling this out? Or is transparency and open discussion the right approach?</p><p>I’m asking because this affects all of us. Every time we let poor methodology slide, we’re accepting a world where marketing budgets matter more than product quality, where personal preferences get presented as expert evaluation, and where the companies trying to do right by their communities get penalized for it.</p><p><strong>Drop a comment below</strong> or reach out directly—I read every message, and your perspectives help shape how we navigate these situations. Because at the end of the day, we’re not just building monitors; we’re trying to build a better way of doing business, and that requires all of us pushing for higher standards.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NautilusTrader: Open-source algorithmic trading platform (117 pts)]]></title>
            <link>https://nautilustrader.io/</link>
            <guid>44810552</guid>
            <pubDate>Wed, 06 Aug 2025 11:23:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nautilustrader.io/">https://nautilustrader.io/</a>, See on <a href="https://news.ycombinator.com/item?id=44810552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Complex strategies, simple solutions</p><p>Easily implement complex trading strategies using simple, modular components like Clock, Cache, MessageBus, Portfolio, and Actors.</p></div><div><p>Accurate timing</p><p>Achieve precise time handling with our nanosecond-resolution clock, ensuring consistent alerts and timers for both backtesting and live trading environments.</p></div><div><p>Fast config</p><p>Leverage powerful configuration options to trade across various venues, instruments, and parameter sets without altering your strategy code.</p></div><div><p>Advanced orders</p><p>Utilize advanced order types and execution instructions, including post-only, reduce-only, OCO, OTO, and other contingencies.</p></div><div><p>API integrations</p><p>Streamline the integration of new venues and data providers. Trade across multiple markets seamlessly on a single platform.</p></div><div><p>High performance</p><p>Experience unparalleled performance with core components meticulously crafted in Rust, ensuring speed and reliability.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM Inflation (112 pts)]]></title>
            <link>https://tratt.net/laurie/blog/2025/llm_inflation.html</link>
            <guid>44810307</guid>
            <pubDate>Wed, 06 Aug 2025 10:44:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tratt.net/laurie/blog/2025/llm_inflation.html">https://tratt.net/laurie/blog/2025/llm_inflation.html</a>, See on <a href="https://news.ycombinator.com/item?id=44810307">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<div id="article-other-blogs">
<p>Recent posts</p>






















<p><a href="https://tratt.net/laurie/blog/archive.html">Blog archive</a>
</p></div>

<div id="article-body">





<p>One of the signal achievements of computing is data <em>compression</em><span><a name="3016482be788use"></a>1</span>: we take in data, make it smaller while retaining <em>all</em>
information (“lossless” compression), transmit it, and then decompress it back to the original at the
other end.</p>
<p>For many years, compression was an absolute requirement to get things done:
storage devices were too small for the data we wanted to store and networks too
slow to transmit what we wanted at an acceptable speed.</p>
<p>Today compression is less often an absolute requirement but it can still make
our lives usefully better. For example, the page you’re reading right now has,
almost certainly, arrived to you in a <a href="https://en.wikipedia.org/wiki/HTTP_compression">compressed
form</a>. It was worth my time
making this work, because my site now displays more quickly on your screen and
the load on my server is reduced.</p>
<p>All of which makes me greatly amused to see that in 2025 we are now sometimes
doing the very opposite.</p>
<p>Here’s a simple example. Bob needs a new computer for his job. In order to
obtain a new work computer he has to create a 4 paragraph business case
explaining why the new computer will improve his productivity. Creating the
necessary prose is torturous for most of us, so Bob fires up the LLM du jour,
types in “Please create a 4 paragraph long business case for my manager,
explaining why I need to replace my old, slow computer” and copies the result
into his email.</p>
<p>Bob’s manager receives 4 paragraphs of dense prose and realises from the first line
that he’s going to have to read the whole thing carefully to work out what he’s
being asked for and why. Instead, he copies the email into the LLM du jour and
types at the start “Please summarise this email for me in one sentence”. The 4
paragraphs are summarised as “The sender needs a new computer as his current one is
old and slow and makes him unproductive.” The manager approves the request.</p>
<p>I have started to refer to this pattern – which I’ve now seen happen several
times in very different contexts – as <em>LLM inflation</em>. It is very easy to use
LLMs to turn short, simple content into something long and seemingly profound — and to
use LLMs to turn long and seemingly profound content into something short and simple.</p>
<p>That we are using LLMs for inflation should not be taken as a criticism of
these wonderful tools. It might, however, make us consider <em>why</em> we find
ourselves inflating content. At best we’re implicitly rewarding obfuscation
and time wasting; at worst we’re allowing a lack of clear thinking
to be covered up. I think we’ve all known this to be true, but LLMs allow us to
see the full extent of this with our own eyes. Perhaps it will encourage us to
change!</p>


<p>

2025-08-06 10:50

<a href="https://tratt.net/laurie/blog/2025/comparing_the_glove80_and_maltron_keyboards.html">Older</a>

</p>

<div id="article-updates"><p>
If you’d like updates on new blog posts: follow me on
<a href="https://mastodon.social/@ltratt">Mastodon</a>
or <a href="https://twitter.com/laurencetratt">Twitter</a>;
or <a href="https://tratt.net/laurie/blog/blog.rss">subscribe to the RSS feed</a>;
or <a href="https://tratt.net/laurie/newsletter/">subscribe to email updates</a>:

</p>

</div>


<h3>Footnotes</h3>




<h3>Comments</h3>







</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Japan: Apple Must Lift Browser Engine Ban by December (305 pts)]]></title>
            <link>https://open-web-advocacy.org/blog/japan-apple-must-lift-engine-ban-by-december/</link>
            <guid>44810061</guid>
            <pubDate>Wed, 06 Aug 2025 10:07:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://open-web-advocacy.org/blog/japan-apple-must-lift-engine-ban-by-december/">https://open-web-advocacy.org/blog/japan-apple-must-lift-engine-ban-by-december/</a>, See on <a href="https://news.ycombinator.com/item?id=44810061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Readers may recall that Japan recently passed the <a rel="noreferrer" target="_blank" href="https://open-web-advocacy.org/blog/japan-ends-the-apple-browser-ban/">Smartphone Act</a>, officially the <em>Bill on the Promotion of Competition for Specified Software Used in Smartphones</em>. Among its most important reforms is a direct prohibition on Apple’s long-standing ban on third-party browser engines on iOS.</p>
<p>This ban has functioned as an <a rel="noreferrer" target="_blank" href="https://open-web-advocacy.org/walled-gardens-report/#apple-has-effectively-banned-all-third-party-browsers">effective ban on browsers like Firefox, Chrome, Edge, Opera, Brave &amp; Vivaldi</a>, by forcing them to use Apple’s WebKit engine, which they cannot modify or control. This results in no effective browser competition on iOS, and web apps being deprived of the APIs and performance they need to compete with native apps.</p>
<p>The legislation was based on the <a rel="noreferrer" target="_blank" href="https://www.kantei.go.jp/jp/singi/digitalmarket/pdf_e/documents_230616.pdf">Final Report by Japan’s Headquarters for Digital Market Competition</a>, a report Open Web Advocacy consulted on. <a rel="noreferrer" target="_blank" href="https://open-web-advocacy.org/files/OWA%20-%20HDMC%20(Japan)%20-%20Competition%20in%20the%20Mobile%20App%20Ecosystem%20-%20v1.1.pdf">Our submission is available here</a>.</p>
<p>Last week, Japan published the <a rel="noreferrer" target="_blank" href="https://www.jftc.go.jp/file/MSCA_Guidelines_tentative_translation.pdf">Mobile Software Competition Act (MSCA) Guidelines</a>. These subordinate rules clarify how the Act will be interpreted and enforced. Here's what they mean for browser competition.</p>
<h2 id="ban-on-blocking-or-hindering-alternative-browser-engines" tabindex="-1"> Ban on Blocking or Hindering Alternative Browser Engines</h2>
<p>The guidelines explicitly prohibit any measures that would prevent or hinder the adoption of third-party browser engines:</p>
<blockquote>
<div><p><strong>Actions that "Prevent" the Adoption of Alternative Browser Engines"</strong></p><p>
Such actions may include: imposing unreasonable technical restrictions on individual app providers while allowing them to adopt alternative browser engines, placing excessive financial burdens on individual app providers for adopting alternative browser engines, and steering smartphone users away from using individual software that incorporates alternative browser engines.</p><p>
The determination of whether a designated provider's action constitutes "preventing" the adoption of alternative browser engines <span><strong>does not require that it be completely impossible for individual app providers to adopt alternative browser engines</strong></span>. Instead, the determination is made based <strong>on the degree of likelihood that such a result will occur.</strong><br>
<cite><a rel="noreferrer" target="_blank" href="https://www.jftc.go.jp/file/MSCA_Guidelines_tentative_translation.pdf">Mobile Software Competition Act Guidelines</a><br>
(emphasis added)</cite></p></div>
</blockquote>
<p>This clause is crucial. It means that designated providers (i.e. Apple) must not only eliminate outright bans (like <a rel="noreferrer" target="_blank" href="https://developer.apple.com/app-store/review/guidelines/#:~:text=Apps%20that%20browse%20the%20web%20must%20use%20the%20appropriate%20WebKit%20framework%20and%20WebKit%20JavaScript.">App Store Guideline 2.5.6</a>), but must also refrain from practices that, while technically permitting browser engines, <strong>render their use impractical or commercially unviable</strong>.</p>
<p>This is directly relevant to Apple’s current iOS behavior, even under the EU’s Digital Markets Act, where <a rel="noreferrer" target="_blank" href="https://open-web-advocacy.org/blog/apples-browser-engine-ban-persists-even-under-the-dma/">technical and procedural restrictions continue to block meaningful competition</a>. Japan’s guidance is clearly designed to avoid similar outcomes.</p>
<h2 id="api-access-must-be-functionally-equivalent" tabindex="-1"> API Access Must Be Functionally Equivalent</h2>
<p>The MSCA also mandates fair access to OS APIs, mirroring Article 6(7) of the EU DMA. For browsers this is particularly critical as they need extensive access to a broad range of APIs typically reserved for Safari and WebKit.</p>
<blockquote>
<p>Article 7, Item 2 of this Act prohibits designated providers of basic operation software from preventing other businesses (businesses other than designated providers, etc.) from using OS functions with equivalent performance for the provision of individual software, when such OS functions are used by the designated provider, etc., to provide individual software. <strong>By prohibiting actions that prevent other businesses from using OS functions with equivalent performance as utilized by designated providers</strong>, etc., for the provision of individual software, this Item aims to promote competition regarding individual software.<br>
<cite><a rel="noreferrer" target="_blank" href="https://www.jftc.go.jp/file/MSCA_Guidelines_tentative_translation.pdf">Mobile Software Competition Act Guidelines</a><br>
(emphasis added)</cite></p>
</blockquote>
<p>The act does specify that alternative APIs are allowed. However it clarifies that they may not be materially inferior.</p>
<blockquote>
<p>However, for example, if other businesses are allowed to use that OS function for individual software provision using a technical method different from that used by designated providers, etc., <strong>but the performance is materially inferior to that when designated providers</strong>, etc., use that OS function for individual software provision, it does not constitute "other businesses using the equivalent performance to provide individual software".<br>
<cite><a rel="noreferrer" target="_blank" href="https://www.jftc.go.jp/file/MSCA_Guidelines_tentative_translation.pdf">Mobile Software Competition Act Guidelines</a><br>
(emphasis added)</cite></p>
</blockquote>
<h2 id="choice-screens" tabindex="-1"> Choice Screens</h2>
<p>The act also mandates choice screens for browsers among other items. Importantly it specifies that the choice screen must display “<em>Promptly after the first activation",</em> an important improvement on the EU’s Digital Markets Act.</p>
<blockquote>
<p>that smartphone users must be made to select a specific individual software from the options displayed on the choice screen promptly after the first activation of the smartphone by the user. <strong>"Promptly after the first activation"</strong> refers to, for example, <strong>displaying the choice screen at the time of initial setup after the first activation of the smartphone</strong>, or displaying the choice screen at the time of the first launch of the individual software subject to the choice screen, and making users select a specific individual software from the options.<br>
<cite><a rel="noreferrer" target="_blank" href="https://www.jftc.go.jp/file/MSCA_Guidelines_tentative_translation.pdf">Mobile Software Competition Act Guidelines</a><br>
(emphasis added)</cite></p>
</blockquote>
<h2 id="what-happens-next%3F" tabindex="-1"> What Happens Next?</h2>
<p>The Mobile Software Competition Act is expected to come <a rel="noreferrer" target="_blank" href="https://globalcompetitionreview.com/review/the-asia-pacific-antitrust-review/2025/article/japan-authorities-prepare-use-bolstered-anti-monopoly-framework-scrutinise-digital-sector#:~:text=The%20Mobile%20Software%20Competition%20Act%20is%20expected%20to%20come%20into%20force%20by%20December%202025.">into force by December 2025</a>. With Japan joining the EU and UK, there are now three jurisdictions where Apple will be required to permit browsers to run their own engines. As Japan prepares for enforcement, it is likely studying the regulatory approaches and challenges already unfolding in Europe and the UK.</p>
<p>As the EU and UK have already shown (<a rel="noreferrer" target="_blank" href="https://open-web-advocacy.org/blog/uk-regulators-final-verdict--apples-browser-engine-ban-harms-competition/">UK MIR</a>, <a rel="noreferrer" target="_blank" href="https://open-web-advocacy.org/blog/uk-launches-investigation-into-apple-and-google-under-dmcc/">CMA SMS case</a>, <a rel="noreferrer" target="_blank" href="https://open-web-advocacy.org/blog/apples-browser-engine-ban-persists-even-under-the-dma/">EU DMA enforcement</a>), enforcement will be a long and difficult process.</p>
<p>Now that Japan, the EU, and the UK all require Apple to support third-party browser engines, 2026 may become the decisive year in restoring browser competition on iOS. But much depends on regulators’ resolve, and on Apple’s willingness to comply in substance, not just form.</p>
<p>We’d like to extend our gratitude to the extensive work over many years by the <a rel="noreferrer" target="_blank" href="https://www.kantei.go.jp/jp/singi/digitalmarket/index_e.html">HDMC</a>, <a rel="noreferrer" target="_blank" href="https://www.jftc.go.jp/en/">JFTC</a> and others in improving browser, browser engine and web app competition.</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Python performance myths and fairy tales (137 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1031707/73cb0cf917307a93/</link>
            <guid>44809387</guid>
            <pubDate>Wed, 06 Aug 2025 08:36:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1031707/73cb0cf917307a93/">https://lwn.net/SubscriberLink/1031707/73cb0cf917307a93/</a>, See on <a href="https://news.ycombinator.com/item?id=44809387">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<center>
<table>
<tbody><tr><td>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider accepting the discount offer on the right.  Thank you
for visiting LWN.net!
</p></td><td>
<div>
<h3>Special discount offer</h3>
           <p>
           <a href="https://lwn.net/Promo/sl-discount-3/claim">Subscribe to LWN now</a> at the
           "professional hacker" level for at least six months,
           and you will
           receive a special discount of 25%.
           
</p></div>
</td>
</tr>

</tbody></table>
</center>

<p>
Antonio Cuni, who
is a longtime Python performance engineer and <a href="https://pypy.org/">PyPy</a> developer, gave a presentation at <a href="https://ep2025.europython.eu/">EuroPython
2025</a> about "Myths and fairy tales around Python performance" on
the first day of the conference in Prague.  As might be guessed from the
title, he thinks that much of the conventional wisdom about Python
performance is misleading at best.  With lots of examples, he showed where
the real problems that he sees lie.  He has come to the conclusion that memory
management will ultimately limit what can be done about Python performance,
but he has an
early-stage project called <a href="https://github.com/spylang/spy?tab=readme-ov-file#spy">SPy</a> that
might be a way toward a super-fast Python.
</p>

<p>
He started by asking the audience to raise their hands if they thought
"<q>Python is slow or not fast enough</q>"; lots of hands went up, which
was rather different than when he gave the presentation at PyCon Italy,
where almost no one raised their hand. "<q>Very different audience</q>", he
said with a smile.  He has been working on Python performance for many
years, has talked with many Python developers, and heard some persistent
myths, which he would like to try to dispel.
</p>

<h4>Myths</h4>

<p>
The first is that "<q>Python is not slow</q>"; based on the raised hands,
though, he thought that most attendees already knew that was a myth.  These
days, he hears developers say that Python speed doesn't really matter,
because it is a glue language; "<q>nowadays only the GPU matters</q>", so
Python is fast enough.  Python <i>is</i> fast enough for some tasks, he
said, which is why there are so many people using it and attending
conferences like EuroPython.
</p>

<p>
There is a set of programs where Python is fast enough, but that set does
not hold all of the Python programs in use—it is only a subset.  The
programs that need more Python performance are what is driving all of the
different efforts to optimize the interpreter, but are also causing
developers to constantly work to improve the performance of their programs, often by using <a href="https://cython.org/">Cython</a>, <a href="https://numba.pydata.org/">Numba</a>, and the like.
</p>

<p><a href="https://lwn.net/Articles/1032100/">
<img src="https://static.lwn.net/images/2025/europy-cuni-sm.png" alt="[Antonio Cuni]" title="Antonio Cuni" width="220" height="280">
</a></p><p>
In his <a href="https://antocuni.eu/talk/2025/07/europython-myths-and-fairy-tales/">slides</a>,
he represented the two sets as circles, with "programs where Python is fast
enough" fully inside "Python programs"; he then added the set of "all
possible programs" fully encompassing the other two.  In his ideal world,
all possible programs would be able to be written with Python; currently,
programs that need all of the performance of the processor cannot use
Python.  He would like to see the inner circles grow so that Python can
be used in more programs.
</p>

<p>
The corollary of the "it's just a glue language" statement is that you
"just need to rewrite the hot parts in <span>C/C++</span>", though that is a little out
of date; "<q>nowadays they say that we should rewrite it in Rust</q>".
That is "<q>not completely false</q>", it is a good technique to speed up
your code, but soon enough it will "<q>hit a wall</q>".  The <a href="https://en.wikipedia.org/wiki/Pareto_principle">Pareto
principle</a>—described with a slide created by ChatGPT for unclear
reasons—says that 80% of the time will be spent in 20% of the code.  So
optimizing that 20% will help.
</p>

<p>
But the program will then run into <a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl's law</a>, which
says that the improvement for optimizing one part of the code is limited by
the time spent in the now-optimized code;
"<q>what was the hot part now is very very fast and then you need to
optimize everything else</q>".  He showed a diagram where some
<tt>inner()</tt> function was taking 80% of the time; if that gets reduced
to, say, 10% of what it was, the rest of the program now dominates the run
time. 
</p>

<p>
Another "myth" is that Python is slow because it is interpreted; again,
there is some truth to that, but interpretation is only a small part of
what makes Python slow.  He gave the example of a simple Python
expression:
</p><pre>    p.x * 2
</pre><p>
A compiler for C/C++/Rust could turn that kind of expression into three
operations: load the value of </p><tt>x</tt><p>, multiply it by two, and then
store the result.  In Python, however, there is a long list of operations
that have to be performed, starting with finding the type of </p><tt>p</tt><p>,
calling its </p><tt>__getattribute__()</tt><p> method, through <a href="https://en.wikipedia.org/wiki/Boxing_(computer_programming)">unboxing</a></p><tt>p.x</tt><p> and </p><tt>2</tt><p>, to finally boxing the result, which requires
memory allocation.  None of that is dependent on whether Python is
interpreted or not, those steps are required based on the language
semantics.
</p>

<h4>Static types</h4>

<p>
Now people are using static types in Python, so he hears people say that
compilers for the language can now skip past all of those steps and
simply do the operation directly.  He put up an example:
</p><pre>    def add(x: int, y: int) -&gt; int:
        return x + y

    print(add(2, 3))
</pre><p>
But static typing is not enforced at run time, so there are various ways to
call </p><tt>add()</tt><p> with non-integers, for example:
</p><pre>    print(add('hello ', 'world')) # type: ignore
</pre><p>
That is perfectly valid code and the type-checker is happy because of the
comment, but string addition is not the same as for integers.  The
static types "<q>are completely useless from the point of view of
optimization and performance</q>".  Beyond that, the following is legal
Python too:
</p><pre>    class MyClass:
        def __add__(self, other):
            ...

    def foo(x: MyClass, y: MyClass) -&gt; MyClass:
        return x + y

    del MyClass.__add__
</pre><p>
"<q>Static compilation of Python is problematic because everything can
change</q>", he said.
</p>

<p>
So, maybe, "<q>a JIT compiler can solve all of your problems</q>"; they can
go a long way toward making Python, or any dynamic language, faster, Cuni
said.  But that leads to "<q>a more subtle problem</q>".  He put up a slide
with a <a href="https://en.wikipedia.org/wiki/Trilemma">trilemma</a>
triangle: a dynamic language, speed, or a simple implementation.
You can have two of those, but not all three.
</p>

<p>
Python has historically favored a dynamic, simply implemented language, but
it is moving toward a dynamic, fast language with projects like the <a href="https://lwn.net/Articles/1029307/">CPython JIT compiler</a>.  That loses the simple
implementation, but he does not have to care "<q>because there are people
in the front row doing it for me</q>", he said with a grin.
</p>

<p>
In practice, though, it becomes hard to predict performance with a JIT.
Based on his experience with PyPy, and as a consultant improving Python
performance for customers, it is necessary to think about what the JIT will
do in order to get the best performance.  That is a complex and error-prone
process; he found situations where he was "<q>unable to trigger
optimizations in PyPy's compiler because the code was too complicated</q>".
</p>

<p>
All of this leads to what he calls "<q>optimization chasing</q>".  It
starts with a slow program that gets its fast path optimized, which results
in a faster program and everyone is happy.  Then they start to rely on that
extra speed, which can suddenly disappear with a seemingly unrelated change
somewhere in the program.  His favorite example is a program that was
running on PyPy (using Python 2) and suddenly got 10x slower; it turned out
that a Unicode key was being used in a dictionary of strings
that led the JIT to de-optimize the code so that everything got much
slower.
</p>

<h4>Dynamic</h4>

<p>
He put up some code that did not really do anything exciting or useful, he
said, but did demonstrate some of the problems that Python compilers
encounter: 
</p><pre>    import numpy as np

    N = 10

    def calc(v: np.ndarray[float], k: float) -&gt; float:
        return (v * k).sum() + N
</pre><p>
The compiler really can assume nothing from that code.  Seemingly, it
imports <a href="https://numpy.org/">NumPy</a> in the usual way, the
</p><tt>calc()</tt><p> function multiplies each element of the </p><tt>v</tt><p> array by
</p><tt>k</tt><p>, adds them all up with </p><tt>sum()</tt><p> and then adds the constant
</p><tt>N</tt><p> to that.  First off, the </p><tt>import</tt><p> may not bring in NumPy
at all; there could be some import hook somewhere that does something
completely unexpected.  </p><tt>N</tt><p> cannot be assumed to be ten, because
that could be changed elsewhere in the code; as with the earlier
</p><tt>add()</tt><p> function, the type declarations on </p><tt>calc()</tt><p> are not
ironclad either.
</p>

<p>
But, in almost all cases, that code would do exactly what it looks like it
does.  Developers rarely do these kinds of things that the language would
allow, but the
gap between the way programmers normally write Python and the definition of
the language is what "<q>makes life complicated for the interpreter</q>".
In practice, a lot of what Python allows does not actually happen.
</p>

<p>
It is the extremely dynamic nature of the language that makes it slow,
"<q>but at the same time it's what makes Python very nice</q>".  The
dynamic features are not needed 99% of the time, Cuni said, but "<q>in that
1% are what you need to make Python awesome</q>".  Libraries often use
patterns that rely on the dynamic nature of the language in order to make
APIs "<q>that end users can use nicely</q>" so those features cannot simply
be removed.
</p>

<h4>Game</h4>

<p>
The "compiler game" was up next; he progressively showed some code snippets
to point out how little a compiler can actually "know" about the code.
This code might seem like it should give an error of some sort:
</p><pre>    class Point:
        def __init__(self, x, y):
            self.x = x
            self.y = y

    def foo(p: Point):
        assert isinstance(p, Point)
        print(p.name) # ???
</pre><p>
Inside </p><tt>foo()</tt><p>, the compiler knows that </p><tt>p</tt><p> is a
</p><tt>Point</tt><p>, which has no </p><tt>name</tt><p> attribute.  But, of course,
Python is a dynamic language:
</p><pre>    def bar():
        p = Point(1, 2)
        p.name = 'P0'
        foo(p)
</pre><p>
Meanwhile, here is an example where the compiler cannot even assume that
the method exists:
</p><pre>    import random

    class Evil:
        if random.random() &gt; 0.5:
            def hello(self):
                print('hello world')

    Evil().hello() # 🤷🏻‍♂️
</pre><p>
Legal Python, but "<q>this is not something to define in production, I
hope</q>", he said with a laugh.  "<q>Half of the time it still works, half
of the time
it raises an exception. Good luck compiling it.</q>"
</p>

<p>
In another example, he showed a function:
</p><pre>    def foo():
        p = Person('Alice', 16)
        print(p.name, p.age)
        assert isinstance(p, Person) # &lt;&lt;&lt;
</pre><p>
The </p><tt>Person</tt><p> class was not shown (yet), but there was an empty class
(just "</p><tt>pass</tt><p>") called </p><tt>Student</tt><p>.  In this case, the
</p><tt>assert</tt><p> will fail, because of the definition of </p><tt>Person</tt><p>:
</p><pre>    class Person:
        def __new__(cls, name, age):
            if age &lt; 18:
                p = object.__new__(Student)
            else:
                p = object.__new__(Person)
            p.name = name
            p.age = age
            return p
</pre><p>
"<q>You can have a class with a dunder-new [i.e. <tt>__new__()</tt>], which
returns something which is unrelated and is not an instance of the class.
Good luck optimizing that.</q>"
</p>

<p>
The final entrant in the game was the following:
</p><pre>    N = 10

    @magic
    def foo():
       return N
</pre><p>
He "de-sugared" the </p><tt>@magic</tt><p> decorator and added some assertions:
</p><pre>    def foo():
       return N

    bar = magic(foo)

    assert foo.__code__ == bar.__code__
    assert bar.__module__ == '__main__'
    assert bar.__closure__ is None

    assert foo() == 10
    assert bar() == 20 # 🤯😱
</pre><p>
The code object for </p><tt>foo()</tt><p> and </p><tt>bar()</tt><p> are the same, but
they give different results.  As might be guessed, the value of </p><tt>N</tt><p>
has been changed by </p><tt>magic()</tt><p>; the code is as follows:
</p><pre>    def rebind_globals(func, newglobals):
        newfunc = types.FunctionType(
            func.__code__,
            newglobals,
            func.__name__,
            func.__defaults__,
            func.__closure__)
        newfunc.__module__ = func.__module__
        return newfunc

    def magic(fn):
        return rebind_globals(fn, {'N': 20})
</pre><p>
That returns a version of the function (</p><tt>foo()</tt><p> was passed) that has
a different view of the values of the global variables.  That may seem like
a far-fetched example, but he wrote <a href="https://github.com/pdbpp/pdbpp/blob/master/src/pdbpp.py#L114-L134">code
much like that</a> for the <a href="https://github.com/pdbpp/pdbpp/tree/master?tab=readme-ov-file#pdb-a-drop-in-replacement-for-pdb">pdb++
Python debugger</a> many years ago. "<q>I claim I had good reason to do
that</q>", he said with a chuckle.
</p>

<h4>Abstraction</h4>

<p>
So there are parts of the language that need to be accounted for, as he
showed in the game, but there is a more fundamental problem: "<q>in Python,
abstractions are not free</q>".  When code is written, developers want
performance, but they also want the code to be understandable and
maintainable. That comes at a cost.  He started with a simple function:
</p><pre>    def algo(points: list[tuple[float, float]]):
        res = 0
        for x, y in points:
            res += x**2 * y + 10
        return
</pre><p>
It takes a list of points, each represented as a tuple of floating-point
numbers, and performs a calculation using them.
Then he factored out the calculation into its own function:
</p><pre>    def fn(x, y):
        return x**2 * y + 10
</pre><p>
That is already slower than the original, because there 
is overhead for calling a function: the function has to be looked up, a
frame object has to be created, and so on.  A JIT compiler can help, but it
will still have more overhead.  He took things one step further by
switching to a </p><tt>Point</tt><p> data class:
</p><pre>    @dataclass
    class Point:
        x: float
        y: float

    def fn(p):
        return p.x**2 * p.y + 10

    def algo(items: list[Point]):
        res = 0
        for p in items:
            res += fn(p)
        return
</pre><p>
That, of course, slows it down even further.  This is a contrived example,
Cuni said, but the idea is that every abstraction has a cost, "<q>and then
you end up with a program that is very slow</q>".  It was an example of
what he calls "Python to Python" abstraction, where the code is being
refactored strictly within the language.
</p>

<p>
A "Python to C" abstraction, where the hot parts of the code are factored
out into C or some other compiled language, also suffers from added costs.
One could imagine that Python implementations get more and more
optimizations such that the list of <tt>Point</tt> objects is represented
in a simple linear array of floating-point numbers, without boxing, but if
<tt>fn()</tt> is written for Python's C API, those numbers will need to be
boxed and unboxed (in both directions), which is completely wasted work.
It is "<q>unavoidable with the current C API</q>".  One of the ways to
speed up programs that were running under PyPy was to remove the C code and
perform the calculations directly in Python, which PyPy could optimize
well.
</p>

<h4>An elephant</h4>

<p>
There is an elephant in the room, however, with regard to Python
performance, though it is one he rarely hears about: memory management.  In
today's hardware, "<q>computation is very cheap</q>", but memory is the
bottleneck.   If the data is in a cache at any level, accessing it is
inexpensive, but RAM accesses are quite slow.  "<q>Generally speaking, if
you want to have very very good performance, we should avoid cache misses
as much as possible.</q>"
</p>

<p>
But Python is prone to having a memory layout that is cache-unfriendly.  He
showed a simple example:
</p><pre>    class Person:
        def __init__(self, name, age):
            self.name = name
            self.age = age

    p = [Person('Alice', 16), Person('Bob', 21)]
</pre><p>
Each </p><tt>Person</tt><p> has two fields, which ideally would be placed together
in memory, and the two objects in the list would also be placed together,
for a cache-friendly layout.  In practice, though, those objects are all
scattered throughout memory; he showed a <a href="https://pythontutor.com/render.html#code=class%20Person%3A%0A%20%20%20%20def%20__init__%28self,%20name,%20age%29%3A%0A%20%20%20%20%20%20%20%20self.name%20%3D%20name%0A%20%20%20%20%20%20%20%20self.age%20%3D%20age%0A%20%20%20%20%0Ap%20%3D%20%5BPerson%28'Alice',%2016%29,%20Person%28'Bob',%2021%29%5D%0Adel%20Person&amp;cumulative=false&amp;curInstr=11&amp;heapPrimitives=true&amp;mode=display&amp;origin=opt-frontend.js&amp;py=3&amp;rawInputLstJSON=%5B%5D&amp;textReferences=false">visualization from
Python Tutor</a>.  Each arrow represented a pointer that needed to be
followed, thus a potential cache miss; there were nearly a dozen arrows for
this simple data structure.
</p>

<p>
"<q>This is something you cannot just solve with a JIT compiler; it's
impossible to solve it without changing semantics.</q>" Python is
inherently cache-unfriendly, he said, "<q>and I honestly don't know how to
solve this problem</q>".  His "sad truth" conclusion is that "<q>Python
cannot be super-fast</q>" without breaking compatibility.  Some of the
dynamic features ("<q>let's call it
craziness</q>")  he had described in the talk will eventually hamper performance improvements. "<q>If we
want to keep this craziness, well, we have to leave some performance on the
table.</q>"
</p>

<p>
His next slide was "The end", complete with emojis of sadness ("😢💔🥹"),
which is where he ended the talk when he gave it at PyCon Italy a year
earlier.  This time, though, he wanted to "<q>give a little hope</q>" so he
added a question mark, then reiterated that without breaking
compatibility Python could not get super-fast.
</p>

<p>
He has a proposal for the community if it decides that Python should try to
reach top-level performance, which he hopes the community does, but
"<q>it's fine to say 'no'</q>".  He suggests tweaking the language
semantics by keeping the dynamic features where they are actually useful,
perhaps by limiting the kinds of dynamic changes that can be made to
specific points in time, so that compilers can depend on certain behavior
and structure.  "<q>Not to allow the world to change at any point in
time as it is now.</q>"
</p>

<p>
Meanwhile, the type system should be revamped with an eye on performance.
Currently, the types are optional and not enforced, so they cannot be
used for optimizations.  The intent would be that performance-oriented code
could be written in Python, not in some other language called from Python.
But, for cases where calling another language is still desirable, the
extra cost (e.g. boxing) of doing so should be removed. "<q>Most
importantly, we want something which stays Pythonic, because we like this
language or we wouldn't be here.</q>"
</p>

<p>
Cuni said that he has a potential solution, "<q>which is not to make Python
faster</q>", because he claims that is not possible. SPy, which stands for
"Static Python", is a project he started a few years ago to address the
performance problems.  All of the standard disclaimers apply to SPy, it is
"<q>a work in progress, research and development, [and] we don't know where it
will go</q>".  The best information can be found on the GitHub page linked
above or in his <a href="https://antocuni.eu/2025/05/31/spy--pycon-it-2025/">talk on SPy at PyCon
Italy</a> in late May.
</p>

<p>
He showed a quick <a href="https://antocuni.pyscriptapps.com/sobel/latest/">demo</a> of doing
realtime edge detection from a camera; it ran in the browser using <a href="https://pyscript.net/">PyScript</a>.  The demo shows the raw camera
feed on the left
and, at first, edge detection being run in NumPy on the right; NumPy
achieves fewer than two frames per second (fps).  Switching to a SPy-based
edge-detection algorithm makes the right-hand image keep up with the
camera, running at around 60fps.  The <a href="https://github.com/spylang/demos/tree/main/sobel">code for the
demo</a> is available on GitHub as well.
</p>

<p>
He recommended the SPy repository and its issue tracker in particular for
interested attendees; some issues have been tagged as "good first issue"
and "help wanted".  There is also a <a href="https://discord.com/invite/wRb29FGZpP">Discord server</a> for
chatting about the project. Before too long, a video of the talk should
appear on the <a href="https://www.youtube.com/@EuroPythonConference">EuroPython YouTube channel</a>.
</p>

<p>
[I would like to thank the Linux Foundation, LWN's travel sponsor, for
travel assistance to Prague for EuroPython.]
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/ConferenceIndex/">Conference</a></td><td><a href="https://lwn.net/Archives/ConferenceIndex/#EuroPython-2025">EuroPython/2025</a></td></tr>
            <tr><td><a href="https://lwn.net/Archives/PythonIndex/">Python</a></td><td><a href="https://lwn.net/Archives/PythonIndex/#Performance">Performance</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic rejects the main developer of the library they use (599 pts)]]></title>
            <link>https://grell.dev/blog/ai_rejection</link>
            <guid>44808794</guid>
            <pubDate>Wed, 06 Aug 2025 07:25:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://grell.dev/blog/ai_rejection">https://grell.dev/blog/ai_rejection</a>, See on <a href="https://news.ycombinator.com/item?id=44808794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
                <article>
                    <figure>
                        <picture>
                            <!-- AVIF first -->
                            <source type="image/avif" srcset="https://grell.dev/assets/img/ai_rejection/320.avif  320w,
                                https://grell.dev/assets/img/ai_rejection/640.avif  640w,
                                https://grell.dev/assets/img/ai_rejection/1280.avif 1280w,
                                https://grell.dev/assets/img/ai_rejection/1920.avif 1920w,
                                https://grell.dev/assets/img/ai_rejection/4096.avif 4096w" sizes="(max-width: 640px) 100vw,
                                (max-width:1280px) 1280px, 1920px">
                            <!-- then WebP -->
                            <source type="image/webp" srcset="https://grell.dev/assets/img/ai_rejection/320.webp  320w,
                                https://grell.dev/assets/img/ai_rejection/640.webp  640w,
                                https://grell.dev/assets/img/ai_rejection/1280.webp 1280w,
                                https://grell.dev/assets/img/ai_rejection/1920.webp 1920w,
                                https://grell.dev/assets/img/ai_rejection/4096.webp 4096w" sizes="(max-width: 640px) 100vw,
                                (max-width:1280px) 1280px, 1920px">
                            <!-- finally JPEG fallback -->
                            <img src="https://grell.dev/assets/img/ai_rejection/1280.jpg" srcset="
                                https://grell.dev/assets/img/ai_rejection/320.jpg  320w,
                                https://grell.dev/assets/img/ai_rejection/640.jpg  640w,
                                https://grell.dev/assets/img/ai_rejection/1280.jpg 1280w,
                                https://grell.dev/assets/img/ai_rejection/1920.jpg 1920w,
                                https://grell.dev/assets/img/ai_rejection/4096.jpg 4096w" sizes="(max-width: 640px) 100vw,
                                (max-width:1280px) 1280px, 1920px" alt="An AI generated image in a cyberpunk style of an AI using it's hands to reject me. The AI looks humanoid and is sitting in front of a computer." fetchpriority="high">
                        </picture>
                        <figcaption>An AI generated image of an AI using its hands to reject me.
                            Very meta, I know
                        </figcaption>
                    </figure>

                    
                    <p><time datetime="2025-07-03">2025-07-03</time></p>
                    <p>In October 2024, Anthropic released <a href="https://www.youtube.com/watch?v=ODaHJzOyVCQ">"Claude
                            Computer Use"</a>. It allows an AI to control a computer and for example copy data from a
                        browser to a spreadsheet. It's a really cool feature and since I am the maintainer of a library
                        that allows controlling a computer, I was curious to find out how they do it and learn from
                        them. I didn't have time to look into it until this spring. Anthropic is one of the leaders in
                        AI and was <a href="https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation">valued
                            at a cool 60+ billion dollars</a> in March 2025, so I was surprised to learn that
                        Anthropic is actually using my library <a href="https://crates.io/crates/enigo">enigo</a> for
                        it.</p>

                    <figure>
                        <picture>
                            <img src="https://grell.dev/assets/img/blinking.webp" alt="My reaction to the finding represented by the 'white guy blinking' meme." fetchpriority="high">
                        </picture>
                        <figcaption>My reaction to the finding
                        </figcaption>
                    </figure>

                    <p>You can confirm that enigo is used in Claude Desktop for macOS by running the following two
                        commands:</p>

                    <pre><code><span>$ 7z x Claude.dmg</span>
<span>$ perl -nle 'print $&amp; while /.{0,67}enigo.{0,30}/g' Claude/Claude.app/Contents/Resources/app.asar.unpacked/node_modules/claude-native/claude-native-binding.node</span>
<span>/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/enigo-0.2.1/src/macos/macos_impl.rs</span>
<span>/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/enigo-0.2.1/src/macos/macos_impl.rs</span></code></pre>

                    <p>It is also used in Claude Desktop for Windows. You can confirm it by running:</p>

                    <pre><code><span>$ 7z x Claude-Setup-x64.exe</span>
<span>$ 7z x AnthropicClaude-0.11.6-full.nupkg</span>
<span>$ perl -nle 'print $&amp; while /.{0,75}enigo.{0,26}/g' Claude-Setup-x64/AnthropicClaude-0.11.6-full/lib/net45/resources/app.asar.unpacked/node_modules/claude-native/claude-native-binding.node</span>
<span>C:\Users\runneradmin\.cargo\registry\src\index.crates.io-1949cf8c6b5b557f\enigo-0.2.1\src\win\win_impl.rs</span></code></pre>


                    <p>In the output, you can see that on both platforms <a href="https://crates.io/crates/enigo/0.2.1">enigo version
                            0.2.1</a> is used (you might have to scroll right to see it).</p>

                    <p>I am very proud that enigo matured enough for a company with a seemingly infinite development
                        budget to choose it for their commercial project. Input simulation is surprisingly difficult due
                        to little documentation
                        and a lot of OS-specific quirks and warrants its own blog post. In my (admittedly not
                        completely
                        objective) opinion enigo is a great
                        choice for the job. As far as I know, it is the only library that works on Windows, macOS,
                        *BSD and Linux (Wayland, X11 and libei) without root. It is written in Rust and thus is mostly
                        memory safe
                        while being very fast. It is the most popular choice on crates.io
                        with almost 300,000 downloads and 1,200+ stars on Github. And yet it makes me a little nervous
                        knowing that my hobby project is used by Claude Desktop and deployed to thousands of devices.
                    </p>


                    <p>If you're not familiar with open-source software, you might wonder how much money I made from
                        them — and how many Ferraris I’m going to buy. If you <em>are</em>
                        familiar with
                        open-source software, it will come as no surprise to you that I am not earning any money from
                        it.
                        enigo is published under the <a href="https://en.wikipedia.org/wiki/MIT_License">MIT
                            license</a>. That means everyone can use it free of charge. The only thing I
                        get in return is more stars on GitHub and higher download counts on crates.io (the nerd
                        equivalent of street creds).</p>

                    <p>Interestingly, although Claude Desktop is an Electron app, it's only available on
                        macOS and Windows. The benefit of Electron applications is that they work on all platforms.
                        Other people found ways to run Claude Desktop on Linux as well (<a href="https://github.com/k3d3/claude-desktop-linux-flake">1</a>, <a href="https://github.com/aaddrick/claude-desktop-debian">2</a>, <a href="https://aur.archlinux.org/packages/claude-desktop-bin">3</a>). They had to replace the
                        code that uses enigo with stubs. This is very curious, because enigo is also cross-platform.</p>

                    <p>Through a friend of a friend, I found out that Anthropic had an open position in the team
                        implementing the secret, unreleased feature of Claude Desktop using enigo. I wrote a cover
                        letter and sent out my application. An automatic reply informed me that they might take some
                        time to respond and that they only notify applicants if they made it to the next round. After a
                        few weeks without an answer, I had assumed they chose other applicants. I already forgot about
                        the
                        application when I received an e-mail from Anthropic. I excitedly opened it. Unfortunately they
                        thanked me for my application but said the team doesn't have the capacity to review additional
                        applications.</p>

                    <p>I would have loved working at Anthropic, implementing a feature similar to Computer Use and
                        bringing Claude Desktop to Linux. I thought I had a pretty good shot to get the position,
                        considering some of my code is already
                        part of their software. Through the years I accumulated a lot of knowledge in the niche that is
                        input simulation that I would have brought to the table. Working full-time on enigo for a few
                        months could lift the project to a whole different level of polish and professionalism and would
                        have helped Anthropic to be able to focus on their AI models and not input quirks.
                    </p>

                    <p>Overall I am overjoyed enigo is used in Claude Desktop and I tell everyone who listens to me
                        about it :P.
                        It's so cool to think that I metaphorically created the arms and legs for Claude AI, but I can't
                        help but wonder if the rejection letter was written by a human or Claude AI. Did the very AI I
                        helped equip with new capabilities just reject my application? On the bright side, I should now
                        be
                        safe
                        from <a href="https://wikipedia.org/wiki/Roko's_basilisk">Roko's Basilisk</a>.
                    </p>





                </article>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rethinking DOM from first principles (101 pts)]]></title>
            <link>https://acko.net/blog/html-is-dead-long-live-html/</link>
            <guid>44808542</guid>
            <pubDate>Wed, 06 Aug 2025 06:51:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://acko.net/blog/html-is-dead-long-live-html/">https://acko.net/blog/html-is-dead-long-live-html/</a>, See on <a href="https://news.ycombinator.com/item?id=44808542">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

  
    <img src="https://acko.net/files/dom-cruft-2025/cover.jpg" alt="HTML is Dead, Long Live HTML">
  

  
  
    <div><p>
  <h2>Rethinking DOM from first principles</h2>
</p></div>



<img src="https://acko.net/files/dom-cruft-2025/cover.jpg" alt="Cover Image">

<div>

<p>Browsers are in a very weird place. While WebAssembly has succeeded, even on the server, the client still feels largely the same as it did <a href="https://acko.net/blog/shadow-dom/" target="_blank">10 years ago</a>.</p>

<p>Enthusiasts will tell you that accessing native web APIs via WASM is a solved problem, with some <a href="https://queue.acm.org/detail.cfm?id=3746174" target="_blank">minimal JS glue</a>.</p>

<p>But the question not asked is why you would want to access the DOM. It's just the only option. So I'd like to explain why it really is time to send the DOM and its assorted APIs off to a farm somewhere, with some ideas on&nbsp;how.</p>

<p>I won't pretend to know everything about browsers. Nobody knows everything anymore, and that's the problem.</p>

</div>



<p><img src="https://acko.net/files/dom-cruft-2025/netscape-upside-down.jpg" alt="Netscape or something">
</p>



<div>

<h2>The 'Document' Model</h2>

<p>Few know how bad the DOM really is. In Chrome, <code>document.body</code> now has 350+ keys, grouped roughly like this:</p>

</div>

<div>
  <p><img src="https://acko.net/files/dom-cruft-2025/document-body-chart.png" alt="document.body properties"></p>
</div>

<div>

<p>This doesn't include the CSS properties in <code>document.body.style</code> of which there are... <b>660</b>.</p>

<p>The boundary between properties and methods is very vague. Many are just facades with an invisible setter behind them. Some getters may trigger a just-in-time re-layout. There's ancient legacy stuff, like all the <code>onevent</code> properties nobody uses anymore.</p>

<p>The DOM is not lean and continues to get fatter. Whether you notice this largely depends on whether you are making web pages or web applications.</p>

<p>Most devs now avoid working with the DOM directly, though occasionally some purist will praise pure DOM as being superior to the various JS component/templating frameworks. What little declarative facilities the DOM has, like <code>innerHTML</code>, do not resemble modern UI patterns at all. The DOM has too many ways to do the same thing, none of them nice.</p>




<pre><code>connectedCallback() {
  const
    shadow = this.attachShadow({ mode: 'closed' }),
    template = document.getElementById('hello-world')
      .content.cloneNode(true),
    hwMsg = `Hello ${ this.name }`;

  Array.from(template.querySelectorAll('.hw-text'))
    .forEach(n =&gt; n.textContent = hwMsg);

  shadow.append(template);
}
</code></pre>


<p>Web Components deserve a mention, being the web-native equivalent of JS component libraries. But they came too late and are unpopular. The API seems clunky, with its Shadow DOM introducing new nesting and scoping layers. Proponents kinda <a href="https://kinsta.com/blog/web-components/" target="_blank">read like apologetics</a>.</p>

<p>The achilles heel is the DOM's SGML/XML heritage, making everything stringly typed. React-likes do not have this problem, their syntax only <i>looks</i> like XML. Devs have learned not to keep state in the document, because it's inadequate for it.</p>

</div>




<div>
  <p><img src="https://acko.net/files/dom-cruft-2025/w3c-logo.png" alt="W3C logo"></p>
  <p><img src="https://acko.net/files/dom-cruft-2025/whatwg.png" alt="WHATWG logo"></p>
</div>

<div>

<p>For HTML itself, there isn't much to critique because nothing has changed in 10-15 years. Only <a href="https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA" target="_blank">ARIA</a> (accessibility) is notable, and only because this was what Semantic HTML was supposed to do and didn't.</p>

<p>Semantic HTML never quite reached its goal. Despite dating from around 2011, there is e.g. no <code>&lt;thread&gt;</code> or <code>&lt;comment&gt;</code> tag, when those were well-established idioms. Instead, an article inside an article <a href="https://www.w3.org/TR/2011/WD-html5-author-20110809/the-article-element.html" target="_blank">is probably</a> a comment. The guidelines are... weird.</p>

<p>There's this feeling that HTML always had paper-envy, and couldn't quite embrace or fully define its hypertext nature, and did not trust its users to follow clear rules.</p>

<p>Stewardship of HTML has since firmly passed to WHATWG, really the browser vendors, who have not been able to define anything more concrete as a vision, and have instead just added epicycles at the margins.</p>

<p>Along the way even CSS has grown expressions, because every templating language wants to become a programming language.</p>

</div>



<p><img src="https://acko.net/files/dom-cruft-2025/composer.gif" alt="netscape composer">
</p>



<div><p>Editability of HTML remains a sad footnote. While technically supported via <code>contentEditable</code>, actually wrangling this feature into something usable for applications is a dark art. I'm sure the Google Docs and Notion people have horror&nbsp;stories.</p></div>

<div>

<p>Nobody really believes in the old gods of progressive enhancement and separating markup from style anymore, not if they make apps.</p>

<p>Most of the applications you see nowadays will kitbash HTML/CSS/SVG into a pretty enough shape. But this comes with immense overhead, and is looking more and more like the opposite of a decent UI toolkit.</p>

</div>



<div>
  <p><img src="https://acko.net/files/dom-cruft-2025/slack-html.png" alt="slack input editor"></p><p><i>The Slack input box</i></p>
</div>



<div>
  <p><img src="https://acko.net/files/dom-cruft-2025/slack-abs.png" alt="layout hack"></p><p><i>Off-screen clipboard hacks</i></p>
</div>

<div>

<p>Lists and tables must be virtualized by hand, taking over for layout, resizing, dragging, and so on. Making a chat window's scrollbar stick to the bottom is somebody's TODO, every single time. And the more you virtualize, the more you have to reinvent find-in-page, right-click menus, etc.</p>

<p>The web blurred the distinction between UI and fluid content, which was novel at the time. But it makes less and less sense, because the UI part is a decade obsolete, and the content has largely homogenized.</p>

</div>




<p><img src="https://acko.net/files/dom-cruft-2025/css-is-awesome.jpg" alt="'css is awesome' mug, truncated layout">
</p>

<div>

<h2>CSS is inside-out</h2>

<p>CSS doesn't have a stellar reputation either, but few can put their finger on exactly&nbsp;why.</p>

<p>Where most people go wrong is to start with the wrong mental model, approaching it like a constraint solver. This is easy to show with e.g.:</p>

</div>

<div>

<pre><code>&lt;div&gt;
  &lt;div style="height: 50%"&gt;...&lt;/div&gt;
  &lt;div style="height: 50%"&gt;...&lt;/div&gt;
&lt;/div&gt;</code></pre>

</div>




<div>

<pre><code>&lt;div&gt;
  &lt;div style="height: 100%"&gt;...&lt;/div&gt;
  &lt;div style="height: 100%"&gt;...&lt;/div&gt;
&lt;/div&gt;</code></pre>

</div>



<div>

<p>The first might seem reasonable: divide the parent into two halves vertically. But what about the second?</p>

<p>Viewed as a set of constraints, it's contradictory, because the parent div is twice as tall as... itself. What will happen instead in <i>both cases</i> is the <code>height</code> is ignored. The parent height is unknown and CSS doesn't backtrack or iterate here. It just shrink-wraps the contents.</p>

<p>If you set e.g. <code>height: 300px</code> on the parent, then it works, but the latter case will still just spill out.</p>

</div>

<div>
<p><img src="https://acko.net/files/dom-cruft-2025/layout-modes.png" alt="Outside-in vs inside-out layout"></p>
<p><i>Outside-in and inside-out layout modes</i></p>
</div>

<div>

<p>Instead, your mental model of CSS should be applying two passes of constraints, first going outside-in, and then inside-out.</p>

<p>When you make an application frame, this is <i>outside-in</i>: the available space is divided, and the content inside does not affect sizing of panels.</p>

<p>When paragraphs stack on a page, this is <i>inside-out</i>: the text stretches out its containing parent. This is what HTML wants to do naturally.</p>

<p>By being structured this way, CSS layouts are computationally pretty simple. You can propagate the parent constraints down to the children, and then gather up the children's sizes in the other direction. This is attractive and allows webpages to scale well in terms of elements and text content.</p>

<p>CSS is always inside-out by default, reflecting its document-oriented nature. The outside-in is not obvious, because it's up to you to pass all the constraints down, starting with <code>body { height: 100%; }</code>. This is why they always say vertical alignment in CSS is hard.</p>


</div>

<div>
<p><img src="https://acko.net/files/dom-cruft-2025/flex.png" alt="Flex grow/shrink"></p>
<p><i>Use flex grow and shrink for spill-free auto-layouts with completely reasonable gaps</i></p>
</div>

<div>

<p>The scenario above is better handled with a CSS3 flex box (<code>display: flex</code>), which provides explicit control over how space is divided.</p>

<p>Unfortunately flexing muddles the simple CSS model. To auto-flex, the <a href="https://www.w3.org/TR/css-flexbox-1/#algo-main-item" target="_blank">layout algorithm</a> must measure the "natural size" of every child. This means laying it out twice: first speculatively, as if floating in aether, and then again after growing or shrinking to fit:</p>

</div>

<div><p><img src="https://acko.net/files/dom-cruft-2025/speculative-layout.png" alt="Flex speculative layout"></p></div>

<div>

<p>This sounds reasonable but can come with hidden surprises, because it's recursive. Doing speculative layout of a parent often requires <i>full</i> layout of unsized children. e.g. to know how text will wrap. If you nest it right, it could in theory cause an exponential blow up, though I've never heard of it being an issue.</p>

<p>Instead you will only discover this when someone drops some large content in somewhere, and suddenly everything gets stretched out of whack. It's the opposite of the problem on the mug.</p>

<p>To avoid the recursive dependency, you need to isolate the children's contents from the outside, thus making speculative layout trivial. This can be done with <code>contain: size</code>, or by manually setting the <code>flex-basis</code> size.</p>

<p>CSS has gained a few constructs like <code>contain</code> or <code>will-transform</code>, which work directly with the layout system, and drop the pretense of <i>one big happy layout</i>. It reveals some of the layer-oriented nature underneath, and is a substitute for e.g. using <code>position: absolute</code> wrappers to do the same.</p>

<p>What these do is strip <i>off</i> some of the semantics, and break the flow of DOM-wide constraints. These are overly broad by default and too document-oriented for the simpler cases. </p>

<p>This is really a metaphor for all DOM APIs.</p>

</div>



<div>
  <p><img src="https://acko.net/files/dom-cruft-2025/css-props-mini.png" alt="CSS props"></p>
  <p><img src="https://acko.net/files/dom-cruft-2025/css-props.png" alt="CSS props"></p>
</div>

<div>

<h2>The Good Parts?</h2>

<p>That said, flex box is pretty decent if you understand these caveats. Building layouts out of nested rows and columns with gaps is intuitive, and adapts well to varying sizes. There is a <i>"CSS: The Good Parts"</i> here, which you can make ergonomic with sufficient love. CSS grids also work similarly, they're just very painfully... CSSy in their syntax.</p>

<p>But if you designed CSS layout from scratch, you wouldn't do it this way. You wouldn't have a subtractive API, with additional extra containment barrier hints. You would instead break the behavior down into its component facets, and use them à la carte. Outside-in and inside-out would both be legible as different kinds of containers and placement models.</p>

<p>The <code>inline-block</code> and <code>inline-flex</code> display models illustrate this: it's a <span><code>block</code></span> or <span><span><code>f</code></span><span><code>l</code></span><span><code>e</code></span><span><code>x</code></span></span> on the inside, but an <code>inline</code> element on the outside. These are two (mostly) orthogonal aspects of a box in a box model.</p>

<p>Text and font styles are in fact the odd ones out, in hypertext. Properties like font size inherit from parent to child, so that formatting tags like <code>&lt;b&gt;</code> can work. But most of those 660 CSS properties do <i>not</i> do that. Setting a border on an element does not apply the same border to all its children recursively, that would be silly.</p>

<p>It shows that CSS is at least two different things mashed together: a system for styling rich text based on inheritance... and a layout system for block and inline elements, nested recursively but without inheritance, only containment. They use the same syntax and APIs, but don't really cascade the same way. Combining this under one style-umbrella was a mistake.</p>

<p>Worth pointing out: early ideas of relative <code>em</code> scaling have largely become irrelevant. We now think of logical vs device pixels instead, which is a far more sane solution, and closer to what users actually expect.</p>

</div>




<div>
  <p><img src="https://acko.net/files/dom-cruft-2025/tiger.svg" alt="Tiger SVG"></p>
</div>

<div>

<p>SVG is natively integrated as well. Having SVGs in the DOM instead of just as <code>&lt;img&gt;</code> tags is useful to dynamically generate shapes and adjust icon styles.</p>

<p>But while SVG is powerful, it's neither a subset nor superset of CSS. Even when it overlaps, there are subtle differences, like the affine <code>transform</code>. It has its own warts, like serializing all coordinates to strings.</p>

<p>CSS has also gained the ability to round corners, draw gradients, and apply arbitrary clipping masks: it clearly has SVG-envy, but falls very short. SVG can e.g. do polygonal hit-testing for mouse events, which CSS cannot, and SVG has its own set of graphical layer effects.</p>

<p>Whether you use HTML/CSS or SVG to render any particular element is based on specific annoying trade-offs, even if they're all scalable vectors on the back-end.</p>

</div>



<div>

<p>In either case, there are also some roadblocks. I'll just mention three:</p>

<ul>

<li><code>text-ellipsis</code> can only be used to truncate <i>unwrapped</i> text, not entire paragraphs. Detecting truncated text is even harder, as is just measuring text: the APIs are inadequate. Everyone just counts letters instead.</li>

<li><code>position: sticky</code> lets elements stay in place while scrolling with zero jank. While tailor-made for this purpose, it's subtly broken. Having elements remain <i>unconditionally</i> sticky requires an absurd nesting hack, when it should be trivial.</li>

<li>The <code>z-index</code> property determines layering by absolute index. This inevitably leads to a <code>z-index-war.css</code> where everyone is putting in a new number +1 or -1 to make things layer correctly. There is no concept of relative Z positioning.</li>

</ul>

<p>For each of these features, we got stuck with v1 of whatever they could get working, instead of providing the right primitives.</p>

<p>Getting this right isn't easy, it's the hard part of API design. You can only iterate on it, by building real stuff with it before finalizing it, and looking for the holes.</p>


<h2>Oil on Canvas</h2>

<p>So, DOM is bad, CSS is single-digit X% good, and SVG is ugly but necessary... and nobody is in a position to fix it?</p>

<p>Well no. The diagnosis is that the middle layers don't suit anyone particularly well anymore. Just an HTML6 that finally <i>removes</i> things could be a good start.</p>

<p>But most of what needs to happen is to liberate the functionality that is there already. This can be done in good or bad ways. Ideally you design your system so the "escape hatch" for custom use is the <i>same API</i> you built the user-space stuff with. That's what dogfooding is, and also how you get good kernels.</p>

<p>A recent proposal here is <a href="https://github.com/WICG/html-in-canvas/tree/main" target="_blank">HTML in Canvas</a>, to draw HTML content into a <code>&lt;canvas&gt;</code>, with full control over the visual output. It's not very good.</p>

<p>While it might seem useful, the only reason the API has the shape that it does is because it's shoehorned into the DOM: elements must be descendants of <code>&lt;canvas&gt;</code> to fully participate in layout and styling, and to make accessibility work. There are also <i>"technical concerns"</i> with using it off-screen. </p>

<p>One example is this spinny cube:</p>

</div>

<p><img src="https://acko.net/files/dom-cruft-2025/canvas-cube.png" alt="html-in-canvas spinny cube thing">
</p>

<div>

<p>To make it interactive, you attach hit-testing rectangles and respond to paint events. This is a new kind of hit-testing API. But it only works in 2D... so it seems 3D-use is only cosmetic? I have many questions.</p>

<p>Again, if you designed it from scratch, you wouldn't do it this way! In particular, it's absurd that you'd have to take over <i>all</i> interaction responsibilities for an element and its descendants just to be able to customize how it <i>looks</i> i.e. renders. Especially in a browser that has projective CSS 3D transforms.</p>

<p>The use cases not covered by that, e.g. curved re-projection, will also need more complicated hit-testing than rectangles. Did they think this through? What happens when you put a dropdown in there?</p>

<p>To me it seems like they couldn't really figure out how to unify CSS and SVG filters, or how to add shaders to CSS. Passing it thru canvas is the only viable option left. <i>"At least it's programmable."</i> Is it really? Screenshotting DOM content is 1 good use-case, but not what this is sold as at all.</p>

<p>The whole reason to do "complex UIs on canvas" is to do all the things the DOM <i>doesn't do</i>, like virtualizing content, just-in-time layout and styling, visual effects, custom gestures and hit-testing, and so on. It's all nuts and bolts stuff. Having to pre-stage all the DOM content you want to draw sounds... very counterproductive.</p>

<p>From a reactivity point-of-view it's also a bad idea to route this stuff back through the same document tree, because it sets up potential cycles with observers. A canvas that's rendering DOM content isn't really a document element anymore, it's doing something else entirely.</p>

</div>

<div>
  <p><a href="https://farseerdev.github.io/sheet-happens/" target="_blank"><img src="https://acko.net/files/dom-cruft-2025/sheet.png" alt="sheet-happens"></a></p><p><i>Canvas-based spreadsheet that skips the DOM entirely</i></p>
</div>

<div>

<p>The actual achilles heel of canvas is that you don't have any real access to system fonts, text layout APIs, or UI utilities. It's quite absurd how basic it is. You have to <a href="https://farseerdev.github.io/sheet-happens/" target="_blank">implement everything</a> from scratch, including Unicode word splitting, just to get wrapped text.</p>

<p>The proposal is <i>"just use the DOM as a black box for content."</i> But we already know that you can't do anything except more CSS/SVG kitbashing this way. <code>text-ellipsis</code> and friends will still be broken, and you will still need to implement UIs circa 1990 from scratch to fix it.</p>

<p>It's all-or-nothing when you actually want something right in the middle. That's why the lower level needs to be opened up.</p>


<h2>Where To Go From Here</h2>

<p>The goals of <i>"HTML in Canvas"</i> do strike a chord, with chunks of HTML used as free-floating fragments, a notion that has always existed under the hood. It's a composite value type you can handle. But it should not drag 20 years of useless baggage along, while not enabling anything truly novel.</p>

<p>The kitbashing of the web has also resulted in enormous stagnation, and a loss of general UI finesse. When UI behaviors have to be mined out of divs, it limits the kinds of solutions you can even consider. Fixing this within DOM/HTML seems unwise, because there's just too much mess inside. Instead, new surfaces should be opened up outside of it.</p>

</div>



<div>
  <p><a href="https://usegpu.live/demo/layout/display" target="_blank"><img src="https://acko.net/files/dom-cruft-2025/use.gpu-layout.png" alt="use-gpu-layout"></a>
  <a href="https://usegpu.live/demo/layout/align" target="_blank"><img src="https://acko.net/files/dom-cruft-2025/use.gpu-layout-2.png" alt="use-gpu-layout"></a></p><p><i>WebGPU-based box model</i></p>
</div>

<div>

<p>My schtick here has become to point awkwardly at Use.GPU's <a href="https://usegpu.live/demo/layout/display" target="_blank">HTML-like renderer</a>, which does a full X/Y flex model in a fraction of the complexity or code. I don't mean my stuff is super great, no, it's pretty bare-bones and kinda niche... and yet definitely nicer. Vertical centering is easy. Positioning makes sense.</p>

<p>There is no semantic HTML or CSS cascade, just first-class layout. You don't need 61 different accessors for <code>border*</code> either. You can just <a href="https://usegpu.live/demo/rtt/cfd-compute" target="_blank">attach shaders</a> <a href="https://acko.net/files/bluebox/#!/" target="_blank">to divs</a>. Like, that's what people wanted right? Here's <a href="https://usegpu.live/docs/guides-layout-and-ui" target="_blank">a blueprint</a>, it's mostly just <a href="https://iquilezles.org/articles/distfunctions2d/" target="_blank">SDFs</a>.</p>

<p>Font and markup concerns only appear at the leaves of the tree, where the text sits. It's striking how you can do like 90% of what the DOM does here, with a fraction of the complexity of HTML/CSS/SVG, if you just reinvent that wheel. Done by 1 guy. And yes, I know about the second 90% too.</p>

<p>The classic data model here is of a view tree and a render tree. What should the view tree actually look like? And what can it be lowered into? What is it being lowered into right now, by a giant pile of legacy crud?</p>

</div>



<p><a href="https://servo.org/" target="_blank"><img src="https://acko.net/files/dom-cruft-2025/servo.png" alt="servo"></a>
  <a href="https://ladybird.org/" target="_blank"><img src="https://acko.net/files/dom-cruft-2025/ladybird.png" alt="ladybird"></a>
</p>

<div>

<p>Alt-browser projects like Servo or Ladybird are in a position to make good proposals here. They have the freshest implementations, and are targeting the most essential features first. The big browser vendors could also do it, but well, taste matters. Good big systems grow from good small ones, not bad big ones. Maybe if Mozilla hadn't imploded... but alas.</p>

<p>Platform-native UI toolkits are still playing catch up with declarative and reactive UI, so that's that. Native Electron-alternatives like Tauri could be helpful, but they don't treat origin isolation as a design constraint, which makes security teams antsy.</p>

</div>

<div>

<p>There's a feasible carrot to dangle for them though, namely in the form of better process isolation. Because of CPU exploits like Spectre, multi-threading via <code>SharedArrayBuffer</code> and Web Workers is kinda dead on arrival anyway, and that affects all WASM. The <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Cross-Origin-Embedder-Policy" target="_blank">details</a> are <a href="https://github.com/WebKit/standards-positions/issues/45#issuecomment-2077465281" target="_blank">boring</a> but right now it's an impossible sell when websites have to have things like OAuth and Zendesk integrated into them.</p>

<p>Reinventing the DOM to ditch all legacy baggage could coincide with redesigning it for a more multi-threaded, multi-origin, and async web. The browser engines are already multi-process... what did they learn? A lot has happened since Netscape, with advances in structured concurrency, ownership semantics, FP effects... all could come in handy here.</p>

<p>* * *</p>

<p>Step 1 should just be a data model that doesn't have 350+ properties per node tho.</p>

<p>Don't be under the mistaken impression that this isn't entirely fixable.</p>




</div>

<p><img src="https://acko.net/files/dom-cruft-2025/netscape.png" alt="netscape wheel">
</p>






  

  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Teacher AI Use Is Already Out of Control and It's Not Ok (165 pts)]]></title>
            <link>https://simonwillison.net/2025/Aug/5/greyduet-on-rteachers/</link>
            <guid>44808122</guid>
            <pubDate>Wed, 06 Aug 2025 05:44:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Aug/5/greyduet-on-rteachers/">https://simonwillison.net/2025/Aug/5/greyduet-on-rteachers/</a>, See on <a href="https://news.ycombinator.com/item?id=44808122">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<blockquote cite="https://www.reddit.com/r/Teachers/comments/1mhntjh/unpopular_opinion_teacher_ai_use_is_already_out/"><p>I teach HS Science in the south. I can only speak for my district, but a few teacher work days in the wave of enthusiasm I'm seeing for AI tools is overwhelming. We're getting district approved ads for AI tools by email, Admin and ICs are pushing it on us, and at least half of the teaching staff seems all in at this point.</p>
<p>I was just in a meeting with my team and one of the older teachers brought out a powerpoint for our first lesson and almost everyone agreed to use it after a quick scan - but it was missing important tested material, repetitive, and just totally airy and meaningless. Just slide after slide of the same handful of sentences rephrased with random loosely related stock photos. When I asked him if it was AI generated, he said 'of course', like it was a strange question. [...]</p>
<p>We don't have a leg to stand on to teach them anything about originality, academic integrity/intellectual honesty, or the importance of doing things for themselves when they catch us indulging in it just to save time at work.</p></blockquote>
<p>— <a href="https://www.reddit.com/r/Teachers/comments/1mhntjh/unpopular_opinion_teacher_ai_use_is_already_out/">greyduet on r/teachers</a>, <span>Unpopular Opinion: Teacher AI use is already out of control and it's not ok</span></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Kitten TTS – 25MB CPU-Only, Open-Source TTS Model (599 pts)]]></title>
            <link>https://github.com/KittenML/KittenTTS</link>
            <guid>44807868</guid>
            <pubDate>Wed, 06 Aug 2025 05:04:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/KittenML/KittenTTS">https://github.com/KittenML/KittenTTS</a>, See on <a href="https://news.ycombinator.com/item?id=44807868">Hacker News</a></p>
<div id="readability-page-1" class="page"><p dir="auto">Kitten TTS is an open-source realistic text-to-speech model with just 15 million parameters, designed for lightweight deployment and high-quality voice synthesis.</p><div data-snippet-clipboard-copy-content="pip install https://github.com/KittenML/KittenTTS/releases/download/0.1/kittentts-0.1.0-py3-none-any.whl"><pre><code>pip install https://github.com/KittenML/KittenTTS/releases/download/0.1/kittentts-0.1.0-py3-none-any.whl
</code></pre></div><div data-snippet-clipboard-copy-content="from kittentts import KittenTTS
m = KittenTTS(&quot;KittenML/kitten-tts-nano-0.1&quot;)

audio = m.generate(&quot;This high quality TTS model works without a GPU&quot;, voice='expr-voice-2-f' )

# available_voices : [  'expr-voice-2-m', 'expr-voice-2-f', 'expr-voice-3-m', 'expr-voice-3-f',  'expr-voice-4-m', 'expr-voice-4-f', 'expr-voice-5-m', 'expr-voice-5-f' ]

# Save the audio
import soundfile as sf
sf.write('output.wav', audio, 24000)
"><pre><code>from kittentts import KittenTTS
m = KittenTTS("KittenML/kitten-tts-nano-0.1")

audio = m.generate("This high quality TTS model works without a GPU", voice='expr-voice-2-f' )

# available_voices : [  'expr-voice-2-m', 'expr-voice-2-f', 'expr-voice-3-m', 'expr-voice-3-f',  'expr-voice-4-m', 'expr-voice-4-f', 'expr-voice-5-m', 'expr-voice-5-f' ]

# Save the audio
import soundfile as sf
sf.write('output.wav', audio, 24000)

</code></pre></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I'm Archiving Picocrypt (199 pts)]]></title>
            <link>https://github.com/Picocrypt/Picocrypt/issues/134</link>
            <guid>44807210</guid>
            <pubDate>Wed, 06 Aug 2025 03:14:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Picocrypt/Picocrypt/issues/134">https://github.com/Picocrypt/Picocrypt/issues/134</a>, See on <a href="https://news.ycombinator.com/item?id=44807210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      



    <div>
      <p><a href="#start-of-content" data-skip-target-assigned="false">Skip to content</a>

      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p>

<react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  
  
</react-partial>




      

          

              



<header role="banner" data-is-top="true" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  <h2>Navigation Menu</h2>

  

  <div>
          <nav aria-label="Global">
            <ul>


                <li>
      

      <div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_copilot&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_copilot_link_product_navbar&quot;}" href="https://github.com/features/copilot">
      
      <div>
          <p>
            GitHub Copilot
          </p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_spark&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_spark_link_product_navbar&quot;}" href="https://github.com/features/spark">
      
      <div>
          <p>
            GitHub Spark
              <span>
                New
              </span>
          </p><p>
        Build and deploy intelligent apps
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_models&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_models_link_product_navbar&quot;}" href="https://github.com/features/models">
      
      <div>
          <p>
            GitHub Models
              <span>
                New
              </span>
          </p><p>
        Manage and compare prompts
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_advanced_security_link_product_navbar&quot;}" href="https://github.com/security/advanced-security">
      
      <div>
          <p>
            GitHub Advanced Security
          </p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;actions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;actions_link_product_navbar&quot;}" href="https://github.com/features/actions">
      
      <div>
          <p>
            Actions
          </p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                    
                </ul>
              </div>
          <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;codespaces&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;codespaces_link_product_navbar&quot;}" href="https://github.com/features/codespaces">
      
      <div>
          <p>
            Codespaces
          </p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;issues&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;issues_link_product_navbar&quot;}" href="https://github.com/features/issues">
      
      <div>
          <p>
            Issues
          </p><p>
        Plan and track work
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_review&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_review_link_product_navbar&quot;}" href="https://github.com/features/code-review">
      
      <div>
          <p>
            Code Review
          </p><p>
        Manage code changes
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;discussions&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;discussions_link_product_navbar&quot;}" href="https://github.com/features/discussions">
      
      <div>
          <p>
            Discussions
          </p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;code_search&quot;,&quot;context&quot;:&quot;product&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;code_search_link_product_navbar&quot;}" href="https://github.com/features/code-search">
      
      <div>
          <p>
            Code Search
          </p><p>
        Find more, search less
      </p></div>

    
</a></li>

                </ul>
              </div>
          

      </div>
</li>


                <li>
      

      
</li>


                <li>
      

      <div>
                    <p><span id="resources-explore-heading">Explore</span></p><ul aria-labelledby="resources-explore-heading">
                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;learning_pathways&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;learning_pathways_link_resources_navbar&quot;}" href="https://resources.github.com/learn/pathways">
      Learning Pathways

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;events_amp_webinars&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;events_amp_webinars_link_resources_navbar&quot;}" href="https://resources.github.com/">
      Events &amp; Webinars

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;ebooks_amp_whitepapers&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;ebooks_amp_whitepapers_link_resources_navbar&quot;}" href="https://github.com/resources/whitepapers">
      Ebooks &amp; Whitepapers

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;customer_stories&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                    <li>
  <a target="_blank" data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;partners&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;executive_insights&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;executive_insights_link_resources_navbar&quot;}" href="https://github.com/solutions/executive-insights">
      Executive Insights

    
</a></li>

                </ul>
              </div>
</li>


                <li>
      

      <div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;github_sponsors&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}" href="https://github.com/sponsors">
      
      <div>
          <p>
            GitHub Sponsors
          </p><p>
        Fund open source developers
      </p></div>

    
</a></li>

                </ul>
              </div>
              <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;the_readme_project&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;the_readme_project_link_open_source_navbar&quot;}" href="https://github.com/readme">
      
      <div>
          <p>
            The ReadME Project
          </p><p>
        GitHub community articles
      </p></div>

    
</a></li>

                </ul>
              </div>
              
          </div>
</li>


                <li>
      

      <div>

                <ul>
                    <li>
  <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}" href="https://github.com/enterprise">
      
      <div>
          <p>
            Enterprise platform
          </p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

                </ul>
              </div>
</li>


                <li>
    <a data-analytics-event="{&quot;location&quot;:&quot;navbar&quot;,&quot;action&quot;:&quot;pricing&quot;,&quot;context&quot;:&quot;global&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;label&quot;:&quot;pricing_link_global_navbar&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:Picocrypt/Picocrypt" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="zyatvmHNj3emlsIXnzoJ09nqT9TR8a2AFFbGZON_3xjjSH77KDy4FV9Wpk0MGCW_KoblJLPzyN1PSqVFE0KRJw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="Picocrypt/Picocrypt" data-current-org="Picocrypt" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fissues_fragments%2Fissue_layout&amp;source=header-repo&amp;source_repo=Picocrypt%2FPicocrypt" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/Picocrypt/Picocrypt/issues/134&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="3d2412517e62b603d6c586f0521540dece7960044eb554d72b1a56d5b18c73a6" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/voltron/issues_fragments/issue_layout;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-2cb88353-0221-4dff-aa1f-879b578f4862" for="icon-button-0a5de433-30ac-4001-8ada-0a2aae97d72d" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.76259b61ecc822265749.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>

      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div>
</header>

      
    </div>

  








    


    






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="" data-project-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
      
    

    






  
  

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
    



    
      
    


      





  



<react-app app-name="issues-react" initial-path="/Picocrypt/Picocrypt/issues/134" data-attempted-ssr="true" data-ssr="true" data-lazy="false" data-alternate="false" data-data-router-enabled="false" data-react-profiling="false">
  
  
  <div data-testid="issue-viewer-container" data-target="react-app.reactRoot"><div data-testid="issue-viewer-issue-container"><p><a href="https://github.com/HACKERALERT" data-hovercard-url="/users/HACKERALERT/hovercard" aria-label="@HACKERALERT's profile"><img data-component="Avatar" alt="@HACKERALERT" width="40" height="40" src="https://avatars.githubusercontent.com/u/48808396?u=77677d8507d0a44c6632def2a3f1262752359e11&amp;v=4&amp;size=80" data-testid="github-avatar"></a></p><div data-testid="issue-body" data-hpc="true"><h2>Description</h2><div><div><p><a href="https://github.com/HACKERALERT" data-hovercard-url="/users/HACKERALERT/hovercard" aria-label="@HACKERALERT's profile"><img data-component="Avatar" alt="@HACKERALERT" width="24" height="24" src="https://avatars.githubusercontent.com/u/48808396?u=77677d8507d0a44c6632def2a3f1262752359e11&amp;v=4&amp;size=48" data-testid="github-avatar"></a></p></div><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><p dir="auto">Hey Gemini, I need your help analyzing and understanding a final parting message left by a developer for his archived open-source file encryption software. Can you me with that?</p></div></div></div></div><div data-testid="issue-viewer-metadata-container"><h2>Metadata</h2><h2>Metadata</h2><!--$--><!--/$--><!--$--><div data-testid="sidebar-section"><p><h3>Development</h3></p><p><span>No branches or pull requests</span></p></div><!--/$--><h2>Issue actions</h2><ul data-dividers="false" data-variant="full"></ul></div></div>
</react-app>



  



  </div>

</turbo-frame>

    </main>
  </div>

          



    <ghcc-consent id="ghcc" data-locale="en" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></ghcc-consent>



  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Marines now have an official drone-fighting handbook (111 pts)]]></title>
            <link>https://www.marinecorpstimes.com/news/your-marine-corps/2025/08/04/the-marines-now-have-an-official-drone-fighting-handbook/</link>
            <guid>44807154</guid>
            <pubDate>Wed, 06 Aug 2025 03:05:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.marinecorpstimes.com/news/your-marine-corps/2025/08/04/the-marines-now-have-an-official-drone-fighting-handbook/">https://www.marinecorpstimes.com/news/your-marine-corps/2025/08/04/the-marines-now-have-an-official-drone-fighting-handbook/</a>, See on <a href="https://news.ycombinator.com/item?id=44807154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-chain-name="c0fUbP76WXURanD" data-gtm-name="Article Body" id="c0fUbP76WXURanD"><article><p>On the heels of fielding the military’s first attack drone team, the U.S. Marine Corps added another weapon to their drone-fighting arsenal: a 90-page handbook all about employing small, unmanned aerial systems against the enemy and integrating them into formations.</p><p>The 1st Marine Division Schools’ Small UAS/Counter-small UAS Integration Handbook was published in June and approved for public release. It’s intended to support the 10-day sUAS/C-sUAS Integration Course recently launched at Camp Pendleton, which expects to see a throughput of about 400 students by the end of the year, according to a report from <a href="https://news.usni.org/2025/06/25/attack-of-the-drones" rel="">USNI News</a>.</p><p>A foreword to the handbook is signed by Lt. Col. Nick Freeman, director of 1st Marine Division Schools, and co-signed by two first lieutenants leading the drone integration and signature management courses. It emphasizes that the handbook will be updated and rewritten often to keep up with evolving capabilities and practices.</p><p>The book “is not a general reference on broader aspects of sUAS-related equipping, organization, and training. Instead, it synthesizes lessons learned and best practices from across 1st Marine Division and elsewhere to provide basic considerations and ‘how to’ instructions that are missing or underdeveloped in other references,” the officers write. </p><p>“In doing so, this guide also develops and seeks to standardize common sUAS procedures for the infantry, fires, reconnaissance, and aviation units that will operate together with this capability.”</p><p>The manual’s publication comes amid a clear shift in defense priorities to favor drone warfare and emphasize, in particular, proficiency with “first-person view” or “one-way attack” small drones designed to pack a lethal punch. </p><p>In addition to the fielding of the <a href="https://www.militarytimes.com/news/your-military/2025/04/30/marine-training-chief-wants-to-let-ncos-loose-with-more-drone-access/#:~:text=The%20current%20outfit%2C%20Cuomo%20said,26%20hours%20of%20simulator%20training." rel="">Marine Corps’ Attack Drone Team</a>, a small group of troops who will develop ways to employ these kinds of drones and integrate them into formations, the Pentagon in <a href="https://www.defensenews.com/pentagon/2025/07/10/hegseth-calls-for-extensive-reforms-to-pentagon-drone-buying-practices/" rel="">July announced a slate of changes to drone acquisition</a> designed to “establish UAS dominance” by 2027.</p><div><h6>RELATED</h6><article itemscope="" itemtype="http://schema.org/Article" data-story-url="/news/your-military/2025/04/30/marine-training-chief-wants-to-let-ncos-loose-with-more-drone-access/" data-story-id="72G6QKME7ZAZBFJAWULPLKE5GE" data-feature-id="false" data-story-index="1" data-promo-type="image"><div><figure type="story"><picture><img data-chromatic="ignore" alt="" loading="lazy" src="https://www.armytimes.com/resizer/v2/KJR7CJVNEFFBTLG7O53WDXCTRE.jpg?auth=84949f1555a57c62cc1c12d36ae09d827477fd9c4cb0baa9e547e46447a91732&amp;width=8192&amp;height=5462" srcset="https://www.armytimes.com/resizer/v2/KJR7CJVNEFFBTLG7O53WDXCTRE.jpg?auth=84949f1555a57c62cc1c12d36ae09d827477fd9c4cb0baa9e547e46447a91732&amp;width=800&amp;height=533 800w, https://www.armytimes.com/resizer/v2/KJR7CJVNEFFBTLG7O53WDXCTRE.jpg?auth=84949f1555a57c62cc1c12d36ae09d827477fd9c4cb0baa9e547e46447a91732&amp;width=1024&amp;height=682 1024w" width="8192" height="5462"></picture></figure></div></article></div><p>It’s a marked pivot from previous years, in which the services largely emphasized surveillance and logistics as the role of friendly, small drones in warfare and lacked a definitive approach to defending against hostile attack drones. </p><p>In 2020, a <a href="https://www.military.com/daily-news/2020/07/29/these-marines-just-published-how-guide-hiding-enemy-drones.html" rel="">small group of infantry Marines crowdsourced</a> an unofficial standard operating procedure for camouflaging small units from drone surveillance, underscoring the ad-hoc nature of efforts to account for this threat.</p><p>By contrast, the new 1st Marine Division handbook standardizes employment of various drones down to a common vocabulary. </p><p>Drone holding areas “are named after women and [battle positions] are named after animals (beginning with snakes), [loitering areas] are given the names of cigarettes and [task positions] are named after insects,” the manual states.</p><p>In addition, “hot walls” and “pizza slices” describe drone operating areas for hasty airspace deconfliction. </p><p>Charts and schematics show sample drone strikes in various personnel and equipment configurations. Tables break down specifications of the Corps’ most widely fielded small UAS. A sample strike brief provides a precise template for communications around drone operations. And a multi-page section on camouflage and evasion provides formal guidance on everything from hiding heat signatures to using vegetation to blend visually with the environment.</p><p>Drone employment as a team effort is emphasized throughout the book.</p><p>“In all cases, the operator of any one aerial system is unlikely to accomplish the unit’s mission by him/herself; instead, the operator performs tasks as part of a sUAS-equipped team whose other members may variously perform roles related to communications, targeting, mobility, protection, fires, maneuver/exploitation, and others,” the guide reads. </p><p>“For this reason, this handbook refers to sUAS teams as the basic unit of employment for these systems, even when the unit operating them (for example, a rifle squad or an artillery battery headquarters) may not have sUAS employment as its primary purpose.”</p><p>The handbook concludes with a list of chapters and sections the book is still missing, including weaponeering considerations for employing drones carrying munitions and a full chapter on tactics, techniques and procedures for one-way attack drones.</p><p>“Make no mistake, we are in a very tight race with our adversaries to master the possibilities of small aerial drones,” the authors write. “Consider this handbook a baton—now take it, and run with it!”</p><figure><figure><picture><img data-chromatic="ignore" alt="" loading="lazy" src="https://www.marinecorpstimes.com/resizer/v2/J5ON7Y2ROFB7DMYM5UTINDJOSU.jpg?auth=cd9b602d7f718768f26b0d82fe0c9ff854115b8ca8ea13e4ae4e81d259b03703&amp;width=7210&amp;height=4809" srcset="https://www.marinecorpstimes.com/resizer/v2/J5ON7Y2ROFB7DMYM5UTINDJOSU.jpg?auth=cd9b602d7f718768f26b0d82fe0c9ff854115b8ca8ea13e4ae4e81d259b03703&amp;width=800&amp;height=533 800w, https://www.marinecorpstimes.com/resizer/v2/J5ON7Y2ROFB7DMYM5UTINDJOSU.jpg?auth=cd9b602d7f718768f26b0d82fe0c9ff854115b8ca8ea13e4ae4e81d259b03703&amp;width=1024&amp;height=682 1024w" width="7210" height="4809"></picture></figure><figcaption>Marines with Advanced Infantry Training Battalion, School of Infantry – East load a Mjolnir munitions system on a SkyRaider. (Cpl. Zachariah Ferraro/Marine Corps)</figcaption></figure><p>While 1st Marine Division did not make anyone available to talk about the handbook by press time, Samuel Bendett, an adviser focusing on Russian military technology and capabilities including drones, called the book’s publication evidence of a “psychological shift” about the realities of the drone threat and the need to employ small UAS skillfully.</p><p>The handbook itself acknowledges lessons in drone use absorbed from the ongoing Russia-Ukraine war, where both sides have employed UAS to great effect. </p><p>While Bendett acknowledged that the next U.S. fight might not resemble the conflict in Ukraine, he maintained that the echoes of that war “will be heard in every conflict going forward,” adding that China was paying close attention and already training its military in drone warfare.</p><p>“It’s not just an abstract notion that there are adversary drones somewhere and we have to defend against them,” Bendett said. “It’s the fact that it’s right there, just around the bend, just just beyond the next building, just beyond the next tree. It’s there just two or three klicks away, and it’s observing and flying at you, and you won’t be able to react in time if you’re not prepared for it.”</p><p>Another helpful shift evidenced in the document, he said, was in viewing UAS not as small aircraft, but as weapons.</p><p>“These are not aircraft,” he said. “These are cheaper, attritable tactical systems that should be available to every military formation, based on what they’re doing and based on how they’re fighting.”</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Software Rot (177 pts)]]></title>
            <link>https://permacomputing.net/software_rot/</link>
            <guid>44807002</guid>
            <pubDate>Wed, 06 Aug 2025 02:35:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://permacomputing.net/software_rot/">https://permacomputing.net/software_rot/</a>, See on <a href="https://news.ycombinator.com/item?id=44807002">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="pagebody" role="main" class="page">
<p><strong>Software rot</strong> is generally thought of as degradation of <a href="https://permacomputing.net/software/">software</a> due to a changing environment. For example, a program written a decade ago may no longer work with new versions of the libraries it depends on because some of them have changed without retaining backwards compatibility. This kind of thinking encourages a culture where software becomes <a href="https://permacomputing.net/obsolescence/">obsolete</a> unless it is constantly maintained.</p>

<p>A better approach might be to talk about the reliability of the environment the software depends on. Would you build a house on a bog?</p>

<p>It is often necessary to build on "bogs" (i.e. "actively developed" platforms), but it might be a good idea to also be compatible with a <a href="https://permacomputing.net/bedrock_platform/">bedrock platform</a> whose specifications are static and solid.</p>

<p>Software rot is a big issue for cultures that constantly produce new programs (such as <a href="https://permacomputing.net/games/">games</a> or <a href="https://permacomputing.net/demoscene/">demos</a>) that are not supposed to be constantly maintained after release. Programs written for classical platforms (such as <a href="https://permacomputing.net/DOS/">DOS</a> or <span><a href="https://permacomputing.net/ikiwiki.cgi?do=create&amp;from=software_rot&amp;page=NES" rel="nofollow">?</a>NES</span>) usually need no post-release maintentance at all, while those written for e.g. <span><a href="https://permacomputing.net/ikiwiki.cgi?do=create&amp;from=software_rot&amp;page=Linux" rel="nofollow">?</a>Linux</span> will likely cease working in a decade or two. Sometimes, serious <span><a href="https://permacomputing.net/ikiwiki.cgi?do=create&amp;from=software_rot&amp;page=media_archeology" rel="nofollow">?</a>media archeology</span> work (such as finding specific versions of old libraries) is needed to get a program to run again.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kitten TTS: 25MB CPU-Only, Open-Source Voice Model (190 pts)]]></title>
            <link>https://algogist.com/kitten-tts-the-25mb-ai-voice-model-thats-about-to-change-everything-runs-on-a-potato/</link>
            <guid>44806543</guid>
            <pubDate>Wed, 06 Aug 2025 01:13:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://algogist.com/kitten-tts-the-25mb-ai-voice-model-thats-about-to-change-everything-runs-on-a-potato/">https://algogist.com/kitten-tts-the-25mb-ai-voice-model-thats-about-to-change-everything-runs-on-a-potato/</a>, See on <a href="https://news.ycombinator.com/item?id=44806543">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        



        

        <main>



<article>
    

    <section id="post-body"><p>Alright, let's have a real talk. For years, the AI world has been obsessed with BIG. Big models, big data, big GPUs, and even bigger cloud bills. Most text-to-speech (TTS) models today are heavyweight champs of burning cash. We're talking about multi-billion parameter, GPU-guzzling monsters that need more silicon than your phone, your laptop, and maybe your entire neighborhood combined. They give you great voices, sure... but only if you're willing to sign away your firstborn to AWS.</p><p>Forget that. The era of bloated AI is OVER.</p><p>What if I told you the real revolution isn't coming from a massive, air-conditioned data center? It's coming from a model so small, it's almost a joke. A model that fits on a thumb drive with room to spare.</p><h2 id="say-hello-to-kitten-tts-%F0%9F%98%BB">Say hello to <strong>Kitten TTS</strong>. 😻</h2><figure><blockquote><div lang="en" dir="ltr"><p>Introducing Kitten TTS, a SOTA tiny text-to-speech model</p><p>- Just 15M parameters <br>- Runs without a GPU<br>- Model size less than 25 MB<br>- Multiple high-quality voices<br>- Ultra-fast - even runs on low-end edge devices</p><p>Github and HF links below <a href="https://t.co/9T3u1M0WGo">pic.twitter.com/9T3u1M0WGo</a></p></div>— Divam Gupta (@divamgupta) <a href="https://twitter.com/divamgupta/status/1952762876504187065?ref_src=twsrc%5Etfw">August 5, 2025</a></blockquote>
</figure><p>This isn't just another model dropped on Hugging Face; it's a statement. It's the David to the Goliath of big tech AI. Developed by the wizards at KittenML, this thing is here to prove that size ISN'T everything.</p><p>And listen, this isn't happening in a vacuum. The whole industry is waking up and smelling the coffee smaller, smarter, more efficient models are the future. We are witnessing a massive shift towards lean, on-device AI that actually respects your privacy and your wallet. This is about putting power back into the hands of the builders, the creators, the hobbyists the people who don't have a venture capitalist on speed dial. This move away from centralized, big-tech-controlled AI toward a distributed, community-driven ecosystem is the most exciting thing happening in tech right now. And Kitten TTS isn't just following the trend; it's leading the charge.</p><h2 id="the-specs-that-will-blow-your-mind-%F0%9F%A4%AF"><strong>The Specs That Will BLOW YOUR MIND 🤯</strong></h2><p>Okay, let's get into the nitty-gritty. What makes this little beast tick? These aren't just bullet points on a GitHub README; these are the specs that will fundamentally redefine what you thought was possible with local AI.</p><h3 id="15m-parameters-25mb-size-no-thats-not-a-typo"><strong>15M Parameters &amp; &lt;25MB Size. NO, THAT'S NOT A TYPO.</strong></h3><p>Most so-called "lightweight" models are still chunky boys, coming in at hundreds of megabytes. Kitten TTS? It clocks in at <strong>under 25MB</strong> with just <strong>15 million parameters</strong>. Let that sink in. That's smaller than most of the photos you take on your phone. It's about one-fifth the size of the previous "small" champion, Kokoro-82M, a model that was already celebrated for its efficiency. This ridiculously small footprint means it downloads in seconds and can be deployed on literally anything.</p><h3 id="runs-without-a-gpu-your-wallet-can-thank-us"><strong>Runs WITHOUT A GPU. Your Wallet Can Thank Us.</strong></h3><p>This is the big one. This is for all my "GPU-poor" folks out there who have been watching the AI revolution from the sidelines.<strong><sup> </sup></strong>YOU DO NOT NEED AN EXPENSIVE GRAPHICS CARD. Kitten TTS is aggressively CPU-optimized to run on your everyday laptop, a cheap Raspberry Pi, your Android phone... and probably even a smart toaster if you're feeling adventurous. I'm not kidding people have tested this on a free Google Colab CPU instance, and it was generating audio in SECONDS. The barrier to entry just got obliterated.</p><h3 id="multiple-expressive-voices-the-whole-fam"><strong>Multiple Expressive Voices (The Whole Fam!)</strong></h3><p>For a model this tiny, you'd expect a single, robotic, "Stephen Hawking circa 1988" voice, right? WRONG. Kitten TTS ships with <strong>eight different expressive voices</strong> four female and four male right out of the box. For a model of this size, the level of expressivity is honestly shocking and a massive advantage for anyone looking to build applications with character. We'll meet them all in a bit.</p><h3 id="ultra-fast-inference-for-real-time-apps"><strong>Ultra-Fast Inference for Real-Time Apps</strong></h3><p>This thing is BUILT FOR SPEED. It’s optimized for real-time speech synthesis, which means no more awkward, laggy delays in your applications. This is absolutely critical for building responsive chatbots, voice assistants that don't make you wait, and on-the-fly narration for accessibility tools. Anecdotal reports from community demos show it generating audio faster than real-time even on consumer hardware.</p><h3 id="open-source-baby-apache-20-license"><strong>OPEN SOURCE, BABY! (Apache 2.0 License)</strong></h3><p>And here's the cherry on top. The best part. It's completely open source under the permissive Apache 2.0 license. This means you can use it for free. For your personal projects. For your commercial products. For whatever you want. No strings attached. Go build something amazing and make some money! The code is on GitHub, the model is on Hugging Face... the playground is yours.</p><p>What's truly remarkable here is the cascade of innovation. It all starts with the core architectural breakthrough: achieving impressive quality with a tiny number of parameters. This single achievement directly causes the sub-25MB model size. That small size, in turn, is what allows it to run so efficiently on CPU-only systems. And that CPU efficiency is what unlocks its potential on low-power edge devices like the Raspberry Pi. It's a beautiful domino effect where one smart design choice solves for size, cost, and speed all at once the holy trinity for edge AI.</p><h2 id="enough-talk-lets-get-this-running-now-the-5-minute-guide"><strong>Enough Talk! Let's Get This Running NOW (The 5-Minute Guide)</strong></h2><p>Theory is great, but code is king. Let's get this running on your machine. No more excuses, this is copy-paste-ready. Let's GO! 🚀</p><h3 id="step-1-the-magical-one-line-install"><strong>Step 1: The Magical One-Line Install</strong></h3><p>Open your terminal. Do the right thing and create a virtual environment (<code>python -m venv.venv &amp;&amp; source.venv/bin/activate</code>). Now, paste this in. That's it. You're done.</p><pre><code># It's this easy, seriously.
pip install https://github.com/KittenML/KittenTTS/releases/download/0.1/kittentts-0.1.0-py3-none-any.whl</code></pre><h3 id="step-2-your-first-hello-world-basic-generation"><strong>Step 2: Your First "Hello World" (Basic Generation)</strong></h3><p>Create a Python file, call it <code>test_kitten.py</code>, and drop this code in. This will automatically grab the model from Hugging Face the first time you run it and generate your very first audio file.</p><pre><code># test_kitten.py
from kittentts import KittenTTS
import soundfile as sf

print("Loading KittenTTS model... Meow! 🐱")
# This downloads the model from Hugging Face the first time
m = KittenTTS("KittenML/kitten-tts-nano-0.1")

text = "This high quality TTS model works without a GPU, which is pretty awesome!"

print(f"Generating audio for: '{text}'")
# Generate the audio waveform
audio = m.generate(text)

# Save the audio to a file at 24kHz sample rate
output_file = 'hello_kitten.wav'
sf.write(output_file, audio, 24000)

print(f"✅ Audio saved to {output_file}! Go listen to it!")</code></pre><p>Run it with <code>python test_kitten.py</code> and go check out <code>hello_kitten.wav</code>. Welcome to the future.</p><h3 id="step-3-meet-the-whole-crew-looping-through-all-voices"><strong>Step 3: Meet the Whole Crew (Looping Through All Voices)</strong></h3><p>Okay, that was cool, but you just used the default voice. <strong>PRO TIP:</strong> The default voice (<code>expr-voice-5-m</code>) is... let's just say it has <em>character</em>. Some of the other voices are WAY better for general use. Let's generate a sample for every single voice so you can pick your favorite for your next project.</p><p>Create a new file, <code>all_voices.py</code>:</p><pre><code># all_voices.py
from kittentts import KittenTTS
import soundfile as sf

m = KittenTTS("KittenML/kitten-tts-nano-0.1")

TEXT = "Kitten TTS is an open-source series of tiny and expressive Text-to-Speech models for on-device applications."

# Get the list of all available voices
available_voices = m.available_voices
print(f"Available voices: {available_voices}")

for voice in available_voices:
    output_file = f"output_{voice}.wav"
    print(f"▶️ Generating for voice '{voice}' -&gt; {output_file}")
    
    # The magic is here: specify the voice!
    m.generate_to_file(TEXT, output_file, voice=voice)
    
print("✅ All voice samples generated!")</code></pre><p>Run this script, and you'll get a <code>.wav</code> file for each voice. To make it even easier, here's the official roster.</p>
<!--kg-card-begin: html-->
<table><colgroup><col><col><col></colgroup><tbody><tr><td colspan="1" rowspan="1"><p>Voice ID</p></td><td colspan="1" rowspan="1"><p>Gender</p></td><td colspan="1" rowspan="1"><p>Vibe Check (Our Description)</p></td></tr><tr><td colspan="1" rowspan="1"><p><code spellcheck="false">expr-voice-2-f</code></p></td><td colspan="1" rowspan="1"><p>Female</p></td><td colspan="1" rowspan="1"><p>Clear, professional, great for narration.</p></td></tr><tr><td colspan="1" rowspan="1"><p><code spellcheck="false">expr-voice-2-m</code></p></td><td colspan="1" rowspan="1"><p>Male</p></td><td colspan="1" rowspan="1"><p>Solid, standard male voice. The reliable choice.</p></td></tr><tr><td colspan="1" rowspan="1"><p><code spellcheck="false">expr-voice-3-f</code></p></td><td colspan="1" rowspan="1"><p>Female</p></td><td colspan="1" rowspan="1"><p>A bit more expressive, good for character work.</p></td></tr><tr><td colspan="1" rowspan="1"><p><code spellcheck="false">expr-voice-3-m</code></p></td><td colspan="1" rowspan="1"><p>Male</p></td><td colspan="1" rowspan="1"><p>Deep, thoughtful. Perfect for storytelling.</p></td></tr><tr><td colspan="1" rowspan="1"><p><code spellcheck="false">expr-voice-4-f</code></p></td><td colspan="1" rowspan="1"><p>Female</p></td><td colspan="1" rowspan="1"><p>Upbeat and friendly. Your go-to for assistants.</p></td></tr><tr><td colspan="1" rowspan="1"><p><code spellcheck="false">expr-voice-4-m</code></p></td><td colspan="1" rowspan="1"><p>Male</p></td><td colspan="1" rowspan="1"><p>Energetic and clear. Gets the point across.</p></td></tr><tr><td colspan="1" rowspan="1"><p><code spellcheck="false">expr-voice-5-m</code></p></td><td colspan="1" rowspan="1"><p>Male</p></td><td colspan="1" rowspan="1"><p>The default. A bit... unique. Use with caution! 😉</p></td></tr><tr><td colspan="1" rowspan="1"><p><code spellcheck="false">expr-voice-5-f</code></p></td><td colspan="1" rowspan="1"><p>Female</p></td><td colspan="1" rowspan="1"><p>Note: Sources are conflicting. Some list 7 voices, some list 8. The official GitHub lists 7, ending with 5-m. We'll update as the project evolves!</p></td></tr></tbody></table>
<!--kg-card-end: html-->
<h2 id="under-the-hood-how-does-this-magic-work-a-technical-deep-dive"><strong>Under the Hood: How Does This Magic Work? (A Technical Deep Dive)</strong></h2><p>So, how in the world did KittenML pull this off? How do you squeeze a decent-quality voice out of a model that's smaller than a cat video? While the team hasn't released a full research paper just yet, the open-source community has put on its detective hat, and the consensus is pretty clear.</p><p>The smart money, especially among the folks at r/LocalLLaMA, is that Kitten TTS is built on an architecture that's very similar to <strong>VITS</strong> (Variational Inference with Adversarial Learning for End-to-End Text-to-Speech) or possibly <strong>StyleTTS2</strong>.</p><p>Don't let the alphabet soup of an acronym scare you. VITS is a brilliantly clever end-to-end system that mashes up several powerful AI concepts into one elegant package:</p><ol><li><strong>Variational Autoencoder (VAE):</strong> At its core, a VAE is great at learning a compressed, meaningful representation of data. In this case, it learns the essential "essence" of speech.</li><li><strong>Normalizing Flows:</strong> This is a fancy mathematical trick that helps the model produce more diverse and natural-sounding variations in the speech, avoiding a monotonous, robotic tone.</li><li><strong>Generative Adversarial Network (GAN):</strong> This is the secret sauce that pushes the quality over the top. A GAN consists of two models locked in a battle to the death.<ul><li>The <strong>Generator</strong> creates the audio from text.</li><li>The <strong>Discriminator</strong> acts like a critic, trying to tell if the audio it hears is from a real human or a fake from the Generator.</li><li>They are trained together. The Generator's only goal is to fool the Discriminator, and the Discriminator's only goal is to not be fooled. Through this adversarial process, the Generator gets incredibly good at producing highly realistic speech.</li></ul></li></ol><p>This architecture is perfect for a model named "Kitten" because it's known for being incredibly efficient. It's a non-autoregressive model, which means it generates audio chunks in parallel instead of one sample at a time. This makes it blazing fast compared to older, step-by-step models like Tacotron 2. This combination of a VAE, GAN, and a parallel transformer backbone is what allows models like VITS and likely Kitten to be small, fast, and high-quality all at once.</p><p>The success here isn't necessarily about inventing a single, brand-new algorithm from thin air. It's about masterful engineering. It's the art of taking several powerful, proven concepts and synthesizing them into a highly optimized and refined implementation. This is a testament to the fact that in modern AI, execution and clever combination are just as important as pure research.</p><h2 id="kitten-tts-vs-the-world-a-local-tts-showdown"><strong>Kitten TTS vs. The World: A Local TTS Showdown</strong></h2><p>Okay, Kitten TTS is cool on its own. But how does it stack up against the other legends of local TTS? Let's throw it in the ring and see what happens. Ding ding! 🥊</p><h3 id="kitten-tts-vs-piper-tts"><strong>Kitten TTS vs. Piper TTS</strong></h3><p>This is the ultimate battle for the soul of your Raspberry Pi. For a long time, Piper TTS has been the undisputed king of fast, offline, on-device speech synthesis. It's known for being incredibly fast, running on minimal hardware, and having solid voice quality.</p><p><strong>The Verdict:</strong> Kitten is the new challenger, and it's coming in with a significant weight advantage. It is even <em>smaller</em> than most of Piper's voice models and targets the same CPU-only performance profile. For pure, bare-metal efficiency and the absolute smallest possible footprint, Kitten has a real edge. However, Piper has a more mature ecosystem, a wider variety of community-trained voices, and better language support at the moment. It's a close fight.</p><p>Your Choice: If you need the absolute tiniest model for an English-language project, try Kitten first. If you need broader language support or want to tap into a larger library of voices, Piper is still a fantastic choice.</p><h3 id="kitten-tts-vs-kokoro-tts"><strong>Kitten TTS vs. Kokoro TTS</strong></h3><p>Kokoro was the model that first made the community truly believe that small, high-quality TTS was possible. At 82M parameters, it was a huge step down from the billion-parameter giants and delivered impressive "Siri-like" quality on standard CPUs.</p><p><strong>The Verdict:</strong> This is a generational leap. Kitten TTS is <strong>~15M parameters</strong>. Kokoro walked so that Kitten could run a marathon. While Kokoro proved the concept, Kitten has refined it to an extreme degree, offering comparable or better expressiveness at a fraction of the size.</p><p>Your Choice: For any new project where efficiency is a concern, Kitten TTS is the clear winner in the size-to-quality trade-off.</p><h3 id="kitten-tts-vs-coqui-xtts"><strong>Kitten TTS vs. Coqui XTTS</strong></h3><p>This isn't a direct fight; it's about picking the right tool for the job. Coqui's XTTS is a heavyweight champion in its own right, but for a different reason: its incredible <strong>zero-shot voice cloning</strong>. You can feed it a mere 6-second audio clip of a voice, and it can start speaking in that voice. It's magic, but it's a different kind of magic.</p><p><strong>The Verdict:</strong> If your project requires cloning a specific voice, XTTS is the model you want. No question. But this power comes at a cost it's a much larger model and really wants a GPU to run smoothly. Kitten TTS is built for a different purpose: providing a set of high-quality, pre-built voices in the most lightweight and efficient package possible.</p><p>Your Choice: Use Kitten for speed, efficiency, and on-device deployment. Use XTTS for voice cloning and advanced style transfer features.</p><p>To make it even clearer, here's a handy comparison table:</p>
<!--kg-card-begin: html-->
<table><colgroup><col><col><col><col><col></colgroup><tbody><tr><td colspan="1" rowspan="1"><p>Feature</p></td><td colspan="1" rowspan="1"><p>Kitten TTS (Nano)</p></td><td colspan="1" rowspan="1"><p>Piper TTS</p></td><td colspan="1" rowspan="1"><p>Kokoro TTS</p></td><td colspan="1" rowspan="1"><p>Coqui XTTS-v2</p></td></tr><tr><td colspan="1" rowspan="1"><p><strong>Model Size</strong></p></td><td colspan="1" rowspan="1"><p><strong>&lt;25 MB (15M params)</strong></p></td><td colspan="1" rowspan="1"><p>~50-100 MB per voice</p></td><td colspan="1" rowspan="1"><p>~165 MB (82M params)</p></td><td colspan="1" rowspan="1"><p>~1.5 GB+</p></td></tr><tr><td colspan="1" rowspan="1"><p><strong>Resource Needs</strong></p></td><td colspan="1" rowspan="1"><p><strong>CPU-only, low RAM</strong></p></td><td colspan="1" rowspan="1"><p>CPU-only, low RAM (RPi)</p></td><td colspan="1" rowspan="1"><p>CPU-only, moderate RAM</p></td><td colspan="1" rowspan="1"><p>GPU Recommended</p></td></tr><tr><td colspan="1" rowspan="1"><p><strong>Key Feature</strong></p></td><td colspan="1" rowspan="1"><p><strong>Extreme Size &amp; Efficiency</strong></p></td><td colspan="1" rowspan="1"><p>Speed &amp; Language Support</p></td><td colspan="1" rowspan="1"><p>Good quality for its size</p></td><td colspan="1" rowspan="1"><p>Zero-Shot Voice Cloning</p></td></tr><tr><td colspan="1" rowspan="1"><p><strong>Use Case</strong></p></td><td colspan="1" rowspan="1"><p>Edge AI, IoT, Accessibility</p></td><td colspan="1" rowspan="1"><p>Offline Assistants, RPi</p></td><td colspan="1" rowspan="1"><p>General CPU-based TTS</p></td><td colspan="1" rowspan="1"><p>Custom Voice Applications</p></td></tr><tr><td colspan="1" rowspan="1"><p><strong>License</strong></p></td><td colspan="1" rowspan="1"><p><strong>Apache 2.0 (Commercial OK)</strong></p></td><td colspan="1" rowspan="1"><p>Apache 2.0 (Commercial OK)</p></td><td colspan="1" rowspan="1"><p>Apache 2.0 (Commercial OK)</p></td><td colspan="1" rowspan="1"><p>Coqui Public License (Non-Commercial)</p></td></tr></tbody></table>
<!--kg-card-end: html-->
<h2 id="the-game-changing-applications-this-is-why-were-all-here"><strong>The Game-Changing Applications (This Is Why We're All Here)</strong></h2><p>Specs and code are fun, but what can you <em>actually build</em> with this? This is where it gets really exciting. Kitten TTS isn't just a cool tech demo; it's an enabler for a whole new generation of applications that were previously impossible, impractical, or just too damn expensive.</p><h3 id="application-1-true-edge-ai-private-iot"><strong>Application 1: True Edge AI &amp; Private IoT</strong></h3><p>Because Kitten runs entirely locally, it is the perfect engine for <strong>Edge AI</strong>. Think about it: smart home devices that can talk to you without sending your conversations to a server in another country.<strong><sup> </sup></strong>This means three huge things:</p><ol><li><strong>Lower Latency:</strong> Responses are instant because there's no round-trip to the cloud.</li><li><strong>Better Privacy:</strong> Your data never leaves your device. This is a massive deal.</li><li><strong>Offline Functionality:</strong> It works even when your internet is down.</li></ol><p>This unlocks applications like voice-enabled industrial sensors, talking toys for kids that don't spy on them, and smart home assistants that actually respect your privacy.<strong><sup> </sup></strong>The move to on-device processing is a direct response to growing public concern over data privacy, and Kitten is perfectly positioned to power this new wave of secure-by-design products. By eliminating the need to transmit sensitive voice data, it not only protects users but also simplifies compliance with data sovereignty laws.</p><h3 id="application-2-revolutionizing-accessibility-tools"><strong>Application 2: Revolutionizing Accessibility Tools</strong></h3><p>This one is HUGE, and it's something the community is genuinely excited about. People with visual impairments or learning disabilities like dyslexia rely on screen readers to access the digital world. But let's be honest, many of the default voices are still robotic and fatiguing to listen to. A user on Reddit specifically brought up this pain point, wishing for a better voice for the <strong>NVDA screen reader</strong> that wouldn't hog system resources.</p><p>Kitten TTS is the answer. It is small and fast enough to be integrated directly into accessibility tools like NVDA, providing a much more natural, human-sounding voice without slowing down the user's computer. This isn't just a cool feature; it's technology that can genuinely improve people's daily lives and make the digital world more inclusive.</p><h3 id="application-3-the-indie-dev-hobbyists-dream"><strong>Application 3: The Indie Dev &amp; Hobbyist's Dream</strong></h3><p>Want to build a voice for your custom robot? Need to give dialogue to characters in your indie game? Want to create a custom Jarvis-like assistant for your workshop? Before Kitten, you'd need to wrestle with a pricey API or set up a dedicated server. Now, you can do it all on a <strong>Raspberry Pi</strong>. Kitten TTS democratizes high-quality voice synthesis, putting it directly into the hands of every creator, student, and hobbyist, regardless of their budget.</p><h2 id="the-final-verdict-the-future-of-kitten"><strong>The Final Verdict &amp; The Future of Kitten</strong></h2><p>So, is Kitten TTS the perfect, flawless model that will end all others? Let's be real: <strong>not yet</strong>. It's still in <strong>developer preview</strong>. Some users have noted a bit of "soft distortion" in the audio or that the quality isn't quite at the level of the massive, expensive cloud APIs. There's a reason it's called a "preview," after all.</p><p>BUT, that's completely missing the point. The magic of Kitten TTS isn't that it's better than a model 1000x its size. The magic is that it's <strong>so damn good <em>for</em> its size</strong>. The performance-to-parameter ratio is absolutely off the charts. It represents a quantum leap in efficiency and accessibility.</p><p>And the story isn't over. The KittenML team has already announced they're working on a larger, <strong>~80M parameter model</strong> that will use the same eight expressive voices. This "big brother" version will likely smooth out the minor quality issues of the 'nano' model while still being small and efficient enough to run on a CPU. The future is incredibly bright.</p><p>Kitten TTS is a game-changer. It's a testament to the power of open-source innovation and the unstoppable trend toward smarter, smaller, more accessible AI.</p><p>Don't just read about it. <strong>Go build something!</strong></p><ul><li><strong>GitHub Repo: </strong><a href="https://github.com/KittenML/KittenTTS" rel="noreferrer">https://github.com/KittenML/KittenTTS</a></li><li><strong>Hugging Face Model:</strong> <a href="https://huggingface.co/KittenML/kitten-tts-nano-0.1" rel="noopener">https://huggingface.co/KittenML/kitten-tts-nano-0.1</a></li><li><strong>Live Web Demo (Community Built!):</strong> <a href="https://clowerweb.github.io/kitten-tts-web-demo/" rel="noopener">https://clowerweb.github.io/kitten-tts-web-demo/</a></li><li><strong>Join the Discord:</strong> <a href="https://discord.gg/upcyF5s6" rel="noopener">https://discord.gg/upcyF5s6</a></li></ul><h2 id="faq-disambiguation"><strong>FAQ &amp; Disambiguation</strong></h2><h3 id="wait-isnt-kitten-a-character-from-warhammer-40k"><strong>Wait, Isn't Kitten a Character from Warhammer 40k?</strong></h3><p>LOL, you got us. If you searched for "Kitten TTS" and were expecting the glorious Captain-General of the Adeptus Custodes, you're in the wrong place... but welcome! That legendary "Kitten" is from the amazing YouTube series <em>If the Emperor had a Text-to-Speech Device</em>. THIS Kitten TTS is an AI model. Both are pretty awesome, though.</p><h3 id="is-there-a-research-paper"><strong>Is there a research paper?</strong></h3><p>Not yet! The team has mentioned they plan to release more details about their training techniques and architecture soon, likely after the full release. The community is eagerly waiting!</p><h3 id="what-about-benchmarks-like-rtf-or-mos"><strong>What about benchmarks like RTF or MOS?</strong></h3><p>No official, formal benchmarks have been published by the creators yet. However, we can get a clue from the community. In a web demo, one user on an M1 Mac clocked a generation time of about 19 seconds for a 26-second audio clip. This gives us a rough</p><p><strong>Real-Time Factor (RTF)</strong> of about 0.73. RTF is simply the time it takes to generate the audio divided by the duration of the audio itself, so anything under 1.0 is faster than real-time. For a CPU-only model running in a browser, that's very promising!</p><h3 id="what-languages-does-it-support"><strong>What languages does it support?</strong></h3><p>Currently, the <code>nano-0.1</code> preview model only supports <strong>English</strong>. However, the team has stated that multilingual support is on the roadmap for future releases.</p><h3 id="what-is-kitten-tts-and-why-is-it-a-big-deal"><strong>What is Kitten TTS and why is it a big deal?</strong></h3><p>It’s a ~15M-parameter, <strong>&lt;25 MB</strong> text-to-speech model that runs well on plain CPUs and is Apache-2.0 licensed so you can ship it in products without GPU or API costs.</p><h3 id="does-kitten-tts-need-a-gpu"><strong>Does Kitten TTS need a GPU?</strong></h3><p>No. That’s the point it’s optimized for CPU and even runs fully in the browser in a community demo.</p><h3 id="how-big-is-the-download-exactly"><strong>How big is the download, exactly?</strong></h3><p>Under ~25 MB for the nano 0.1 preview (about 15M params). It pulls down fast and works out of the box.</p><h3 id="how-many-voices-does-it-have"><strong>How many voices does it have?</strong></h3><p>Multiple expressive presets (commonly cited as ~8: 4F/4M). Use <code>available_voices</code> in the API to list and pick.</p><h3 id="is-it-multilingual-yet"><strong>Is it multilingual yet?</strong></h3><p>The nano-0.1 preview is <strong>English-only</strong>. Multilingual support is on the roadmap.</p><h3 id="can-it-run-in-the-browser"><strong>Can it run in the browser?</strong></h3><p>Yes, there’s a community web demo using <code>transformers.js</code> that runs fully client-side on CPU. Great for quick tests.</p><h3 id="does-kitten-tts-support-ssml"><strong>Does Kitten TTS support SSML?</strong></h3><p>Not officially in the preview docs. Community threads have asked about it; for now, I control prosody with punctuation and chunking.</p><h3 id="does-it-do-zero-shot-voice-cloning"><strong>Does it do zero-shot voice cloning?</strong></h3><p>No that’s where <strong>Coqui XTTS-v2</strong> shines, but XTTS is heavier and GPU-friendly rather than tiny CPU-only. Use Kitten for preset voices and speed; XTTS for cloning.</p><h3 id="kitten-tts-vs-piper-what-should-i-pick"><strong>Kitten TTS vs Piper what should I pick?</strong></h3><p>If you want the <strong>smallest</strong> footprint for English on CPU, start with Kitten. If you need broader language coverage and a mature ecosystem, Piper is still excellent. I use both depending on target.</p><h3 id="kitten-tts-vs-kokoro-who-wins-on-cpus"><strong>Kitten TTS vs Kokoro who wins on CPUs?</strong></h3><p>Kokoro (≈82M) proved small can sound good; Kitten pushes size/latency further at ~15M. For super-lean builds, Kitten has the edge; Kokoro has more established usage and voices.</p><h3 id="is-the-license-ok-for-commercial-use"><strong>Is the license OK for commercial use?</strong></h3><p>Yes. <strong>Apache-2.0</strong> permissive and business-friendly. Ship it.</p><h3 id="how-do-i-install-it-quickly"><strong>How do I install it quickly?</strong></h3><p>Create a venv and <code>pip install</code> the wheel from the latest GitHub release, then load <code>"KittenML/kitten-tts-nano-0.1"</code> in your code. Simple, reproducible, and offline-friendly.</p><h3 id="is-it-fast-enough-for-real-time"><strong>Is it fast enough for real-time?</strong></h3><p>Early community reports and the browser demo are promising on CPU. For snappy UX, stream shorter chunks and cache frequent phrases.</p><h3 id="any-gotchas-i-should-know"><strong>Any gotchas I should know?</strong></h3><p>It’s a <strong>developer preview</strong> some users note mild artifacts on certain voices. I pick the cleaner presets (e.g., “2-f/2-m/4-f”) for narration.</p><h3 id="what-are-good-use-cases-right-now"><strong>What are good use cases right now?</strong></h3><p>On-device assistants, offline accessibility tools, indie games/NPCs, and privacy-sensitive apps that can’t rely on cloud TTS. (That’s exactly where I’d ship it first.)</p></section>




</article>


    




</main>

        

        

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HHS Winds Down mRNA Vaccine Development Under BARDA (132 pts)]]></title>
            <link>https://www.hhs.gov/press-room/hhs-winds-down-mrna-development-under-barda.html</link>
            <guid>44805261</guid>
            <pubDate>Tue, 05 Aug 2025 22:29:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hhs.gov/press-room/hhs-winds-down-mrna-development-under-barda.html">https://www.hhs.gov/press-room/hhs-winds-down-mrna-development-under-barda.html</a>, See on <a href="https://news.ycombinator.com/item?id=44805261">Hacker News</a></p>
Couldn't get https://www.hhs.gov/press-room/hhs-winds-down-mrna-development-under-barda.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Create personal illustrated storybooks in the Gemini app (185 pts)]]></title>
            <link>https://blog.google/products/gemini/storybooks/</link>
            <guid>44804411</guid>
            <pubDate>Tue, 05 Aug 2025 21:14:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/products/gemini/storybooks/">https://blog.google/products/gemini/storybooks/</a>, See on <a href="https://news.ycombinator.com/item?id=44804411">Hacker News</a></p>
<div id="readability-page-1" class="page"><div slot="uni-short-post-description-slot"><p data-block-key="kyyrc">Today we’re announcing a new way to bring your ideas to life in the Gemini app: personalized, illustrated storybooks complete with read-aloud narration.</p><p data-block-key="7m5p9">Simply describe any story you can imagine, and Gemini generates a unique 10-page book with custom art and audio. For a truly personal touch, you can ask Gemini to draw inspiration from your own photos and files. Bring your vision to life in any style imaginable: from pixel art and comics to claymation, crochet, and even coloring books, in more than 45 languages.</p><ul><li data-block-key="2gt88"><b>Help your child understand a complex topic:</b> Create a story that explains the solar system to my 5 year old.</li><li data-block-key="e908b"><b>Teach a lesson through storytelling:</b> Teach a 7-year-old boy about the importance of being kind to his little brother. My son loves elephants so let’s make the main character an elephant.</li><li data-block-key="68oot"><b>Bring personal artwork to life</b>: Upload an image of a kid's drawing and modify this example prompt for your use case: "This is my kid’s drawing. He’s 7 years old. Write a creative storybook that brings his drawing to life.”</li><li data-block-key="5v3mr"><b>Turn memories into magical stories:</b> Upload photos from your family trip to Paris and create a personalized adventure.</li></ul><p data-block-key="26fhm"><a href="http://gemini.google.com/gem/storybook">Try it today in the Gemini app</a>. Available globally on desktop and mobile, in all languages Gemini is available.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A new database on police use of force and misconduct in California (132 pts)]]></title>
            <link>https://journalism.berkeley.edu/police-records-access/</link>
            <guid>44803196</guid>
            <pubDate>Tue, 05 Aug 2025 19:38:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://journalism.berkeley.edu/police-records-access/">https://journalism.berkeley.edu/police-records-access/</a>, See on <a href="https://news.ycombinator.com/item?id=44803196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-node="jhblr3yv2e8m">
	<h4><img loading="lazy" decoding="async" src="https://journalism.berkeley.edu/wp-content/uploads/2025/07/Logo.Police-Records-Access.bw_-600x96.png" alt="" width="555" height="89" srcset="https://journalism.berkeley.edu/wp-content/uploads/2025/07/Logo.Police-Records-Access.bw_-600x96.png 600w, https://journalism.berkeley.edu/wp-content/uploads/2025/07/Logo.Police-Records-Access.bw_-1140x182.png 1140w, https://journalism.berkeley.edu/wp-content/uploads/2025/07/Logo.Police-Records-Access.bw_-768x123.png 768w, https://journalism.berkeley.edu/wp-content/uploads/2025/07/Logo.Police-Records-Access.bw_.png 1159w" sizes="auto, (max-width: 555px) 100vw, 555px"></h4>
<p><strong>For Immediate Release</strong><br>
<strong>August 4, 2025<br>
Contact: Journalism@berkeley.edu</strong></p>
<h2><b>A new database on police use of force and misconduct in California makes public 1.5 million pages of once-secret police records</b></h2>

<p><span>Public records about use of force and misconduct by California law enforcement officers — some 1.5 million pages obtained from nearly 500 law enforcement agencies — will now be searchable by the public for the first time thanks to a new database built by UC Berkeley and Stanford University and published today by the <a href="https://www.latimes.com/california/story/2025-08-04/clean-database-launch">Los Angeles Times</a>, <a href="https://www.sfchronicle.com/bayarea/article/california-police-records-20797063.php">San Francisco Chronicle</a>, <a href="https://www.kqed.org/news/12050816/thousands-of-california-police-records-now-publicly-available">KQED</a> and <a href="https://calmatters.org/justice/2025/08/police-misconduct-records-database/">CalMatters</a>.</span></p>
<p><span>The database — the first of its kind in the nation — will vastly expand public access to internal affairs records that disclose how law enforcement agencies throughout the state h</span><span>andle misconduct allegations as well as uses of police force that result in death or serious injury. The database, funded by the State of California, currently has records from nearly 12,000 cases, including thousands involving police shootings. Every record in the database was released by a law enforcement agency after being redacted in compliance with California’s public records laws. As a result, journalists and members of the public will now be able to </span><span>search statewide for particular types of misconduct and use-of-force. Police chiefs will be able to use the data to aid in hiring decisions. Researchers will be able to identify trends and patterns.&nbsp;</span></p>
<p><span><img fetchpriority="high" decoding="async" src="https://journalism.berkeley.edu/wp-content/uploads/2025/08/Public-Records-Access-Project-hero-600x381.png" alt="A stack of paper on a red stool." width="422" height="267" srcset="https://journalism.berkeley.edu/wp-content/uploads/2025/08/Public-Records-Access-Project-hero-600x381.png 600w, https://journalism.berkeley.edu/wp-content/uploads/2025/08/Public-Records-Access-Project-hero-1140x723.png 1140w, https://journalism.berkeley.edu/wp-content/uploads/2025/08/Public-Records-Access-Project-hero-768x487.png 768w, https://journalism.berkeley.edu/wp-content/uploads/2025/08/Public-Records-Access-Project-hero-1536x975.png 1536w, https://journalism.berkeley.edu/wp-content/uploads/2025/08/Public-Records-Access-Project-hero.png 1967w" sizes="(max-width: 422px) 100vw, 422px">The database, called the Police Records Access Project, is the product of years of work by a multidisciplinary team of journalists, data scientists, lawyers and civil liberties advocates, led by UC Berkeley Journalism’s </span><a href="https://journalism.berkeley.edu/programs/mj/investigative-reporting/"><span>Investigative Reporting Program </span></a><span>(IRP)</span><span>, </span><span>the </span><a href="https://bids.berkeley.edu/"><span>Berkeley Institute for Data Science</span></a><span> (BIDS), and Stanford University’s </span><a href="http://cjlab.stanford.edu/projects/big-local-news/"><span>Big Local News</span></a><span>. Other key contributors include the </span><a href="https://www.aclusocal.org/"><span>ACLU Foundation of Southern California</span></a><span>, California innocence organizations, the </span><a href="https://www.nacdl.org/"><span>National Association of Criminal Defense Lawyers</span></a><span>, UC Irvine law school’s </span><a href="https://www.law.uci.edu/academics/real-life-learning/clinics/press-freedom-project.html"><span>Press Freedom Project</span></a><span> and UC Berkeley law school’s </span><a href="https://www.law.berkeley.edu/research/criminal-law-and-justice-center/"><span>Criminal Law &amp; Justice Center</span></a><span>.&nbsp;</span></p>
<p><span>The team systematically collected, organized and vetted millions of public records, used emerging technologies such as generative AI to build the database, and created from scratch a searchable user-interface.&nbsp;</span></p>
<p><span>“The creation of a public facing database is critical for all of the stakeholders in the criminal legal system: whether public defenders, innocence organizations, prosecutors, police departments or academics,” said Barry Scheck, co-founder and special counsel to the Innocence Project. “This information can be used to understand the system and reform it.”</span></p>
<p><span>Aditya Parameswaran, an associate professor at </span><span>UC Berkeley’s Department of Electrical Engineering and Computer Sciences, </span><span>led work on the database at BIDS. “Here we have an amazing example of how generative AI </span><span>— with humans in the loop — </span><span>can be used for good, at a scale that’s unprecedented, for a task that’s never been done before and for societal impact,” he said.</span></p>
<p><span>Creation of the database was made possible by a series of landmark laws adopted recently by the state of California to improve transparency around law enforcement. Through </span><a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB1421"><span>S.B. 1421</span></a><span>, approved in 2018, and </span><a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202120220SB16"><span>S.B. 16</span></a><span>, approved in 2021, California made records related to uses of force and misconduct accessible to the public for the first time. However, requesting these documents through the Public Records Act required going agency by agency, a laborious process that has made it impossible until now to identify trends and patterns across the state.</span></p>
<p><span>“For 40 years California hid police misconduct,’’ said former state Sen. Nancy Skinner, who helped lead the legislative push for the new transparency laws and played a key role in securing state funding to create the database. “We were able to open those records to the public when the legislature passed S.B. 1421 in 2019. Now with this new database, Californians will have even better access, making it easier to find out which law enforcement officers have a history of bad behavior and which of our police departments do the right thing to hold their officers accountable.’’&nbsp;</span></p>
<p><span>Tiffany Bailey, senior staff attorney at the ACLU Foundation of Southern California, which will contribute an additional 200,000 records to the database from its own efforts to obtain police records under S.B. 1421, said the Police Records Access Project will be a “vital tool’’ in holding law enforcement agencies accountable. “Critically, families who have lost loved ones in California will now have direct access to the information they need to seek meaningful accountability that has too often been denied,” Bailey said.</span></p>
<p><span>The database released today can now be accessed online via KQED, The San Francisco Chronicle, Los Angeles Times and CalMatters. The database does not include audio recordings or videos, and additional steps were taken to redact or remove graphic imagery along with personal information about sexual assault or domestic violence victims. </span></p>
<p><span><img loading="lazy" decoding="async" src="https://journalism.berkeley.edu/wp-content/uploads/2025/08/policedatabase0804_ph1-1-600x400.jpg" alt="A screen with the words Police Records Access Project that looks like a page of the datagase, surrounded by a green three-dimensional border." width="411" height="274" srcset="https://journalism.berkeley.edu/wp-content/uploads/2025/08/policedatabase0804_ph1-1-600x400.jpg 600w, https://journalism.berkeley.edu/wp-content/uploads/2025/08/policedatabase0804_ph1-1-1140x760.jpg 1140w, https://journalism.berkeley.edu/wp-content/uploads/2025/08/policedatabase0804_ph1-1-768x512.jpg 768w, https://journalism.berkeley.edu/wp-content/uploads/2025/08/policedatabase0804_ph1-1-1536x1024.jpg 1536w, https://journalism.berkeley.edu/wp-content/uploads/2025/08/policedatabase0804_ph1-1.jpg 2048w" sizes="auto, (max-width: 411px) 100vw, 411px">Work on the database began in 2018, when journalists in some 40 newsrooms formed the <a href="https://projects.scpr.org/california-reporting-project/">California Reporting Project</a> and began sharing documents obtained through records requests. Early funding to support this work was provided by the Sony Foundation and Roc Nation. In all, reporters sent more than 3,500 public records requests to nearly 700 police departments, district attorney offices, </span><span>sheriff’s offices, oversight agencies, probation, corrections</span><span> and coroners all across the state. “The idea was to collaborate among organizations to build up this system, to make it easy to access these public records,’’ said Cheryl Phillips, founder of Stanford’s Big Local News, which specializes in helping local newsrooms incorporate data into their reporting.</span></p>
<p><span>The California Reporting Project has produced more than 100 stories to date from these records. Leading contributors to the California Reporting Project include the </span><a href="https://www.mercurynews.com/"><span>Bay Area News Group</span></a><span>/</span><a href="https://www.ocregister.com/"><span>Southern California News Group</span></a><span>, </span><a href="https://www.capradio.org/"><span>CapRadio</span></a><span>, KPCC/</span><a href="https://laist.com/"><span>LAist</span></a><span> and the newsrooms publishing the database today. Additional newsrooms are expected to publish the database as part of a broader rollout effort, and new features and tools will be added to the database as they are developed by the Police Records Access Project.</span></p>
<p><span>“Making police misconduct records more transparent, searchable, and accessible to the public is a monumental leap for accountability,” said Lisa Wayne, executive director of the National Association of Criminal Defense Lawyers.</span></p>
<p><span>###</span></p>
<p><span>_________________________________________________________________________________________________________</span></p>
<p><strong>For more information about the Police Records Access Project, please contact David Barstow, Investigative Reporting Program chair, through Andrea Lampros, UC Berkeley Journalism’s communications director: 510.847.4469, alampros@berkeley.edu or journalism@berkeley.edu.</strong></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI is propping up the US economy (289 pts)]]></title>
            <link>https://www.bloodinthemachine.com/p/the-ai-bubble-is-so-big-its-propping</link>
            <guid>44802916</guid>
            <pubDate>Tue, 05 Aug 2025 19:19:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloodinthemachine.com/p/the-ai-bubble-is-so-big-its-propping">https://www.bloodinthemachine.com/p/the-ai-bubble-is-so-big-its-propping</a>, See on <a href="https://news.ycombinator.com/item?id=44802916">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>Greetings all — </p><p><span>Busy week in BITM land. Got home from some travel only to take off to SF to speak on a panel about AI and work for </span><a href="https://www.youtube.com/watch?v=9E_q277l7KM&amp;list=PL7ZZjNmMAwA3Liz0XuJ5isKBLIkCfYLwt&amp;index=1" rel="">a CalMatters conference</a><span> with state lawmakers and labor leaders, and made it back to LA in time for </span><a href="https://www.404media.co/behind-the-blog-party-vibes-and-spilling-tea/" rel="">the 404 live event night</a><span>, where I had the pleasure of bumping into a bunch of BITM readers. I met ambitious students examining the history of tech and labor in the entertainment industry, veteran tech policy campaigners, and some young critical journos. You all are the best. It ALSO makes me think I should throw a BITM event or meetup of some kind one of these days—readers have suggested this before, and I like the idea, I’ve just been so slammed. I digress.</span></p><p><span>For those interested, I did an interview </span><a href="https://time.com/charter/7307090/how-ai-adoption-is-sitting-with-workers/" rel="">with TIME Magazine’s Charter project</a><span>. It was not my most eloquent spot—exhaustion!—but I think some good points came through. Also chatted with indy media/freelancer advocacy outlet </span><a href="https://thestudyhallpodcast.podbean.com/e/inside-the-ai-jobs-crisis-ft-brian-merchant/" rel="">Study Hall about AI in journalism</a><span>.</span></p><p><span>It was </span><em>also</em><span> a busy week in the AI world, though that’s hardly surprising, for reasons pertinent to the news we’re about to discuss. Without further ado, in this issue, we’ll dive into</span></p><p><span>-Microsoft’s AI-fueled $4 trillion valuation</span><br><span>-AI is driving so much investment it’s like a “private sector stimulus program” papering over losses to tariffs </span></p><p><span>And this week’s CRITICAL AI REPORT, with </span><br><span>-How America’s professors are organizing to fight the push of AI into higher ed</span><br><span>-The fury over an AI model appearing in Vogue</span><br><span>-Inside the collapse of Builder.AI, the AI company that wasn’t</span><br><span>-Good bloody news: Courts uphold ruling that Google’s Play store and billing system are illegal monopolies</span></p><p>For the full report, and to support my 100% grade A independent journalism, please consider becoming a paid subscriber so, well, I can do more of it. I’m very grateful to every subscriber who shells out their hard-earned bucks—equivalent to a cheap-ish beer on draft or a decent latte a month—to make this all possible. Hammers up. </p><p>Last week, Microsoft became the second company, after Nvidia, to reach a $4 trillion valuation. </p><p><span>CNN </span><a href="https://www.cnn.com/2025/07/31/tech/microsoft-4-trillion-valuation" rel="">reports</a><span>: </span></p><blockquote><p><span>Microsoft’s shares ﻿(</span><strong><a href="https://www.cnn.com/markets/stocks/MSFT" rel="">MSFT</a></strong><span>) jumped nearly 4.5% after the market opened on Thursday, pushing its intraday valuation to $4.01 trillion. The company’s shares have risen roughly 28% since the start of this year.</span></p><p><span>The milestone comes just a year and a half after Microsoft reached a </span><strong><a href="https://www.cnn.com/2024/01/24/investing/microsoft-three-trillion-market-value" rel="">$3 trillion valuation</a></strong><span>. The company first cracked the </span><strong><a href="https://www.cnn.com/2019/07/11/investing/amazon-microsoft-trillion-dollar-market-value" rel="">$1 trillion mark </a></strong><span>in April 2019. It follows Nvidia into the $4 trillion valuation club, which hit the mark earlier this month.</span></p></blockquote><p>There are so many wild and noteworthy things about this milestone that it’s hard to know when to start. </p><p><span>First, let’s take a second to note the sheer insanity of these numbers. The first company to hit a $1 trillion valuation in the modern era was Apple, in 2018. Now, just seven years later, there are </span><em>nine</em><span> $1 trillion+ tech companies, and Google, Amazon, and Meta are soaring past $2 trillion, and now Apple is well past $3 trillion. Much of this expansion of value has occurred in just the last two years, on the back of the AI boom. Nvidia tripled its valuation, becoming the first-ever $4 trillion company in the process, in less than </span><em>one year</em><span>. </span></p><p><span>Second, note the </span><em>source</em><span> of Microsoft’s new investor enthusiasm: Like Nvidia, MS seems to be primarily benefitting from the AI boom by selling shovels during the gold rush. While Nvidia cornered the market on chips needed to run AI’s resource-intensive computation, Microsoft is winning by selling cloud compute in bulk. (Notably its biggest client is OpenAI, which is part of a byzantine deal between the two companies, but more on that in a second.) Microsoft’s largest revenue source is now Azure, its cloud compute business. Azure used to be a distant second, behind industry leader Amazon Web Services, but better-than-expected sales there last quarter have driven Microsoft to its new peak. </span></p><p><span>Third, Microsoft and Nvidia are benefitting from bona fide historic levels of investment. Chris Mims sites the analysis of investor and programmer </span><a href="https://paulkedrosky.com/honey-ai-capex-ate-the-economy/" rel="">Paul Kedrosky</a><span> in his most recent column in </span><a href="https://www.wsj.com/tech/ai/silicon-valley-ai-infrastructure-capex-cffe0431?st=oLCAcg&amp;reflink=desktopwebshare_permalink" rel="">the Wall Street Journal</a><span>:</span></p><blockquote><p><span>spending on AI infrastructure has already exceeded spending on telecom and internet infrastructure from the dot-com boom—and it’s still growing… one explanation for the U.S. economy’s ongoing strength, despite tariffs, is that spending on IT infrastructure is so big that it’s acting as a sort of </span><a href="https://paulkedrosky.com/honey-ai-capex-ate-the-economy/" rel="">private-sector stimulus program</a><span>.</span></p><p><span>Capex spending for AI </span><a href="https://x.com/RenMacLLC/status/1950544075989377196" rel="">contributed more to growth</a><span> in the U.S. economy in the past two quarters than </span><em>all of consumer spending</em><span>, says Neil Dutta, head of economic research at Renaissance Macro Research, citing data from the Bureau of Economic Analysis.</span></p></blockquote><p><span>I’ll just repeat that. Over the last six months, capital expenditures on AI—counting just information processing equipment and software, by the way—added more to the growth of the US economy than all consumer spending combined. You can just pull any of those quotes out—spending on IT for AI is so big it might be making up for economic losses from the tariffs, serving as a </span><em>private sector stimulus program</em><span>.</span></p><p><span>To me, this is just screaming bubble. I’m sure I’m not alone. In fact I </span><em>know</em><span> I’m not alone. I’m thinking especially of </span><a href="https://www.wheresyoured.at/the-haters-gui/" rel="">Ed Zitron’s impassioned and</a><em><a href="https://www.wheresyoured.at/the-haters-gui/" rel=""> </a></em><a href="https://www.wheresyoured.at/the-haters-gui/" rel="">thorough guide to the AI bubble</a><span>; a rundown of how much money is being poured into and spent on AI vs how much money these products are making, and surprise, the situation as it stands is not sustainable. Worrying signs abound, and not least that so far, the companies benefitting most from AI are those selling the tools to simply build </span><em>more</em><span> of it (Nvidia, Microsoft), or who have monopolies through which they can force AI tools onto users en masse with limited repercussions (Google, Meta). Consumers routinely evince negative</span><em> </em><span>sentiment towards AI and AI products in polls, outweighing enthusiasm. And meanwhile, what I’d say is the only truly runaway, organically popular AI product category, chatbots, largely remain big money losers due to the resources they take to run. </span></p><p>As such, these massive valuations feel fishy. I asked Ed for his thoughts on Microsoft’s $4 trillion earnings report. He said: </p><blockquote><p>Microsoft broke out Azure revenue for the first time in history, yet has not updated their annualized revenue for AI since January 29 2025. If things were going so well with AI, why are they not providing these numbers? It's because things aren't going well at all, and they're trying to play funny games with numbers to confuse and excite investors. </p><p>Also, $10bn+ of that Azure revenue is OpenAI's compute costs, paid at-cost, meaning no profit (and maybe even loss!) for Microsoft.</p></blockquote><p>Look, I’m no prophet, clearly. I’ve predicted that we were probably witnessing the peak of the AI boom *nearly a year ago*, and while I think I was right with regard to genuine consumer and pop cultural interest, obviously the investment and expansion has kept right on flowing. It’s to the point that we’re well past dot com boom levels of investment, and, as Kedrosky points out, approaching railroad-levels of investment, last seen in the days of the robber barons. </p><p>I have no idea what’s going to happen next. But if AI investment is so massive that it’s quite actually helping to prop up the US economy in a time of growing stress, what happens if the AI stool does get kicked out from under it all? </p><p><span>There could be a crash that exceeds the dot com bust, at a time when the political situation through which such a crash would be navigated would be nightmarish. There could be a smaller bust, which weeds out the less monopolistic and unprofitable AI companies, or drives them to survive with the help of the tech giants or </span><a href="https://www.bloodinthemachine.com/p/trumps-ai-action-plan-is-a-blueprint" rel="">the ever more AI-friendly US state</a><span>. Who knows. It does, however, seem increasingly unlikely that there will be </span><em>no </em><span>correction at all. (I also should say that I find it implausible any crash will simply wipe out AI as a product category, either; as a surveillance and automation tool, AI is simply too alluring to business, and as a chatbot product, it’s already addicted millions of users. There are fresh challenges here.)</span></p><div data-component-name="DigestPostEmbed"><a href="https://www.bloodinthemachine.com/p/trumps-ai-action-plan-is-a-blueprint" rel="noopener" target="_blank"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!oWg2!,w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e968424-422c-4108-94e9-58884a2c62a5_1599x900.webp"><img src="https://substackcdn.com/image/fetch/$s_!oWg2!,w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8e968424-422c-4108-94e9-58884a2c62a5_1599x900.webp" sizes="100vw" alt="Trump's AI Action Plan is a blueprint for dystopia" width="140" height="140"></picture></div></a></div><p>Recognizing from history the possibilities of where this all might lead, the prospect of any serious economic downturn being met with a widespread push of mass automation—paired with a regime overwhelmingly friendly to the tech and business class, and executing a campaign of oppression and prosecution of precarious manual and skilled laborers—well, it should make us all sit up and pay attention. </p><p>But take heart! There are people organizing against the wanton encroachment of AI into every pore of society. To wit: There are few sectors that have been as inundated with AI as education. From a deluge of AI-generated student papers to the AI companies hard push to sell administration on new edtech tools, education, and especially higher ed, is one of the front lines of AI. </p><p>Frustrated by their lack of a voice in how AI was being deployed in classrooms and beyond, with administrations too willing to greenlight AI deals without their consent, professors and academic workers are officially pushing organizing to push back. </p><p><span>In July, the American Association of University Professors (AAUP), the union that represents faculty and academic workers at colleges across the nation, issued a report, </span><em><a href="https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic" rel="">Artificial Intelligence and Academic Professions</a></em><span>. The report distill the results a survey of its membership and “calls for the establishment of policies in colleges and universities that prioritize economic security, faculty working conditions, and student learning conditions as advancements in artificial intelligence (AI) technologies accelerate.”</span></p><p><span>I spoke to </span><a href="https://www.britt-paris.net/about.html" rel="">Britt Paris</a><span>, the head of the AAUP's ad hoc Committee on Artificial Intelligence and Academic Professions, about the report and what it means. </span></p><p><strong>BLOOD IN THE MACHINE: What are the report’s key findings?</strong></p><p><strong>Britt Paris:</strong><span> This report… found that overall while faculty are often mandated to use specific technologies, 71% say they have no means to have a say or make decisions around whether or how technology is procured, deployed and used at their institutions. </span></p><p>People want to have meaningful power around tech decisions at their institutions. They also want the ability to opt out of technology without risk of punishment. They want better working conditions and are concerned about technology driving down wages, being used for surveillance, and to destroy academic freedom. They are also concerned about their intellectual property being used to amass profit for massive technology companies. </p><p>We want to be paid meaningfully for our labor and accommodated appropriately, as we educate young people to become active and knowledgeable participants in democracy, with the ability to form thoughtful relationships with one another and with the environment, and live full happy lives. We have done interviews and other engagements since the survey, and have overwhelmingly found that instead of improving working and learning conditions, administrators are too easily wooed by tech industry hype, and they are quick to deploy technology solutions which are cheaper and far less effective in bringing about positive change in higher education.</p><p><strong>Why did you deem it crucial to address the spread of AI in higher education? Do you consider there to be a crisis of AI in higher ed?</strong></p><p><span>We started this work last year, and even then higher education was severely evaluated and under attack, though in a different way than it has been since January 2025. We have needed better policy around technology in higher education for years, at least for 15, when the software-as-a-service model came to be the norm for bringing higher education online. Higher education has always had a problem with handing out massive amounts of money for technical solutions that could have been, and </span><em>were </em><span>until 15 years ago, developed and governed in-house, so to speak. </span></p><p><span>Educational technology, such as learning management systems (LMS) like Canvas, has always allowed these corporate firms to do anything they want with user data— tracking their clicks and activities both within and outside of the LMS, as well as retains the data that comprises course materials posted within these LMS, like syllabi and videos produced by instructors, and students completed assignments. In 2021, Concordia University notably got in hot water because they were </span><a href="https://slate.com/technology/2021/01/dead-professor-teaching-online-class.html" rel="">repurposing videos in an online class</a><span> from an instructor who was deceased. And even now, we have heard from members that administrators are floating the idea of creating AI avatars to "teach" in online courses to repurpose course material from people who they lay off.</span></p><p>As AI has been incorporated into educational technology, often without users knowing it, with no announcement from the university, and as universities had, before this year been trading enormous sums of money for AI partnerships, it makes the need to address technology issues in higher ed even more urgent. Arizona State began their partnership with OpenAI and other tech firms in 2024, which has now started sweeping higher education from the University of Michigan, which houses the largest, oldest, and most well curated datasets in higher education to the massive California State University system that is comprised of over 500,000 students.</p><p><strong>What are the risks to students?</strong></p><p><span>Students </span><a href="https://www.nytimes.com/2025/05/14/technology/chatgpt-college-professors.html" rel="">don't like </a><span>when their instructors use AI for grading or for developing course materials. They find that it "enshittifies" their education, in the words of one survey respondent. Students do want to be challenged, to learn, to develop different perspectives on the world and to hope for a future than is better than what we have at present. They understand that education provides them at least some of the knowledge, skills, and outlook necessary to live a good life. Another survey respondent noted that for students, they were not so concerned with the "academic honesty" discourse around using generative AI for assignments than the "failure to learn", thus the solutions we develop should not be punitive towards students, but rather engage them in solidarity, because it is their learning conditions that are at stake as much as our faculty working conditions are.</span></p><p><strong>What are the solutions and ideas for countermeasures you propose? How are you approaching organizing around AI?</strong></p><p>In our report, we focus on developing shared governance collectives comprised of workers—instructors, staff, and students, elected by those workers—to make decisions around technology procurement, deployment, and training. We have a collective bargaining guide for faculty across the country lucky enough to have union contracts, that includes wishlists and sample language that has been effective. We include other types of documents that faculty across the country with no union contract have used in faculty governing bodies like academic senate resolutions, memoranda of understanding and the like. </p><p>We suggest that these shared governance collectives would be better suited to decide on the many issues in higher education than the trustees who currently set the policy in institutions, and have, as a result, deeply devalued higher education in favor of the market driven bottom line, and lining their own pockets. We also suggest that there is a vast need to develop people's tech advocacy units to provide counter narratives to tech industry and adjacent consulting firms for state level policymakers, so that policymakers have the knowledge and ability to develop and vote on policy in favor of the people.</p><p><span>Across industries and communities, no one except the very rich who are profiting off of the gutting of what's left of the public good, no one is happy with AI's takeover. AI applications are </span><a href="https://www.cnet.com/personal-finance/retirement/social-security-has-a-maddening-new-ai-phone-bot-heres-how-to-deal-with-it/" rel="">taking over</a><span> government services and agencies like the Social Security Administration, and even to </span><a href="https://www.wired.com/story/doge-college-student-ai-rewrite-regulations-deregulation/" rel="">write regulation</a><span>. </span><a href="https://www.404media.co/ice-taps-into-nationwide-ai-enabled-camera-network-data-shows/" rel="">ICE is using it</a><span> to incarcerate people. Social media is unusable, </span><a href="https://www.theguardian.com/technology/2025/jul/24/ai-summaries-causing-devastating-drop-in-online-news-audiences-study-finds" rel="">most traffic on the internet goes to AI summaries</a><span>, and not to actual pages. Big tech advocate for AI taking over </span><a href="https://www.zdnet.com/article/this-new-ai-video-editor-is-an-all-in-one-production-service-for-filmmakers-how-to-try-it/" rel="">creative</a><span>, </span><a href="https://www.ndtv.com/feature/wikipedia-set-to-embrace-ai-what-does-it-mean-for-human-editors-8313745" rel="">knowledge</a><span>, and</span><a href="https://www.bloodinthemachine.com/p/inside-the-escalating-struggle-over" rel=""> journalism</a><span> sectors, which results in more muddying the waters around current and past events, as well as obliterating the personal fulfillment that comes from the act of creation itself, as well as the act of enjoying cultural content. </span></p><p>People are largely unhappy with the way that big tech has rolled out AI. But that dissatisfaction does not change things. AI here, as a tech industry product being rammed down our throats is part of the drive towards immiseration of the masses we have been seeing ramp up since the pandemic – in the minds of the oligarchs, there's no longer a future, so they are making what cash grabs they can so they can build their bunkers to save them while the world they have immiserated burns. Making common cause with people across areas of concern and pushing towards change across multiple avenues is the only way out, we cannot have a technical infrastructure that serves humanity when the roots of it are so rotten. </p><p><span>So we must refuse and cut off what doesn't serve us, and strive to let people be in charge of technology—not oligarchs and not authoritarian governments. We must develop counter narratives to the dominant view of technology, and especially AI, as a unquestionable good. We must provide examples of fighting back, as we have done in the contract language, and will continue to develop and communicate. We must connect the multiple attacks we see at present to corporate ownership of technology and capture of the electoral process. And we have to do this</span><em> while</em><span> bringing people in to the big tent to organize at the local level, from the bottom up.</span></p><p><span>(I have a </span><a href="https://www.ucpress.edu/books/radical-infrastructure/paper" rel="">book coming out in January 2026</a><span> about this very topic—not connected to A—but based in Internet infrastructure.)</span></p><p><strong>How does Trump's AI action plan complicate matters here, if at all?</strong></p><p>Our work on the AI committee both with the AAUP and in our own activities counters the dominant narrative by providing examples where to push back and how that will hopefully be compelling to ordinary people, and easier to pick up, organize around, and move forward with. </p><p><span>One of the first Executive Orders coming from the second Trump administration—on its second day—established Executive Order (EO) 14179, “Removing Barriers to American Leadership in Artificial Intelligence” that “revokes certain existing AI policies and directives that act as barriers to American AI innovation” and requires that AI expands “free from ideological bias or engineered social agendas” of course, the ideological bias referenced here is taken up and made more explicit in this AI Action Plan unveiled by the Trump administration July 23, as it states it's against "</span><a href="https://www.whitehouse.gov/fact-sheets/2025/07/fact-sheet-president-donald-j-trump-prevents-woke-ai-in-the-federal-government/" rel="">woke AI</a><span>" and threatens what little Biden area regulation around AI exists. It promises no regulation around AI at the federal level and opens the door for massive data center construction and regulating energy consumption related to data center use. Data centers are crucial for AI function as AI requires a glut of data and processing power to function. This is all great news to the tech oligarchs like Mark Zuckerberg and Sam Altmann who were in the front row of Trump's inauguration speech.</span></p><p><span>Less attention has been given Department of Education head Linda McMahon's statement around </span><a href="https://www.ed.gov/about/news/press-release/us-department-of-education-issues-guidance-artificial-intelligence-use-schools-proposes-additional-supplemental-priority" rel="">AI in education</a><span> released on July 23 in conjunction with the Trump AI Action Plan that uses standard language around AI as a necessary good, and that training and responsible AI use is necessary—without detailing what that means—while it ostensibly seeks to extend the same battles against "woke AI". </span></p><p><span>This and the AI action plan does not necessarily complicate matters for organizing around AI in higher ed, but it could. There is a dearth of framing around anything that would run counter to this standard line around AI as an unquestionable good, with policymakers and boards of trustees, and administrators of universities. The AAUP has signed on to the </span><a href="https://peoplesaiaction.com/" rel="">People's AI Action Plan</a><span> to provide and begin organizing around a narrative that people, not oligarchs should control technology. We will continue organizing for this end.</span></p><p>I have to say, it’s nice and affirming to see that, even two plus years into the AI boom, there’s still a strong reaction to efforts to replace human work and art with AI. This week’s exhibit A: </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[TIL that You can spot base64 encoded JSON, certificates, and private keys (323 pts)]]></title>
            <link>https://ergaster.org/til/base64-encoded-json/</link>
            <guid>44802886</guid>
            <pubDate>Tue, 05 Aug 2025 19:17:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ergaster.org/til/base64-encoded-json/">https://ergaster.org/til/base64-encoded-json/</a>, See on <a href="https://news.ycombinator.com/item?id=44802886">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-astro-cid-37fxchfa="" id="main-content" data-astro-cid-fur5yujb=""><nav><ol></ol></nav><p>I was working on my homelab and examined a file that was supposed to contain encrypted content that I could safely commit on a Github repository. The file looked like this</p>
<div><figure><pre data-language="json"><code><div><p><span>{</span></p></div><div><p><span>  </span><span>"serial"</span><span>: </span><span>13</span><span>,</span></p></div><div><p><span>  </span><span>"lineage"</span><span>: </span><span>"24d431ee-3da9-4407-b649-b0d2c0ca2d67"</span><span>,</span></p></div><div><p><span>  </span><span>"meta"</span><span>: {</span></p></div><div><p><span>    </span><span>"key_provider.pbkdf2.password_key"</span><span>: </span><span>"eyJzYWx0IjoianpHUlpMVkFOZUZKcEpSeGo4UlhnNDhGZk9vQisrR0YvSG9ubTZzSUY5WT0iLCJpdGVyYXRpb25zIjo2MDAwMDAsImhhc2hfZnVuY3Rpb24iOiJzaGE1MTIiLCJrZXlfbGVuZ3RoIjozMn0="</span></p></div><div><p><span><span>  </span></span><span>},</span></p></div><div><p><span>  </span><span>"encrypted_data"</span><span>: </span><span>"ONXZsJhz37eJA[...]"</span><span>,</span></p></div><div><p><span>  </span><span>"encryption_version"</span><span>: </span><span>"v0"</span></p></div><div><p><span>}</span></p></div></code></pre></figure></div>
<p>Hm, key provider? Password key? In an encrypted file? That doesn’t sound right. The problem is that this file is generated by taking a password, deriving a key from it, and encrypting the content with that key. I don’t know what the derived key could look like, but it could be that long indecipherable string.</p>
<p>I asked a colleague to have a look and he said “Oh that? It looks like a base64 encoded JSON. Give it a go to see what’s inside.”</p>
<p>I was incredulous but gave it a go, and it worked!!</p>
<div><figure><pre data-language="console"><code><div><p><span>$ echo </span><span>"eyJzYW[...]"</span><span> </span><span>|</span><span> </span><span>base64</span><span> </span><span>-d</span></p></div><div><p><span>{"salt":"jzGRZLVANeFJpJRxj8RXg48FfOoB++GF/Honm6sIF9Y=","iterations":600000,"hash_function":"sha512","key_length":32}</span></p></div></code></pre></figure></div>
<p>I couldn’t believe my colleague had decoded the base64 string on the fly, so I asked. “What gave it away? Was it the trailing equal signs at the end for padding? But how did you know it was base64 encoded <em>JSON</em> and not just a base64 string?”</p>
<p>He replied,</p>
<blockquote>
<p>Whenever you see&nbsp;<code>ey</code>, that’s&nbsp;<code>{"</code>&nbsp;and then if it’s followed by a letter, you’ll get&nbsp;<code>J</code>&nbsp;followed by a letter.</p>
</blockquote>
<p>I did a few tests in my terminal, and he was right! You can spot base64 json with your naked eye, and you don’t need to decode it on the fly!</p>
<div><figure><pre data-language="console"><code><div><p><span>$ echo </span><span>"{"</span><span> </span><span>|</span><span> </span><span>base64</span></p></div><div><p><span>ewo=</span></p></div><div><p><span>$ echo </span><span>"{</span><span>\"</span><span>"</span><span> </span><span>|</span><span> </span><span>base64</span></p></div><div><p><span>eyIK</span></p></div><div><p><span>$ echo </span><span>"{</span><span>\"</span><span>s"</span><span> </span><span>|</span><span> </span><span>base64</span></p></div><div><p><span>eyJzCg==</span></p></div><div><p><span>$ echo </span><span>"{</span><span>\"</span><span>a"</span><span> </span><span>|</span><span> </span><span>base64</span></p></div><div><p><span>eyJhCg==</span></p></div><div><p><span>$ echo </span><span>"{</span><span>\"</span><span>word</span><span>\"</span><span>"</span><span> </span><span>|</span><span> </span><span>base64</span></p></div><div><p><span>eyJ3b3JkIgo=</span></p></div></code></pre></figure></div>
<p>But there’s even better! As <a href="https://toot.now/@tyzbit">tyzbit</a> reported on the fediverse, you can even spot base64 encoded certificates and private keys! They all start with <code>LS</code>, which reminds of the LS in “TLS certificate.”</p>
<div><figure><pre data-language="console"><code><div><p><span>$ echo -en </span><span>"-----BEGIN CERTIFICATE-----"</span><span> </span><span>|</span><span> </span><span>base64</span></p></div><div><p><span>LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0t</span></p></div></code></pre></figure></div>
<blockquote><div><p>As pointed out by <a href="https://news.ycombinator.com/item?id=44803260">gnabgib</a> and <a href="https://news.ycombinator.com/item?id=44803259">athorax</a> on Hacker News, this actually detects the leading dashes of the PEM format, commonly used for certificates, and a YAML file that starts with <code>---</code> will yield the same result</p><div><figure><pre data-language="console"><code><div><p><span>$ echo </span><span>"---\n"</span><span> </span><span>|</span><span> </span><span>base64</span></p></div><div><p><span>LS0tXG4K</span></p></div></code></pre></figure></div><p>This is not a silver bullet!</p></div></blockquote>
<p><em>Thanks Davide and Denis for showing me this simple but pretty useful trick, and thanks tyzbit for completing it with certs and private keys!</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ollama Turbo (399 pts)]]></title>
            <link>https://ollama.com/turbo</link>
            <guid>44802414</guid>
            <pubDate>Tue, 05 Aug 2025 18:46:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ollama.com/turbo">https://ollama.com/turbo</a>, See on <a href="https://news.ycombinator.com/item?id=44802414">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
<main>
  <section>
    <h2>
      Turbo<sup>Preview</sup>
    </h2>
    <img src="https://ollama.com/public/turbo.png" alt="Turbo">
    <h2>
      Supercharge models with faster hardware
    </h2>
    <h3>
      $20<span>/mo</span>
    </h3>
    <a href="https://ollama.com/upgrade">Upgrade to Turbo</a>
  </section>
  <section>
    <h2>
      Turbo lets you
    </h2>
    <ul>
      <li>
        <img src="https://ollama.com/public/supercharge.png" alt="Turbo speed">
        <div>
          <h3>Speed up model inference</h3>
          <p>
            Run models using datacenter-grade hardware, returning responses much
            faster.
          </p>
        </div>
      </li>
      <li>
        <img src="https://ollama.com/public/bigger-models.png" alt="Turbo speed">
        <div>
          <h3>Run larger models</h3>
          <p>
            Upgrade to the newest hardware, making it possible to run larger
            models.
          </p>
        </div>
      </li>
      <li>
        <img src="https://ollama.com/public/privacy-first.png" alt="Privacy first">
        <div>
          <h3>Privacy first</h3>
          <p>
            Ollama does not retain your data to ensure privacy and security.
          </p>
        </div>
      </li>
      <li>
        <img src="https://ollama.com/public/battery-life.png" alt="Turbo speed">
        <div>
          <h3>Save battery life</h3>
          <p>
            Take the load of running models off your Mac, Windows or Linux
            computer, giving you performance back for your other apps.
          </p>
        </div>
      </li>
    </ul>
  </section>
  <section>
    <h2>
      Frequently asked questions
    </h2>
    <ul>
      <li>
        <h3>What is Turbo?</h3>
        <p>
          Turbo is a new way to run open models using datacenter-grade hardware.
          Many new models are too large to fit on widely available GPUs, or run
          very slowly. Ollama Turbo provides a way to run these models fast
          while using Ollama's App, CLI, and API.
        </p>
      </li>
      <li>
        <h3>
          Which models are available in Turbo?
        </h3>
        <p>
          While in preview, the <code>gpt-oss-20b</code> and
          <code>gpt-oss-120b</code> models are available.
        </p>
      </li>
      <li>
        <h3>
          Does Turbo work with Ollama's CLI?
        </h3>
        <p>
          Yes, Ollama's CLI works with Turbo mode. See the
          <a href="http://github.com/ollama/ollama/blob/main/docs/turbo.md">docs</a>
          for more information.
        </p>
      </li>
      <li>
        <h3>
          Does Turbo work with Ollama's API and JavaScript/Python libraries?
        </h3>
        <p>
          Yes, Ollama's API and JavaScript/Python libraries work with Turbo
          mode. See the
          <a href="http://github.com/ollama/ollama/blob/main/docs/turbo.md">docs</a>
          for more information.
        </p>
      </li>
      <li>
        <h3>
          What data do you retain in Turbo mode?
        </h3>
        <p>Ollama does not log or retain any queries made via Turbo mode.</p>
      </li>
      <li>
        <h3>
          Where is the hardware that power Turbo located?
        </h3>
        <p>All hardware is located in the United States.</p>
      </li>
      <li>
        <h3>
          What are the usage limits for Turbo?
        </h3>
        <p>
          Turbo includes hourly and daily limits to avoid capacity issues.
          Usage-based pricing will soon be available to consume models in a
          metered fashion.
        </p>
      </li>
    </ul>
  </section>
</main>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US reportedly forcing TSMC to buy 49% stake in Intel to secure tariff relief (429 pts)]]></title>
            <link>https://www.notebookcheck.net/Desperate-measures-to-save-Intel-US-reportedly-forcing-TSMC-to-buy-49-stake-in-Intel-to-secure-tariff-relief-for-Taiwan.1079424.0.html</link>
            <guid>44801486</guid>
            <pubDate>Tue, 05 Aug 2025 17:44:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.notebookcheck.net/Desperate-measures-to-save-Intel-US-reportedly-forcing-TSMC-to-buy-49-stake-in-Intel-to-secure-tariff-relief-for-Taiwan.1079424.0.html">https://www.notebookcheck.net/Desperate-measures-to-save-Intel-US-reportedly-forcing-TSMC-to-buy-49-stake-in-Intel-to-secure-tariff-relief-for-Taiwan.1079424.0.html</a>, See on <a href="https://news.ycombinator.com/item?id=44801486">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="c14039108">
		<header><figure><a href="https://www.notebookcheck.net/fileadmin/_processed_/2/9/csm_US-is-forcing-TSMC-to-invest-in-intel_211a9abb3f.jpg" data-caption="Intel plans to have a new chip plant in Ohio in 2030. (Image source: Brecht Corbeel on Unsplash)"><picture><source srcset="https://www.notebookcheck.net/fileadmin/_processed_/webp/Notebooks/News/_nc5/US-is-forcing-TSMC-to-invest-in-intel-q82-w240-h.webp 1x, https://www.notebookcheck.net/fileadmin/_processed_/webp/Notebooks/News/_nc5/US-is-forcing-TSMC-to-invest-in-intel-q82-w480-h.webp 2x" type="image/webp"><img src="https://www.notebookcheck.net/fileadmin/_processed_/2/9/csm_US-is-forcing-TSMC-to-invest-in-intel_f52eb0a048.jpg" loading="lazy" width="240" height="180" alt="Intel plans to have a new chip plant in Ohio in 2030. (Image source: Brecht Corbeel on Unsplash)"></picture></a><figcaption>Intel plans to have a new chip plant in Ohio in 2030. (Image source: Brecht Corbeel on Unsplash)</figcaption></figure><div><p>A new report out of Taiwan has revealed that the current US administration is tying the reduction on trade of trade tariffs on Taiwan to significant TSMC investment in the US. This investment includes a 49% stake in Intel.</p></div></header>
	</div><div id="c14039107">
	<p>The current US administration, led by President Donald Trump, has employed trade tariffs as the primary way to reduce the US's trade deficit with partner countries. However, the US tariff policy has been erratic and unpredictable. These tariffs have also affected Taiwan, one of the US’s biggest trading partners.</p><p>Currently standing at 20%, the US trade tariff on Taiwan is more than that of countries like Japan, which pays a baseline tariff of 15%. Unsurprisingly, a 20% levy on Taiwanese exports to the US will hurt Taiwanese businesses quite a bit, which is why the country needs to negotiate further with the US to bring the tariffs down or downright eliminate then. However, according to a new report out of Taiwan, US President Trump has presented Taiwan with two conditions for tariff relief, both of which are quite stringent.</p><p><a href="https://www.mnews.tw/story/mm-20250804fin003" target="_blank">Per the Taiwanese outlet mnews.tw</a>, an industry source told the publication that President Donald Trump has mandated TSMC fulfill two conditions if Taiwan is to see any tariff reduction:</p><div><ol><li>Buy a 49% stake in Intel</li><li>Invest a further $400 billion in the US</li></ol></div><p>While ignoring the industrial ramifications of TSMC owning 49% of Intel, these are humongous financial commitments. <a href="https://www.notebookcheck.net/TSMC-s-second-Kumamoto-fab-reportedly-slips-to-2029-as-attention-shifts-to-U-S-expansion.1069608.0.html" target="_self">TSMC is already investing a ton of money in the US</a>, with one of the company’s fabs achieving volume production in 2024. The Taiwanese giant is on its way to building two more fabs at its Arizona facility, alongside an R&amp;D Center and a packaging facility. Overall, TSMC has planned to invest a total of $165 billion in the US.</p><p>So, President Trump’s alleged condition of investing a further $400 billion in the US alongside buying a stake in Intel seems improbable from a purely financial standpoint.</p><p><h2>The US is trying to save Intel from collapse</h2></p><p><a href="https://www.notebookcheck.net/AMD-is-absolutely-hammering-Intel-in-CPU-sales-as-even-old-AM4-chips-outsold-latest-Arrow-Lake-CPUs-last-week.1063493.0.html" target="_self">Intel is on the decline</a>. From the <a href="https://www.notebookcheck.net/Intel-now-reportedly-focusing-on-14A-production-as-18A-proves-to-be-a-billion-dollar-failure.1049302.0.html" target="_self">company’s fab operations</a> to consumer products, virtually every Intel department is taking a hit. This has shown in the company’s reported annual revenue figure, which has come down from a high of $79 billion in 2021 to $53 billion in 2024, is a massive 33% reduction.</p><p>Intel is a vital part of the US’s plans regarding domestic semiconductor manufacturing. As such, <a href="https://techcrunch.com/2025/01/30/intel-has-already-received-2-2b-in-federal-grants-for-chip-production/" target="_blank">the company has already received billions in federal grants</a>. However, this funding has not solved Intel’s troubles, as the company has delayed its Ohio fab from an initial target of 2025 to 2030/31. A primary reason for this delay seems to be Intel’s efforts to preserve capital resources amidst a lack of significant help from the US CHIPS Act and outside partners.</p><div><p>In short, the US administration seems to be pressuring TSMC to acquire a considerable stake in Intel to inject some much-needed capital into the company and keep the US government's plans of a domestic chip supply chain afloat. However, TSMC is unlikely to agree to this proposal.
</p>
<p>It'd be interesting to see what happens to Intel the coming months. Intel has a few exciting products in the works with <a href="https://www.notebookcheck.net/Intel-s-Panther-Lake-could-take-handheld-gaming-to-the-next-level.1072173.0.html" target="_self">Panther Lake</a> and <a href="https://www.notebookcheck.net/Intel-s-Nova-Lake-based-Ryzen-X3D-equivalent-to-launch-with-28-CPU-cores.1066983.0.html" target="_self">Nova Lake CPUs</a>. So, things could improve for Team Blue if these products deliver.</p></div></div><div itemscope="" itemtype="http://schema.org/Person" rel="author"><div><a href="https://www.notebookcheck.net/Notebookcheck-Team.212978.0.html?&amp;tx_nbc2journalist_pi1%5Bmode%5D=show&amp;tx_nbc2journalist_pi1%5Buid%5D=316"><picture><source srcset="https://www.notebookcheck.net/fileadmin/_processed_/6/1/csm_IMG_1224_1_162133c8b0.jpg 1x, https://www.notebookcheck.net/fileadmin/_processed_/6/1/csm_IMG_1224_1_c8771969bf.jpg 2x"><img src="https://www.notebookcheck.net/fileadmin/_processed_/6/1/csm_IMG_1224_1_162133c8b0.jpg" loading="lazy" width="120" height="120" alt="Fawad Murtaza"></picture></a></div><p><a href="https://www.notebookcheck.net/Notebookcheck-Team.212978.0.html?&amp;tx_nbc2journalist_pi1%5Bmode%5D=show&amp;tx_nbc2journalist_pi1%5Buid%5D=316">Fawad Murtaza</a> - Senior Tech Writer <span title="1302&nbsp;"> - 1302 articles published on Notebookcheck</span> since 2021</p><p>I am Fawad, a fellow tech nerd. As a tech junkie, my relationship with technology goes back to my childhood years. Getting my first Intel Pentium 4 PC was the start of journey that would eventually bring me to Notebookcheck. Finally, I have been writing for tech media since 2018. From small no-name projects to industry leaders, I have worked with a number of tech publications.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Whittle – A shrinking word game (113 pts)]]></title>
            <link>https://playwhittle.com/</link>
            <guid>44801399</guid>
            <pubDate>Tue, 05 Aug 2025 17:39:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://playwhittle.com/">https://playwhittle.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44801399">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Consider using Zstandard and/or LZ4 instead of Deflate (184 pts)]]></title>
            <link>https://github.com/w3c/png/issues/39</link>
            <guid>44801027</guid>
            <pubDate>Tue, 05 Aug 2025 17:18:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/w3c/png/issues/39">https://github.com/w3c/png/issues/39</a>, See on <a href="https://news.ycombinator.com/item?id=44801027">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><p dir="auto">One of the issues we have with .PNG is slow read/write times. There are now new lossless open source codecs without patent concerns, such as Zstandard (maintained by Facebook) or LZ4:</p>
<p dir="auto"><a href="https://facebook.github.io/zstd/" rel="nofollow">https://facebook.github.io/zstd/</a><br>
<a href="https://github.com/lz4/lz4">https://github.com/lz4/lz4</a></p>
<p dir="auto">Zstandard is used by the new Khronos KTX2 GPU texture format specification. I propose that it be added as an option to a future version of .PNG. The possible speedups are quite significant, and for users that read and write a lot of .PNG's as part of their data processing pipelines the speedups will be high value improvements.</p>
<p dir="auto">There are also other far simpler but even faster codecs being developed, such as .QOI's, but using this would likely require changing or not filtering the image before compression:<br>
<a href="https://news.ycombinator.com/item?id=29328750" rel="nofollow">https://news.ycombinator.com/item?id=29328750</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI Open Models (1960 pts)]]></title>
            <link>https://openai.com/open-models/</link>
            <guid>44800746</guid>
            <pubDate>Tue, 05 Aug 2025 17:02:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/open-models/">https://openai.com/open-models/</a>, See on <a href="https://news.ycombinator.com/item?id=44800746">Hacker News</a></p>
Couldn't get https://openai.com/open-models/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Introducing gpt-oss (171 pts)]]></title>
            <link>https://openai.com/index/introducing-gpt-oss/</link>
            <guid>44800730</guid>
            <pubDate>Tue, 05 Aug 2025 17:00:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-gpt-oss/">https://openai.com/index/introducing-gpt-oss/</a>, See on <a href="https://news.ycombinator.com/item?id=44800730">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-gpt-oss/: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>