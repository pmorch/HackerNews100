<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 12 Mar 2024 14:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[I no longer maintain my Emacs projects on Sourcehut (114 pts)]]></title>
            <link>https://protesilaos.com/codelog/2024-01-27-sourcehut-no-more/</link>
            <guid>39678491</guid>
            <pubDate>Tue, 12 Mar 2024 12:00:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://protesilaos.com/codelog/2024-01-27-sourcehut-no-more/">https://protesilaos.com/codelog/2024-01-27-sourcehut-no-more/</a>, See on <a href="https://news.ycombinator.com/item?id=39678491">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
          üèÜ I provide private lessons on Emacs, Linux, and Life in
          general: <a href="https://protesilaos.com/coach/">https://protesilaos.com/coach/</a>.
          Lessons continue <strong>throughout the year</strong>.
        </p><article>

	

	<div>
		<p>On 2022-04-07 I moved all my Emacs projects from GitLab to SourceHut:
<a href="https://protesilaos.com/codelog/2022-04-07-all-emacs-projects-sourcehut/">https://protesilaos.com/codelog/2022-04-07-all-emacs-projects-sourcehut/</a>.
I am now undoing that decision. My code is on GitLab and GitHub, with
the latter as the de facto primary source.</p>

<p>Why the change:</p>

<ul>
  <li>The SourceHut web interface does not show any kind of indication
that a message has an attachment. Last time I tried it, I had to
download an mbox file and extract the patch from there. This was
helped by the fact that I knew what I was searching for, but the
experience was not pleasant anyway.</li>
  <li>Many users are reluctant to subscribe to my project‚Äôs mailing list
and instead email me directly. This is fine, as I do get the work
done eventually, but it beats the point of having a public inbox if
I am the only one who reads those messages anyway.</li>
  <li>Personal emails for package maintenance make it more difficult for
me to apply filters. I cannot easily go from ‚Äúpersonal‚Äù to
‚Äúpackages‚Äù and thus have trouble prioritising tasks.</li>
  <li>Users replying to mailing list threads frequently do not ‚Äúreply to
all‚Äù, so the filter I have for SourceHut lists does not apply and my
inbox is noisy again.</li>
  <li>Coordinating work between my ‚Äúofficial‚Äù SourceHut-based repository
and the de facto GitHub one is a problem, because a user on one may
not be aware of what a user on the other is doing.</li>
</ul>

<p>There are other papercuts as well, but I understand SourceHut is still
in ‚Äúalpha‚Äù, so I will not list them here.</p>

<h2>What happens to the Git repositories on SourceHut?</h2>

<p>I am no longer updating them and plan to delete them in the near
future.</p>

<h2>What about the mailing lists?</h2>

<p>I will continue to reply to messages sent there, but I will eventually
ask people to use other media. If you do not want to use a Git forge
to report an issue or send a patch, then do it via my personal email:
<a href="https://protesilaos.com/contact">https://protesilaos.com/contact</a>.</p>

<h2>Will you host your own Git server?</h2>

<p>I want to do this at some point because I am mindful of the issues
with proprietary Git forges. But this is a task that requires
knowledge and resources. So it will not happen soon.</p>

	</div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The internet is slipping out of our reach (102 pts)]]></title>
            <link>https://injuly.in/blog/darker-internet/index.html</link>
            <guid>39677677</guid>
            <pubDate>Tue, 12 Mar 2024 09:43:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://injuly.in/blog/darker-internet/index.html">https://injuly.in/blog/darker-internet/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=39677677">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
			<hr>

      
      <p>Do you often append "reddit" or "wiki" to your Google search queries? <br>
Seek out human discourse on specific websites,
when search engines were invented to do it for you?
Worry not, for soon search engines will be useless.</p>
<p>If you searched Google for a movie review in 2005, you'd get a review.
Today, you get paid search results, ads, third-party cookies,
trackers, newsletter prompts, notification requests, and,
if you're lucky ‚Äì a movie review.
No matter what you search for, SEO-hacked content farms will blot out the sun.<sup><a href="#fn-1" id="fnref-1">1</a></sup></p>
<p>The average essay is written not to be read, but to be found‚Äîby a search engine,
that is‚Äîso it can monetize every fiber of your eyeballs.
Keep you scrolling, clicking, and sharing.
Like this article? Maybe you'll like this e-book too.
It's free! (Just enter your email ;)).</p>
<p>The internet continues to grow in volume, but shrink in diversity.
As if everything has moved to a few select apps,
which then <a href="https://www.theverge.com/2024/2/17/24075670/reddit-ai-training-license-deal-user-content">sell our words</a> to AI slot machines.
Images on Instagram, videos on YouTube, and discussions on Reddit.
Dare to venture out of this bubble,
and you'll be pelted with SEO spam by content farms squabbling over search ranks and impressions.</p>
<p>But do you know what's worse than SEO spam?</p>
<h2 id="ai-generated-seo-spam-">AI generated SEO spam. <br></h2>
<p>With services that <a href="https://injuly.in/assets/img/darker-internet/seo-heist.webp">generate thousands of articles</a>,
post them on multiple websites,
then create fake profiles to market them<sup><a href="#fn-2" id="fnref-2">2</a></sup>;
the phrase "As an AI language model" now turns up on LinkedIn, Amazon, Twitter,
and even Google scholar.
In a matter of weeks, LLMs can generate more text than has ever been written in human history.
Every passing day, humans grow farther apart, and bots fill the void in between.</p>
<p>This steady decline hearkens me back to my early teenage years ‚Äì
when I was an active user of several RTS video-game forums.
At the time, the internet was smaller and fostered organic growth.
Smaller websites were easier to find, game mods and fanfics weren't subreddits, and personal pages didn't live on Substack or Medium<sup><a href="#fn-3" id="fnref-3">3</a></sup>.</p>
<p>When I'm older, I expect to surf an internet that will have exploded with AI content,
like the big bang leaving <a href="https://en.wikipedia.org/wiki/Dark_forest_hypothesis">a dark forest</a> behind<sup><a href="#fn-4" id="fnref-4">4</a></sup>.
Online TV shows and webtoons with unending episodes generated on the fly,
digital journals overflowing with articles, and chat apps with AI companions to ease loneliness.
Human interaction will stay burrowed underground, in the tight confines of small and heavily moderated groups.</p>
<p>For the past few years, I've been holed up in small discord servers ‚Äì
the only place where I expect to freely interact with meaningful messages that aren't marketing posts in disguise.
And if you too have been hiding in telegram groups and mailing lists  ‚Äì good, don't leave.
It's dark outside.</p>




			
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Bevy Foundation (123 pts)]]></title>
            <link>https://bevyengine.org/news/bevy-foundation/</link>
            <guid>39677009</guid>
            <pubDate>Tue, 12 Mar 2024 07:31:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bevyengine.org/news/bevy-foundation/">https://bevyengine.org/news/bevy-foundation/</a>, See on <a href="https://news.ycombinator.com/item?id=39677009">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It is with excitement that I unveil to the world ...</p>
<p><img src="https://bevyengine.org/news/bevy-foundation/bevy_foundation.svg" alt="Bevy Foundation Logo"></p><p>The <a href="https://bevyengine.org/foundation"><strong>Bevy Foundation</strong></a> is the next step in our journey to build a world class free and open source game engine. <strong>Bevy Foundation</strong> is a non-profit organization formed in Washington State, with a pending federal 501(c)(3) tax-exemption application (more on this later).</p>
<p>This is a big step for us and it was not taken lightly. This will be a long blog post, so I'll cover the highlights here:</p>
<ol>
<li><strong>Bevy Foundation</strong> is largely a legal formalization of the leadership and operational structure we have already been using. The "Bevy leadership" that you know (and hopefully trust) is also at the helm of <strong>Bevy Foundation</strong>. The biggest difference is that power is distributed more evenly.</li>
<li>Nothing is fundamentally changing about how we build and release Bevy as free and open source software. This is <em>not</em> a business model rug-pull. It is a legal formalization of our mission to build and release Bevy as free and open source software to members of the general public (you!).</li>
<li>You can now <a href="https://bevyengine.org/donate">donate to the Bevy Foundation</a> to support us!</li>
</ol>
<h2 id="some-history">Some History
<a href="#some-history">#</a>
</h2>
<p>We have come such a long way since I first released Bevy to the world in 2020. What was once a one person side project is now built by hundreds of people and used by thousands (we recently broke a million downloads!). Bevy is the most popular, most used game engine built in Rust, and the second most popular open source game engine on GitHub.</p>
<p>I (Carter / @cart) am still Bevy's Project Lead, but I no longer make all of the decisions. For years now <a href="https://bevyengine.org/community/people">The Bevy Organization</a> has delegated decision making authority across members of the Bevy community. We have 5 excellent Maintainers and even more Subject Matter Experts that were each pivotal in making Bevy what it is today.</p>
<p>As we have grown, we have accumulated what I will call "organizational debt":</p>
<ul>
<li><strong>Funding Bevy Has Been A Popularity Contest</strong>: For years, our donate button linked directly to my GitHub sponsors page. This was acceptable early on when I was the only one spending significant time on the project, but that is no longer the case. Last year, we replaced that with a page that any Bevy Organization member could list themselves on. While fairer than the "everything goes to @cart" model, it resulted in funding being a "popularity contest". And unfortunately, given how attached my name is to the project, in practice I still received the majority of the sponsorships. Presenting donors with the task of picking someone to back also deeply complicated the donation process, which likely turned some donors off. We need a simple, centralized donation model where we can direct funds fairly to the areas of the project that need them the most.</li>
<li><strong>We Lacked "Organizational Legitimacy"</strong>: We would like to attract the attention (and funding) of people and companies serious about gamedev. An informal collective of Bevy developers (each asking for support individually) lacks the legitimacy required to attract some entities. We need to be able to present a centralized and transparent view of our operations, with accountable and professional people at the helm.</li>
<li><strong>Carter Held (And Owned) The Keys</strong>: Up until this point, I have owned Bevy-related intellectual property such as the copyright for the Bevy logo. I also owned the <code>bevyengine.org</code> domain name and had exclusive administrative control over most of our infrastructure and communities. I have had absolute authority over all aspects of the project since its inception. Yes, I have delegated, but it all stemmed from me at the root. This had its efficiencies, but it is no longer sustainable for me or ethically sound when there are <em>so many</em> others that deserve actual <em>legal</em> say over how the project is run. There is also the <a href="https://en.wikipedia.org/wiki/Bus_factor">"bus factor"</a> to consider. If something were to happen to me, some aspects of the Bevy project would be lost forever.</li>
</ul>
<p>Bevy needs a structure that lets us work together: a legal entity that embodies our goals and principles, where responsibility and decision making are all shared.</p>
<h2 id="introducing-the-bevy-foundation">Introducing the Bevy Foundation!
<a href="#introducing-the-bevy-foundation">#</a>
</h2>
<h3 id="our-mission">Our Mission
<a href="#our-mission">#</a>
</h3>
<p>To quote our 501(c)(3) application:</p>
<blockquote>
<p>Our mission is to promote, protect, and advance the free and open source Bevy Engine and related open source projects. We coordinate and promote its continued maintenance and development, educate and train members of the general public in its usage, and conduct research and development to advance the state of the art of creating real-time applications and simulations.</p>
</blockquote>
<p>In short: <strong>we exist to develop Bevy and teach people how to use it</strong>!</p>
<h3 id="legal-status">Legal Status
<a href="#legal-status">#</a>
</h3>
<p><a href="https://bevyengine.org/foundation"><strong>Bevy Foundation</strong></a> is a non-profit incorporated in Washington State. This means that the money we raise <a href="https://dor.wa.gov/education/industry-guides/nonprofit-organizations#Fundraising">cannot be used to benefit our members, officers, or directors, except as compensation for services rendered</a>. We do not have "owners" or "shareholders". We are formed exclusively to accomplish our mission as stated above as a public benefit.</p>
<h3 id="federal-501-c-3-application">Federal 501(c)(3) Application
<a href="#federal-501-c-3-application">#</a>
</h3>
<p>We have also applied for a federal 501(c)(3) non-profit public charity designation.</p>
<p>If our 501(c)(3) application is accepted, this status will exempt us from federal income tax and allow donors in the United States to deduct their donations on their federal taxes.</p>
<p>Note that the 501(c)(3) application has not yet been approved, and there has been an increasing number of denials for technology-oriented charities. The experts we have talked to have told us not to get our hopes up.</p>
<p>If we are denied, our default strategy will be to continue operating as a state-only charity and pay federal taxes. As long as we spend most of the donations we receive, the taxes shouldn't be too painful. We may also consider other designations, such as 501(c)(6), but we are cautious about that designation. We want to focus on the members of the Bevy community as people, not through the lens of their commercial interests.</p>
<p>If tax-deductibility is important to you, <em>please do not donate until we receive a determination</em>. But also note that if our application is accepted, donations made <em>prior</em> to our acceptance will <em>retroactively</em> be tax-deductible! Neat!</p>
<h3 id="the-board-of-directors">The Board of Directors
<a href="#the-board-of-directors">#</a>
</h3>
<p>We are run by the <strong>Bevy Foundation</strong> Board of Directors. All actions taken by <strong>Bevy Foundation</strong> are determined by a vote of the board.</p>
<p>The current Board of Directors is: Carter Anderson (@cart), Alice Cecile (@alice-i-cecile), Fran√ßois Mockers (@mockersf), Robert Swain (@superdump), and James Liu (@james7132).</p>
<p>I have been elected President of the board and Alice has been elected Secretary. For logistical reasons, I have also been elected interim Treasurer to get our initial financial situation spun up. It makes sense for a US-based board member to set up things like bank accounts for a US-based organization. But we would like to pass off the role of Treasurer as quickly as possible.</p>
<p>You may notice that every member of the board is an active Bevy Maintainer. This is not a coincidence. We believe that people <em>actually developing the project</em> should be at the helm. There is currently no way to buy a board seat. We have no plans to hire "professional board members" that aren't in the thick of it. For now we plan to stick to the Maintainer == Board Member policy. Functionally, new Board Members are selected via an election of the current board. "Board Member" is currently not a paid position.</p>
<p>Note that I am using the phrasing "currently" and "for now" in some places above because I am <em>no longer capable of making absolute statements about the future of the project</em>. I am a board member, which gives me a vote. And as President I have been given some authority by the board to manage our operations. But I must follow the resolutions of the board and I <em>can</em> be voted out. This is a good thing I promise! It means that the project can exist without me. And if I am no longer fulfilling my duties to the satisfaction of the board, I probably <em>should</em> be ousted.</p>
<p>This is a leap of faith, but one that has been carefully measured over the course of years. I have worked with the other maintainers (now board members) for a long time now and I trust them ... both with the future of Bevy and with my job.</p>
<h3 id="bylaws">Bylaws
<a href="#bylaws">#</a>
</h3>
<p>We have adopted public <a href="https://bevyengine.org/foundation/bylaws">Bylaws</a> that govern how we operate. They determine how we are structured, the roles in the organization, how meetings and voting take place, a conflict of interest policy, and more. Note these are largely borrowed from the standard legal boilerplate (and they exist for a legal purpose), so they're stiffer than our normal style.</p>
<h3 id="transparency">Transparency
<a href="#transparency">#</a>
</h3>
<p>Our goal is to make the operations of <strong>Bevy Foundation</strong> as transparent as possible. We make the following public:</p>
<ul>
<li><a href="https://bevyengine.org/foundation/bylaws"><strong>Bylaws</strong></a>: The operational rules we have adopted</li>
<li><a href="https://bevyengine.org/foundation/meeting-minutes"><strong>Meeting Minutes</strong></a>: Learn about the decisions the board makes and why we make them</li>
<li><a href="https://drive.google.com/drive/u/0/folders/1Q_vdAaI0tsWZdcddL9lDicDZhjPl79s9?ths=true"><strong>Documents</strong></a>:  Public Bevy Foundation documents, such as our Articles of Incorporation</li>
<li><a href="https://bevyengine.org/foundation/budget"><strong>Budget</strong></a>: How we plan to spend money this year</li>
</ul>
<p>We will strive to be approachable and accountable for our actions, and we will continue to make our technical decisions in public with the input of the community. You know who we are and where to find us!</p>
<h2 id="our-plans">Our Plans
<a href="#our-plans">#</a>
</h2>
<p><strong>Bevy Foundation</strong> will direct the majority of its funding to fueling Bevy development.</p>
<h3 id="short-term-bring-alice-on-full-time">Short Term: Bring Alice on full-time
<a href="#short-term-bring-alice-on-full-time">#</a>
</h3>
<p>We have exactly one focus for the short term: bring Alice on full-time. For years, Alice has dedicated <em>significant</em> time to the project as a Maintainer. She has been our primary project manager: wrangling issues, <a href="https://mastodon.social/deck/tags/bevymergetrain">keeping the merge train running</a>, connecting developers to the right parts of the project, and doing the work that ensures we can harness and enable our ever increasing developer community. Not to mention her extensive technical contributions and documentation work!</p>
<p>If you have participated in Bevy's development, you are probably intimately aware of two things:</p>
<ol>
<li>Project management is our bottleneck.</li>
<li>Alice is a world-class project manager.</li>
</ol>
<p>In addition to performing the general "project management" role, Alice has the following specific priorities when she starts full-time:</p>
<ol>
<li><strong>The New Bevy Book</strong>: The new Bevy Book aims to be a complete, always up-to-date, and constantly improving guide to using Bevy. Alice has already contributed content and helped facilitate the book's production. If she joins full-time, she will work to finish it, in addition to helping others contribute.</li>
<li><strong>Coordinating the Development of ECS Relations</strong>: Relations are an ECS feature that enable connecting entities to each other via special components called Relations. These are an often-requested feature that will make a number of scenarios (such as parent-child relationships) much nicer. Alice will work to coordinate this effort and fill in functionality gaps.</li>
<li><strong>Building an Action System for Input Devices</strong>: Bevy sorely needs an official way to define "input actions", which are then mapped (and remapped) to inputs from one or more input devices (for example: a <code>Jump</code> action that is mapped to the "A" button on controllers and "Spacebar" on keyboards). Alice built <a href="https://github.com/leafwing-studios/leafwing-input-manager"><code>leafwing-input-manager</code></a>, which is a popular third-party Bevy plugin that does exactly that. Alice plans on taking the lessons learned there to build a proper first-party solution to this problem.</li>
</ol>
<p>Alice deserves to be paid for her work, and Bevy needs her full-time. Our goal for the <strong>Bevy Foundation</strong> is to pay reasonably competitive, roughly market rate salaries. Therefore, our first goal is to pay Alice a salary of $150,000 a year.</p>
<p>We all believe Alice is worth at least that much, but given that funds will take time to build up, Alice has agreed to take a pay cut while things spin up.</p>
<h4 id="what-about-carter">What about Carter?
<a href="#what-about-carter">#</a>
</h4>
<p>Some may ask why I (Carter), the creator and Project Lead of Bevy, am not the first to be paid by the <strong>Bevy Foundation</strong>. Thats easy: I already currently have enough support from my current sponsors to live on, and I have historically received the lion's share of sponsorships. The scales have been tilted in my favor for too long. Prioritizing Alice is the only fair choice, and also happens to be what is best for Bevy right now.</p>
<p>I suspect my current sponsors will transition to the <strong>Bevy Foundation</strong> over time (and I encourage them to do so!). As this happens there will likely be an inflection point where Alice is making more than me. When we hit that point, we will sort out a balancing strategy until we are both paid our target wages.</p>
<p>Note if we receive 501(c)(3) status (see the 501(c)(3) section above), we are allowed to pay competitive wages, but we can't pay <em>excessive</em> wages (nor are we <em>interested in doing so</em>). Once Alice and Carter are paid reasonable wages, we will shift our financial focus elsewhere.</p>
<h3 id="future-plans">Future Plans
<a href="#future-plans">#</a>
</h3>
<p>Our focus will always be on funding more Bevy developers, both full-time and part-time. We will likely explore targeted one time grants for specific efforts.</p>
<p>Later down the line, we will likely explore the development of a Bevy Asset store where the community can list and sell Bevy-compatible assets.</p>
<p>We would also like to re-introduce a Bevy Merch store. While we don't think this is an ideal fundraising strategy, we think it will be a fun way for the Bevy community to show their pride!</p>
<p>When we consider future programs, we will ask the questions:</p>
<ol>
<li>Is this compatible with the Bevy Foundation's mission?</li>
<li>Is this in the best interest of the Bevy community and the general public?</li>
<li>Will this compromise the integrity of Bevy as a free and open source offering in any way?</li>
<li>Will this change our incentives in a way that risks changing our answers to (1), (2), or (3)?</li>
<li>If we are monetizing something, are we doing it in an ethical way?</li>
</ol>
<p>Know that we exist <em>for the benefit of the Bevy community and the general public</em> and we will try our utmost to never compromise that.</p>
<h2 id="a-new-era">A New Era
<a href="#a-new-era">#</a>
</h2>
<p>The <strong>Bevy Foundation</strong> is both an accomplishment, as the culmination of months of work and research, and a Foundation, on top of which we will build the future of Bevy. We are building technology that will enrich the lives of everyone as a common, publicly available good. Game development is currently an industry of rent seekers and gatekeepers. I believe that a better world can exist: one where we collectively build tools <em>for each other</em> in the open. Bevy is ideologically and technologically a rethinking of what this industry should be.</p>
<p>I am deeply proud of what we have accomplished so far and I can't wait for what this new Bevy era will bring.</p>
<h2 id="we-need-your-support">We Need Your Support!
<a href="#we-need-your-support">#</a>
</h2>
<p>Bevy will always be free and open source. However our plans for Bevy's future are grand ... they will require significant financial support. Please consider donating if you enjoy using Bevy and believe in our mission.</p>
<p>If we receive 501(c)(3) public charity status, to maintain that status we will require a significant portion of our funds to come from individual people (not just companies). Your contribution matters!</p>
<p>If you are already supporting us through individual sponsorships, consider switching your donation to <strong>Bevy Foundation</strong>, as that will make it easier for us to direct your contribution to the areas that need it most.</p>
<p>Visit our brand new donation page here:</p>
<p><a href="https://bevyengine.org/donate">Donate <img src="https://bevyengine.org/assets/heart.svg" alt="heart icon"></a></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloning a Laptop over NVMe TCP (266 pts)]]></title>
            <link>https://copyninja.in/blog/clone_laptop_nvmet.html</link>
            <guid>39676767</guid>
            <pubDate>Tue, 12 Mar 2024 06:37:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://copyninja.in/blog/clone_laptop_nvmet.html">https://copyninja.in/blog/clone_laptop_nvmet.html</a>, See on <a href="https://news.ycombinator.com/item?id=39676767">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>Recently, I got a new laptop and had to set it up so I could start using it. But I wasn't really in the mood to go through the same old steps which I had explained in this <a href="https://copyninja.in/blog/live_install_debian.html">post earlier</a>. I was complaining about this to my colleague, and there came the suggestion of why not copy the entire disk to the new laptop. Though it sounded like an interesting idea to me, I had my doubts, so here is what I told him in return.</p><ol><li>I don't have the tools to open my old laptop and connect the new disk over USB to my new laptop.</li><li>I use full disk encryption, and my old laptop has a 512GB disk, whereas the new laptop has a 1TB NVME, and I'm not so familiar with resizing LUKS.</li></ol><p>He promptly suggested both could be done. For step 1, just expose the disk using NVME over TCP and connect it over the network and do a full disk copy, and the rest is pretty simple to achieve. In short, he suggested the following:</p><ol><li>Export the disk using nvmet-tcp from the old laptop.</li><li>Do a disk copy to the new laptop.</li><li>Resize the partition to use the full 1TB.</li><li>Resize LUKS.</li><li>Finally, resize the BTRFS root disk.</li></ol><div id="exporting-disk-over-nvme-tcp"><h2>Exporting Disk over NVME TCP</h2><p>The easiest way suggested by my colleague to do this is using <a href="https://www.freedesktop.org/software/systemd/man/latest/systemd-storagetm.service.html">systemd-storagetm.service</a>. This service can be invoked by simply booting into <em>storage-target-mode.target</em> by specifying <em>rd.systemd.unit=storage-target-mode.target</em>. But he suggested not to use this as I need to tweak the dracut initrd image to involve network services as well as configuring WiFi from this mode is a painful thing to do.</p><p>So alternatively, I simply booted both my laptops with GRML rescue CD. And the following step was done to export the NVME disk on my current laptop using the nvmet-tcp module of Linux:</p><div><pre><span></span>modprobe<span> </span>nvemt-tcp
<span>cd</span><span> </span>/sys/kernel/config/nvmet
mkdir<span> </span>ports/0
<span>cd</span><span> </span>ports/0
<span>echo</span><span> </span><span>"ipv4"</span><span> </span>&gt;<span> </span>addr_adrfam
<span>echo</span><span> </span><span>0</span>.0.0.0<span> </span>&gt;<span> </span>addr_traaddr
<span>echo</span><span> </span><span>4420</span><span> </span>&gt;<span> </span>addr_trsvcid
<span>echo</span><span> </span>tcp<span> </span>&gt;<span> </span>addr_trtype

<span>cd</span><span> </span>/sys/kernel/config/nvmet/subsystems
mkdir<span> </span>testnqn
<span>echo</span><span> </span><span>1</span><span> </span>&gt;testnqn/allow_any_host
mkdir<span> </span>testnqn/namespaces/1

<span>cd</span><span> </span>testnqn
<span># replace the device name with the disk you want to export</span>
<span>echo</span><span> </span><span>"/dev/nvme0n1"</span><span> </span>&gt;<span> </span>namespaces/1/device_path
<span>echo</span><span> </span><span>1</span><span> </span>&gt;<span> </span>namespaces/1/enable

ln<span> </span>-s<span> </span><span>"../../subsystems/testnqn"</span><span> </span>/sys/kernel/config/nvmet/ports/0/subsystems/testnqn
</pre></div><p>These steps ensure that the device is now exported using NVME over TCP. The next step is to detect this on the new laptop and connect the device:</p><div><pre><span></span>nvme<span> </span>discover<span> </span>-t<span> </span>tcp<span> </span>-a<span> </span>&lt;ip&gt;<span> </span>-s<span> </span><span>4420</span>
nvme<span> </span>connectl-all<span> </span>-t<span> </span>tcp<span> </span>-a<span> </span>&lt;&gt;<span> </span>-s<span> </span><span>4420</span>
</pre></div><p>Finally, <tt>nvme list</tt> shows the device which is connected to the new laptop, and we can proceed with the next step, which is to do the disk copy.</p></div><div id="copying-the-disk"><h2>Copying the Disk</h2><p>I simply used the <tt>dd</tt> command to copy the root disk to my new laptop. Since the new laptop didn't have an Ethernet port, I had to rely only on WiFi, and it took about 7 and a half hours to copy the entire 512GB to the new laptop. The speed at which I was copying was about 18-20MB/s. The other option would have been to create an initial partition and file system and do an rsync of the root disk or use BTRFS itself for file system transfer.</p><div><pre><span></span>dd<span> </span><span>if</span><span>=</span>/dev/nvme2n1<span> </span><span>of</span><span>=</span>/dev/nvme0n1<span> </span><span>status</span><span>=</span>progress<span> </span><span>bs</span><span>=</span>40M
</pre></div></div><div id="resizing-partition-and-luks-container"><h2>Resizing Partition and LUKS Container</h2><p>The final part was very easy. When I launched <tt>parted</tt>, it detected that the partition table does not match the disk size and asked if it can fix it, and I said yes. Next, I had to install <tt><span>cloud-guest-utils</span></tt> to get <tt>growpart</tt> to fix the second partition, and the following command extended the partition to the full 1TB:</p><p>Next, I used <tt><span>cryptsetup-resize</span></tt> to increase the LUKS container size.</p><div><pre><span></span>cryptsetup<span> </span>luksOpen<span> </span>/dev/nvme0n1p2<span> </span>ENC
cryptsetup<span> </span>resize<span> </span>ENC
</pre></div><p>Finally, I rebooted into the disk, and everything worked fine. After logging into the system, I resized the BTRFS file system. BTRFS requires the system to be mounted for resize, so I could not attempt it in live boot.</p><div><pre><span></span>btfs<span> </span>fielsystem<span> </span>resize<span> </span>max<span> </span>/
</pre></div></div><div id="conclussion"><h2>Conclussion</h2><p>The only benefit of this entire process is that I have a new laptop, but I still feel like I'm using my existing laptop. Typically, setting up a new laptop takes about a week or two to completely get adjusted, but in this case, that entire time is saved.</p><p>An added benefit is that I learned how to export disks using NVME over TCP, thanks to my colleague. This new knowledge adds to the value of the experience.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a free animator. Think Adobe Illustrator but for animation (368 pts)]]></title>
            <link>https://www.trangram.com</link>
            <guid>39675807</guid>
            <pubDate>Tue, 12 Mar 2024 03:15:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.trangram.com">https://www.trangram.com</a>, See on <a href="https://news.ycombinator.com/item?id=39675807">Hacker News</a></p>
<div id="readability-page-1" class="page"><!--nghm-->
    <app-root _nghost-ng-c3984273398="" ng-version="17.2.4" ngh="12" ng-server-context="ssr"><router-outlet _ngcontent-ng-c3984273398=""></router-outlet><app-home _nghost-ng-c1665951462="" ngh="11"><mat-sidenav-container _ngcontent-ng-c1665951462="" ngh="0"><!----><mat-sidenav _ngcontent-ng-c1665951462="" tabindex="-1" role="navigation" mode="over" opened="false" ngh="1"><div cdkscrollable=""><div _ngcontent-ng-c1665951462=""><a _ngcontent-ng-c1665951462="" routerlink="/" href="https://www.trangram.com/"><p><img _ngcontent-ng-c1665951462="" src="https://www.trangram.com/assets/logo.png" alt="Trangram" title="Trangram"></p><p>Trangram</p></a></div><mat-nav-list _ngcontent-ng-c1665951462="" role="navigation" aria-disabled="false" ngh="1"><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/" href="https://www.trangram.com/" aria-disabled="false" ngh="3"><span><span> Home </span></span></a><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/explore" href="https://www.trangram.com/explore" aria-disabled="false" ngh="3"><span><span> Explore </span></span></a><mat-divider _ngcontent-ng-c1665951462="" role="separator" aria-orientation="horizontal" ngh="1"></mat-divider><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/auth/login" href="https://www.trangram.com/auth/login" aria-disabled="false" ngh="3"><span><span> Log In </span></span></a><!----><!----><mat-divider _ngcontent-ng-c1665951462="" role="separator" aria-orientation="horizontal" ngh="1"></mat-divider><div _ngcontent-ng-c1665951462=""><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/about" href="https://www.trangram.com/about" aria-disabled="false" ngh="4"><span><span> About </span></span></a><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/guidelines" href="https://www.trangram.com/guidelines" aria-disabled="false" ngh="4"><span><span> Guidelines </span></span></a><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/privacy" href="https://www.trangram.com/privacy" aria-disabled="false" ngh="4"><span><span> Privacy Policy </span></span></a><a _ngcontent-ng-c1665951462="" mat-list-item="" routerlink="/terms" href="https://www.trangram.com/terms" aria-disabled="false" ngh="4"><span><span> Terms of Use </span></span></a></div></mat-nav-list></div></mat-sidenav><mat-sidenav-content _ngcontent-ng-c1665951462="" ngh="1"><div _ngcontent-ng-c1665951462=""><div _ngcontent-ng-c1665951462=""><a _ngcontent-ng-c1665951462="" routerlink="/" href="https://www.trangram.com/"><p><img _ngcontent-ng-c1665951462="" src="https://www.trangram.com/assets/logo.png" title="Trangram"></p><p>Trangram</p></a></div><!----><!----><app-search-bar _ngcontent-ng-c1665951462="" _nghost-ng-c1314027463="" ngh="6"><mat-form-field _ngcontent-ng-c1314027463="" appearance="outline" ngh="5"><!----></mat-form-field></app-search-bar><app-account-menu _ngcontent-ng-c1665951462="" _nghost-ng-c1621978163="" ngh="9"><div _ngcontent-ng-c1621978163=""><p><a _ngcontent-ng-c1621978163="" mat-flat-button="" routerlink="/auth/login" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/auth/login" aria-disabled="false" ngh="7"><span></span><span>Log in</span><span></span><span></span></a><a _ngcontent-ng-c1621978163="" mat-flat-button="" color="primary" routerlink="/auth/signup" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/auth/signup" aria-disabled="false" ngh="7"><span></span><span>Sign up</span><span></span><span></span></a></p><a _ngcontent-ng-c1621978163="" mat-icon-button="" routerlink="/auth/login" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/auth/login" aria-disabled="false" ngh="8"><span></span><span></span><span></span><span></span></a></div><!----><!----><!----><!----></app-account-menu></div><router-outlet _ngcontent-ng-c1665951462=""></router-outlet><app-landing _nghost-ng-c721036369="" ngh="10"><div _ngcontent-ng-c721036369=""><div _ngcontent-ng-c721036369=""><p>Create Motion Graphics</p><p>On One Platform</p></div><p> Create ‚Ä¢ Animate ‚Ä¢ Share </p><div _ngcontent-ng-c721036369=""><p><a _ngcontent-ng-c721036369="" mat-raised-button="" routerlink="/editor" color="primary" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/editor" aria-disabled="false" ngh="7"><span></span><span> Open Editor </span><span></span><span></span></a><a _ngcontent-ng-c721036369="" mat-raised-button="" routerlink="/about" mat-ripple-loader-uninitialized="" mat-ripple-loader-class-name="mat-mdc-button-ripple" href="https://www.trangram.com/about" aria-disabled="false" ngh="7"><span></span><span>Features</span><span></span><span></span></a></p></div></div><svg _ngcontent-ng-c721036369="" viewBox="70 0 400 90"><path _ngcontent-ng-c721036369="" d="M0,65c198,81.64723 325.5,-117.13249 510.5,23.86751c0,0 -55,-64.5 -181.5,-66.85277c-100.29841,-1.86545 -220,83.48527 -329,10.98527" fill="#00a8a8" opacity="0.5"></path><path _ngcontent-ng-c721036369="" d="M0,35.5c212,110.5 322.5,-101.5 509.5,53.5c0,0 -53.5648,-75.21727 -177,-70.5c-78.5,3 -208.5,71 -332.5,-18.5" fill="#00a8a8"></path><path _ngcontent-ng-c721036369="" d="M0,43.5c213.5,93.5 322.5,-119.63249 510.5,45.36751c0,0 -55.5,-72 -159,-72.5c-109.27475,-0.5279 -224.5,82.63249 -351.5,2.63249" fill="#008080"></path></svg><p>Explore &amp; Get Inspired</p><!----><!----><p> Loading gallery... </p><!----></app-landing><!----></mat-sidenav-content><!----></mat-sidenav-container></app-home><!----></app-root>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stealing Part of a Production Language Model (157 pts)]]></title>
            <link>https://arxiv.org/abs/2403.06634</link>
            <guid>39675735</guid>
            <pubDate>Tue, 12 Mar 2024 03:01:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.06634">https://arxiv.org/abs/2403.06634</a>, See on <a href="https://news.ycombinator.com/item?id=39675735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Carlini,+N">Nicholas Carlini</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Paleka,+D">Daniel Paleka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dvijotham,+K+D">Krishnamurthy Dj Dvijotham</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Steinke,+T">Thomas Steinke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hayase,+J">Jonathan Hayase</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cooper,+A+F">A. Feder Cooper</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+K">Katherine Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jagielski,+M">Matthew Jagielski</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nasr,+M">Milad Nasr</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Conmy,+A">Arthur Conmy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wallace,+E">Eric Wallace</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rolnick,+D">David Rolnick</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tram%C3%A8r,+F">Florian Tram√®r</a></p></div>            
    <p><a href="https://arxiv.org/pdf/2403.06634">Download PDF</a></p><blockquote>
            <span>Abstract:</span>We introduce the first model-stealing attack that extracts precise, nontrivial information from black-box production language models like OpenAI's ChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding projection layer (up to symmetries) of a transformer model, given typical API access. For under \$20 USD, our attack extracts the entire projection matrix of OpenAI's Ada and Babbage language models. We thereby confirm, for the first time, that these black-box models have a hidden dimension of 1024 and 2048, respectively. We also recover the exact hidden dimension size of the gpt-3.5-turbo model, and estimate it would cost under \$2,000 in queries to recover the entire projection matrix. We conclude with potential defenses and mitigations, and discuss the implications of possible future work that could extend our attack.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Nicholas Carlini [<a href="https://arxiv.org/show-email/d18236f1/2403.06634">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 11 Mar 2024 11:46:12 UTC (697 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Cosine-Similarity of Embeddings Really About Similarity? (135 pts)]]></title>
            <link>https://arxiv.org/abs/2403.05440</link>
            <guid>39675585</guid>
            <pubDate>Tue, 12 Mar 2024 02:29:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.05440">https://arxiv.org/abs/2403.05440</a>, See on <a href="https://news.ycombinator.com/item?id=39675585">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.05440">Download PDF</a>
    <a href="https://arxiv.org/html/2403.05440v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Cosine-similarity is the cosine of the angle between two vectors, or equivalently the dot product between their normalizations. A popular application is to quantify semantic similarity between high-dimensional objects by applying cosine-similarity to a learned low-dimensional feature embedding. This can work better but sometimes also worse than the unnormalized dot-product between embedded vectors in practice. To gain insight into this empirical observation, we study embeddings derived from regularized linear models, where closed-form solutions facilitate analytical insights. We derive analytically how cosine-similarity can yield arbitrary and therefore meaningless `similarities.' For some linear models the similarities are not even unique, while for others they are implicitly controlled by the regularization. We discuss implications beyond linear models: a combination of different regularizations are employed when learning deep models; these have implicit and unintended effects when taking cosine-similarities of the resulting embeddings, rendering results opaque and possibly arbitrary. Based on these insights, we caution against blindly using cosine-similarity and outline alternatives.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Harald Steck [<a href="https://arxiv.org/show-email/bb917e91/2403.05440">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 8 Mar 2024 16:48:20 UTC (5,737 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Breaking Down Tasks (249 pts)]]></title>
            <link>https://jacobian.org/2024/mar/11/breaking-down-tasks/</link>
            <guid>39675249</guid>
            <pubDate>Tue, 12 Mar 2024 01:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jacobian.org/2024/mar/11/breaking-down-tasks/">https://jacobian.org/2024/mar/11/breaking-down-tasks/</a>, See on <a href="https://news.ycombinator.com/item?id=39675249">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p><a href="https://jacobian.org/series/estimation/">Estimating Software Projects</a>:</p><p>In a management group, someone asked for resources on teaching planning. I shared a link to <a href="https://jacobian.org/series/estimation/">this series on estimation</a>, but quickly they came back and told me that there was something missing. The previous parts in this series assume you‚Äôre starting with a clearly defined task list, but the people this manager is teach aren‚Äôt there yet. They need help with an earlier step: ‚Äúbreaking down‚Äù a project into a clearly defined set of tasks.</p><p>I‚Äôve not previously written about how to break down a task list because, to me, it largely feels intuitive. Prior to writing what follows, I think I would describe my process as: ‚Äúthink about the project and then write down the task list.‚Äù But that‚Äôs a deeply unsatisfying, <a href="https://knowyourmeme.com/memes/how-to-draw-an-owl">‚Äújust draw the rest of the owl‚Äù</a> sort of answer.</p><p>So here I‚Äôll fill that gap, and dig into what‚Äôs going on when I break down a project into constitute tasks. I‚Äôll start by <a href="#example">working through an example</a>, showing how do it, and then step back and <a href="#what-just-happened">explain the steps I took</a>. But if you want to skip the scenic route, you can skip to the end where I‚Äôve <a href="#tldr">summarized the process</a>.</p><h2 id="example">Breaking down tasks - by example</h2><p>I‚Äôm building a personal streak tracker, tracking days I do some sort of outdoor activity. I want something similar to the <a href="https://streaksapp.com/">Streaks</a> app, except with different options for outdoor activities (running, biking, skiing, etc.), and I want to incorporate the <a href="https://blog.duolingo.com/how-duolingo-streak-builds-habit/">‚Äústreak freeze‚Äù feature from Duolingo</a>.</p><h3 id="iteration-1">Iteration 1</h3><p>I often start with a sketch. Having a visual mockup of what I‚Äôm going to build is a great starting point. The final result rarely ends up looking like this, but it‚Äôs a good way of articulating all the features I want in an easy-to-understand format.</p><figure><a href="https://jacobian.org/2024/mar/11/breaking-down-tasks/mockup.excalidraw.png"><img src="https://jacobian.org/2024/mar/11/breaking-down-tasks/mockup.excalidraw.png" alt="A mockup of the tracker interface, showing a week of activities tracked, a currently active streak, and the freeze features."></a></figure><p>Is this sufficiently ‚Äúbroken down‚Äù? If it‚Äôs just me, if it‚Äôs just a personal project ‚Äì probably yes! For a personal project like this I‚Äôll just sit down and start writing code now. But this is essentially just a one-step task: ‚Äúbuild this picture‚Äù. If I‚Äôm going to delegate some of this work, or if I need to estimate how long this‚Äôll take, I need more granularity. So I‚Äôll continue.</p><h3 id="iteration-2">Iteration 2</h3><p>The next step is to expand my single-step ‚Äúbuild this‚Äù task into its constituent pieces. I‚Äôll think through and plan the the steps I‚Äôd need to go through to get this done. I‚Äôll try to account for dependancies ‚Äì which tasks need to come before other tasks ‚Äì but I‚Äôm not worried about size or scope of the steps yet, just rough list. Here‚Äôs my first attempt:</p><ol><li>Model the data</li><li>Calendar view, showing days of the current week</li><li>Interactive calendar: click an icon records an activity and marks the day as ‚Äúcompleted‚Äù for streak tracking.</li><li>Calculate and show the length of the current streak</li><li>Implement streak freezes</li></ol><p>To keep this blog post simple, I‚Äôm ignoring deployment and other operational steps like setting up a database, etc. I‚Äôm also mostly collapsing front-end and back-end development into single steps like ‚Äúbuild calendar view‚Äù. In a real project, especially a multi-person one, I‚Äôd have separate line items for deployment, front/backend tasks, etc., but that would quickly get more complex than I want to get for this example.</p><p>Ok, is <em>this</em> sufficiently ‚Äúbroken down‚Äù? For some purposes, maybe: I can probably produce an accurate-enough estimation of time for each step, and if I‚Äôm working with a sufficiantly-aligned and autonomous team there might be enough detail here to delegate tasks. But there‚Äôs still a good deal of uncertainty. I don‚Äôt have clear view of how freezes will be accumulated and tracked, so step 5 is a bit vague. I also don‚Äôt have anything about past history in here, adding/removing activity types, and other potential rabbit holes. Those make estimates uncertain, and mean that if I delegate a task I might not get the result I was picturing. So, I‚Äôll continue by breaking things down even further, and add more detail.</p><h3 id="iteration-3">Iteration 3</h3><p>I‚Äôll continue the same process of taking a ‚Äúbig‚Äù step, and breaking it down into constitution pieces, added detail as I go:</p><ol><li><strong>Model the data:</strong><ol><li>activity types: run/bike/ski/climb/etc. (hardcoded list of types is fine)</li><li>recorded activities: date, type</li><li>freezes: date earned, date spent</li><li>streaks: date started, date ended, stats on count of different activity types during the streak (no need for calculation, that can come later)</li></ol></li><li><strong>Static calendar view:</strong><ol><li>Weekly view: shows days of the current week, recorded activities or spent freezes on that day</li><li>Index (home page) view: shows the current week</li><li>Monthly view: show a whole month - no individual activity types, just recorded/not recorded</li><li>Browsing: browse backwards/forwards on the week and month views, and allow switching between weeks and months.</li><li>‚ÄúGo to date‚Äù quick entry box - don‚Äôt get fancy, no fuzzy date input, can use the html5 date widget for now</li></ol></li><li><strong>Dynamic week calendar view</strong> (no dynamic entry on monthly views):<ol><li>clicking an activity type on a day records that day as completed</li></ol></li><li><strong>Streak calculation and display:</strong><ol><li>Streak calculation: walk activity history and calculate streaks ‚Äî start date, end date, stats (e.g. ‚Äú14 days: 9 runs, 3 bikes, 1 climb, 1 freeze‚Äù)</li><li>Streak display: show current streak in the UI. No functionality for viewing past streaks yet</li><li>Recording an activity in the UI recalculates streaks</li></ol></li><li><strong>Streak freezes:</strong><ol><li>When calculating streaks, a streak of X days (without a freeze) accumulates a streak freeze. X can be hardcoded, make up a number for now (roll a die, whatever). Freezes ‚Äúroll over‚Äù: if 3 freezes are accumulated during Streak A, one is spent, and then the streak ends, you still have 2 freezes available to use on Streak B.</li><li>Prevent ‚Äúdouble accumulation‚Äù when recalculating streaks - make sure that if you go back in time and update an activity, causing streak recalculation, you don‚Äôt re-earn streaks.</li><li>Add freezes to the UI: spend a freeze on a day to extend the streak, and display the number of accumulated freezes in the UI.</li></ol></li></ol><p>How about now ‚Äî is this sufficiently broken down? Again the answer is contextual. I think for most engineers and most teams it is: we have clear steps, clear definitions of done, and have carefully avoided all the potential rabbit holes I can think of. I can imagine situations (junior developers, mission-critical projects with intense oversight/scrutiny) where even <em>more</em> detail might be necessary. But for the purposes of this example, let‚Äôs call this done.</p><h2 id="what-just-happened">What just happened?</h2><p>Fundamentally, what I showed is an interactive process of breaking down tasks:</p><ol><li>Start with a list of tasks ‚Äì or just one big project!</li><li>Think through the steps you‚Äôd need to take to accomplish that task, and write them down. Don‚Äôt worry about completeness or accuracy or depth, each pass just needs to expand, even slightly, on the previous one.</li><li>Is every task on your new list sufficiently defined? (I‚Äôll define ‚Äúsufficiently‚Äù in a minute.) If not, GOTO 1.</li></ol><p>This algorithm is simple, but it makes some assumptions. Remember, the context here is someone who‚Äôs never done this sort of thing before, and is pretty new to software development overall. So we need to define a couple of key terms:</p><h3 id="whats-a-task">What‚Äôs a ‚Äútask‚Äù?</h3><p>Merriam-Websters defines task as ‚Ä¶ haha no, just kidding. I can do better than that.</p><p>Many people ‚Äì me included ‚Äì have an intuitive understanding of what ‚Äútask‚Äù means, but when asked to define it, we struggle to articulate it out loud. For the purposes of talking about software development and project estimation, I think this definition is best:</p><dl><dt>Task</dt><dd>A sufficiently defined, complete piece of work that delivers change.</dd></dl><ul><li><em>Sufficiently defined</em> because tasks need some sort of clear outline of what‚Äôs required; ‚Äúwork on stuff‚Äù isn‚Äôt a task. More on this in a minute.</li><li><em>Complete</em> because a task needs to encompass all of the work required. The ‚Äúcut down tree‚Äù task isn‚Äôt complete if you‚Äôve only fetched the chainsaw.</li><li><em>Delivers change</em> because, in a work context, a task only ‚Äúmatters‚Äù if something is different because of the work.</li></ul><p>For the purposes of estimation, we need to go a little deeper into what ‚Äúdefined‚Äù means. In order for a task to be ‚Äúsufficiently defined‚Äù, it needs to have ‚Äúenough‚Äù detail ‚Äì but the definition of ‚Äúenough‚Äù is going to differ based on context. As I explored above, if it‚Äôs just me, a simple sketch is ‚Äúsufficiently defined‚Äù, but in other contexts we need more detail. Still, although there isn‚Äôt one globally-applicable definition of ‚Äúsufficiently defined‚Äù, I think there is a common-enough definition to be useful as a starting point.</p><h3 id="sufficiantly-defined">What is ‚Äúsufficiently defined‚Äù?</h3><p><strong>A task is sufficiently defined if the person working on the task<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> can answer ‚Äúyes‚Äù to these questions:</strong></p><ul><li>Do I understand what change is desired?</li><li>Do I understand what ‚Äúdone‚Äù will look like?</li><li>Can I define <em>all</em> the steps I would take to get to ‚Äúdone‚Äù? E.g., could I write down a TODO list for this task, and would it be complete?</li><li>Assuming no blockers or dependancies, do I have all the information I need to start this task right now?</li></ul><p>If the person working on the task, within the context of their workplace and their skillset, can‚Äôt answer ‚Äúyes‚Äù to all these questions, then the task isn‚Äôt sufficiently broken down, and you should repeat the breaking-down algorithm until you can answer ‚Äúyes‚Äù to these questions for all tasks on the list.</p><p>Now, there are going to be situations where you <em>can‚Äôt</em> decompose tasks further, or add more context, because of unknowns. Fixing bugs is often like this: you know what the change is (no more bug), you know what ‚Äúdone‚Äù looks like (bug is fixed), but you can‚Äôt know all the steps. There are techniques you can employ in these situations ‚Äì <a href="https://jacobian.org/2021/oct/20/simple-pm-tricks/#timeboxing">timeboxing</a> is my favorite ‚Äì but a deep discussion here is out of scope.</p><h2 id="this-is-a-skill-and-it-takes-practice">This is a skill, and it takes practice</h2><p>It‚Äôs important to point out at this point that this is a skill, and it takes practice to develop to the point where it‚Äôs easy. If you‚Äôre new to this kind of thinking, you‚Äôll probably find it doesn‚Äôt come particularly easy ‚Äì and that‚Äôs normal.</p><p>Take, for example, what I did above in <a href="#iteration-2">iteration 2</a>: how did I know, or how did I decide, that ‚Äúmodel data‚Äù was going to be the first step? Why wasn‚Äôt step 1 ‚Äúdesign the interface‚Äù, or ‚Äúwrite the streak calculation algorithm‚Äù, or anything else?</p><p>The answer is deeply unsatisfying to people new to this kind of work: it was largely intuitive. I know, for decades of experience writing software, that when I‚Äôm working on tools like this, things go better when I model data first. I also know that I‚Äôll be using Django, which has better affordances for a model-data-first workflow. There‚Äôs no algorithm here: I‚Äôm essentially pattern-matching this project against all the other projects I‚Äôve worked on or seen, and, based on that experience, intuiting the right steps in the right order. Which means, of course, if you <em>haven‚Äôt</em> seen a bunch of projects like this one, it can be incredibly difficult to know where to start!</p><p>There are no shortcuts here; the only way to learn is by doing. Which means, to circle all the way back to the discussion that inspired this piece: the best thing this manager can do to help her team get better here is give them safe opportunities to try it out. Ask for project plans, help them break them down, give feedback ‚Äì but don‚Äôt penalize them when they‚Äôre wrong. Because they‚Äôll probably be quite wrong, at first, but in a safe learning environment those mistakes become the data they‚Äôll pattern match against the next time, and get better and better.</p><h2 id="tldr">Summary: breaking down tasks, the algorithm</h2><ol><li><p>Begin where you are: with a list of tasks, a sketch, or even just an idea.</p></li><li><p>Think through the steps you‚Äôd need to take to accomplish that task, and write them down. Don‚Äôt worry about completeness or accuracy or depth, each pass just needs to expand, even slightly, on the previous one.</p></li><li><p>For each item on your list, decide if that item is <a href="#sufficiently-defined">sufficiently defined</a>:</p><ul><li>Do I understand what change is desired?</li><li>Do I understand what ‚Äúdone‚Äù will look like?</li><li>Can I define <em>all</em> the steps I would take to get to ‚Äúdone‚Äù?</li><li>Assuming no blockers or dependancies, do I have all the information I need to start this task right now?</li></ul><p>If the answer to any of these questions is ‚Äúno‚Äù, take that task and recurse - breaking it down further using this algorithm again.</p></li><li><p>Repeat until all tasks are sufficiently broken down.</p></li></ol><h2 id="howd-it-go">How‚Äôd it go?</h2><p>As usual, if you try this out and it works for you or doesn‚Äôt, I‚Äôd love to hear about it! <a href="https://jacobian.org/contact/">Get in touch</a>.</p><hr><h2 id="bonus-estimating-this-project">Bonus: estimating this project</h2><p>Because this a series on estimation, it seems reasonable to complete the work and produce an estimate for this project:</p><table><thead><tr><th>Task</th><th>Complexity</th><th>Uncertainty</th><th>Expected (days)</th><th>Worst-case (days)</th></tr></thead><tbody><tr><td>1. model data</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>2a. weekly view</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>2b. home page view</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>2c. monthly view</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>2d. browsing</td><td>small</td><td>low</td><td>1</td><td>1.1</td></tr><tr><td>3. dynamic week</td><td>small</td><td>low</td><td>1</td><td>1.1</td></tr><tr><td>4a. streak calculation</td><td>medium</td><td>moderate</td><td>3</td><td>4.5</td></tr><tr><td>4b. streak display</td><td>x-small</td><td>low</td><td>0.5</td><td>0.5</td></tr><tr><td>4c. streak recalculation</td><td>medium</td><td>low</td><td>3</td><td>3.3</td></tr><tr><td>5a. freeze accumulation</td><td>medium</td><td>moderate</td><td>3</td><td>4.5</td></tr><tr><td>5b. prevent double accumulation</td><td>small</td><td>extreme</td><td>1</td><td>5</td></tr><tr><td>5c. freeze spending</td><td>small</td><td>moderate</td><td>1</td><td>1.5</td></tr><tr><td></td><td></td><td><strong>Total</strong>:</td><td><strong>15.5 days</strong></td><td><strong>23.5 days</strong></td></tr></tbody></table><p>In reality, this overestimates the work somewhat; I completed this in about a dozen evenings and one long plane ride. But I also cut some serious corners ‚Äì the design is essentially non-existent ‚Äì and I‚Äôm fairly sure the ‚Äúfreeze‚Äù algorithm has some silly bugs I‚Äôll run into eventually.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI ‚Äì transformer debugger release (263 pts)]]></title>
            <link>https://github.com/openai/transformer-debugger</link>
            <guid>39675054</guid>
            <pubDate>Tue, 12 Mar 2024 01:12:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/openai/transformer-debugger">https://github.com/openai/transformer-debugger</a>, See on <a href="https://news.ycombinator.com/item?id=39675054">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Transformer Debugger</h2><a id="user-content-transformer-debugger" aria-label="Permalink: Transformer Debugger" href="#transformer-debugger"></a></p>
<p dir="auto">Transformer Debugger (TDB) is a tool developed by OpenAI's <a href="https://openai.com/blog/introducing-superalignment" rel="nofollow">Superalignment
team</a> with the goal of
supporting investigations into specific behaviors of small language models. The tool combines
<a href="https://openai.com/research/language-models-can-explain-neurons-in-language-models" rel="nofollow">automated interpretability</a>
techniques with <a href="https://transformer-circuits.pub/2023/monosemantic-features" rel="nofollow">sparse autoencoders</a>.</p>
<p dir="auto">TDB enables rapid exploration before needing to write code, with the ability to intervene in the
forward pass and see how it affects a particular behavior. It can be used to answer questions like,
"Why does the model output token A instead of token B for this prompt?" or "Why does attention head
H to attend to token T for this prompt?" It does so by identifying specific components (neurons,
attention heads, autoencoder latents) that contribute to the behavior, showing automatically
generated explanations of what causes those components to activate most strongly, and tracing
connections between components to help discover circuits.</p>
<p dir="auto">These videos give an overview of TDB and show how it can be used to investigate <a href="https://arxiv.org/abs/2211.00593" rel="nofollow">indirect object
identification in GPT-2 small</a>:</p>
<ul dir="auto">
<li><a href="https://www.loom.com/share/721244075f12439496db5d53439d2f84?sid=8445200e-c49e-4028-8b8e-3ea8d361dec0" rel="nofollow">Introduction</a></li>
<li><a href="https://www.loom.com/share/21b601b8494b40c49b8dc7bfd1dc6829?sid=ee23c00a-9ede-4249-b9d7-c2ba15993556" rel="nofollow">Neuron viewer pages</a></li>
<li><a href="https://www.loom.com/share/3478057cec484a1b85471585fef10811?sid=b9c3be4b-7117-405a-8d31-0f9e541dcfb6" rel="nofollow">Example: Investigating name mover heads</a></li>
<li><a href="https://www.loom.com/share/6bd8c6bde84b42a98f9a26a969d4a3ad?sid=4a09ac29-58a2-433e-b55d-762414d9a7fa" rel="nofollow">Example: Beyond name mover heads</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">What's in the release?</h2><a id="user-content-whats-in-the-release" aria-label="Permalink: What's in the release?" href="#whats-in-the-release"></a></p>
<ul dir="auto">
<li><a href="https://github.com/openai/transformer-debugger/blob/main/neuron_viewer/README.md">Neuron viewer</a>: A React app that hosts TDB as well as pages with information about individual model components (MLP neurons, attention heads and autoencoder latents for both).</li>
<li><a href="https://github.com/openai/transformer-debugger/blob/main/neuron_explainer/activation_server/README.md">Activation server</a>: A backend server that performs inference on a subject model to provide data for TDB. It also reads and serves data from public Azure buckets.</li>
<li><a href="https://github.com/openai/transformer-debugger/blob/main/neuron_explainer/models/README.md">Models</a>: A simple inference library for GPT-2 models and their autoencoders, with hooks to grab activations.</li>
<li><a href="https://github.com/openai/transformer-debugger/blob/main/datasets.md">Collated activation datasets</a>: top-activating dataset examples for MLP neurons, attention heads and autoencoder latents.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">Follow these steps to install the repo.  You'll first need python/pip, as well as node/npm.</p>
<p dir="auto">Though optional, we recommend you use a virtual environment or equivalent:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# If you're already in a venv, deactivate it.
deactivate
# Create a new venv.
python -m venv ~/.virtualenvs/transformer-debugger
# Activate the new venv.
source ~/.virtualenvs/transformer-debugger/bin/activate"><pre><span><span>#</span> If you're already in a venv, deactivate it.</span>
deactivate
<span><span>#</span> Create a new venv.</span>
python -m venv <span>~</span>/.virtualenvs/transformer-debugger
<span><span>#</span> Activate the new venv.</span>
<span>source</span> <span>~</span>/.virtualenvs/transformer-debugger/bin/activate</pre></div>
<p dir="auto">Once your environment is set up, follow the following steps:</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone git@github.com:openai/transformer-debugger.git
cd transformer-debugger

# Install neuron_explainer
pip install -e .

# Set up the pre-commit hooks.
pre-commit install

# Install neuron_viewer.
cd neuron_viewer
npm install
cd .."><pre>git clone git@github.com:openai/transformer-debugger.git
<span>cd</span> transformer-debugger

<span><span>#</span> Install neuron_explainer</span>
pip install -e <span>.</span>

<span><span>#</span> Set up the pre-commit hooks.</span>
pre-commit install

<span><span>#</span> Install neuron_viewer.</span>
<span>cd</span> neuron_viewer
npm install
<span>cd</span> ..</pre></div>
<p dir="auto">To run the TDB app, you'll then need to follow the instructions to set up the <a href="https://github.com/openai/transformer-debugger/blob/main/neuron_explainer/activation_server/README.md">activation server backend</a> and <a href="https://github.com/openai/transformer-debugger/blob/main/neuron_viewer/README.md">neuron viewer frontend</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Making changes</h2><a id="user-content-making-changes" aria-label="Permalink: Making changes" href="#making-changes"></a></p>
<p dir="auto">To validate changes:</p>
<ul dir="auto">
<li>Run <code>pytest</code></li>
<li>Run <code>mypy --config=mypy.ini .</code></li>
<li>Run activation server and neuron viewer and confirm that basic functionality like TDB and neuron
viewer pages is still working</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Links</h2><a id="user-content-links" aria-label="Permalink: Links" href="#links"></a></p>
<ul dir="auto">
<li><a href="https://github.com/openai/transformer-debugger/blob/main/terminology.md">Terminology</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to cite</h2><a id="user-content-how-to-cite" aria-label="Permalink: How to cite" href="#how-to-cite"></a></p>
<p dir="auto">Please cite as:</p>
<div data-snippet-clipboard-copy-content="Mossing, et al., ‚ÄúTransformer Debugger‚Äù, GitHub, 2024."><pre><code>Mossing, et al., ‚ÄúTransformer Debugger‚Äù, GitHub, 2024.
</code></pre></div>
<p dir="auto">BibTex citation:</p>
<div data-snippet-clipboard-copy-content="@misc{mossing2024tdb,
  title={Transformer Debugger},
  author={Mossing, Dan and Bills, Steven and Tillman, Henk and Dupr√© la Tour, Tom and Cammarata, Nick and Gao, Leo and Achiam, Joshua and Yeh, Catherine and Leike, Jan and Wu, Jeff and Saunders, William},
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/openai/transformer-debugger}},
}"><pre><code>@misc{mossing2024tdb,
  title={Transformer Debugger},
  author={Mossing, Dan and Bills, Steven and Tillman, Henk and Dupr√© la Tour, Tom and Cammarata, Nick and Gao, Leo and Achiam, Joshua and Yeh, Catherine and Leike, Jan and Wu, Jeff and Saunders, William},
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/openai/transformer-debugger}},
}
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Key Boeing Whistleblower Found Dead from Apparent Suicide (158 pts)]]></title>
            <link>https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/</link>
            <guid>39674829</guid>
            <pubDate>Tue, 12 Mar 2024 00:41:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/">https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/</a>, See on <a href="https://news.ycombinator.com/item?id=39674829">Hacker News</a></p>
Couldn't get https://thehill.com/policy/transportation/4524968-boeing-whistleblower-found-dead-in-apparent-suicide/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Create More, Consume Less (2021) (114 pts)]]></title>
            <link>https://www.omaritani.com/blog/create-more-consume-less</link>
            <guid>39674619</guid>
            <pubDate>Tue, 12 Mar 2024 00:04:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.omaritani.com/blog/create-more-consume-less">https://www.omaritani.com/blog/create-more-consume-less</a>, See on <a href="https://news.ycombinator.com/item?id=39674619">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="block-c771f4d65dbb1013d3af">
  <p>When the global pandemic was first announced, it was a bewildering feeling. It was scary, and yet, somehow thrilling‚Äîa novel experience, to say the least.</p><p>So throughout the rest of the year‚Äîand setting aside traumas and deaths‚Äîwe got to enjoy the slower pace of remote living. But now, almost a year later, the mental, emotional, and physical toll has surfaced. We‚Äôve absorbed the knocks, and now, we‚Äôre just plain and simply, tired. </p><p>When the new year kicked in, I thought that taking a break from my creative writing to ‚Äúlive a little more‚Äù would energize me‚Äîit didn‚Äôt. I felt lethargic instead. I felt a void inside myself that was snowballing by the day.</p><p>If you find yourself growing more weary and anxious about the future, <a href="https://www.tandfonline.com/doi/full/10.1080/07421656.2016.1166832?journalCode=uart20#.V2GKm-YrI6g" target="_blank">scientific research</a> indicates that regardless of your artistic experience or talent, just a 45-minute session of focused creative activity significantly lowers the cortisol levels in your body, and thus, reduces your overall stress.</p><p>In other words, a small dose of daily creativity is good for us. It can help us become more present, less anxious, and much more fulfilled. And as I continue to learn more about the mental health benefits of creativity, I realize now how the simple habit of writing every day has boosted my self-esteem, given me a sense of purpose, and formed a buffer for emotional release. </p><h2>The Problem with Too Much Consumption</h2><p>In this 21st century, there‚Äôs so much you could do to fill all the hours in your day. You can binge-watch anything on demand. You can order a bunch of stuff and have them arrive at your doorstep on the same day. You can spend countless hours scrolling on your phone, at the mercy of an algorithm, and you‚Äôd only scratch less than 1% of the social entertainment that‚Äôs at your disposal. </p><p>Extrapolate that across the next week, month, and years, and you begin to see why some people can fall into the trap of endless consumption and inactivity.  </p><p>Look, life is meant to be lived and enjoyed, but that‚Äôs not the ultimate purpose of life. What‚Äôs the purpose? Your purpose is just to be alive and do what good you can with it. Your purpose is to keep growing and evolving. It‚Äôs as simple as that.</p><p>I think Ralph Waldo Emerson captured it well:</p><blockquote><p><em>‚ÄúThe purpose of life is not to be happy. It is to be useful, to be honorable, to be compassionate, to have it make some difference that you have lived and lived well.‚Äù</em></p></blockquote><p>As you make yourself useful by enjoying the process of adding more arsenal to your realm of knowledge and skills, so that you can put them to good use and contribute threads to the fabric of society, happiness simply becomes a byproduct of that journey. <a href="https://medium.com/mind-cafe/10-daily-habits-of-truly-happy-people-9efd3d8bd2eb?sk=3e4e69b02620d4bcf0a6e7b6e5ea8216&amp;source=friends_link" target="_blank"><em>Happiness becomes the way.</em></a></p><p>The problem with too much consumption, however, is that it can leave us feeling utterly empty inside. Ironically, it creates something:<em> One big void. </em></p><p>You start to feel stuck, not knowing what to do with your life. You lose agency over your habits, you lose control over your emotions, and eventually, this loss in energy translates into zero productivity.</p><p><strong>Too much consumption leads to a life of escapism. </strong>Instead of learning how to sit with your emotions or exploring what‚Äôs inside of you and self-expressing yourself through art, you seek an escape that would make you forget.</p><p>Another problem with consumption is that it can give you a false illusion that you‚Äôre doing something. Spending hours watching fitness videos will not help you become fit. You‚Äôve got to get up and move to do so. Spending hours watching cooking shows won‚Äôt make you a better cook‚Äîcooking will.  </p><p>With that in mind, you‚Äôve got to make a decision: <em>Do you want to define your life by self-expressive creation or mindless consumption? By selflessly giving or selfishly taking? </em>As Martin Luther King Jr. once said: ‚ÄúEvery man must decide whether he will walk in the light of creative altruism or in the darkness of destructive selfishness.‚Äù </p><h2>Creativity is Your Duty, But it Can Also Be a Therapeutic Tool</h2><p>Art is the most beautiful form of human expression. Whether it be through prose, science, technology, innovation, visual arts, or illustration, the sheer act of living in a state of wonder and creation is what makes us human. </p><p>When we create, we enter a state of flow, and flow is what psychologist Mih√°ly Cs√≠kszentmih√°ly defines as a state of complete immersion in an activity for its own sake. In his book, <em>Flow: The psychology of optimal experience, </em>he writes:</p><blockquote><p><em>‚ÄúFlow is a state in which people are so involved in an activity that nothing else seems to matter; the experience is so enjoyable that people will continue to do it even at great cost, for the sheer sake of doing it.‚Äù</em></p></blockquote><p>There‚Äôs a distinct word here that I should highlight. Notice how he says enjoyable and not pleasurable because they‚Äôre two different things.</p><p>Activities like eating, drinking, sleeping, and watching entertainment, are examples of pleasurable experiences. They‚Äôre passive and transient in nature. Enjoyable experiences, on the other hand, are active. Think of sports, writing, and art‚Äîthey demand you to show up to it and be fully immersed in it. </p><p>In other words: <strong><em>Pleasurable experiences are rooted in consumption. Enjoyable experiences are rooted in creation.</em></strong></p><p>And this relates to what Rabindranath Tagore, the Bengali poet, philosopher, and the first non-European to win the Nobel Prize, wrote somewhere between the 19th and 20th century:</p><blockquote><p><em>‚ÄúI slept and dreamt that life was joy.&nbsp;<br>I awoke and saw that life was duty.&nbsp;<br>I worked‚Äîand behold, duty was joy.‚Äù</em></p></blockquote><p><em>Work is duty and ‚Äúduty is joy.‚Äù </em></p><p>That‚Äôs one great mantra to live by. Duty is not pleasure; duty is joy. <em>Duty is not consumption; duty is creation. </em>And since creativity is a form of work and work is a duty, then creativity is a duty‚Äîa joy. </p><p>When I sit down on my chair to write, I face a lot of resistance, but I insist on carrying myself through because the simple practice of plucking thoughts from the garden of my mind and glazing them in words gives me a sense of peace and calm. It arms me with joy. Hours flash by in minutes. The focus drifts me away from my worries of the future and returns me to the present. </p><p>And after I walk away from my desk, the knowledge that I just painted an entire white page with black ink‚Äî<em>that I‚Äôve done something good today</em>‚Äîgraces me with the freedom to feel fulfilled. </p><p>Writing is my art of choice, and it helps me make sense of my life, but <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2804629/" target="_blank">as this published study</a> on the connection between art and healing demonstrates, any form of art can help you improve your mental and emotional wellbeing. </p><p>When you focus on one activity and become fully absorbed by it, your heart rate slows and your breath deepens. This immersive nature of being creative helps you control what thoughts you pay attention to. In a way, creative work transforms into a form of meditation.</p><p>But what makes it all worthwhile? The sense of accomplishment you feel by the end of it. Even if no one else checks out your work, it‚Äôll still help you see yourself in a new light and earn your own appreciation. The artistic process of self-express helps you understand yourself better. And if you ask me, that‚Äôs the single most potent benefit of having your own creative outlet.</p><p>In fact, that‚Äôs why in the field of psychotherapy, they utilize the creative process to help people explore self-expression and, in doing so, find new ways to gain personal insight and develop more robust coping skills. It‚Äôs called <a href="https://www.verywellmind.com/what-is-art-therapy-2795755" target="_blank">art therapy</a>. </p><h2>How to Tip The Scale Toward Creation</h2><p>In her book, Big Magic, Elizabeth Gilbert writes that ‚Äúa creative life is an amplified life. It‚Äôs a bigger life, a happier life, an expanded life, and a hell of a lot more interesting life. Living in this manner‚Äîcontinually and stubbornly bringing forth the jewels that are hidden within you‚Äîis a fine art, in and of itself.‚Äù I love that, and I couldn‚Äôt agree more.</p><p>Truth is, it doesn't take much to live a creative life. You don‚Äôt need to quit your job. You don‚Äôt need to move to a new place to feel inspired. You don‚Äôt need to sign-up for a $1,000 course to learn how to to make art.</p><p><em>You just need to start. </em></p><p><em>And you just need to tip the scale toward creation.</em></p><p>Here‚Äôs a simple 3-step strategy to help you get started:</p><ol data-rte-list="default"><li><p><strong>Choose something you enjoy. </strong>Not something that gives you pleasure‚Äîsomething you enjoy. Something that pulls you in. Whether it be writing, drawing, designing, knitting, or baking, you‚Äôre only able to enter a state of flow when you do something for its own sake rather than an extrinsic reward. When the work <strong><em>is</em></strong> the reward‚Äîthat‚Äôs what it means to enjoy it. </p></li><li><p><strong>Block time for it, show up to it and say ‚Äúno‚Äù to protect it. </strong>Set aside an<strong> </strong>hour a day for creativity. Switch off your phone. Create an interruption-free space. Say no to whatever or whoever pulls you in another direction. And if you can‚Äôt commit to a daily practice, can you do it twice per week?</p></li><li><p><strong>Leave your judgment at the door and replace it with self-compassion. </strong>Seriously. Remember that growth is a process. Good things take time to bloom. What keeps you growing, however, is your ability to suspend your critical judgment, honor your creative work, and be compassionate with yourself. <a href="https://www.omaritani.com/blog/capability-and-mindset" target="_blank">You‚Äôre more capable than you think</a>.</p></li></ol><h2>A New Mantra to Live By</h2><p>Scottish novelist Robert Louis Stevenson once these words:</p><blockquote><p><em>"Don't judge each day by the harvest you reap but by the seeds that you plant." </em></p></blockquote><p>In other words, don‚Äôt judge each day by what you consume but by what you create. <em>Create more, consume less. </em>That‚Äôs a good mantra to live by. </p><p>I won't judge each day by what I consume, but by what I create.</p><p>I think you should too.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Roku data breach: Over 15k accounts affected (106 pts)]]></title>
            <link>https://www.claimdepot.com/data-breach/roku</link>
            <guid>39674041</guid>
            <pubDate>Mon, 11 Mar 2024 22:41:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.claimdepot.com/data-breach/roku">https://www.claimdepot.com/data-breach/roku</a>, See on <a href="https://news.ycombinator.com/item?id=39674041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="w-node-_92fa5ae9-ab49-2ea3-a7d5-292e19eba4d7-0e0c48f6"><h2>Understanding the Roku Data Breach</h2>
<p>Roku, Inc., the leader in streaming television platforms in the U.S., recently faced a data breach that may have affected your account. As a company that values viewer privacy and security, Roku is actively addressing the situation and has provided detailed information to assist affected users.</p>
<p>Between December 28, 2023, and February 21, 2024, unauthorized parties accessed certain individual Roku accounts. The breach was discovered during this period, and the company promptly initiated an investigation. It was found that usernames, passwords, and account login information were compromised, potentially affecting 15,363 individuals in the United States, including 76 in the state of Maine.</p>
<p>Roku has reported the incident to the Attorney General's offices, including the state of California. For further details, you can review the disclosures on the <a href="https://apps.web.maine.gov/online/aeviewer/ME/40/e9cc298b-379b-47ba-a10d-e2263963b574.shtml">Maine Attorney General's website</a> and the <a href="https://oag.ca.gov/ecrime/databreach/reports/sb24-582208">California Attorney General's website</a>.</p>
<h2>Immediate Steps Taken by Roku</h2>
<p>Upon detecting the breach, Roku took swift action to secure the accounts from further unauthorized access. This included resetting passwords for potentially impacted accounts, investigating account activity, canceling unauthorized subscriptions, and issuing refunds for any unauthorized charges. Roku has also been actively monitoring for any additional suspicious activity to ensure the security of customer data.</p>
<h2>What You Should Do</h2>
<p>If you suspect your account may have been affected, it is crucial to take the following steps:</p>
<ul>
<li>Reset your Roku account password by visiting <a href="https://my.roku.com/">my.roku.com</a> and using the "Forgot password?" option.</li>
<li>Review the subscriptions and devices linked to your Roku account from your account dashboard.</li>
<li>Use strong, unique passwords for all your online accounts.</li>
<li>Monitor your account statements, credit reports, and other online account information for any suspicious activity.</li>
</ul>
<p>Roku has provided a <strong>Notice to Consumers</strong> on March 8, 2024, via written communication, outlining the steps affected users should take to protect their information. This notice includes detailed instructions and resources to help you safeguard your identity and personal data.</p>
<h2>Additional Resources and Support</h2>
<p>For more information on identity theft protection and credit monitoring services, you can refer to the <strong>Information about Identity Theft Protection</strong> included in the Notice to Consumers. Roku has also set up a dedicated phone line at 1-816-272-8106 and an email address at account-help@roku.com for any questions or concerns you may have regarding the breach.</p>
<p>Roku sincerely regrets the inconvenience caused by this incident and is committed to maintaining the privacy and security of your information. As we navigate the aftermath of this breach, Roku will continue to update affected users and take any additional steps necessary to prevent future incidents.</p>
<p>For further guidance, you can also contact the Federal Trade Commission or your state Attorney General's office for assistance with identity theft and fraud prevention. Remember, staying informed and vigilant is key to protecting your personal information in the digital age.</p></div><div><p>Weekly newsletter</p><p>Stay up to date with new class action settlements you may join.</p><div><div><p>Thank you! Your submission has been received!</p></div><div><p>Oops! Something went wrong while submitting the form.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Simpson's paradox (265 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/Simpson%27s_paradox</link>
            <guid>39673754</guid>
            <pubDate>Mon, 11 Mar 2024 22:02:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">https://en.wikipedia.org/wiki/Simpson%27s_paradox</a>, See on <a href="https://news.ycombinator.com/item?id=39673754">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							

						<p>From Wikipedia, the free encyclopedia</p>
					</div><div lang="en" dir="ltr" id="mw-content-text">
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Simpson%27s_paradox_continuous.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Simpson%27s_paradox_continuous.svg/220px-Simpson%27s_paradox_continuous.svg.png" decoding="async" width="220" height="147" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Simpson%27s_paradox_continuous.svg/330px-Simpson%27s_paradox_continuous.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/Simpson%27s_paradox_continuous.svg/440px-Simpson%27s_paradox_continuous.svg.png 2x" data-file-width="390" data-file-height="260"></a><figcaption>Simpson's paradox for quantitative data: a positive trend (<span>&nbsp;</span>, <span>&nbsp;</span>) appears for two separate groups, whereas a negative trend (<span>&nbsp;</span>) appears when the groups are combined.</figcaption></figure>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Simpsons_paradox_-_animation.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Simpsons_paradox_-_animation.gif/220px-Simpsons_paradox_-_animation.gif" decoding="async" width="220" height="157" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Simpsons_paradox_-_animation.gif/330px-Simpsons_paradox_-_animation.gif 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/Simpsons_paradox_-_animation.gif/440px-Simpsons_paradox_-_animation.gif 2x" data-file-width="700" data-file-height="500"></a><figcaption>Visualization of Simpson's paradox on data resembling real-world variability indicates that risk of misjudgment of true causal relationship can be hard to spot.</figcaption></figure>
<p><b>Simpson's paradox</b> is a phenomenon in <a href="https://en.wikipedia.org/wiki/Probability" title="Probability">probability</a> and <a href="https://en.wikipedia.org/wiki/Statistics" title="Statistics">statistics</a> in which a trend appears in several groups of data but disappears or reverses when the groups are combined. This result is often encountered in social-science and medical-science statistics,<sup id="cite_ref-1"><a href="#cite_note-1">[1]</a></sup><sup id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup><sup id="cite_ref-VogelFranks2017_3-0"><a href="#cite_note-VogelFranks2017-3">[3]</a></sup> and is particularly problematic when frequency data are unduly given <a href="https://en.wikipedia.org/wiki/Causal" title="Causal">causal</a> interpretations.<sup id="cite_ref-pearl_4-0"><a href="#cite_note-pearl-4">[4]</a></sup> The paradox can be resolved when <a href="https://en.wikipedia.org/wiki/Confounding_variable" title="Confounding variable">confounding variables</a> and causal relations are appropriately addressed in the statistical modeling<sup id="cite_ref-pearl_4-1"><a href="#cite_note-pearl-4">[4]</a></sup><sup id="cite_ref-5"><a href="#cite_note-5">[5]</a></sup> (e.g., through <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a><sup id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup>).
</p><p>Simpson's paradox has been used to illustrate the kind of misleading results that the <a href="https://en.wikipedia.org/wiki/Misuse_of_statistics" title="Misuse of statistics">misuse of statistics</a> can generate.<sup id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup><sup id="cite_ref-8"><a href="#cite_note-8">[8]</a></sup>
</p><p><a href="https://en.wikipedia.org/wiki/Edward_H._Simpson" title="Edward H. Simpson">Edward H. Simpson</a> first described this phenomenon in a technical paper in 1951,<sup id="cite_ref-9"><a href="#cite_note-9">[9]</a></sup> but the statisticians <a href="https://en.wikipedia.org/wiki/Karl_Pearson" title="Karl Pearson">Karl Pearson</a> (in 1899<sup id="cite_ref-10"><a href="#cite_note-10">[10]</a></sup>) and <a href="https://en.wikipedia.org/wiki/Udny_Yule" title="Udny Yule">Udny Yule</a> (in 1903<sup id="cite_ref-yule_11-0"><a href="#cite_note-yule-11">[11]</a></sup>) had mentioned similar effects earlier. The name <i>Simpson's paradox</i> was introduced by Colin R. Blyth in 1972.<sup id="cite_ref-blyth-72_12-0"><a href="#cite_note-blyth-72-12">[12]</a></sup> It is also referred to as <b>Simpson's reversal</b>, the <b>Yule‚ÄìSimpson effect</b>, the <b>amalgamation paradox</b>, or the <b>reversal paradox</b>.<sup id="cite_ref-13"><a href="#cite_note-13">[13]</a></sup>
</p><p>Mathematician <a href="https://en.wikipedia.org/wiki/Jordan_Ellenberg" title="Jordan Ellenberg">Jordan Ellenberg</a> argues that Simpson's paradox is misnamed as "there's no contradiction involved, just two different ways to think about the same data" and suggests that its lesson "isn't really to tell us which viewpoint to take but to insist that we keep both the parts and the whole in mind at once."<sup id="cite_ref-14"><a href="#cite_note-14">[14]</a></sup>
</p>
<meta property="mw:PageProp/toc">
<h2><span id="Examples">Examples</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=1" title="Edit section: Examples"><span>edit</span></a><span>]</span></span></h2>
<h3><span id="UC_Berkeley_gender_bias">UC Berkeley gender bias</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=2" title="Edit section: UC Berkeley gender bias"><span>edit</span></a><span>]</span></span></h3>
<p>One of the best-known examples of Simpson's paradox comes from a study of gender bias among graduate school admissions to <a href="https://en.wikipedia.org/wiki/University_of_California,_Berkeley" title="University of California, Berkeley">University of California, Berkeley</a>. The admission figures for the fall of 1973 showed that men applying were more likely than women to be admitted, and the difference was so large that it was unlikely to be due to chance.<sup id="cite_ref-freedman_15-0"><a href="#cite_note-freedman-15">[15]</a></sup><sup id="cite_ref-Bickel_16-0"><a href="#cite_note-Bickel-16">[16]</a></sup>
</p>
<table>

<tbody><tr>
<th rowspan="2">
</th>
<th colspan="2">All
</th>
<th colspan="2">Men
</th>
<th colspan="2">Women
</th></tr>
<tr>
<th>Applicants
</th>
<th>Admitted
</th>
<th>Applicants
</th>
<th>Admitted
</th>
<th>Applicants
</th>
<th>Admitted
</th></tr>
<tr>
<th>Total
</th>
<td>12,763
</td>
<td>41%
</td>
<td>8,442
</td>
<td>44%
</td>
<td>4,321
</td>
<td>35%
</td></tr></tbody></table>
<p>However, when taking into account the information about departments being applied to, the different rejection percentages reveal the different difficulty of getting into the department, and at the same time it showed that women tended to apply to more competitive departments with lower rates of admission, even among qualified applicants (such as in the English department), whereas men tended to apply to less competitive departments with higher rates of admission (such as in the engineering department). The pooled and corrected data showed a "small but statistically significant bias in favor of women".<sup id="cite_ref-Bickel_16-1"><a href="#cite_note-Bickel-16">[16]</a></sup>
</p><p>The data from the six largest departments are listed below:
</p>
<table>

<tbody><tr>
<th rowspan="2">Department
</th>
<th colspan="2">All
</th>
<th colspan="2">Men
</th>
<th colspan="2">Women
</th></tr>
<tr>
<th>Applicants
</th>
<th>Admitted
</th>
<th>Applicants
</th>
<th>Admitted
</th>
<th>Applicants
</th>
<th>Admitted
</th></tr>
<tr>
<th>A
</th>
<td>933
</td>
<td>64%
</td>
<td><b>825</b>
</td>
<td>62%
</td>
<td>108
</td>
<td>82%
</td></tr>
<tr>
<th>B
</th>
<td>585
</td>
<td>63%
</td>
<td><b>560</b>
</td>
<td>63%
</td>
<td>25
</td>
<td>68%
</td></tr>
<tr>
<th>C
</th>
<td>918
</td>
<td>35%
</td>
<td>325
</td>
<td>37%
</td>
<td><b>593</b>
</td>
<td>34%
</td></tr>
<tr>
<th>D
</th>
<td>792
</td>
<td>34%
</td>
<td>417
</td>
<td>33%
</td>
<td>375
</td>
<td>35%
</td></tr>
<tr>
<th>E
</th>
<td>584
</td>
<td>25%
</td>
<td>191
</td>
<td>28%
</td>
<td><b>393</b>
</td>
<td>24%
</td></tr>
<tr>
<th>F
</th>
<td>714
</td>
<td>6%
</td>
<td>373
</td>
<td>6%
</td>
<td>341
</td>
<td>7%
</td></tr>
<tr>
<th>Total
</th>
<th>4526
</th>
<th>39%
</th>
<th>2691
</th>
<th>45%
</th>
<th>1835
</th>
<th>30%
</th></tr>
<tr>
<td colspan="7">
<p>Legend:<br>
</p>
<p><span>&nbsp;</span>&nbsp;greater percentage of successful applicants than the other gender</p>
<p><span>&nbsp;</span>&nbsp;greater number of applicants than the other gender</p>
<p><b>bold</b> - the two 'most applied for' departments for each gender
</p>
</td></tr></tbody></table>
<p>The entire data showed total of 4 out of 85 departments to be significantly biased against women, while 6 to be significantly biased against men (not all present in the 'six largest departments' table above). Notably, the numbers of biased departments were not the basis for the conclusion, but rather it was the gender admissions pooled across all departments, while weighing by each department's rejection rate across all of its applicants.<sup id="cite_ref-Bickel_16-2"><a href="#cite_note-Bickel-16">[16]</a></sup>
</p>
<h3><span id="Kidney_stone_treatment">Kidney stone treatment</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=3" title="Edit section: Kidney stone treatment"><span>edit</span></a><span>]</span></span></h3>
<p>Another example comes from a real-life medical study<sup id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup> comparing the success rates of two treatments for <a href="https://en.wikipedia.org/wiki/Kidney_stone" title="Kidney stone">kidney stones</a>.<sup id="cite_ref-KidneyParadox_18-0"><a href="#cite_note-KidneyParadox-18">[18]</a></sup> The table below shows the success rates (the term <i>success rate</i> here actually means the success proportion) and numbers of treatments for treatments involving both small and large kidney stones, where Treatment A includes open surgical procedures and Treatment B includes closed surgical procedures. The numbers in parentheses indicate the number of success cases over the total size of the group.
</p>
<table summary="results accounting for stone size">
<tbody><tr>
<th><p>Treatment</p><p>Stone size &nbsp; &nbsp;</p>
</th>
<th>Treatment A
</th>
<th>Treatment B
</th></tr>
<tr>
<th>Small stones
</th>
<td><i>Group 1</i><br><b>93% (81/87)</b></td>
<td><i>Group 2</i><br>87% (234/270)
</td></tr>
<tr>
<th>Large stones
</th>
<td><i>Group 3</i><br><b>73% (192/263)</b></td>
<td><i>Group 4</i><br>69% (55/80)
</td></tr>
<tr>
<th>Both
</th>
<td>78% (273/350)</td>
<td><b>83% (289/350)</b>
</td></tr></tbody></table>
<p>The paradoxical conclusion is that treatment A is more effective when used on small stones, and also when used on large stones, yet treatment B appears to be more effective when considering both sizes at the same time. In this example, the "lurking" variable (or <a href="https://en.wikipedia.org/wiki/Confounding" title="Confounding">confounding variable</a>) causing the paradox is the size of the stones, which was not previously known to researchers to be important until its effects were included.
</p><p>Which treatment is considered better is determined by which success ratio (successes/total) is larger. The reversal of the inequality between the two ratios when considering the combined data, which creates Simpson's paradox, happens because two effects occur together:
</p>
<ol><li>The sizes of the groups, which are combined when the lurking variable is ignored, are very different. Doctors tend to give cases with large stones the better treatment A, and the cases with small stones the inferior treatment B. Therefore, the totals are dominated by groups 3 and 2, and not by the two much smaller groups 1 and 4.</li>
<li>The lurking variable, stone size, has a large effect on the ratios; i.e., the success rate is more strongly influenced by the severity of the case than by the choice of treatment. Therefore, the group of patients with large stones using treatment A (group 3) does worse than the group with small stones, even if the latter used the inferior treatment B (group 2).</li></ol>
<p>Based on these effects, the paradoxical result is seen to arise because the effect of the size of the stones overwhelms the benefits of the better treatment (A). In short, the less effective treatment B appeared to be more effective because it was applied more frequently to the small stones cases, which were easier to treat.<sup id="cite_ref-KidneyParadox_18-1"><a href="#cite_note-KidneyParadox-18">[18]</a></sup>
</p>
<h3><span id="Batting_averages">Batting averages</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=4" title="Edit section: Batting averages"><span>edit</span></a><span>]</span></span></h3>
<p>A common example of Simpson's paradox involves the <a href="https://en.wikipedia.org/wiki/Batting_average_(baseball)" title="Batting average (baseball)">batting averages</a> of players in <a href="https://en.wikipedia.org/wiki/Professional_baseball" title="Professional baseball">professional baseball</a>. It is possible for one player to have a higher batting average than another player each year for a number of years, but to have a lower batting average across all of those years. This phenomenon can occur when there are large differences in the number of <a href="https://en.wikipedia.org/wiki/At_bat" title="At bat">at bats</a> between the years. Mathematician <a href="https://en.wikipedia.org/wiki/Kenneth_A._Ross" title="Kenneth A. Ross">Ken Ross</a> demonstrated this using the batting average of two baseball players, <a href="https://en.wikipedia.org/wiki/Derek_Jeter" title="Derek Jeter">Derek Jeter</a> and <a href="https://en.wikipedia.org/wiki/David_Justice" title="David Justice">David Justice</a>, during the years 1995 and 1996:<sup id="cite_ref-RossBaseball_19-0"><a href="#cite_note-RossBaseball-19">[19]</a></sup><sup id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup>
</p>
<table>

<tbody><tr>
<th><p>Year</p><p>Batter &nbsp;</p>
</th>
<th colspan="2">1995
</th>
<th colspan="2">1996
</th>
<th colspan="2">Combined
</th></tr>
<tr>
<td>Derek Jeter
</td>
<td>12/48
</td>
<td>.250
</td>
<td>183/582
</td>
<td>.314
</td>
<td>195/630
</td>
<td><b>.310</b>
</td></tr>
<tr>
<td>David Justice
</td>
<td>104/411
</td>
<td><b>.253</b>
</td>
<td>45/140
</td>
<td><b>.321</b>
</td>
<td>149/551
</td>
<td>.270
</td></tr></tbody></table>
<p>In both 1995 and 1996, Justice had a higher batting average (in bold type) than Jeter did. However, when the two baseball seasons are combined, Jeter shows a higher batting average than Justice. According to Ross, this phenomenon would be observed about once per year among the possible pairs of players.<sup id="cite_ref-RossBaseball_19-1"><a href="#cite_note-RossBaseball-19">[19]</a></sup>
</p>
<h2><span id="Vector_interpretation">Vector interpretation</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=5" title="Edit section: Vector interpretation"><span>edit</span></a><span>]</span></span></h2>
<figure typeof="mw:File/Thumb"><a href="https://en.wikipedia.org/wiki/File:Simpson_paradox_vectors.svg"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Simpson_paradox_vectors.svg/220px-Simpson_paradox_vectors.svg.png" decoding="async" width="220" height="122" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Simpson_paradox_vectors.svg/330px-Simpson_paradox_vectors.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Simpson_paradox_vectors.svg/440px-Simpson_paradox_vectors.svg.png 2x" data-file-width="512" data-file-height="284"></a><figcaption>Vector interpretation of Simpson's paradox</figcaption></figure>
<p>Simpson's paradox can also be illustrated using a 2-dimensional <a href="https://en.wikipedia.org/wiki/Vector_space" title="Vector space">vector space</a>.<sup id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup> A success rate of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c397ad17b7e3f7fc729917de5e232d377001a4aa" aria-hidden="true" alt="{\textstyle {\frac {p}{q}}}"></span> (i.e., <i>successes/attempts</i>) can be represented by a <a href="https://en.wikipedia.org/wiki/Vector_(geometry)" title="Vector (geometry)">vector</a> <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a90e0a52a4344e502e3594adb30fe8495f3e723f" aria-hidden="true" alt="{\displaystyle {\vec {A}}=(q,p)}"></span>, with a <a href="https://en.wikipedia.org/wiki/Slope" title="Slope">slope</a> of <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c397ad17b7e3f7fc729917de5e232d377001a4aa" aria-hidden="true" alt="{\textstyle {\frac {p}{q}}}"></span>. A steeper vector then represents a greater success rate. If two rates <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d44f68c00f83caa6c0d1d0e3ee4625f2ee69f236" aria-hidden="true" alt="{\textstyle {\frac {p_{1}}{q_{1}}}}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9c3442f046526c1ba4a01215f00bcc0a9d989a59" aria-hidden="true" alt="{\textstyle {\frac {p_{2}}{q_{2}}}}"></span> are combined, as in the examples given above, the result can be represented by the sum of the vectors <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/45c51e78a7807e640a7812b0aed132c44ed8efe9" aria-hidden="true" alt="{\displaystyle (q_{1},p_{1})}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2d9cc6f49f6d44ff6259c71eb1c354e5de82d044" aria-hidden="true" alt="{\displaystyle (q_{2},p_{2})}"></span>, which according to the <a href="https://en.wikipedia.org/wiki/Parallelogram_rule" title="Parallelogram rule">parallelogram rule</a> is the vector <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d038de36b188adf784455b6d1b2e2cd302cfdbee" aria-hidden="true" alt="{\displaystyle (q_{1}+q_{2},p_{1}+p_{2})}"></span>, with slope <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3372cef291da5115a42ab7b218b74f13fe93169" aria-hidden="true" alt="{\textstyle {\frac {p_{1}+p_{2}}{q_{1}+q_{2}}}}"></span>.
</p><p>Simpson's paradox says that even if a vector <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ed9036854c76a8301527b47d95c46ef5b1557fdb" aria-hidden="true" alt="{\displaystyle {\vec {L}}_{1}}"></span> (in orange in figure) has a smaller slope than another vector <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cbee09705bc97f67527b36a7596d3102cc8ab5f8" aria-hidden="true" alt="{\displaystyle {\vec {B}}_{1}}"></span> (in blue), and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b0777f2814eff5a60f939802651004eb08fbed12" aria-hidden="true" alt="{\displaystyle {\vec {L}}_{2}}"></span> has a smaller slope than <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7e12b9fa9d99623b6eb14d2840b5cd9e276659d0" aria-hidden="true" alt="{\displaystyle {\vec {B}}_{2}}"></span>, the sum of the two vectors <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/35a5f178db4693ecf5ca4c0dd1781fa8adf99a37" aria-hidden="true" alt="{\displaystyle {\vec {L}}_{1}+{\vec {L}}_{2}}"></span> can potentially still have a larger slope than the sum of the two vectors <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f06a5e904687d727d687f74367b2319c71c119c" aria-hidden="true" alt="{\displaystyle {\vec {B}}_{1}+{\vec {B}}_{2}}"></span>, as shown in the example.  For this to occur one of the orange vectors must have a greater slope than one of the blue vectors (here <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b0777f2814eff5a60f939802651004eb08fbed12" aria-hidden="true" alt="{\displaystyle {\vec {L}}_{2}}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cbee09705bc97f67527b36a7596d3102cc8ab5f8" aria-hidden="true" alt="{\displaystyle {\vec {B}}_{1}}"></span>), and these will generally be longer than the alternatively subscripted vectors&nbsp;‚Äì thereby dominating the overall comparison.
</p>
<h2><span id="Correlation_between_variables">Correlation between variables</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=6" title="Edit section: Correlation between variables"><span>edit</span></a><span>]</span></span></h2>
<p>Simpson's reversal can also arise in <a href="https://en.wikipedia.org/wiki/Correlation" title="Correlation">correlations</a>,  in which two variables appear to have (say) a positive correlation towards one another, when in fact they have a negative correlation, the reversal having been brought about by a "lurking" confounder.  Berman et al.<sup id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup> give an example from economics, where a dataset suggests overall demand is positively correlated with price (that is, higher prices lead to <i>more</i> demand), in contradiction of expectation.   Analysis reveals time to be the confounding variable: plotting both price and demand against time reveals the expected negative correlation over various periods, which then reverses to become positive if the influence of time is ignored by simply plotting demand against price.
</p>
<h2><span id="Psychology">Psychology</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=7" title="Edit section: Psychology"><span>edit</span></a><span>]</span></span></h2>
<p>Psychological interest in Simpson's paradox seeks to explain why people deem sign reversal to be impossible at first, offended by the idea that an action preferred both under one condition and under its negation should be rejected when the condition is unknown. The question is where people get this strong <a href="https://en.wikipedia.org/wiki/Intuition" title="Intuition">intuition</a> from, and how it is encoded in the <a href="https://en.wikipedia.org/wiki/Mind" title="Mind">mind</a>.
</p><p>Simpson's paradox demonstrates that this intuition cannot be derived from either <a href="https://en.wikipedia.org/wiki/Classical_logic" title="Classical logic">classical logic</a> or <a href="https://en.wikipedia.org/wiki/Probability_calculus" title="Probability calculus">probability calculus</a> alone, and thus led <a href="https://en.wikipedia.org/wiki/Philosopher" title="Philosopher">philosophers</a> to speculate that it is supported by an innate causal logic that guides people in reasoning about actions and their consequences.<sup id="cite_ref-pearl_4-2"><a href="#cite_note-pearl-4">[4]</a></sup> Savage's <a href="https://en.wikipedia.org/wiki/Sure-thing_principle" title="Sure-thing principle">sure-thing principle</a><sup id="cite_ref-blyth-72_12-1"><a href="#cite_note-blyth-72-12">[12]</a></sup> is an example of what such logic may entail. A qualified version of Savage's sure thing principle can indeed be derived from Pearl's <i>do</i>-calculus<sup id="cite_ref-pearl_4-3"><a href="#cite_note-pearl-4">[4]</a></sup> and reads: "An action <i>A</i> that increases the probability of an event <i>B</i> in each subpopulation <i>C<sub>i</sub></i> of <i>C</i> must also increase the probability of <i>B</i> in the population as a whole, provided that the action does not change the distribution of the subpopulations." This suggests that knowledge about actions and consequences is stored in a form resembling Causal <a href="https://en.wikipedia.org/wiki/Bayesian_Networks" title="Bayesian Networks">Bayesian Networks</a>.
</p>
<h2><span id="Probability">Probability</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=8" title="Edit section: Probability"><span>edit</span></a><span>]</span></span></h2>
<p>A paper by Pavlides and Perlman presents a proof, due to Hadjicostas, that in a random 2 √ó 2 √ó 2 table with uniform distribution, Simpson's paradox will occur with a <a href="https://en.wikipedia.org/wiki/Probability" title="Probability">probability</a> of exactly <span><span>1</span>‚ÅÑ<span>60</span></span>.<sup id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> A study by Kock suggests that the probability that Simpson's paradox would occur at random in path models (i.e., models generated by <a href="https://en.wikipedia.org/wiki/Path_analysis_(statistics)" title="Path analysis (statistics)">path analysis</a>) with two predictors and one criterion variable is approximately 12.8 percent; slightly higher than 1 occurrence per 8 path models.<sup id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup>
</p>
<h2><span id="Simpson.27s_second_paradox"></span><span id="Simpson's_second_paradox">Simpson's second paradox</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=9" title="Edit section: Simpson's second paradox"><span>edit</span></a><span>]</span></span></h2>
<p>A second, less well-known paradox was also discussed in Simpson's 1951 paper. It can occur when the "sensible interpretation" is not necessarily found in the separated data, like in the Kidney Stone example, but can instead reside in the combined data. Whether the partitioned or combined form of the data should be used hinges on the process giving rise to the data, meaning the correct interpretation of the data cannot always be determined by simply observing the tables.<sup id="cite_ref-25"><a href="#cite_note-25">[25]</a></sup>
</p><p><a href="https://en.wikipedia.org/wiki/Judea_Pearl" title="Judea Pearl">Judea Pearl</a> has shown that, in order for the partitioned data to represent the correct causal relationships between any two variables, <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="{\displaystyle X}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" aria-hidden="true" alt="{\displaystyle Y}"></span>, the partitioning variables must satisfy a graphical condition called "back-door criterion":<sup id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup><sup id="cite_ref-27"><a href="#cite_note-27">[27]</a></sup>
</p>
<ol><li>They must block all spurious paths between <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="{\displaystyle X}"></span> and <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" aria-hidden="true" alt="{\displaystyle Y}"></span></li>
<li>No variable can be affected by <span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" aria-hidden="true" alt="{\displaystyle X}"></span></li></ol>
<p>This criterion provides an algorithmic solution to Simpson's second paradox, and explains why the correct interpretation cannot be determined by data alone; two different graphs, both compatible with the data, may dictate two different back-door criteria.
</p><p>When the back-door criterion is satisfied by a set <i>Z</i> of covariates, the adjustment formula (see <a href="https://en.wikipedia.org/wiki/Confounding" title="Confounding">Confounding</a>) gives the correct causal effect of <i>X</i> on <i>Y</i>. If no such set exists, Pearl's <i>do</i>-calculus can be invoked to discover other ways of estimating the causal effect.<sup id="cite_ref-pearl_4-4"><a href="#cite_note-pearl-4">[4]</a></sup><sup id="cite_ref-pearl-bow_28-0"><a href="#cite_note-pearl-bow-28">[28]</a></sup> The completeness of <i>do</i>-calculus <sup id="cite_ref-29"><a href="#cite_note-29">[29]</a></sup><sup id="cite_ref-pearl-bow_28-1"><a href="#cite_note-pearl-bow-28">[28]</a></sup> can be viewed as offering a complete resolution of the Simpson's paradox.
</p>
<h2><span id="Criticism">Criticism</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=10" title="Edit section: Criticism"><span>edit</span></a><span>]</span></span></h2>
<p>One criticism is that the paradox is not really a paradox at all, but rather a failure to properly account for confounding variables or to consider causal relationships between variables.<sup id="cite_ref-30"><a href="#cite_note-30">[30]</a></sup>
</p><p>Another criticism of the apparent Simpson's paradox is that it may be a result of the specific way that data is stratified or grouped. The phenomenon may disappear or even reverse if the data is stratified differently or if different confounding variables are considered. Simpson's example actually highlighted a phenomenon called noncollapsibility,<sup id="cite_ref-31"><a href="#cite_note-31">[31]</a></sup> which occurs when subgroups with high proportions do not make simple averages when combined. This suggests that the paradox may not be a universal phenomenon, but rather a specific instance of a more general statistical issue.
</p><p>Critics of the apparent Simpson's paradox also argue that the focus on the paradox may distract from more important statistical issues, such as the need for careful consideration of confounding variables and causal relationships when interpreting data.<sup id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup>
</p><p>Despite these criticisms, the apparent Simpson's paradox remains a popular and intriguing topic in statistics and data analysis. It continues to be studied and debated by researchers and practitioners in a wide range of fields, and it serves as a valuable reminder of the importance of careful statistical analysis and the potential pitfalls of simplistic interpretations of data.
</p>
<h2><span id="See_also">See also</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=11" title="Edit section: See also"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Aliasing" title="Aliasing">Aliasing</a>&nbsp;‚Äì Signal processing effect</li>
<li><a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" title="Anscombe's quartet">Anscombe's quartet</a>&nbsp;‚Äì Four data sets with the same descriptive statistics, yet very different distributions</li>
<li><a href="https://en.wikipedia.org/wiki/Berkson%27s_paradox" title="Berkson's paradox">Berkson's paradox</a>&nbsp;‚Äì Tendency to misinterpret statistical experiments involving conditional probabilities</li>
<li><a href="https://en.wikipedia.org/wiki/Cherry_picking" title="Cherry picking">Cherry picking</a>&nbsp;‚Äì Fallacy of incomplete evidence</li>
<li><a href="https://en.wikipedia.org/wiki/Condorcet_paradox" title="Condorcet paradox">Condorcet paradox</a>&nbsp;‚Äì Situation in social choice theory where collective preferences are cyclic</li>
<li><a href="https://en.wikipedia.org/wiki/Ecological_fallacy" title="Ecological fallacy">Ecological fallacy</a>&nbsp;‚Äì Logical fallacy that occurs when group characteristics are applied to individuals</li>
<li><a href="https://en.wikipedia.org/wiki/Gerrymandering" title="Gerrymandering">Gerrymandering</a>&nbsp;‚Äì Form of political manipulation</li>
<li><a href="https://en.wikipedia.org/wiki/Low_birth-weight_paradox" title="Low birth-weight paradox">Low birth-weight paradox</a>&nbsp;‚Äì Statistical quirk of babies' birth weights</li>
<li><a href="https://en.wikipedia.org/wiki/Modifiable_areal_unit_problem" title="Modifiable areal unit problem">Modifiable areal unit problem</a>&nbsp;‚Äì Source of statistical bias</li>
<li><a href="https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy" title="Prosecutor's fallacy">Prosecutor's fallacy</a>&nbsp;‚Äì Error in thinking which involves under-valuing base rate information</li>
<li><a href="https://en.wikipedia.org/wiki/Will_Rogers_phenomenon" title="Will Rogers phenomenon">Will Rogers phenomenon</a>&nbsp;‚Äì Statistical phenomenon and paradox</li>
<li><a href="https://en.wikipedia.org/wiki/Spurious_correlation" title="Spurious correlation">Spurious correlation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Omitted-variable_bias" title="Omitted-variable bias">Omitted-variable bias</a></li></ul>
<h2><span id="References">References</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=12" title="Edit section: References"><span>edit</span></a><span>]</span></span></h2>
<div><ol>
<li id="cite_note-1"><span><b><a href="#cite_ref-1">^</a></b></span> <span>
<cite id="CITEREFClifford_H._Wagner1982">Clifford H. Wagner (February 1982). "Simpson's Paradox in Real Life". <i><a href="https://en.wikipedia.org/wiki/The_American_Statistician" title="The American Statistician">The American Statistician</a></i>. <b>36</b> (1): 46‚Äì48. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2684093">10.2307/2684093</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2684093">2684093</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Statistician&amp;rft.atitle=Simpson%27s+Paradox+in+Real+Life&amp;rft.volume=36&amp;rft.issue=1&amp;rft.pages=46-48&amp;rft.date=1982-02&amp;rft_id=info%3Adoi%2F10.2307%2F2684093&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2684093%23id-name%3DJSTOR&amp;rft.au=Clifford+H.+Wagner&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-2"><span><b><a href="#cite_ref-2">^</a></b></span> <span>Holt, G. B. (2016).  <a rel="nofollow" href="http://jco.ascopubs.org/content/34/9/1016.1.full">Potential Simpson's paradox in multicenter study of intraperitoneal chemotherapy for ovarian cancer.</a> Journal of Clinical Oncology, 34(9), 1016‚Äì1016.</span>
</li>
<li id="cite_note-VogelFranks2017-3"><span><b><a href="#cite_ref-VogelFranks2017_3-0">^</a></b></span> <span><cite id="CITEREFFranksAiroldiSlavov2017">Franks, Alexander; <a href="https://en.wikipedia.org/wiki/Edoardo_Airoldi" title="Edoardo Airoldi">Airoldi, Edoardo</a>; Slavov, Nikolai (2017). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5440056">"Post-transcriptional regulation across human tissues"</a>. <i>PLOS Computational Biology</i>. <b>13</b> (5): e1005535. <a href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" title="ArXiv (identifier)">arXiv</a>:<span title="Freely accessible"><a rel="nofollow" href="https://arxiv.org/abs/1506.00219">1506.00219</a></span>. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/2017PLSCB..13E5535F">2017PLSCB..13E5535F</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1371%2Fjournal.pcbi.1005535">10.1371/journal.pcbi.1005535</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1553-7358">1553-7358</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5440056">5440056</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/28481885">28481885</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+Computational+Biology&amp;rft.atitle=Post-transcriptional+regulation+across+human+tissues&amp;rft.volume=13&amp;rft.issue=5&amp;rft.pages=e1005535&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5440056%23id-name%3DPMC&amp;rft_id=info%3Abibcode%2F2017PLSCB..13E5535F&amp;rft_id=info%3Aarxiv%2F1506.00219&amp;rft.issn=1553-7358&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pcbi.1005535&amp;rft_id=info%3Apmid%2F28481885&amp;rft.aulast=Franks&amp;rft.aufirst=Alexander&amp;rft.au=Airoldi%2C+Edoardo&amp;rft.au=Slavov%2C+Nikolai&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5440056&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-pearl-4"><span>^ <a href="#cite_ref-pearl_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-pearl_4-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-pearl_4-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-pearl_4-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-pearl_4-4"><sup><i><b>e</b></i></sup></a></span> <span><a href="https://en.wikipedia.org/wiki/Judea_Pearl" title="Judea Pearl">Judea Pearl</a>. <i>Causality: Models, Reasoning, and Inference</i>, Cambridge University Press (2000, 2nd edition 2009). <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-521-77362-8" title="Special:BookSources/0-521-77362-8">0-521-77362-8</a>.</span>
</li>
<li id="cite_note-5"><span><b><a href="#cite_ref-5">^</a></b></span> <span>Kock, N., &amp; Gaskins, L. (2016).  <a rel="nofollow" href="http://cits.tamiu.edu/kock/pubs/journals/2016JournalIJANS_ModJCveNetCorrp/Kock_Gaskins_2016_IJANS_SimpPdox.pdf">Simpson's paradox, moderation and the emergence of quadratic relationships in path models: An information systems illustration.</a> International Journal of Applied Nonlinear Science, 2(3), 200‚Äì234.</span>
</li>
<li id="cite_note-6"><span><b><a href="#cite_ref-6">^</a></b></span> <span>Rogier A. Kievit, Willem E. Frankenhuis, Lourens J. Waldorp and Denny Borsboom, Simpson's paradox in psychological science: a practical guide <a rel="nofollow" href="https://doi.org/10.3389/fpsyg.2013.00513">https://doi.org/10.3389/fpsyg.2013.00513</a></span>
</li>
<li id="cite_note-7"><span><b><a href="#cite_ref-7">^</a></b></span> <span>Robert L. Wardrop (February 1995). "Simpson's Paradox and the Hot Hand in Basketball". <i>The American Statistician</i>, <b> 49 (1)</b>: pp. 24‚Äì28.</span>
</li>
<li id="cite_note-8"><span><b><a href="#cite_ref-8">^</a></b></span> <span><a href="https://en.wikipedia.org/wiki/Alan_Agresti" title="Alan Agresti">Alan Agresti</a> (2002). "Categorical Data Analysis" (Second edition). <a href="https://en.wikipedia.org/wiki/John_Wiley_and_Sons" title="John Wiley and Sons">John Wiley and Sons</a> <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-471-36093-7" title="Special:BookSources/0-471-36093-7">0-471-36093-7</a></span>
</li>
<li id="cite_note-9"><span><b><a href="#cite_ref-9">^</a></b></span> <span>
<cite id="CITEREFSimpson,_Edward_H.1951">Simpson, Edward H. (1951). "The Interpretation of Interaction in Contingency Tables". <i>Journal of the Royal Statistical Society, Series B</i>. <b>13</b>: 238‚Äì241.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Royal+Statistical+Society%2C+Series+B&amp;rft.atitle=The+Interpretation+of+Interaction+in+Contingency+Tables&amp;rft.volume=13&amp;rft.pages=238-241&amp;rft.date=1951&amp;rft.au=Simpson%2C+Edward+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-10"><span><b><a href="#cite_ref-10">^</a></b></span> <span>
<cite id="CITEREFPearsonLeeBramley-Moore1899"><a href="https://en.wikipedia.org/wiki/Karl_Pearson" title="Karl Pearson">Pearson, Karl</a>; Lee, Alice; Bramley-Moore, Lesley (1899). <a rel="nofollow" href="https://doi.org/10.1098%2Frsta.1899.0006">"Genetic (reproductive) selection: Inheritance of fertility in man, and of fecundity in thoroughbred racehorses"</a>. <i><a href="https://en.wikipedia.org/wiki/Philosophical_Transactions_of_the_Royal_Society_A" title="Philosophical Transactions of the Royal Society A">Philosophical Transactions of the Royal Society A</a></i>. <b>192</b>: 257‚Äì330. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1098%2Frsta.1899.0006">10.1098/rsta.1899.0006</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Transactions+of+the+Royal+Society+A&amp;rft.atitle=Genetic+%28reproductive%29+selection%3A+Inheritance+of+fertility+in+man%2C+and+of+fecundity+in+thoroughbred+racehorses&amp;rft.volume=192&amp;rft.pages=257-330&amp;rft.date=1899&amp;rft_id=info%3Adoi%2F10.1098%2Frsta.1899.0006&amp;rft.aulast=Pearson&amp;rft.aufirst=Karl&amp;rft.au=Lee%2C+Alice&amp;rft.au=Bramley-Moore%2C+Lesley&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1098%252Frsta.1899.0006&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-yule-11"><span><b><a href="#cite_ref-yule_11-0">^</a></b></span> <span>
<cite id="CITEREFG._U._Yule1903">G. U. Yule (1903). <a rel="nofollow" href="https://zenodo.org/record/1431599">"Notes on the Theory of Association of Attributes in Statistics"</a>. <i><a href="https://en.wikipedia.org/wiki/Biometrika" title="Biometrika">Biometrika</a></i>. <b>2</b> (2): 121‚Äì134. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1093%2Fbiomet%2F2.2.121">10.1093/biomet/2.2.121</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biometrika&amp;rft.atitle=Notes+on+the+Theory+of+Association+of+Attributes+in+Statistics&amp;rft.volume=2&amp;rft.issue=2&amp;rft.pages=121-134&amp;rft.date=1903&amp;rft_id=info%3Adoi%2F10.1093%2Fbiomet%2F2.2.121&amp;rft.au=G.+U.+Yule&amp;rft_id=https%3A%2F%2Fzenodo.org%2Frecord%2F1431599&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-blyth-72-12"><span>^ <a href="#cite_ref-blyth-72_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-blyth-72_12-1"><sup><i><b>b</b></i></sup></a></span> <span>
<cite id="CITEREFColin_R._Blyth1972">Colin R. Blyth (June 1972). "On Simpson's Paradox and the Sure-Thing Principle". <i>Journal of the American Statistical Association</i>. <b>67</b> (338): 364‚Äì366. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2284382">10.2307/2284382</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2284382">2284382</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Statistical+Association&amp;rft.atitle=On+Simpson%27s+Paradox+and+the+Sure-Thing+Principle&amp;rft.volume=67&amp;rft.issue=338&amp;rft.pages=364-366&amp;rft.date=1972-06&amp;rft_id=info%3Adoi%2F10.2307%2F2284382&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2284382%23id-name%3DJSTOR&amp;rft.au=Colin+R.+Blyth&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-13"><span><b><a href="#cite_ref-13">^</a></b></span> <span>
<cite id="CITEREFI._J._Good,_Y._Mittal1987"><a href="https://en.wikipedia.org/wiki/I._J._Good" title="I. J. Good">I. J. Good</a>, Y. Mittal (June 1987). <a rel="nofollow" href="https://doi.org/10.1214%2Faos%2F1176350369">"The Amalgamation and Geometry of Two-by-Two Contingency Tables"</a>. <i><a href="https://en.wikipedia.org/wiki/The_Annals_of_Statistics" title="The Annals of Statistics">The Annals of Statistics</a></i>. <b>15</b> (2): 694‚Äì711. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1214%2Faos%2F1176350369">10.1214/aos/1176350369</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0090-5364">0090-5364</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2241334">2241334</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Annals+of+Statistics&amp;rft.atitle=The+Amalgamation+and+Geometry+of+Two-by-Two+Contingency+Tables&amp;rft.volume=15&amp;rft.issue=2&amp;rft.pages=694-711&amp;rft.date=1987-06&amp;rft.issn=0090-5364&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2241334%23id-name%3DJSTOR&amp;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176350369&amp;rft.au=I.+J.+Good%2C+Y.+Mittal&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1214%252Faos%252F1176350369&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-14"><span><b><a href="#cite_ref-14">^</a></b></span> <span><cite id="CITEREFEllenberg2021">Ellenberg, Jordan (May 25, 2021). <a rel="nofollow" href="https://www.worldcat.org/oclc/1226171979"><i>Shape: The Hidden Geometry of Information, Biology, Strategy, Democracy and Everything Else</i></a>. New York: <a href="https://en.wikipedia.org/wiki/Penguin_Press" title="Penguin Press">Penguin Press</a>. p.&nbsp;228. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-9848-7905-9" title="Special:BookSources/978-1-9848-7905-9"><bdi>978-1-9848-7905-9</bdi></a>. <a href="https://en.wikipedia.org/wiki/OCLC_(identifier)" title="OCLC (identifier)">OCLC</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/oclc/1226171979">1226171979</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Shape%3A+The+Hidden+Geometry+of+Information%2C+Biology%2C+Strategy%2C+Democracy+and+Everything+Else&amp;rft.place=New+York&amp;rft.pages=228&amp;rft.pub=Penguin+Press&amp;rft.date=2021-05-25&amp;rft_id=info%3Aoclcnum%2F1226171979&amp;rft.isbn=978-1-9848-7905-9&amp;rft.aulast=Ellenberg&amp;rft.aufirst=Jordan&amp;rft_id=https%3A%2F%2Fwww.worldcat.org%2Foclc%2F1226171979&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-freedman-15"><span><b><a href="#cite_ref-freedman_15-0">^</a></b></span> <span><a href="https://en.wikipedia.org/wiki/David_A._Freedman" title="David A. Freedman">David Freedman</a>, Robert Pisani, and Roger Purves (2007), <i>Statistics</i> (4th edition), <a href="https://en.wikipedia.org/wiki/W._W._Norton_%26_Company" title="W. W. Norton &amp; Company">W. W. Norton</a>. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-393-92972-8" title="Special:BookSources/0-393-92972-8">0-393-92972-8</a>.</span>
</li>
<li id="cite_note-Bickel-16"><span>^ <a href="#cite_ref-Bickel_16-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Bickel_16-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Bickel_16-2"><sup><i><b>c</b></i></sup></a></span> <span><cite id="CITEREFP.J._Bickel,_E.A._Hammel_and_J.W._O'Connell1975"><a href="https://en.wikipedia.org/wiki/Peter_J._Bickel" title="Peter J. Bickel">P.J. Bickel</a>, E.A. Hammel and J.W. O'Connell (1975). <a rel="nofollow" href="http://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf">"Sex Bias in Graduate Admissions: Data From Berkeley"</a> <span>(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Science_(journal)" title="Science (journal)">Science</a></i>. <b>187</b> (4175): 398‚Äì404. <a href="https://en.wikipedia.org/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" href="https://ui.adsabs.harvard.edu/abs/1975Sci...187..398B">1975Sci...187..398B</a>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1126%2Fscience.187.4175.398">10.1126/science.187.4175.398</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/17835295">17835295</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:15278703">15278703</a>. <a rel="nofollow" href="https://web.archive.org/web/20160604220121/http://homepage.stat.uiowa.edu/~mbognar/1030/Bickel-Berkeley.pdf">Archived</a> <span>(PDF)</span> from the original on 2016-06-04.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Sex+Bias+in+Graduate+Admissions%3A+Data+From+Berkeley&amp;rft.volume=187&amp;rft.issue=4175&amp;rft.pages=398-404&amp;rft.date=1975&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.187.4175.398&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A15278703%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F17835295&amp;rft_id=info%3Abibcode%2F1975Sci...187..398B&amp;rft.au=P.J.+Bickel%2C+E.A.+Hammel+and+J.W.+O%27Connell&amp;rft_id=http%3A%2F%2Fhomepage.stat.uiowa.edu%2F~mbognar%2F1030%2FBickel-Berkeley.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-17"><span><b><a href="#cite_ref-17">^</a></b></span> <span><cite id="CITEREFC._R._CharigD._R._WebbS._R._PayneJ._E._Wickham1986">C. R. Charig; D. R. Webb; S. R. Payne; J. E. Wickham (29 March 1986). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1339981">"Comparison of treatment of renal calculi by open surgery, percutaneous nephrolithotomy, and extracorporeal shockwave lithotripsy"</a>. <i><a href="https://en.wikipedia.org/wiki/Br_Med_J_(Clin_Res_Ed)" title="Br Med J (Clin Res Ed)">Br Med J (Clin Res Ed)</a></i>. <b>292</b> (6524): 879‚Äì882. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1136%2Fbmj.292.6524.879">10.1136/bmj.292.6524.879</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1339981">1339981</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/3083922">3083922</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Br+Med+J+%28Clin+Res+Ed%29&amp;rft.atitle=Comparison+of+treatment+of+renal+calculi+by+open+surgery%2C+percutaneous+nephrolithotomy%2C+and+extracorporeal+shockwave+lithotripsy&amp;rft.volume=292&amp;rft.issue=6524&amp;rft.pages=879-882&amp;rft.date=1986-03-29&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC1339981%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F3083922&amp;rft_id=info%3Adoi%2F10.1136%2Fbmj.292.6524.879&amp;rft.au=C.+R.+Charig&amp;rft.au=D.+R.+Webb&amp;rft.au=S.+R.+Payne&amp;rft.au=J.+E.+Wickham&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC1339981&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-KidneyParadox-18"><span>^ <a href="#cite_ref-KidneyParadox_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-KidneyParadox_18-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFSteven_A._JuliousMark_A._Mullee1994">Steven A. Julious; Mark A. Mullee (3 December 1994). <a rel="nofollow" href="http://bmj.bmjjournals.com/cgi/content/full/309/6967/1480">"Confounding and Simpson's paradox"</a>. <i><a href="https://en.wikipedia.org/wiki/BMJ" title="BMJ">BMJ</a></i>. <b>309</b> (6967): 1480‚Äì1481. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1136%2Fbmj.309.6967.1480">10.1136/bmj.309.6967.1480</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2541623">2541623</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/7804052">7804052</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BMJ&amp;rft.atitle=Confounding+and+Simpson%27s+paradox&amp;rft.volume=309&amp;rft.issue=6967&amp;rft.pages=1480-1481&amp;rft.date=1994-12-03&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2541623%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F7804052&amp;rft_id=info%3Adoi%2F10.1136%2Fbmj.309.6967.1480&amp;rft.au=Steven+A.+Julious&amp;rft.au=Mark+A.+Mullee&amp;rft_id=http%3A%2F%2Fbmj.bmjjournals.com%2Fcgi%2Fcontent%2Ffull%2F309%2F6967%2F1480&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-RossBaseball-19"><span>^ <a href="#cite_ref-RossBaseball_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-RossBaseball_19-1"><sup><i><b>b</b></i></sup></a></span> <span>Ken Ross. "<i>A Mathematician at the Ballpark: Odds and Probabilities for Baseball Fans (Paperback)</i>" Pi Press, 2004. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-13-147990-3" title="Special:BookSources/0-13-147990-3">0-13-147990-3</a>. 12‚Äì13</span>
</li>
<li id="cite_note-20"><span><b><a href="#cite_ref-20">^</a></b></span> <span>Statistics available from <a href="https://en.wikipedia.org/wiki/Baseball-Reference.com" title="Baseball-Reference.com">Baseball-Reference.com</a>: <a rel="nofollow" href="https://www.baseball-reference.com/j/jeterde01.shtml">Data for Derek Jeter</a>; <a rel="nofollow" href="https://www.baseball-reference.com/j/justida01.shtml">Data for David Justice</a>.</span>
</li>
<li id="cite_note-21"><span><b><a href="#cite_ref-21">^</a></b></span> <span><cite id="CITEREFKocik_Jerzy2001">Kocik Jerzy (2001). <a rel="nofollow" href="http://www.math.siu.edu/kocik/papers/simpson2.pdf">"Proofs without Words: Simpson's Paradox"</a> <span>(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Mathematics_Magazine" title="Mathematics Magazine">Mathematics Magazine</a></i>. <b>74</b> (5): 399. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2307%2F2691038">10.2307/2691038</a>. <a href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" title="JSTOR (identifier)">JSTOR</a>&nbsp;<a rel="nofollow" href="https://www.jstor.org/stable/2691038">2691038</a>. <a rel="nofollow" href="https://web.archive.org/web/20100612220747/http://www.math.siu.edu/kocik/papers/simpson2.pdf">Archived</a> <span>(PDF)</span> from the original on 2010-06-12.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematics+Magazine&amp;rft.atitle=Proofs+without+Words%3A+Simpson%27s+Paradox&amp;rft.volume=74&amp;rft.issue=5&amp;rft.pages=399&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.2307%2F2691038&amp;rft_id=https%3A%2F%2Fwww.jstor.org%2Fstable%2F2691038%23id-name%3DJSTOR&amp;rft.au=Kocik+Jerzy&amp;rft_id=http%3A%2F%2Fwww.math.siu.edu%2Fkocik%2Fpapers%2Fsimpson2.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-22"><span><b><a href="#cite_ref-22">^</a></b></span> <span>Berman, S. DalleMule, L.  Greene, M., Lucker, J. (2012), "<a rel="nofollow" href="http://www.statslife.org.uk/the-statistics-dictionary/2012-simpson-s-paradox-a-cautionary-tale-in-advanced-analytics">Simpson's Paradox: A Cautionary Tale in Advanced Analytics</a> <a rel="nofollow" href="https://web.archive.org/web/20200510171740/https://www.statslife.org.uk/the-statistics-dictionary/2012-simpson-s-paradox-a-cautionary-tale-in-advanced-analytics">Archived</a> 2020-05-10 at the <a href="https://en.wikipedia.org/wiki/Wayback_Machine" title="Wayback Machine">Wayback Machine</a>", <i><a href="https://en.wikipedia.org/wiki/Significance_(magazine)" title="Significance (magazine)">Significance</a></i>.</span>
</li>
<li id="cite_note-23"><span><b><a href="#cite_ref-23">^</a></b></span> <span>
<cite id="CITEREFMarios_G._PavlidesMichael_D._Perlman2009">Marios G. Pavlides &amp; Michael D. Perlman (August 2009). "How Likely is Simpson's Paradox?". <i><a href="https://en.wikipedia.org/wiki/The_American_Statistician" title="The American Statistician">The American Statistician</a></i>. <b>63</b> (3): 226‚Äì233. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1198%2Ftast.2009.09007">10.1198/tast.2009.09007</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:17481510">17481510</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Statistician&amp;rft.atitle=How+Likely+is+Simpson%27s+Paradox%3F&amp;rft.volume=63&amp;rft.issue=3&amp;rft.pages=226-233&amp;rft.date=2009-08&amp;rft_id=info%3Adoi%2F10.1198%2Ftast.2009.09007&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A17481510%23id-name%3DS2CID&amp;rft.au=Marios+G.+Pavlides&amp;rft.au=Michael+D.+Perlman&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-24"><span><b><a href="#cite_ref-24">^</a></b></span> <span>Kock, N. (2015). <a rel="nofollow" href="http://cits.tamiu.edu/kock/pubs/journals/2015JournalIJeC/Kock_2015_IJeC_SimpPdox.pdf">How likely is Simpson's paradox in path models?</a> International Journal of e-Collaboration, 11(1), 1‚Äì7.</span>
</li>
<li id="cite_note-25"><span><b><a href="#cite_ref-25">^</a></b></span> <span><cite id="CITEREFNortonDivine2015">Norton, H. James; Divine, George (August 2015). <a rel="nofollow" href="https://doi.org/10.1111%2Fj.1740-9713.2015.00844.x">"Simpson's paradox ... and how to avoid it"</a>. <i>Significance</i>. <b>12</b> (4): 40‚Äì43. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1111%2Fj.1740-9713.2015.00844.x">10.1111/j.1740-9713.2015.00844.x</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Significance&amp;rft.atitle=Simpson%27s+paradox+...+and+how+to+avoid+it&amp;rft.volume=12&amp;rft.issue=4&amp;rft.pages=40-43&amp;rft.date=2015-08&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1740-9713.2015.00844.x&amp;rft.aulast=Norton&amp;rft.aufirst=H.+James&amp;rft.au=Divine%2C+George&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1111%252Fj.1740-9713.2015.00844.x&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-26"><span><b><a href="#cite_ref-26">^</a></b></span> <span><cite id="CITEREFPearl2014">Pearl, Judea (2014). "Understanding Simpson's Paradox". <i>The American Statistician</i>. <b>68</b> (1): 8‚Äì13. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.2139%2Fssrn.2343788">10.2139/ssrn.2343788</a>. <a href="https://en.wikipedia.org/wiki/S2CID_(identifier)" title="S2CID (identifier)">S2CID</a>&nbsp;<a rel="nofollow" href="https://api.semanticscholar.org/CorpusID:2626833">2626833</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Statistician&amp;rft.atitle=Understanding+Simpson%27s+Paradox&amp;rft.volume=68&amp;rft.issue=1&amp;rft.pages=8-13&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.2139%2Fssrn.2343788&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A2626833%23id-name%3DS2CID&amp;rft.aulast=Pearl&amp;rft.aufirst=Judea&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-27"><span><b><a href="#cite_ref-27">^</a></b></span> <span><cite id="CITEREFPearl1993">Pearl, Judea (1993). <a rel="nofollow" href="https://doi.org/10.1214%2Fss%2F1177010894">"Graphical Models, Causality, and Intervention"</a>. <i>Statistical Science</i>. <b>8</b> (3): 266‚Äì269. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1214%2Fss%2F1177010894">10.1214/ss/1177010894</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Statistical+Science&amp;rft.atitle=Graphical+Models%2C+Causality%2C+and+Intervention&amp;rft.volume=8&amp;rft.issue=3&amp;rft.pages=266-269&amp;rft.date=1993&amp;rft_id=info%3Adoi%2F10.1214%2Fss%2F1177010894&amp;rft.aulast=Pearl&amp;rft.aufirst=Judea&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1214%252Fss%252F1177010894&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-pearl-bow-28"><span>^ <a href="#cite_ref-pearl-bow_28-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-pearl-bow_28-1"><sup><i><b>b</b></i></sup></a></span> <span><cite id="CITEREFPearlMackenzie2018">Pearl, J.; Mackenzie, D. (2018). <i>The Book of Why: The New Science of Cause and Effect</i>. New York, NY: Basic Books.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Book+of+Why%3A+The+New+Science+of+Cause+and+Effect&amp;rft.place=New+York%2C+NY&amp;rft.pub=Basic+Books&amp;rft.date=2018&amp;rft.aulast=Pearl&amp;rft.aufirst=J.&amp;rft.au=Mackenzie%2C+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-29"><span><b><a href="#cite_ref-29">^</a></b></span> <span><cite id="CITEREFShpitserPearl2006">Shpitser, I.; Pearl, J. (2006). Dechter, R.; Richardson, T.S. (eds.). "Identification of Conditional Interventional Distributions". <i>Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence</i>. Corvallis, OR: AUAI Press: 437‚Äì444.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Twenty-Second+Conference+on+Uncertainty+in+Artificial+Intelligence&amp;rft.atitle=Identification+of+Conditional+Interventional+Distributions&amp;rft.pages=437-444&amp;rft.date=2006&amp;rft.aulast=Shpitser&amp;rft.aufirst=I.&amp;rft.au=Pearl%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-30"><span><b><a href="#cite_ref-30">^</a></b></span> <span><cite id="CITEREFBlyth1972">Blyth, Colin R. (June 1972). <a rel="nofollow" href="http://www.tandfonline.com/doi/abs/10.1080/01621459.1972.10482387">"On Simpson's Paradox and the Sure-Thing Principle"</a>. <i>Journal of the American Statistical Association</i>. <b>67</b> (338): 364‚Äì366. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1080%2F01621459.1972.10482387">10.1080/01621459.1972.10482387</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0162-1459">0162-1459</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Statistical+Association&amp;rft.atitle=On+Simpson%27s+Paradox+and+the+Sure-Thing+Principle&amp;rft.volume=67&amp;rft.issue=338&amp;rft.pages=364-366&amp;rft.date=1972-06&amp;rft_id=info%3Adoi%2F10.1080%2F01621459.1972.10482387&amp;rft.issn=0162-1459&amp;rft.aulast=Blyth&amp;rft.aufirst=Colin+R.&amp;rft_id=http%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Fabs%2F10.1080%2F01621459.1972.10482387&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-31"><span><b><a href="#cite_ref-31">^</a></b></span> <span><cite id="CITEREFGreenland2021">Greenland, Sander (2021-11-01). <a rel="nofollow" href="https://www.jclinepi.com/article/S0895-4356(21)00182-7/fulltext">"Noncollapsibility, confounding, and sparse-data bias. Part 2: What should researchers make of persistent controversies about the odds ratio?"</a>. <i>Journal of Clinical Epidemiology</i>. <b>139</b>: 264‚Äì268. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<span title="Freely accessible"><a rel="nofollow" href="https://doi.org/10.1016%2Fj.jclinepi.2021.06.004">10.1016/j.jclinepi.2021.06.004</a></span>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/0895-4356">0895-4356</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/34119647">34119647</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Clinical+Epidemiology&amp;rft.atitle=Noncollapsibility%2C+confounding%2C+and+sparse-data+bias.+Part+2%3A+What+should+researchers+make+of+persistent+controversies+about+the+odds+ratio%3F&amp;rft.volume=139&amp;rft.pages=264-268&amp;rft.date=2021-11-01&amp;rft.issn=0895-4356&amp;rft_id=info%3Apmid%2F34119647&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jclinepi.2021.06.004&amp;rft.aulast=Greenland&amp;rft.aufirst=Sander&amp;rft_id=https%3A%2F%2Fwww.jclinepi.com%2Farticle%2FS0895-4356%2821%2900182-7%2Ffulltext&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
<li id="cite_note-32"><span><b><a href="#cite_ref-32">^</a></b></span> <span><cite id="CITEREFHern√°nClaytonKeiding2011">Hern√°n, Miguel A.; Clayton, David; Keiding, Niels (June 2011). <a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3147074">"The Simpson's paradox unraveled"</a>. <i>International Journal of Epidemiology</i>. <b>40</b> (3): 780‚Äì785. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" title="Doi (identifier)">doi</a>:<a rel="nofollow" href="https://doi.org/10.1093%2Fije%2Fdyr041">10.1093/ije/dyr041</a>. <a href="https://en.wikipedia.org/wiki/ISSN_(identifier)" title="ISSN (identifier)">ISSN</a>&nbsp;<a rel="nofollow" href="https://www.worldcat.org/issn/1464-3685">1464-3685</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" title="PMC (identifier)">PMC</a>&nbsp;<span title="Freely accessible"><a rel="nofollow" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3147074">3147074</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" title="PMID (identifier)">PMID</a>&nbsp;<a rel="nofollow" href="https://pubmed.ncbi.nlm.nih.gov/21454324">21454324</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Epidemiology&amp;rft.atitle=The+Simpson%27s+paradox+unraveled&amp;rft.volume=40&amp;rft.issue=3&amp;rft.pages=780-785&amp;rft.date=2011-06&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3147074%23id-name%3DPMC&amp;rft.issn=1464-3685&amp;rft_id=info%3Apmid%2F21454324&amp;rft_id=info%3Adoi%2F10.1093%2Fije%2Fdyr041&amp;rft.aulast=Hern%C3%A1n&amp;rft.aufirst=Miguel+A.&amp;rft.au=Clayton%2C+David&amp;rft.au=Keiding%2C+Niels&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3147074&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASimpson%27s+paradox"></span></span>
</li>
</ol></div>
<h2><span id="Bibliography">Bibliography</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=13" title="Edit section: Bibliography"><span>edit</span></a><span>]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Leila_Schneps" title="Leila Schneps">Leila Schneps</a> and <a href="https://en.wikipedia.org/wiki/Coralie_Colmez" title="Coralie Colmez">Coralie Colmez</a>, <i><a href="https://en.wikipedia.org/wiki/Math_on_Trial" title="Math on Trial">Math on trial. How numbers get used and abused in the courtroom</a></i>, Basic Books, 2013. <a href="https://en.wikipedia.org/wiki/ISBN_(identifier)" title="ISBN (identifier)">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-465-03292-1" title="Special:BookSources/978-0-465-03292-1">978-0-465-03292-1</a>. (Sixth chapter: "Math error number 6: Simpson's paradox. The Berkeley sex bias case: discrimination detection").</li></ul>
<h2><span id="External_links">External links</span><span><span>[</span><a href="https://en.wikipedia.org/w/index.php?title=Simpson%27s_paradox&amp;action=edit&amp;section=14" title="Edit section: External links"><span>edit</span></a><span>]</span></span></h2>

<ul><li><a rel="nofollow" href="http://plato.stanford.edu/entries/paradox-simpson/">Simpson's Paradox</a> at the <a href="https://en.wikipedia.org/wiki/Stanford_Encyclopedia_of_Philosophy" title="Stanford Encyclopedia of Philosophy">Stanford Encyclopedia of Philosophy</a>, by Jan Sprenger and Naftali Weinberger.</li>
<li><a rel="nofollow" href="http://ed.ted.com/lessons/how-statistics-can-be-misleading-mark-liddell">How statistics can be misleading ‚Äì Mark Liddell</a> ‚Äì TED-Ed video and lesson.</li>
<li><a href="https://en.wikipedia.org/wiki/Judea_Pearl" title="Judea Pearl">Pearl, Judea</a>, <a rel="nofollow" href="https://ftp.cs.ucla.edu/pub/stat_ser/r414.pdf">"Understanding Simpson‚Äôs Paradox"</a> (PDF)</li>
<li><a rel="nofollow" href="http://www.cut-the-knot.org/Curriculum/Algebra/SimpsonParadox.shtml">Simpson's Paradox</a>, a short article by <a href="https://en.wikipedia.org/wiki/Alexander_Bogomolny" title="Alexander Bogomolny">Alexander Bogomolny</a> on the vector interpretation of Simpson's paradox</li>
<li><a rel="nofollow" href="https://www.wsj.com/articles/SB125970744553071829">The Wall Street Journal column "The Numbers Guy"</a> for December 2, 2009 dealt with recent instances of Simpson's paradox in the news. Notably a Simpson's paradox in the comparison of unemployment rates of the 2009 recession with the 1983 recession.</li>
<li><a rel="nofollow" href="http://www.stateoftheusa.org/content/at-the-plate-a-statistical-puz.php">At the Plate, a Statistical Puzzler: Understanding Simpson's Paradox</a> by Arthur Smith, August 20, 2010</li>
<li><a rel="nofollow" href="https://www.youtube.com/watch?v=ebEkn-BiW5k">Simpson's Paradox</a>, a video by Henry Reich of <a href="https://en.wikipedia.org/wiki/MinutePhysics" title="MinutePhysics">MinutePhysics</a></li></ul>

<!-- 
NewPP limit report
Parsed by mw1415
Cached time: 20240311132754
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‚Äêrevision‚Äêsha1, show‚Äêtoc]
CPU time usage: 0.528 seconds
Real time usage: 0.697 seconds
Preprocessor visited node count: 4454/1000000
Post‚Äêexpand include size: 60342/2097152 bytes
Template argument size: 2012/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‚Äêexpand size: 88383/5000000 bytes
Lua time usage: 0.321/10.000 seconds
Lua memory usage: 14136232/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  577.815      1 -total
 39.21%  226.581      1 Template:Reflist
 27.95%  161.473     11 Template:Annotated_link
 25.33%  146.343     19 Template:Cite_journal
 14.82%   85.614      1 Template:Short_description
  8.91%   51.459      2 Template:Pagetype
  6.43%   37.148      5 Template:Isbn
  5.77%   33.341      1 Template:Commons_category
  5.41%   31.231      1 Template:Sister_project
  5.09%   29.420      1 Template:Side_box
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:46096-0!canonical and timestamp 20240311132754 and revision id 1192014464. Rendering was triggered because: page-view
 -->
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing whistleblower found dead in US (952 pts)]]></title>
            <link>https://www.bbc.com/news/business-68534703</link>
            <guid>39673589</guid>
            <pubDate>Mon, 11 Mar 2024 21:45:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/business-68534703">https://www.bbc.com/news/business-68534703</a>, See on <a href="https://news.ycombinator.com/item?id=39673589">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header data-component="legacy-header-block"></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg.webp 976w" type="image/webp"><img alt="A Boeing 787 Dreamliner plane at the Paris Air Show" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/D654/production/_132886845_gettyimages-1258810033.jpg" width="976" height="549" loading="eager"></picture></span><span role="text"><span>Image source, </span>Getty Images</span></p></figure></div><div data-component="text-block"><p><b>A former Boeing employee known for raising concerns about the firm's production standards has been found dead in the US. </b></p></div><div data-component="text-block"><p>John Barnett had worked for Boeing for 32 years, until his retirement in 2017. </p></div><div data-component="text-block"><p>In the days before his death, he had been giving evidence in a whistleblower lawsuit against the company. </p></div><div data-component="text-block"><p>Boeing said it was saddened to hear of Mr Barnett's passing. The Charleston County coroner confirmed his death to the BBC on Monday. </p></div><div data-component="text-block"><p>It said the 62-year-old had died from a "self-inflicted" wound on 9 March and police were investigating. </p></div><div data-component="text-block"><p>Mr Barnett had worked for the US plane giant for 32 years, until his retirement in 2017 on health grounds. </p></div><div data-component="text-block"><p>From 2010, he worked as a quality manager at the North Charleston plant making the 787 Dreamliner, a state-of-the-art airliner used mainly on long-haul routes. </p></div><div data-component="text-block"><p><a href="https://www.bbc.co.uk/news/business-50293927">In 2019, Mr Barnett told the BBC</a> that under-pressure workers had been deliberately fitting sub-standard parts to aircraft on the production line. </p></div><div data-component="text-block"><p>He also said he had uncovered serious problems with oxygen systems, which could mean one in four breathing masks would not work in an emergency.</p></div><div data-component="text-block"><p>He said soon after starting work in South Carolina he had become concerned that the push to get new aircraft built meant the assembly process was rushed and safety was compromised, something the company denied.</p></div><div data-component="image-block"><figure><p><span><span></span></span><span role="text"><span>Image source, </span>John Barnett</span></p><figcaption><span>Image caption, </span><p>John Barnett was a former quality control manager at Boeing</p></figcaption></figure></div><div data-component="text-block"><p>He later told the BBC that workers had failed to follow procedures intended to track components through the factory, allowing defective components to go missing. </p></div><div data-component="text-block"><p>He said in some cases, sub-standard parts had even been removed from scrap bins and fitted to planes that were being built to prevent delays on the production line.</p></div><div data-component="text-block"><p>He also claimed that tests on emergency oxygen systems due to be fitted to the 787 showed a failure rate of 25%, meaning that one in four could fail to deploy in a real-life emergency.</p></div><div data-component="text-block"><p>Mr Barnett said he had alerted managers to his concerns, but no action had been taken. </p></div><div data-component="text-block"><p>Boeing denied his assertions. However, a 2017 review by the US regulator, the Federal Aviation Administration (FAA), did uphold some of Mr Barnett's concerns.</p></div><div data-component="text-block"><p>It established that the location of at least 53 "non-conforming" parts in the factory was unknown, and that they were considered lost. Boeing was ordered to take remedial action.</p></div><div data-component="text-block"><p>On the oxygen cylinders issue, the company said that in 2017 it had "identified some oxygen bottles received from the supplier that were not deploying properly". But it denied that any of them were actually fitted on aircraft.</p></div><div data-component="text-block"><p>After retiring, he embarked on a long-running legal action against the company. </p></div><div data-component="text-block"><p>He accused it of denigrating his character and hampering his career because of the issues he pointed out - charges rejected by Boeing.</p></div><div data-component="text-block"><p>At the time of his death, Mr Barnett had been in Charleston for legal interviews linked to that case.</p></div><div data-component="text-block"><p>Last week, he gave a formal deposition in which he was questioned by Boeing's lawyers, before being cross-examined by his own counsel.</p></div><div data-component="text-block"><p>He had been due to undergo further questioning on Saturday. When he did not appear, enquiries were made at his hotel. </p></div><div data-component="text-block"><p>He was subsequently found dead in his truck in the hotel car park. </p></div><div data-component="text-block"><p>Speaking to the BBC, his lawyer described his death as "tragic". </p></div><div data-component="text-block"><p>In a statement Boeing said: "We are saddened by Mr. Barnett's passing, and our thoughts are with his family and friends."</p></div><div data-component="text-block"><p>His death comes at a time when production standards at both Boeing and its key supplier Spirit Aerosystems are under intense scrutiny.</p></div><div data-component="text-block"><p>This follows an incident in early January when an unused emergency exit door blew off a brand-new Boeing 737 Max shortly after take-off from Portland International Airport.</p></div><div data-component="text-block"><p>A preliminary report from the US National Transportation Safety Board suggested that four key bolts, designed to hold the door securely in place, were not fitted.</p></div><div data-component="text-block"><p>Last week, the FAA said a six-week audit of the company had found "multiple instances where the company allegedly failed to comply with manufacturing quality control requirements".</p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Webb and Hubble confirm Universe's expansion rate (563 pts)]]></title>
            <link>https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate</link>
            <guid>39673087</guid>
            <pubDate>Mon, 11 Mar 2024 20:49:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate">https://www.esa.int/ESA_Multimedia/Images/2024/03/Webb_Hubble_confirm_Universe_s_expansion_rate</a>, See on <a href="https://news.ycombinator.com/item?id=39673087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="modal__tab-content--details">
				<p>The rate at which the Universe is expanding, known as the Hubble constant, is one of the fundamental parameters for understanding the evolution and ultimate fate of the cosmos. However, a persistent difference, called the Hubble Tension, is seen between the value of the constant measured with a wide range of independent distance indicators and its value predicted from the afterglow of the Big Bang. The NASA/ESA/CSA James Webb Space Telescope has confirmed that the Hubble Space Telescope‚Äôs keen eye was right all along, erasing any lingering doubt about Hubble‚Äôs measurements.</p><p>This image of NGC 5468, a galaxy located about 130 million light-years from Earth, combines data from the <a href="https://www.esa.int/Science_Exploration/Space_Science/Hubble_overview" target="_blank">Hubble</a> and <a href="https://www.esa.int/Science_Exploration/Space_Science/Webb" target="_blank">James Webb space</a> telescopes. This is the most distant galaxy in which Hubble has identified Cepheid variable stars. These stars are important milepost markers for measuring the expansion rate of the Universe. The distance calculated from Cepheids has been cross-correlated with a Type Ia supernova in the galaxy. Type Ia supernovae are so bright they are used to measure cosmic distances far beyond the range of the Cepheids, extending measurements of the Universe‚Äôs expansion rate deeper into space.</p><p><a href="https://www.esa.int/Science_Exploration/Space_Science/Webb/Webb_Hubble_confirm_Universe_s_expansion_rate" target="_blank">Read more</a></p><p>[<i>Image description</i>: A face-on spiral galaxy with four spiral arms that curve outward in a counterclockwise direction. The spiral arms are filled with young, blue stars and peppered with purplish star-forming regions that appear as small blobs. The middle of the galaxy is much brighter and more yellowish, and has a distinct narrow linear bar angled from 11 o‚Äôclock to 5 o‚Äôclock. Dozens of red background galaxies are scattered across the image. The background of space is black.]</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Miles Davis hired John Coltrane (112 pts)]]></title>
            <link>https://www.honest-broker.com/p/how-miles-davis-hired-john-coltrane</link>
            <guid>39673014</guid>
            <pubDate>Mon, 11 Mar 2024 20:42:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.honest-broker.com/p/how-miles-davis-hired-john-coltrane">https://www.honest-broker.com/p/how-miles-davis-hired-john-coltrane</a>, See on <a href="https://news.ycombinator.com/item?id=39673014">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png" width="1057" height="352" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:352,&quot;width&quot;:1057,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:27483,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F69d41606-e855-4069-b4c1-3ea154d6fc45_1057x352.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>I‚Äôm sharing below a section from James Kaplan‚Äôs outstanding new book </span><em><a href="https://www.penguinrandomhouse.com/books/592345/3-shades-of-blue-by-james-kaplan/" rel="">3 Shades of Blue</a></em><span>. Kaplan focuses on three artists who had a transformative impact on jazz: Miles Davis, John Coltrane, and Bill Evans.</span></p><p>The stature of these artists is sufficient reason to pay attention to this well-researched work. But Kaplan is especially skilled as a storyteller, and has delivered the most readable narrative account to date of a milestone moment in American music.</p><p>Below he tells how Miles Davis hired John Coltrane. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg" width="658" height="1000" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1000,&quot;width&quot;:658,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:57429,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff875f934-1d4b-4c6f-8649-7b260bc2cb7b_658x1000.jpeg 1456w" sizes="100vw"></picture></div></a></figure></div><p>By James Kaplan</p><h5><span>From </span><em><a href="https://www.penguinrandomhouse.com/books/592345/3-shades-of-blue-by-james-kaplan/" rel="">3 Shades of Blue: Miles Davis, John Coltrane, Bill Evans, and the Lost Empire of Cool</a></em><span> (Penguin 2024)</span></h5><p>After Johnny Hodges fired him, John Coltrane was back in Philadelphia and spottily employed‚Äîso spottily that he was without a job for New Year‚Äôs Eve of 1954. And so, out of pity, and in gratitude for some musical pointers Coltrane had given him, a trumpeter friend named Ted Curson took him along to a gig in Vineland, New Jersey. Curson, who went on to have a solid jazz career, remembered the evening decades later. ‚ÄúHe played ‚ÄòNancy with the Laughing Face,‚Äô I‚Äôll never forget that,‚Äù he said. ‚ÄúI never heard anything so great, so intense, with so much feeling.‚Äù</p><p><span>Along with all his other musical strengths, Coltrane had a beautiful way with ballads. But his work problems had nothing to do with musical skill. He was a heroin addict, with the same set of handicaps that had crippled Miles Davis‚Äôs career from 1949 to 1954: unreliability and general dishevelment made steady work next to impossible. Leading a working band was out of the question. So he picked up whatever gigs he could, when he could. One now-and-then job in 1955 was as part of an </span><em>ad hoc</em><span> trio called the Hi-Tones, with the great jazz organist Shirley Scott and the youngest Heath brother, Albert (‚ÄúTootie‚Äù), on drums. All three were dead-serious, technically sophisticated players. ‚ÄúWe were too musical for certain rooms,‚Äù Coltrane recalled.</span></p><p>‚ÄúColtrane was phenomenal even then,‚Äù Scott said. ‚ÄúWe played in and around Philadelphia on and off for at least a year‚Ä¶.We played bebop (including ‚ÄòHalf Nelson‚Äô and ‚ÄòGroovin‚Äô High‚Äô), straight-ahead music‚Ä¶.We did rehearse a lot, and we had a lot of arrangements, most of them John‚Äôs.‚Äù </p><p>Then came the call from Philly Joe Jones.</p><p><span>Philly Joe‚ÄîJoseph Rudolph Jones had taken on the nickname to differentiate himself from the great swing drummer Jo Jones‚Äîwent way back with Coltrane: the two had started gigging around town with Percy Heath soon after Coltrane‚Äôs discharge from the navy. Three years older than the saxophonist, Jones was as different from the quiet, serious, monomaniacal Trane as different could be: he was a brilliant, highly articulate extrovert, a skilled tap dancer and mimic. He was also a serious heroin addict. ‚ÄúPhilly Joe Jones was the Babe </span><em>Ruth</em><span> of junkies,‚Äù one longtime observer of jazz told me. ‚ÄúI mean, he was </span><em>the</em><span> junkie. He was </span><em>the</em><span> cat.‚Äù</span></p><p>Jones and Miles also went back: for a while during Davis‚Äôs dark years, the two had barnstormed around the Midwest together, Philly Joe going ahead of Miles to pick up local sidemen in each city. The results were consistently disappointing. Still, it had fed them for a while‚Äîfed the monkey on their backs, too.</p><p><span>Miles never made a secret of the fact that Philly Joe was his favorite drummer. ‚ÄúHe </span><em>knew</em><span> everything I was going to do, everything I was going to play,‚Äù Davis said; ‚Äúhe anticipated me, felt what I was thinking.‚Äù Jones had a special rim shot that he liked to hit immediately after a Miles solo: it became known around jazz as a Philly lick. Soon, other musicians began asking their drummers for it, too. ‚ÄúI left a lot of space in the music for Philly to fill up,‚Äù Miles said. ‚ÄúPhilly Joe was the kind of drummer that I knew my music had to have. (Even after he left I would listen for a little of Philly Joe in all the drummers I had later.)‚Äù</span></p><p>Miles had tapped Jones to contact Coltrane much as he‚Äôd deputized the drummer to find sidemen back in the day, only in this case the need was more urgent: Jack Whittemore had set up a tour for the Miles Davis Quintet‚Äî Baltimore, Detroit, then back to New York at Birdland and Caf√© Bohemia‚Äîbut with Sonny Rollins and now John Gilmore out of the picture, the quintet was a quartet.</p><p>Coltrane was working at Spider Kelly‚Äôs in Philadelphia with the organist Jimmy Smith when Philly Joe called and asked if he could come rehearse with the band. Recognizing that this could be his shot at the big time, Trane asked Smith for a few days off and went to New York, where things did not go well.</p><p>John Coltrane would ultimately become a jazz deity, by virtue of his supreme technical skills, his ceaseless exploration of the far bounds of the music, and the intense spirituality that informed his life and art. But in 1955 he was an awkward outsider, as far as possible from any kind of distinction in his field. (Even his heroin addiction‚Äîdesperate, furtive, ashamed‚Äîdidn‚Äôt fit into the cool model of jazz culture.) In auditioning for Miles he was virtually coming out from hiding, having spent the past decade freelancing around jazz‚Äôs seamy outskirts as he searched musically; yet even as his playing improved, he gained little faith in his own abilities. His ceaseless questing for musical and spiritual enlightenment filled him with questions about everything, especially music. And in reencountering a newly ascendant Miles Davis, he was coming up against the ultimate non-answerer.</p><p>‚ÄúMiles is sort of a strange guy,‚Äù he would tell Fran√ßois Postif in 1961. ‚ÄúHe doesn‚Äôt talk a lot, and he rarely discusses music. You always have the impression that he‚Äôs in a bad mood, and that he‚Äôs not interested in or affected by what other people are doing. It‚Äôs very hard, in a situation like that, to know exactly what you should do‚Ä¶.‚Äù</p><p>Two good things quickly became apparent in those September tryouts: that Coltrane‚Äôs abilities as a player had advanced considerably since that long-ago gig at the Audubon Ballroom, and that he knew Miles‚Äôs repertoire. What was less good, Davis recalled many years later, was that ‚ÄúTrane like to ask all these motherfucking questions‚Ä¶about what he should or shouldn‚Äôt play. Man, fuck that shit; to me he was a professional musician and I have always wanted whoever played with me to find their own place in the music. So my silence and evil looks probably turned him off.‚Äù</p><p>It was an odd business, this angry aloofness of Miles: on the one hand it seems to have been kind of worked up, put on like a vestment of fame and entitlement; on the other hand, at first he and Coltrane seem to have genuinely rubbed each other the wrong way. After a couple of days of rehearsing, the saxophonist told Davis he had to go back to Philadelphia, and left.</p><p>If he had secretly wanted Miles to cut out the unpleasant behavior and hire him, Coltrane couldn‚Äôt have come up with a better strategy than walking away. The first date on the tour, at Baltimore‚Äôs Club Las Vegas, was rapidly approaching, and Davis still had a hole in the lineup. Coltrane could play, and he knew all the tunes: ‚ÄúWe practically had to beg him to join the band,‚Äù Miles recalled. Coltrane joined.</p><p><span>For more information on </span><em>3 Shades of Blue</em><span>, </span><a href="https://www.penguinrandomhouse.com/books/592345/3-shades-of-blue-by-james-kaplan/" rel="">click here</a><span>. </span></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenPlotter (149 pts)]]></title>
            <link>https://openmarine.net/openplotter</link>
            <guid>39672901</guid>
            <pubDate>Mon, 11 Mar 2024 20:29:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openmarine.net/openplotter">https://openmarine.net/openplotter</a>, See on <a href="https://news.ycombinator.com/item?id=39672901">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      

<article role="article">
  
  <div>
        
            <div><p>Tool kit to be used as navigational aid in small and medium-length boats. It can be installed as a Raspberry OS disk image or by parts on your favourite Debian-based Linux distribution. We offer some Raspberry OS images with the most used applications installed and configured but you can also customize your image with the <a href="https://openmarine.net/openplotter/a-la-carte">OpenPlotter √Ä la Carte</a> tool.</p><p><img data-entity-uuid="ea24b044-a277-4d36-898d-eefbd216ac7d" data-entity-type="file" src="https://openmarine.net/sites/default/files/inline-images/Screenshot%20from%202023-01-23%2017-51-51.png" alt="" width="1024" height="768" loading="lazy"></p></div>
      
  <div>
    <p>Features</p>
              <div><ul><li><strong>Chart Plotter - </strong>Chart a course and track your position using OpenCPN, a concise and robust chart plotter navigation software designed to be used at the helm station of your boat while underway. You can also run AvNav as server to have a chart plotter in any device remotely connected to it using a web browser.</li><li><strong>Weather - </strong>Download and display GRIB files using XyGrib.</li><li><strong>NMEA 0183 - </strong>Connect your NMEA 0183 devices to receive and send data.</li><li><strong>NMEA 2000 - </strong>Connect your NMEA 2000 network to receive and send data.</li><li><strong>Seatalk<sup>1</sup> - </strong>Connect your old Seatalk<sup>1</sup> network to receive data.</li><li><strong>Signal K -</strong> The free and open source universal marine data exchange format.</li><li><strong>Access point - </strong>Share data with laptops, tablets, phones‚Ä¶</li><li><strong>Headless - </strong>You can connect OpenPlotter to any HDMI monitor and/or access to OpenPlotter desktop from the cockpit through your mobile devices.</li><li><strong>Dashboards - </strong>Customize your instrument panels to visualize data or create charts to see its evolution.</li><li><strong>AIS - </strong>Build open source AIS receivers/transmitters.</li><li><strong>Compass - </strong>Get magnetic heading, heel and trim using cheap Inertial Measurement Units (IMU).</li><li><strong>Autopilot - </strong>Full pypilot integration.</li><li><strong>Sensors - </strong>Easily connect all kinds of sensors (temperature, pressure, humidity, voltage, current, luminance, tank level, RPM, doors‚Ä¶)</li><li><strong>Notifications - </strong>Set thresholds for any parameter to trigger visual and sound notifications or trigger multiple custom actions.</li><li><strong>IoT - </strong>Monitor what happens on your boat when you are not there or activate devices remotely.</li></ul><p><br><img src="https://openmarine.net/sites/default/files/inline-images/op1.jpg" data-entity-uuid="93d7b49c-3d41-404c-92a9-07a393fa9941" data-entity-type="file" alt="" width="960" height="540" loading="lazy">&nbsp;<br><img src="https://openmarine.net/sites/default/files/inline-images/op2.jpg" data-entity-uuid="cc989c01-cd9f-4d4f-9f70-ba23706578c1" data-entity-type="file" width="960" height="540" loading="lazy"></p></div>
          </div>

  </div>
  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's threat model for post-quantum cryptography (208 pts)]]></title>
            <link>https://bughunters.google.com/blog/5108747984306176/google-s-threat-model-for-post-quantum-cryptography</link>
            <guid>39672583</guid>
            <pubDate>Mon, 11 Mar 2024 19:56:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bughunters.google.com/blog/5108747984306176/google-s-threat-model-for-post-quantum-cryptography">https://bughunters.google.com/blog/5108747984306176/google-s-threat-model-for-post-quantum-cryptography</a>, See on <a href="https://news.ycombinator.com/item?id=39672583">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Diffusion models from scratch, from a new theoretical perspective (324 pts)]]></title>
            <link>https://www.chenyang.co/diffusion.html</link>
            <guid>39672450</guid>
            <pubDate>Mon, 11 Mar 2024 19:43:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chenyang.co/diffusion.html">https://www.chenyang.co/diffusion.html</a>, See on <a href="https://news.ycombinator.com/item?id=39672450">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
<article>
<h2>Diffusion models from scratch, from a new theoretical perspective</h2>
<section>
\[\newcommand{\Kset}{\mathcal{K}}
\newcommand{\distK}{ {\rm dist}_{\Kset} }
\newcommand{\projK}{ {\rm proj}_{\Kset} }
\newcommand{\eps}{\epsilon}
\newcommand{\Loss}{\mathcal{L}}
\newcommand{\norm}[1]{\left\lVert #1 \right\lVert}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator{\softmin}{softmin}
\DeclareMathOperator{\distop}{dist}\]
<p>Diffusion models have recently produced impressive results in generative
modeling, in particular sampling from multimodal distributions. Not only has
diffusion models seen widespread adoption in text-to-image generation tools such
as <a href="https://github.com/Stability-AI/stablediffusion">Stable Diffusion</a>, they
also excel in other application domains such as
<a href="https://text-to-audio.github.io/">audio</a>/<a href="https://openai.com/research/video-generation-models-as-world-simulators">video</a>/<a href="https://zero123.cs.columbia.edu/">3D</a>
generation, <a href="https://www.nature.com/articles/s41586-023-06415-8">protein
design</a>, <a href="https://diffusion-policy.cs.columbia.edu/">robotics path
planning</a>, all of which require
sampling from multimodal distributions.</p>
<p>This tutorial aims to introduce diffusion models from an optimization
perspective as introduced in <a href="https://arxiv.org/abs/2306.04848">our paper</a>. It will go over both
theory and code, using the theory to explain how to implement diffusion models
from scratch. By the end of the tutorial, you will learn how to implement
training and sampling code for a toy dataset, which will also work for larger
datasets and models.</p>
<p>In this tutorial we will mainly reference code from
<a href="https://github.com/yuanchenyang/smalldiffusion"><code>smalldiffusion</code></a>. For
pedagogical purposes, the code presented here will be simplified from the
<a href="https://github.com/yuanchenyang/smalldiffusion/blob/main/src/smalldiffusion/">original library code</a>, which is on its own well-commented
and easy to read.</p>
<h4 id="training-diffusion-models">Training diffusion models</h4>
<p>Diffusion models aim to generate samples from a set that is learned from
training examples, which we will denote by \(\mathcal{K}\). For example, if we
want to generate images, \(\mathcal{K} \subset \mathbb{R}^{c\times h \times w}\)
is the set of pixel values that correspond to realistic images. Diffusion models
also work for \(\mathcal{K}\) corresponding to modalities other than images,
such as audio, video, robot trajectories, and even in discrete domains such as
text generation.</p>
<p>In a nutshell, diffusion models are trained by:</p>
<ol>
<li>Sampling \(x_0 \sim \mathcal{K}\), noise level \(\sigma \sim [\sigma_\min,
\sigma_\max]\), noise \(\epsilon \sim N(0, I)\)</li>
<li>Generating noisy data \(x_\sigma = x_0 + \sigma \epsilon\)</li>
<li>Predicting \(\epsilon\) (direction of noise) from \(x_\sigma\) by minimizing squared loss</li>
</ol>
<p>This amounts to training a \(\theta\)-parameterized neural network
\(\epsilon_\theta(x, \sigma)\), by minimizing the loss function</p>
\[\Loss(\theta) = \mathop{\mathbb{E}}
\lVert\epsilon_\theta(x_0 + \sigma_t \epsilon, \sigma_t) - \epsilon \lVert^2\]
<p>In practice, this is done by the following simple <code>training_loop</code>:</p>
<div><pre><code><span>def</span> <span>training_loop</span><span>(</span><span>loader</span>  <span>:</span> <span>DataLoader</span><span>,</span>
                  <span>model</span>   <span>:</span> <span>nn</span><span>.</span><span>Module</span><span>,</span>
                  <span>schedule</span><span>:</span> <span>Schedule</span><span>,</span>
                  <span>epochs</span>  <span>:</span> <span>int</span> <span>=</span> <span>10000</span><span>):</span>
    <span>optimizer</span> <span>=</span> <span>torch</span><span>.</span><span>optim</span><span>.</span><span>Adam</span><span>(</span><span>model</span><span>.</span><span>parameters</span><span>())</span>
    <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
        <span>for</span> <span>x0</span> <span>in</span> <span>loader</span><span>:</span>
            <span>optimizer</span><span>.</span><span>zero_grad</span><span>()</span>
            <span>sigma</span><span>,</span> <span>eps</span> <span>=</span> <span>generate_train_sample</span><span>(</span><span>x0</span><span>,</span> <span>schedule</span><span>)</span>
            <span>eps_hat</span> <span>=</span> <span>model</span><span>(</span><span>x0</span> <span>+</span> <span>sigma</span> <span>*</span> <span>eps</span><span>,</span> <span>sigma</span><span>)</span>
            <span>loss</span> <span>=</span> <span>nn</span><span>.</span><span>MSELoss</span><span>()(</span><span>eps_hat</span><span>,</span> <span>eps</span><span>)</span>
            <span>optimizer</span><span>.</span><span>backward</span><span>(</span><span>loss</span><span>)</span>
            <span>optimizer</span><span>.</span><span>step</span><span>()</span>
</code></pre></div>
<p>The training loop iterates over batches of <code>x0</code>, then samples noise level
<code>sigma</code> and noise vector <code>eps</code> using <code>generate_train_sample</code>:</p>
<div><pre><code><span>def</span> <span>generate_train_sample</span><span>(</span><span>x0</span><span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>,</span> <span>schedule</span><span>:</span> <span>Schedule</span><span>):</span>
    <span>sigma</span> <span>=</span> <span>schedule</span><span>.</span><span>sample_batch</span><span>(</span><span>x0</span><span>)</span>
    <span>eps</span> <span>=</span> <span>torch</span><span>.</span><span>randn_like</span><span>(</span><span>x0</span><span>)</span>
    <span>return</span> <span>sigma</span><span>,</span> <span>eps</span>
</code></pre></div>
<h5 id="noise-schedules">Noise schedules</h5>
<p>In practice, \(\sigma\) is not sampled uniformly from the interval \([\sigma_\min,
\sigma_\max]\), instead this interval is discretized into \(N\) distinct values
called a <em>\(\sigma\) schedule</em>: \(\{ \sigma_t \}_{t=1}^N\), and \(\sigma\) is instead
sampled uniformly from the \(N\) possible values of \(\sigma_t\). We define the
<code>Schedule</code> class that encapsulates the list of possible <code>sigmas</code>, and sample
from this list during training.</p>
<div><pre><code><span>class</span> <span>Schedule</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>sigmas</span><span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>):</span>
        <span>self</span><span>.</span><span>sigmas</span> <span>=</span> <span>sigmas</span>
    <span>def</span> <span>__getitem__</span><span>(</span><span>self</span><span>,</span> <span>i</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>return</span> <span>self</span><span>.</span><span>sigmas</span><span>[</span><span>i</span><span>]</span>
    <span>def</span> <span>__len__</span><span>(</span><span>self</span><span>)</span> <span>-&gt;</span> <span>int</span><span>:</span>
        <span>return</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>sigmas</span><span>)</span>
    <span>def</span> <span>sample_batch</span><span>(</span><span>self</span><span>,</span> <span>x0</span><span>:</span><span>torch</span><span>.</span><span>FloatTensor</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>return</span> <span>self</span><span>[</span><span>torch</span><span>.</span><span>randint</span><span>(</span><span>len</span><span>(</span><span>self</span><span>),</span> <span>(</span><span>x0</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],))].</span><span>to</span><span>(</span><span>x0</span><span>)</span>
</code></pre></div>
<p>In this tutorial, we will use a log-linear schedule defined below:</p>
<div><pre><code><span>class</span> <span>ScheduleLogLinear</span><span>(</span><span>Schedule</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>N</span><span>:</span> <span>int</span><span>,</span> <span>sigma_min</span><span>:</span> <span>float</span><span>=</span><span>0.02</span><span>,</span> <span>sigma_max</span><span>:</span> <span>float</span><span>=</span><span>10</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>(</span><span>torch</span><span>.</span><span>logspace</span><span>(</span><span>math</span><span>.</span><span>log10</span><span>(</span><span>sigma_min</span><span>),</span> <span>math</span><span>.</span><span>log10</span><span>(</span><span>sigma_max</span><span>),</span> <span>N</span><span>))</span>
</code></pre></div>
<p>Other commonly used schedules include <code>ScheduleDDPM</code> for pixel-space diffusion
models and <code>ScheduleLDM</code> for latent diffusion models such as
Stable Diffusion. The following plot compares these three schedules with default
parameters.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/schedule.png" alt="">
</p>
<figcaption>A comparison plot of different diffusion schedules</figcaption>
</figure>
<h5 id="toy-example">Toy example</h5>
<p>In this tutorial we will start with a toy dataset used in one of the first
diffusion papers <a href="https://arxiv.org/abs/1503.03585">[Sohl-Dickstein
et.al. 2015]</a>, where \(\Kset \subset \R^2\)
are points sampled from a spiral. We first construct and visualize this dataset:</p>
<div><pre><code><span>dataset</span> <span>=</span> <span>Swissroll</span><span>(</span><span>np</span><span>.</span><span>pi</span><span>/</span><span>2</span><span>,</span> <span>5</span><span>*</span><span>np</span><span>.</span><span>pi</span><span>,</span> <span>100</span><span>)</span>
<span>loader</span>  <span>=</span> <span>DataLoader</span><span>(</span><span>dataset</span><span>,</span> <span>batch_size</span><span>=</span><span>2048</span><span>)</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/swissroll.png" alt="">
</p>
<figcaption>Swissroll toy dataset</figcaption>
</figure>
<p>For this simple dataset, we can implement the denoiser using a multi-layer
perceptron (MLP):</p>
<div><pre><code><span>def</span> <span>get_sigma_embeds</span><span>(</span><span>sigma</span><span>):</span>
    <span>return</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>torch</span><span>.</span><span>sin</span><span>(</span><span>torch</span><span>.</span><span>log</span><span>(</span><span>sigma</span><span>)</span><span>/</span><span>2</span><span>),</span>
                      <span>torch</span><span>.</span><span>cos</span><span>(</span><span>torch</span><span>.</span><span>log</span><span>(</span><span>sigma</span><span>)</span><span>/</span><span>2</span><span>)],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>

<span>class</span> <span>TimeInputMLP</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dim</span><span>,</span> <span>hidden_dims</span><span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>()</span>
        <span>layers</span> <span>=</span> <span>[]</span>
        <span>for</span> <span>in_dim</span><span>,</span> <span>out_dim</span> <span>in</span> <span>pairwise</span><span>((</span><span>dim</span> <span>+</span> <span>2</span><span>,)</span> <span>+</span> <span>hidden_dims</span><span>):</span>
            <span>layers</span><span>.</span><span>extend</span><span>([</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>in_dim</span><span>,</span> <span>out_dim</span><span>),</span> <span>nn</span><span>.</span><span>GELU</span><span>()])</span>
        <span>layers</span><span>.</span><span>append</span><span>(</span><span>nn</span><span>.</span><span>Linear</span><span>(</span><span>hidden_dims</span><span>[</span><span>-</span><span>1</span><span>],</span> <span>dim</span><span>))</span>
        <span>self</span><span>.</span><span>net</span> <span>=</span> <span>nn</span><span>.</span><span>Sequential</span><span>(</span><span>*</span><span>layers</span><span>)</span>
        <span>self</span><span>.</span><span>input_dims</span> <span>=</span> <span>(</span><span>dim</span><span>,)</span>

    <span>def</span> <span>rand_input</span><span>(</span><span>self</span><span>,</span> <span>batchsize</span><span>):</span>
        <span>return</span> <span>torch</span><span>.</span><span>randn</span><span>((</span><span>batchsize</span><span>,)</span> <span>+</span> <span>self</span><span>.</span><span>input_dims</span><span>)</span>

    <span>def</span> <span>forward</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>sigma</span><span>):</span>
        <span>sigma_embeds</span> <span>=</span> <span>get_sigma_embeds</span><span>(</span><span>x</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],</span> <span>sigma</span><span>.</span><span>squeeze</span><span>())</span> <span># shape: b x 2
</span>        <span>nn_input</span> <span>=</span> <span>torch</span><span>.</span><span>cat</span><span>([</span><span>x</span><span>,</span> <span>sigma_embeds</span><span>],</span> <span>dim</span><span>=</span><span>1</span><span>)</span>               <span># shape: b x (dim + 2)
</span>        <span>return</span> <span>self</span><span>.</span><span>net</span><span>(</span><span>nn_input</span><span>)</span>

<span>model</span> <span>=</span> <span>TimeInputMLP</span><span>(</span><span>dim</span><span>=</span><span>2</span><span>,</span> <span>hidden_dims</span><span>=</span><span>(</span><span>16</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>128</span><span>,</span><span>16</span><span>))</span>
</code></pre></div>
<p>The MLP takes the concatenation of \(x \in \R^2\) and an embedding of the noise
level \(\sigma\), then predicts the noise \(\epsilon \in \R^2\). Although many
diffusion models use a sinusoidal positional embedding for \(\sigma\), the
simple two-dimensional embedding works just as well:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/sigma_embedding.png" alt="">
</p>
<figcaption>Two-dimensional \(\sigma_t\) embedding</figcaption>
</figure>
<p>Now we have all the ingredients to train a diffusion model.</p>
<div><pre><code><span>schedule</span> <span>=</span> <span>ScheduleLogLinear</span><span>(</span><span>N</span><span>=</span><span>200</span><span>,</span> <span>sigma_min</span><span>=</span><span>0.005</span><span>,</span> <span>sigma_max</span><span>=</span><span>10</span><span>)</span>
<span>trainer</span>  <span>=</span> <span>training_loop</span><span>(</span><span>loader</span><span>,</span> <span>model</span><span>,</span> <span>schedule</span><span>,</span> <span>epochs</span><span>=</span><span>15000</span><span>)</span>
<span>losses</span>   <span>=</span> <span>[</span><span>ns</span><span>.</span><span>loss</span><span>.</span><span>item</span><span>()</span> <span>for</span> <span>ns</span> <span>in</span> <span>trainer</span><span>]</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/training_loss.png" alt="">
</p>
<figcaption>Training loss over 15000 epochs, smoothed with moving average</figcaption>
</figure>
<p>The learned denoiser \(\eps_\theta(x, \sigma)\) can be visualized as a vector
field parameterized by the noise level \(\sigma\), by plotting \(x - \sigma
\eps_\theta(x, \sigma)\) for different \(x\) and levels of \(\sigma\).</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/predicted_eps_field.png" alt="">
</p>
<figcaption>Plot of predicted \(\hat{x}_0 = x - \sigma \eps_\theta(x, \sigma)\) for
different \(x\) and \(\sigma\)</figcaption>
</figure>
<p>In the plots above, the arrows point from each noisy datapoint \(x\) to the
‚Äúclean‚Äù datapoint predicted by the denoiser with noise level \(\sigma\). At high
levels of \(\sigma\), the denoiser tends to predict the mean of the data, but at
low noise levels the denoiser predicts actual data points, provided that its
input \(x\) is also close to the data.</p>
<p>How do we interpret what the denoiser is learning, and how do we create a
procedure to sample from diffusion models? We will next build a theory of
diffusion models, then draw on this theory to derive sampling algorithms.</p>
<h4 id="denoising-as-approximate-projection">Denoising as approximate projection</h4>
<p>The diffusion training procedure learns a denoiser \(\eps_\theta(x,
\sigma)\). In <a href="https://arxiv.org/abs/2306.04848">our paper</a>, we interpret the learned denoiser as an
approximate projection to the data manifold \(\Kset\), and the goal of the
diffusion process as minimizing the distance to \(\Kset\). This motivates us to
introduce a relative-error approximation model to analyze the convergence of
diffusion sampling algorithms. First we introduce some basic properties of
distance and projection functions.</p>
<h5 id="distance-and-projection-functions">Distance and projection functions</h5>
<p>The <em>distance function</em> to a set \(\Kset \subseteq \R^n\) is defined as</p>
\[\distK(x) := \min \{ \norm{x-x_0} : x_0 \in \Kset \}.\]
<p>The <em>projection</em> of \(x \in \R^n\), denoted \(\projK(x)\), is the set of points
that attain this distance:</p>
\[\projK(x) := \{ x_0 \in \Kset : \distK(x) = \norm{x-x_0} \}\]
<p>If \(\projK(x)\) is unique, the gradient of \(\distK(x)\), the direction of
steepest descent of the distance function, points towards this unique
projection:</p>
<p><strong>Proposition</strong> Suppose \(\Kset\) is closed and \(x \not \in \Kset\). If \(\projK(x)\)
is unique, then</p>
\[\nabla \frac{1}{2} \distK(x)^2 = \distK(x) \nabla \distK(x) = x-\projK(x).\]
<p>This tells us that if we can learn \(\nabla \distK(x)\) for every \(x\), we can
simply move in this direction to find the projection of \(x\) onto \(\Kset\).
One issue with learning this gradient is that \(\distK\) is not differentiable
everywhere, thus \(\nabla \distK\) is not a continuous function. To solve this
problem, we introduce a squared-distance function smoothed by a parameter
\(\sigma\) using the \(\softmin\) operator instead of \(\min\).</p>
\[\distop^2_\Kset(x, \sigma)
:= \substack{\softmin_{\sigma^2} \\ x_0 \in \Kset} \norm{x_0 - x}^2
= {\textstyle -\sigma^2 \log\left(\sum_{x_0 \in \Kset}
\exp\left(-\frac{\norm{x_0 - x}^2}{2\sigma^2}\right)\right)}\]
<p>The following picture from <a href="https://arxiv.org/pdf/2108.10480.pdf">[Madan and Levin
2022]</a> shows the contours of both the
distance function and its smoothed version.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/smoothed_dist_contour.png" alt="">
</p>
<figcaption>Smoothed distance function has continuous gradients</figcaption>
</figure>
<p>From this picture we can see that \(\nabla \distK(x)\) points toward the closest
point to \(x\) in \(\Kset\), and \(\nabla \distop^2(x, \sigma)\) points toward a
weighted average of points in \(\Kset\) determined by \(x\).</p>
<h5 id="ideal-denoiser">Ideal denoiser</h5>
<p>The ideal or optimal denoiser \(\epsilon^*\) for a particular noise level
\(\sigma\) is an exact minimizer of the training loss function. When the data is
a discrete uniform distribution over a finite set \(\Kset\), the ideal
denoiser has an exact closed-form expression given by:</p>
\[\eps^*(x_\sigma, \sigma) = \frac{\sum_{x_0 \in \Kset} (x_\sigma-x_0) \exp(-\norm{x_\sigma-x_0}^2/2\sigma^2)}
{\sigma \sum_{x_0 \in \Kset} \exp(-\norm{x_\sigma-x_0}^2/2\sigma^2)}\]
<p>From the above expression, we see that the ideal denoiser points towards a
weighted mean of all the datapoints in \(\Kset\), where the weight for each
\(x_0 \in \Kset\) determines the distance to \(x_0\). Using this expression, we
can also implement the ideal denoiser, which is computationally tractable for
small datasets:</p>
<div><pre><code><span>def</span> <span>sq_norm</span><span>(</span><span>M</span><span>,</span> <span>k</span><span>):</span>
    <span># M: b x n --(norm)--&gt; b --(repeat)--&gt; b x k
</span>    <span>return </span><span>(</span><span>torch</span><span>.</span><span>norm</span><span>(</span><span>M</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span><span>**</span><span>2</span><span>).</span><span>unsqueeze</span><span>(</span><span>1</span><span>).</span><span>repeat</span><span>(</span><span>1</span><span>,</span><span>k</span><span>)</span>

<span>class</span> <span>IdealDenoiser</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dataset</span><span>:</span> <span>torch</span><span>.</span><span>utils</span><span>.</span><span>data</span><span>.</span><span>Dataset</span><span>):</span>
        <span>self</span><span>.</span><span>data</span> <span>=</span> <span>torch</span><span>.</span><span>stack</span><span>(</span><span>list</span><span>(</span><span>dataset</span><span>))</span>

    <span>def</span> <span>__call__</span><span>(</span><span>self</span><span>,</span> <span>x</span><span>,</span> <span>sigma</span><span>):</span>
        <span>x</span> <span>=</span> <span>x</span><span>.</span><span>flatten</span><span>(</span><span>start_dim</span><span>=</span><span>1</span><span>)</span>
        <span>d</span> <span>=</span> <span>self</span><span>.</span><span>data</span><span>.</span><span>flatten</span><span>(</span><span>start_dim</span><span>=</span><span>1</span><span>)</span>
        <span>xb</span><span>,</span> <span>db</span> <span>=</span> <span>x</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],</span> <span>d</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
        <span>sq_diffs</span> <span>=</span> <span>sq_norm</span><span>(</span><span>x</span><span>,</span> <span>db</span><span>)</span> <span>+</span> <span>sq_norm</span><span>(</span><span>d</span><span>,</span> <span>xb</span><span>).</span><span>T</span> <span>-</span> <span>2</span> <span>*</span> <span>x</span> <span>@</span> <span>d</span><span>.</span><span>T</span>
        <span>weights</span> <span>=</span> <span>torch</span><span>.</span><span>nn</span><span>.</span><span>functional</span><span>.</span><span>softmax</span><span>(</span><span>-</span><span>sq_diffs</span><span>/</span><span>2</span><span>/</span><span>sigma</span><span>**</span><span>2</span><span>,</span> <span>dim</span><span>=</span><span>1</span><span>)</span>
        <span>return </span><span>(</span><span>x</span> <span>-</span> <span>torch</span><span>.</span><span>einsum</span><span>(</span><span>'</span><span>ij,j...-&gt;i...</span><span>'</span><span>,</span> <span>weights</span><span>,</span> <span>self</span><span>.</span><span>data</span><span>))</span><span>/</span><span>sigma</span>
</code></pre></div>
<p>For our toy dataset, we can plot the direction of \(\epsilon^*\) as predicted by
the ideal denoiser for different noise levels \(\sigma\):</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ideal_eps_field.png" alt="">
</p>
<figcaption>Plot of direction of \(\eps^*(x, \sigma)\) for different \(x\) and
\(\sigma\)</figcaption>
</figure>
<p>From our plots we see that for large values of \(\sigma\), \(\epsilon^*\) points
towards the mean of the data, but for smaller values of \(\sigma\),
\(\epsilon^*\) points towards the nearest data-point.</p>
<p>One insight from our paper is that the ideal denoiser for a fixed \(\sigma\) is
equivalent to the gradient of a \(\sigma\)-smoothed squared-distance function:</p>
<p><strong>Theorem</strong> For all \(\sigma &gt; 0\) and \(x \in \R^n\), we have</p>
\[\frac{1}{2} \nabla_x \distop^2_\Kset(x, \sigma) = \sigma \eps^*(x, \sigma).\]
<p>This tells us that the ideal denoiser found by minimizing the diffusion training
objective \(\Loss(\theta)\) is in fact the gradient of a smoothed
squared-distance function to the underlying data manifold \(\Kset\). This
connection is key to motivating our interpretation that the denoiser is an
approximate projection.</p>
<h5 id="relative-error-model">Relative error model</h5>
<p>In order to analyze the convergence of diffusion sampling algorithms, we
introduced a relative error model which states that the projection predicted by
the denoiser \(x-\sigma \epsilon_{\theta}( x, \sigma)\) well approximates
\(\projK(x)\) when the input to the denoiser \(\sigma\) well estimates
\(\distK(x)/\sqrt{n}\). For constants \(1 &gt; \eta \ge 0\) and \(\nu \ge 1\), we
assume that</p>
\[\norm{ x-\sigma \epsilon_{\theta}( x, \sigma) - \projK(x)} \le \eta \distK(x)\]
<p>when \((x, \sigma)\) satisfies \(\frac{1}{\nu}\distK(x) \le \sqrt{n}\sigma \le
\nu \distK(x)\). In addition to the discussion about ideal denoisers above, this
error model is motivated by the following observations.</p>
<p><em>Low noise</em> When \(\sigma\) is small and the manifold hypothesis holds,
denoising approximates projection because most of the added noise is orthogonal
to the data manifold.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/low_noise.png" alt="">
</p>
<figcaption>When added noise is small, most of noise is orthogonal to tangent space
of manifold. Under the manifold hypothesis, denoising is approximately projection.</figcaption>
</figure>
<p><em>High noise</em> When \(\sigma\) is large relative to the diameter of \(\Kset\),
then any denoiser predicting any weighted mean of the data \(\Kset\) has small
relative error.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/high_noise.png" alt="">
</p>
<figcaption>When added noise is large compared to diameter of data, denoising and
projection point in the same direction</figcaption>
</figure>
<p>We also perform empirical tests of our error model for pre-trained diffusion
models on image datasets. The CIFAR-10 dataset is small enough for tractable
computation of the ideal denoiser. Our experiments show that for this dataset,
the relative error between the exact projection and ideal denoiser output is
small over sampling trajectories.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ideal_denoiser_error.png" alt="">
</p>
<figcaption>Ideal denoiser well-approximates projection onto the CIFAR-10 dataset
under relative-error model</figcaption>
</figure>

<h4 id="sampling-from-diffusion-models">Sampling from diffusion models</h4>
<p>Given a learned denoiser \(\epsilon_\theta(x, \sigma)\), how do we sample from
it to obtain a point \(x_0 \in \Kset\)? Given noisy \(x_t\) and noise level
\(\sigma_t\), the denoiser \(\eps_\theta(x_t, \sigma_t)\) predicts \(x_0\) via</p>
\[\hat x_0^t := x_t - \sigma_t \eps_\theta(x_t, \sigma_t)\]
<p>Intuition from the relative error assumption tells us that we want to start with
\((x_T, \sigma_T)\) where \(\distK(x_T)/\sqrt{n} \approx \sigma_T\). This is
achieved by choosing \(\sigma_T\) to be large relative to the diameter of
\(\Kset\), and \(x_T\) sampled i.i.d. from \(N(0, \sigma_T)\), a Gaussian with
variance \(\sigma_T\). This ensures that \(x_T\) is far away from
\(\Kset\).</p>
<p>Although \(\hat x_0^T = x_T - \sigma_T \eps_\theta(x_T, \sigma_T)\) has small
relative error, the absolute error \(\distK(\hat x_0^T)\) can still be large as
\(\distK(x_T)\) is large. In fact, at high noise levels, the expression of the
ideal denoiser tells us that \(\hat x_0^T\) should be close to the mean of the
data \(\Kset\). We cannot obtain a sample close to \(\Kset\) with a single call
to the denoiser.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/denoise.png" alt="">
</p>
<figcaption>Sampling process iteratively calls the denoiser based on \(\sigma_t\)
schedule.</figcaption>
</figure>
<p>Thus we want to <em>iteratively call the denoiser</em> to obtain a sequence \(x_T,
\ldots, x_t, \ldots x_0\) using a pre-specified schedule of \(\sigma_t\), hoping
that \(\distK(x_t)\) decreases in concert with \(\sigma_t\).</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t-1}) \eps_\theta(x_t, \sigma_t)\]
<p>This is exactly the deterministic <a href="https://arxiv.org/abs/2010.02502">DDIM sampling algorithm</a>, though
presented in different coordinates through a change of variable. See <a href="https://arxiv.org/abs/2306.04848">Appendix A
of our paper</a> for more details and a proof of equivalence.</p>
<h4 id="diffusion-sampling-as-distance-minimization">Diffusion sampling as distance minimization</h4>
<p>We can interpret the diffusion sampling iterations as gradient descent on the
squared-distance function \(f(x) = \frac{1}{2} \distK(x)^2\). In a nutshell,</p>
<p><strong>DDIM is approximate gradient descent on \(f(x)\) with stepsize \(1-
\sigma_{t-1}/\sigma_t\), with \(\nabla f(x_t)\) estimated by \(\eps_\theta(x_t,
\sigma_t)\).</strong></p>
<p>How should we choose the \(\sigma_t\) schedule? This determines the number and
size of gradient steps we take during sampling. If there are too few steps,
\(\distK(x_t)\) might not decrease and the algorithm may not converge. On the
other hand, if we take many small steps, we need to evaluate the denoiser for as
many times, a computationally expensive operation. This motivates our definition
of admissible schedules.</p>
<p><strong>Definition</strong> An <em>admissible schedule</em> \(\{ \sigma_t \}_{t=0}^T\) ensures
\(\frac{1}{\nu} \distK(x_t) \le \sqrt{n} \sigma_t \le \nu \distK(x_t)\) holds at
each iteration. In particular, a geometrically decreasing (i.e. log-linear)
sequence of \(\sigma_t\) is an admissible schedule.</p>
<p>Our main theorem states that if \(\{\sigma_t\}_{t=0}^T\) is an admissible
schedule and \(\epsilon_\theta(x_t, \sigma_t)\) satisfies our relative error
model, the relative error can be controlled, and the sampling procedure aiming
to minimize distance converges.</p>
<p><strong>Theorem</strong> Let \(x_t\) denote the sequence generated by DDIM and suppose that
\(\nabla \distK(x)\) exists for all \(x_t\) and \(\distK(x_T) = \sqrt n
\sigma_T\). Then</p>
<ol>
<li>\(x_t\) is generated by gradient descent on the squared-distance function
with stepsize \(1 - \sigma_{t-1}/\sigma_{t}\)</li>
<li>\(\distK(x_{t})/\sqrt{n} \approx \sigma_{t}\) for all \(t\)</li>
</ol>
<p>Coming back to our toy example, we can find an admissible schedule by
subsampling from the original log-linear schedule, and implement the DDIM
sampler as follows:</p>
<div><pre><code><span>class</span> <span>Schedule</span><span>:</span>
    <span>...</span>
    <span>def</span> <span>sample_sigmas</span><span>(</span><span>self</span><span>,</span> <span>steps</span><span>:</span> <span>int</span><span>)</span> <span>-&gt;</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>:</span>
        <span>indices</span> <span>=</span> <span>list</span><span>((</span><span>len</span><span>(</span><span>self</span><span>)</span> <span>*</span> <span>(</span><span>1</span> <span>-</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>steps</span><span>)</span><span>/</span><span>steps</span><span>))</span>
                       <span>.</span><span>round</span><span>().</span><span>astype</span><span>(</span><span>np</span><span>.</span><span>int64</span><span>)</span> <span>-</span> <span>1</span><span>)</span>
        <span>return</span> <span>self</span><span>[</span><span>indices</span> <span>+</span> <span>[</span><span>0</span><span>]]</span>

<span>batchsize</span> <span>=</span> <span>2000</span>
<span>sigmas</span> <span>=</span> <span>schedule</span><span>.</span><span>sample_sigmas</span><span>(</span><span>20</span><span>)</span>
<span>xt</span> <span>=</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>)</span> <span>*</span> <span>sigmas</span><span>[</span><span>0</span><span>]</span>
<span>for</span> <span>sig</span><span>,</span> <span>sig_prev</span> <span>in</span> <span>pairwise</span><span>(</span><span>sigmas</span><span>):</span>
    <span>eps</span> <span>=</span> <span>model</span><span>(</span><span>xt</span><span>,</span> <span>sig</span><span>.</span><span>to</span><span>(</span><span>xt</span><span>))</span>
    <span>xt</span> <span>-=</span> <span>(</span><span>sig</span> <span>-</span> <span>sig_prev</span><span>)</span> <span>*</span> <span>eps</span>
</code></pre></div>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/toy_example_samples_ddim.png" alt="">
</p>
<figcaption>Samples from 20-step DDIM</figcaption>
</figure>

<h4 id="improved-sampler-with-gradient-estimation">Improved sampler with gradient estimation</h4>
<p>Next, we use our interpretation to derive a new efficient sampler. Since
\(\nabla \distK(x)\) is invariant between \(x\) and \(\projK(x)\), we aim to
minimize estimation error \(\sqrt{n} \nabla \distK(x) - \epsilon_{\theta}(x_t,
\sigma_t)\) with the update:</p>
\[\bar\eps_t = \gamma \eps_{\theta}(x_t, \sigma_t) + (1-\gamma) \eps_{\theta}(x_{t+1}, \sigma_{t+1})\]
<p>Intuitively, this update corrects any error made in the previous step using the current estimate:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/ge_step.png" alt="">
</p>
<figcaption>Our gradient estimation update step</figcaption>
</figure>
<p>This leads to faster convergence compared to the DDIM sampler, as seen from the
samples on our toy model lying closer to the original data.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/toy_example_samples_ge.png" alt="">
</p>
<figcaption>Samples from 20-step gradient estimation sampler</figcaption>
</figure>
<p>Compared to the default DDIM sampler, our sampler can be interpreted as adding
momentum, causing the trajectory to potentially overshoot but converge faster.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/trajectories_varying_gamma.png" alt="">
</p>
<figcaption>Sampling trajectories varying momentum term \(\gamma\)</figcaption>
</figure>
<p>Empirically, adding noise during the generation process also improves the
sampling quality. In order to do so while sticking to our original \(\sigma_t\)
schedule, we need to denoise to a smaller \(\sigma_{t'}\) then add back noise
\(w_t \sim N(0, I)\).</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t'}) \epsilon_\theta(x_t, \sigma_t) + \eta w_t\]
<p>If we assume that \(\mathop{\mathbb{E}}\norm{w_t}^2 = \norm{\epsilon_\theta(x_t,
\sigma_t)}^2\), we should choose \(\eta\) so that the norm of the update is
constant in expectation. This leads to the choice of \(\sigma_{t-1} =
\sigma_t^\mu \sigma_{t-1}^{1-\mu}\) and \(\eta = \sqrt{\sigma_{t-1}^2 -
\sigma_{t'}^2}\) where \(0 \le \mu &lt; 1\). When \(\mu = \frac{1}{2}\), we exactly
recover the <a href="https://arxiv.org/abs/2006.11239">DDPM sampler</a>.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/trajectories_varying_mu.png" alt="">
</p>
<figcaption>Sampling trajectories varying amount of noise added during sampling</figcaption>
</figure>
<p>Our gradient estimation update can be combined with adding noise during
sampling. In summary, our full update step is</p>
\[x_{t-1} = x_t - (\sigma_t - \sigma_{t'}) \bar\eps_t + \eta w_t\]
<p>The full sampler that generalizes DDIM (<code>gam=1, mu=0</code>), DDPM (<code>gam=1, mu=0.5</code>)
and our gradient estimation sampler (<code>gam=2, mu=0</code>) is implemented below.</p>
<div><pre><code><span>@torch.no_grad</span><span>()</span>
<span>def</span> <span>samples</span><span>(</span><span>model</span>      <span>:</span> <span>nn</span><span>.</span><span>Module</span><span>,</span>
            <span>sigmas</span>     <span>:</span> <span>torch</span><span>.</span><span>FloatTensor</span><span>,</span> <span># Iterable with N+1 values for N sampling steps
</span>            <span>gam</span>        <span>:</span> <span>float</span> <span>=</span> <span>1.</span><span>,</span>        <span># Suggested to use gam &gt;= 1
</span>            <span>mu</span>         <span>:</span> <span>float</span> <span>=</span> <span>0.</span><span>,</span>        <span># Requires mu in [0, 1)
</span>            <span>xt</span>         <span>:</span> <span>Optional</span><span>[</span><span>torch</span><span>.</span><span>FloatTensor</span><span>]</span> <span>=</span> <span>None</span><span>,</span>
            <span>batchsize</span>  <span>:</span> <span>int</span> <span>=</span> <span>1</span><span>):</span>
    <span>xt</span> <span>=</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>)</span> <span>*</span> <span>sigmas</span><span>[</span><span>0</span><span>]</span>
    <span>eps</span> <span>=</span> <span>None</span>
    <span>for</span> <span>i</span><span>,</span> <span>(</span><span>sig</span><span>,</span> <span>sig_prev</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>pairwise</span><span>(</span><span>sigmas</span><span>)):</span>
        <span>eps</span><span>,</span> <span>eps_prev</span> <span>=</span> <span>model</span><span>(</span><span>xt</span><span>,</span> <span>sig</span><span>.</span><span>to</span><span>(</span><span>xt</span><span>)),</span> <span>eps</span>
        <span>eps_av</span> <span>=</span> <span>eps</span> <span>*</span> <span>gam</span> <span>+</span> <span>eps_prev</span> <span>*</span> <span>(</span><span>1</span><span>-</span><span>gam</span><span>)</span>  <span>if</span> <span>i</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>eps</span>
        <span>sig_p</span> <span>=</span> <span>(</span><span>sig_prev</span><span>/</span><span>sig</span><span>**</span><span>mu</span><span>)</span><span>**</span><span>(</span><span>1</span><span>/</span><span>(</span><span>1</span><span>-</span><span>mu</span><span>))</span> <span># sig_prev == sig**mu sig_p**(1-mu)
</span>        <span>eta</span> <span>=</span> <span>(</span><span>sig_prev</span><span>**</span><span>2</span> <span>-</span> <span>sig_p</span><span>**</span><span>2</span><span>).</span><span>sqrt</span><span>()</span>
        <span>xt</span> <span>=</span> <span>xt</span> <span>-</span> <span>(</span><span>sig</span> <span>-</span> <span>sig_p</span><span>)</span> <span>*</span> <span>eps_av</span> <span>+</span> <span>eta</span> <span>*</span> <span>model</span><span>.</span><span>rand_input</span><span>(</span><span>batchsize</span><span>).</span><span>to</span><span>(</span><span>xt</span><span>)</span>
        <span>yield</span> <span>xt</span>
</code></pre></div>
<h4 id="large-scale-examples">Large-scale examples</h4>
<p>The training code above not only works for our toy dataset, they can also be
used to train image diffusion models from scratch. See <a href="https://github.com/yuanchenyang/smalldiffusion/blob/main/examples/fashion_mnist.py">this
example</a> for an example of training on the FashionMNIST
dataset to get a second-place FID score on <a href="https://paperswithcode.com/sota/image-generation-on-fashion-mnist">this
leaderboard</a>:</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/fashion-mnist-samples-large.png" alt="">
</p>
<figcaption>Samples from a diffusion model trained on the FashionMNIST dataset</figcaption>
</figure>
<p>The sampling code works without modifications to sample from state-of-the-art
pretrained latent diffusion models:</p>
<div><pre><code><span>schedule</span> <span>=</span> <span>ScheduleLDM</span><span>(</span><span>1000</span><span>)</span>
<span>model</span>    <span>=</span> <span>ModelLatentDiffusion</span><span>(</span><span>'</span><span>stabilityai/stable-diffusion-2-1-base</span><span>'</span><span>)</span>
<span>model</span><span>.</span><span>set_text_condition</span><span>(</span><span>'</span><span>An astronaut riding a horse</span><span>'</span><span>)</span>
<span>*</span><span>xts</span><span>,</span> <span>x0</span> <span>=</span> <span>samples</span><span>(</span><span>model</span><span>,</span> <span>schedule</span><span>.</span><span>sample_sigmas</span><span>(</span><span>50</span><span>))</span>
<span>decoded</span>  <span>=</span> <span>model</span><span>.</span><span>decode_latents</span><span>(</span><span>x0</span><span>)</span>
</code></pre></div>
<p>We can visualize the different effects of our momentum term \(\gamma\) on high
resolution text-to-image generation.</p>
<figure>
<p><img src="https://www.chenyang.co/assets/images/diffusion/sd_examples.jpg" alt="">
</p>
<figcaption>Samples from a pretrained stable diffusion model</figcaption>
</figure>
<h4 id="other-resources">Other resources</h4>
<p>Also recommended are the following blog posts on diffusion models:</p>
<ol>
<li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" target="_blank">What are diffusion models</a> introduces
diffusion models from the discrete-time perspective of reversing a Markov
process</li>
<li><a href="https://yang-song.net/blog/2021/score/" target="_blank">Generative modeling by estimating gradients of the data
distribution</a> introduces diffusion models from
the continuous time perspective of reversing a stochastic differential
equation</li>
<li><a href="https://huggingface.co/blog/annotated-diffusion" target="_blank">The annotated diffusion model</a> goes over
a pytorch implementation of a diffusion model in detail</li>
</ol>

</section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The later we meet someone in a sequence, the more negatively we describe them (256 pts)]]></title>
            <link>https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/</link>
            <guid>39672111</guid>
            <pubDate>Mon, 11 Mar 2024 19:05:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/">https://suchscience.org/people-encountered-later-in-a-sequence-described-more-negatively/</a>, See on <a href="https://news.ycombinator.com/item?id=39672111">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<p>Imagine you‚Äôre the 20th candidate to interview for your dream job, or the 10th hopeful stepping onto the stage to audition for a coveted role. You‚Äôre just as qualified and just as talented as those who went before you. </p>



<p>But according to a new study, you might be at a surprising disadvantage, thanks to an unconscious bias in how we perceive and describe others. It found that the later we encounter someone in a sequence, the more negatively we tend to describe them.</p>



<p><a href="https://pubmed.ncbi.nlm.nih.gov/38421750/" target="_blank" rel="noopener">The study</a> was published in the <em>Journal of Personality and Social Psychology</em> on February 29, 2024.</p>



<h2>The Serial Position-Negativity Effect</h2>



<p>The researchers coined this phenomenon the ‚Äúserial position-negativity effect.‚Äù </p>



<p>They hypothesized that when we sequentially encounter people, we focus on distinct attributes that differentiate each new person from those we‚Äôve already met. And because distinct attributes tend to be negative in the grand scheme of things, our descriptions of later-encountered people become increasingly negative.</p>



<p>To test this, the researchers conducted a number of studies. In one, they had 992 participants (recruited from Prolific Academic) describe 20 people based on their Facebook profile pictures. </p>



<p>The participants described the first few individuals quite positively, using an average of 6.2 positive words each. But as they progressed through the sequence, their descriptions became significantly more negative, dipping to an average of just 4.7 positive words by the 20th person.</p>



<p>In another experiment, 987 participants (about evenly split between male and female, with an average age of 42) were shown short video clips of women introducing themselves on the popular TV show <em>The Bachelor</em>. In these clips, each woman tried to make a memorable first impression on the bachelor, often in creative and attention-grabbing ways.</p>



<p>As the study participants progressed through the sequence of videos, their descriptions of the women became increasingly negative: the tenth woman was described significantly more negatively, on average, than the first woman, despite the fact that the order of the videos was randomized for each participant.</p>



<p>And their descriptions also became increasingly specific over time. As participants encountered more women, they focused more on what made each new woman stand out, leading to more unique and ultimately more negative descriptions.</p><!-- wp:shortcode -->


<!-- /wp:shortcode -->



<h2>Further Directions</h2>



<p>The study opens up many possibilities for future research. The researchers next want to examine how our personal quirks and the groups we belong to might amp up or tone down this negative bias towards people we meet later on. </p>



<p>They also want to see how long these snap judgments stick around, and how they might be throwing a wrench into things like job interviews and performance reviews in the real world.</p>



<h2>Implications </h2>



<p>The research suggests that this unconscious bias could disadvantage people who happen to be evaluated later in a sequence, whether it‚Äôs job applicants, contestants on a reality show, or Tinder dates. </p>



<p>So the next time you‚Äôre in a situation where you‚Äôre meeting a lot of new people sequentially, whether a networking event or speed dating night, keep in mind that your impressions may be tainted by this subtle cognitive bias. </p>



<h2>Study information:</h2>



<ul>
<li><strong>Journal: </strong><a href="https://www.apa.org/pubs/journals/psp" target="_blank" rel="noopener">Journal of Personality and Social Psychology</a></li>



<li><strong>Publication Date:</strong> February 29, 2024 (Online ahead of print)</li>



<li><strong>DOI: </strong>10.1037/pspa0000383</li>



<li><strong>Title:</strong> ‚Äú<a href="https://pubmed.ncbi.nlm.nih.gov/38421750/" target="_blank" rel="noopener">Differentiation in social perception: Why later-encountered individuals are described more negatively</a>‚Äú</li>



<li><strong>Authors: </strong>
<ul>
<li>Alex Koch: Booth School of Business, University of Chicago</li>



<li>Andrew Bromley: Booth School of Business, University of Chicago</li>



<li>Johanna Woitzel: Department of Psychology, Ruhr University Bochum</li>



<li>Hans Alves: Department of Psychology, Ruhr University Bochum</li>
</ul>
</li>
</ul>



		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kdenlive 24.02 open source video editor released (260 pts)]]></title>
            <link>https://kdenlive.org/en/2024/03/kdenlive-24-02-0-released/</link>
            <guid>39671218</guid>
            <pubDate>Mon, 11 Mar 2024 17:46:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kdenlive.org/en/2024/03/kdenlive-24-02-0-released/">https://kdenlive.org/en/2024/03/kdenlive-24-02-0-released/</a>, See on <a href="https://news.ycombinator.com/item?id=39671218">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The team is thrilled to introduce the much-anticipated release of Kdenlive 24.02, featuring a substantial upgrade to our frameworks with the adoption of <em>Qt6</em> and <em>KDE Frameworks 6</em>. This significant under-the-hood transformation establishes a robust foundation, shaping the trajectory of Kdenlive for the next decade. The benefits of this upgrade are particularly noteworthy for Linux users, as improved Wayland support enhances the overall experience. Additionally, users on Windows, MacOS, and Linux will experience a substantial performance boost since Kdenlive now runs natively on <em>DirectX</em>, <em>Metal</em>, and <em>Vulkan </em>respectively, replacing the previous abstraction layer reliance on <em>OpenGL</em> and <em>Angle</em>, resulting in a more efficient and responsive application. This upgrade brings significant changes to packaging, featuring the introduction of a dedicated package for <em>Apple Silicon</em>, the discontinuation of <em>PPA</em> support and an enhanced method for installing the <em>Whisper</em> and <em>Vosk</em> speech-to-text engines.</p>
<p>While a significant effort has been invested in providing a stable user experience in this transition, we want to acknowledge that, like any evolving software, there might be some rough edges. Some known issues include: themes and icons not properly applied in Windows and AppImage, text not properly displayed in clips in the timeline when using Wayland and a crash in the Subtitle Manager under MacOS. Worth noting also is the temporary removal of the audio recording feature pending its migration to Qt6. We appreciate your understanding and encourage you to provide feedback in this release cycle so that we can continue refining and improving Kdenlive. In the upcoming release cycles (24.05 and 24.08), our development efforts will concentrate on stabilizing any remaining issues stemming from this upgrade. We‚Äôll also prioritize short-term tasks outlined in our <a href="https://kdenlive.org/en/kdenlive-roadmap/">roadmap</a>, with a specific emphasis on enhancing performance and streamlining the effects workflow.</p>
<p>In terms of performance enhancements, this release introduces optimized RAM usage during the import of clips into the Project Bin. Furthermore, it addresses Nvidia encoding and transcoding issues with recent ffmpeg versions.</p>
<p>To safeguard project integrity, measures have been implemented to prevent corruptions. Projects with non-standard and variable frame rates are not allowed to be created. When rendering a project containing variable frame rate clips, users will receive a warning with the option to transcode these clips, mitigating potential audio-video synchronization issues.</p>
<p>Users can now enjoy the convenience of an automatic update check <strong>without</strong> an active network connection. Glaxnimate animations now default to the rawr format, replacing Lottie. Furthermore, we‚Äôve introduced an FFv1 render preset to replace the previously non-functional Ut Video. And multiple project archiving issues have been fixed.</p>
<p>Beyond performance and stability we‚Äôve managed to sneak in several nifty quality-of-life and usability improvements, the highlights include:</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rnote ‚Äì An open-source vector-based drawing app (104 pts)]]></title>
            <link>https://rnote.flxzt.net/</link>
            <guid>39671212</guid>
            <pubDate>Mon, 11 Mar 2024 17:46:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rnote.flxzt.net/">https://rnote.flxzt.net/</a>, See on <a href="https://news.ycombinator.com/item?id=39671212">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
            <p>Overview</p>
            <h2 id="features">Features</h2>
<ul>
<li>Adaptive UI focused on stylus input</li>
<li>Pressure-sensitive stylus input with different and configurable stroke styles</li>
<li>Create many different shapes with the shape tool</li>
<li>Move, rotate, resize and modify existing content with the selection tool</li>
<li>Different document expansion layouts ( fixed pages, continuous vertical, infinite in every direction, .. )</li>
<li>Customizable background colors, patterns, sizes</li>
<li>Customizable page format</li>
<li>(Optional) pen sounds</li>
<li>Reconfigurable stylus button shortcuts</li>
<li>An integrated workspace browser for quick access to related files</li>
<li>Drag &amp; Drop, clipboard support</li>
<li>PDF, Bitmap and SVG image import</li>
<li>Document, document pages and selection export to many formats including SVG, PDF, Xopp</li>
<li>Save and load the documents in the native <code>.rnote</code> file format</li>
<li>Tabs to work on multiple documents at the same time</li>
<li>Autosave, printing</li>
</ul>
<p><strong>Disclaimer</strong></p>
<p>The file format is still unstable. It might change and break compatibility between versions.</p>
<h2 id="installation">Installation</h2>
<h2 id="linux">Linux</h2>
<p>Download the official flatpak on Flathub <a href="https://flathub.org/apps/details/com.github.flxzt.rnote">here</a>.</p>
<p><a href="https://flathub.org/apps/details/com.github.flxzt.rnote"><img width="250" alt="Download on Flathub" src="https://raw.githubusercontent.com/flxzt/rnote/main/misc/assets/flathub-badge.svg"></a>
</p><br>
<h2 id="macos">MacOS</h2>
<p>Thanks to @dehesselle the app is available on MacOS as an app bundle.<br>
Check out the <a href="https://gitlab.com/dehesselle/rnote_macos">repository</a>, the latest release can be downloaded <a href="https://gitlab.com/dehesselle/rnote_macos/-/releases/permalink/latest">here</a>.</p>
<p><a href="https://gitlab.com/dehesselle/rnote_macos/-/releases/permalink/latest"><img height="150" alt="Download MacOS app bundle" src="https://raw.githubusercontent.com/flxzt/rnote/main/misc/assets/rnote-macos-app-bundle-badge.png"></a>
</p><br>
<h2 id="windows">Windows</h2>
<p>Download the Windows installer from the latest release which can be found <a href="https://github.com/flxzt/rnote/releases/latest">here</a>.</p>
<p><a href="https://github.com/flxzt/rnote/releases/latest"><img width="250" alt="Download Windows installer" src="https://raw.githubusercontent.com/flxzt/rnote/main/misc/assets/windows-installer-badge.svg"></a>
</p><br>
<h2 id="downgrading">Downgrading</h2>
<p>Because the file format still is unstable, downgrading to a specific version might be necessary.</p>
<p>List all available past versions on flathub:</p>
<pre data-lang="bash"><code data-lang="bash"><span>flatpak remote-info</span><span> --log</span><span> flathub com.github.flxzt.rnote
</span></code></pre>
<p>Pick the commit from the desired version and downgrade with:</p>
<pre data-lang="bash"><code data-lang="bash"><span>sudo flatpak update</span><span> --commit</span><span>=&lt;</span><span>commit-hash</span><span>&gt;</span><span> com.github.flxzt.rnote 
</span></code></pre>
<p>After downgrading, the flatpak version can be pinned or unpinned with:</p>
<pre><code><span>$ flatpak mask com.github.flxzt.rnote
</span><span>$ flatpak mask --remove com.github.flxzt.rnote
</span></code></pre>
<p>To update to the lastest version again, unpin and run <code>flatpak update</code>.</p>
<h2 id="screenshots">Screenshots</h2>
<p><img src="https://raw.githubusercontent.com/flxzt/rnote/main/crates/rnote-ui/data/screenshots/overview.png" alt="overview"><br>
<img src="https://raw.githubusercontent.com/flxzt/rnote/main/crates/rnote-ui/data/screenshots/selection.png" alt="selection"><br>
<img src="https://raw.githubusercontent.com/flxzt/rnote/main/crates/rnote-ui/data/screenshots/typewriter.png" alt="typewriter"><br>
<img src="https://raw.githubusercontent.com/flxzt/rnote/main/crates/rnote-ui/data/screenshots/focus-mode.png" alt="focus-mode"><br>
<img src="https://raw.githubusercontent.com/flxzt/rnote/main/crates/rnote-ui/data/screenshots/workspaces.png" alt="workspaces"><br>
<img src="https://raw.githubusercontent.com/flxzt/rnote/main/crates/rnote-ui/data/screenshots/document-settings.png" alt="document-settings"><br>
<img src="https://raw.githubusercontent.com/flxzt/rnote/main/crates/rnote-ui/data/screenshots/action-shortcuts.png" alt="action-shortcuts"> </p>
<h2 id="fonts">Fonts</h2>
<p>The following fonts are bundled with the application:</p>
<ul>
<li><a href="https://fonts.google.com/specimen/Grape+Nuts">Grape Nuts</a>: Grape Nuts is a simple handwritten casual font.</li>
<li><a href="https://github.com/antijingoist/opendyslexic">OpenDyslexic-Regular</a>: OpenDyslexic is a typeface designed against some
common symptoms of dyslexia.</li>
<li><a href="https://github.com/ctrlcctrlv/TT2020">TT2020Base-Regular</a>: TT2020 is an advanced, open source, hyperrealistic,
multilingual typewriter font for a new decade.</li>
<li><a href="https://virgil.excalidraw.com/">Virgil</a>: The font that powers Excalidraw.</li>
</ul>
<h2 id="license">License</h2>
<p>Rnote is available under the GPL-3.0-or-later. See the LICENSE file for more info.</p>
<pre><code><span>Copyright (C) 2023  The Rnote Authors
</span><span>
</span><span>This program is free software: you can redistribute it and/or modify
</span><span>it under the terms of the GNU General Public License as published by
</span><span>the Free Software Foundation, either version 3 of the License, or
</span><span>(at your option) any later version.
</span><span>
</span><span>This program is distributed in the hope that it will be useful,
</span><span>but WITHOUT ANY WARRANTY; without even the implied warranty of
</span><span>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
</span><span>GNU General Public License for more details.
</span><span>
</span><span>You should have received a copy of the GNU General Public License
</span><span>along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.
</span></code></pre>
<h2 id="more-information">More Information</h2>
<p>More detailed information regarding contributions, building, etc. is available at the
<a href="https://github.com/flxzt/rnote">Github Repository</a></p>

            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma doesn't suck anymore ‚Äì 8 bug fixes (157 pts)]]></title>
            <link>https://colab.research.google.com/drive/1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5?usp=sharing</link>
            <guid>39671146</guid>
            <pubDate>Mon, 11 Mar 2024 17:41:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://colab.research.google.com/drive/1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5?usp=sharing">https://colab.research.google.com/drive/1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5?usp=sharing</a>, See on <a href="https://news.ycombinator.com/item?id=39671146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div ng-non-bindable="" data-ogsr-up="" id="gb"><p><a aria-label="Sign in" href="https://accounts.google.com/ServiceLogin?passive=true&amp;continue=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1fxDWAfPIbC-bHwDSVj5SBmEJ6KG3bUu5%3Fusp%3Dsharing&amp;ec=GAZAqQM" target="_top"><span>Sign in</span></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[JSON Canvas ‚Äì An open file format for infinite canvas data (696 pts)]]></title>
            <link>https://jsoncanvas.org/</link>
            <guid>39670922</guid>
            <pubDate>Mon, 11 Mar 2024 17:22:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jsoncanvas.org/">https://jsoncanvas.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39670922">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="readme" data-node-type="file" data-node-file="readme.md">
          <p>readme</p>
          <h2 id="an-open-file-format-for-infinite-canvas-data">An open file format for infinite canvas data.</h2>

<p>Infinite canvas tools are a way to view and organize information spatially, like a digital whiteboard. Infinite canvases encourage freedom and exploration, and have become a popular interface pattern across many apps.</p>

<p>The JSON Canvas format was created to provide longevity, readability, interoperability, and extensibility to data created with infinite canvas apps. The format is designed to be easy to parse and give users <a href="https://stephango.com/file-over-app">ownership over their data</a>. JSON Canvas files use the <code>.canvas</code> extension.</p>

<p>JSON Canvas was originally created for <a href="https://obsidian.md/blog/json-canvas/">Obsidian</a>. JSON Canvas can be implemented freely as an import, export, and storage format for any app or tool. This site, and all the resources associated with JSON Canvas are <a href="https://github.com/obsidianmd/jsoncanvas">open source</a> under the MIT license.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Are We Watching the Internet Die? (174 pts)]]></title>
            <link>https://www.wheresyoured.at/are-we-watching-the-internet-die/</link>
            <guid>39670900</guid>
            <pubDate>Mon, 11 Mar 2024 17:19:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wheresyoured.at/are-we-watching-the-internet-die/">https://www.wheresyoured.at/are-we-watching-the-internet-die/</a>, See on <a href="https://news.ycombinator.com/item?id=39670900">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <p>Sometime this month,<a href="https://www.cnbc.com/2024/03/01/reddit-seeking-a-valuation-of-up-to-6point5-billion-in-ipo.html?ref=wheresyoured.at"> <u>Reddit will go public at a valuation of $6.5bn</u></a>.<a href="https://www.cnn.com/2024/02/26/tech/reddit-ipo-users-can-buy-shares/index.html?ref=wheresyoured.at"> <u>Select Redditors were offered the chance to buy stock at the initial listing price</u></a>, which it hasn‚Äôt announced yet but is <a href="https://www.ft.com/content/b3199303-d419-482d-96c7-e73c0b0ee8ed?ref=wheresyoured.at"><u>expected to be in the range of $31-34 per share</u></a>. Regardless of the actual price, I wouldn‚Äôt be surprised if Reddit shares quickly fall below the IPO price, based on the fact that Reddit is an absolute dog of a company,<a href="https://www.reuters.com/technology/reddit-makes-us-ipo-filing-public-2024-02-22/?ref=wheresyoured.at"> <u>losing $90.8 million on $804 million of revenue in 2023</u></a> and <a href="https://www.cnn.com/2024/02/23/tech/reddit-ipo-filing-business-plan/index.html?ref=wheresyoured.at"><u>never having turned a profit</u></a>.<a href="https://www.sec.gov/Archives/edgar/data/1713445/000162828024006294/reddits-1q423.htm?ref=wheresyoured.at"> <u>Reddit's S1</u></a><u> </u>(the initial registration form for taking a company public) laughably claims that advertising on the site is "rapidly evolving" and that it is "still in the early phases of growing this business,"<a href="https://techcrunch.com/2009/11/12/reddit-advertising/?ref=wheresyoured.at"> <u>with "this business" referring to one that Reddit launched 15 years ago</u></a>.</p><p>The Reddit IPO is one of the biggest swindles in corporate history, where millions of unpaid contributors made billions of posts so that<a href="https://www.bizjournals.com/sanfrancisco/inno/stories/news/2024/02/23/reddit-ceo-steve-huffman-total-compensation-2023.html?ref=wheresyoured.at#:~:text=Bay%20Area%20Inno%20-%20Reddit%20CEO,exceeded%20%24193M%20last%20year"> <u>CEO Steve Huffman could make $193 million</u></a> in 2023<a href="https://www.reuters.com/technology/reddit-lay-off-about-5-workforce-wsj-2023-06-06/?ref=wheresyoured.at"> <u>while laying off 90 people</u> and</a> effectively <a href="https://www.theverge.com/2023/6/15/23762501/reddit-ceo-steve-huffman-interview-protests-blackout?ref=wheresyoured.at"><u>pushing third party apps off of the platform</u></a> by charging exorbitant rates for API access, which in <a href="https://www.theverge.com/2023/6/12/23755974/reddit-subreddits-going-dark-private-protest-api-changes?ref=wheresyoured.at"><u>turn prompted several prolonged ‚Äústrikes‚Äù by users</u></a>, with some of the most popular subreddits going silent for a short period of time. Reddit, in turn, effectively ‚Äúcouped‚Äù these subreddits, replacing their longstanding moderators with <a href="https://www.theverge.com/2023/7/20/23802370/reddit-over-reopens-subreddit-protest-male-fashion-advice?ref=wheresyoured.at"><u>ones of its own choosing</u></a> ‚Äî people who would <a href="https://arstechnica.com/gadgets/2023/07/reddit-calls-for-a-few-new-mods-after-axing-polarizing-some-of-its-best/?ref=wheresyoured.at"><u>happily toe the party line and reopen them to the public</u></a>.&nbsp;</p><p>None of the people that spent hours of their lives lovingly contributing to Subreddits, or performing the vital-but-thankless role of moderation, will make a profit off of Reddit's public listing, but<a href="https://www.cnbc.com/2024/02/22/openai-ceo-sam-altman-stands-to-net-millions-as-reddit-goes-public.html?ref=wheresyoured.at"> <u>Sam Altman will make hundreds of millions of dollars for his $50 million investment from 2014</u></a>.<a href="https://www.reuters.com/technology/reddit-ai-content-licensing-deal-with-google-sources-say-2024-02-22/?ref=wheresyoured.at"> <u>Reddit also announced that it had cut a $60 million deal to allow Google to train its models on Reddit's posts</u></a>, once again offering users nothing in return for their hard work.</p><p><a href="https://www.sec.gov/Archives/edgar/data/1713445/000162828024006294/reddits-1q423.htm?ref=wheresyoured.at#:~:text=As%20a%20way,enjoyable%20as%20possible."><u>Huffman's letter to investors</u></a> waxes poetic about Redditors' "deep sense of ownership over the communities they create," and justifies taking the company public by claiming that he wants "this sense of ownership to be reflected in real ownership" as he offers them a chance to buy non-voting stock in a company that they helped enrich. Huffman ends his letter by saying that Reddit is "one of the internet's largest corpuses of authentic and constantly updated human-generated experience" before referring to it as the company's "data advantage and intellectual property," describing Redditors' posts as "data [that] constantly grows and regenerates as users converse."</p><p>We're at the end of a vast, multi-faceted con of internet users, where ultra-rich technologists tricked their customers into building their companies for free. And while the trade once seemed fair, it's become apparent that these executives see users not as willing participants in some sort of fair exchange, but as veins of data to be exploitatively mined as many times as possible, given nothing in return other than access to a platform that may or may not work properly.</p><p>This is, of course, the crux of Cory Doctorow's<a href="https://en.wikipedia.org/wiki/Enshittification?ref=wheresyoured.at"> <u>Enshittification theory</u></a>, where Reddit has moved from pleasing users to pleasing its business customers to, now, pleasing shareholders at what will inevitably be the cost of the platform's quality.</p><p>Yet what's happening to the web is far more sinister than simple <em>greed</em>, but the destruction of the user-generated internet, where executives think they've found a way to replace human beings making cool things with generative monstrosities trained on datasets controlled and monetized by trillion-dollar firms.</p><p>Their ideal situation isn't one where you visit distinct websites with content created by human beings, but a return to the dark ages of the internet where most traffic ran through a series of heavily-curated portals operated by a few select companies, with results generated based on datasets that are <a href="https://futurism.com/ai-trained-ai-generated-data-interview?ref=wheresyoured.at"><u>increasingly poisoned by generative content</u></a> built to fill space rather than be consumed by a customer.</p><p>The algorithms are easily-tricked, and the tools used to trick them are becoming easier to use and scale.</p><p>And it's slowly killing the internet.</p><h2 id="degenerative-ai"><strong>Degenerative AI</strong></h2><p>After the world's governments began their above-ground nuclear weapons tests in the mid-1940s, radioactive particles made their way into the atmosphere, permanently tainting all modern steel production, making it challenging (or impossible) to build certain machines (such as those that measure radioactivity). As a result, we've a limited supply of something called "<a href="https://qz.com/emails/quartz-obsession/1849564217/low-background-metal-pure-unadulterated-treasure?ref=wheresyoured.at"><u>low-background steel</u></a>," pre-war metal that oftentimes has to be harvested from ships sunk before the first detonation of a nuclear weapon, including <a href="https://www.theatlantic.com/science/archive/2019/10/search-dark-matter-depends-ancient-shipwrecks/600718/?ref=wheresyoured.at"><u>those dating back to the Roman Empire</u></a>.</p><p>Generative AI models are trained by using massive amounts of text scraped from the internet, meaning that the consumer adoption of generative AI has brought a degree of radioactivity to its own dataset. As more internet content is created, either partially or entirely through generative AI, the models themselves will find themselves increasingly inbred, training themselves on content written by their own models which are, on some level,<a href="https://lowbackgroundsteel.ai/?ref=wheresyoured.at"> <u>permanently locked in 2023</u></a>, before the advent of a tool that is specifically intended to replace content created by human beings.</p><p>This is a phenomenon that Jathan Sadowski calls "<a href="https://twitter.com/jathansadowski/status/1625245803211272194?lang=en&amp;ref=wheresyoured.at"><u>Habsburg AI</u></a>," where "a system that is so heavily trained on the outputs of other generative AIs that it becomes an inbred mutant, likely with exaggerated, grotesque features." In reality, a Habsburg AI will be one that is increasingly more <em>generic</em> and <em>empty</em>, normalized into a slop of anodyne business-speak as its models are trained on increasingly-identical content.</p><p>LinkedIn, already a repository of empty-headed corpo-nonsense,<a href="https://techcrunch.com/2023/03/15/linkedin-expands-its-generative-ai-assistant-to-recruitment-ads-and-writing-profiles/?ref=wheresyoured.at"> <u>already lets users write generate messages, profiles and job descriptions using AI</u></a>, and<a href="https://www.linkedin.com/help/linkedin/answer/a5538339?ref=wheresyoured.at#:~:text=For%20example%2C%20some%20of%20our,data%20from%20the%20training%20dataset."> <u>anything you create using these generative features is immediately fed back into Azure's OpenAI models owned by its parent company Microsoft</u></a>, which invested $10 billion in OpenAI in early 2023. While LinkedIn is yet to introduce fully-automated replies,<a href="https://twitter.com/nilansaha/status/1762390930550927823?ref=wheresyoured.at"> <u>Chrome extensions already exist to flood the platform with generic responses</u></a>, feeding more genericisms into the mouth of Microsoft and OpenAI's models.</p><p>Generative AI also naturally aligns with the toxic incentives created by the largest platforms. Google's algorithmic catering to the Search Engine Optimization industry naturally benefits those who can spin up large amounts of "relevant" content rather than content created by humans.<a href="https://searchengineland.com/google-released-massive-search-quality-improvements-with-march-2024-core-update-and-multiple-spam-updates-438144?ref=wheresyoured.at"> <u>While Google has claimed that their upcoming "core" update will help promote "content for people and not to rank in search engines,"</u></a><a href="https://www.theregister.com/2022/08/19/google_search_algorithm/?ref=wheresyoured.at"> <u>it‚Äôs made this promise before</u></a>, and I severely doubt anything meaningfully changes. After all, Google makes up more than 85% of all search traffic and<a href="https://www.theverge.com/2023/10/26/23933206/google-apple-search-deal-safari-18-billion?ref=wheresyoured.at"> <u>pays Apple billions a year to make Google search the default on Apple devices</u></a>.</p><p>And because these platforms were built to reward scale and volume far more often than quality, AI naturally rewards those who can find the spammiest ways to manipulate the algorithm. 404 Media reports that<a href="https://www.404media.co/inside-the-world-of-tiktok-spammers-and-the-ai-tools-that-enable-them/?ref=wheresyoured.at"> <u>spammers are making thousands of dollars from TikTok's creator program by making "faceless reels" where AI-generated voices talk over spliced-together videos ripped from YouTube</u></a>, and<a href="https://www.instagram.com/reel/C1ZidLAuksO/?ref=404media.co"> <u>a cottage industry of automation gurus</u></a> are cashing in by helping others flood Facebook, TikTok and Instagram with low-effort videos that are irresistible to algorithms.</p><p>Amazon's Kindle eBook platform has been flooded with AI-generated content that<a href="https://www.wired.com/story/scammy-ai-generated-books-flooding-amazon/?ref=wheresyoured.at"> <u>briefly dominated bestseller lists</u></a>,<a href="https://arstechnica.com/information-technology/2023/09/ai-generated-books-force-amazon-to-cap-ebook-publications-to-3-per-day/?ref=wheresyoured.at"> <u>forcing Amazon to limit authors to publishing three books a day</u></a>.<a href="https://www.wired.com/story/scammy-ai-generated-books-flooding-amazon/?ref=wheresyoured.at"> <u>This hasn't stopped spammers from publishing awkward rewrites and summaries of other people's books</u></a>, and because Amazon's policies don't outright ban AI-generated content, ChatGPT has become an inoperable cancer on the body of the publishing industry.</p><p>"Handmade" goods store Etsy has its own AI problem,<a href="https://www.theatlantic.com/technology/archive/2023/06/ai-chatgpt-side-hustle/674415/?ref=wheresyoured.at"> <u>with The Atlantic reporting last year</u></a> that the platform was now pumped full of AI-generated art, t-shirts and mugs that, in turn, use ChatGPT to optimize listings to rank highly in Google search. As a profitable public company, Etsy has little incentive to change things, even if the artisanal products on the platform are being crowded out by generative art pasted on drop-shipped shirts.<a href="https://www.pymnts.com/artificial-intelligence-2/2023/ebay-to-add-ai-powered-image-based-listing-tool/?ref=wheresyoured.at"> <u>eBay, on the other hand, is leaning into the spam, offering tools to generate entire listings based on a single image using generative AI</u></a>.</p><p>The Wall Street Journal reported last year that<a href="https://www.wsj.com/articles/chatgpt-already-floods-some-corners-of-the-internet-with-spam-its-just-the-beginning-9c86ea25?ref=wheresyoured.at"> <u>magazines are now inundated with AI-generated pitches for articles</u></a>, and<a href="https://www.theguardian.com/technology/2023/feb/21/sci-fi-publisher-clarkesworld-halts-pitches-amid-deluge-of-ai-generated-stories?ref=wheresyoured.at"> <u>renowned sci-fi publisher Clarkesworld was forced to close submissions after receiving an overwhelming amount of AI-generated stories</u></a>. Help A Reporter Out used to be a way for journalists to find potential sources and quotes, except requests are now met with a deluge of AI-generated spam.</p><p>These stories are, of course, all manifestations of a singular problem: that generative artificial intelligence is poison for an internet dependent on algorithms.</p><p>There are simply too many users, too many websites and too many content providers to manually organize and curate the contents of the internet, making algorithms necessary for platforms to provide a service. Generative AI is a perfect tool for soullessly churning out content to match a particular set of instructions ‚Äî such as those that an algorithm follows ‚Äî and while an algorithm can theoretically be tuned to evaluate content as "human," so can scaled content be tweaked to make it <em>seem</em> more human.</p><p>Things get worse when you realize that the sheer <em>volume</em> of internet content makes algorithmic recommendations a necessity to sift through an ever-growing pile of crap. Generative AI<a href="https://twitter.com/sugarsmorecake/status/1763941484653343056?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1763941484653343056%7Ctwgr%5Ea872ae864382ff6ce94a7bc72d711eaacb954876%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.404media.co%2Finside-the-world-of-tiktok-spammers-and-the-ai-tools-that-enable-them%2F&amp;ref=wheresyoured.at"> <u>allows creators to weaponize the algorithms' weaknesses</u></a> to monetize and popularize low-effort crap, and ultimately, what is a platform to do? Ban anything that uses AI-generated content? Adjust the algorithm to penalize videos without people's faces? How does a platform judge the difference between a popular video and a video that the platform made popular? And if these videos are made by humans and enjoyed by humans, why <em>should</em> it stop them?</p><p>Google might <em>pretend</em> it cares about the quality of search results, but nothing about search's decade-long decline has suggested it‚Äôs actually going to do anything.<a href="https://developers.google.com/search/docs/essentials/spam-policies?hl=en&amp;ref=wheresyoured.at"> <u>Google's spam policies</u></a> have claimed<a href="https://web.archive.org/web/20221207161654/https://developers.google.com/search/docs/essentials/spam-policies?hl=en"> <em><u>for years</u></em></a> that scraped content (outright ripping the contents of another website) was grounds for removal from Google, but even the most cursory glance at any news search shows how often sites thinly rewrite or outright steal others' content. And I can't express enough how bad (yet inevitable) the existence of the<a href="https://www.acumenresearchandconsulting.com/search-engine-optimization-services-market?ref=wheresyoured.at#:~:text=The%20market%20size%20of%20search,USD%2046.7%20Billion%20in%202021."> <u>$40 billion Search Engine Optimization industry</u></a> is, and how much of a boon being able to semi-automate the creation and optimization of content<a href="https://developers.google.com/search/docs/essentials?ref=wheresyoured.at"> <u>to the standards of an algorithm that Google has explained in exhaustive detail</u></a>. While it's plausible that Google might genuinely try and fight the influx of SEO-generated articles, one has to wonder why it‚Äôd bother to try now after spending decades catering to the industry.</p><p>As we speak, the battle that platforms are fighting is against <em>generative spam</em>, a cartoonish and obvious threat of outright nonsense, meaningless chum that can and should (and likely will) be stopped. In the process, they're failing to see that this isn't a war against <em>spam</em>, but a war against <em>crap</em>, and the overall normalization and intellectual numbing that comes when content is created to please algorithms and provide a minimum viable product for consumers. Google's "useless" results problem isn't one borne of content that has no meaning, but of content that only sort of helps, that is the "right" result but doesn't actually provide any real thought behind it, like the endless "how to fix error code X" results full of well-meaning and plausibly helpful content that doesn't really help at all.</p><p>The same goes for Etsy and Amazon. While Etsy's "spam" is an existential threat to actual artisans building something with their hands, it's not actual spam ‚Äî it's cheaply-made crap that nevertheless fulfills a need and <em>sort of </em>fits Etsy's remit. Amazon doesn't have any incentive to get rid of low-quality books that sell for<a href="https://nymag.com/intelligencer/2023/01/why-does-it-feel-like-amazon-is-making-itself-worse.html?ref=wheresyoured.at"> <u>the same reason that it doesn't get rid of its other low-quality items</u></a>. People aren't looking for the best, they're looking to fulfill a need, even if that need is fulfilled with poorly-constructed crap. </p><p>Platforms likely conflate positioning with popularity, failing to see the self-fulfilling prophecy of an algorithm making stuff popular because said stuff is built to please the algorithm creating more demand for content to please the algorithm. "Viral" content is no longer a result of lots of people deciding that they find something interesting ‚Äî it's a condition created by algorithms manipulated by forces that are getting stronger and more nuanced thanks to generative AI.</p><p>We're watching the joint hyper-scaling and hyper-normalization of the internet, where all popular content begins to look the same to appeal to algorithms run by companies obsessed with growth. Quality control in AI models only exists to stop people from nakedly exploiting the network through unquestionably iniquitous intent, rather than people making shitty stuff that kind of sucks but gets popular because an algorithm says so.</p><p>This isn't a situation where these automated tools are giving life to new forms of art or interesting new concepts, but regurgitations of an increasingly less unique internet, <em>because these models are trained on data drawn from the internet.</em> Like a plant turning to capture sunlight,<a href="https://www.wheresyoured.at/the-anti-economy/"> <u>parts of the internet have already twisted toward the satisfaction of algorithms</u></a>, and as others become dependent on generative AI (<a href="https://slate.com/technology/2024/02/quora-what-happened-ai-decline.html?ref=wheresyoured.at"><u>like Quora, which now promotes ChatGPT-generated answers at the top of results</u></a>), so will the web become more dependent and dictated by automated systems.</p><p>The ultimate problem is that this morass of uselessness will lead companies like Google to force their generative AIs to "fix" the problem by generating answers to sift through the crap.<a href="https://www.theverge.com/2023/8/14/23831391/amazon-review-summaries-generative-ai?ref=wheresyoured.at"> <u>Amazon now summarizes reviews using generative AI</u></a>, legitimizing<a href="https://www.wired.com/story/fake-amazon-reviews-underground-market/?ref=wheresyoured.at"> <u>the thousands of faked and paid-for reviews on the platform</u></a> and presenting them as verified and trusted information from Amazon itself.<a href="https://blog.google/products/search/google-search-generative-ai-international-expansion/?ref=wheresyoured.at"> <u>Google has already been experimenting with its "Search Generative Experience</u></a>" that <a href="https://www.theverge.com/2023/8/15/23833045/google-artificial-intelligence-summary-chrome-sge?ref=wheresyoured.at"><u>summarizes entire articles on iOS and Chrome</u></a>, and Microsoft's Bing search has already integrated summaries from Copilot, with both basing their answers off of a combination of search and training data.</p><p>Yet in doing so, these platforms gain a dangerous hold on the world's information.<a href="https://www.searchenginejournal.com/google-announces-deal-to-show-more-reddit-content/509132/?ref=wheresyoured.at"> <u>Google's deal with Reddit also gave it real time access to Reddit's content</u></a>, allowing it to show Reddit posts natively in search (and directly access Reddit posts data for training purposes). Yet at some point these portals will generate an answer based off of the data they have (or have access to, <a href="https://www.404media.co/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools/?ref=wheresyoured.at"><u>in the case of Tumblr and Wordpress</u></a>) rather than linking you to a place where you can find an answer by reading something created by another person. There could be a future where the majority of web users experience the web through a series of portals,<a href="https://www.theverge.com/2024/1/28/24053882/arc-search-browser-web-app-ios?ref=wheresyoured.at"> <u>like Arc Search's "browse for me" feature, which visits websites for you and summarizes their information using AI</u></a>.</p><p>Right now, the internet is controlled by a few distinct platforms, each one intent on interrupting the exploratory and creative forces that made the web great. I believe that their goal is to intrude on our ability to browse the internet, to further obfuscate the source of information while paying the platforms for content that their users make for free. Their eventual goal, in my mind, is to remove as much interaction with the larger internet as possible, summarizing and regurgitating as much as they can so that they can control and monetize the results as much as possible.</p><p>On some level, I fear that the current platforms intend to use AI to become something akin to an Internet Service Provider, offering "clean" access to a web that has become too messy and unreliable as a direct result of the platforms' actions, eventually finding ways to monetize your information's prominence in their portals, models and chatbots. As that happens, it will begin to rot out the rest of the internet, depriving media entities and social networks of traffic as executives like Steve Huffman cut further deals to monetize free labor with platforms that will do everything they can to centralize all internet traffic to two or three websites.</p><p>And as the internet becomes dominated by these centralized platforms and the sites they trawl for content, so begins the vicious cycle of the Habsburg AI. OpenAI's ChatGPT and Anthropic's Claude are dependent on a constant flow of training data to improve their models, to the point that it's<a href="https://www.theguardian.com/technology/2024/jan/08/ai-tools-chatgpt-copyrighted-material-openai?ref=wheresyoured.at"> <u>effectively impossible for them to operate without violating copyright</u></a>. As a result, they can't be too picky when it comes to the information they choose, meaning that they're more than likely going to depend on openly-available content from the internet, which as I've suggested earlier will become increasingly normalized by the demands of algorithms and the ease of automating the generic content that satisfies them.</p><p>I am not saying that user-generated content will <em>disappear,</em> but that human beings cannot create content at the scale that automation can, and<a href="https://www.theverge.com/23753963/google-seo-shopify-small-business-ai?ref=wheresyoured.at"> <u>when a large chunk of the internet is content for robots</u></a>, <em>that</em> is the content that will inform tomorrow's models. The only thing that can truly make them better is <em>more stuff</em>, but when the majority of stuff being created isn't good, or interesting, or even written for a human being, ChatGPT or Claude's models will learn the rotten habits of rotten content. This is why so many models' responses sound so similar ‚Äî they're heavily dependent on the stuff they're fed for their outputs, and so much of their "intelligence" comes from the same training data.</p><p>It's a different flavor of the same problem ‚Äî these models don't really "know" anything. They're copying other people's homework.</p><blockquote>As an aside, I also fear for the software code that's created by generative AI products like GitHub Co-pilot.<a href="https://www.infoworld.com/article/3713141/github-copilot-makes-insecure-code-even-less-secure-snyk-says.html?ref=wheresyoured.at"> <u>A study by security firm Snyk</u></a> found that GitHub Copilot and other AI-powered coding platforms, which were trained on publicly-available code (and based on the user's own codebase), can replicate existing security issues, proliferating problems rather than fixing them.<a href="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/?ref=wheresyoured.at"> <u>NYU's Center for Cybersecurity also found in 2023 study that CoPilot generated code with security vulnerabilities 40% of the time</u></a>.</blockquote><p>These are also the hard limits that you're going to see with generative images and video. While the internet is a giant hole of content you can easily and cheaply consume for training, visual media requires a great deal of significantly more complex data ‚Äî and that‚Äôs on top of the significant and obvious copyright issues. ChatGPT's DALL-E (images) and Sora (video) products are,<a href="https://www.wheresyoured.at/sam-altman-fried/"> <u>as I've noted</u></a>, limited by the availability of ways to teach them as well as the limits of generative AI itself, meaning that video may continue to dominate the internet as text-based content finds itself crowded out by AI-generated content.<a href="https://finance.yahoo.com/news/openai-sam-altman-says-giant-164924270.html?ref=wheresyoured.at"> <u>This may be why Sam Altman is trying to claim that giant AI models are not the future</u></a> ‚Äî because there may not be enough fuel to grow them much further. After all,<a href="https://cnbc.com/2024/01/18/openai-ceo-on-nyt-lawsuit-ai-models-dont-need-publishers-data-.html?ref=wheresyoured.at"> <u>Altman claims that any one data source "doesn't move the needle" for OpenAI</u></a>.</p><p>There's also no way to escape the fact that these hungry robots require legal plagiarism, and any number of copyright assaults could massively slow their progress.<a href="https://www.axios.com/2024/01/12/ai-forget-unlearn-data-privacy?ref=wheresyoured.at"> <u>It's incredibly difficult to make a model forget information</u></a>, meaning that there may, at some point, be steps <em>back </em>in the development of models if datasets have to be reverted to previous versions with copyrighted materials removed.</p><p>The<a href="https://www.vox.com/technology/2024/1/18/24041598/openai-new-york-times-copyright-lawsuit-napster-google-sony?ref=wheresyoured.at"> <u>numerous</u></a><a href="https://www.nytimes.com/2024/02/28/technology/openai-copyright-suit-media.html?ref=wheresyoured.at"> <u>lawsuits</u></a><a href="https://www.cnbc.com/2024/01/05/microsoft-openai-sued-over-copyright-infringement-by-authors.html?ref=wheresyoured.at"> <u>against</u></a> OpenAI could break the back of the company, and while Altman and other AI fantasists may pretend that these models are an intractable path to the future of society, any force that controls (or makes them pay for) the data that they use will kneecap the company and force them to come up with a way to make these models ethically.</p><p>Yet the world I fear is one where these people are allowed to run rampant, turning unique content into food for an ugly, inbred monster of an internet, one that turns everybody's information sources into semi-personalized versions of the same content. These people have names ‚Äî Sam Altman of OpenAI, Sundar Pichai of Google, Mark Zuckerberg of Meta (which has its own model called LLaMA), Dario Amodei of Anthropic, and Satya Nadella of Microsoft ‚Äî and they are responsible for trying to standardize the internet and turn it into a series of toll roads that all lead to the same place.</p><p>And they will gladly misinform and disadvantage billions of people to do so. Their future is one that is less colorful, less exciting, one that caters to the entitled and suppresses the creative. Those who rely on generative AI to create are not creators any more than a person that commissions a portrait is an artist. Altman and his ilk believe they're the new Leonardo Da Vincis, but they're little more than petty kings and rent-seekers trying to steal the world's magic.</p><p>They can, however, be fought. Don't buy their lies. Generative AI might be steeped in the language of high fantasy, but it‚Äôs a tool, one that they will not admit is a terribly-flawed and unprofitable way to feed the growth-at-all-costs tech engine. Question everything they say. Don't accept that AI "might one day" be great.&nbsp; Demand that it is today, and reject anything less than perfection from men that make billions of dollars shipping you half-finished shit. Reject their marketing speak and empty fantasizing and interrogate the tools put in front of you, and be a thorn in their side when they try to tell you that mediocrity is the future.</p><p>You are not stupid. You are not "missing anything.‚Äù These tools are not magic ‚Äî they're fantastical versions of autocomplete that can't help but make the same mistakes it's learned from the petabytes of information it's stolen from others.</p>
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Prompts as WASM Programs (184 pts)]]></title>
            <link>https://github.com/microsoft/aici</link>
            <guid>39670665</guid>
            <pubDate>Mon, 11 Mar 2024 17:00:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/aici">https://github.com/microsoft/aici</a>, See on <a href="https://news.ycombinator.com/item?id=39670665">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Artificial Intelligence Controller Interface (AICI)</h2><a id="user-content-artificial-intelligence-controller-interface-aici" aria-label="Permalink: Artificial Intelligence Controller Interface (AICI)" href="#artificial-intelligence-controller-interface-aici"></a></p>
<p dir="auto">The Artificial Intelligence Controller Interface (AICI) lets you build Controllers that constrain and direct output of a Large Language Model (LLM) in real time.
Controllers are flexible programs capable of implementing constrained decoding, dynamic editing of prompts and generated text, and coordinating execution across multiple, parallel generations.
Controllers incorporate custom logic during the token-by-token decoding and maintain state during an LLM request. This allows diverse Controller strategies, from programmatic or query-based decoding to multi-agent conversations to execute efficiently in tight integration with the LLM itself.</p>
<p dir="auto"><strong>The purpose of AICI is to make it easy to build and experiment with both existing and entirely new Controller strategies for improving LLM generations.</strong>
By abstracting away implementation details of the underlying LLM inference and serving engine, AICI aims to simplify the development of Controllers, make it easier to
write fast Controllers, and ease compatibility across LLM inference and serving engines.</p>
<p dir="auto">AICI is designed for both local and cloud execution, including (eventually) multi-tenant LLM deployments.
Controllers are implemented as light-weight WebAssembly (Wasm) modules which run on the same machine as the LLM inference engine, utilizing the CPU while the GPU is busy with token generation.
AICI is one layer in the inference stack, and is designed to allow control libraries such as Guidance, LMQL, and others to run on top of it and gain both efficiency and performance improvements, as well as portability across LLM inference and serving engines.</p>
<p dir="auto">AICI currently integrates with llama.cpp, HuggingFace Transformers, and rLLM (custom tch-based LLM inference engine), with vLLM in the works.</p>
<p dir="auto">AICI is:</p>
<ul dir="auto">
<li><a href="#flexibility">Flexible</a>: Controllers can be written in any language that can compile to Wasm (Rust, C, C++, ...),
or be interpreted inside Wasm (Python, JavaScript, ...)</li>
<li><a href="#security">Secure</a>: Controllers are sandboxed and cannot access the filesystem, network, or any other resources</li>
<li><a href="#performance">Fast</a>: Wasm modules are compiled to native code and run in parallel with the LLM inference engine, inducing only a
minimal overhead to the generation process</li>
</ul>
<p dir="auto">AICI is a prototype, designed and built at <a href="https://www.microsoft.com/en-us/research/" rel="nofollow">Microsoft Research</a>.</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">We are <a href="https://jobs.careers.microsoft.com/us/en/job/1659267" rel="nofollow">looking for a research intern</a>. You have to be accepted or currently enrolled in a PhD program or an equivalent research-oriented program in Computer Science or related STEM field.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#artificial-intelligence-controller-interface-aici">Artificial Intelligence Controller Interface (AICI)</a></li>
<li><a href="#quickstart-example-walkthrough">QuickStart: Example Walkthrough</a>
<ul dir="auto">
<li><a href="#development-environment-setup">Development Environment Setup</a></li>
<li><a href="#build-and-start-rllm-server-and-aici-runtime">Build and start rLLM server and AICI Runtime</a></li>
<li><a href="#control-ai-output-using-aici-controllers">Control AI output using AICI controllers</a></li>
</ul>
</li>
<li><a href="#comprehensive-guide-exploring-further">Comprehensive Guide: Exploring Further</a></li>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#security">Security</a></li>
<li><a href="#performance">Performance</a></li>
<li><a href="#flexibility">Flexibility</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#trademarks">Trademarks</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">QuickStart: Example Walkthrough</h2><a id="user-content-quickstart-example-walkthrough" aria-label="Permalink: QuickStart: Example Walkthrough" href="#quickstart-example-walkthrough"></a></p>
<p dir="auto">In this quickstart, we'll guide you through the following steps:</p>
<ul dir="auto">
<li>Set up <strong>rLLM Server</strong> and <strong>AICI Runtime</strong>.</li>
<li>Build and deploy a <strong>Controller</strong>.</li>
<li>Use AICI to control LLM output, so you can <strong>customize a LLM to follow specific rules</strong> when generating text.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development Environment Setup</h2><a id="user-content-development-environment-setup" aria-label="Permalink: Development Environment Setup" href="#development-environment-setup"></a></p>
<p dir="auto">To compile AICI components, you need to set up your development environment for Rust. For this quickstart you also need Python 3.11 or later to create a controller.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Windows WSL / Linux / macOS</h3><a id="user-content-windows-wsl--linux--macos" aria-label="Permalink: Windows WSL / Linux / macOS" href="#windows-wsl--linux--macos"></a></p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto"><strong>Windows users</strong>: please use WSL2 or the included <a href="https://containers.dev/" rel="nofollow">devcontainer</a>. Adding native Windows support <a href="https://github.com/microsoft/aici/issues/42" data-hovercard-type="issue" data-hovercard-url="/microsoft/aici/issues/42/hovercard">is tracked here</a>.</p>
<p dir="auto"><strong>MacOS users</strong>: please make sure you have XCode command line tools installed by running <code>xcode-select -p</code> and, if not installed, run <code>xcode-select --install</code>.</p>
<p dir="auto"><strong>CUDA</strong>: the CUDA build relies on specific libtorch installation. It's highly recommended you use the included devcontainer.</p>
</div>
<p dir="auto">If you're using devcontainer, you can skip to the <a href="#build-and-start-rllm-server-and-aici-runtime">next section</a>.</p>
<p dir="auto">Using the system package manager, install the necessary tools for building code in the repository, including <code>git</code>, <code>cmake</code> and <code>ccache</code>.</p>
<p dir="auto">For instance in WSL / Ubuntu using <code>apt</code>:</p>
<div data-snippet-clipboard-copy-content="sudo apt-get install --assume-yes --no-install-recommends \
    build-essential cmake ccache pkg-config libssl-dev libclang-dev clang llvm-dev git-lfs"><pre><code>sudo apt-get install --assume-yes --no-install-recommends \
    build-essential cmake ccache pkg-config libssl-dev libclang-dev clang llvm-dev git-lfs
</code></pre></div>
<p dir="auto">or using Homebrew on macOS:</p>
<div data-snippet-clipboard-copy-content="brew install git cmake ccache"><pre><code>brew install git cmake ccache
</code></pre></div>
<p dir="auto">Then install <strong>Rust, Rustup and Cargo</strong>, following the instructions provided <a href="https://doc.rust-lang.org/cargo/getting-started/installation.html" rel="nofollow">here</a> and <a href="https://www.rust-lang.org/learn/get-started" rel="nofollow">here</a>:</p>
<div data-snippet-clipboard-copy-content="curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"><pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
</code></pre></div>
<p dir="auto">After installation, verify that the <code>rustup --version</code> command is accessible by running it from the terminal. If the command isn't recognized, try opening a new terminal session.</p>
<p dir="auto">Next install wasm32-wasi Rust component:</p>
<div data-snippet-clipboard-copy-content="rustup target add wasm32-wasi"><pre><code>rustup target add wasm32-wasi
</code></pre></div>
<p dir="auto">If you already had Rust installed, or are getting complaints from Cargo about outdated versions, run:</p>

<p dir="auto">Last, to work with <strong>Python</strong> controllers and scripts (like this tutorial), run this command to install the required packages:</p>
<div data-snippet-clipboard-copy-content="pip install pytest pytest-forked ujson posix_ipc numpy requests"><pre><code>pip install pytest pytest-forked ujson posix_ipc numpy requests
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build and start rLLM server and AICI Runtime</h2><a id="user-content-build-and-start-rllm-server-and-aici-runtime" aria-label="Permalink: Build and start rLLM server and AICI Runtime" href="#build-and-start-rllm-server-and-aici-runtime"></a></p>
<p dir="auto">The rLLM server has two backends, one based on <code>libtorch</code> and CUDA
(<code>rllm-cuda</code>), and the other based on <code>llama.cpp</code> (<code>rllm-llamacpp</code>).</p>
<p dir="auto">The <code>rllm-cuda</code> backend only works with NVidia GPUs with compute capability 8.0 or later
(A100 and later; RTX 30x0 and later) and requires a fiddly setup of libtorch
-- it's strongly recommended to use the included devcontainer.
While this guide focuses on the <code>rllm-llamacpp</code> backend,
the build steps are the same for <code>rllm-cuda</code>, modulo the folder name.</p>
<p dir="auto">After <a href="#development-environment-setup">dev env setup</a> above,
clone the AICI repository and proceed with the next steps outlined below.</p>
<p dir="auto">Use the following command to build and run <code>aicirt</code> and <code>rllm-llamacpp</code>:</p>
<div data-snippet-clipboard-copy-content="cd rllm/rllm-llamacpp
./server.sh phi2"><pre><code>cd rllm/rllm-llamacpp
./server.sh phi2
</code></pre></div>
<p dir="auto">You can pass other model names as argument (run <code>./server.sh</code> without arguments to see available models).
You can also use a HuggingFace URL to <code>.gguf</code> file or a local path to a <code>.gguf</code> file.
(For <code>rllm-cuda</code> use HuggingFace model id or path to folder).</p>

<p dir="auto">You can find more details about <code>rllm-llamacpp</code> <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-llamacpp/README.md">here</a>.</p>
<p dir="auto">The rLLM server provides a HTTP interface, utilized for configuration tasks and processing requests. You can also use this interface to promptly verify its status. For instance, if you open <a href="http://127.0.0.1:4242/v1/models" rel="nofollow">http://127.0.0.1:4242/v1/models</a>, you should see:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
  &quot;object&quot;: &quot;list&quot;,
  &quot;data&quot;: [
    {
      &quot;object&quot;: &quot;model&quot;,
      &quot;id&quot;: &quot;TheBloke/phi-2-GGUF&quot;,
      &quot;created&quot;: 946810800,
      &quot;owned_by&quot;: &quot;owner&quot;
    }
  ]
}"><pre>{
  <span>"object"</span>: <span><span>"</span>list<span>"</span></span>,
  <span>"data"</span>: [
    {
      <span>"object"</span>: <span><span>"</span>model<span>"</span></span>,
      <span>"id"</span>: <span><span>"</span>TheBloke/phi-2-GGUF<span>"</span></span>,
      <span>"created"</span>: <span>946810800</span>,
      <span>"owned_by"</span>: <span><span>"</span>owner<span>"</span></span>
    }
  ]
}</pre></div>
<p dir="auto">confirming that the selected model is loaded.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Control AI output using AICI controllers</h2><a id="user-content-control-ai-output-using-aici-controllers" aria-label="Permalink: Control AI output using AICI controllers" href="#control-ai-output-using-aici-controllers"></a></p>
<p dir="auto">AICI allows hosting custom logic, called <strong>Controllers</strong>, that initiate, terminate, and interact with LLMs token generation. Controllers take input arguments, process them, and return a result with logs, LLM tokens, and variables.</p>
<p dir="auto">The repository includes some examples, in particular:</p>
<ul dir="auto">
<li><strong>jsctrl</strong>: a controller that accepts JavaScript code as input for execution. This code can interact with the model to generate text and tokens.</li>
<li><strong>pyctrl</strong>: a controller that accepts Python code as input for execution. This code can also interact with the model to generate text and tokens.</li>
</ul>
<p dir="auto">In this example we'll utilize <strong>pyctrl</strong> to manage token generation using a simple <strong>Python script</strong>.
If you want, you can <a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl/README.md">build and upload pyctrl</a>,
however by default the server will automatically
download the <a href="https://github.com/microsoft/aici/releases/latest">latest release</a> of pyctrl from GitHub.</p>
<p dir="auto">In general, controllers require building and deployment, while scripts (Python or JavaScript) are sent with each request.</p>
<p dir="auto">The following illustrates the relationship between the rLLM server, the AICI runtime, and the controller:</p>
<section data-identity="8bb29307-949e-423a-ac8f-c6a02eb10c4e" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;erDiagram\n    Host    ||--|{ CPU : \&quot;\&quot;\n    Host    ||--|{ GPU : \&quot;\&quot;\n    \n    CPU     ||--|| \&quot;rLLM Server\&quot; : execute\n    CPU     ||--|{ \&quot;AICI Runtime\&quot; : execute\n\n    \&quot;AICI Runtime\&quot; ||--|| \&quot;Controller\&quot; : instantiate\n\n    GPU     ||--|{ \&quot;LLM token generation\&quot; : execute\n&quot;}" data-plain="erDiagram
    Host    ||--|{ CPU : &quot;&quot;
    Host    ||--|{ GPU : &quot;&quot;
    
    CPU     ||--|| &quot;rLLM Server&quot; : execute
    CPU     ||--|{ &quot;AICI Runtime&quot; : execute

    &quot;AICI Runtime&quot; ||--|| &quot;Controller&quot; : instantiate

    GPU     ||--|{ &quot;LLM token generation&quot; : execute
">
      <pre lang="mermaid" aria-label="Raw mermaid code">erDiagram
    Host    ||--|{ CPU : ""
    Host    ||--|{ GPU : ""
    
    CPU     ||--|| "rLLM Server" : execute
    CPU     ||--|{ "AICI Runtime" : execute

    "AICI Runtime" ||--|| "Controller" : instantiate

    GPU     ||--|{ "LLM token generation" : execute
</pre>
    </div>
  <span role="presentation">
    <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" data-view-component="true">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" fill="none"></circle>
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>
  </span>
</section>

<p dir="auto"><h3 tabindex="-1" dir="auto">Controlling the LLM token generation</h3><a id="user-content-controlling-the-llm-token-generation" aria-label="Permalink: Controlling the LLM token generation" href="#controlling-the-llm-token-generation"></a></p>
<p dir="auto">Suppose we aim for a model to generate a list, adhering to a specific format and containing only five items.</p>
<p dir="auto">Typically, achieving this involves prompt engineering, crafting the prompt precisely with clear instructions, such as:</p>
<div data-snippet-clipboard-copy-content="What are the five most popular types of vehicles?
Return the result as a numbered list.
Do not add explanations, only the list."><pre><code>What are the five most popular types of vehicles?
Return the result as a numbered list.
Do not add explanations, only the list.
</code></pre></div>
<p dir="auto">The prompt would also vary depending on the model in use, given that each model tends to add explanations and understands instructions in different ways.</p>
<p dir="auto">With AICI, we shift control back to code, and we can simplify the prompt to:</p>
<div data-snippet-clipboard-copy-content="What are the most popular types of vehicles?"><pre><code>What are the most popular types of vehicles?
</code></pre></div>
<p dir="auto">using code to:</p>
<ol dir="auto">
<li>Limit the list to 5 items</li>
<li>Prevent the model from adding some initial explanation</li>
<li>Format to a numbered list</li>
<li>Stop the model from adding some text after the list.</li>
</ol>
<p dir="auto">Let's create a <code>list-of-five.py</code> python file with the following content:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import pyaici.server as aici

# Force the model to generate a well formatted list of 5 items, e.g.
#   1. name 1
#   2. name 2
#   3. name 3
#   4. name 4
#   5. name 5
async def main():
    
    # This is the prompt we want to run.
    # Note how the prompt doesn't mention a number of vehicles or how to format the result.
    prompt = &quot;What are the most popular types of vehicles?\n&quot;

    # Tell the model to generate the prompt string, ie. let's start with the prompt &quot;to complete&quot;
    await aici.FixedTokens(prompt)

    # Store the current position in the token generation process
    marker = aici.Label()

    for i in range(1,6):
      # Tell the model to generate the list number
      await aici.FixedTokens(f&quot;{i}.&quot;)

      # Wait for the model to generate a vehicle name and end with a new line
      await aici.gen_text(stop_at = &quot;\n&quot;)

    await aici.FixedTokens(&quot;\n&quot;)

    # Store the tokens generated in a result variable
    aici.set_var(&quot;result&quot;, marker.text_since())

aici.start(main())"><pre><span>import</span> <span>pyaici</span>.<span>server</span> <span>as</span> <span>aici</span>

<span># Force the model to generate a well formatted list of 5 items, e.g.</span>
<span>#   1. name 1</span>
<span>#   2. name 2</span>
<span>#   3. name 3</span>
<span>#   4. name 4</span>
<span>#   5. name 5</span>
<span>async</span> <span>def</span> <span>main</span>():
    
    <span># This is the prompt we want to run.</span>
    <span># Note how the prompt doesn't mention a number of vehicles or how to format the result.</span>
    <span>prompt</span> <span>=</span> <span>"What are the most popular types of vehicles?<span>\n</span>"</span>

    <span># Tell the model to generate the prompt string, ie. let's start with the prompt "to complete"</span>
    <span>await</span> <span>aici</span>.<span>FixedTokens</span>(<span>prompt</span>)

    <span># Store the current position in the token generation process</span>
    <span>marker</span> <span>=</span> <span>aici</span>.<span>Label</span>()

    <span>for</span> <span>i</span> <span>in</span> <span>range</span>(<span>1</span>,<span>6</span>):
      <span># Tell the model to generate the list number</span>
      <span>await</span> <span>aici</span>.<span>FixedTokens</span>(<span>f"<span><span>{</span><span>i</span><span>}</span></span>."</span>)

      <span># Wait for the model to generate a vehicle name and end with a new line</span>
      <span>await</span> <span>aici</span>.<span>gen_text</span>(<span>stop_at</span> <span>=</span> <span>"<span>\n</span>"</span>)

    <span>await</span> <span>aici</span>.<span>FixedTokens</span>(<span>"<span>\n</span>"</span>)

    <span># Store the tokens generated in a result variable</span>
    <span>aici</span>.<span>set_var</span>(<span>"result"</span>, <span>marker</span>.<span>text_since</span>())

<span>aici</span>.<span>start</span>(<span>main</span>())</pre></div>
<p dir="auto">Running the script is not too different from sending a prompt. In this case, we're sending control logic and instructions all together.</p>
<p dir="auto">To see the final result, execute the following command:</p>
<div data-snippet-clipboard-copy-content="./aici.sh run list-of-five.py"><pre><code>./aici.sh run list-of-five.py
</code></pre></div>
<p dir="auto">Result:</p>
<div data-snippet-clipboard-copy-content="Running with tagged AICI Controller: gh:microsoft/aici/pyctrl
[0]: FIXED 'What are the most popular types of vehicles?\n'
[0]: FIXED '1.'
[0]: GEN ' Cars\n'
[0]: FIXED '2.'
[0]: GEN ' Motorcycles\n'
[0]: FIXED '3.'
[0]: GEN ' Bicycles\n'
[0]: FIXED '4.'
[0]: GEN ' Trucks\n'
[0]: FIXED '5.'
[0]: GEN ' Boats\n'
[0]: FIXED '\n'
[DONE]
[Response] What are the most popular types of vehicles?
1. Cars
2. Motorcycles
3. Bicycles
4. Trucks
5. Boats

response saved to tmp/response.json
Usage: {'sampled_tokens': 16, 'ff_tokens': 37, 'cost': 69}
Timing: {'http_response': 0.05193686485290527, 'data0': 0.05199289321899414, 'first_token': 0.0658726692199707, 'last_token': 0.1784682273864746}
Tokens/sec: {'prompt': 861.0913072488067, 'sampling': 89.65181217019571}
Storage: {'result': '1. Cars\n2. Motorcycles\n3. Bicycles\n4. Trucks\n5. Boats\n\n'}"><pre><code>Running with tagged AICI Controller: gh:microsoft/aici/pyctrl
[0]: FIXED 'What are the most popular types of vehicles?\n'
[0]: FIXED '1.'
[0]: GEN ' Cars\n'
[0]: FIXED '2.'
[0]: GEN ' Motorcycles\n'
[0]: FIXED '3.'
[0]: GEN ' Bicycles\n'
[0]: FIXED '4.'
[0]: GEN ' Trucks\n'
[0]: FIXED '5.'
[0]: GEN ' Boats\n'
[0]: FIXED '\n'
[DONE]
[Response] What are the most popular types of vehicles?
1. Cars
2. Motorcycles
3. Bicycles
4. Trucks
5. Boats

response saved to tmp/response.json
Usage: {'sampled_tokens': 16, 'ff_tokens': 37, 'cost': 69}
Timing: {'http_response': 0.05193686485290527, 'data0': 0.05199289321899414, 'first_token': 0.0658726692199707, 'last_token': 0.1784682273864746}
Tokens/sec: {'prompt': 861.0913072488067, 'sampling': 89.65181217019571}
Storage: {'result': '1. Cars\n2. Motorcycles\n3. Bicycles\n4. Trucks\n5. Boats\n\n'}
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comprehensive Guide: Exploring Further</h2><a id="user-content-comprehensive-guide-exploring-further" aria-label="Permalink: Comprehensive Guide: Exploring Further" href="#comprehensive-guide-exploring-further"></a></p>
<p dir="auto">This repository contains a number of components, and which ones you need depends on your use case.</p>
<p dir="auto">You can <strong>use an existing controller module</strong>.
We provide <a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl">PyCtrl</a> and <a href="https://github.com/microsoft/aici/blob/main/controllers/jsctrl">JsCtrl</a>
that let you script controllers using server-side Python and JavaScript, respectively.
The <a href="https://github.com/microsoft/aici/blob/main/py/pyaici">pyaici</a> package contains <code>aici</code> command line tool that lets you
<a href="https://github.com/microsoft/aici/blob/main/docs/proxy.md">upload and run scripts</a> with any controller
(we also provide <a href="https://github.com/microsoft/aici/blob/main/docs/REST.md">REST API definition</a> for the curious).</p>
<blockquote>
<p dir="auto">üßë‚Äçüíª<a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl">Python code samples for scripting PyCtrl</a> and a <a href="https://github.com/microsoft/aici/blob/main/controllers/jsctrl/samples/hello.js">JavaScript Hello World for JSCtrl</a></p>
</blockquote>
<p dir="auto">We anticipate <a href="#architecture">libraries</a> will be built on top of controllers.
We provide an example in <a href="https://github.com/microsoft/aici/blob/main/py/promptlib">promptlib</a> - a client-side Python library
that generates interacts with <a href="https://github.com/microsoft/aici/blob/main/controllers/declctrl">DeclCtrl</a> via the pyaici package.</p>
<blockquote>
<p dir="auto">üßë‚Äçüíª <a href="https://github.com/microsoft/aici/blob/main/py/promptlib/notebooks/basics_tutorial.ipynb">Example notebook that uses PromptLib to interact with DeclCtrl</a>.</p>
</blockquote>
<p dir="auto">The controllers can be run in a cloud or local AICI-enabled LLM inference engine.
You can <strong>run the provided reference engine (rLLM) locally</strong> with either
<a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda">libtorch+CUDA</a> or <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-llamacpp">llama.cpp backend</a>.</p>
<p dir="auto">To <strong>develop a new controller</strong>, use a Rust <a href="https://github.com/microsoft/aici/blob/main/controllers/uppercase">starter project</a> that shows usage of <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi">aici_abi</a>
library, which simplifies implementing the <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/README.md#low-level-interface">low-level AICI interface</a>.</p>
<blockquote>
<p dir="auto">üßë‚Äçüíª<a href="https://github.com/microsoft/aici/blob/main/controllers/uppercase">Sample code for a minimal new controller</a> to get you started</p>
</blockquote>
<p dir="auto">To <strong>add AICI support to a new LLM inference engine</strong>,
you will need to implement LLM-side of the <a href="https://github.com/microsoft/aici/blob/main/docs/aicirt-proto.md">protocol</a>
that talks to <a href="https://github.com/microsoft/aici/blob/main/aicirt">AICI runtime</a>.</p>
<p dir="auto">Finally, you may want to modify any of the provided components - PRs are most welcome!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">AICI abstracts LLM inference engine from the controller and vice-versa, as in the picture below.
The rounded nodes are aspirational.
Additional layers can be built on top - we provide <a href="https://github.com/microsoft/aici/blob/main/py/promptlib">promptlib</a>,
but we strongly believe that
<a href="https://github.com/guidance-ai/guidance">Guidance</a>,
<a href="https://lmql.ai/" rel="nofollow">LMQL</a>,
<a href="https://github.com/sgl-project/sglang">SGLang</a>,
<a href="https://github.com/outlines-dev/outlines">Outlines</a>,
<a href="https://github.com/1rgs/jsonformer">jsonformer</a>,
<a href="https://github.com/noamgat/lm-format-enforcer">LMFE</a>,
etc.
can also run on top of AICI (either with custom controllers or utilizing PyCtrl or JsCtrl).</p>
<section data-identity="b9f7b882-d4a7-4edf-a14d-a3235821615b" data-host="https://viewscreen.githubusercontent.com" data-src="https://viewscreen.githubusercontent.com/markdown/mermaid?docs_host=https%3A%2F%2Fdocs.github.com" data-type="mermaid" aria-label="mermaid rendered output container">
  <div dir="auto" data-json="{&quot;data&quot;:&quot;graph TD\n    PyCtrl -- AICI --&amp;gt; aicirt[AICI-runtime]\n    JsCtrl -- AICI --&amp;gt; aicirt\n    guidance([GuidanceCtrl]) -- AICI --&amp;gt; aicirt\n    lmql([LMQL Ctrl]) -- AICI --&amp;gt; aicirt\n    aicirt -- POSIX SHM --&amp;gt; rLLM\n    aicirt -- POSIX SHM --&amp;gt; llama[llama.cpp]\n    aicirt -- POSIX SHM --&amp;gt; pyaici\n    pyaici -- Python --&amp;gt; vLLM(vLLM)\n    pyaici -- Python --&amp;gt; hf[HF Transformers]\n&quot;}" data-plain="graph TD
    PyCtrl -- AICI --> aicirt[AICI-runtime]
    JsCtrl -- AICI --> aicirt
    guidance([GuidanceCtrl]) -- AICI --> aicirt
    lmql([LMQL Ctrl]) -- AICI --> aicirt
    aicirt -- POSIX SHM --> rLLM
    aicirt -- POSIX SHM --> llama[llama.cpp]
    aicirt -- POSIX SHM --> pyaici
    pyaici -- Python --> vLLM(vLLM)
    pyaici -- Python --> hf[HF Transformers]
">
      <pre lang="mermaid" aria-label="Raw mermaid code">graph TD
    PyCtrl -- AICI --&gt; aicirt[AICI-runtime]
    JsCtrl -- AICI --&gt; aicirt
    guidance([GuidanceCtrl]) -- AICI --&gt; aicirt
    lmql([LMQL Ctrl]) -- AICI --&gt; aicirt
    aicirt -- POSIX SHM --&gt; rLLM
    aicirt -- POSIX SHM --&gt; llama[llama.cpp]
    aicirt -- POSIX SHM --&gt; pyaici
    pyaici -- Python --&gt; vLLM(vLLM)
    pyaici -- Python --&gt; hf[HF Transformers]
</pre>
    </div>
  <span role="presentation">
    <svg style="box-sizing: content-box; color: var(--color-icon-primary);" width="16" height="16" viewBox="0 0 16 16" fill="none" data-view-component="true">
  <circle cx="8" cy="8" r="7" stroke="currentColor" stroke-opacity="0.25" stroke-width="2" vector-effect="non-scaling-stroke" fill="none"></circle>
  <path d="M15 8a7.002 7.002 0 00-7-7" stroke="currentColor" stroke-width="2" stroke-linecap="round" vector-effect="non-scaling-stroke"></path>
</svg>
  </span>
</section>

<p dir="auto">The <a href="https://github.com/microsoft/aici/blob/main/py/pyaici">pyaici</a> package makes it easier to integrate AICI with Python-based LLM inference engines.
Take a look at integration with <a href="https://github.com/microsoft/aici/blob/main/scripts/py/run_hf.py">HuggingFace Transformers</a>,
though note that it doesn't support forking (generation of multiple sequences in parallel).
The <a href="https://github.com/microsoft/aici/blob/main/scripts/py/vllm_server.py">vLLM REST server</a> is currently out of date.
Please use the <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda">rLLM-cuda</a> or <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-llamacpp">rLLM-llama.cpp</a> for now.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<ul dir="auto">
<li><code>aicirt</code> runs in a separate process, and can run under a different user than the LLM engine</li>
<li>Wasm modules are <a href="https://docs.wasmtime.dev/security.html" rel="nofollow">sandboxed by Wasmtime</a></li>
<li>Wasm only have access to <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/src/host.rs"><code>aici_host_*</code> functions</a>,
implemented in <a href="https://github.com/microsoft/aici/blob/main/aicirt/src/hostimpl.rs">hostimpl.rs</a></li>
<li><code>aicirt</code> also exposes a partial WASI interface; however almost all the functions are no-op, except
for <code>fd_write</code> which shims file descriptors 1 and 2 (stdout and stderr) to print debug messages</li>
<li>each Wasm module runs in a separate process, helping with Spectre/Meltdown mitigation
and allowing limits on CPU usage</li>
</ul>
<p dir="auto">In particular, Wasm modules cannot access the filesystem, network, or any other resources.
They also cannot spin threads or access any timers (this is relevant for Spectre/Meltdown attacks).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">Most of computation in AICI Controllers occurs on the CPU, in parallel with the logit generation on the GPU.
The generation occurs in steps, where logits are generated in parallel for a new token for each sequence in a batch
(typically between 1 and 50).
This involves reading the whole model and KV caches for sequences in the batch from the GPU memory.
For optimal batch throughput, the model and KV caches should utilize a major fraction of the GPU memory,
and reading the whole memory takes about 40ms on A100 GPU (80GB).</p>
<p dir="auto">Thus, each step of generation takes on the order of 20-50ms.
With careful engineering,
this is more than enough to compute the set of allowed tokens in Rust compiled to Wasm.
These can be combined either natively in Rust, or via Python or JavaScript interpreters
we provide.</p>
<p dir="auto">For example, computing allowed token set in the 32000-strong vocabulary of Llama model takes:</p>
<ul dir="auto">
<li>about 2.0ms for Yacc grammar of the C programming language</li>
<li>about 0.3ms for a regular expression</li>
<li>about 0.2ms for a substring constraint, from 4kB string</li>
</ul>
<p dir="auto">The above numbers are for a single sequence, however each sequence is processed in separate process,
and thus if there is more cores than sequences (which is typical), they do not change.
They also include overhead of calling into Python interpreter implemented in Wasm, and then back into
Rust-generated Wasm code for the constraint itself.
They are all well within the 20-50ms budget, so do not affect the generation time at all.</p>
<p dir="auto">There is also some overhead in the critical path of sampling. It comes down to about 0.3ms per generation step
when executing 10 sequences in parallel (this is irrespective of the constraint used).
The overhead goes up to around 0.7ms for 40 sequences (though it has not been fully optimized yet).</p>
<p dir="auto">WebAssembly is designed to have minimal overhead, compared to native code.
In our experience, <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/implementation.md#token-trie">highly optimized</a>
Rust code is less than 2x slower when run in
<a href="https://wasmtime.dev/" rel="nofollow">Wasmtime</a> than native.
This is 10-100x better than JavaScript or Python.</p>
<p dir="auto">All measurements done on AMD EPYC 7V13 with nVidia A100 GPU with 80GB of VRAM.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Flexibility</h2><a id="user-content-flexibility" aria-label="Permalink: Flexibility" href="#flexibility"></a></p>
<p dir="auto">The low-level interface that AICI runtime provides allows for:</p>
<ul dir="auto">
<li>interaction with the LLM inference engine before, during, and after every generated token</li>
<li>constraining decoding to a set of tokens</li>
<li>backtracking KV-cache to a previous state</li>
<li>fast-forwarding several tokens at a time (if they are known)</li>
<li>forking generation into multiple branches</li>
<li>communication between forks via shared variables</li>
<li>utility functions for converting between tokens and byte strings</li>
</ul>
<p dir="auto">It can be utilized from any language that compiles to Wasm.</p>
<p dir="auto">This repository provides a Rust library that makes it easy to implement controllers in Rust,
and provides <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/implementation.md">efficient implementations</a>
of specific constraints (<a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/README.md#regular-expressions">regular expressions</a>,
<a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/README.md#lr1-grammars">yacc grammars</a>, substrings).
We also provide <a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl">Python</a> and <a href="https://github.com/microsoft/aici/blob/main/controllers/jsctrl">JavaScript</a> interpreters
that allow to glue these constraints together.
All of these can be easily extended.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgements</h2><a id="user-content-acknowledgements" aria-label="Permalink: Acknowledgements" href="#acknowledgements"></a></p>
<ul dir="auto">
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/tch-cuda/kernels/flash_attn">Flash Attention kernels</a> are copied from
<a href="https://github.com/Dao-AILab/flash-attention">flash-attention repo</a>;
see <a href="https://github.com/microsoft/aici/blob/main/rllm/tch-cuda/kernels/flash_attn/LICENSE">BSD LICENSE</a></li>
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/tch-cuda/kernels/vllm">Paged Attention kernels</a> are copied from
<a href="https://github.com/vllm-project/vllm">vLLM repo</a>;
see <a href="https://github.com/microsoft/aici/blob/main/rllm/tch-cuda/kernels/vllm/LICENSE">Apache LICENSE</a></li>
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/server/openai">OpenAI API definitions</a> are copied and modified from
<a href="https://github.com/EricLBuehler/candle-vllm">candle-vllm</a>;
see <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/server/openai/LICENSE">MIT LICENSE</a></li>
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda/src/llm/paged/cache_engine.rs">cache_engine.rs</a>,
<a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/config.rs">config.rs</a>,
and <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/scheduler.rs">scheduler.rs</a>
are loosely based on <a href="https://github.com/vllm-project/vllm">vLLM</a></li>
<li><a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda/src/llm/llama.rs">llama.rs</a>, <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-cuda/src/llm/phi.rs">phi.rs</a>
and <a href="https://github.com/microsoft/aici/blob/main/rllm/rllm-base/src/logits.rs">logits.rs</a> are based on
<a href="https://github.com/huggingface/candle/tree/main/candle-transformers">candle-transformers</a></li>
<li>specific <a href="https://github.com/microsoft/aici/blob/main/controllers/pyctrl/Lib">Python library</a> files are copied from
<a href="https://github.com/RustPython/RustPython">RustPython</a>
(as we only use a subset of them)</li>
<li><a href="https://github.com/guidance-ai/guidance">Guidance</a> files copied under
<a href="https://github.com/microsoft/aici/blob/main/controllers/guidance_ctrl/Lib/guidance">guidance_ctrl</a></li>
<li>the <a href="https://github.com/microsoft/aici/blob/main/controllers/aici_abi/grammars/c.y">example ANSI C grammar</a> is based on
<a href="https://www.lysator.liu.se/c/ANSI-C-grammar-y.html" rel="nofollow">https://www.lysator.liu.se/c/ANSI-C-grammar-y.html</a> by Jeff Lee (from 1985)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com/" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
<a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Who uses Google TPUs for inference in production? (106 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39670121</link>
            <guid>39670121</guid>
            <pubDate>Mon, 11 Mar 2024 16:22:33 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39670121">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39672298"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39672298" href="https://news.ycombinator.com/vote?id=39672298&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>We've previously tried and almost always regretted the decision. I think the tech stack needs another 12-18 months to mature (doesn't help that almost all work ex Google is being done in torch).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39672771"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39672771" href="https://news.ycombinator.com/vote?id=39672771&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><p><span>&gt; I think the tech stack needs another 12-18 months to mature<p>Google has been doing AI before any other company even thought about it. They are on the 6th generation of TPU hardware.</p><p>I don't think there is any maturity issue, just an availability issue because they are all being used internally.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39672845"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39672845" href="https://news.ycombinator.com/vote?id=39672845&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><p><span>100% agree, if I have access to the TPU team internally, it'll be very easy to use in production.<p>If you aren't internal, the documentation, support, and even just general bug fixing is impossible to get.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39673902"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39673902" href="https://news.ycombinator.com/vote?id=39673902&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><p><span>(Has an expert team dedicated solely to optimizing for exotic hardware) = an option<p>(Doesn't have a team like that) = stick to mass-use, commodity hardware</p><p>That's generally been the trade-off since ~1970. And usually, the performance isn't worth the people-salaries.</p><p>How many examples of successful hardware that isn't well-documented and doesn't have drop-in 1:1 SDK coverage vs (more popular solution) are there?</p><p>It seems like a heavy-lift to even get something that does have parity in those ways adopted, given you're fighting market inertia.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39674518"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39674518" href="https://news.ycombinator.com/vote?id=39674518&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>Google sells access to TPUs in its cloud platform, so you'd think they would be more open about sharing development and tooling frameworks for TPUs. It's like Borg (closed source, never used outside Google, made them no profit) vs. Kubernetes (open source, used everywhere, makes them profit).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39675874"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39675874" href="https://news.ycombinator.com/vote?id=39675874&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><p><span>&gt; Google has been doing AI before any other company even thought about it<p>This not even remotely true. SRI was working on AI in various forms long before google existed
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39673723"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39673723" href="https://news.ycombinator.com/vote?id=39673723&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>I feel like I have been hearing that since V1 TPU. I think Google is the perfect solution because they are teams whose job is to take a model and TPUify it. Elsewhere there is no team, so it's no fun.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39675579"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39675579" href="https://news.ycombinator.com/vote?id=39675579&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><p><span>They aren‚Äôt really an alternative to anything. For one thing they‚Äôre now often slower on per-accelerator basis than NVIDIA stuff. They‚Äôre cheaper, of course, but because of disparity in performance you‚Äôll need to estimate cost per flop on your own particular workload. They are also more difficult and slower to develop against, and SWE cost is always an issue if you don‚Äôt own a money printer like Google. Furthermore, for advanced users who can do their own CUDA kernels or Triton, that too can unlock additional efficiency from GPU. Such capability can‚Äôt even be contemplated on the TPU side because you basically get a black box. Then there‚Äôs the issue of limited capacity, further exacerbated by the fact that this capacity is provided by a single supplier who is struggling to fulfill its internal needs (which is why you can‚Äôt get v5). You can‚Äôt just get TPUs elsewhere. You can‚Äôt get them under your desk for dev work either.<p>That said, it wouldn‚Äôt be too difficult to port most models to Jax, load in the existing weights, and export the result for serving. Should you bother? IMO, no, unless we‚Äôre talking really large scale inference. Your time and money are almost certainly better spent iterating on the models.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39672432"><td></td></tr>
                <tr id="39673698"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39673698" href="https://news.ycombinator.com/vote?id=39673698&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>The quote from the linked press release is that they do training on TPUv4, while inference is running on GPUs. I have also heard this separately from people associated with Midjourney recently, and that they solely do training on TPUs.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39671890"><td></td></tr>
                <tr id="39672868"><td></td></tr>
                  <tr id="39672885"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39672885" href="https://news.ycombinator.com/vote?id=39672885&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>TPUs are tightly coupled to JAX and the XLA compiler. If your model is based on Pytorch you can use a bridge to export your model to StableHLO and then compile it to a TPU accelerator. In theory the XLA compiler should be more performant than the Pytorch Inductor.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39673828"><td></td></tr>
            <tr id="39671847"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39671847" href="https://news.ycombinator.com/vote?id=39671847&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>Google is using them in prod. I think they're so hungry for chips internally that cloud isn't getting much support in selling them.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39672910"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39672910" href="https://news.ycombinator.com/vote?id=39672910&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>I think this is right, in part because I've been told exactly this from people who work for Google and their job is to sell me cloud stuff- i.e., they say they have so much internal demand they aren't pushing TPUs for external use. Hence external pricing and support just isn't that great right now. But presumably when capacity catches up they'll start pushing TPUs again.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39673799"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39673799" href="https://news.ycombinator.com/vote?id=39673799&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>Feels like a bad point in the curve to try and sell them. ‚ÄúOh our internal hypecycle is done‚Ä¶ we‚Äôll put them in the market now that they‚Äôre all worn out.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39674098"><td></td></tr>
                        <tr id="39672521"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39672521" href="https://news.ycombinator.com/vote?id=39672521&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>I would guess that Google's vertexAI managed solution uses TPUs. Also Google uses them internally to train and infer for all their research products.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39672538"><td></td></tr>
            <tr id="39672585"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39672585" href="https://news.ycombinator.com/vote?id=39672585&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>While you <i>can</i> use TPUs with vertexai, it's just virtual machines - you can have one with an nvidia card if you like.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39672738"><td></td></tr>
            <tr id="39672212"><td></td></tr>
                <tr id="39672526"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39672526" href="https://news.ycombinator.com/vote?id=39672526&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><p><span>Beyond the fact that this is hardly a secret, there‚Äôs lots of other signs.<p>1. They have bought far less from NVidia than other hyper scalers, and they literally can‚Äôt vomit without saying ‚ÄúAI‚Äù. They have to be running those models on something. They have purchased huge amounts of chips from fabs, and what else would that be?</p><p>2. They have said they use them. Should be pretty obvious here.</p><p>3. They maintain a whole software stack for them, they design the chips, etc. Then they don‚Äôt really try to sell the TPU. Why else would they do this?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39673231"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39673231" href="https://news.ycombinator.com/vote?id=39673231&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>They have announced publicly using TPUs for inference, as far back as 2016. They did not offer TPUs for Cloud customers until 2017. The development is clearly driven by internal use cases. One of the features they publicly disclosed as TPU-based was Smart Reply and that launched in 2015. So their internal use of TPUs for inference goes back nearly a decade.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39672217"><td></td></tr>
                <tr id="39672305"><td></td></tr>
                              <tr id="39673203"><td></td></tr>
                <tr id="39675668"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39675668" href="https://news.ycombinator.com/vote?id=39675668&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>They were lucky to get that going. The software support for the USB TPU was abandoned by google years ago now. Works fine if you run ubuntu 16 I think.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39673290"><td></td></tr>
            <tr id="39672568"><td></td></tr>
                <tr id="39672637"><td></td></tr>
                <tr id="39672673"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39672673" href="https://news.ycombinator.com/vote?id=39672673&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>Indeed. The user made no comments in first 6 months of the account, then starting 4 hours ago has been somewhat prolific.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39672674"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39672674" href="https://news.ycombinator.com/vote?id=39672674&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><br><div>
                  <p><span>Did you forget to include the links? I searched "TPU inference docs" but the results are either general TPU docs or just some inference examples.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39673827"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39673827" href="https://news.ycombinator.com/vote?id=39673827&amp;how=up&amp;goto=item%3Fid%3D39670121"></a></center>    </td><td><p><span>I've seen people connecting these to Raspberry Pis to run local LLMs but I'm not sure how effective it is. Check YouTube for some videos about it.<p>Speaking of SBCs, prior to the Raspberry Pi, I was looking at the Orange Pi 5 which has a Rockchip RK3588S with an NPU (Neural Processing Unit). This was the first I had heard of such a thing but I was curious how/what exactly it does. Unfortunately, there's very little support for Orange Pi &amp; not a large community for it so I couldn't find any feedback on how well it worked or what it did.</p><p><a href="http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-5-Pro.html" rel="nofollow">http://www.orangepi.org/html/hardWare/computerAndMicrocontro...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39675100"><td></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[gh-116167: Allow disabling the GIL (432 pts)]]></title>
            <link>https://github.com/python/cpython/pull/116338</link>
            <guid>39670102</guid>
            <pubDate>Mon, 11 Mar 2024 16:21:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/python/cpython/pull/116338">https://github.com/python/cpython/pull/116338</a>, See on <a href="https://news.ycombinator.com/item?id=39670102">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content">Skip to content</a>
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><react-partial partial-name="keyboard-shortcuts-dialog" data-ssr="false">
  
  
  
</react-partial>



      

        

            


<header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  

  <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:python/cpython" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="TexTkrmpiwVRQISjQ9u6_4mK_zmlnys5088MAaIZmTFxAt9kMmMirUgeDCbYYv6w6A0n5ttFgtQ3tzX90RrxJw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="python/cpython" data-current-org="python" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=9DYFdB4AVYJCOlHuSHpLLLMgajv1B0gBMsEWUmYoqphRYw4QoRuVo4Wo4KGGV68fbw8NqrvgBh8t6YUcWWa9pjjnAl%2B%2Fui9J5%2FM7zRkPl4jkjX58M5hbS5%2FmBrcnK%2FAYsQUVc8bNbldATnP71iq9%2FTgt0WXMZCqL6t%2BYqpGOoDw6l41tOoAp3p2VivCLo5geKGu28lKK6INsFm2AewbWbveXI0W%2FQyt9JaayLuYlwER9VJEZhqbQtqH8DAz%2BXam1oywIaR8zuYrriNNieBM4wDrvfb5QI2u6mZVu2GUwswvS3Ih4oDcKZMiKYsp7zPY22yW80s0P7RJWEXhz6zjUTKd4HsKevCX727YBvKEmgr%2Fu7bIpAAEQGznqputFVyCXQWov6ZTWgWU0W%2BryxaGG9R8vObgSfCvfDjyQfp%2FCQGrcvxQKhNT9YGJju34%2B7yCgkQrp2H6F1CQNzc7IT3tPbjteD4dQWJ81BDHPCPGldN3EUbTd6SiPooZuWec%2BqNJFj75WhXW22Jb6cQ%2FMX9kXZlmuGvmBxZ7F%2F8Q%3D--sKPkpGqNwtPVkRS5--KOndQgPYCWF1AMn1mE3UGg%3D%3D&quot; />">
  
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fvoltron%2Fpull_requests_fragments%2Fpull_request_layout&amp;source=header-repo&amp;source_repo=python%2Fcpython" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/python/cpython/pull/116338&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="5cc4a524fc6f3d5f2a3644821fc97cd1bc2ea91e37bf816d3f32b5af23c94e48" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/voltron/pull_requests_fragments/pull_request_layout;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
  



      
    

    






  
  <div id="repository-container-header" data-turbo-replace="">

      

        


          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true">

  
    <div data-view-component="true">      <action-menu data-select-variant="none" data-view-component="true">
  <focus-group direction="vertical" mnemonics="" retain="">
    <tool-tip id="tooltip-7c6059c6-578c-4aac-bb20-d3c6fa8b96a4" for="action-menu-00e7492a-ec88-43e5-861b-a92fded039ff-button" popover="manual" data-direction="s" data-type="label" data-view-component="true">Additional navigation options</tool-tip>


<anchored-position id="action-menu-00e7492a-ec88-43e5-861b-a92fded039ff-overlay" anchor="action-menu-00e7492a-ec88-43e5-861b-a92fded039ff-button" side="outside-bottom" anchor-offset="normal" popover="auto" data-view-component="true">
  </anchored-position>  </focus-group>
</action-menu></div>
</nav>

  </div>

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container" data-channel="eyJjIjoicHVsbF9yZXF1ZXN0OjE3NTU3MTkxMzkiLCJ0IjoxNzEwMTgwMDA1fQ==--eb5ff5551ddc51b3c705151cd94626da6bcabdb5d105b6f8d1ffcb8de834fb8e" data-url="/python/cpython/pull/116338/partials/title?sticky=true" data-pull-is-open="false" data-gid="PR_kwDOBN0Z8c5opiXj" data-pjax="" data-turbo-frame="">



          
<details>
  <summary data-ga-click="Issues, create new issue, view:issue_show location:issue_header style:button logged_in:false">
    
    New issue
  </summary>
  <details-dialog aria-label="Sign up for GitHub">
            <div>
  <p>
  <strong>Have a question about this project?</strong> Sign up for a free GitHub account to open an issue and contact its maintainers and the community.
  </p>

  <!-- '"` --><!-- </textarea></xmp> -->
  <p>By clicking ‚ÄúSign up for GitHub‚Äù, you agree to our <a href="https://docs.github.com/terms" target="_blank">terms of service</a> and
  <a href="https://docs.github.com/privacy" target="_blank">privacy statement</a>. We‚Äôll occasionally send you account related emails.</p>

  <p>
    Already on GitHub?
    <a data-ga-click="(Logged out) New issue modal, clicked Sign in, text:sign-in" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;new issue modal&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/python/cpython/pull/116338&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="da6957ed603101f7ef551fcae7cb94064e81005d8a56ac7bab72cea9ee98341a" href="https://github.com/login?return_to=%2Fpython%2Fcpython%2Fissues%2Fnew%2Fchoose">Sign in</a>
    to your account
  </p>
</div>
  </details-dialog>
</details>
        
      </div>

</turbo-frame>


    </main>
  </div>

          




    <cookie-consent id="cookie-consent-banner" data-initial-cookie-consent-allowed="" data-cookie-consent-required="true"></cookie-consent>


  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Improving Performance in Firefox and Across the Web with Speedometer 3 (113 pts)]]></title>
            <link>https://hacks.mozilla.org/2024/03/improving-performance-in-firefox-and-across-the-web-with-speedometer-3/</link>
            <guid>39670046</guid>
            <pubDate>Mon, 11 Mar 2024 16:17:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hacks.mozilla.org/2024/03/improving-performance-in-firefox-and-across-the-web-with-speedometer-3/">https://hacks.mozilla.org/2024/03/improving-performance-in-firefox-and-across-the-web-with-speedometer-3/</a>, See on <a href="https://news.ycombinator.com/item?id=39670046">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-main">
  <article role="article">
    <p>In collaboration with the other major browser engine developers, Mozilla is thrilled to <a href="https://browserbench.org/announcements/speedometer3">announce Speedometer 3</a> today. Like previous versions of Speedometer, this benchmark measures <a href="https://www.mozilla.org/en-US/about/webvision/full/#performance">what we think matters most</a> for performance online: responsiveness. But today‚Äôs release is more open and more challenging than before, and is the best tool for driving browser performance improvements that we‚Äôve ever seen.</p>
<p>This fulfills the <a href="https://twitter.com/mozhacks/status/1603435347190419456">vision set out in December 2022</a> to bring experts across the industry together in order to rethink how we measure browser performance, guided by a shared goal to reflect the real-world Web as much as possible. This is the first time the Speedometer benchmark, or any major browser benchmark, has been developed through a cross-industry collaboration supported by each major browser engine: Blink, Gecko, and WebKit. Working together means we can build a shared understanding of what matters to optimize, and facilitates broad review of the benchmark itself: both of which make it a stronger lever for improving the Web as a whole.</p>
<p>And we‚Äôre seeing results: <a href="https://hacks.mozilla.org/2023/10/down-and-to-the-right-firefox-got-faster-for-real-users-in-2023/">Firefox got faster for real users in 2023</a> as a direct result of <a href="https://hacks.mozilla.org/2023/09/faster-vue-js-execution-in-firefox/">optimizing</a> <a href="https://spidermonkey.dev/blog/2023/11/27/newsletter-firefox-118-121.html">for</a> Speedometer 3. This took a coordinated effort from many teams: understanding real-world websites, building new tools to drive optimizations, and making a huge number of improvements inside Gecko to make web pages run more smoothly for Firefox users. In the process, we‚Äôve shipped <a href="https://mzl.la/4bYLwtn">hundreds of bug fixes</a> across JS, DOM, Layout, CSS, Graphics, frontend, memory allocation, profile-guided optimization, and more.</p>
<p>We‚Äôre happy to see core optimizations in all the major browser engines turning into improved responsiveness for real users, and are looking forward to continuing to work together to build performance tests that improve the Web.</p>
    <section>
                                
                                <p><a href="https://hacks.mozilla.org/author/bgrinsteadmozilla-com/">More articles by Brian Grinstead‚Ä¶</a></p>
                  </section>
  </article>
  
  

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Speedometer 3.0: A shared browser benchmark for web application responsiveness (198 pts)]]></title>
            <link>https://browserbench.org/announcements/speedometer3/</link>
            <guid>39670035</guid>
            <pubDate>Mon, 11 Mar 2024 16:17:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://browserbench.org/announcements/speedometer3/">https://browserbench.org/announcements/speedometer3/</a>, See on <a href="https://news.ycombinator.com/item?id=39670035">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>




<p>Since the initial version of the Speedometer benchmark was released in 2014 by the WebKit team, it has become a key tool for browser engines to drive performance optimizations as users and developers continue to demand richer and smoother experiences online.</p>

<p>We‚Äôre proud to release Speedometer 3.0 today as a collaborative effort between the three major browser engines: Blink, Gecko, and WebKit. Like previous releases (<a href="https://webkit.org/blog/8063/speedometer-2-0-a-benchmark-for-modern-web-app-responsiveness/">Speedometer 2 in 2018</a> and <a href="https://webkit.org/blog/3395/speedometer-benchmark-for-web-app-responsiveness/">Speedometer 1 in 2014</a>), it‚Äôs designed to measure web application responsiveness by simulating user interactions on real web pages. Today‚Äôs release of Speedometer 3.0 marks a major step forward in web browser performance testing: it introduces a better way of measuring performance and a more representative set of tests that reflect the modern Web.</p>

<h2>A New Governance Model</h2>

<p>This is the first time the Speedometer benchmark, or any major browser benchmark, has been developed through a cross-industry collaboration supported by each major browser engine: Blink/V8, Gecko/SpiderMonkey, and WebKit/JavaScriptCore. It‚Äôs been developed under a new governance model, driven by consensus, and is hosted in a <a href="https://github.com/WebKit/Speedometer/">shared repository</a> that‚Äôs open to contribution. This new structure involves a lot of collective effort: discussions, research, debates, decisions, and hundreds of PRs since we announced the project in December 2022. </p>

<h2>A Broader Range of User Experiences</h2>

<p>Speedometer 3 adds many new tests. We started designing this new benchmark by identifying some key scenarios and user interactions that we felt were important for browsers to optimize.</p>

<p>In particular, we added new tests that simulate rendering canvas and SVG charts (React Stockcharts, Chart.js, Perf Dashboard, and Observable Plot), code editing (CodeMirror), WYSIWYG editing (TipTap), and reading news sites (Next.js and Nuxt.js).</p>

<p>We‚Äôve also improved the TodoMVC tests: updating the code to adapt to the most common versions of the most popular frameworks based on data from the the HTTP Archive. The following frameworks and libraries are included: Angular, Backbone, jQuery, Lit, Preact, React, React+Redux, Svelte, and Vue; along with vanilla JavaScript implementations targeting ES5 and ES6, and a Web Components version. We also introduced more complex versions of these tests which are embedded into a bigger DOM tree with many complex CSS rules that more closely emulate the page weight and structure from popular webapps today.</p>

<p>Taken together these exercise a more broad and representative cross section of the engine, providing new opportunities to optimize JS, Layout, CSS, Graphics, and DOM APIs in order to improve user experience on the Web. Take a look at <a href="https://www.browserbench.org/Speedometer3.0/about.html">this page</a> for more details about the tests themselves.</p>

<h2>Improvements to the Test Runner</h2>

<p>The test runner itself in Speedometer 3 has been improved to measure more of the work the browser does in response to user actions, such as painting and asynchronous tasks. Speedometer 2.0 measured the time to run a test script synchronously as "sync" time, as well as any additional work before a 0 second timer fires as "async" time. However, this missed some work browser engines have to do to update the rendering of a web page.</p>

<p>In Speedometer 3.0, we are able to measure this previously-missing rendering work, which creates more opportunities to optimize real-world content. To do this, we measure a test script within a requestAnimationFrame callback as "sync" time, and a 0 second timer scheduled in a second requestAnimationFrame fires as "async" time. This async time is guaranteed to include work from timers in the test itself, as well as page rendering by the browser engine. These changes greatly improve the accuracy of the benchmark, and translate into real-world improvements for users as engines optimize this previously-missing work.</p>

<p>There are some more behind the scenes improvements as well. There‚Äôs improved developer tooling so browser engineers can better understand results, profile, and customize the test. We redesigned the test runner architecture to make it easier to write and maintain complex test cases. And there are many code quality improvements and migrations to modern features that weren‚Äôt broadly available when Speedometer 2.0 was released, such as native promises, async / await, classes, and modules.</p>

<h2>Improving Web Performance</h2>

<p>The <a href="https://github.com/WebKit/Speedometer/?tab=readme-ov-file#what-is-speedometer">primary goal</a> of Speedometer 3 is to reflect the real-world Web as much as possible, so that users benefit when a browser improves its score on the benchmark. It has already had some success at this before publicly launching, with core optimizations in each major engine throughout the last year turning into responsiveness improvements for users across the Web.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we engineer feedback at Figma with eng crits (117 pts)]]></title>
            <link>https://www.figma.com/blog/how-we-run-eng-crits-at-figma/</link>
            <guid>39669858</guid>
            <pubDate>Mon, 11 Mar 2024 16:04:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.figma.com/blog/how-we-run-eng-crits-at-figma/">https://www.figma.com/blog/how-we-run-eng-crits-at-figma/</a>, See on <a href="https://news.ycombinator.com/item?id=39669858">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Engineering crits encourage a diversity of perspectives and unblock teams to pursue new ideas. Here‚Äôs how we structure and run them&nbsp;at&nbsp;Figma.</p><div colorscheme="[object Object]"><p>At Figma, we‚Äôre encouraged to give and get feedback on <a href="https://www.figma.com/blog/welcome-to-the-wip/">in-progress work</a>‚Äîthe earlier the better. This is especially true on the engineering team. But sharing early work can be daunting. It requires an environment and culture in which individual contributors feel comfortable <a href="https://www.figma.com/blog/designer-developer-handoff-with-figma-and-jira/#present-your-work-even-while-it-s-in-progress">bringing unpolished ideas</a> to the table to ask for feedback and have an open conversation, not face another approval step. Most engineering teams have some sort of approval process, often called a technical review, which is usually reserved for the later stages in a project. The problem with late-stage technical reviews is that when they happen <em>too late, </em>like when a direction or design has already been built out, they can lead to launch-blocking feedback.</p><div><p><a href="https://www.figma.com/blog/design-critiques-at-figma/"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAACWElEQVQokW2Sa08TQRSG9ydjt5fdbbfdXui9HwjlUsCWgiByU8AmSARjI+52MrMzi1yiJsZLiNH4yR8wr5kdonzww5OzOZuc82aeY5g2l3fgXv1Lwgrh5DmWexzHBwLrqwJukcN2OVYfctCxABsLbKwJOAUOI2GF8g7cq/GwpKOHTtc4TkcCt1cRgjOBZosjX+I42hP4fhPh9jrC6KlAocxhqFRJJyZOqKpK0Gxr1NZ6k8M/E/j9OQJ/I9Bqc2Q9jv0nAj8/RPj1McLhrkDOiwdS6eSpzHkU6SyVmRzFzAzD3hbDsM/gFmnM/hbDxdsQBzsMhRJF0qbozlJMXjGwcYjegu4ZicxEOvmJrNUJOh0iOx2CuS7B0iJBq02Qzk6QtCeoNwgW5wkaTYKkTTCVJnA9gu4swfwcQaGke0bCotIrU/loyDB+EcqXz0P0VxjyJQrTpshkKYoVhvI0g+2yOEXC0pg2Q8rRqO9EhsLI5Lhc7nFJXwt8fRfJq0mE3cfapHrPVodje1NgOBCxiAexNP5fTJvDKFa4PD4Q8stFhG+XkbwmeqAyVm3w+LHFubZYqeklSpRaaLn/TkvDYZSrXJ6OhPzxPlLGpDgX8c2pNMM+xw2JVHKcHApU67qv/qt7VPZTjl6iqmmHMCyXytU+k8rgJx7Kk6MQtQaFk6dYG2izl0GInU0Gr0xRrVM822YY7TO0O0xdBrIFCnUlpkVhTKV8War4crASYHsjkEsLAVzPR8r20WgGWOsHWB8EaLcDZLI+vJKP3kKAxfkAhZIP0/KR8zRTaR9/ACSBhgxUh+LIAAAAAElFTkSuQmCC" alt="An abstract illustration featuring silhouetted shapes against a solid blue background. The top half shows three simplified, bald human head profiles with different irregular features‚Äîone has a wavy outline, another has a heart-shaped indentation, and the third has a split top. Below, there are various abstract shapes resembling melting forms or puddles with parts that interlock like puzzle pieces." data-lqip="true"><img data-loading="true" loading="lazy" alt="An abstract illustration featuring silhouetted shapes against a solid blue background. The top half shows three simplified, bald human head profiles with different irregular features‚Äîone has a wavy outline, another has a heart-shaped indentation, and the third has a split top. Below, there are various abstract shapes resembling melting forms or puddles with parts that interlock like puzzle pieces." src="https://cdn.sanity.io/images/599r6htc/localized/82da97f2dad2e60cab257c741cf9cdd27fef6b3f-2784x1566.png?w=2784&amp;h=1566&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/82da97f2dad2e60cab257c741cf9cdd27fef6b3f-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 1392w, https://cdn.sanity.io/images/599r6htc/localized/82da97f2dad2e60cab257c741cf9cdd27fef6b3f-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 2088w, https://cdn.sanity.io/images/599r6htc/localized/82da97f2dad2e60cab257c741cf9cdd27fef6b3f-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 2784w"></a></p><p>Noah Levin, Vice President of Product Design at Figma, calls design critiques a ‚Äú<a href="https://www.figma.com/blog/design-critiques-at-figma/">safe space for exploration and feedback</a>.‚Äù It‚Äôs not about approval, but giving a designer what they need to move a project forward.</p></div><p>Inspired by the Figma design team‚Äôs principles and methods for <a href="https://www.figma.com/blog/design-critiques-at-figma/">running design crits</a>, a core group of Figma engineers, led by Ojan Vafai, set out to introduce a process somewhere in between a design crit and a technical review. This was the genesis of Figma‚Äôs <strong>engineering critiques</strong>, dedicated time for the engineering team to brainstorm novel approaches to technical problems, get feedback on existing work, and unblock each other. Today, engineering crits are a core part of our workflow, but it didn‚Äôt start out&nbsp;that&nbsp;way.</p><h2 id="lifting-ideas-up"><a href="#lifting-ideas-up">Lifting ideas up</a></h2><p>In the early stages of an engineering team, everyone is both a landscape architect and a gardener‚Äîthey not only shape the overall concept and design, but also nurture how it comes to life. There‚Äôs often a surplus of work and a dearth of resources at this stage. As the team scales, you begin to bring in more people who are unfamiliar with the existing design, but who have agreed to stewarding the concept. Ojan likened this stage to an anecdote about his grandfather: ‚ÄúI have a vivid memory of my grandfather wanting to build a small garden in our backyard. He asked my siblings and I to dig out the rocks to clear the area, but as soon as the soil was prepared, the garden was off limits.‚Äù He and his siblings weren‚Äôt allowed anywhere near the garden, lest they accidentally step on a burgeoning plant.</p><p>On a high-growth team, the instinct might be the same‚Äîto limit others to the perimeter, allowing them only to clear rocks but never plant flowers. On the one hand, you need help building the garden; on the other hand, you‚Äôre afraid that someone might trample something that‚Äôs been growing for a long time and is about&nbsp;to&nbsp;bloom.</p><div><figure><blockquote><span>The engineering crit plays a very specific role. It‚Äôs a place to solicit feedback early and often. It is a forum to get expert support on technical designs. It is not an approval process.</span></blockquote><figcaption><span>Kris Rasmussen, Chief Technology Officer, Figma</span></figcaption></figure></div><p>When we brought the idea of hosting engineering crit to the broader team, many were skeptical. Design crits center visual UX details, so some of us wondered if it was even possible to critique the design of a technical solution in the same way. Similarly, engineering reviews are often based on sharing a detailed eng spec for approval, and the result is thumbs up or thumbs down. With so many of us used to this binary approach, we wondered how we might architect engineering crits to plant flowers together.</p><p>We reflected on the times when we felt like we were really in the flow as a team; brainstorms and retros‚Äîwhich we run in FigJam‚Äîcame to mind. We also thought about what didn‚Äôt quite work in technical reviews. When we ran technical reviews synchronously, we anchored on the input of just a few team leads; they weren‚Äôt structured for everyone on the call to weigh in. When we ran them asynchronously in other tools, everyone‚Äôs instinct was to share feedback as a comment, which led to unwieldy comment threads that were just a list of disparate pieces of feedback, rather than a conversation. Immediately, the format seemed like the key to unlocking a more collaborative process.</p><div><p><a href="https://www.figma.com/blog/why-roles-are-not-rules/"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAC4jAAAuIwF4pT92AAACPElEQVQokSWPzU6rQABGeVFX7nwB48bowmrQQAckSJtSWwrECA4DAwPy00ILDlhboAt/nubmpusv55x8TJqm6/W6qqo4jgEAZ2dnJycnAIA8z33fn06nEMLVaqWq6sXFxc3NTRAEXdd9fHwghJi6roMgcF2XEDKfz6+vr09PTweDQRAEaZoSQpbLZRRFj4+PLMvKskwICcNQEIS7uzvm6+tL13VZlj3Py7LM87zZbGbbtu/7z8/PoiiOx2OEEITQdV3f94MgME1zOBxKksQ0TaNpGgDA87z9ft/3/Xa7raoKYzwajR4eHjiOwxjvdrv9fh+GIcdxl5eXAABCCJOmqaqqPM8bhtE0zeFw2G63URS9vr4ea67rFkXRtu3hcCCEDAaD8/NzlmXDMGQ8z1NVFQAgy3Icx23b1nVtWdZoNLIsK03T1WpFKf38/NxsNgghTdNeXl5s287z/D8MIVwsFoqiOI6z2Wze3981TRNFEQDw9PSkqmqSJFVVmabJcZwkSRjjPM/rumaCICiKIkkSy7IwxgghnucFQdB1XZIkjuOGw6Hv+2VZGoYhCALP84qi6LqOMWaOl5IkgRASQjDGoihOJpM4jjHGb29vEMIsy6IoMk3Ttm3HcQzDmM1muq4zlNL1ej2fz1mWHY/Hy+WyLEtKadd1u92uLMuiKLIsUxTl9vbWcZyu65qmOaqZ7+9vSulkMrm6urq/vyeE9H3/8/Pz9/fXti1CaLFYRFE0nU6P6+/vb9/3TdNQSv8Br4/ANozyMTIAAAAASUVORK5CYII=" alt="Faces peeking through a brick wall that they're building together." data-lqip="true"><img data-loading="true" loading="lazy" alt="Faces peeking through a brick wall that they're building together." src="https://cdn.sanity.io/images/599r6htc/localized/8e5444ab7a00b475670c9eea1a706122e8944027-2784x1566.png?w=2784&amp;h=1566&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/8e5444ab7a00b475670c9eea1a706122e8944027-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 1392w, https://cdn.sanity.io/images/599r6htc/localized/8e5444ab7a00b475670c9eea1a706122e8944027-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 2088w, https://cdn.sanity.io/images/599r6htc/localized/8e5444ab7a00b475670c9eea1a706122e8944027-2784x1566.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 2784w"></a></p><p>Figma CTO Kris Rasmussen shares more about how we <a href="https://www.figma.com/blog/why-roles-are-not-rules/">lift ideas up</a> with engineering crits.</p></div><p>By running engineering crits in FigJam, we could solicit a lot of feedback in a short amount of time, making it easier for more collaborators to jump in. Rather than having one person talk at a time or respond to a long thread of comments, everyone on the team could contribute. And FigJam‚Äôs open canvas would allow engineers to share early thinking, inspiration, and even screenshots of in-progress work, alongside helpful context or prompts for the broader team. As soon as we piloted our first few engineering crits with our immediate team of eight to ten people, everyone was bought into the collaborative approach. Once we arrived at a repeatable format, we opened up calendar invites and welcomed more and more people into the process, growing to upwards of&nbsp;200&nbsp;people.</p><h2 id="anatomy-of-an-eng-crit"><a href="#anatomy-of-an-eng-crit">Anatomy of an&nbsp;eng&nbsp;crit</a></h2><div><p>We send a calendar invite to every engineer working on the Figma editor and list them as ‚Äúoptional‚Äù so they can pop in or opt out&nbsp;as&nbsp;needed.</p></div><p>Today, the invite list includes the entire organization. While anyone‚Äîeven cross-functional teams‚Äîis free to join, most Figmates opt in if a topic is related to their expertise. Sometimes, we‚Äôll have insights to share on work that doesn‚Äôt directly involve us, and other times, many of us will simply be curious about what‚Äôs going on in the rest of the organization. My teammate and fellow Software Engineer Shirley Miao and I now host these sessions, which we record for&nbsp;posterity.</p><p>Our goals are&nbsp;to:</p><ul><li>Brainstorm and generate&nbsp;ideas</li><li>Identify difficult engineering challenges</li><li>Validate hypotheses</li><li>Share knowledge</li><li>Call out irreversible decisions</li></ul><p>The engineering crit has become core to the early and middle phases of developing technical designs‚Äîand even sometimes in the late phases for a particularly targeted question. It has spread through much of the engineering organization, at every altitude: crits that involve existential questions, ones that center a specific technical challenge, or a series of focused crits to gradually arrive on&nbsp;an&nbsp;approach.</p><div><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC7ElEQVQ4jYWUSXPcNhBG/ddzSeUSH5L8AB/i3LKc4qTiqGRL5VhSjR1JI42kGQ2HHG7DBdwJEgT5UqQSWbIOQdVXaGwPaADdz7pOUcuauq6psoqdLwjihKZt6XuN7jqUUozzxvYwDAxDP6nXmqaRlEVBFlfEpubZOKB7Td/3KNkg3C2ReUEV3CDFhrYI0DKnqxO0TNFVhtx5lM6WMgwpRUJT5HRVhcrlCBx3HBiLVg25tyS6eUuyesPu+pDIOKUKVhTugsK7QlhXnO79weHPP3Hw6ncOXu9jr1Z0eUpf5p+AI7PXiiJ2iK0zhPmBzfyI1dkxwe0x0eod0XrG7fWMX3/7he9fvuD5t9/xxVfP2XvzliwK6fLsc2BHkQdEwYo0XBJsrzFXc1xjRmy+R9gfcd0FV8ack7MjXvzwki+//obXe/tEnotKxAOXB1Ba4aQ2F84plr8gCtZ4OwMnXONHS0KxJMts2sSnck1WsxM+7O1jz8+RjoXeOY+BrVbc7Ez2zv/ibHmK562xxRYjdyeZhUtchCixo/dtOsdE2Ru0Z9H720kPXB7oeo0V+by/PGdhLHEDh3VocyVsbhIHI/YIs5i2yhnKf1U9sB8/yh0wKBNudhZG6LCJXP42LzkyPzL3l2xchzhOpn85/cX+qZ4ARZPjpDvcOMCMthzdHvLn4hUz55x17BEVKUp34w3drxuGT/YTYN5WRGVClCdYqcvMPuHd8oALd4VVhMRtjuq7B7DhkZ4As7okyAVxlRLUAiNx2DgWOz9ElDllJ9Gje/8LZJjCr5KStCgoZY1ULXUrkWVNU0pape5hY/kc1t/f4QQbUHJAVt0U8KqpaWVDW3eo9r8E0dHrflqo9V38PwSOffcnVE1P6ncIr6ZOHaRYkG5tYqumSNu7bFRVk7I0JY5CyrK426Tvp7ooCp6NRqc7mloRORLfjEidY/L1j/hnJ9jzDLGrybIMIQRxHLO1LK4Wl3ieO6W+kVGWJYZh8A8FPrvSBhVOngAAAABJRU5ErkJggg==" alt="Many cursors float around a FigJam with many sections and sticky notes." width="390" height="316" data-lqip="true"><img data-loading="true" width="390" height="316" loading="lazy" alt="Many cursors float around a FigJam with many sections and sticky notes." src="https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?rect=0,2,2364,1915&amp;w=390&amp;h=316&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 195w, https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 293w, https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format 390w, https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 585w, https://cdn.sanity.io/images/599r6htc/localized/972cfe08ce1d34cb9589d8340b2a7220f8a97b02-2364x1918.png?w=390&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 780w"></p></div><figcaption>Running engineering crits in FigJam welcomes more people into the process, while keeping feedback focused.</figcaption></figure><figure><div><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAADcklEQVQ4jT2Sy27cRhBF+f9fYGQVOEEcIItYWchObBiQLVkBLHkkWzOSZjRPckgOH93N5qubTfIEkowsDgpVQN26wC2vbRqElIRxQi4K6sZgu57ODRjjnvpHWtNh7fPc9SPDONL3PVVZkWcZMtOo2OF11lI2Bj8pWW0KwqAhzRxJ3hHHLUlmSZUjTlrCqCYRlqId6XoYf4h2XUfXOZwZ8ZxzmG4gzloW94rlncYPLH5s2QYtwaEjlD3buOEh0OySlqwaaN3IMPSM/Q+G4emA96hubYdKFNHDgWityBKDVBYpDbkwZLkle6yyQegWXXeUTUORZRR+RB0ldIVmtBavqhvaqqZNU/RmTzaPEauUOhG0QiJ8STRXSF9jVUlflri6QheS9WzJ7ck3gssFbZQwlhqv0BVNVeOUIpvvuHl/xfT9hN3tnGizYv5lzvnxDbOzBTo4MCjBWGnasmIzDZm8f2D71cckOWiJ1xqDsxYnCzYXM45fvuHol3ecn18xvZ1y+u4zv7044s3vH/C/L2nDPUOe4JQmngsWZyGHaYLLJWj1HMroHL0qiKcLPh+f8vfRZ87/nTK/m3P58ZK/fv2HT0cn7L5eo+Y32N0CF0eIRUx4GaAWCb0soCqfQxmcYyhLsuWaiw+nfHx7wvRiSjjbsL5Y8u3Td+anZ+z+fUv67QQT3NOLFJNJ6kRji4bBWMauw3Pdo8OOXgnWlxNe/fSSly9+5vrsgv10jz+N2N+u2Vydc3d+TDg5xYRbRlP/eJeecRz+xzOdpXcGJw9srif8+eo1f7x6zc31jK0vWO0E4SYgWs7YLWakqy02l4xPey2d0XRtQWee8fJSU9uGtpHILGa3C9jtQ7LH1HXNQZbkuUCL5OlVyrLCmBbnDE2VIQ4bZLRGRlvizQov2EcIrShM9SSSpgqlS7SpyYuSJNMIXaGbEllopK6orKEbLLrOOEQPiGBJuHzg8myCF29ukOkaqRP2+xj/ISD3fYo0JM9z0lwjq5qiKpAifxItTYt9dGgVWoc0ak+0X/NlcoWXhxOK8BoZLdkvQ/zZnnx+j767pthtUbJE1w1FqZAyQ2mFVopGptg6x3Wa3pXoKmMdrvGkDlDpEhGuiLYHglXK4e4O8eUMcTMljwSqKBFKkuYpWZYjd3v06h6TB/SdeUq3aSqWqyX/AUsmqeHH+JthAAAAAElFTkSuQmCC" alt="Many cursors float around a FigJam with many sections and sticky notes." width="804" height="649" data-lqip="true"><img data-loading="true" width="804" height="649" loading="lazy" alt="Many cursors float around a FigJam with many sections and sticky notes." src="https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?rect=0,1,2466,1991&amp;w=804&amp;h=649&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=1.5 1206w, https://cdn.sanity.io/images/599r6htc/localized/398b54714391b419158654d219c97966800d951f-2466x1992.png?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1608w"></p></div></figure></div><p>Now that engineering crits are such a central part of our work, we have the process dialed in. Here‚Äôs a look at what we expect before, during, and after the&nbsp;session.</p><h4 id="before-the-crit"><a href="#before-the-crit">Before the crit</a></h4><div><p>I‚Äôll admit that we sometimes take a shortcut and paste screenshots of targeted parts of our design doc into a FigJam file, rather than filling out the template from scratch, but this is the exception, not&nbsp;the&nbsp;rule!</p></div><p>To run engineering crits as efficiently as possible, we ask engineers to do some prep work. The must-have is a FigJam file with the design they want critiqued. We use <a href="https://www.figma.com/community/file/1259186143617682581/engineering-crit?searchSessionId=lsuz5497-4bthwob3vxr" target="_blank" rel="noreferrer">this template</a>, which has different prompts than a technical design doc. Reviewers need to quickly get the context they need and give meaningful feedback on it, as we know they don‚Äôt have time to digest full&nbsp;PRDs.</p><p>Before we begin, the presenter provides background context and frames the discussion: Are they looking for high-level ideas on large-scale architecture, or are they diving deep on a specific component? They also provide some details on ideas they‚Äôve considered and leave plenty of room for&nbsp;discussion.</p><h4 id="during-the-crit"><a href="#during-the-crit">During the crit</a></h4><p>Engineering crits are explicitly <em>not</em> an approval or decision-making meeting. They are about getting the right feedback to lift ideas up. We ask reviewers to lead with suggestions as opposed to mandates, because we want to ensure we use the time in these meetings to capture as much of the collective learnings and wisdom of the group. We have other forums, including technical reviews, to ensure key feedback is acknowledged and&nbsp;acted&nbsp;on.</p><p>After that, the bulk of the meeting is still synchronous, but silent, as reviewers leave stickies in FigJam. This means we are able to have multiple conversations in parallel‚Äîallowing everyone to speak up‚Äîand reviewers can focus on the part of the conversation where they have the most expertise. If there‚Äôs time at the end of the meeting, the facilitator will turn attendees‚Äô attention to other discussion topics or questions that might help the working&nbsp;group.</p><h4 id="after-the-crit"><a href="#after-the-crit">After the crit</a></h4><p>In most cases, one crit is enough for a team to figure out their approach, but sometimes they come out of the crit concluding that none of the proposed options are feasible. In that case, they‚Äôll go back to the drawing board and attend another&nbsp;crit.</p><p>Of course, the process isn‚Äôt perfect, and it‚Äôs unrealistic to outline every possible edge case. Engineering crits are also not meant to replace technical reviews entirely. If we‚Äôre working on features that touch many different engineering teams or feel particularly high stakes, the engineering crit is a solid stepping stone on the way to a technical review.</p><h2 id="an-example-real-eng-crit"><a href="#an-example-real-eng-crit">An example (real) eng&nbsp;crit</a></h2><p>We recently launched <a href="https://www.figma.com/blog/introducing-ai-to-figjam/">AI in FigJam</a>, which helps teams overcome the ‚Äúblank canvas‚Äù problem. It‚Äôs the perfect example of how a project flows through the engineering crit process and comes out better as a result. Here‚Äôs a look at how this project evolved at every stage of the engineering crit:</p><div><figure><div><p><img src="data:image/jpeg;base64,/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAANABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQCBQYH/8QAIRAAAQMEAwADAAAAAAAAAAAAAQIDBAAREiEFEzEiQZH/xAAWAQEBAQAAAAAAAAAAAAAAAAACBQb/xAAZEQADAQEBAAAAAAAAAAAAAAAAARECEyH/2gAMAwEAAhEDEQA/AOdcVHLsVeGyVDWqfMRnBfX2h24BzSLUlxS+pgFN/md7q6mERojT7efYoEm6vv8AKrc21TMZymqzP8k0hmQEpFxiD5RUuXnOTZYeeCcygDQt5RQjBqXw/9k=" alt="A series of gardening illustrations corresponding to each stage in the engineering crit process." width="804" height="536" data-lqip="true"><img data-loading="true" width="804" height="536" loading="lazy" alt="A series of gardening illustrations corresponding to each stage in the engineering crit process." src="https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;h=536&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 402w, https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 603w, https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;q=75&amp;fit=max&amp;auto=format 804w, https://cdn.sanity.io/images/599r6htc/localized/9c4dc06426d780986529b4e3edcae2066d5edbf0-1056x704.jpg?w=804&amp;q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1056w"></p></div><figcaption></figcaption></figure></div><h4 id="step-1-scope"><a href="#step-1-scope">Step 1: Scope</a></h4><p><strong>Goal:</strong> Gut check concepts with a diverse group of people across engineering, design, and product management.</p><p>The AI team kicked off an engineering crit by exploring how we might leverage AI to empower users to work more efficiently in FigJam. We‚Äôve been thinking a lot as a team about how to bring AI into Figma and FigJam, so the design team began dreaming up ‚Äúwhat ifs‚Äù while the engineering team started reasoning about this problem space at its highest level, offering context around how prompt engineering works. The AI team shared one requirement: AI features need to read and write content on the current file. Other folks with expertise in areas like machine learning helped provide more information from various perspectives, identified challenges, or surfaced relevant solutions we‚Äôve used in the past. The team shared four options, each with pros and cons, and aimed to get a sense for overall feasibility and architecture. Ultimately, they came away with a gut check on which approaches might be realistic, and which might be particularly challenging. Based on the feedback, they were empowered to take one of the options and start running with it, prototyping out an end-to-end functioning flow.</p><h4 id="step-2-iterate"><a href="#step-2-iterate">Step 2: Iterate</a></h4><p><strong>Goal: </strong>Solicit feedback at a regular cadence as the work evolves, addressing new challenges and revisiting early work&nbsp;as&nbsp;needed.</p><p>Next, the team sought feedback from a wider range of engineering teams because building AI functionality into FigJam required touching many different parts of the Figma editor. At this point, they also wanted guidance on how to score AI models. They posed a question to the group: ‚ÄúHow might we improve our processes around measuring quality?‚Äù</p><p>In response, crit participants shared that the current system for quality measurement was manual: a large spreadsheet with an evaluation of how comprehensive, precise, structurally sound, and useful a model is. Rather than trying to engineer a quantitative approach, the ML team suggested focusing on qualitative attributes by running AI feature bashes for feedback on AI summarization and generation; since the dataset is so small, a more quantitative approach wouldn‚Äôt have been statistically rigorous anyway.</p><h4 id="step-3-refine"><a href="#step-3-refine">Step 3: Refine</a></h4><p><strong>Goal:</strong> Focus on polishing specific aspects of the project with relevant experts.</p><p>When it comes to building AI features, the devil is in the details. In this case, the team chose to do another deep dive crit specifically on versioning and got more feedback from the ML platform team on how we want to think about updating the version of our AI prompts and safely roll-out changes. In doing so, they learned about an existing versioning system we already use in <a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/">Figma‚Äôs multiplayer technology</a>, and went back to the drawing board with this newfound knowledge. This system also helped inform the technical details around data input and summarization.</p><h4 id="step-4-review"><a href="#step-4-review">Step 4: Review</a></h4><p><strong>Goal: </strong>Make any outstanding decisions and review the plan holistically.</p><p>At this point, the team made final decisions informed by earlier crit feedback. In this case, they didn‚Äôt need to attend a crit because the directly responsible individual (DRI) had enough information to make a confident decision. They were empowered to move forward, without working their way through a more formal approval checklist.</p><h4 id="step-5-ship"><a href="#step-5-ship">Step 5: Ship</a></h4><p><strong>Goal: </strong>Attend technical reviews as needed, move forward with the project, and schedule any follow-up crit sessions for further&nbsp;work.</p><p>While this project didn‚Äôt go through a formal technical review, the working group did follow up with specific subject matter experts and stakeholders. After getting the green light on implementation, the team shipped <a href="https://www.figma.com/blog/introducing-ai-to-figjam/">FigJam AI</a>.</p><div><p><a href="https://www.figma.com/community/file/1259186143617682581" target="_blank" rel="noreferrer"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB2ElEQVQokY2SO28TURCF/e+REA0NBUgU/AEaRAEUNEiIIFkoCpCkMDGK117b613vfc19zId21xCXFEe3uKOjc2a+GapQAmQzqXgoAprR4Q/G9381o0TwSzh+nWSuICwh96CFwfLcOKWMhEh0hWiV6BMppn8zM7KD7jNUL2H1HNav4PARYo1qeUiIUkomBIe1FjEJMYXeCJ0LSJoazcgWmg+wfAx3j9DfT9H9W1TWaEkMDYa3qCC5w4QNJtRI7PESqW1i2QX2xhNT+mv4Hl0+gdULdPcGPV6gskHlgNqK4raINBzDHQf/nTZc04cVne1ZG+FX66kOR7wPU2UdKt4/Q7evoZ+DuwWpwVbo4ZrcLrBmRWN/snUX7Nycpl/QdA07G6htpLXhlHC4sLlCm3fQfkLNJbm/Jro1JRzAbSh+j8jhLOENJlRTQhtZuUKflDJdWdBwP5maS2L7jW4zp9kuCN5Me9RM0UhIHcbXONkTkx13uB0MTcKkMhI4YqNSgbtBzQ/sbk69/MLm/gZjekp5QCbnhPcO7zziEsEleus5Ok/Mw5UHwwHg1IJUY1LXLWg2t7T7ihD8ia9zDhNx4NDnkUEJQkrxjMOR8HzCI1JyIEZ/GnoA+/8EfwAyVlGUDmfRvwAAAABJRU5ErkJggg==" alt="An abstract wireframe in FigJam with a thumbs up." data-lqip="true"><img data-loading="true" loading="lazy" alt="An abstract wireframe in FigJam with a thumbs up." src="https://cdn.sanity.io/images/599r6htc/localized/ae1c410c4aae6881124858b1ce46ed6623a80f61-1706x960.png?w=1706&amp;h=960&amp;q=75&amp;fit=max&amp;auto=format" srcset="https://cdn.sanity.io/images/599r6htc/localized/ae1c410c4aae6881124858b1ce46ed6623a80f61-1706x960.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.5 853w, https://cdn.sanity.io/images/599r6htc/localized/ae1c410c4aae6881124858b1ce46ed6623a80f61-1706x960.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=0.75 1280w, https://cdn.sanity.io/images/599r6htc/localized/ae1c410c4aae6881124858b1ce46ed6623a80f61-1706x960.png?q=75&amp;fit=max&amp;auto=format&amp;dpr=2 1706w"></a></p><p>You can try out engineering crits with your team with <a href="https://www.figma.com/community/file/1259186143617682581" target="_blank" rel="noreferrer">this template</a>, the same one we use&nbsp;at&nbsp;Figma.</p></div><p>Many projects won‚Äôt fit neatly into this structure; some steps are collapsed into one crit, while others require a series of dedicated sessions. Still, these five steps are helpful scaffolding for anyone who isn‚Äôt sure where to begin. As we continue to evolve the process, our priority is making engineering crits a process that teams look forward to because it accelerates their work, rather than feeling like it gets in&nbsp;the&nbsp;way.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airbnb is banning indoor security cameras (227 pts)]]></title>
            <link>https://www.theverge.com/2024/3/11/24097107/airbnb-indoor-security-camera-ban</link>
            <guid>39669167</guid>
            <pubDate>Mon, 11 Mar 2024 15:08:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/3/11/24097107/airbnb-indoor-security-camera-ban">https://www.theverge.com/2024/3/11/24097107/airbnb-indoor-security-camera-ban</a>, See on <a href="https://news.ycombinator.com/item?id=39669167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Airbnb will no longer allow hosts to use indoor security cameras, regardless of where they‚Äôre placed or what they‚Äôre used for. In <a href="https://news.airbnb.com/an-update-on-our-policy-on-security-cameras/">an update on Monday</a>, Airbnb says the change to ‚Äúprioritize the privacy‚Äù of renters goes into effect on April 30th.</p><p>The vacation rental app <a href="https://web.archive.org/web/20231223165134/https://www.airbnb.com/help/article/3061">previously let</a> hosts install security cameras in ‚Äúcommon areas‚Äù of listings, including hallways, living rooms, and front doors. Airbnb required hosts to disclose the presence of security cameras in their listings and make them clearly visible, and it prohibited hosts from using cameras in bedrooms and bathrooms.</p><p>But now, hosts can‚Äôt use indoor security cameras at all. The change comes after <a href="https://www.newsweek.com/airbnb-camera-hidden-couple-reddit-lawsuit-1743269">numerous reports</a> of <a href="https://www.foxnews.com/tech/couple-finds-hidden-camera-disguised-as-smoke-detector-in-florida-airbnb">guests</a> finding <a href="https://www.buzzfeed.com/bradesposito/people-keep-finding-hidden-cameras-in-their-airbnb">hidden cameras</a> within their rental, leading some vacation-goers to <a href="https://www.theverge.com/23550845/smartphone-hidden-camera-android-ios-how-to">scan their rooms</a> for cameras.</p><p><a href="https://airbnb.pvxt.net/c/482924/264339/4273?u=https%3A%2F%2Fwww.airbnb.com%2Fhelp%2Farticle%2F3061%23%3A~%3Atext%3Dhere%2520to%2520help-%2CWhat%2520we%2520do%2520allow%2Cof%2520a%2520reservation%2520are%2520permitted.">Airbnb‚Äôs new policy</a> also introduces new rules for outdoor security cameras, and will now require hosts to disclose their use and locations before guests book a listing. Hosts can‚Äôt use outdoor cams to keep tabs on indoor spaces, either, nor can they use them in ‚Äúcertain outdoor areas where there‚Äôs a great expectation of privacy,‚Äù such as an outdoor shower or sauna. </p><p>Additionally, listings will have to disclose noise decibel monitors, which hosts might use to measure <a href="https://www.theverge.com/circuitbreaker/2018/10/29/18037604/noiseaware-gen-3-indoor-outdoor-security-microphone">whether there‚Äôs a party going on</a> in their rental ‚Äî <a href="https://www.theverge.com/2022/8/17/23309433/airbnb-anti-party-technology-north-america-usa-canada-party-ban">something that Airbnb banned</a> in 2022. ‚ÄúThese changes were made in consultation with our guests, hosts, and privacy experts, and we‚Äôll continue to seek feedback to help ensure our policies work for our global community,‚Äù Juniper Downs, Airbnb‚Äôs head of community policy and partnership, says in a statement. </p><p>Airbnb hosts will have until the end of April to remove the security cameras from inside their listings. If a guest reports the presence of an indoor camera after that, Airbnb says it will investigate and that it could remove the host‚Äôs listing or account as a result. The new policy still can‚Äôt control the presence of hidden cameras, but it will at least offer some peace of mind knowing that rule-abiding hosts can no longer put cameras anywhere in their rentals.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel Gaudi2 chips outperform Nvidia H100 on diffusion transformers (140 pts)]]></title>
            <link>https://stability.ai/news/putting-the-ai-supercomputer-to-work</link>
            <guid>39669008</guid>
            <pubDate>Mon, 11 Mar 2024 14:56:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/news/putting-the-ai-supercomputer-to-work">https://stability.ai/news/putting-the-ai-supercomputer-to-work</a>, See on <a href="https://news.ycombinator.com/item?id=39669008">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709158639151_29172">
  <p>In our <a href="https://stability.ai/news/using-the-new-ai-supercomputer"><span>last installment</span></a>, we spoke about how we plan to utilize our state-of-the-art AI Supercomputer.</p><p>In this installment, we delve deeper into performance benchmarks and benefits of various compute solutions.</p><p>Our commitment to developing cutting-edge open models in multiple modalities necessitates a compute solution capable of handling diverse tasks with efficiency. To this end, we conducted a performance analysis, training two of our models, including the highly anticipated <a href="https://stability.ai/news/stable-diffusion-3"><span>Stable Diffusion 3</span></a>.</p><p>In our analysis, we compared the training speed of Intel Gaudi 2 accelerators versus Nvidia's A100 and H100, two of the most common choices for startups and developers training LLMs.</p><p><strong>Model 1:</strong></p><p><a href="https://stability.ai/news/stable-diffusion-3"><span><strong>Stable Diffusion 3</strong></span></a> is our most capable text-to-image model, soon to be in <a href="https://stability.ai/stablediffusion3"><span>early preview</span></a>.&nbsp;</p><p>Upon public release of Stable Diffusion 3, it will be available in sizes ranging from 800M to 8B parameters. Our analysis utilized the 2B parameter version and showed pleasantly surprising results.&nbsp;</p><p>We measured the training throughput for the 2B <a href="https://stability.ai/news/stable-diffusion-3-research-paper"><span>Multimodal Diffusion Transformer</span></a> (MMDiT) architecture model with d=24, BFloat16mixed precision, optimized attention (xFormers for A100 and the FusedSDPA for Intel Gaudi). We call this model version MMDiT-ps2-d24.</p><p>First, let‚Äôs examine our training benchmark results across 2 nodes, a total of 16 accelerators (Gaudi/GPU). Here‚Äôs an excerpt of the raw data:</p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1709158639151_25063">
  <p>In this configuration, the Gaudi 2 cluster processed over 3x more images per second, compared to A100-80GB GPUs. This is particularly impressive considering that the A100s have a very optimized software stack.&nbsp;</p><p>On inference tests with the Stable Diffusion 3 8B parameter model the Gaudi 2 chips offer inference speed similar to Nvidia A100 chips using base PyTorch. However, with TensorRT optimization, the A100 chips produce images 40% faster than Gaudi 2. We anticipate that with further optimization, Gaudi 2 will soon outperform A100s on this model. In earlier tests on our SDXL model with base PyTorch, Gaudi 2 generates a 1024x1024 image in 30 steps in 3.2 seconds, versus 3.6 seconds for PyTorch on A100s and 2.7 seconds for a generation with TensorRT on an A100.&nbsp;</p><p>The higher memory and fast interconnect of Gaudi 2, plus other design considerations, make it competitive to run the Diffusion Transformer architecture that underpins this next generation of media models.</p><p><br><strong>Model 2:</strong></p><p><a href="https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models"><span><strong>Stable Beluga 2.5 70B</strong></span></a><strong> </strong>is our fine-tuned version of LLaMA 2 70B, building on the <a href="https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models"><span>Stable Beluga 2</span></a> model which was the first open model to best ChatGPT 3.5 in select benchmarks. We ran this training benchmark on 256 Gaudi 2 accelerators. Running our PyTorch code out of the box, with no extra optimizations, we measured an impressive total average throughput of 116,777 tokens/second. More specifically, this involves using a FP16 datatype, a global batch size of 1024, gradient accumulation steps of 2, and micro batch size of 2.</p><p>On inference tests with our 70B language model on Gaudi 2, it generates 673 tokens/second per accelerator, using an input token size of 128 and output token size of 2048. In comparison to <a href="https://nvidia.github.io/TensorRT-LLM/performance.html"><span>TensorRT-LLM</span></a>, Gaudi 2 appears to be 28% faster than the 525 tokens/second for the A100. We also anticipate further speed improvements with FP8.</p><p>Companies like ours face an increasing demand for more powerful and efficient computing solutions. Our findings underscore the need for alternatives like the Gaudi 2, which not only offers superior performance to other 7nm chips, but also addresses critical market needs such as affordability, reduced lead times, and superior price-to-performance ratios. Ultimately, the opportunity for choice in computing options broadens participation and innovation, thereby making advanced AI technologies more accessible to all.</p><p>Stay tuned for more insights in our next installment of "Behind the Compute."&nbsp;</p><p>To stay updated on our progress follow us on <a href="https://twitter.com/stabilityai"><span>Twitter</span></a>, <a href="https://www.instagram.com/stability.ai/"><span>Instagram</span></a>, <a href="https://www.linkedin.com/company/stability-ai"><span>LinkedIn</span></a>, and join our <a href="https://discord.gg/stablediffusion"><span>Discord Community</span></a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Onedoc (YC W24) ‚Äì A better way to create PDFs (199 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39668962</link>
            <guid>39668962</guid>
            <pubDate>Mon, 11 Mar 2024 14:52:56 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39668962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39671945"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39671945" href="https://news.ycombinator.com/vote?id=39671945&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Congratulations on the launch ‚Äî it looks fantastic! My company is also developing a similar product. We've chosen to create a visual report designer that enables end-users (non-developers) to create and tweak PDF reports, and integrate with the existing IT infrastructure via the API. Our experience is that users want changes in reports very often and that it's best to allow them do it on their own.<p><a href="https://www.cx-reports.com/" rel="nofollow">https://www.cx-reports.com</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39671939"><td></td></tr>
                <tr id="39671996"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671996" href="https://news.ycombinator.com/vote?id=39671996&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>We spent many hours designing and generating PDFs at our previous venture.. terrible experience. Which is why we're now focused on solving this issue!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670933"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670933" href="https://news.ycombinator.com/vote?id=39670933&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I've also spent much longer than I'd like on this same problem. Having a lightweight-enough service to convert html-&gt;pdf on the fly, with good fidelity, and that can create an <i>accessible</i> pdf seems to be impossible.<p>If you can nail accessible PDFs then you'd open up a <i>very</i> big government market.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671067"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671067" href="https://news.ycombinator.com/vote?id=39671067&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>We felt the same, and that's precisely why we built this tool! The key, as you mentioned, is fidelity, especially for designing complex layouts. We hope to bring something new and valuable to the table. And yes, documents are central to many industries including government, legal, banking etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671252"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671252" href="https://news.ycombinator.com/vote?id=39671252&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Can you directly answer whether your tool generates <i>tagged</i> PDFs?<p>Of course, you can't guarantee that the resulting document is 100% compliant because you can't enforce that the input is valid, but are you at least outputting a complete tag tree with as much semantics as possible given the input?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671430"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671430" href="https://news.ycombinator.com/vote?id=39671430&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Yes, Onedoc generates tagged PDFs as long as you add a `title` property to the API call to make the PDF UA/1 compliant.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670314"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670314" href="https://news.ycombinator.com/vote?id=39670314&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>FYI: the open source state of the art in this area is Playwright (the successor to Puppeteer) with Paged.js (<a href="https://pagedjs.org/" rel="nofollow">https://pagedjs.org/</a>). I highly recommend that everyone check out and donate to paged.js, it's a fantastic project with lots to like. It certainly blows commercial alternatives like Prince XML out of the water.<p>That forms a solid foundation that I find it hard to imagine paying for. The things where you might still command a premium are basically safety mechanisms/CI checks/library components that ensure the PDF renders correctly in the presence of variable-length content, etc. as well as maybe PDF-specific features like metadata and fillable forms. Naive ways to format headers, footers, tables/grids/flexboxes etc. often fail in PDFs because of unexpected layout complications. So having a methodology, process, and validation system for ensuring that a mission critical piece of information appears on a PDF in the presence of these constraints could be attractive.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671264"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671264" href="https://news.ycombinator.com/vote?id=39671264&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I think <a href="https://github.com/diegomura/react-pdf">https://github.com/diegomura/react-pdf</a> is closer to what this company is doing.<p>In fact their open source library, <a href="https://github.com/OnedocLabs/react-print-pdf">https://github.com/OnedocLabs/react-print-pdf</a>, seems like a higher-level library that sits above react-pdf. Reminds me a lot of the set of react-pdf based components I built for a corporate job where letting users create PDFs was a huge part of the value proposition.</p><p>They're solving a really cool problem, actually, because building out into certain difficult use cases like SVG support was a huge pain.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670434"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670434" href="https://news.ycombinator.com/vote?id=39670434&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>We are currently experimenting with this approach. A good thing about paged.js is that we would be able to provide hot-reload and live preview of files without actually converting to PDF.<p>Your second point is very interesting, seems like some kind of .assert('text').isVisible() API. We may want to dig into that further!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670470"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670470" href="https://news.ycombinator.com/vote?id=39670470&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>(How) does it handle CMYK and print PDFs? I see images of printed books created by Paged.js, were these post-processed, or printed using a printer that does a best-effort RGB conversion?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670569"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670569" href="https://news.ycombinator.com/vote?id=39670569&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I'm not sure - we don't do color correction on our PDFs because we don't have photos in them and color rendering is not mission critical - but paged.js is focused on the concern of layout for print media. I would imagine color rendering can be solved orthogonally to what paged.js does for you, as long as you specify the color data in CSS. I'm pretty sure paged.js will pass it through without messing with it, so you're good if the browser that Playwright/puppeteer is driving supports the correct color profile when emitting the PDF. I honestly don't know if browsers have sufficient support for that when emitting a PDF, though.<p>Overall you're right that color correction is another area where you could probably command a premium.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671894"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671894" href="https://news.ycombinator.com/vote?id=39671894&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It's certainly an area with more depth than I anticipated when I first started getting into it. Adobe is still pretty much the only one that can get a PDF compliant with print standards.<p>As far as I know, there's no way to currently get colors adhering to print color profiles in CMYK out of browsers.</p><p>Indeed, if color correctness isn't mission critical, I can imagine that going with Paged.js can be a nice experience!</p><p>(Edit: in my experience so far, it's been really really hard to 'correct' colors from an existing PDF in a way that gets a satisfying end result---the colors are usually muted/washed out)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671992"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39671992" href="https://news.ycombinator.com/vote?id=39671992&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I was curious and searched around and found this presentation: <a href="https://www.w3.org/Graphics/Color/Workshop/slides/Erias.pdf" rel="nofollow">https://www.w3.org/Graphics/Color/Workshop/slides/Erias.pdf</a><p>You're right - although many of the building blocks are there, it appears there is no way to specify a colorspace or print profile when asking Chrome to emit a PDF (and I doubt the other browsers are any better). Skia (the PDF rendering engine that Chromium uses) actually supports colorspace transforms, but Chromium doesn't seem to hook that up to CSS or even support non-RGBA colors in its rendering pipeline.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="39669425"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669425" href="https://news.ycombinator.com/vote?id=39669425&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>May be this is just me but this looks extremely costly to me! It will cost $2,500 to generate 50,000 PDFs. Are edits/corrections additional cost?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671397"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671397" href="https://news.ycombinator.com/vote?id=39671397&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It sounds like this is as advanced as DocRaptor[1]. They have what I consider to be the best PDF generation API, giving complete control over the documents you need to create. The pricing is similar.<p>If you'd rather do it for free weasyprint[2] is the best open source alternative.</p><p>Another more affordable option you might want to consider is Urlbox[3]. (Disclosure: I work on this)</p><p>Urlbox's rendering engine is based on Chrome. It's been refined over the last 11 years to render pages as images or PDFs[4] that look great. I was a customer for 5 years before I joined the team. Everything we'd tried before Urlbox was a disappointment.</p><p>Urlbox probably can't match the power of either Onedoc or DocRaptor, but pricing starts at less than $0.01 per document and drops significantly with scale. If your PDF looks great when saving as PDF in Chrome it should look identically brilliant with Urlbox.</p><p>[1]: <a href="https://docraptor.com/" rel="nofollow">https://docraptor.com</a>
[2]: <a href="https://weasyprint.org/" rel="nofollow">https://weasyprint.org</a>
[3]: <a href="https://urlbox.com/" rel="nofollow">https://urlbox.com</a>
[4]: <a href="https://urlbox.com/html-to-pdf" rel="nofollow">https://urlbox.com/html-to-pdf</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39669512"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669512" href="https://news.ycombinator.com/vote?id=39669512&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>This is a good point, and we are still trying to figure out how to price things fairly. Depending on the type of PDF, whether it is a simple receipt or a large multi-pages report, associated costs are very different on our side. At this time, we rely on other proprietary software that we are aiming to replace but that incur high costs on our side as well.<p>Edits and corrections on generated PDFs is not provided as the PDFs are signed as-is, however you can attach the metadata to the PDF and rerender with the modifications.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670331"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670331" href="https://news.ycombinator.com/vote?id=39670331&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>As a point of reference on pricing, convertAPI charges $0.05 per document conversion at their most expensive tier, and with any level of fixed commitment ($80 - $300 per month) it goes down to $0.016-0.006 per document.<p>Their PDF conversion is pretty good (I use it for PPT/Word -&gt; PDF conversion), though your product is obviously different and has different/better capabilities for programmatic PDF creation. Still, a reference point.</p><p>Pricing page: <a href="https://www.convertapi.com/prices" rel="nofollow">https://www.convertapi.com/prices</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39669654"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39669654" href="https://news.ycombinator.com/vote?id=39669654&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Edits would be limited to certain pages but may spill over (e.g. tables) so the whole PDF need not be generated. Only edited pages can be inserted back to previously generated PDFs. Could be an optimization to reduce cost.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669631"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669631" href="https://news.ycombinator.com/vote?id=39669631&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I second this. Maybe I'm missing something in the value proposition, but we already generate PDFs from .docx/.html templates using open source libraries and Docker microservices.<p>Do not misunderstand. A Stripe for generating PDFs can be great, but for a small team, $0.50/PDF is way more than I can afford (after all, you can create a small number of PDFs without too much fuss). Maybe you are oriented towards large companies?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670787"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670787" href="https://news.ycombinator.com/vote?id=39670787&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Indeed, and as you mentioned, open-source libraries are always an option. It's worth noting that our open-source library assists in document design, allowing freedom in renderer choice. While the open-source library is aimed at individuals, our API targets businesses of any size. Our pricing can be as low as $0.05 per PDF for high-volume or annual commitments. Additionally, we offer cloud hosting for your documents for up to 90 days, and our pricing includes analytics.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670373"><td></td></tr>
                  <tr id="39671520"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39671520" href="https://news.ycombinator.com/vote?id=39671520&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Looks awesome, will keep this in mind - every so often you need to create complex documents in code, and it's always a pain. Doing it with a familiar modern programming interface would be nice.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671577"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671577" href="https://news.ycombinator.com/vote?id=39671577&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Exactly, that's one of the main reasons we began working on this. We aim to bring the modern web technologies used for website design into the document world. This includes enabling the use of React and, of course, Tailwind, Chakra UI, etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670676"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670676" href="https://news.ycombinator.com/vote?id=39670676&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Love the demo on the homepage with the render button. Really helps explain the product!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670796"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670796" href="https://news.ycombinator.com/vote?id=39670796&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Thanks! We try to make our product as accessible as possible for anyone to use (or at least to test). It's good to hear that our efforts have been worthwhile!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669738"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669738" href="https://news.ycombinator.com/vote?id=39669738&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Really interesting product. I do agree that the pricing seems steep ($0.25/document on Pro on the most generous tier) but I don't know enough about pricing B2B products to know if that would be a blocker.<p>I agree that HTML -&gt; PDF can be a really powerful tool. I worked on the UK government's tool to generate energy efficiency labels for consumer goods [0] and we ended up doing PDF generation with SVG templates, using Open HTML to PDF for the conversion. That ended up working very well, though as you allude to there can be some gotchas (eg unsupported CSS features) that you need to work around.</p><p>A few questions:</p><p>- Do the rendered documents support PDF's various accessibility features?</p><p>- How suitable is this for print PDF generation? For example, what version of the PDF spec do you target? What's your colour profile support like? Do you support the different PDF page boxes (MediaBox, CropBox, BleedBox, TrimBox, ArtBox)?</p><p>[0] <a href="https://github.com/UKGovernmentBEIS/energy-label-service">https://github.com/UKGovernmentBEIS/energy-label-service</a></p><p>[1] <a href="https://github.com/danfickle/openhtmltopdf">https://github.com/danfickle/openhtmltopdf</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670010"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670010" href="https://news.ycombinator.com/vote?id=39670010&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>The pricing does go down for larger volumes and is something we still have to narrow down to the exact place that makes sense to companies and is also viable.<p>- We do not force PDF/* profiles down to the user, but it seems that for most of them PDF/UA-1 would be a sensible default. We can extract most of the tags from the HTML semantics by themselves which makes it much easier.</p><p>- We target the PDF 1.7 spec. Color profiles can be changed and you can use a custom .icc profile, with the corresponding embedding restrictions based on the document format. MediaBox is supported through the @page size property. Bleed, trim and marks can be added using vendor specific css properties. We don't support ArtBox yet but this is something we can look into! So far none of our customers really wanted to take this out to a real print shop, but we would be glad to help people go down this route :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671287"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671287" href="https://news.ycombinator.com/vote?id=39671287&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>So are you saying that you don't output tagged PDFs now?<p>For those who don't know, if you use Chromium's print-to-pdf feature you get a tagged PDF. And it's scriptable from the command-line too.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39669806"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669806" href="https://news.ycombinator.com/vote?id=39669806&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I had to deal a lot with PDF generation over the past few years and I was very unhappy with the eco-system that was available:<p>1. HTML-to-PDF: The web has a great layout system that works well for dynamic content. So using that seems like a good idea. BUT it is not very efficient as a lot of these libraries simply spin up a headless browser or deal with virtual doms.</p><p>2. PDF Libraries (like jsPDF): They mostly just have methods like ".text(x, y, string) which is an absolute pain to work with when building dynamic content or creating complex layouts.</p><p>This was such a pain point in various projects I worked on that I built my own library that has a component system to build dynamic layouts (like tables over multiple pages) and then computes that down to simple jsPDF commands. Giving you the best of both worlds.</p><p>Hope this makes somebody's life a bit easier: <a href="https://github.com/DevLeoko/painless-pdf">https://github.com/DevLeoko/painless-pdf</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670020"><td></td></tr>
                <tr id="39670140"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670140" href="https://news.ycombinator.com/vote?id=39670140&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Yes, page breaks are probably the most significant difference between the layout of a web page and a PDF document, and thereby a major drawback when using HTML-to-PDF. There is little to no tooling for this in the web.<p>If you want granular control over how your PDF will look with content that is more than one page long, you will have a hard time using html.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670251"><td></td></tr>
            <tr id="39670311"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39670311" href="https://news.ycombinator.com/vote?id=39670311&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>That's what we are trying to solve at Onedoc, we want developers to be able to have full control over the PDF layout as they write content. react-print is built with the intention of creating the illusion that React was meant for PDFs.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39671294"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39671294" href="https://news.ycombinator.com/vote?id=39671294&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Hmm interesting... I just went through this user experience on iOS generating PDF invoices locally. I attempted the HTML &gt; PDF route, but Webkit is thorny wrt to layouts (as you mentioned). I did settle in with drawing everything from the ground up &gt; which with LLMs wasn't as hairy as it used to be, even got a little Swift framework out of the deal.<p>Am I understanding the docs correctly that you don't have a local library available (the SDKs are just calling the APIs right?)? Mind going through why you chose a remote API?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671497"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671497" href="https://news.ycombinator.com/vote?id=39671497&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>You are right in the sense we do not provide a local library. We considered the option but would have brought a lot of challenges to accommodate the various runtimes and device capabilities.<p>This may come at a later stage once we have built our own rendering engine though
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39671553"><td></td></tr>
                <tr id="39671571"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671571" href="https://news.ycombinator.com/vote?id=39671571&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>There are many reasons behind it, to name a few: files are self-contained(*) and easily portable, can guarantee some security features, the format is easily extended, and the ecosystem is very large.<p>It seems that a better format should exist, but the fact that PDF is the de-facto for portable documents make it unlikely things can change overnight.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669582"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669582" href="https://news.ycombinator.com/vote?id=39669582&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Super interesting and potentially a fit for a project I'm working on right now. What are the benefits of going this route vs styling your page for print (ex. tailwind print modifier) and relying on the browser's print dialogue?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39669652"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669652" href="https://news.ycombinator.com/vote?id=39669652&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>There is both commonalities and differences! Both approaches rely on web technology to provide the layout and are flexible in terms of frameworks and integrations.<p>Where things differ is that we don't actually use a browser under the hood. This allows a much better control over typesetting and layout - and you can do it on the server. We have also more controls over the outputted PDF and the ability to use more advanced features such as form fields or embedding other files and metadata in the PDF.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669634"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669634" href="https://news.ycombinator.com/vote?id=39669634&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Is this just a wrapper around Puppeteer that renders a pdf? I do this currently with an AWS lambda that has a chrome-aws-lambda layer.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39669717"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669717" href="https://news.ycombinator.com/vote?id=39669717&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>We use a dedicated HTML to PDF engine (such as PrinceXML) rather than building on top of a browser. Main issue with browser-backed implementations is that PDFs are often of subpar quality. However, the main good thing is you can rely on the latest CSS features.<p>In the end, what was the main decisive factor is the support for the PrintCSS and PagedMedia specifications, which have been completely discarded by major vendors and only implemented by specific engines.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669457"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669457" href="https://news.ycombinator.com/vote?id=39669457&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Congrats! My career has also revolved around PDF generation (once for federal compliance at large companies, second for scrubbing data from PDFs for HIPAA compliance and then generating a new pdf based on the scrubbed data). I think I've seen your tool around, I ended up creating a workflow that generated LateX scripts then converted them to pdfs, and the second a python library. The most difficult aspect for our tools was formatting - the pdfs were generally 60-100 pages and tables could show up anywhere and break the page/formatting. Quite curious to see how your company will grow, good luck!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39669964"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669964" href="https://news.ycombinator.com/vote?id=39669964&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Curious, which python library did you use to convert to PDFs? currently looking into a couple options myself</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671056"><td></td></tr>
                        <tr id="39669517"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669517" href="https://news.ycombinator.com/vote?id=39669517&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It seems TeX/LaTeX is a major inspiration in this, though there can be seen some room for improvement for details like hyphenation, expansion/protusion and microtypography. Not sure if/how a web engine can reach to those points but still it seems this has a potential niche and market outcome, so congrats.<p>Though personally I wish stuff like ConTeXt was more popular and approachable - to my humble knowledge their Lua backend seems to have huge potential, I am doing my invoices with ConTeXt/Lua.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39669561"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39669561" href="https://news.ycombinator.com/vote?id=39669561&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It definitely is! Typesetting quality was the main reason we chose not to go down the Puppeteer/headless browser route but rather use a completely separate engine where typography is a first-class citizen.<p>We like LaTeX, but even for advanced users laying things out can be a difficult thing. Given that documents are a frontend, we wanted to bring the same tools frontend developers already use.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669987"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669987" href="https://news.ycombinator.com/vote?id=39669987&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>This looks really interesting! One of the main reasons we've opted to writing a more complex rending code is for speed. We're getting around 500ms for a single document, which is (last I tested) quicker than any headless chrome setup.<p>How long does it take to render using your API? :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670106"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670106" href="https://news.ycombinator.com/vote?id=39670106&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Rendering time scales with the length / complexity of the document. At the moment, our self-serve API renders slower than a headless chrome setup. We are working on speeding this up as it is currently in the order of seconds.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670471"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670471" href="https://news.ycombinator.com/vote?id=39670471&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>How is this better than writing out an HTML file, then using headless chrome to export to PDF, like this:<pre><code>    "C:\Program Files\Google\Chrome\Application\chrome.exe" --headless --disable-gpu --print-to-pdf=C:\temp\foo.pdf --no-margins --print-to-pdf-no-header C:\temp\test.mhtml</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670651"><td></td></tr>
            <tr id="39670575"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670575" href="https://news.ycombinator.com/vote?id=39670575&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>This brings its own set of challenges. Headers and footers are strictly limited in terms of features, you cannot add footnotes, the notion of page spreads is harder to implement. Then you need to combine that with having a Chrome instance at hand + exposing the needed assets for URL resolution. Definitely not difficult let alone impossible, but not the easiest way to get started :)</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671295"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671295" href="https://news.ycombinator.com/vote?id=39671295&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>The easier way costs $0.05 cents per page. Imagine sending an invoice to your customer and the invoice itself costs 5 cents per page! That's prohibitively expensive for many applications. I wouldn't consider any solution that costs more than 1 cent per page.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671361"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671361" href="https://news.ycombinator.com/vote?id=39671361&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>We bill per document, so the number of pages wouldn't impact the pricing. A 5 pages invoice would come at 1 cent per page. However, it seems that each and every company has different needs and the pricing may or may not make sense for them. There are alternative billing options that we are considering but we want to keep it easy to grasp rather than go into billing kilobytes or ms of execution. We would be more than happy to discuss use cases and see what can work for each company :)</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670235"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670235" href="https://news.ycombinator.com/vote?id=39670235&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>We're using Gotenberg[1] to convert a rendered web page (with Elixir/Phoenix, in our case) to PDF. Works like a charm and we can use our existing frontend code/styling (including SVG graph generators) which is a huge bonus.<p>1: <a href="https://gotenberg.dev/" rel="nofollow">https://gotenberg.dev/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670326"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670326" href="https://news.ycombinator.com/vote?id=39670326&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>We actually experimented with Gotenberg! Ultimately it is a layer on top of Chromium for conversion and we were dissatisfied with the results. I am curious so as to how are you handling assets and other static media / attachments: do you embed everything in a single HTML file or do you use some kind of bucketing system to resolve URLs?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671790"><td></td></tr>
                        <tr id="39669684"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669684" href="https://news.ycombinator.com/vote?id=39669684&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>This is definitely a huge market. Are you targeting React developers only? I've successfully used html2pdf in the past, but looking again at their Github, it seems there has been no update in the last three years.<p>I think SOC2 is a must to start engaging with companies. Most PDFs will have sensitive data, and not many companies will feel comfortable sending customer data to a 3rd party platform, so you need security measures and certifications.</p><p>Good luck!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670222"><td></td></tr>
                <tr id="39670284"><td></td></tr>
                  <tr id="39670911"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670911" href="https://news.ycombinator.com/vote?id=39670911&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>I wonder what YC expects from such investments (considering the multitude of FOSS solutions in this area).</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670992"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670992" href="https://news.ycombinator.com/vote?id=39670992&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>While this may sound a bit counterintuitive (maybe?) we actually pivoted to this field based on YC input and discussions they have had with their previous companies. The multitude of FOSS solutions in this area indicates this is a real problem people are willing to spend time on, and yet there is no go-to solution and every team we have talked to selected different tools based on a very specific requirement.<p>This may not mean success, it means that game is not over in the documents field :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671047"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671047" href="https://news.ycombinator.com/vote?id=39671047&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Thanks for the perspective. Indeed, this is an area with real demand. I haven't evaluated YC's recent startups but I trust they do know a bit about what has a better chance in the market. Best of luck :)<p>ps.: As someone with very minimal PDF needs personally and at work, I'd say the beautiful templates are what caught my attention the most.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39670014"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670014" href="https://news.ycombinator.com/vote?id=39670014&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Glad to see people building in the PDF space, which as a format is unfortunately both awful and ubiquitous. Are you planning to build any support for programmatically filling out existing PDF forms? That's a huge pain point our product is facing that doesn't seem easy to solve.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670980"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670980" href="https://news.ycombinator.com/vote?id=39670980&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>I'm facing that same pain point of programmatic PDF filling. I noodled around in the PDF format and learned it's a bit difficult to deal with fonts and formatting. But I think this client-side library works well enough, as a start: <a href="https://pdf-lib.js.org/#fill-form" rel="nofollow">https://pdf-lib.js.org/#fill-form</a><p>I've also heard of one paid API that I forgot but seemed to work well, and this related service <a href="https://www.jotform.com/" rel="nofollow">https://www.jotform.com/</a>, and I also considered porting some server-side libraries to WASM. One day I'll collect all the libraries and findings in a blog post.</p><p>Are you looking to programmatically fill any PDF form by detecting the fields? Or are you filling one known PDF template?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670221"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670221" href="https://news.ycombinator.com/vote?id=39670221&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Yes, our focus is on programmatic interactions with PDFs, form filling is on our roadmap, alongside programmatic digital signature and many more.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670242"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670242" href="https://news.ycombinator.com/vote?id=39670242&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Amazing, is there anywhere I can follow along to find out when form filling will be available?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670352"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39670352" href="https://news.ycombinator.com/vote?id=39670352&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Sure! Feel free to join our Discord, we post announcements as soon as new features are released. You can also ask for features, we prioritise these requests with enterprise customer's in our development roadmap.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670924"><td></td></tr>
            <tr id="39670708"><td></td></tr>
                <tr id="39670798"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670798" href="https://news.ycombinator.com/vote?id=39670798&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>It is similar to pspdfkit. We add an abstraction layer over the HTML and assets hosting to make it easier to use without having to think too hard about security and serving assets.<p>We also hope to keep the focus on the PDF generation part rather than expanding super-horizontal style to provide all imaginable PDF tools at the expense that none is really good.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669730"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669730" href="https://news.ycombinator.com/vote?id=39669730&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>So are you using PrinceXML for your "completely separate engine where typography is a first-class citizen"?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670196"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670196" href="https://news.ycombinator.com/vote?id=39670196&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Yes, we use an API layer on top of PrinceXML with additional polyfills to support modern features. This is a meh solution but it allowed us to iterate quickly and get to work with customers without building a full blown PDF engine firsthand. However building this engine ourselves is the key to reduced latency and overall better feature support. But we need to engage with our users first and see exactly where we should head first :)</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39669889"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39669889" href="https://news.ycombinator.com/vote?id=39669889&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Can we not have an alternative to PDFs? I get that they're more standardized but why would everyone let adobe have the hammer for a file type that's so important</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39670094"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670094" href="https://news.ycombinator.com/vote?id=39670094&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>We quite agree on this - but getting a new alternative out will require a significant critical mass before it can be of any interest. While PDF has its challenges, it remains a light portable format and its security features make it a good fit for binding documents. The ecosystem, although it is dominated by Adobe, also includes other major players and existing integrations.<p>The way we look at it is PDFs allows embedding of other files and metadata. It is easy to provide a platform where we can enrich PDFs to display different contents than the one in the PDF itself. If this gets interesting enough, we can then phase out the PDF in the first place. But this is a long way ahead.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670615"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670615" href="https://news.ycombinator.com/vote?id=39670615&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>PDF is an incredibly (stupidly) extensible format. There are tons of government forms that (sadly) bake in complex workflows into PDF forms.<p>Given that the whole world has been running on PDFs for decades it's makes more sense to leverage the existing infrastructure and move it towards something more functional over time. Introducing a new format will just lead to another format the achieves 0.5% marketshare and then is abandoned after a few years. Microsoft basically forcing people to use XPS in windows (&gt;70% market share of computing) still wasn't able to achieve meaningful usage or change.</p><p>I expect that PDFs will not go away for 20 years at least, but who knows
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39670421"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670421" href="https://news.ycombinator.com/vote?id=39670421&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>PDF is an open format in the sense that you don't need to pay Adobe a license fee for generating PDFs, or for reading and rendering PDFs. The format is fully documented, although the specification is controlled by Adobe.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39670151"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670151" href="https://news.ycombinator.com/vote?id=39670151&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>For supply chain workflows the ASC X12 Electronic Data Interchange (EDI) industry standard works much better than PDFs. Unfortunately, despite being around for decades in has only been adopted by forward thinking organizations such as Walmart. Most smaller companies and their vendors still haven't implemented EDI.<p><a href="https://developer.walmart.com/home/us-edi/" rel="nofollow">https://developer.walmart.com/home/us-edi/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670591"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670591" href="https://news.ycombinator.com/vote?id=39670591&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Insanity.<p>EDI is the only place where people are regularly still paying for message by the kilobyte, where unsecured FTP over the open internet is still a norm, and where entire cottage industries exist to support AVOIDING using EDI.</p><p>Source: I work in EDI. it's a pain in the rump.</p><p>Also, EDI is really only good for things like PO's, shipping notices, invoices, sales orders, etc.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671515"><td></td></tr>
            <tr id="39671078"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671078" href="https://news.ycombinator.com/vote?id=39671078&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>&gt; Also, EDI is really only good for things like PO's, shipping notices, invoices, sales orders, etc.<p>Don't forget health insurance claims, eligibility &amp; benefits, and prior auth requests!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39671493"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39671493" href="https://news.ycombinator.com/vote?id=39671493&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>EDI is used in a lot of situations for machine-to-machine communications, but outside USA I believe EDIFACT is much more used (X12 is mostly used in USA).<p>Today many EDIFACT documents have been converted to ebXML: <a href="https://en.wikipedia.org/wiki/EbXML" rel="nofollow">https://en.wikipedia.org/wiki/EbXML</a></p><p>Source: Worked in EDI for a few years
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670131"><td></td></tr>
                <tr id="39670652"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39670652" href="https://news.ycombinator.com/vote?id=39670652&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Giving credit where it's due, I can appreciate Microsoft for introducing XPS as an alternative to pdf.<p>There was a time, when not every software had "export to pdf". So, having a "print to pdf" meant installing (often pirated) Adobe Acrobat or installing a sketchy free(ware) printdriver software downloaded from sourceforge.</p><p>MS adding xps print driver to windows enabled sharing docs consistently (within windows ecosystem) without resorting to hacks.</p><p>I don't know why it didn't catch up. May be it was the general mistrust of anything MS,  it arrived too late or it was something else.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670931"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39670931" href="https://news.ycombinator.com/vote?id=39670931&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Indeed, we need to give credit to MS for what they did. However, it didn't catch up as you mentioned, maybe due to timing, skepticism toward MS, or the complexity of moving from Adobe to MS for PDF management. I will dig a bit into it and come back later if I find anything interesting.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="39670584"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670584" href="https://news.ycombinator.com/vote?id=39670584&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>I just wanted to add that if you want to convert plaintext files to pdf, vim has a builtin feature to do so:<pre><code>  vim filename.txt -c "hardcopy &gt; filename.ps | q" &amp;&amp; ps2pdf filename.ps #convert ps to pdf</code></pre></span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39669934"><td></td></tr>
                <tr id="39670148"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670148" href="https://news.ycombinator.com/vote?id=39670148&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>No, we don't currently do that. However, we are considering adding metadata to PDFs, and using pdfmark could be very helpful!</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670037"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39670037" href="https://news.ycombinator.com/vote?id=39670037&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>The problem with using Tailwind is that I can't just say &lt;h1&gt;Some Heading&lt;/h1&gt;. As noted in the Tailwind documents "All heading elements are completely unstyled by default, and have the same font-size and font-weight as normal text."[1]<p>Most of the time when I'm writing HTML I want a set of default styles for the most common elements,
It's tedious and error-prone to have to specify a class <i>every single time</i>.</p><p>1 <a href="https://tailwindcss.com/docs/preflight" rel="nofollow">https://tailwindcss.com/docs/preflight</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39670147"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39670147" href="https://news.ycombinator.com/vote?id=39670147&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Makes total sense. There is no real requirement to use Tailwind to create the PDFs, we just have grown accustomed to Tailwind :) If you don't use the &lt;Tailwind&gt; tag, the browser defaults are used to generate the PDF.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39670174"><td></td></tr>
            <tr id="39671147"><td></td></tr>
                <tr id="39671451"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39671451" href="https://news.ycombinator.com/vote?id=39671451&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><p><span>Editors such as Overleaf, and those offered by MS and Adobe, have been around for a long time. Recently, companies like Pandadoc and Docusign have started offering services around PDFs (generation or other aspects of their lifecycle).<p>It might seem odd, given our long history with PDFs, but I believe there's still much to be done with these documents. They're everywhere‚Äîinvoices, tickets, reports, etc.‚Äîyet the technology for generating and managing them hasn't evolved much in years. Our approach is to apply the same modern technologies used for web design to document design.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39671313"><td></td></tr>
                <tr id="39671490"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39671490" href="https://news.ycombinator.com/vote?id=39671490&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>When I was first hired 15 years ago my first task was to create a PDF report. It was easy back then in PHP+fPDF. Two years ago I was hired to work on a Heroku-hosted NodeJS app. I was surprised to find that generating a PDF turned out to be substantially more difficult task, requiring running a browser emulator or connecting to an external service. And now, seeing PDF generation as a premium pay-as-you-go product is just too much.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39671557"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39671557" href="https://news.ycombinator.com/vote?id=39671557&amp;how=up&amp;goto=item%3Fid%3D39668962"></a></center>    </td><td><br><div>
                  <p><span>Makes sense. Actually, if you keep the layout/content very simple, aren't constrained by throughput, and don't need to integrate dynamic data or other similar processes, then simple FOSS could indeed get the job done! That's exactly why we developed the open-source library react-print-pdf</span></p></div></td></tr>
        </tbody></table></td></tr>
                              </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Extropic is building (155 pts)]]></title>
            <link>https://www.extropic.ai/future</link>
            <guid>39668430</guid>
            <pubDate>Mon, 11 Mar 2024 14:09:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.extropic.ai/future">https://www.extropic.ai/future</a>, See on <a href="https://news.ycombinator.com/item?id=39668430">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>Mar 11th, 2024								</p><p>/</p><p>Litepaper</p></div><div><p>Message from the team</p><div><p><em>We are very excited to finally share more about what Extropic is building: a full-stack hardware platform to harness matter's natural fluctuations as a computational resource for Generative AI.</em></p><p>What does this novel paradigm of computing practically mean for the world?</p><ul role="list"><li><em>Extends hardware scaling well beyond the constraints of digital computing</em></li><li><em>Enables AI accelerators that are many orders of magnitude faster and more energy efficient than digital processors (CPUs/GPUs/TPUs/FPGAs)</em></li><li><em>Unlocks powerful probabilistic AI algorithms that are not feasible on digital processors</em></li></ul><p><em>Our brief Litepaper (below) provides an early glimpse at our technologies. We hope the following excites you for the journey ahead. Join us as we accelerate towards the thermodynamically intelligent future.</em></p><p><strong>- Gill and Trev</strong></p></div></div><p><span>T</span>he demand for computing power in the AI era is increasing at an unprecedented exponential rate. Luckily, for the past several decades, the miniaturization of CMOS transistor technology following Moore‚Äôs law <a href="#references">[1]</a> has allowed much of this exponential growth to be accounted for by increasing computer efficiency.		</p><div><p>Unfortunately, Moore‚Äôs law is starting to slow down <a href="#references">[2]</a>. The reason for this is rooted in fundamental physics: transistors are approaching the atomic scale where effects like thermal noise start to forbid rigid digital operation <a href="#references">[3, 4, 5]</a>.</p><p>As a result, the energy requirements of modern AI are beginning to take off. Major players are proposing measures as extreme as building nuclear reactor-powered data centers dedicated to large model training and inference. Continuing this scaling for a few more decades will require infrastructure engineering efforts of unprecedented scale and represents an arduous path forward for scaling humanity‚Äôs aggregate intelligence.</p><p>On the other hand, biology is neither rigid nor digital and hosts computing circuitry that is much more efficient than anything humanity has built to date. Inter-cellular chemical reaction networks drive computation in biological systems. Cells are small, and as a result, the number of reactants in these networks is countable <a href="#references">[6, 7]</a>. Therefore, reactions between reactants are genuinely discrete and intrinsically random. The relative effect of this intrinsic randomness scales inversely with the number of reactant molecules, and as such, fluctuations tend to dominate the dynamics of these systems.</p><p>From this, we can say with certainty that there is no fundamental reason for the constraints of digital logic to bind the efficiency of computing devices. The engineering challenge is clear: how can we design a complete AI hardware and software system from the ground up that thrives in an intrinsically noisy environment?</p><p>Energy-Based Models (EBMs) offer hints at a potential solution, as they are a concept that appears both in thermodynamic physics and in fundamental probabilistic machine learning. In physics, they are known as parameterized thermal states, arising from steady-states of systems with tunable parameters. In machine learning, they are known as exponential families.</p><p>Exponential families are known to be the optimal way to parameterize probability distributions, requiring the minimal amount of data to uniquely determine their parameters <a href="#references">[8]</a>. They are thus excellent in the low-data regime, which encompasses scenarios where one needs to model tail events in mission-critical applications, as depicted in figure 1. The way they achieve this is by filling the blanks in data with noise; they seek to maximize their entropy while matching the statistics of the target distribution. This process of hallucinating every possibility that is not included in a dataset and penalizing such occurrences energetically requires the usage of a lot of randomness, both at training and inference time.</p></div><div id="figure-1"><p><span>Figure 1: The principles of Extropic probabilistic AI accelerators</span> A simple example of low-complexity distributional learning failing to capture the effect of a tail event. Low air pressure almost always means rain and high crop yields. However, every so often, very low air pressure corresponds to a hurricane.</p></div><div><p>This requirement of sampling has been the main limiter in production uses of EBMs. The fundamental reason for this is that sampling from generic energy landscapes is very difficult on digital hardware, which must expend substantial electrical energy to generate and shape the entropy required for the diffusion processes. From a hardware perspective, digital sampling seems quite contrived: why put so much effort into building increasingly complex pristine digital computers when the most common and compute-intensive algorithms turn around and pump them full of noise?</p><p>Extropic is shortcutting this inefficiency and unlocking the full potential of generative AI by implementing EBMs directly as parameterized stochastic analog circuits. Extropic accelerators will achieve many orders of magnitude of improvement over digital computers in terms of both runtime and energy efficiency for algorithms based on sampling from complex landscapes.</p><p>The operational principle of Extropic accelerators is analogous to Brownian motion. In Brownian motion, macroscopic but lightweight particles suspended in a fluid experience random forces due to many collisions with microscopic liquid molecules. These collisions lead to the random diffusion of the particles around the vessel. One could imagine anchoring the Brownian particles to the vessel walls and each other with springs, as depicted in Fig. 2 (a). In this case, the springs will resist the random forces, and the particles will prefer to reside in particular parts of the vessel more than others. If one repeatedly sampled the positions of the particles, waiting sufficiently long in between samples (as illustrated in Fig. 2 (b)), one would find that they follow a predictable <em>steady-state</em> probability distribution. If we changed the stiffness of the springs, this distribution would change. This simple mechanical system is a source of programmable randomness.</p></div><div id="figure-2"><p><span>Figure 2: The operational principle of Extropic accelerators &nbsp;<strong>(a)</strong></span> A simple mechanical analogy to Extropic accelerators. Since there are three masses in two dimensions, the steady state of this device would be a probability distribution over a 6-dimensional space. <span>(b)</span> Samples may be drawn from an Extropic accelerator by repeatedly observing the system, waiting at least the equilibration time teq between observations. teq is how long it takes for the noise in the system to destroy all correlations with the previous sample.</p></div><div><p>There is a direct connection between this mechanical picture and the parameterized stochastic analog circuits that make up Extropic accelerators. The lightweight particles represent electrons, and the liquid molecules represent the atoms of the conducting medium, which can transfer energy to the electrons upon collision. The springs represent circuit components that confine the motion of the electrons, such as inductors or transistors. Control voltages/currents can be applied to tune the values of these components, changing the distribution from which the circuit samples.</p><p>Although every circuit is noisy, not every circuit is useful as an Extropic accelerator. Making a noise-dominated yet well-behaved device is challenging from an engineering perspective. Thermal fluctuations are small, so devices must be physically small and low power to be strongly affected by them. For this reason, if one wanted to make an Extropic accelerator out of macroscopic components (on a PCB, for example), one would have to introduce synthetic noise. Doing so erodes the fundamental time and energy savings and ends up performing similarly to running the algorithm digitally.					</p><p>Extropic‚Äôs first processors are nano fabricated from aluminum and run at low temperatures where they are superconducting. Fig. 3 shows an early device that tested several possible superconducting neuron designs. Some of these neurons are similar to existing super conducting flux qubits <a href="#references">[9]</a>. These neurons exploit the Josephson effect as a source of nonlinearity, which occurs when two superconductors are near one another. This nonlinearity is required for the device to access non-Gaussian probability distributions, which are necessary to model real-world applications with fat tails. Additionally, digital Gaussian sampling routines are ubiquitous and highly optimized. So, if an analog device is to provide a considerable speedup compared to a traditional processor, non-Gaussianity is required.</p></div><div id="figure-3"><p><span>Figure 3:&nbsp;Microscope image of an Extropic chip.</span> The inset shows two Josephson junctions, which are the devices that provide the processor with its critical nonlinearity.												</p></div><div><p>These neurons provide the basic building blocks that are combined to form a larger superconducting system. In such a larger system, many linear and non-linear neurons are combined together to create a circuit that samples from a rich and high-dimensional distribution. The neuron biases and interaction strengths are all tunable parameters of the distribution, allowing a single device to embody a wide family of probability distributions.</p><p>Extropic‚Äôs superconducting chips are entirely passive, meaning we only expend energy when measuring or manipulating its state. This likely makes these neurons the most energy-efficient in the universe. These systems will be highly energy efficient at scale: Extropic targets low-volume, high-value customers like governments, banks, and private clouds with these systems.</p><p>Extropic is also building semiconductor devices that operate at room temperature to extend our reach to a larger market. These devices trade the Josephson junction for the transistor. Doing so sacrifices some energy efficiency compared to superconducting devices. In exchange, it allows one to build them using standard manufacturing processes and supply chains, unlocking massive scale. Since they operate at room temperature, it will be possible to pack them into a GPU-like expansion card form factor. This will allow us to put an Extropic accelerator in every home, enabling everyone to partake in the thermodynamic AI acceleration.</p><p>To support a wide range of hardware substrates, Extropic is also building a software layer that compiles from abstract specifications of EBMs to the relevant hardware control language. This compilation layer is built upon the theoretical framework of factor graphs <a href="#references">[8]</a>. Factor graphs specify how large distributions factorize into local chunks. This allows Extropic accelerators to breakdown and run programs that are too big to fit on any given analog core.				</p><p>Many previous AI accelerator companies have struggled to find an advantage because of the memory-boundedness of deep learning; today‚Äôs algorithms spend around 25% of their time moving numbers around in memory. As a result of this, via Amdahl‚Äôs law, any chip that accelerates a particular operation (such as matrix multiplication) will struggle to achieve more than a 4x speedup. As Extropic chips natively accelerate a broad class of probabilistic algorithms by running them physically as a rapid and energy-efficient process in their entirety, we are bound to unlock a whole new regime of artificial intelligence acceleration well beyond what was previously thought achievable.</p></div><a href="https://www.extropic.ai/careers" target="_blank"></a><div id="references"><h3>References</h3><a href="https://ieeexplore.ieee.org/document/4785860" target="_blank"></a><a href="https://ieeexplore.ieee.org/document/7878935" target="_blank"></a><a href="https://link.springer.com/article/10.1007/BF01250732" target="_blank"></a><a href="https://ieeexplore.ieee.org/document/752515" target="_blank"></a><a href="https://ieeexplore.ieee.org/document/1182065" target="_blank"></a><a href="https://www.annualreviews.org/doi/10.1146/annurev.physchem.58.032806.104637" target="_blank"></a><a href="https://www.pnas.org/doi/10.1073/pnas.94.3.814" target="_blank"></a><a href="https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/" target="_blank"></a><a href="https://escholarship.org/uc/item/9844c3h3" target="_blank"></a></div></div></div>]]></description>
        </item>
    </channel>
</rss>