<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 04 Oct 2023 15:00:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[An Interactive Intro to CRDTs (114 pts)]]></title>
            <link>https://jakelazaroff.com/words/an-interactive-intro-to-crdts/</link>
            <guid>37764581</guid>
            <pubDate>Wed, 04 Oct 2023 13:12:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jakelazaroff.com/words/an-interactive-intro-to-crdts/">https://jakelazaroff.com/words/an-interactive-intro-to-crdts/</a>, See on <a href="https://news.ycombinator.com/item?id=37764581">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-content="" data-astro-cid-rnrqlda2="">
<p>Have you heard about CRDTs and wondered what they are? Maybe you’ve looked into them a bit, but ran into a wall of academic papers and math jargon? That was me before I started my <a href="https://www.recurse.com/" data-astro-cid-bi7aps5f="">Recurse Center</a><a data-tooltip="" href="https://www.recurse.com/" data-astro-cid-bi7aps5f=""><img src="https://d29xw0ra2h4o4u.cloudfront.net/assets/logo_square-60e12570c3a1b0b0798e651a0755f71a40ff15421761b786f720e4c02fc89a1f.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">The Recurse Center</span><span data-astro-cid-bi7aps5f="">The Recurse Center is a self-directed, community-driven educational retreat for programmers in New York City.</span><span data-astro-cid-bi7aps5f=""><img src="https://d29xw0ra2h4o4u.cloudfront.net/assets/favicon-fbfd4d3b58909892b7caada24c507292a4f96f5ab03fda9a7ae3050ce4618f78.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">www.recurse.com/</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"><use href="/icons.svg#share"></use></svg></span></a> batch. But I’ve spent the past month or so doing research and writing code, and it turns out that you can build a lot with just a few simple things!</p>
<p>In this two-part series, we’ll learn what a CRDT is. Then we’ll write a primitive CRDT, compose it into more complex data structures, and finally use what we’ve learned to build a collaborative pixel art editor. All of this assumes no prior knowledge about CRDTs, and only a rudimentary knowledge of TypeScript.</p>
<p>To pique your curiosity, this is what we’ll end up with:</p>
<pixelart-demo resolution="100"></pixelart-demo>

<p>Draw by clicking and dragging with your mouse. Change the paint color by using the color input on the bottom left. You can draw on either canvas and your changes will show up on the other, as if they were collaborating on the same picture.</p>
<p>Clicking the network button prevents changes from reaching the other canvas (although they’ll sync up again when they come back “online”). The latency slider adds a delay before changes on one canvas show up on the other.</p>
<p>We’ll build that in the next post. First, we need to learn about CRDTs!</p>
<h3 id="what-is-a-crdt">What is a CRDT?</h3>
<p>Okay, let’s start from the top. CRDT stands for “Conflict-free Replicated Data Type”. That’s a long acronym, but the concept isn’t too complicated. It’s a kind of data structure that can be stored on different computers (peers). Each peer can update its own state instantly, without a network request to check with other peers. Peers may have different states at different points in time, but are guaranteed to eventually converge on a single agreed-upon state. That makes CRDTs great for building rich collaborative apps, like Google Docs and Figma — without requiring a central server to sync changes.</p>
<p>Broadly, there are two kinds of CRDTs: state-based and operation-based.<sup><a href="#user-content-fn-cvrdt" id="user-content-fnref-cvrdt" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">1</a></sup> State-based CRDTs transmit their full state between peers, and a new state is obtained by merging all the states together. Operation-based CRDTs transmit only the actions that users take, which can be used to calculate a new state.</p>
<p>That might make operation-based CRDTs sound way better. For example, if a user updates one item in a list, an operation-based CRDT can send a description of only that update, while a state-based CRDT has to send the whole list! The drawback is that operation-based CRDTs impose constraints on the communication channel: messages must be delivered exactly once, in causal order, to each peer.<sup><a href="#user-content-fn-delta" id="user-content-fnref-delta" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">2</a></sup></p>
<p>This post will exclusively focus on on state-based CRDTs. For brevity, I’ll just say “CRDTs” from here on out, but know that I’m referring specifically to state-based CRDTs.</p>
<p>I’ve been talking about what CRDTs do, but what <strong>is</strong> a CRDT? Let’s make it concrete: a CRDT is any data structure that implements this interface:<sup><a href="#user-content-fn-ackshually" id="user-content-fnref-ackshually" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">3</a></sup></p>
<pre><code><span>interface</span> <span><span>CRDT</span><span>&lt;</span><span>T</span><span>,</span> <span>S</span><span>&gt;</span></span> <span>{</span>
  value<span>:</span> <span>T</span><span>;</span>
  state<span>:</span> <span>S</span><span>;</span>
  <span>merge</span><span>(</span>state<span>:</span> <span>S</span><span>)</span><span>:</span> <span>void</span><span>;</span>
<span>}</span></code></pre>
<p>That is to say, a CRDT contains at least three things:</p>
<ul>
<li>A value, <code>T</code>. This is the part the rest of our program cares about. The entire point of the CRDT is to reliably sync the value between peers.</li>
<li>A state, <code>S</code>. This is the metadata needed for peers to agree on the same value. To update other peers, the whole state is serialized and sent to them.</li>
<li>A merge function. This is a function that takes some state (probably received from another peer) and merges it with the local state.</li>
</ul>
<p>The merge function must satisfy three properties to ensure that all peers arrive at the same result (I’ll use the notation <code>A ∨ B</code> to indicate merging state <code>A</code> into state <code>B</code>):</p>
<ul>
<li>Commutativity: states can be merged in any order; <code>A ∨ B = B ∨ A</code>. If Alice and Bob exchange states, they can each merge the other’s state into their own and arrive at the same result.</li>
<li>Associativity: when merging three (or more) states, it doesn’t matter which are merged first; <code>(A ∨ B) ∨ C = A ∨ (B ∨ C)</code>. If Alice receives states from both Bob and Carol, she can merge them into her own state in any order and the result will be the same.<sup><a href="#user-content-fn-thesamepicture" id="user-content-fnref-thesamepicture" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">4</a></sup></li>
<li>Idempotence: merging a state with itself doesn’t change the state; <code>A ∨ A = A</code>. If Alice merges her own state with itself, the result will be the same state she started with.</li>
</ul>
<p>Mathematically proving that a merge function has all these properties might sound hard. But luckily, we don’t have to do that! Instead, we can just combine CRDTs that already exist, leaning on the fact that someone has proven these things for us.</p>
<p>Speaking of CRDTs that already exist: let’s learn about one!</p>
<h3 id="last-write-wins-register">Last Write Wins Register</h3>
<p>A register is a CRDT that holds a single value. There are a couple kinds of registers, but the simplest is the Last Write Wins Register (or LWW Register).</p>
<p>LWW Registers, as the name suggests, simply overwrite their current value with the last value written. They determine which write occurred last using timestamps, represented here by integers that increment whenever the value is updated.<sup><a href="#user-content-fn-logical" id="user-content-fnref-logical" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">5</a></sup> Here’s the algorithm:</p>
<ul>
<li>If the received timestamp is less than the local timestamp, the register doesn’t change its state.</li>
<li>If the received timestamp is greater than the local timestamp, the register overwrites its local value with the received value. It also stores the received timestamp and some sort of identifier unique to the peer that last wrote the value (the peer ID).</li>
<li>Ties are broken by comparing the local peer ID to the peer ID in the received state.</li>
</ul>
<p>Try it out with the playground below.</p>
<lwwregister-demo></lwwregister-demo>

<p>Did you get a sense for how LWW Registers work? Here are a couple specific scenarios to try:</p>
<ul>
<li>Turn the network off, make a bunch of updates to <code>bob</code>, and then turn it back on. When you send updates from <code>alice</code>, they’ll be rejected until the timestamp exceeds <code>bob</code>’s timestamp.</li>
<li>Perform the same setup, but once you turn the network back on, send an update from <code>bob</code> to <code>alice</code>. Notice how the timestamps are now synced up and <code>alice</code> can write to <code>bob</code> again!</li>
<li>Increase the latency and send an update from both peers simultaneously. <code>alice</code> will accept <code>bob</code>’s update, but <code>bob</code> will reject <code>alice</code>‘s. Since <code>bob</code>’s peer ID is greater, it breaks the timestamp tie.</li>
</ul>
<p>Here’s the code for the LWW Register:</p>
<pre><code><span>class</span> <span>LWWRegister<span>&lt;</span><span>T</span><span>&gt;</span></span> <span>{</span>
  <span>readonly</span> id<span>:</span> <span>string</span><span>;</span>
  state<span>:</span> <span>[</span>peer<span>:</span> <span>string</span><span>,</span> timestamp<span>:</span> <span>number</span><span>,</span> value<span>:</span> <span>T</span><span>]</span><span>;</span>

  <span>get</span> <span>value</span><span>(</span><span>)</span> <span>{</span>
    <span>return</span> <span>this</span><span>.</span>state<span>[</span><span>2</span><span>]</span><span>;</span>
  <span>}</span>

  <span>constructor</span><span>(</span>id<span>:</span> <span>string</span><span>,</span> state<span>:</span> <span>[</span><span>string</span><span>,</span> <span>number</span><span>,</span> <span>T</span><span>]</span><span>)</span> <span>{</span>
    <span>this</span><span>.</span>id <span>=</span> id<span>;</span>
    <span>this</span><span>.</span>state <span>=</span> state<span>;</span>
  <span>}</span>

  <span>set</span><span>(</span>value<span>:</span> <span>T</span><span>)</span> <span>{</span>
    <span>// set the peer ID to the local ID, increment the local timestamp by 1 and set the value</span>
    <span>this</span><span>.</span>state <span>=</span> <span>[</span><span>this</span><span>.</span>id<span>,</span> <span>this</span><span>.</span>state<span>[</span><span>1</span><span>]</span> <span>+</span> <span>1</span><span>,</span> value<span>]</span><span>;</span>
  <span>}</span>

  <span>merge</span><span>(</span>state<span>:</span> <span>[</span>peer<span>:</span> <span>string</span><span>,</span> timestamp<span>:</span> <span>number</span><span>,</span> value<span>:</span> <span>T</span><span>]</span><span>)</span> <span>{</span>
    <span>const</span> <span>[</span>remotePeer<span>,</span> remoteTimestamp<span>]</span> <span>=</span> state<span>;</span>
    <span>const</span> <span>[</span>localPeer<span>,</span> localTimestamp<span>]</span> <span>=</span> <span>this</span><span>.</span>state<span>;</span>

    <span>// if the local timestamp is greater than the remote timestamp, discard the incoming value</span>
    <span>if</span> <span>(</span>localTimestamp <span>&gt;</span> remoteTimestamp<span>)</span> <span>return</span><span>;</span>

    <span>// if the timestamps are the same but the local peer ID is greater than the remote peer ID, discard the incoming value</span>
    <span>if</span> <span>(</span>localTimestamp <span>===</span> remoteTimestamp <span>&amp;&amp;</span> localPeer <span>&gt;</span> remotePeer<span>)</span> <span>return</span><span>;</span>

    <span>// otherwise, overwrite the local state with the remote state</span>
    <span>this</span><span>.</span>state <span>=</span> state<span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Let’s see how this stacks up to the CRDT interface:</p>
<ul>
<li><code>state</code> is a tuple of the peer ID that last wrote to the register, the timestamp of the last write and the value stored in the register.</li>
<li><code>value</code> is simply the last element of the <code>state</code> tuple.</li>
<li><code>merge</code> is a method that implements the algorithm described above.</li>
</ul>
<p>LWW Registers have one more method named <code>set</code>, which is called locally to set the register’s value. It also updates the local metadata, recording the local peer ID as the last writer and incrementing the local timestamp by one.</p>
<p>That’s it! Although it appears simple, the humble LWW Register is a powerful building block with which we can create actual applications.</p>
<h3 id="last-write-wins-map">Last Write Wins Map</h3>
<p>Most programs involve more than one value,<sup><a href="#user-content-fn-citation" id="user-content-fnref-citation" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">6</a></sup> which means we’ll need a more complex CRDT than the LWW Register. The one we’ll learn about today is called the Last Write Wins Map (or LWW Map).</p>
<p>Let’s start by defining a couple types. First, our value type:</p>
<pre><code><span>type</span> <span>Value<span>&lt;</span><span>T</span><span>&gt;</span></span> <span>=</span> <span>{</span>
  <span>[</span>key<span>:</span> <span>string</span><span>]</span><span>:</span> <span>T</span><span>;</span>
<span>}</span><span>;</span></code></pre>
<p>If each individual map value holds type <code>T</code>, then the value of the entire LWW Map is a mapping of string keys to <code>T</code> values.</p>
<p>Here’s our state type:</p>
<pre><code><span>type</span> <span>State<span>&lt;</span><span>T</span><span>&gt;</span></span> <span>=</span> <span>{</span>
  <span>[</span>key<span>:</span> <span>string</span><span>]</span><span>:</span> LWWRegister<span>&lt;</span><span>T</span> <span>|</span> <span>null</span><span>&gt;</span><span>[</span><span>"state"</span><span>]</span><span>;</span>
<span>}</span><span>;</span></code></pre>
<p>Do you see the trick? From our application’s perspective, the LWW Map just holds normal values — but <strong>it actually holds LWW Registers</strong>. When we look at the full state, each key’s state is the state of the LWW Register at that key.<sup><a href="#user-content-fn-null" id="user-content-fnref-null" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">7</a></sup></p>
<p>I want to pause on that for a moment, because it’s important. Composition lets us combine primitive CRDTs into more complex ones. When it’s time to merge, all the parent does is pass slices of incoming state to the appropriate child’s merge function. We can nest this process as many times as we want; each complex CRDT passing ever-smaller slices of state down to the next level, until we finally hit a primitive CRDT that performs the actual merge.</p>
<p>From this perspective, the LWW Map merge function is simple: iterate through each key and hand off the incoming state at that key to the corresponding LWW Register to merge. Try it out in the playground below:</p>
<lwwmap-demo add="off" delete="off" state="{ &quot;foo&quot;: [&quot;alice&quot;, 1, &quot;lorem ipsum&quot;], &quot;bar&quot;: [&quot;bob&quot;, 1, &quot;dolor sit amet&quot;] }"></lwwmap-demo>

<p>It’s kind of difficult to trace what’s happening here, so let’s split up the state for each key. Note, though, that this is just a visualization aid; the full state is still being transmitted as a single unit.</p>
<p>Try increasing the latency and then updating different keys on each peer. You’ll see that each peer accepts the updated value with a higher timestamp, while rejecting the value with a lower timestamp.</p>
<lwwmap-demo split="true" add="off" delete="off" state="{ &quot;foo&quot;: [&quot;alice&quot;, 1, &quot;lorem ipsum&quot;], &quot;bar&quot;: [&quot;bob&quot;, 1, &quot;dolor sit amet&quot;] }"></lwwmap-demo>

<p>The full LWW Map class is kinda beefy, so let’s go through each property one by one. Here’s the start of it:</p>
<pre><code><span>class</span> <span>LWWMap<span>&lt;</span><span>T</span><span>&gt;</span></span> <span>{</span>
  <span>readonly</span> id <span>=</span> <span>""</span><span>;</span>
  #data <span>=</span> <span>new</span> <span>Map<span>&lt;</span><span>string</span><span>,</span> LWWRegister<span>&lt;</span><span>T</span> <span>|</span> <span>null</span><span>&gt;&gt;</span></span><span>(</span><span>)</span><span>;</span>

  <span>constructor</span><span>(</span>id<span>:</span> <span>string</span><span>,</span> state<span>:</span> State<span>&lt;</span><span>T</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>this</span><span>.</span>id <span>=</span> id<span>;</span>

    <span>// create a new register for each key in the initial state</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span>key<span>,</span> register<span>]</span> <span>of</span> Object<span>.</span><span>entries</span><span>(</span>state<span>)</span><span>)</span> <span>{</span>
      <span>this</span><span>.</span>#data<span>.</span><span>set</span><span>(</span>key<span>,</span> <span>new</span> <span>LWWRegister</span><span>(</span><span>this</span><span>.</span>id<span>,</span> register<span>)</span><span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>
<span>}</span></code></pre>
<p><code>#data</code> is a private property holding a map of the keys to LWW Register instances. To instantiate a LWW Map with preexisting state, we need to iterate through the state and instantiate each LWW Register.</p>
<p>Remember, CRDTs need three properties: <code>value</code>, <code>state</code> and <code>merge</code>. We’ll look at <code>value</code> first:</p>
<pre><code>  <span>get</span> <span>value</span><span>(</span><span>)</span> <span>{</span>
    <span>const</span> value<span>:</span> Value<span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>{</span><span>}</span><span>;</span>

    <span>// build up an object where each value is set to the value of the register at the corresponding key</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span>key<span>,</span> register<span>]</span> <span>of</span> <span>this</span><span>.</span>#data<span>.</span><span>entries</span><span>(</span><span>)</span><span>)</span> <span>{</span>
      <span>if</span> <span>(</span>register<span>.</span>value <span>!==</span> <span>null</span><span>)</span> value<span>[</span>key<span>]</span> <span>=</span> register<span>.</span>value<span>;</span>
    <span>}</span>

    <span>return</span> value<span>;</span>
  <span>}</span></code></pre>
<p>It’s a getter that iterates through the keys and gets each register’s <code>value</code>. As far as the rest of the app is concerned, it’s just normal map!</p>
<p>Now let’s look at <code>state</code>:</p>
<pre><code>  <span>get</span> <span>state</span><span>(</span><span>)</span> <span>{</span>
    <span>const</span> state<span>:</span> State<span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>{</span><span>}</span><span>;</span>

    <span>// build up an object where each value is set to the full state of the register at the corresponding key</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span>key<span>,</span> register<span>]</span> <span>of</span> <span>this</span><span>.</span>#data<span>.</span><span>entries</span><span>(</span><span>)</span><span>)</span> <span>{</span>
      <span>if</span> <span>(</span>register<span>)</span> state<span>[</span>key<span>]</span> <span>=</span> register<span>.</span>state<span>;</span>
    <span>}</span>

    <span>return</span> state<span>;</span>
  <span>}</span></code></pre>
<p>Similar to <code>value</code>, it’s a getter that builds up a map from each register’s <code>state</code>.</p>
<p>There’s a clear trend here: iterating through the keys in <code>#data</code> and handing things off to the register stored at that key. You’d think <code>merge</code> would work the same way, but it’s a little more involved:</p>
<pre><code>  <span>merge</span><span>(</span>state<span>:</span> State<span>&lt;</span><span>T</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>// recursively merge each key's register with the incoming state for that key</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span>key<span>,</span> remote<span>]</span> <span>of</span> Object<span>.</span><span>entries</span><span>(</span>state<span>)</span><span>)</span> <span>{</span>
      <span>const</span> local <span>=</span> <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>;</span>

      <span>// if the register already exists, merge it with the incoming state</span>
      <span>if</span> <span>(</span>local<span>)</span> local<span>.</span><span>merge</span><span>(</span>remote<span>)</span><span>;</span>
      <span>// otherwise, instantiate a new `LWWRegister` with the incoming state</span>
      <span>else</span> <span>this</span><span>.</span>#data<span>.</span><span>set</span><span>(</span>key<span>,</span> <span>new</span> <span>LWWRegister</span><span>(</span><span>this</span><span>.</span>id<span>,</span> remote<span>)</span><span>)</span><span>;</span>
    <span>}</span>
  <span>}</span></code></pre>
<p>First, we iterate through the incoming <code>state</code> parameter rather than the local <code>#data</code>. That’s because if the incoming state is missing a key that <code>#data</code> has, we know that we don’t need to touch that key.<sup><a href="#user-content-fn-delete" id="user-content-fnref-delete" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">8</a></sup></p>
<p>For each key in the incoming state, we get the local register at that key. If we find one, the peer is <strong>updating</strong> an existing key that we already know about, so we call that register’s <code>merge</code> method with the incoming state at that key. Otherwise, the peer has <strong>added</strong> a new key to the map, so we instantiate a new LWW Register using the incoming state at that key.</p>
<p>In addition to the CRDT methods, we need to implement methods more commonly found on maps: <code>set</code>, <code>get</code>, <code>delete</code> and <code>has</code>.</p>
<p>Let’s start with <code>set</code>:</p>
<pre><code>  <span>set</span><span>(</span>key<span>:</span> <span>string</span><span>,</span> value<span>:</span> <span>T</span><span>)</span> <span>{</span>
    <span>// get the register at the given key</span>
    <span>const</span> register <span>=</span> <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>;</span>

    <span>// if the register already exists, set the value</span>
    <span>if</span> <span>(</span>register<span>)</span> register<span>.</span><span>set</span><span>(</span>value<span>)</span><span>;</span>
    <span>// otherwise, instantiate a new `LWWRegister` with the value</span>
    <span>else</span> <span>this</span><span>.</span>#data<span>.</span><span>set</span><span>(</span>key<span>,</span> <span>new</span> <span>LWWRegister</span><span>(</span><span>this</span><span>.</span>id<span>,</span> <span>[</span><span>this</span><span>.</span>id<span>,</span> <span>1</span><span>,</span> value<span>]</span><span>)</span><span>)</span><span>;</span>
  <span>}</span></code></pre>
<p>Just like in the merge method, we’re either calling the register’s <code>set</code> to update an existing key, or instantiating a new LWW Register to add a new key. The initial state uses the local peer ID, a timestamp of 1 and the value passed to <code>set</code>.</p>
<p><code>get</code> is even simpler:</p>
<pre><code>  <span>get</span><span>(</span>key<span>:</span> <span>string</span><span>)</span> <span>{</span>
    <span>return</span> <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>?.</span>value <span>??</span> <span>undefined</span><span>;</span>
  <span>}</span></code></pre>
<p>Get the register from the local map, and return its value if it has one.</p>
<p>Why coalesce to <code>undefined</code>? Because each register holds <code>T | null</code>. And with the <code>delete</code> method, we’re ready to explain why:</p>
<pre><code>  <span>delete</span><span>(</span>key<span>:</span> <span>string</span><span>)</span> <span>{</span>
    <span>// set the register to null, if it exists</span>
    <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>?.</span><span>set</span><span>(</span><span>null</span><span>)</span><span>;</span>
  <span>}</span></code></pre>
<p>Rather than fully removing the key from the map, we set the register value to <code>null</code>. The metadata is kept around so we can disambiguate deletions from states that simply don’t have a key yet. These are called <strong>tombstones</strong> — the ghosts of CRDTs past.</p>
<p>Consider what would happen if we really did delete the keys from the map, rather than leaving a tombstone. Here’s a playground where peers can add keys, but not delete them. Can you figure out how to get a peer to delete a key?</p>
<lwwmap-demo delete="off" merge="naive" state="{ &quot;foo&quot;: [&quot;alice&quot;, 1, &quot;lorem ipsum&quot;] }"></lwwmap-demo>

<p>Turn off the network, add a key to <code>alice</code>’s map, then turn the network back on. Finally, make a change to <code>bob</code>’s map. Since <code>alice</code> sees that the incoming state from <code>bob</code> is missing that key, she removes it from her own state — even though <code>bob</code> never knew about that key in the first place. Whoops!</p>
<p>Here’s a playground with the correct behavior. You can also see what happens when a key is deleted.</p>
<lwwmap-demo state="{ &quot;foo&quot;: [&quot;alice&quot;, 1, &quot;lorem ipsum&quot;] }"></lwwmap-demo>

<p>Notice how we never remove deleted keys from the map. This is one drawback to CRDTs — we can only ever add information, not remove it. Although from the application’s perspective the key has been fully deleted, the underlying state still records that the key was once there. In technical terms, we say that CRDTs are <strong>monotonically increasing</strong> data structures.<sup><a href="#user-content-fn-efficiency" id="user-content-fnref-efficiency" data-footnote-ref="" aria-describedby="footnote-label" data-astro-cid-bi7aps5f="">9</a></sup></p>
<p>The final LWW Map method is <code>has</code>, which returns a boolean indicating whether the map contains a given key.</p>
<pre><code>  <span>has</span><span>(</span>key<span>:</span> <span>string</span><span>)</span> <span>{</span>
    <span>// if a register doesn't exist or its value is null, the map doesn't contain the key</span>
    <span>return</span> <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>?.</span>value <span>!==</span> <span>null</span><span>;</span>
  <span>}</span></code></pre>
<p>There’s a special case here: if the map contains a register at the given key, but the register contains <code>null</code>, the map is considered to not contain the key.</p>
<p>For posterity, here’s the full LWW Map code:</p>
<pre><code><span>class</span> <span>LWWMap<span>&lt;</span><span>T</span><span>&gt;</span></span> <span>{</span>
  <span>readonly</span> id<span>:</span> <span>string</span><span>;</span>
  #data <span>=</span> <span>new</span> <span>Map<span>&lt;</span><span>string</span><span>,</span> LWWRegister<span>&lt;</span><span>T</span> <span>|</span> <span>null</span><span>&gt;&gt;</span></span><span>(</span><span>)</span><span>;</span>

  <span>constructor</span><span>(</span>id<span>:</span> <span>string</span><span>,</span> state<span>:</span> State<span>&lt;</span><span>T</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>this</span><span>.</span>id <span>=</span> id<span>;</span>

    <span>// create a new register for each key in the initial state</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span>key<span>,</span> register<span>]</span> <span>of</span> Object<span>.</span><span>entries</span><span>(</span>state<span>)</span><span>)</span> <span>{</span>
      <span>this</span><span>.</span>#data<span>.</span><span>set</span><span>(</span>key<span>,</span> <span>new</span> <span>LWWRegister</span><span>(</span><span>this</span><span>.</span>id<span>,</span> register<span>)</span><span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>

  <span>get</span> <span>value</span><span>(</span><span>)</span> <span>{</span>
    <span>const</span> value<span>:</span> Value<span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>{</span><span>}</span><span>;</span>

    <span>// build up an object where each value is set to the value of the register at the corresponding key</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span>key<span>,</span> register<span>]</span> <span>of</span> <span>this</span><span>.</span>#data<span>.</span><span>entries</span><span>(</span><span>)</span><span>)</span> <span>{</span>
      <span>if</span> <span>(</span>register<span>.</span>value <span>!==</span> <span>null</span><span>)</span> value<span>[</span>key<span>]</span> <span>=</span> register<span>.</span>value<span>;</span>
    <span>}</span>

    <span>return</span> value<span>;</span>
  <span>}</span>

  <span>get</span> <span>state</span><span>(</span><span>)</span> <span>{</span>
    <span>const</span> state<span>:</span> State<span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>{</span><span>}</span><span>;</span>

    <span>// build up an object where each value is set to the full state of the register at the corresponding key</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span>key<span>,</span> register<span>]</span> <span>of</span> <span>this</span><span>.</span>#data<span>.</span><span>entries</span><span>(</span><span>)</span><span>)</span> <span>{</span>
      <span>if</span> <span>(</span>register<span>)</span> state<span>[</span>key<span>]</span> <span>=</span> register<span>.</span>state<span>;</span>
    <span>}</span>

    <span>return</span> state<span>;</span>
  <span>}</span>

  <span>has</span><span>(</span>key<span>:</span> <span>string</span><span>)</span> <span>{</span>
    <span>return</span> <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>?.</span>value <span>!==</span> <span>null</span><span>;</span>
  <span>}</span>

  <span>get</span><span>(</span>key<span>:</span> <span>string</span><span>)</span> <span>{</span>
    <span>return</span> <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>?.</span>value<span>;</span>
  <span>}</span>

  <span>set</span><span>(</span>key<span>:</span> <span>string</span><span>,</span> value<span>:</span> <span>T</span><span>)</span> <span>{</span>
    <span>// get the register at the given key</span>
    <span>const</span> register <span>=</span> <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>;</span>

    <span>// if the register already exists, set the value</span>
    <span>if</span> <span>(</span>register<span>)</span> register<span>.</span><span>set</span><span>(</span>value<span>)</span><span>;</span>
    <span>// otherwise, instantiate a new `LWWRegister` with the value</span>
    <span>else</span> <span>this</span><span>.</span>#data<span>.</span><span>set</span><span>(</span>key<span>,</span> <span>new</span> <span>LWWRegister</span><span>(</span><span>this</span><span>.</span>id<span>,</span> <span>[</span><span>this</span><span>.</span>id<span>,</span> <span>1</span><span>,</span> value<span>]</span><span>)</span><span>)</span><span>;</span>
  <span>}</span>

  <span>delete</span><span>(</span>key<span>:</span> <span>string</span><span>)</span> <span>{</span>
    <span>// set the register to null, if it exists</span>
    <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>?.</span><span>set</span><span>(</span><span>null</span><span>)</span><span>;</span>
  <span>}</span>

  <span>merge</span><span>(</span>state<span>:</span> State<span>&lt;</span><span>T</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>// recursively merge each key's register with the incoming state for that key</span>
    <span>for</span> <span>(</span><span>const</span> <span>[</span>key<span>,</span> remote<span>]</span> <span>of</span> Object<span>.</span><span>entries</span><span>(</span>state<span>)</span><span>)</span> <span>{</span>
      <span>const</span> local <span>=</span> <span>this</span><span>.</span>#data<span>.</span><span>get</span><span>(</span>key<span>)</span><span>;</span>

      <span>// if the register already exists, merge it with the incoming state</span>
      <span>if</span> <span>(</span>local<span>)</span> local<span>.</span><span>merge</span><span>(</span>remote<span>)</span><span>;</span>
      <span>// otherwise, instantiate a new `LWWRegister` with the incoming state</span>
      <span>else</span> <span>this</span><span>.</span>#data<span>.</span><span>set</span><span>(</span>key<span>,</span> <span>new</span> <span>LWWRegister</span><span>(</span><span>this</span><span>.</span>id<span>,</span> remote<span>)</span><span>)</span><span>;</span>
    <span>}</span>
  <span>}</span>
<span>}</span></code></pre>
<h3 id="next-steps">Next Steps</h3>
<p>If you’re reading this now, we’re in the short period when this post has been published, but the next one has not. <a href="https://jakelazaroff.com/rss.xml" data-astro-cid-bi7aps5f="">Subscribe to my RSS feed</a> to be notified when it’s ready!</p>
<section data-footnotes="">
<ol>
<li id="user-content-fn-cvrdt">
<p>They’re also known as CvRDTs (“Cv” standing for “convergent”) and CmRDTs (“Cm” standing for “commutative”), respectively, although I think “state-based” and “operation-based” are the preferred terms. <a href="#user-content-fnref-cvrdt" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-delta">
<p>There are also delta CRDTs, or hybrid CRDTs, which allow peers to negotiate the subset of the state they need to send each other. That’s one example of blending operation-based and state-based CRDTs. But the fundamental tradeoff remains: sending less data between peers means more constraints on communication. <a href="#user-content-fnref-delta" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-ackshually">
<p>Technically, a CRDT can be anything that follows the merging rules described below. This is a working definition; in practice, implementations in object-oriented languages will end up looking something like this. <a href="#user-content-fnref-ackshually" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-thesamepicture">
<p>Commutativity and associativity might sound the same, and indeed most commutative operations are also associative. But there are a few math operations that are only one or the other. Matrix multiplication, for example, is <a href="https://en.wikipedia.org/wiki/Matrix_multiplication#Non-commutativity" data-astro-cid-bi7aps5f="">associative but not commutative</a><a data-tooltip="" href="https://en.wikipedia.org/wiki/Matrix_multiplication#Non-commutativity" data-astro-cid-bi7aps5f=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/Matrix_multiplication_qtl1.svg/1200px-Matrix_multiplication_qtl1.svg.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">Matrix multiplication - Wikipedia</span><span data-astro-cid-bi7aps5f=""><img src="https://en.wikipedia.org/static/favicon/wikipedia.ico#Non-commutativity" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/Matrix_multiplication#Non-commutativity</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"><use href="/icons.svg#share"></use></svg></span></a>. And surprisingly, floating point arithmetic — i.e. any math operator in JavaScript — <a href="https://en.wikipedia.org/wiki/Associative_property#Nonassociativity_of_floating_point_calculation" data-astro-cid-bi7aps5f="">is commutative but not associative</a><a data-tooltip="" href="https://en.wikipedia.org/wiki/Associative_property#Nonassociativity_of_floating_point_calculation" data-astro-cid-bi7aps5f=""><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2e/Associativity_of_binary_operations_%28without_question_marks%29.svg/1200px-Associativity_of_binary_operations_%28without_question_marks%29.svg.png" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">Associative property - Wikipedia</span><span data-astro-cid-bi7aps5f=""><img src="https://en.wikipedia.org/static/favicon/wikipedia.ico#Nonassociativity_of_floating_point_calculation" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/Associative_property#Nonassociativity_of_floating_point_calculation</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"><use href="/icons.svg#share"></use></svg></span></a>! <a href="#user-content-fnref-thesamepicture" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-logical">
<p>You might ask: why not use the actual time? Unfortunately, accurately syncing clocks between two computers is an extremely hard problem. Using incrementing integers like this is one simple version of a <a href="https://en.wikipedia.org/wiki/Logical_clock" data-astro-cid-bi7aps5f="">logical clock</a><a data-tooltip="" href="https://en.wikipedia.org/wiki/Logical_clock" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">Logical clock - Wikipedia</span><span data-astro-cid-bi7aps5f=""><img src="https://en.wikipedia.org/static/favicon/wikipedia.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">en.wikipedia.org/wiki/Logical_clock</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"><use href="/icons.svg#share"></use></svg></span></a>, which captures the order of events relative to each other rather than to the “wall clock”. <a href="#user-content-fnref-logical" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-citation">
<p>[citation needed] <a href="#user-content-fnref-citation" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-null">
<p>If the value type is <code>T</code>, why is the state type a union of <code>T | null</code>? More on that in a bit! <a href="#user-content-fnref-null" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-delete">
<p>You might be wondering: if another peer deleted a key from their map, shouldn’t we remove it from our local map as well? It’s the same reason the state holds <code>T | null</code>. We’re getting there! <a href="#user-content-fnref-delete" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
<li id="user-content-fn-efficiency">
<p>There are a couple ways mitigate this drawback, both of which are outside the scope of this article. One is “garbage collection”: pruning tombstones from CRDTs, which prevents you from merging states with any changes made before the tombstones were removed. Another is creating an efficient format to encode the data. You can also combine these methods. Research suggests that <a href="https://youtu.be/x7drE24geUw?t=3587" data-astro-cid-bi7aps5f="">this can result in as little as 50% overhead compared to the “plain” data</a><a data-tooltip="" href="https://youtu.be/x7drE24geUw?t=3587" data-astro-cid-bi7aps5f=""><img src="https://i.ytimg.com/vi/x7drE24geUw/maxresdefault.jpg" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">CRDTs: The Hard Parts</span><span data-astro-cid-bi7aps5f="">A talk on the latest research on CRDTs, originally given at the Hydra distributed computing conference on 6 July 2020.References: https://martin.kleppmann.co...</span><span data-astro-cid-bi7aps5f=""><img src="https://www.youtube.com/s/desktop/a24ea7cc/img/favicon.ico" alt="" onerror="this.remove()" data-astro-cid-bi7aps5f=""><span data-astro-cid-bi7aps5f="">youtu.be/x7drE24geUw?t=3587</span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16"><use href="/icons.svg#share"></use></svg></span></a>. <a href="#user-content-fnref-efficiency" data-footnote-backref="" aria-label="Back to content" data-astro-cid-bi7aps5f="">↩</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Strong static typing, a hill I'm willing to die on (116 pts)]]></title>
            <link>https://www.svix.com/blog/strong-typing-hill-to-die-on/</link>
            <guid>37764326</guid>
            <pubDate>Wed, 04 Oct 2023 12:49:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.svix.com/blog/strong-typing-hill-to-die-on/">https://www.svix.com/blog/strong-typing-hill-to-die-on/</a>, See on <a href="https://news.ycombinator.com/item?id=37764326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img alt="Cover image" src="https://www.svix.com/blog/static/images/generated/strong-typing-hill-to-die-on/cover-IT6ATRYD.png"></p>
<p>Svix is the enterprise ready webhooks sending service. With Svix, you can build a secure,
reliable, and scalable webhook platform in minutes. Looking to send webhooks?<!-- --> <!-- -->
<a href="https://www.svix.com/">Give it a try!</a></p>
<p><strong>Edit:</strong> previous version of this post didn't have "static" in the title to keep it short (but the body did). Added it in the title for clarification.</p>
<p>I've been writing software for over 20 years, and with every day that goes by I grow more certain that strong static typing is not just a good idea, but also almost always the right choice.</p>
<p>There are definitely uses for untyped languages (or language variants), for example they are much nicer when using a REPL, or for throwaway scripts in environments that are already hopelessly untyped (e.g. the shell). In almost every other case, however, strong typing is strongly preferred.</p>
<p>There are advantages to not using types, such as a faster development speed, but they pale in comparison to all of the advantages. To that I say:</p>
<blockquote>
<p>Writing software without types lets you go at full speed. Full speed towards the cliff.</p>
</blockquote>
<p>The question around strong static typing is simple: would you rather work a bit more and get invariants checked at compile-time (or type-checking time for non-compiled languages), or work a bit less and have them be enforced at runtime, or even worse not enforced even at runtime (JavaScript, I'm looking at you... <code>1 + "2" == 12</code>).</p>
<p>Getting errors at runtime is a terrible idea. First, it means that you won't always catch them during development. Second, when you do catch them, it will happen in a customer facing manner. Yes, tests help, but writing tests for every possible mistyped function parameter is impossible given the endless possibilities. Even if you could, having types is much easier than testing for wrong types.</p>
<h2 id="types-lead-to-less-bugs">Types lead to less bugs</h2>
<p>Types also offer annotations to code that benefit both humans and machines. Having types is a way to more strictly define the contract between different pieces of code.</p>
<p>Consider the following four examples. They all do exactly the same thing just with varying level of contract definition.</p>
<div><pre><code><span><span>// Params: Name (a string) and age (a number).</span>
</span><span><span>function</span> <span>birthdayGreeting1</span><span>(</span><span><span>...</span>params</span><span>)</span> <span>{</span>
</span><span>    <span>return</span> <span><span>`</span><span><span>${</span>params<span>[</span><span>0</span><span>]</span><span>}</span></span><span> is </span><span><span>${</span>params<span>[</span><span>1</span><span>]</span><span>}</span></span><span>!</span><span>`</span></span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>// Params: Name (a string) and age (a number).</span>
</span><span><span>function</span> <span>birthdayGreeting2</span><span>(</span><span>name<span>,</span> age</span><span>)</span> <span>{</span>
</span><span>    <span>return</span> <span><span>`</span><span><span>${</span>name<span>}</span></span><span> is </span><span><span>${</span>age<span>}</span></span><span>!</span><span>`</span></span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>function</span> <span>birthdayGreeting3</span><span>(</span><span>name<span>:</span> string<span>,</span> age<span>:</span> number</span><span>)</span><span>:</span> string <span>{</span>
</span><span>    <span>return</span> <span><span>`</span><span><span>${</span>name<span>}</span></span><span> is </span><span><span>${</span>age<span>}</span></span><span>!</span><span>`</span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div>
<p>The first one doesn't even define the number of parameters, so it's hard to know what it does without reading the docs. I believe most people will agree the first one is an abomination and wouldn't write code like that. Though it's a very similar idea to typing, it's about defining the contract between the caller and the callee.</p>
<p>As for the second and the third, because of the typing, the third will need less documentation. The code is simpler, but admittedly, the advantages are fairly limited. Well, until you actually change this function...</p>
<p>In both the second and the third functions, the author assumes the age is a number. So it is absolutely fine to change the code as below:</p>
<div><pre><code><span><span>// Params: Name (a string) and age (a number).</span>
</span><span><span>function</span> <span>birthdayGreeting2</span><span>(</span><span>name<span>,</span> age</span><span>)</span> <span>{</span>
</span><span>    <span>return</span> <span><span>`</span><span><span>${</span>name<span>}</span></span><span> will turn </span><span><span>${</span>age <span>+</span> <span>1</span><span>}</span></span><span> next year!</span><span>`</span></span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>function</span> <span>birthdayGreeting3</span><span>(</span><span>name<span>:</span> string<span>,</span> age<span>:</span> number</span><span>)</span><span>:</span> string <span>{</span>
</span><span>    <span>return</span> <span><span>`</span><span><span>${</span>name<span>}</span></span><span> will turn </span><span><span>${</span>age <span>+</span> <span>1</span><span>}</span></span><span> next year!</span><span>`</span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div>
<p>The problem is that some of the places that use this code accept user input which was collected from an HTML input (so always a string). Which will result in:</p>
<div><pre><code><span><span>&gt;</span> <span>birthdayGreeting2</span><span>(</span><span>"John"</span><span>,</span> <span>"20"</span><span>)</span>
</span><span><span>"John will turn 201 next year!"</span>
</span></code></pre></div>
<p>While the typed version will correctly fail to compile because this function excepts age to be a number, not a string.</p>
<p>Having the contract between a caller and callee is important for a codebase, so that callers can know when callees change. This is especially important for an open source library, where the callers and the callees are not written by the same group of people. With this contract it's impossible to know how things change when they do.</p>
<h2 id="types-lead-to-a-better-development-experience">Types lead to a better development experience</h2>
<p>Typing can also be used by IDEs and other development tools to vastly improve the development experience. You get notified as you code if any of your expectations are wrong. This significantly reduces cognitive load. You no longer need to remember the types of all the variables and the function in the context. The compiler will be there with you and tell you when something is wrong.</p>
<p>This also leads to a very nice additional benefit: easier refactoring. You can trust the compiler to let you know whether a change you make (e.g. the change in our example above) will break assumptions made elsewhere in the code or not.</p>
<p>Types also make it much easier to onboard new engineers to a codebase or library:</p>
<ol>
<li>They can follow the type definitions to understand where things are used.</li>
<li>It's much easier to tinker with things as changes will trigger a compile error.</li>
</ol>
<p>Let's consider the following changes to our above code:</p>
<div><pre><code><span><span>class</span> <span>Person</span> <span>{</span>
</span><span>  name<span>:</span> <span>string</span><span>;</span>
</span><span>  age<span>:</span> <span>number</span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>function</span> <span>birthdayGreeting2</span><span>(</span>person<span>)</span> <span>{</span>
</span><span>    <span>return</span> <span><span>`</span><span><span>${</span>person<span>.</span>name<span>}</span></span><span> will turn </span><span><span>${</span>person<span>.</span>age <span>+</span> <span>1</span><span>}</span></span><span> next year!</span><span>`</span></span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>function</span> <span>birthdayGreeting3</span><span>(</span>person<span>:</span> Person<span>)</span><span>:</span> <span>string</span> <span>{</span>
</span><span>    <span>return</span> <span><span>`</span><span><span>${</span>person<span>.</span>name<span>}</span></span><span> will turn </span><span><span>${</span>person<span>.</span>age <span>+</span> <span>1</span><span>}</span></span><span> next year!</span><span>`</span></span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span>function</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
</span><span>  <span>const</span> person<span>:</span> Person <span>=</span> <span>{</span> name<span>:</span> <span>"Hello"</span><span>,</span> age<span>:</span> <span>12</span> <span>}</span><span>;</span>
</span><span>
</span><span>  <span>birthdayGreeting2</span><span>(</span>person<span>)</span><span>;</span>
</span><span>
</span><span>  <span>birthdayGreeting3</span><span>(</span>person<span>)</span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div>
<p>It's very easy to see (or use your IDE to find) all the places where <code>Person</code> is used. You can see it's initiated in <code>main</code> and you can see it's used by birthdayGreeting3<code>. However, in order to know it's used in </code>birthdayGreeting2`, you'd need to read the entire codebase.</p>
<p>The flip side of this is also that when looking at <code>birthdayGreeting2</code>, it's hard to know that it expects a <code>Person</code> as a parameter. Some of these things can be solved by exhaustive documentation, but: (1) why bother if you can achieve more with types? (2) documentation goes stale, here the code is the documentation.</p>
<p>It's very similar to how you wouldn't write code like:</p>
<div><pre><code><span><span>// a is a person</span>
</span><span><span>function</span> <span>birthdayGreeting2</span><span>(</span><span>a</span><span>)</span> <span>{</span>
</span><span>    b <span>=</span> person<span>.</span><span>name</span><span>;</span>
</span><span>    c <span>=</span> person<span>.</span><span>age</span><span>;</span>
</span><span>    <span>return</span> <span><span>`</span><span><span>${</span>b<span>}</span></span><span> will turn </span><span><span>${</span>c <span>+</span> <span>1</span><span>}</span></span><span> next year!</span><span>`</span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre></div>
<p>You would want to use useful variable names. Typing is the same, it's just variable names on steriods.</p>
<h2 id="we-encode-everything-in-the-type-system">We encode everything in the type system</h2>
<p>At Svix we love types. In fact, we try to encode as much information as we possibly can in the type system, so that all of the errors that can be caught at compile time will be caught at compile time; and also to squeeze that extra mileage of developer experience improvements.</p>
<p>For example, Redis is a string based protocol with no inherent typing. We use Redis for caching (among other things). The problem is that all of our nice typing benefits will be lost at the Redis layer, and bugs can happen.</p>
<p>Consider the following piece of code:</p>
<div><pre><code><span><span>pub</span> <span>struct</span> <span>Person</span> <span>{</span>
</span><span>    <span>pub</span> id<span>:</span> <span>String</span><span>,</span>
</span><span>    <span>pub</span> name<span>:</span> <span>String</span><span>,</span>
</span><span>    <span>pub</span> age<span>:</span> <span>u16</span><span>,</span>
</span><span><span>}</span>
</span><span>
</span><span><span>pub</span> <span>struct</span> <span>Pet</span> <span>{</span>
</span><span>    <span>pub</span> id<span>:</span> <span>String</span><span>,</span>
</span><span>    <span>pub</span> owner<span>:</span> <span>String</span><span>,</span>
</span><span><span>}</span>
</span><span>
</span><span>
</span><span><span>let</span> id <span>=</span> <span>"p123"</span><span>;</span>
</span><span><span>let</span> person <span>=</span> <span>Person</span><span>::</span><span>new</span><span>(</span><span>"John"</span><span>,</span> <span>20</span><span>)</span><span>;</span>
</span><span>cache<span>.</span><span>set</span><span>(</span><span>format!</span><span>(</span><span>"person-{id}"</span><span>)</span><span>,</span> person<span>)</span><span>;</span>
</span><span><span>// ...</span>
</span><span><span>let</span> pet<span>:</span> <span>Pet</span> <span>=</span> cache<span>.</span><span>get</span><span>(</span><span>format!</span><span>(</span><span>"preson-{id}"</span><span>)</span><span>)</span><span>;</span>
</span></code></pre></div>
<p>There are a couple of bugs in the snippet:</p>
<ol>
<li>There's a typo in the second key name.</li>
<li>We are trying to load a person into a pet type.</li>
</ol>
<p>To avoid such issues we do two things at Svix. The first is that we require the key to be of a certain type (not a generic string), and to create this type you need to call a specific fuction. The second thing we do, is force pairing a key to a value.</p>
<p>So the above example would look something like:</p>
<div><pre><code><span><span>pub</span> <span>struct</span> <span>PersonCacheKey</span><span>(</span><span>String</span><span>)</span><span>;</span>
</span><span>
</span><span><span>impl</span> <span>PersonCacheKey</span> <span>{</span>
</span><span>    <span>fn</span> <span>new</span><span>(</span>id<span>:</span> <span>&amp;</span><span>str</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span> <span>...</span> <span>}</span>
</span><span><span>}</span>
</span><span>
</span><span><span>pub</span> <span>struct</span> <span>Person</span> <span>{</span>
</span><span>    <span>pub</span> id<span>:</span> <span>String</span><span>,</span>
</span><span>        <span>pub</span> name<span>:</span> <span>String</span><span>,</span>
</span><span>        <span>pub</span> age<span>:</span> <span>u16</span><span>,</span>
</span><span><span>}</span>
</span><span>
</span><span><span>pub</span> <span>struct</span> <span>PetCacheKey</span><span>;</span>
</span><span>
</span><span><span>pub</span> <span>struct</span> <span>Pet</span> <span>{</span>
</span><span>    <span>pub</span> id<span>:</span> <span>String</span><span>,</span>
</span><span>        <span>pub</span> owner<span>:</span> <span>String</span><span>,</span>
</span><span><span>}</span>
</span><span>
</span><span>
</span><span><span>let</span> id <span>=</span> <span>"p123"</span><span>;</span>
</span><span><span>let</span> person <span>=</span> <span>Person</span><span>::</span><span>new</span><span>(</span>id<span>,</span> <span>"John"</span><span>,</span> <span>20</span><span>)</span><span>;</span>
</span><span>cache<span>.</span><span>set</span><span>(</span><span>PersonCacheKey</span><span>::</span><span>new</span><span>(</span>id<span>)</span><span>,</span> person<span>)</span><span>;</span>
</span><span><span>// ...</span>
</span><span><span>// Compilation will fail on the next line</span>
</span><span><span>let</span> pet<span>:</span> <span>Pet</span> <span>=</span> cache<span>.</span><span>get</span><span>(</span><span>PersonCacheKey</span><span>::</span><span>new</span><span>(</span>id<span>)</span><span>)</span><span>;</span>
</span></code></pre></div>
<p>This is already much better, and makes it impossible to get either of the previously mentioned bugs. Though we can do even better!</p>
<p>Consider the following function:</p>
<div><pre><code><span><span>pub</span> <span>fn</span> <span>do_something</span><span>(</span>id<span>:</span> <span>String</span><span>)</span> <span>{</span>
</span><span>    <span>let</span> person<span>:</span> <span>Person</span> <span>=</span> cache<span>.</span><span>get</span><span>(</span><span>PersonCacheKey</span><span>::</span><span>new</span><span>(</span>id<span>)</span><span>)</span><span>;</span>
</span><span>    <span>// ...</span>
</span><span><span>}</span>
</span></code></pre></div>
<p>There are a couple of problems with it. The first is that it's not very clear which <code>id</code> it should be for. Is it a person? A pet? It's very easy to accidentally call it with the wrong one, like in the following example:</p>
<div><pre><code><span><span>let</span> pet <span>=</span> <span>...</span><span>;</span>
</span><span><span>do_something</span><span>(</span>pet<span>.</span>id<span>)</span><span>;</span> <span>// &lt;-- should be pet.owner!</span>
</span></code></pre></div>
<p>The second is that we are losing the discoverability. It's a bit hard to know that a Pet has a relationship to a Person.</p>
<p>So at Svix, we have a special type for each <code>id</code> to ensure that there are no mistakes. The adjusted code looks something like:</p>
<div><pre><code><span><span>pub</span> <span>struct</span> <span>PersonId</span><span>(</span><span>String</span><span>)</span><span>;</span>
</span><span><span>pub</span> <span>struct</span> <span>PetId</span><span>(</span><span>String</span><span>)</span><span>;</span>
</span><span>
</span><span><span>pub</span> <span>struct</span> <span>Person</span> <span>{</span>
</span><span>    <span>pub</span> id<span>:</span> <span>PersonId</span><span>,</span>
</span><span>    <span>pub</span> name<span>:</span> <span>String</span><span>,</span>
</span><span>    <span>pub</span> age<span>:</span> <span>u16</span><span>,</span>
</span><span><span>}</span>
</span><span>
</span><span><span>pub</span> <span>struct</span> <span>Pet</span> <span>{</span>
</span><span>    <span>pub</span> id<span>:</span> <span>PetId</span><span>,</span>
</span><span>    <span>pub</span> owner<span>:</span> <span>PersonId</span><span>,</span>
</span><span><span>}</span>
</span></code></pre></div>
<p>This is indeed much better than our previous example.</p>
<p>There is still one issue. If we accept the <code>id</code>s from the API, how do we know that they are valid? All of the pet <code>id</code>s in Svix, for example, are prefixed with <code>pet_</code> and are then followed by a Ksuid like so: <code>pet_25SVqQSCVpGZh5SmuV0A7X0E3rw</code>.</p>
<p>We want to be able to tell our customers that they pass the wrong <code>id</code> in the API, e.g. they pass a person <code>id</code> when a pet one is expected. One simple solution for this is to validate this (duh...) but it can be easy to forget to validate it everywhere that it's used.</p>
<p>So we enforce that a <code>PetId</code> can never be created without first being validated. This way we know that all of the code paths that create a <code>PetId</code> first make sure it's valid. This means that when we return a <code>404 Not Found</code> to a customer because a pet isn't found in the database, we can be sure it was actually a valid <code>id</code> that wasn't found in the database. If it wasn't a valid <code>id</code>, we would have already returned a <code>422</code> or <code>400</code> when it was passed to the API handlers.</p>
<h2 id="so-why-doesnt-everyone-like-types">So why doesn't everyone like types?</h2>
<p>The main topic of argument against types are:</p>
<ol>
<li>Development speed</li>
<li>Learning curve and types complexity</li>
<li>The amount of effort and boilerplate required</li>
</ol>
<p>First of all, I'd argue that even if all of the above were true, the advantages mentioned above are well worth the trouble. Though I also don't agree with all of the above.</p>
<p>The first one is development speed. Prototyping without types is definitely much faster. You can comment out pieces of the code and won't have a compiler complain to you. You can set the wrong values for some of the fields until you're ready to figure out the right ones, etc.</p>
<p>Though like I said above: "Writing software without types lets you go at full speed. Full speed towards the cliff." The problem is that this is just aggressive and unnecessary technical debt. You'll pay it mulitple times over when you need to debug why your code doesn't work (either locally, in the test suite, or in production).</p>
<p>As for the learning curve: Yes, learning more things takes time. Though I'd say that most people don't need to be typing experts. They can just get by with very simple type expressions, and ask if they ever hit a wall. However, if you keep things simple, you'll probably rarely if ever hit one.</p>
<p>Additionally, people are already required to learn how to code, learn frameworks (React, Axum, etc.), and so many other things. I don't think the learning burden is as significant as it's made out to be.</p>
<p>Last, but not least, regarding the learning curve: I strongly believe that the benefits of a lessened learning curve by not having to know types, are much less than the benefits of using the type script to onboard on a specific codebase. Especially since learning types is a one time cost.</p>
<p>The last point is about the amount of effort and boilerplate required to use types in your codebase. I strongly believe that the amount of effort is actually less than the amount of effort required by not writing types.</p>
<p>Not using types requires significant documentation and testing in order to reach even a basic level of sanity. Documentation can go stale, and so can testing; and either way they require more effort than just adding the right types. Reading code with types is also easier, because you get the types inline instead of in the function documentation where it's in an inconsistent format with a lot of added noise.</p>
<p>Yes, typing can be a pain in languages that don't support inference, e.g Java can be tedious:</p>
<p><strong>Edit:</strong> It seems that Java has type inference nowadays. Thanks <code>pron</code> for <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=37765419">the correction on HN</a>.</p>
<div><pre><code><span><span>Person</span> person1 <span>=</span> <span>newPerson</span><span>(</span><span>)</span><span>;</span>
</span><span><span>Person</span> person2 <span>=</span> <span>newPerson</span><span>(</span><span>)</span><span>;</span>
</span><span><span>Person</span> child <span>=</span> <span>makeChild</span><span>(</span>person1<span>,</span> person2<span>)</span><span>;</span>
</span></code></pre></div>
<p>while mother languages that have inference (like Rust), are much nicer:</p>
<div><pre><code><span><span>let</span> person1 <span>=</span> <span>new_person</span><span>(</span><span>)</span><span>;</span>
</span><span><span>let</span> person2 <span>=</span> <span>new_person</span><span>(</span><span>)</span><span>;</span>
</span><span><span>let</span> child <span>=</span> <span>make_child</span><span>(</span>person1<span>,</span> person2<span>)</span><span>;</span>
</span></code></pre></div>
<p>So having the right tools definitely helps.</p>
<p>Speaking of tools, in order to reap the benefits of your typing, you probably need to use a code editor (or IDE) that supports modern code completion that is language aware.</p>
<h2 id="closing-words">Closing words</h2>
<p>I can see both side of the arguments on many topics, such as <code>vim</code> vs. <code>emacs</code>, tabs vs. spaces, and even much more controversial ones. Though in this case, the costs are so low compared to the benefits that I just don't understand why anyone would ever choose not to use types.</p>
<p>I'd love to know what I'm missing, but until then: Strong typing is a hill I'm willing to die on.</p>
<hr>
<p>For more content like this, make sure to follow us on <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/SvixHQ">Twitter</a>, <a target="_blank" rel="noopener noreferrer" href="https://github.com/svix">Github</a> or <a target="_blank" rel="noopener noreferrer" href="https://www.svix.com/blog/rss/">RSS</a> for the latest updates for the <a target="_blank" rel="noopener noreferrer" href="https://www.svix.com/">Svix webhook service</a>, or join the discussion on <a target="_blank" rel="noopener noreferrer" href="https://www.svix.com/slack/">our community Slack</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The FTC Sues to Break Up Amazon over an Economy-Wide “Hidden Tax” (166 pts)]]></title>
            <link>https://www.thebignewsletter.com/p/the-ftc-sues-to-break-up-amazon-over</link>
            <guid>37763424</guid>
            <pubDate>Wed, 04 Oct 2023 11:03:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thebignewsletter.com/p/the-ftc-sues-to-break-up-amazon-over">https://www.thebignewsletter.com/p/the-ftc-sues-to-break-up-amazon-over</a>, See on <a href="https://news.ycombinator.com/item?id=37763424">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><em><span>Welcome to BIG, a newsletter on the politics of monopoly power. If you’d like to sign up to receive issues over email, you can do so </span><a href="https://mattstoller.substack.com/subscribe" rel="">here</a><span>.</span></em></p><p>Yesterday, the Federal Trade Commission and 17 states filed an antitrust suit against Amazon, one of the biggest companies in the world, for monopolization and unfair methods of competition. This piece is about what this case means, the government’s claims, and whether the legal arguments are strong. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png" width="806" height="946" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:946,&quot;width&quot;:806,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:958360,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa4743f04-1158-4fa0-8cf1-0197dd322589_806x946.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Jeff Bezos just wants to delight you.</figcaption></figure></div><p><span>For almost 20 years, Amazon’s business has existed on a very strange premise that almost no one questions. Ostensibly, the retail firm is the lowest price option in the market, and it offers “free shipping” to over 100 million Amazon Prime customers, for which it charges a $139 a year membership fee. The revenue from Prime does not come close to covering the cost of its logistics arm, which was $85 billion in 2022. According to one J.P. Morgan analyst, the </span><a href="https://www.cnbc.com/select/amazon-prime-is-it-worth-it/" rel="">actual value of Prime is roughly $1,000</a><span> for a customer, which means that Amazon is subsidizing its Prime business to the tune of tens of billions of dollars a year. Where is this money coming from? To put it differently, how can Amazon be the lowest-cost retailer and offer free shipping, all while making money?</span></p><p><span>A lot of people have tried to argue that this is some grand technological feat, or business genius. But the answer, as it turns out, is that there is no such thing as a free lunch. Consumers pay for the free shipping, it’s just a hidden tax baked into the price of what you buy through an extraordinarily clever scheme put forward by Amazon. And that’s what the Federal Trade Commission and 17 states are suing over. This hidden tax is well-known in the industry. Here’s Mike Beckham, the CEO of Simple Modern and an exceptionally successful retailer, </span><a href="https://twitter.com/mikebeckhamsm/status/1691615816217768304" rel="">making that point</a><span> in August.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png" width="1164" height="572" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:572,&quot;width&quot;:1164,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120693,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9810f714-3a1a-4cca-888f-ce55e6692ce3_1164x572.png 1456w" sizes="100vw"></picture></div></a></figure></div><p><span>Most people think of Amazon as a retailer who sells to retail customers. But retail end users — you and me — aren’t really the customer. We are the product. And Amazon doesn’t really sell to us; it’s a middleman who sells </span><em>access</em><span> to us. The actual customers of Amazon are third-party businesses that rely on Amazon’s infrastructure to get their wares to the public.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png" width="1228" height="492" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aabe8d85-2874-4406-957e-0459ba14de68_1228x492.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:492,&quot;width&quot;:1228,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:246466,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faabe8d85-2874-4406-957e-0459ba14de68_1228x492.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>In 2022, Amazon CEO Andy Jassy said this explicitly, “small and medium sized” sellers use Amazon not because of the “eCommerce software” Amazon provides but “because they get access to a few hundred million customers.” There are extremely high switching costs to move from one online superstore to another, so this is a business with significant barriers to entry.</p><p>Indeed, this barrier to entry is baked into the very structure of Amazon Prime and its free shipping, which was created, Jeff Bezos said, “to draw a moat around our best customers.” Here’s how the FTC complaint describes the allure of Prime, which is a bundle of services customers would prefer to order separately.</p><blockquote><p>“This current restrictive structure of Prime reflects a deliberate strategy by Amazon to artificially increase barriers to entry and competition. As one former Amazon executive explained in recalling Amazon's motivation for adding non-shipping services to Prime, ‘[a]ny competitor might launch a Prime shipping clone, or they could potentially build a new Netflix-type service, but it was unlikely that any one of them would be able to do both.’”</p></blockquote><p>Amazon’s strategy is to use Prime to build, extend, and fortify barriers to entry. One result of Prime is that Amazon now has an overwhelming monopoly share of online shoppers. And online shoppers aren’t the customer, but the product that Amazon brokers to third-party sellers, who  must either pay Amazon what it demands or lose access to the market. As one seller put it, "We have nowhere else to go and Amazon knows it." </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png" width="1102" height="806" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:806,&quot;width&quot;:1102,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:436864,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea0cf560-2f53-4e96-9d1b-4357d79ffbd0_1102x806.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Once it achieved monopoly power, Amazon squeezed on price through fees to third-party sellers. As a third-party seller, you pay fees for listing on Amazon; for using Amazon’s warehouse services, known as Fulfillment by Amazon (FBA); and for advertising services. If you don’t pay, you don’t get put in a place on the site where consumers click. "Advertised products on Amazon,” reads the complaint, “are 46 times more likely to be clicked on when compared with products that are not advertised." And these fees have all increased steadily over the years.</p><p><span>At this point, the price Amazon charges these third party sellers has grown to nearly 50% of its revenue. It is this money, </span><a href="https://cdn.ilsr.org/wp-content/uploads/2023/03/AmazonMonopolyTollbooth-2023.pdf" rel="">estimated at $123 billion</a><span> in total last year, that pays for “free” shipping, as well as its video service, its music service, Twitch, and everything else that comes bundled with Prime. These third-party sellers in turn raise their prices to consumers, aka you and me, and then send that money back to Amazon in the form of fees. It’s basically money laundering.</span></p><p>This dynamic increasingly ruins the consumer experience. Amazon is now so full of pay-to-play ads that consumers are complaining they can’t find organic results, but are instead directed to higher priced items. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png" width="855" height="479" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:479,&quot;width&quot;:855,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:399812,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee348173-8c7f-4e8b-9701-5c5bba57d27c_855x479.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>In the red box are sponsored links, which are hard to distinguish from organic best results.</figcaption></figure></div><p>As the FTC noted, one “senior Amazon executive reportedly compared Amazon's advertising and search divisions to the parable of the scorpion and the frog: it was in the advertising division's nature as the proverbial ‘scorpion’ to poison organic search results."</p><p>Still, why can’t a third-party seller offer a lower price outside of Amazon? Good question! That’s where the scheme gets very clever. Originally, Amazon imposed contracts, as the FTC noted, “barring all sellers from offering their goods for lower prices anywhere else.” But Europeans, and Senator Richard Blumenthal, complained about these price parity agreements, so Amazon dropped its explicit contractual requirements in 2019. </p><p><span>However, this change was a farce. The firm simply did through code what it couldn’t do through contract. "Amazon,” claims the FTC, “has implemented an algorithm for the express purpose of deterring other online stores from offering lower prices." Just weeks after dropping this requirement, Amazon wrote it would not </span><a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-reveals-additional-evidence-antitrust-violations-lawsuit" rel="">change its policies</a><span>, as California’s attorney general revealed in April. The firm even speculated that “media and selling partners may claim the removal of the clause was not only trivial but a trick and an attempt to garner goodwill with policymakers amid increasing competition concerns.”</span></p><p>Today, Amazon tells sellers that if it detects a lower price for their products on any other online store, they will be punished, which is to say, their ability to get their products onto a place on the Amazon website where customers click will go away. The net effect, as Amazon itself wrote, is that "prices will go up."</p><p><span>Indeed, Amazon tells third-party sellers to </span><a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-reveals-additional-evidence-antitrust-violations-lawsuit" rel="">raise prices</a><span>. In 2019, a seller complained to Amazon that they were being asked “to take our prices down [on&nbsp;Amazon] to match our own [website] store.”&nbsp;Amazon’s VP of Pricing told the seller’s account manager that “[Y]ou might want to ask him to check if his sales on other sites directly or through distributors is&nbsp;putting him and us at a relative competitive disadvantage…&nbsp; He might&nbsp;get the hint. :)”</span></p><p><span>If this whole scheme sounds familiar, it’s because I described it in May of 2021 when then-D.C. Attorney General Karl Racine filed a similar though much less developed suit, in a piece called </span><a href="https://www.thebignewsletter.com/p/amazon-primes-free-shipping-promise" rel="">“Amazon Prime Is an Economy-Distorting Lie.”</a><span> Racine left out some key details, and got a bad judge, so that case was dismissed, though it’s on appeal. California Attorney General Rob Bonta filed a very similar suit, which has </span><a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-secures-court-decision-denying-amazon%E2%80%99s-attempt-evade" rel="">survived important procedural elements</a><span> and will go to trial. Another related suit, this one class action, in Washington state also passed a summary judgment.</span></p><p><span>The overall point is that Amazon is degrading the shopping experience, raising prices, and yet somehow still gaining market share. It is able to block third-party sellers from going elsewhere, and therefore also stop potential rivals from gaining share. It’s a fairly strong complaint, and while antitrust cases are always random, </span><a href="https://twitter.com/billbaer50/status/1706745133922767041" rel="">Bill Baer</a><span>, the former Assistant Attorney General for Antitrust under the Obama administration, said it’s a strong argument. “If the FTC and states can prove even some of the factual allegations in this 172 page complaint,” he said, “Amazon is in a world of hurt.”</span></p><p><span>If this suit is resolved in the government’s favor, the remedy could be anything from ending anti-discounting measures to breaking up the firm. Regardless, there will be lower prices and discounting all over the web, and a lot of new firms will be able to enter the market. As one legal analyst told me last year, “Let's say a product today is sold for $10 on Amazon with 'free shipping'. If Amazon is forced to unbundle the FBA fee from the product price then it would cost $6 + $4 shipping. Prime makes no sense in this world unless Amazon again decided to subsidize Prime.” And even Amazon doesn’t have the tens of billions of dollars required to do that. Indeed, the stakes here are one reason that antitrust legend Bill Kovacic </span><a href="https://www.politico.com/news/2023/09/27/ftc-bill-kovacic-amazon-khan-00118353" rel="">called</a><span> the Amazon complaint “the most important case that the FTC has brought in its 109-year history.”</span></p><p><span>One of the more disturbing trends in antitrust is how secretive courts have become on antitrust matters. I wrote this up on Sunday in </span><a href="https://www.thebignewsletter.com/p/how-to-hide-a-2-trillion-antitrust" rel="">my piece</a><span> on how Judge Amit Mehta is hiding the Google trial, and </span><em>the New York Times</em><span> did a big story on </span><a href="https://www.nytimes.com/2023/09/26/technology/google-antitrust-trial-secrecy.html" rel="">the “unprecedented” secrecy</a><span> of the trial, which does seem to have embarrassed Mehta. Journalists are getting increasingly angry over these redactions and closed sessions, because they want to write about stuff, and when you remove all the interesting tidbits it means they can’t. (The same journalists who cover the Google trial are also covering the Amazon complaint.)</span></p><p>Much of the interesting information in this Amazon complaint is redacted, including two particular elements I want to highlight. The first is that Amazon, like Google, has been destroying documents and internal conversations. The extent of their perfidy is not clear, since the paragraph alleging wrongdoing is redacted.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png" width="1456" height="496" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:496,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:574817,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc24032c-69d0-476f-933c-3db0be2ac2ea_1566x534.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The second is something called Project Nessie, which is an algorithmic pricing system so egregious that the FTC determined that it deserved its own charge as an unfair method of competition. What is Project Nessie? I don’t know, since nearly all of the information about it is redacted. But it looks fascinating.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png" width="1456" height="833" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:833,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:184850,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc6e84c8-3ef1-4278-ab77-72db686e78e8_1532x876.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Fortunately, our work on the secrecy of the Google trial is paying dividends. Here’s FTC official Douglas Farrar arguing that the commission will </span><a href="https://twitter.com/DouglasLFarrar/status/1706734139456589857" rel="">push</a><span> to get rid of the redactions:</span></p><blockquote><p>We share the frustration that much of the data and quotes by Amazon executives in the complaint that describe what we allege is monopolistic and illegal behavior is redacted. Amazon has 14 days from the entry of a temporary sealing order to provide legitimate justification for preventing this information from being revealed. We do not believe that there are compelling reasons to keep much of this information secret from the public.</p></blockquote><p><span>So there we go. This has been an extraordinary week, and it’s only Wednesday. As Axios reporter Ashley Gold </span><a href="https://twitter.com/ashleyrgold/status/1706834666731548783?s=46&amp;t=S_QOAuL7mteaESYmPFua-Q" rel="">put it</a><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png" width="1178" height="396" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:396,&quot;width&quot;:1178,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:98354,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5605c59e-fb80-4fbd-9c37-710db9a4c2e3_1178x396.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Thanks for reading! Your tips make this newsletter what it is, so please send me tips on weird monopolies, stories I’ve missed, or other thoughts. And if you liked this issue of BIG, you can sign up&nbsp;</span><a href="http://email.mg1.substack.com/c/eJxVUMFuwyAM_ZpwjIAmYT34UHXtb0QEnBSNQARmVf5-pN1hkyzberae37PRhEtMOxBmYiVjGp0FZoEradTEXB7nhLhq54FtZfLOaHIxHFtC9LJjDzhxNUs7nKTlnGsthRKDmj-s4dIYO0i2xUyjLtZhMAj4jWmPAZmHB9GWm9OlkfcaqybKFL3H1OYyZdLmqzVxraMn-togcyC5OPNzzT3ve9WKVnS3y5WL4a54d1PdZ9PxdRH_CFiCX946XA4vL7TaGWtdS3C0jxj05NECpYKM3v94Cad9Qwj4zB6JML3Bw74chFCsHrKxcgb4o_8H-RJ1Kg" rel="">here</a><span>&nbsp;for more issues, a newsletter on how to restore fair commerce, innovation, and democracy. And consider becoming a </span><a href="https://email.mg1.substack.com/c/eJxVUMtuwyAQ_JpwtABjEw4ceulvIB6Lg4rBgrUq_31J0kO7Wmk1-xrNeIuw1XZphI7kqB0NXgfoAt89AyI0cnZoJgVNgqaSe-lI6iY2gN2mrMlxupy8xVTLc4uxhQvy0DZKpyjQIL31CqigggOPQYWBIqg3lz1DguJB15Ivc9gUSNYPxKPf5o8b_xy5W8SONWdoUz9dR-u_Jl_3MXpC35IDkjSnnI2Y6cwkZROfhIwuSrXauC7e2_u0rNhtvJaboPvG_v0iTf9SjOH21PXqDmlm1P0sCS8DxboMQWM7geDbsJcHZoMCbRgZjEXN1nmVchaci5W9VQ5bxEzlXVFFBm2o46roP8J-AJJ0hnE" rel="">paying subscriber</a><span> to support this work, or if you are a paying subscriber, giving a </span><a href="https://email.mg1.substack.com/c/eJxVUEtuxSAMPE1YRnwSSBYsKlW9BiJgUlQCETiqcvvy3uuitSxZ9tgezTiLsJd6a4SG5CwNDd4n6AzfLQEiVHI1qCZ6Tbymiju1kdhMqACHjUmT89pSdBZjyY8txmY-kU_tOZOU-bB5urhl9ouc-RomtnoXpOX0xWUvHyE70CWn25w2epL0J-LZBvE28I-eh0VsWFKCOrZra2jd1-jK0aFH62rcYBB9U-4x4CDesV5AouaUsx6CCqYoG_k4qbAFtUob5OycXcZZYrPhnoeJHjv795xU_cvZwf0h9DntWk2vx5Uj3gay3RJ4_STEl4NPU8wOGWp31huLmkkhlRIT55NkL9ndp0lQtax0JZ3Wl36V9R-lPyvjiqk" rel="">gift subscription</a><span> to a friend, colleague, or family member.</span></p><p>cheers,</p><p>Matt Stoller</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Issues with 1.1.1.1 public resolver and WARP (132 pts)]]></title>
            <link>https://www.cloudflarestatus.com/incidents/j3h00yhjyw6p</link>
            <guid>37763143</guid>
            <pubDate>Wed, 04 Oct 2023 10:25:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cloudflarestatus.com/incidents/j3h00yhjyw6p">https://www.cloudflarestatus.com/incidents/j3h00yhjyw6p</a>, See on <a href="https://news.ycombinator.com/item?id=37763143">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <div>
      <p>Issues with 1.1.1.1 public resolver and WARP</p>
      
    </div>

    <div>
      <!-- postmortem if it's published -->

      <!-- incident updates in reverse order -->
        <div>
          <p>
            Resolved
          </p>
          <div>
            <p><span>This incident has been resolved.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1696417257000"></span>Oct <var data-var="date">04</var>, <var data-var="year">2023</var> - <var data-var="time">11:00</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Monitoring
          </p>
          <div>
            <p><span>A fix has been implemented and we are monitoring the results.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1696415803000"></span>Oct <var data-var="date">04</var>, <var data-var="year">2023</var> - <var data-var="time">10:36</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>DNS resolution issues to 1.1.1.1 are reduced but some impact is still present. A fix continues to be implemented.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1696413835000"></span>Oct <var data-var="date">04</var>, <var data-var="year">2023</var> - <var data-var="time">10:03</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Identified
          </p>
          <div>
            <p><span>The issue has been identified and a fix is being implemented.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1696410732000"></span>Oct <var data-var="date">04</var>, <var data-var="year">2023</var> - <var data-var="time">09:12</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Update
          </p>
          <div>
            <p><span>Cloudflare is also aware of increased Pages build failures as a result.</span>
            </p>
            <p>
              Posted <span data-datetime-unix="1696407938000"></span>Oct <var data-var="date">04</var>, <var data-var="year">2023</var> - <var data-var="time">08:25</var> UTC
            </p>
          </div>
        </div>
        <div>
          <p>
            Investigating
          </p>
          <div>
            <p><span>Cloudflare is aware of, and investigating, DNS resolution issues which potentially impacts multiple users using 1.1.1.1 public resolver and/or WARP. <p>Further detail will be provided as more information becomes available.</p></span>
            </p>
            <p>
              Posted <span data-datetime-unix="1696407544000"></span>Oct <var data-var="date">04</var>, <var data-var="year">2023</var> - <var data-var="time">08:19</var> UTC
            </p>
          </div>
        </div>

      <!-- affected components -->
        <p>
          This incident affected: Cloudflare Sites and Services (Pages, Recursive DNS, WARP).
        </p>
    </div>

    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Classic Video Poker (Godot is awesome!) (267 pts)]]></title>
            <link>https://lfgslots.com/classicvideopoker/</link>
            <guid>37763098</guid>
            <pubDate>Wed, 04 Oct 2023 10:18:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lfgslots.com/classicvideopoker/">https://lfgslots.com/classicvideopoker/</a>, See on <a href="https://news.ycombinator.com/item?id=37763098">Hacker News</a></p>
<div id="readability-page-1" class="page">
	<canvas id="canvas">
		HTML5 canvas appears to be unsupported in the current browser.<br>
		Please try updating or use a different browser.
	</canvas>
	

	
	



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kata Containers: The speed of containers, the security of VMs (111 pts)]]></title>
            <link>https://github.com/kata-containers/</link>
            <guid>37762380</guid>
            <pubDate>Wed, 04 Oct 2023 08:24:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kata-containers/">https://github.com/kata-containers/</a>, See on <a href="https://news.ycombinator.com/item?id=37762380">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fc2b272df13c770b08a779c5f96690946039c45998b1bb439eb193b3fcd829ab/68747470733a2f2f7777772e6f70656e737461636b2e6f72672f6173736574732f6b6174612f6b6174612d766572746963616c2d6f6e2d77686974652e706e67"><img src="https://camo.githubusercontent.com/fc2b272df13c770b08a779c5f96690946039c45998b1bb439eb193b3fcd829ab/68747470733a2f2f7777772e6f70656e737461636b2e6f72672f6173736574732f6b6174612f6b6174612d766572746963616c2d6f6e2d77686974652e706e67" width="150" data-canonical-src="https://www.openstack.org/assets/kata/kata-vertical-on-white.png"></a></p>
<h2 id="user-content-welcome-to-kata-containers" dir="auto"><a href="#welcome-to-kata-containers">Welcome to Kata Containers</a></h2>
<p dir="auto">Kata Containers is an open source project and community working to build a
standard implementation of lightweight Virtual Machines (VMs) that feel and
perform like containers, but provide the workload isolation and security
advantages of VMs.</p>
<h2 id="user-content-official-website" dir="auto"><a href="#official-website">Official website</a></h2>
<p dir="auto">The main Kata Containers site is <a href="https://katacontainers.io/" rel="nofollow">https://katacontainers.io</a>.</p>
<h2 id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting started</a></h2>
<p dir="auto">See the
<a href="https://github.com/kata-containers/kata-containers/tree/main/docs/install">installation documentation</a>.</p>
<h2 id="user-content-documentation" dir="auto"><a href="#documentation">Documentation</a></h2>
<p dir="auto">See the
<a href="https://github.com/kata-containers/kata-containers/tree/main/docs">official documentation</a>.</p>
<h2 id="user-content-developers" dir="auto"><a href="#developers">Developers</a></h2>
<p dir="auto">The source code and unit test code for Kata Containers lives in the
<a href="https://github.com/kata-containers/kata-containers">main repository</a>.</p>
<h2 id="user-content-testers" dir="auto"><a href="#testers">Testers</a></h2>
<p dir="auto">The
<a href="https://github.com/kata-containers/tests"><code>tests</code></a>
repository contains the more advanced test code run by the
<a href="http://jenkins.katacontainers.io/" rel="nofollow">Kata Containers Continuous Integration (CI) system</a>.</p>
<p dir="auto">The configuration files for the CI itself live in the
<a href="https://github.com/kata-containers/ci"><code>ci</code></a> repository.</p>
<h2 id="user-content-web-developers" dir="auto"><a href="#web-developers">Web developers</a></h2>
<p dir="auto">The source code for the <a href="#official-website">official website</a> is
<a href="https://github.com/kata-containers/www.katacontainers.io">https://github.com/kata-containers/www.katacontainers.io</a>.</p>
<h2 id="user-content-community" dir="auto"><a href="#community">Community</a></h2>
<p dir="auto">To learn more about the project, its community and governance, see the
<a href="https://github.com/kata-containers/community">community repository</a>. This is
the first place to go if you wish to contribute to the project.</p>
<h2 id="user-content-get-help" dir="auto"><a href="#get-help">Get help</a></h2>
<p dir="auto">See the <a href="#community">community</a> section for ways to contact us.</p>
<h2 id="user-content-get-involved" dir="auto"><a href="#get-involved">Get involved</a></h2>
<p dir="auto">See the <a href="#community">community</a> section for ways to join us.</p>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Enabling IPv6 support for IPv4-only apps on Linux (128 pts)]]></title>
            <link>https://blog.apnic.net/2023/06/21/enabling-ipv6-support-for-ipv4-only-apps-on-linux/</link>
            <guid>37761350</guid>
            <pubDate>Wed, 04 Oct 2023 05:59:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.apnic.net/2023/06/21/enabling-ipv6-support-for-ipv4-only-apps-on-linux/">https://blog.apnic.net/2023/06/21/enabling-ipv6-support-for-ipv4-only-apps-on-linux/</a>, See on <a href="https://news.ycombinator.com/item?id=37761350">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-content">
                            <p><img width="555" height="202" src="https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft-555x202.png?v=3c7ac934af5a6aab58a38abbc2f09528" alt="" decoding="async" fetchpriority="high" srcset="https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft-555x202.png?v=3c7ac934af5a6aab58a38abbc2f09528 555w, https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft-300x109.png?v=3c7ac934af5a6aab58a38abbc2f09528 300w, https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft-1024x373.png?v=3c7ac934af5a6aab58a38abbc2f09528 1024w, https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft-768x280.png?v=3c7ac934af5a6aab58a38abbc2f09528 768w, https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft-624x227.png?v=3c7ac934af5a6aab58a38abbc2f09528 624w, https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft-206x75.png?v=3c7ac934af5a6aab58a38abbc2f09528 206w, https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft-256x93.png?v=3c7ac934af5a6aab58a38abbc2f09528 256w, https://blog.apnic.net/wp-content/uploads/2023/05/green-light-ft.png?v=3c7ac934af5a6aab58a38abbc2f09528 1110w" sizes="(max-width: 555px) 100vw, 555px"></p><p>To prove that IPv6 is ready for production use, I’ve been using an IPv6-only setup on my Ubuntu PC for more than four months. To access the legacy IPv4 Internet, I use a NAT64 gateway based on&nbsp;<a href="http://www.litech.org/tayga/" target="_blank" rel="noreferrer noopener">Tayga</a>&nbsp;deployed on a&nbsp;<a href="https://pine64.com/product-category/rockpro64/" target="_blank" rel="noreferrer noopener">RockPro64</a>&nbsp;SBC in my cupboard:</p>


<div>
<figure><a href="https://blog.apnic.net/wp-content/uploads/2023/05/fig1-2.png"><img decoding="async" width="337" height="212" src="https://blog.apnic.net/wp-content/uploads/2023/05/fig1-2.png" alt="Figure 1 — The RockPro64 is an ARM64-based SBC with a dedicated 1G Ethernet and PCI-E slot." srcset="https://blog.apnic.net/wp-content/uploads/2023/05/fig1-2.png 337w, https://blog.apnic.net/wp-content/uploads/2023/05/fig1-2-300x189.png 300w" sizes="(max-width: 337px) 100vw, 337px"></a><figcaption>Figure 1 — The RockPro64 is an ARM64-based SBC with a dedicated 1G Ethernet and PCI-E slot.</figcaption></figure></div>


<p>Most of the apps I use support IPv6 but there are some cases when a lack of IPv4 connectivity on my machine negatively affects the experience.</p>



<p>One of the cases that affect my daily experience is the constant need to manually prepend IPv4 addresses for SSH as direct attempts to use SSH with an IPv4 address fail:</p>



<pre><code>ssh 1.3.3.7
ssh: connect to host 1.3.3.7 port 22: Network is unreachable</code></pre>



<p>Instead, I have to constantly append my IPv6 NAT64 prefix:</p>



<pre><code>ssh 64:ff9b::1.3.3.7
pavel@64:ff9b::103:307's password:</code></pre>



<p>Luckily, Linux provides an exceptionally easy way to intercept some specific library functions using a simple dynamically linked library: LD_PRELOAD.</p>



<figure><a href="https://blog.apnic.net/wp-content/uploads/2023/05/fig2-2.png"><img decoding="async" width="1024" height="149" src="https://blog.apnic.net/wp-content/uploads/2023/05/fig2-2-1024x149.png" alt="Figure 2 — LD_PRELOAD allows us to override some functions with our own logic." srcset="https://blog.apnic.net/wp-content/uploads/2023/05/fig2-2-1024x149.png 1024w, https://blog.apnic.net/wp-content/uploads/2023/05/fig2-2-300x44.png 300w, https://blog.apnic.net/wp-content/uploads/2023/05/fig2-2-768x112.png 768w, https://blog.apnic.net/wp-content/uploads/2023/05/fig2-2-624x91.png 624w, https://blog.apnic.net/wp-content/uploads/2023/05/fig2-2-1320x192.png 1320w, https://blog.apnic.net/wp-content/uploads/2023/05/fig2-2.png 1333w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Figure 2 — LD_PRELOAD allows us to override some functions with our own logic.</figcaption></figure>



<p>When an app needs to connect to the network it uses Linux system functions such as&nbsp;<a href="https://man7.org/linux/man-pages/man2/socket.2.html" target="_blank" rel="noreferrer noopener">socket</a>(),&nbsp;<a href="https://man7.org/linux/man-pages/man2/connect.2.html?" target="_blank" rel="noreferrer noopener">connect</a>(),&nbsp;<a href="https://man7.org/linux/man-pages/man2/getpeername.2.html" target="_blank" rel="noreferrer noopener">getpeername</a>(),&nbsp;and <a href="https://man7.org/linux/man-pages/man2/getsockname.2.html" target="_blank" rel="noreferrer noopener">getsockname</a>(), and by overriding them with functions that explicitly transform all IPv4 connection attempts to special IPv6 addresses crafted using a NAT64 prefix, we can automatically add IPv6 support for apps with IPv6 support.</p>



<p>Thankfully, that’s exactly how a tool called <a href="https://github.com/andrewshadura/tnat64" target="_blank" rel="noreferrer noopener">TNAT64</a>&nbsp;works. You can find this tool in the Debian or Ubuntu official repositories and installation is easy:</p>



<pre><code>sudo apt install -y tnat64</code></pre>



<p>TNAT64 provides a single dynamic library available on the following path: </p>



<pre><code>/usr/lib/tnat64/libtnat64.so.</code></pre>



<p>To enable it for a specific session in the terminal, we need to run the following commands:</p>



<pre><code>export LD_PRELOAD=/usr/lib/tnat64/libtnat64.so TNAT64_DEBUG=10 TNAT64_DEBUG_FILE=/tmp/tnat64.log</code></pre>



<p>The only mandatory requirement here is that LD_PRELOAD, TNAT64_DEBUG, and TNAT64_DEBUG_FILE enable debugging to provide more information on how this library works.</p>



<p>After that, when you run any app in the same terminal session it will load <tt>libtnat64.so</tt> first. Then all calls to network functions will be intercepted, and as a consequence, any attempt to connect to IPv4 will work just fine even on a machine without external IPv4 connectivity:</p>



<pre><code>ssh 1.3.3.7
pavel@1.3.3.7's password:</code></pre>



<p>I recommend using this approach only for apps with clear IPv6 support issues. Attempts to enable this logic may lead to some issues as some apps may have issues with syscalls.</p>



<p>To get more information about the runtime activity of&nbsp;TNAT64&nbsp;you can check the content of its log file <tt>/tmp/tnat64.log</tt>:</p>



<figure><a href="https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1.png"><img decoding="async" loading="lazy" width="1024" height="259" src="https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1-1024x259.png" alt="Figure 3 —  The TNAT64 log file." srcset="https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1-1024x259.png 1024w, https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1-300x76.png 300w, https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1-768x194.png 768w, https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1-1536x388.png 1536w, https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1-624x158.png 624w, https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1-1320x334.png 1320w, https://blog.apnic.net/wp-content/uploads/2023/05/fig3-1.png 1895w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Figure 3 —  The TNAT64 log file.</figcaption></figure>



<p>Our global current success with IPv6 deployment is the result of a colossal amount of work over the last two decades from an incredible number of very talented engineers. I’d like to acknowledge the author of TNAT64, <a href="https://shadura.me/" target="_blank" rel="noreferrer noopener">Andrej Shadura</a>, which was written around 2011.</p>



<p><em>Pavel Odintsov is a software engineer with a passion for computer networks, having worked in domain registry, cloud hosting, Internet exchange, and global cyber security. He is the technical lead of the <a href="https://fastnetmon.com/" target="_blank" rel="noreferrer noopener">FastNetMon</a> project.</em></p>



<p><em>Adapted from the original at <a href="https://pavel.network/enabling-ipv6-support-for-ipv4-only-apps-on-linux/" target="_blank" rel="noreferrer noopener">Pavel’s blog</a>.</em></p>

                            <!-- DISCUSS ON HN BUTTON: START -->
                            
                                                        <hr>

                            <p id="views-disclaimer">The views expressed by the authors of this blog are their own
                                and do not necessarily reflect the views of APNIC. Please note a <a href="https://blog.apnic.net/?p=395">Code of Conduct</a> applies to this blog.
                            </p>
                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Extracting Hacker News Book Recommendations with the ChatGPT API (258 pts)]]></title>
            <link>https://blog.reyem.dev/post/extracting_hn_book_recommendations_with_chatgpt_api/</link>
            <guid>37761273</guid>
            <pubDate>Wed, 04 Oct 2023 05:48:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.reyem.dev/post/extracting_hn_book_recommendations_with_chatgpt_api/">https://blog.reyem.dev/post/extracting_hn_book_recommendations_with_chatgpt_api/</a>, See on <a href="https://news.ycombinator.com/item?id=37761273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			
<main role="main">
	<article>
		<div>
			<p>I love books and I enjoy reading through the Hacker News(<a href="https://news.ycombinator.com/">HN</a>) book recommendation threads.  On HN, there’s almost 200 stories so far this year that have the separate word “book” in the title, and aren’t linked to another page.  I wondered what the most commonly recommended or mention books are.  Mainly wondering if <a href="https://web.mit.edu/6.001/6.037/sicp.pdf">SICP</a> or <a href="https://gigamonkeys.com/book/">PCL</a> would be the top recommendation.</p>
<p>After reading of the man who <a href="https://news.ycombinator.com/item?id=35073603">categorised his favourite podcast into dewey decimal using GPT</a>, I was aware that the GPT API could be used to categorise data and output the information in json format.  So using the HN data fetched from the hackernews API, I used the subset of stories that seem to be book recommendation threads and extracted book titles, authors and urls from the text using calls to the Chat Completions API.</p>
<p>Here’s the top 50 book recommendations:</p>
<table>
<thead>
<tr>
<th>#</th>
<th>Title</th>
<th>Author</th>
<th>Count</th>
<th>First Mention</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><a href="https://web.mit.edu/6.001/6.037/sicp.pdf">Structure and Interpretation of Computer Programs</a></td>
<td>Abelson and Sussman</td>
<td>376</td>
<td><a href="https://news.ycombinator.com/item?id=5675">5675</a></td>
</tr>
<tr>
<td>2</td>
<td><a href="https://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/ref=sr_1_1?keywords=godel+escher+bach&amp;qid=1680707058&amp;sr=8-1?tag=reyemdev0f-20">Gödel, Escher, Bach</a></td>
<td>Douglas Hofstadter</td>
<td>293</td>
<td><a href="https://news.ycombinator.com/item?id=56795">56795</a></td>
</tr>
<tr>
<td>3</td>
<td><a href="https://www.amazon.com/How-Win-Friends-Influence-People-ebook/dp/B0811Z2NNF/?tag=reyemdev0f-20">How to Win Friends and Influence People</a></td>
<td>Dale Carnegie</td>
<td>292</td>
<td><a href="https://news.ycombinator.com/item?id=5584">5584</a></td>
</tr>
<tr>
<td>4</td>
<td><a href="https://www.amazon.com/Programming-Language-2nd-Brian-Kernighan/dp/0131103628?tag=reyemdev0f-20">The C Programming Language</a></td>
<td>Brian Kernighan, Dennis Ritchie</td>
<td>284</td>
<td><a href="https://news.ycombinator.com/item?id=135262">135262</a></td>
</tr>
<tr>
<td>5</td>
<td><a href="https://www.amazon.com/dp/0441172717/?tag=reyemdev0f-20">Dune</a></td>
<td>Brian Herbert</td>
<td>263</td>
<td><a href="https://news.ycombinator.com/item?id=57231">57231</a></td>
</tr>
<tr>
<td>6</td>
<td><a href="https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555?tag=reyemdev0f-20">Thinking, Fast and Slow</a></td>
<td>Daniel Kahneman</td>
<td>244</td>
<td><a href="https://news.ycombinator.com/item?id=3277457">3277457</a></td>
</tr>
<tr>
<td>7</td>
<td><a href="https://standardebooks.org/ebooks/marcus-aurelius/meditations/george-long">Meditations</a></td>
<td>Descartes</td>
<td>233</td>
<td><a href="https://news.ycombinator.com/item?id=134993">134993</a></td>
</tr>
<tr>
<td>8</td>
<td><a href="https://www.amazon.com/Atlas-Shrugged-Centennial-AYN-RAND/dp/B0027M0HV6?tag=reyemdev0f-20">Atlas Shrugged</a></td>
<td>Ayn Rand</td>
<td>222</td>
<td><a href="https://news.ycombinator.com/item?id=86114">86114</a></td>
</tr>
<tr>
<td>9</td>
<td><a href="https://www.amazon.com/Art-Computer-Programming-Combinatorial-Information/dp/0201038064?tag=reyemdev0f-20">The Art of Computer Programming</a></td>
<td>Donald E. Knuth</td>
<td>213</td>
<td><a href="https://news.ycombinator.com/item?id=135245">135245</a></td>
</tr>
<tr>
<td>10</td>
<td><a href="https://www.audible.in/pd/Sapiens-Audiobook/B079C16D4D">Sapiens: A Brief History of Humankind</a></td>
<td>Yuval Harari</td>
<td>205</td>
<td><a href="https://news.ycombinator.com/item?id=10028239">10028239</a></td>
</tr>
<tr>
<td>11</td>
<td><a href="https://www.amazon.com/Zen-Art-Motorcycle-Maintenance-Inquiry/dp/0060589469?tag=reyemdev0f-20">Zen and the Art of Motorcycle Maintenance</a></td>
<td>Robert M Pirsig</td>
<td>203</td>
<td><a href="https://news.ycombinator.com/item?id=56941">56941</a></td>
</tr>
<tr>
<td>12</td>
<td><a href="https://www.amazon.co.uk/Pragmatic-Programmer-Andrew-Hunt/dp/020161622X">The Pragmatic Programmer</a></td>
<td>Andrew Hunt</td>
<td>203</td>
<td><a href="https://news.ycombinator.com/item?id=5704">5704</a></td>
</tr>
<tr>
<td>13</td>
<td><a href="https://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844?tag=reyemdev0f-20">Introduction to Algorithms</a></td>
<td>Charles E. Leiserson, Clifford Stein, Ronald Rivest, Thomas H. Cormen</td>
<td>171</td>
<td><a href="https://news.ycombinator.com/item?id=55391">55391</a></td>
</tr>
<tr>
<td>14</td>
<td><a href="https://www.amazon.com/Selfish-Gene-Anniversary-Landmark-Paperback/dp/B0722G5V92?tag=reyemdev0f-20">The Selfish Gene</a></td>
<td>Richard Dawkins</td>
<td>168</td>
<td><a href="https://news.ycombinator.com/item?id=85867">85867</a></td>
</tr>
<tr>
<td>15</td>
<td><a href="https://www.codehiddenlanguage.com/">Code: The Hidden Language of Computer Hardware and Software</a></td>
<td>Charles Petzold</td>
<td>160</td>
<td><a href="https://news.ycombinator.com/item?id=135906">135906</a></td>
</tr>
<tr>
<td>16</td>
<td><a href="https://www.amazon.com/Mythical-Man-Month-Software-Engineering-Anniversary-ebook/dp/B000OZ0N6M?tag=reyemdev0f-20">The Mythical Man-Month</a></td>
<td>Fred Brooks</td>
<td>159</td>
<td><a href="https://news.ycombinator.com/item?id=5725">5725</a></td>
</tr>
<tr>
<td>17</td>
<td><a href="https://www.amazon.com/Black-Swan-Improbable-Robustness-Fragility/dp/081297381X?tag=reyemdev0f-20">The Black Swan</a></td>
<td>Nassim Nicholas Taleb</td>
<td>158</td>
<td><a href="https://news.ycombinator.com/item?id=56763">56763</a></td>
</tr>
<tr>
<td>18</td>
<td><a href="https://dataintensive.net/">Designing Data-Intensive Applications</a></td>
<td>Martin Kleppman</td>
<td>153</td>
<td><a href="https://news.ycombinator.com/item?id=8671875">8671875</a></td>
</tr>
<tr>
<td>19</td>
<td><a href="https://www.amazon.com/o/ASIN/0451524934?tag=reyemdev0f-20">1984</a></td>
<td>George Orwell</td>
<td>152</td>
<td><a href="https://news.ycombinator.com/item?id=85938">85938</a></td>
</tr>
<tr>
<td>20</td>
<td><a href="https://stevemcconnell.com/books/">Code Complete</a></td>
<td>Steve McConnell</td>
<td>149</td>
<td><a href="https://news.ycombinator.com/item?id=56709">56709</a></td>
</tr>
<tr>
<td>21</td>
<td><a href="https://www.amazon.com/Snow-Crash-Neal-Stephenson/dp/0553380958/?tag=reyemdev0f-20">Snow Crash</a></td>
<td>Neal Stephenson</td>
<td>146</td>
<td><a href="https://news.ycombinator.com/item?id=85862">85862</a></td>
</tr>
<tr>
<td>22</td>
<td><a href="https://www.amazon.com/dp/B074CF4JFZ?searchxofy=true&amp;binding=kindle_edition&amp;qid=1645177978&amp;sr=1-1?tag=reyemdev0f-20">The Three-Body Problem</a></td>
<td>Cixin Liu</td>
<td>143</td>
<td><a href="https://news.ycombinator.com/item?id=8867599">8867599</a></td>
</tr>
<tr>
<td>23</td>
<td><a href="https://www.amazon.com/gp/product/0812550706/ref=as_li_ss_tl?ie=UTF8&amp;tag=xlracom-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399369&amp;creativeASIN=0812550706?tag=reyemdev0f-20">Ender’s Game</a></td>
<td>Orson Scott Card</td>
<td>143</td>
<td><a href="https://news.ycombinator.com/item?id=56704">56704</a></td>
</tr>
<tr>
<td>24</td>
<td><a href="https://www.amazon.com/Design-Everyday-Things-Revised-Expanded/dp/0465050654?tag=reyemdev0f-20">The Design of Everyday Things</a></td>
<td>Don Norman</td>
<td>136</td>
<td><a href="https://news.ycombinator.com/item?id=85860">85860</a></td>
</tr>
<tr>
<td>25</td>
<td><a href="https://en.wiktionary.org/wiki/Bible">Bible</a></td>
<td>Unknown</td>
<td>134</td>
<td><a href="https://news.ycombinator.com/item?id=85859">85859</a></td>
</tr>
<tr>
<td>26</td>
<td><a href="https://www.amazon.com/Founders-Work-Stories-Startups-Early/dp/B092RC56HW?tag=reyemdev0f-20">Founders at Work</a></td>
<td>Jessica Livingston</td>
<td>133</td>
<td><a href="https://news.ycombinator.com/item?id=5613">5613</a></td>
</tr>
<tr>
<td>27</td>
<td><a href="https://www.amazon.com/Antifragile-Things-That-Gain-Disorder/dp/0812979680?tag=reyemdev0f-20">Antifragile</a></td>
<td>Nassim Nicholas Taleb</td>
<td>130</td>
<td><a href="https://news.ycombinator.com/item?id=4966437">4966437</a></td>
</tr>
<tr>
<td>28</td>
<td><a href="https://existentialstoic.wordpress.com/2018/02/18/victor-frankls-disturbing-book-mans-search-for-meaning-has-no-meaning/">Man’s Search for Meaning</a></td>
<td>Victor E. Frankl</td>
<td>129</td>
<td><a href="https://news.ycombinator.com/item?id=1634144">1634144</a></td>
</tr>
<tr>
<td>29</td>
<td><a href="https://www.amazon.com/Hitchhikers-Guide-Vms-Unsupported-Undocumented-Can-Go-Away-At-Any-Time-Feature/dp/1878956000?tag=reyemdev0f-20">The Hitchhiker’s Guide to the Galaxy</a></td>
<td>Douglas Adams</td>
<td>128</td>
<td><a href="https://news.ycombinator.com/item?id=56709">56709</a></td>
</tr>
<tr>
<td>30</td>
<td><a href="https://www.amazon.com/Cryptonomicon-Neal-Stephenson/dp/B004R96U4A/?tag=reyemdev0f-20">Cryptonomicon</a></td>
<td>Neal Stephenson</td>
<td>127</td>
<td><a href="https://news.ycombinator.com/item?id=85940">85940</a></td>
</tr>
<tr>
<td>31</td>
<td><a href="https://www.amazon.com/Fountainhead-Ayn-Rand/dp/0452273331?tag=reyemdev0f-20">The Fountainhead</a></td>
<td>Ayn Rand</td>
<td>127</td>
<td><a href="https://news.ycombinator.com/item?id=135463">135463</a></td>
</tr>
<tr>
<td>32</td>
<td><a href="https://jamesclear.com/feynman-mental-models">Surely You’re Joking, Mr. Feynman!</a></td>
<td>Richard Feynman</td>
<td>125</td>
<td><a href="https://news.ycombinator.com/item?id=85858">85858</a></td>
</tr>
<tr>
<td>33</td>
<td><a href="https://tylerdevries.com/book-summaries/fooled-by-randomness/">Fooled by Randomness</a></td>
<td>Nassim Nicholas Taleb</td>
<td>125</td>
<td><a href="https://news.ycombinator.com/item?id=57595">57595</a></td>
</tr>
<tr>
<td>34</td>
<td><a href="https://www.amazon.com/Siddhartha-Novel-Hermann-Hesse/dp/0553208845?crid=3SYWBWJQ84BL2&amp;dchild=1&amp;keywords=sidhartha+book+hermann+hesse&amp;qid=1613238520&amp;sprefix=sidhar%2Caps%2C426&amp;sr=8-1&amp;linkCode=ll1&amp;tag=amazonpurch08-20&amp;linkId=abdf00b582fcff391d25816a2b1d28b3&amp;language=en_US&amp;ref_=as_li_ss_tl?tag=reyemdev0f-20">Siddhartha</a></td>
<td>Herman Hesse</td>
<td>124</td>
<td><a href="https://news.ycombinator.com/item?id=86337">86337</a></td>
</tr>
<tr>
<td>35</td>
<td><a href="https://www.youtube.com/playlist?list=PLsO8fxO6PnRfOwK6j8g8X5f6XZV9Kv3h-">Foundation</a></td>
<td>Isaac Asimov</td>
<td>123</td>
<td><a href="https://news.ycombinator.com/item?id=140379">140379</a></td>
</tr>
<tr>
<td>36</td>
<td><a href="https://www.amazon.com/J-R-R-Tolkien-4-Book-Boxed-Set/dp/0345538374?tag=reyemdev0f-20">The Lord of the Rings</a></td>
<td>J. R. R. Tolkien</td>
<td>121</td>
<td><a href="https://news.ycombinator.com/item?id=56629">56629</a></td>
</tr>
<tr>
<td>37</td>
<td><a href="https://www.amazon.com/Zero-One-Notes-Startups-Future/dp/0804139296?tag=reyemdev0f-20">Zero to One</a></td>
<td>Peter Thiel</td>
<td>115</td>
<td><a href="https://news.ycombinator.com/item?id=7968392">7968392</a></td>
</tr>
<tr>
<td>38</td>
<td><a href="https://www.amazon.com/Calculus-4th-Michael-Spivak/dp/0914098918?tag=reyemdev0f-20">Calculus</a></td>
<td>Charles B. Morrey Jr., Murray H. Protter</td>
<td>114</td>
<td><a href="https://news.ycombinator.com/item?id=193554">193554</a></td>
</tr>
<tr>
<td>39</td>
<td><a href="https://www.bearcave.com/bookrev/neuromancer/neuromancer_audio.html">Neuromancer</a></td>
<td>William Gibson</td>
<td>112</td>
<td><a href="https://news.ycombinator.com/item?id=56663">56663</a></td>
</tr>
<tr>
<td>40</td>
<td><a href="https://www.amazon.com/Phoenix-Project-DevOps-Helping-Business-ebook/dp/B078Y98RG8/ref=sr_1_1?keywords=phoenix+project&amp;qid=1577699403&amp;sr=8-1?tag=reyemdev0f-20">The Phoenix Project</a></td>
<td>Gene Kim</td>
<td>110</td>
<td><a href="https://news.ycombinator.com/item?id=5569687">5569687</a></td>
</tr>
<tr>
<td>41</td>
<td><a href="https://www.amazon.com/stores/page/7509FE4F-9888-4384-ACBC-D5B12AA08563?tag=reyemdev0f-20">The Lean Startup</a></td>
<td>Eric Ries</td>
<td>110</td>
<td><a href="https://news.ycombinator.com/item?id=1570888">1570888</a></td>
</tr>
<tr>
<td>42</td>
<td><a href="https://www.amazon.com/dp/0062407805?tag=reyemdev0f-20">Never Split the Difference</a></td>
<td>Chris Voss</td>
<td>108</td>
<td><a href="https://news.ycombinator.com/item?id=12245967">12245967</a></td>
</tr>
<tr>
<td>43</td>
<td><a href="https://geni.us/GQSU">Design Patterns</a></td>
<td>Addy Osmani</td>
<td>107</td>
<td><a href="https://news.ycombinator.com/item?id=80916">80916</a></td>
</tr>
<tr>
<td>44</td>
<td><a href="https://www.amazon.com/Guns-Germs-Steel-Fates-Societies/dp/0393354326/?tag=reyemdev0f-20">Guns, Germs, and Steel</a></td>
<td>Jared Diamond</td>
<td>107</td>
<td><a href="https://news.ycombinator.com/item?id=56777">56777</a></td>
</tr>
<tr>
<td>45</td>
<td><a href="https://bdcampbell.net/javascript/book/javascript_the_good_parts.pdf">JavaScript: The Good Parts</a></td>
<td>Douglas Crockford</td>
<td>106</td>
<td><a href="https://news.ycombinator.com/item?id=259986">259986</a></td>
</tr>
<tr>
<td>46</td>
<td><a href="https://www.oreilly.com/library/view/clean-code-a/9780136083238/">Clean Code</a></td>
<td>Robert C. Martin</td>
<td>106</td>
<td><a href="https://news.ycombinator.com/item?id=1945860">1945860</a></td>
</tr>
<tr>
<td>47</td>
<td><a href="https://www.amazon.com/Deep-Work-Focused-Success-Distracted/dp/1455586692?dchild=1&amp;keywords=deep+work&amp;qid=1613237992&amp;sr=8-1&amp;linkCode=ll1&amp;tag=amazonpurch08-20&amp;linkId=ade63608d1ce0978e4ccca7eb10158ea&amp;language=en_US&amp;ref_=as_li_ss_tl?tag=reyemdev0f-20">Deep Work</a></td>
<td>Cal Newport</td>
<td>105</td>
<td><a href="https://news.ycombinator.com/item?id=11702897">11702897</a></td>
</tr>
<tr>
<td>48</td>
<td><a href="https://mitpress.mit.edu/9780262140874/">The Elements of Computing Systems</a></td>
<td>Noam Nisan, Shimon Schocken</td>
<td>104</td>
<td><a href="https://news.ycombinator.com/item?id=1295307">1295307</a></td>
</tr>
<tr>
<td>49</td>
<td><a href="https://mitpress.mit.edu/9780262560993/the-little-schemer/">The Little Schemer</a></td>
<td>Daniel P. Friedman, Matthias Felleisen</td>
<td>102</td>
<td><a href="https://news.ycombinator.com/item?id=56629">56629</a></td>
</tr>
<tr>
<td>50</td>
<td><a href="https://www.amazon.com/Influence-Psychology-Persuasion-Robert-Cialdini/dp/006124189X?tag=reyemdev0f-20">Influence: The Psychology of Persuasion</a></td>
<td>Robert B. Cialdini</td>
<td>101</td>
<td><a href="https://news.ycombinator.com/item?id=193848">193848</a></td>
</tr>
</tbody>
</table>
<p>Some things I discovered while doing this project:</p>
<ul>
<li>When the API doesn’t return valid JSON, usually this is when chatGPT is saying things
like “I apologize for the confusion…” or “You’re welcome! If you have any more questions, feel free to ask.”, in response to a HN comment that just says “thanks” or asks a question.</li>
<li>Designed the prompt so that I can discard responses with empty titles.  This is because I was unable to get chatgpt to stop including mentions of Authors without a title of a particular book.</li>
<li>Processing 57k comments cost about $40 using gpt 3.5 turbo API.</li>
<li>Even with a temperature of 0, GPT’s results vary from call to call.  <a href="https://152334h.github.io/blog/non-determinism-in-gpt-4/">Others have noticed this effect</a>
<a href="https://news.ycombinator.com/item?id=37006224">(HN discussion)</a>, it’s not just GPT-4 that is non-deterministic - GPT 3.5 turbo exhibits greater variability
compared to earlier GPT-3 models.</li>
<li>It can identify links from the text, but I had to remove the html <!-- raw HTML omitted --> tag and just leave the url otherwise GPT would pick up the truncated link text instead of the URL.</li>
</ul>
<p>Here’s an example of the json output by chatgpt, for <a href="https://news.ycombinator.com/item?id=80984">this comment</a>, it got everything wrong except the link, but it shows the format of the data:</p>
<div><pre><code data-lang="json">[
  {
    <span>"match"</span>: <span>"Hitchhiker's Guide Vms Unsupported Undocumented Can Go Away At Any Time Feature"</span>,
    <span>"title"</span>: <span>"The Hitchhiker's Guide to the Galaxy"</span>,
    <span>"author"</span>: <span>"Douglas Adams"</span>,
    <span>"link"</span>: <span>"http://www.amazon.com/Hitchhikers-Guide-Vms-Unsupported-Undocumented-Can-Go-Away-At-Any-Time-Feature/dp/1878956000"</span>
  }
]
</code></pre></div><p>Edit: someone has asked for the prompt, here it is:</p>
<div><pre><code data-lang="python">prompt <span>=</span> [
            {<span>"role"</span>: <span>"system"</span>, <span>"content"</span>: <span>"Assistant that identifies book titles and authors in the following document and shows the words you match to a book title from. Some titles may be abbreviated, please expand the abbreviated title. If the document talks about an author but doesn't mention a book, leave </span><span>\"</span><span>title</span><span>\"</span><span> blank. If you know who the author is, provide the author. Don't include the book's subtitle. If the text is asking for a recommendation, without mentioning a book, then return an empty array. Provide your answer in a json array."</span>}
            {<span>"role"</span>: <span>"user"</span>, <span>"content"</span>: <span>'Wren</span><span>\'</span><span>s Explosion https://www.amazon.com/gp/395, and any Plath."'</span>},
            {<span>"role"</span>: <span>"assistant"</span>, <span>"content"</span>: <span>'''[{"match":"'Wren's Explosion","title":"Explosion","author":"P.C. Wren","link":"https://www.amazon.com/gp/395"}, {"match":"any Plath","title":"", "author":"Sylvia Plath"}]'''</span>},
            {<span>"role"</span>:<span>"user"</span>, <span>"content"</span>:<span>"3-days free trial isn't freemium."</span>},
            {<span>"role"</span>:<span>"assistant"</span>, <span>"content"</span>:<span>"[]"</span>},
            {<span>"role"</span>:<span>"user"</span>, <span>"content"</span>: <span>"Miranda Hamilton"</span>},
            {<span>"role"</span>:<span>"assistant"</span>, <span>"content"</span>: <span>'''[{"match":"Miranda Hamilton","title":"Hamilton","author":"Lin-Manuel Miranda, Jeremy McCarter","link":""}]'''</span>},
        ]
</code></pre></div><h3 id="the-data">The Data</h3>
<p>Because I enjoy working with data and think you might find it interesting to analyse the results, here’s the <a href="https://blog.reyem.dev/code/bookdata.zip">raw data produced by GPT</a>
, sorted by title.  Note there’s a match column in there which includes an excerpt
of the comment where the book was identified.  I also normalised the book titles, lowercasing and removing ‘the’ if present at the start, and removed any subtitles.  This enabled me to query the top books without missing too many items due to inconsistence in the names that gpt came up with.</p>
<p>Here is the input <a href="https://blog.reyem.dev/code/inputs.zip">data in zipped csv format</a>, it expands out to a 24 MB file.</p>
<h4 id="note">Note</h4>
<p>I have added an amazon affiliate links to amazon urls in the tables above, mainly as a learning exercise.</p>

		</div>
		
	</article>
</main>






			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google User Data Is Police's Top Shortcut for Solving Crimes (128 pts)]]></title>
            <link>https://www.bloomberg.com/news/features/2023-09-28/google-user-data-is-police-s-top-shortcut-for-solving-crimes</link>
            <guid>37760486</guid>
            <pubDate>Wed, 04 Oct 2023 03:22:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/features/2023-09-28/google-user-data-is-police-s-top-shortcut-for-solving-crimes">https://www.bloomberg.com/news/features/2023-09-28/google-user-data-is-police-s-top-shortcut-for-solving-crimes</a>, See on <a href="https://news.ycombinator.com/item?id=37760486">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ben Fry Resigns from the Processing Foundation (168 pts)]]></title>
            <link>https://twitter.com/ben_fry/status/1709400641456501020</link>
            <guid>37760363</guid>
            <pubDate>Wed, 04 Oct 2023 03:02:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/ben_fry/status/1709400641456501020">https://twitter.com/ben_fry/status/1709400641456501020</a>, See on <a href="https://news.ycombinator.com/item?id=37760363">Hacker News</a></p>
Couldn't get https://twitter.com/ben_fry/status/1709400641456501020: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[RT-X: the largest open-source robot dataset (120 pts)]]></title>
            <link>https://robotics-transformer-x.github.io/</link>
            <guid>37760130</guid>
            <pubDate>Wed, 04 Oct 2023 02:24:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://robotics-transformer-x.github.io/">https://robotics-transformer-x.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=37760130">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
        <p><strong>
        <div>
                <br>
                <!-- mail logo from font awesome --><p>
                Open X-Embodiment Collaboration
                <br>
                <b><i> Authors listed in alphabetical order. </i></b></p><p>
                For any inquiries, please email <a href="mailto:open-x-embodiment@googlegroups.com">open-x-embodiment@googlegroups.com</a></p><ul>
                    <br>
                    
                    <li data-affiliation="Google DeepMind">Acorn Pooley</li>
                    <li data-affiliation="Intrinsic LLC">Ajinkya Jain</li>
                    <li data-affiliation="Google DeepMind">Alex Bewley</li>
                    <li data-affiliation="Google DeepMind">Alex Herzog</li>
                    <li data-affiliation="Google DeepMind">Alex Irpan</li>
                    <li data-affiliation="Stanford University">Alexander Khazatsky</li>
                    <br>
                    <li data-affiliation="New York University">Anant Rai</li>
                    <li data-affiliation="Google DeepMind, University of California, Berkeley">Anikait
                        Singh</li>
                    <li data-affiliation="Google DeepMind">Anthony Brohan</li>
                    <li data-affiliation="German Aerospace Center">Antonin Raffin</li>
                    <li data-affiliation="Google DeepMind">Ayzaan Wahid</li>
                    <li data-affiliation="Queensland University of Technology">Ben Burgess-Limerick</li>
                    <li data-affiliation="Korea Advanced Institute of Science &amp; Technology">Beomjoon Kim
                    </li>
                    <br>
                    <li data-affiliation="Max Planck Institutes">Bernhard Schölkopf</li>
                    <li data-affiliation="Google DeepMind">Brian Ichter</li>
                    <li data-affiliation="Shanghai Jiao Tong University, Flexiv Robotics">Cewu Lu</li>
                    <li data-affiliation="University of California, Berkeley">Charles Xu</li>
                    <li data-affiliation="Google DeepMind, Stanford University">Chelsea Finn</li>
                    <li data-affiliation="University of California, Berkeley">Chenfeng Xu</li>
                    <li data-affiliation="Columbia University">Cheng Chi</li>

                    <br>
                    <li data-affiliation="University of Freiburg">Chenguang Huang</li>
                    <li data-affiliation="Google DeepMind">Christine Chan</li>
                    <li data-affiliation="Columbia University">Chuer Pan</li>
                    <li data-affiliation="Google DeepMind">Chuyuan Fu</li>
                    <li data-affiliation="Google DeepMind">Coline Devin</li>
                    <li data-affiliation="Google DeepMind">Danny Driess</li>
                    <li data-affiliation="Carnegie Mellon University">Deepak Pathak</li>
                    <li data-affiliation="University of California, Berkeley">Dhruv Shah</li>

                    <br>
                    <li data-affiliation="Max Planck Institutes">Dieter Büchler</li>
                    <li data-affiliation="Google DeepMind">Dmitry Kalashnikov</li>
                    <li data-affiliation="Google DeepMind">Dorsa Sadigh</li>
                    <li data-affiliation="Imperial College London">Edward Johns</li>
                    <li data-affiliation="Istituto Italiano di Tecnologia">Federico Ceola</li>
                    <li data-affiliation="Google DeepMind">Fei Xia</li>
                    <li data-affiliation="German Aerospace Center">Freek Stulp</li>

                    <br>
                    <li data-affiliation="Carnegie Mellon University">Gaoyue Zhou</li>
                    <li data-affiliation="University of Southern California">Gaurav S. Sukhatme</li>
                    <li data-affiliation="University of Southern California">Gautam Salhotra</li>
                    <li data-affiliation="University of California, San Diego">Ge Yan</li>
                    <li data-affiliation="ETH Zurich">Giulio Schiavi</li>
                    <li data-affiliation="University of California, San Diego">Hao Su</li>
                    <li data-affiliation="Shanghai Jiao Tong University">Hao-Shu Fang</li>

                    <br>
                    <li data-affiliation="Stanford University">Haochen Shi</li>
                    <li data-affiliation="Arizona State University">Heni Ben Amor</li>
                    <li data-affiliation="University of California, San Diego">Henrik I Christensen</li>
                    <li data-affiliation="The University of Tokyo">Hiroki Furuta</li>
                    <li data-affiliation="University of California, Berkeley">Homer Walke</li>
                    <li data-affiliation="Shanghai Jiao Tong University">Hongjie Fang</li>
                    <li data-affiliation="Google DeepMind">Igor Mordatch</li>

                    <br>
                    <li data-affiliation="University of California, Berkeley">Ilija Radosavovic</li>
                    <li data-affiliation="Google DeepMind">Isabel Leal</li>
                    <li data-affiliation="Google DeepMind">Jacky Liang</li>
                    <li data-affiliation="Korea Advanced Institute of Science &amp; Technology">Jaehyung Kim
                    </li>
                    <li data-affiliation="Max Planck Institutes">Jan Schneider</li>
                    <li data-affiliation="Google DeepMind">Jasmine Hsu</li>
                    <li data-affiliation="Stanford University">Jeannette Bohg</li>

                    <br>
                    <li data-affiliation="Google DeepMind">Jeffrey Bingham</li>
                    <li data-affiliation="Stanford University">Jiajun Wu</li>
                    <li data-affiliation="Google Research">Jialin Wu</li>
                    <li data-affiliation="University of California, Berkeley">Jianlan Luo</li>
                    <li data-affiliation="University of California, San Diego">Jiayuan Gu</li>
                    <li data-affiliation="Google DeepMind">Jie Tan</li>
                    <li data-affiliation="The University of Tokyo">Jihoon Oh</li>



                    <br>
                    <li data-affiliation="University of California, Berkeley">Jitendra Malik</li>
                    <li data-affiliation="Google DeepMind">Jonathan Tompson</li>
                    <li data-affiliation="Stanford University">Jonathan Yang</li>
                    <li data-affiliation="Korea Advanced Institute of Science &amp; Technology">Joseph J. Lim
                    </li>
                    <li data-affiliation="German Aerospace Center">João Silvério</li>
                    <li data-affiliation="Korea Advanced Institute of Science &amp; Technology">Junhyek Han
                    </li>
                    <li data-affiliation="Google DeepMind">Kanishka Rao</li>
                    <br>
                    <li data-affiliation="University of California, Berkeley, Stanford University">Karl Pertsch</li>
                    <li data-affiliation="Google DeepMind">Karol Hausman</li>
                    <li data-affiliation="Intrinsic LLC">Keegan Go</li>
                    <li data-affiliation="Google DeepMind">Keerthana Gopalakrishnan</li>
                    <li data-affiliation="University of California, Berkeley">Ken Goldberg</li>
                    <li data-affiliation="Google DeepMind">Kendra Byrne</li>
                    <li data-affiliation="Google DeepMind">Kenneth Oslund</li>
                    <br>
                    <li data-affiliation="The University of Tokyo">Kento Kawaharazuka</li>
                    <li data-affiliation="Carnegie Mellon University">Kevin Zhang</li>
                    <li data-affiliation="Arizona State University">Keyvan Majd</li>
                    <li data-affiliation="Queensland University of Technology">Krishan Rana</li>
                    <li data-affiliation="Stanford University">Krishnan Srinivasan</li>
                    <li data-affiliation="University of California, Berkeley">Lawrence Yunliang Chen</li>
                    <li data-affiliation="New York University">Lerrel Pinto</li>
                    <br>
                    <li data-affiliation="University of California, Berkeley">Liam Tan</li>
                    <li data-affiliation="ETH Zurich">Lionel Ott</li>
                    <li data-affiliation="Google DeepMind">Lisa Lee</li>
                    <li data-affiliation="University of California, Berkeley">Masayoshi Tomizuka</li>
                    <li data-affiliation="Stanford University">Maximilian Du</li>
                    <li data-affiliation="Google DeepMind">Michael Ahn</li>
                    <li data-affiliation="University of Illinois Urbana-Champaign">Mingtong Zhang</li>
                    <br>
                    <li data-affiliation="University of California, Berkeley">Mingyu Ding</li>
                    <li data-affiliation="Carnegie Mellon University">Mohan Kumar Srirama</li>
                    <li data-affiliation="Carnegie Mellon University">Mohit Sharma</li>
                    <li data-affiliation="Stanford University">Moo Jin Kim</li>
                    <li data-affiliation="The University of Tokyo">Naoaki Kanazawa</li>
                    <li data-affiliation="University of California, San Diego">Nicklas Hansen</li>
                    <li data-affiliation="Google DeepMind">Nicolas Heess</li>
                    <br>
                    <li data-affiliation="Google DeepMind">Nikhil J Joshi</li>
                    <li data-affiliation="Queensland University of Technology">Niko Suenderhauf</li>
                    <li data-affiliation="Imperial College London">Norman Di Palo</li>
                    <li data-affiliation="New York University">Nur Muhammad Mahi Shafiullah</li>
                    <li data-affiliation="University of Freiburg">Oier Mees</li>
                    <li data-affiliation="Carnegie Mellon University">Oliver Kroemer</li>
                    <li data-affiliation="Google DeepMind">Pannag R Sanketi</li>


                    <br>
                    <li data-affiliation="Google DeepMind">Paul Wohlhart</li>
                    <li data-affiliation="Google DeepMind">Peng Xu</li>
                    <li data-affiliation="Google DeepMind">Pierre Sermanet</li>
                    <li data-affiliation="Stanford University">Priya Sundaresan</li>
                    <li data-affiliation="Google DeepMind">Quan Vuong</li>
                    <li data-affiliation="Google DeepMind, Stanford University">Rafael Rafailov</li>
                    <li data-affiliation="University of California, Berkeley">Ran Tian</li>

                    <br>
                    <li data-affiliation="University of California, Berkeley">Ria Doshi</li>
                    <li data-affiliation="The University of Texas at Austin">Roberto Martín-Martín</li>
                    <li data-affiliation="Carnegie Mellon University">Russell Mendonca</li>
                    <li data-affiliation="The University of Texas at Austin">Rutav Shah</li>
                    <li data-affiliation="University of California, Berkeley">Ryan Hoque</li>
                    <li data-affiliation="Google DeepMind">Ryan Julian</li>
                    <li data-affiliation="German Aerospace Center">Samuel Bustamante</li>

                    <br>
                    <li data-affiliation="Google DeepMind">Sean Kirmani</li>
                    <li data-affiliation="Google DeepMind, University of California, Berkeley">Sergey
                        Levine</li>
                    <li data-affiliation="Google DeepMind">Sherry Moore</li>
                    <li data-affiliation="Carnegie Mellon University">Shikhar Bahl</li>
                    <li data-affiliation="University of Southern California">Shivin Dass</li>
                    <li data-affiliation="Columbia University">Shuran Song</li>
                    <li data-affiliation="Google DeepMind">Sichun Xu</li>
                    <br>
                    <li data-affiliation="New York University">Siddhant Haldar</li>
                    <li data-affiliation="University of California, Berkeley">Simeon Adebola</li>
                    <li data-affiliation="Max Planck Institutes">Simon Guist</li>
                    <li data-affiliation="The University of Texas at Austin">Soroush Nasiriany</li>
                    <li data-affiliation="Intrinsic LLC">Stefan Schaal</li>
                    <li data-affiliation="Google DeepMind">Stefan Welker</li>
                    <br>
                    <li data-affiliation="Stanford University">Stephen Tian</li>
                    <li data-affiliation="Carnegie Mellon University">Sudeep Dasari</li>
                    <li data-affiliation="Stanford University">Suneel Belkhale</li>
                    <li data-affiliation="The University of Tokyo">Takayuki Osa</li>
                    <li data-affiliation="The University of Tokyo">Tatsuya Harada</li>
                    <li data-affiliation="The University of Tokyo">Tatsuya Matsushima</li>
                    <li data-affiliation="Google DeepMind">Ted Xiao</li>
                    <li data-affiliation="Google DeepMind">Tianhe Yu</li>
                    <br>
                    <li data-affiliation="Google DeepMind">Tianli Ding</li>
                    <li data-affiliation="Google DeepMind">Todor Davchev</li>
                    <li data-affiliation="Stanford University">Tony Z. Zhao</li>
                    <li data-affiliation="Google DeepMind">Travis Armstrong</li>
                    <li data-affiliation="University of California, Berkeley">Trevor Darrell</li>
                    <li data-affiliation="Google DeepMind, Carnegie Mellon University">Vidhi Jain</li>
                    <li data-affiliation="Google DeepMind">Vincent Vanhoucke</li>
                    <br>
                    <li data-affiliation="University of California, Berkeley">Wei Zhan</li>
                    <li data-affiliation="Google DeepMind, Carnegie Mellon University">Wenxuan Zhou</li>
                    <li data-affiliation="University of Technology, Nuremberg">Wolfram Burgard</li>
                    <li data-affiliation="Google DeepMind">Xi Chen</li>
                    <li data-affiliation="University of California, San Diego">Xiaolong Wang</li>
                    <li data-affiliation="University of California, Berkeley">Xinghao Zhu</li>
                    <li data-affiliation="University of California, San Diego">Xuanlin Li</li>
                    <li data-affiliation="Google DeepMind">Yao Lu</li>
                    <br>

                    <li data-affiliation="Google DeepMind">Yevgen Chebotar</li>
                    <li data-affiliation="Arizona State University">Yifan Zhou</li>
                    <li data-affiliation="The University of Texas at Austin">Yifeng Zhu</li>
                    <li data-affiliation="Google DeepMind">Ying Xu</li>
                    <li data-affiliation="University of Illinois Urbana-Champaign">Yixuan Wang</li>
                    <li data-affiliation="Carnegie Mellon University">Yonatan Bisk</li>
                    <li data-affiliation="Korea Advanced Institute of Science &amp; Technology">Yoonyoung Cho
                    </li>
                    <li data-affiliation="University of California, Berkeley">Youngwoon Lee</li>
                    <li data-affiliation="Stanford University">Yuchen Cui</li>
                    <br>
                    <li data-affiliation="University of California, San Diego">Yueh-hua Wu</li>
                    <li data-affiliation="Google DeepMind, The University of Tokyo">Yujin Tang</li>
                    <li data-affiliation="The University of Texas at Austin">Yuke Zhu</li>
                    <li data-affiliation="University of Illinois Urbana-Champaign">Yunzhu Li</li>
                    <li data-affiliation="The University of Tokyo">Yusuke Iwasawa</li>
                    <li data-affiliation="The University of Tokyo">Yutaka Matsuo</li>
                    <li data-affiliation="Google DeepMind">Zhuo Xu</li>
                    <li data-affiliation="New York University">Zichen Jeff Cui</li>

                    <br>


                </ul>
                
                
                <p><img src="https://robotics-transformer-x.github.io/img/logos.png" width="60%"></p><!-- <a href="http://g.co/robotics">
                    <image src="img/rng-logo.png" height="37px"> </a>
                    <a href="https://everydayrobots.com">
                    <image src="img/edr-logo.png" height="40px"> </a>
                    <a href="https://research.google/teams/brain/">   
                    <image src="img/google-research-logo.png" height="25px"> </a>  -->

                
            </div>


        

        <div>
            
            

            <div>
                <h3>
                    <b>Abstract</b>
                </h3>
                <p>
                    Large, high-capacity models trained on diverse datasets have shown remarkable successes on
                    efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led
                    to a consolidation of pretrained models, with general pretrained backbones serving as a starting
                    point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic
                    learning methods train a separate model for every application, every robot, and even every
                    environment. Can we instead train “generalist” X-robot policy that can be adapted efficiently to new
                    robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and
                    models to make it possible to explore this possibility in the context of robotic manipulation,
                    alongside experimental results that provide an example of effective X-robot policies. We assemble a
                    dataset from 22 different robots collected through a collaboration between 21 institutions,
                    demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data,
                    which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by
                    leveraging experience from other platforms.
                </p>
            </div>
            <div>
                    
                    <div>
                        <video onclick="setAttribute('controls', 'true');" autoplay="" loop="" muted="">
                            <source src="https://robotics-transformer-x.github.io/video/pick_ice_cream_cropped_removed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>pick ice cream</p>
                    </div>
                    <div>
                        <video onclick="setAttribute('controls', 'true');" autoplay="" loop="" muted="">
                            <source src="https://robotics-transformer-x.github.io/video/move_red_pepper_to_A_trimmed_cropped_removed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>move red pepper to A</p>
                    </div>
                </div>
            <div>
                <p><span>RT-2-X (55B)</span>:
                    one of the
                    <span>biggest models</span> to date performing
                    <span>unseen tasks</span> in <span>academic labs</span>
                </p>
            </div>
        </div>

        <div>
                <h3>
                    <b>Dataset Overview</b>
                </h3>
                <p><img src="https://robotics-transformer-x.github.io/img/overview.png"></p><p>We introduce the Open X-Embodiment Dataset, the largest open-source real robot dataset to date.
                    It contains 1M+ real robot trajectories spanning 22 robot embodiments,
                    from single robot arms to bi-manual robots and quadrupeds. </p>

                <p><img src="https://robotics-transformer-x.github.io/img/data_analysis.png"></p><p>The dataset was constructed by pooling 60 existing robot datasets from 34 robotic research labs
                    around the world. Our analysis shows that the number of visually distinct scenes is
                    well-distributed across different robot embodiments and that the dataset includes a wide range
                    of common behaviors and household objects.
                    For a detailed listing of all included datasets,
                    see <a target="_blank" href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?usp=sharing">this Google Sheet</a>.
                </p>
            </div>

        <div>
                <h3>
                    <b>Model Overview</b>
                </h3>
                <p><img src="https://robotics-transformer-x.github.io/img/algorithm.png"></p><p>We train two models on the robotics data mixture: (1) RT-1, an efficient Transformer-based
                    architecture designed for robotic control, and (2) RT-2, a large vision-language model co-fine-tuned
                    to output robot actions as natural language tokens.
                </p>

                <p>
                    Both models output robot actions represented with respect to the robot gripper frame. The robot
                    action is a 7-dimensional vector consisting of x, y, z, roll, pitch, yaw, and gripper opening or the
                    rates of these quantities. For data sets where some of these dimensions are not exercised by the
                    robot, during training, we set the value of the corresponding dimensions to zero.
                </p>

                <p>
                    We refer to the RT-1 model trained using the robotic data mixture as <span>RT-1-X</span>,
                    and the RT-2 model
                    trained using the robotic data mixture as <span>RT-2-X</span>.
                </p>

            </div>

        <div>
            <p>
                <h3>
                    <b>Results</b>
                </h3>

                <h4>
                    RT-1-X evaluation on in-distribution skills
                </h4>
            </p>

            <div>
                <div>
                    
                    <div>
                        <video onclick="setAttribute('controls', 'true');" autoplay="" loop="" muted="">
                            <source src="https://robotics-transformer-x.github.io/video/task_agnostic_open_drawer_rt1x_removed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>At University of Freiburg (AiS)</p>
                    </div>
                    <div>
                        <video onclick="setAttribute('controls', 'true');" autoplay="" loop="" muted="">
                            <source src="https://robotics-transformer-x.github.io/video/nyuenv1_removed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>At NYU (CILVR)</p>
                    </div>
                </div>

                <div>
                    
                    <div>
                        <video onclick="setAttribute('controls', 'true');" autoplay="" loop="" muted="">
                            <source src="https://robotics-transformer-x.github.io/video/bridge_cropped_removed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>At Stanford (IRIS)</p>
                    </div>
                    <div>
                        <video onclick="setAttribute('controls', 'true');" autoplay="" loop="" muted="">
                            <source src="https://robotics-transformer-x.github.io/video/jaco_play_removed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>At USC (CLVR)</p>
                    </div>
                </div>
            </div>
            <div>

                <p><span>RT-1-X</span>
                    performing diverse tasks in
                    <span>6 academic labs</span>
                </p>

                <p><img src="https://robotics-transformer-x.github.io/img/rt1x.png"></p><p><span>RT-1-X models</span>
                    outperform RT-1 or Original Methods trained on individual datasets by
                    <span> 50% in the small-data domain</span>
                </p>

                <p>
                    Original Method refers to the model developed by the creators of the dataset
                    trained only on that respective dataset. The Original Method constitutes a reasonable baseline
                    insofar as it can be expected that the model has been optimized to work well with the associated
                    data. The lab logos indicate the physical location of real robot evaluation, and the robot pictures
                    indicate the embodiment used for the evaluation.
                </p>

                <h4>
                    RT-2-X evaluation on emergent skills
                </h4>
            </div>
            <div>
                    
                    <div>
                        <video onclick="setAttribute('controls', 'true');" autoplay="" loop="" muted="">
                            <source src="https://robotics-transformer-x.github.io/video/move_apple_on_cloth_cropped_removed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>move apple on cloth</p>
                    </div>
                    <div>
                        <video onclick="setAttribute('controls', 'true');" autoplay="" loop="" muted="">
                            <source src="https://robotics-transformer-x.github.io/video/move_apple_between_can_and_orange_cropped_removed.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>move apple between can &amp; orange</p>
                    </div>
                </div>
            <div>

                <p><span>RT-2-X</span>
                        modulates low-level behaviors based on
                        <span> small changes in prepositions</span> (see
                        “on” vs “near” above) and demonstrates understanding of
                        <span> spatial relationships between
                            objects</span>
                </p>

                <div>
                    <p><img src="https://robotics-transformer-x.github.io/img/rt2x.png" alt="Image 1">
                    </p>
                    <p><img src="https://robotics-transformer-x.github.io/img/chart.png" alt="Image 2">
                    </p>
                </div>

                <p><span>RT-2-X</span>
                    outperforms RT-2 by <span>3x</span> in
                    emergent skill evaluations
                </p>


                <p>
                    RT-2-X demonstrates skills that
                    the RT-2 model was not capable of previously, including better spatial understanding in both the
                    absolute and relative sense. Small changes in preposition in the task string can also modulate
                    low-level robot behavior. The skills used for evaluation are illustrated in the figure above.
                </p>

            </div>

        </div>

        <div>
                <h3>
                    <b>Citation</b>
                </h3>

                <p>
                    If you're using the Open X-Embodiment dataset and RT-X in your research, <a target="_blank" href="https://robotics-transformer-x.github.io/citation.txt">please cite</a>. If you're specifically using datasets that have been contributed to the joint effort, please cite those as well.
                    
                    We provide a <a target="_blank" href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?usp=sharing">dataset spreadsheet</a> with citation for each dataset for your convenience.
                </p>

            </div>



        <div>
                <h3>
                    <b>Acknowledgements</b>
                </h3>

                <p>
                    We would like to thank John Guilyard for the amazing animations used for this website. The authors
                    would like to acknowledge Yuheng Kuang, Ning Hou, Utsav Malla, Sarah Nguyen, Rochelle Dela Cruz,
                    Justice Carbajal, Brianna Zitkovich, Emily Perez, Elio Prado, Jodilyn Peralta, Tran Pham, Deeksha
                    Manjunath, Samuel Wan, Jaspiar singh and the greater Google DeepMind team for their feedback and
                    contributions. The authors would like to thank Sanah Choudhry, Michael Griessel and Jon Small for their legal advice.
                </p>

                <p>
                    The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
    </strong></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pretty good career and life advice (2013) (255 pts)]]></title>
            <link>https://moxie.org/2013/01/07/career-advice.html</link>
            <guid>37759873</guid>
            <pubDate>Wed, 04 Oct 2023 01:37:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://moxie.org/2013/01/07/career-advice.html">https://moxie.org/2013/01/07/career-advice.html</a>, See on <a href="https://news.ycombinator.com/item?id=37759873">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<p>To my great surprise, young people now somewhat frequently contact me in order to solicit <em>career advice</em>.
They are usually in college or highschool, and want to know what the best next steps are for a career in 
security or software development.</p>

<p>This is, honestly, a really complicated question, mostly because I’m usually concerned that the question
itself might be the wrong one to be asking. What I want to say, more often than not, is something along
the lines of <em>don’t do it</em>; when I got out of highschool and focused on the answer to that same 
question, it was very nearly one of the biggest mistakes of my life.</p>

<p>Since I get these inquiries fairly regularly, I thought I’d write something here that I can use as a sort 
of canonical starting point for a response.</p>



<h3 id="tyler-durden-was-wrong-you-are-your-job">Tyler Durden was wrong, you <em>are</em> your job.</h3>

<p>In 1971, Dr. Philip Zimbardo conducted a psychological experiment that is now popularly known as
the “Stanford Prison Experiment.”  He constructed a makeshift simulated prison in the basement of
Stanford University’s psychology department, took a group of volunteers who tested as psychologically 
“normal” with no criminal background, flipped a coin, and assigned half of them to be “prisoners”
and half of them to be “prison guards.”</p>

<p><img src="https://moxie.org/blog/images/stanford-prison-experiment-bars.jpg"></p>

<p>Initially, everyone acted awkwardly; the situation was so obviously foreign to their sense of self and 
their lived experience that they found it strange and even humorous.  Fairly quickly, however, the situation
eclipsed all of that.  The prisoners became divorced from their former identities, and began to act from the 
place of being a prisoner.  They alternated between staging minor protests and ratting each other out, to 
going on hunger strike and suffering psychological breakdowns.</p>

<p>The prison guards also <em>became</em> prison guards, even though they were allowed to return to their normal
lives in between their eight hour shifts. They went so far as to invent forms of psychological punishment, 
create solitary confinement cells, pit prisoners against each other in order to break them, and give long 
speeches while slowly pacing the hallway and rattling their wooden batons across the bars of the cell doors.</p>

<p>Had the coin landed differently, the individual people would have been on the reverse sides of the cell
doors, but the roles would have been the same.</p>

<iframe width="480" height="360" src="https://www.youtube.com/embed/GlIyD15KS6s?start=225" frameborder="0">
</iframe>

<p>Things got so out of control that the experiment, originally scheduled for two weeks, had to be ended after 
six days.</p>

<p>The experiment was positioned as a comment on
<a href="http://en.wikipedia.org/wiki/Abu_Ghraib_torture_and_prisoner_abuse">institutions</a>, but the simpler lesson 
I draw from it as an individual is just <em>be careful what job you take, because your job will change you</em>.</p>

<p>This is obviously true for these types of hyperbolic examples — it’s not hard to imagine that a customs
agent (who spends 40 hours a week looking suspiciously at people, thinking skeptically about the honesty
of what people say, scientifically observing people’s eye movement, and trying to trip people up with 
confusing questions) might approach a conversation with someone they’ve just met at a party differently 
than they would have before taking the job.</p>

<p>It’s equally easy to imagine that someone who has worked as a prison guard for a decade might approach a 
romantic relationship differently than someone who has worked as a grief counselor for the same period.</p>

<p>But it’s not just cops, prison guards, and customs agents.  The context of one’s life defines not just <em>what</em>
but <em>how</em> one thinks, and a job tends to dominate the context of one’s life — particularly when that job
is considered to be part of a career. Your job will change you.</p>

<h3 id="the-choices-we-make">The choices we make.</h3>

<p>I recently got around to reading <a href="http://en.wikipedia.org/wiki/Aaron_Cometbus">Aaron Cometbus’</a>
enjoyable “Cometbus #54, In China With Green Day?!!”  One of my favorite quotes was:</p>

<blockquote>
  <p>Some things are like that—they strike you as repugnant for instinctive
reasons, probably having to do with your culture and the way you were
raised. The French word “gauche” comes to mind, but I preferred the
Hebrew word “treyf.” Literally, it means not kosher, but I also use it
to describe things like cars, bars, strip clubs, guns, dogs, rock-n-roll,
and football games. Things that are treyf, you avoid, not because you
hate them per se, but because in avoiding them you keep yourself from
becoming like the people you hate.</p>

  <p>– <cite>Aaron Cometbus, “Cometbus #54”</cite></p>
</blockquote>

<p>The concept resonates with me for the same reasons.  Like Aaron, I have my own list of things that 
are <em>treyf</em>, not because I find them necessarily unenjoyable, but because they <em>add up</em> to something
that I ultimately dislike.</p>

<p>For instance, whenever I get on an airplane and walk past first class, I inevitably go through a familiar 
mental process.  First, I’m envious.  The passengers are already in their comfortable seats, drinking from 
champagne flutes, contemplating the moment after takeoff when they can recline into their cocoons and watch 
a movie of their choice on demand.</p>

<p>But then, I register who is sitting in those seats.  It’s usually almost all predominantly unhealthy looking 
middle-aged white men, who it is clear from a glance have spent literally hundreds of hours of their lives 
over the past year in these airplanes.  And suddenly, I’m glad that I’m not sitting there.</p>

<p>Those seats are <em>treyf</em> for me, not because I don’t envy extra leg room, but because I don’t envy the people 
sitting in them.  There’s a reason the bulk of the first class passengers resemble each other, just as there’s 
a reason prison guards tend to act the same.  I know that by making choices designed to land me in the first 
class cabin, it would be difficult to avoid also inheriting the dreariness associated with its current occupants.</p>

<h3 id="the-future-is-looking-back-at-you">The future is looking back at you.</h3>

<p>In the context of a career, these concepts make it simple to look into the future. Jobs at software 
companies are typically advertised in terms of the difficult problems that need solving, the impact the project 
will have, the benefits the company provides, the playful color of the bean bag chairs. Likewise, jobs in other 
fields have their own set of metrics that they use to position themselves within their domains.</p>

<p>As a young person, though, I think the best thing you can do is to ignore all of that and simply 
<em>observe the older people working there</em>.</p>

<p>They are the future you. Do not think that you will be substantially different. Look carefully at how 
they spend their time at work and outside of work, because this is also almost certainly how your life will 
look. It sounds obvious, but it’s amazing how often young people imagine a different projection for themselves.</p>

<p>Look at the real people, and you’ll see the honest future for yourself.</p>

<h3 id="be-careful-not-to-discover-a-career-before-youve-discovered-yourself">Be careful not to discover a career before you’ve discovered yourself.</h3>

<p>This all presupposes we’re starting from a point where considering these questions is a real possibility. 
Sure, pure objectivity is impossible; after all, society itself <em>also</em> defines the context of our thoughts, 
and by now it’s way too late to effectively remove ourselves from <em>that</em>. So how can we evaluate what 
experiences we want for ourselves, when it is experiences themselves which transform our very desires?</p>

<p>At the moment when young people are considering their career strategy, they have typically made all of
their life choices completely within supporting structures.  Even having worked hard to get where they are, 
and even though things like class and race can mean that some have the cards stacked against them, it’s 
rare for young people to have substantially departed from supporting frameworks.  Highschools have “college 
counselors” (not “dropout counselors”), scholarships and financial aid packages lead in a single direction, 
and university overlaps with internships — which then culminates largely in a series of “career fairs.”<br>
There is a tremendous amount of support for these decisions, and very little support for making any deviating 
choices.</p>

<p>When we arrive at the ends of these funnels, it’s possible that the direction we’re facing is more a
reflection of those structures than it is a reflection of ourselves.  Self-determination in a moment like 
that can’t simply be about making a choice, it has to start with transforming the conditions that <em>constitute</em>
our choices.  It requires challenging the “self” in “self-determination” by stepping as far outside of those
supporting structures as possible, for as long as possible.</p>

<p>This is necessarily terrifying.  I think a lot about a quote from 
<a href="http://en.wikipedia.org/wiki/Alfredo_M._Bonanno">Alfredo Bonanno</a>, an anarchist and habitual bank robber, 
on the feeling of leaving prison:</p>

<blockquote>
  <p>The instant you get out of prison you have the sensation that you are leaving something dear to you.
Why? Because you know that you are leaving a part of your life inside, because you spent some of your
life there which, even if it was under terrible conditions, is still a part of you.  And even if you
lived it badly and suffered horribly, which is not always the case, it is always better than the nothing
that your life is reduced to the moment it disappears.</p>

  <p>– <cite>Alfredo Bonanno, “Locked Up”</cite></p>
</blockquote>

<p>I know that the most significant and meaningful periods of my life have all been moments that I could have 
never rationally chosen or even <em>known as possibilities</em> had I not been foolish or lucky enough to step into 
the nothingness that Alfredo Bonanno writes about.  I try to remind myself that if leaving prison is scary,
the same is likely true for any genuine process of discovery.</p>

<h3 id="theres-no-rush-to-get-started-early-on-a-never-ending-task">There’s no rush to get started early on a never-ending task.</h3>

<p>Everything before a career has defined beginnings and endings.  Elementary school, middle school, junior high,
highschool, university.  There’s always been a predefined end, and that contributes a lot to making the
indignities of those institutions bearable.  Once you start working full time, though, it’s just One Long 
Semester that you’re expected to attend for <em>the rest of your life</em>.</p>

<p>So consider caution if you’re overly excited to start down that road as quickly as you can.  Other than being 
forever, it’s not as different from what’s come before as you might imagine.</p>

<h3 id="by-way-of-an-explanation">By way of an explanation.</h3>

<p>This is all just to explain why, when people write me for career advice, I’m as likely to respond 
with something like <em>“if I were you, I’d hitchhike to Alaska this summer instead.”</em></p>

<p>My career advice usually falls within the framework of doing the absolute minimum amount of work necessary 
to prevent starvation, and then doing something that’s not about money, completely outside of supporting 
structures, and not simply a matter of “consuming experience” with the remaining available time.</p>

<p>Learn three chords, start a band, and <a href="http://www.youtube.com/watch?v=U5g8u-2TI3g&amp;feature=youtu.be&amp;t=5m20s">go on tour</a>. Ride a bicycle 
<a href="http://www.microcosmdistribution.com/catalog/zines/2821/">as far as you can</a>. Go <a href="http://www.wwoof.org/">WWOOFing</a> 
or start a <a href="http://hayesvalleyfarm.tumblr.com/">community garden</a> in your neighborhood. Put together a <a href="http://mrpuppetarmy.wordpress.com/">traveling 
puppet show</a>. Build a drone to engage the emerging drones 
<a href="http://www.newser.com/story/156143/sheriff-in-oakland-wants-surveillance-drone.html">controlled by domestic police</a>. Do whatever — but make it uncomfortable (like leaving prison!) and <em>make it count</em>.</p>





<ul><li>Stay in touch,</li></ul>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BBC gives up on Threads, sticks with Mastodon (393 pts)]]></title>
            <link>https://darnell.day/bbc-gives-up-on-threads-by-instagram-sticks-with-mastodon</link>
            <guid>37759871</guid>
            <pubDate>Wed, 04 Oct 2023 01:37:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://darnell.day/bbc-gives-up-on-threads-by-instagram-sticks-with-mastodon">https://darnell.day/bbc-gives-up-on-threads-by-instagram-sticks-with-mastodon</a>, See on <a href="https://news.ycombinator.com/item?id=37759871">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://i.snap.as/LxUHCO5k.jpeg" alt="BBC News icon"></p>

<p>So numerous brands are giving up on <a href="https://threads.net/">Threads</a> by <a href="https://instagram.com/">Instagram</a>, allegedly due to lack of engagement (ironically, most of them are still using <a href="https://x.com/">X</a>, formally known as <a href="https://twitter.com/">Twitter</a>).</p>

<p>What makes this news more interesting is the fact that the <a href="https://www.bbc.com/">British Broadcasting Corporation</a> (BBC) has abandoned its Threads account but is still maintaining its self-hosted Mastodon accounts online.</p>

<blockquote><p>The <a href="https://www.threads.net/@nfl">National Football League</a> hasn't posted anything in six weeks, before the start of the regular season. This is the nation's most popular sports league, and it has completely abandoned Meta's new platform. Even with its 1.9 million followers.</p>

<p>Among news publishers, the British Broadcasting Corporation (BBC) <a href="https://www.threads.net/@bbc">stopped posting</a>to Threads 11 weeks ago, not long after the launch. <a href="https://www.threads.net/@cbsnews">CBS News</a> hasn't posted in five weeks. (<a href="https://ktla.com/news/some-major-brands-are-giving-up-on-threads-as-engagement-craters/">KTLA5</a>)</p></blockquote>

<p>Earlier in the year (at the end of July), the BBC announced that they were <a href="https://www.bbc.com/rd/blog/2023-07-mastodon-distributed-decentralised-fediverse-activitypub">engaging in a six-month experiment</a> with <a href="https://joinmastodon.org/">Mastodon</a>, &amp; even self-hosted their content on their custom domain: <a href="https://social.bbc/">Social.bbc</a> (how cool is that!).</p>

<p>While accounts like <a href="https://darnell.day/@/BBC5Live@social.bbc">@<span>BBC5Live@social.bbc</span></a> &amp; <a href="https://darnell.day/@/BBCRadio4@social.bbc">@<span>BBCRadio4@social.bbc</span></a> remain relatively active, <a href="https://darnell.day/@/bbc@threads.net">@<span>bbc@threads.net</span></a> appears abandoned, despite boasting far higher engagement levels on <a href="https://meta.com/">Meta</a>’s Twitter rival (now called X).</p>

<p>But why would the BBC abandon Threads while maintaining a presence on Mastodon as well as other social media platforms‽ There are three possible theories:</p>
<ul><li><p>BBC might be weary of posting content to Threads after Meta <a href="https://www.wired.com/story/meta-facebook-instagram-news-block-canada-wildfire/">geo-banned Canadian news outlets</a> after Canadian legislators passed a “news tax” law (<strong>note:</strong> the law is silly, but I digress).</p></li>

<li><p>BBC prefers to control its social narrative, &amp; will redirect followers from Threads to their Mastodon accounts once Threads joins the Fediverse.</p></li>

<li><p>BBC thinks Threads is a waste of time &amp; resources as Threads lacks a public API to help automate posting news to their account.</p></li></ul>

<p>Could the BBC eventually shut down their Mastodon account by the time 2024 arrives‽ Sure. But I suspect that <a href="https://www.zdnet.com/article/twitter-seeing-record-user-engagement-the-data-tells-a-different-story/">with X dying</a>, Threads stagnating, &amp; Meta raging, the BBC might keep their Mastodon account active to avoid having their voice limited online.</p>

<p>👨🏾‍💻 by <a href="https://www.darnellclayton.com/">Darnell Clayton</a> 🔛 <a href="https://darnell.day/@/darnell@darnell.day">@<span>darnell@darnell.day</span></a></p>

<p>🕺🏾 Follow my adventures upon:
👉🏾 <a href="https://darnell.day/@/darnell@one.darnell.one">@<span>darnell@one.darnell.one</span></a> 🐘 (<a href="https://one.darnell.one/@darnell">Mastodon</a>)
👉🏾 <a href="https://darnell.day/@/darnell@darnell.moe">@<span>darnell@darnell.moe</span></a> 🦁 (<a href="https://darnell.moe/@darnell">Misskey</a>)
👉🏾 <a href="https://darnell.day/@/darnell@threads.net">@<span>darnell@threads.net</span></a> 🧵 (<a href="https://threads.net/@darnell">Threads</a>)</p>

<p>🦹🏾‍♂️ Other digital havens:
👉🏾 <a href="https://darnell.day/@/darnell@darnell.app">@<span>darnell@darnell.app</span></a> 📸 (<a href="https://darnell.app/darnell">Pixelfed</a>)
👉🏾 <a href="https://darnell.day/@/darnelltv@darnell.tv">@<span>darnelltv@darnell.tv</span></a> 👨🏾‍💻 (<a href="https://darnell.tv/">WordPress</a>)
👉🏾 <a href="https://darnell.day/@/darnell@counter.social">@<span>darnell@counter.social</span></a> 🥷🏾 (<a href="https://counter.social/@darnell">Counter Social</a>)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[D.C.'s ban on cashless businesses takes effect (198 pts)]]></title>
            <link>https://www.axios.com/local/washington-dc/2023/09/29/cashless-business-ban</link>
            <guid>37759662</guid>
            <pubDate>Wed, 04 Oct 2023 01:02:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/local/washington-dc/2023/09/29/cashless-business-ban">https://www.axios.com/local/washington-dc/2023/09/29/cashless-business-ban</a>, See on <a href="https://news.ycombinator.com/item?id=37759662">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-category="story" data-vars-sub-category="story"><div><p><a data-cy="author-image-link" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-category="story" data-vars-sub-category="headshot" data-vars-click-url="/local/washington-dc/authors/aspiegel" href="https://www.axios.com/local/washington-dc/authors/aspiegel"><img alt="Anna Spiegel" username="aspiegel" loading="lazy" width="40" height="40" decoding="async" data-nimg="1" srcset="https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2F1tX3a6b7KUbUTvJaUE7YRqqpUXI%3D%2F52x0%2Fsmart%2F2023%2F05%2F09%2F1683595934348.jpg&amp;w=320&amp;q=75 1x" src="https://www.axios.com/_next/image?url=https%3A%2F%2Fimages.axios.com%2F1tX3a6b7KUbUTvJaUE7YRqqpUXI%3D%2F52x0%2Fsmart%2F2023%2F05%2F09%2F1683595934348.jpg&amp;w=320&amp;q=75"></a></p><div><ul><li data-cy="byline-author"><a data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-category="story" data-vars-sub-category="author-line" data-vars-link-text="Anna Spiegel" data-vars-click-url="/local/washington-dc/authors/aspiegel" href="https://www.axios.com/local/washington-dc/authors/aspiegel"><span>Anna Spiegel</span></a></li></ul></div></div><figure data-cy="au-image"><img data-cy="StoryImage" alt="" fetchpriority="high" width="1920" height="1080" decoding="async" data-nimg="1" sizes="100vw" srcset="https://images.axios.com/KOQ1kq5fCWG7G7q-Stuk72719ao=/0x207:3000x1894/320x180/2023/09/28/1695936835811.jpg?w=320 320w, https://images.axios.com/KOQ1kq5fCWG7G7q-Stuk72719ao=/0x207:3000x1894/320x180/2023/09/28/1695936835811.jpg?w=320 320w, https://images.axios.com/XZXSfZ34YLCmanluIJNdpKNGaZ4=/0x207:3000x1894/640x360/2023/09/28/1695936835811.jpg?w=640 640w, https://images.axios.com/XZXSfZ34YLCmanluIJNdpKNGaZ4=/0x207:3000x1894/640x360/2023/09/28/1695936835811.jpg?w=640 640w, https://images.axios.com/t2_jwoLhPDTiXy_Qy4hoyJsq4rc=/0x207:3000x1894/768x432/2023/09/28/1695936835811.jpg?w=768 768w, https://images.axios.com/t2_jwoLhPDTiXy_Qy4hoyJsq4rc=/0x207:3000x1894/768x432/2023/09/28/1695936835811.jpg?w=768 768w, https://images.axios.com/7KvZo7ktQLsu8ZOrc39_kFPf-7w=/0x207:3000x1894/1024x576/2023/09/28/1695936835811.jpg?w=1024 1024w, https://images.axios.com/7KvZo7ktQLsu8ZOrc39_kFPf-7w=/0x207:3000x1894/1024x576/2023/09/28/1695936835811.jpg?w=1024 1024w, https://images.axios.com/jSJE7pWtNQq5igpxBcDUA0Rqvw8=/0x207:3000x1894/1366x768/2023/09/28/1695936835811.jpg?w=1366 1366w, https://images.axios.com/jSJE7pWtNQq5igpxBcDUA0Rqvw8=/0x207:3000x1894/1366x768/2023/09/28/1695936835811.jpg?w=1366 1366w, https://images.axios.com/9IsmxuyMsQ0Kb-2DWVQAjOYjkPg=/0x207:3000x1894/1600x900/2023/09/28/1695936835811.jpg?w=1600 1600w, https://images.axios.com/9IsmxuyMsQ0Kb-2DWVQAjOYjkPg=/0x207:3000x1894/1600x900/2023/09/28/1695936835811.jpg?w=1600 1600w, https://images.axios.com/hKrhqK091wGqr9bjLcneCn6nYUY=/0x207:3000x1894/1920x1080/2023/09/28/1695936835811.jpg?w=1920 1920w, https://images.axios.com/hKrhqK091wGqr9bjLcneCn6nYUY=/0x207:3000x1894/1920x1080/2023/09/28/1695936835811.jpg?w=1920 1920w" src="https://images.axios.com/hKrhqK091wGqr9bjLcneCn6nYUY=/0x207:3000x1894/1920x1080/2023/09/28/1695936835811.jpg?w=1920"><figcaption><p>A sign from Sweetgreen's cashless days. Photo: ahi Chikwendiu/The Washington Post via Getty Images</p></figcaption></figure><div><p>No cash, no more — D.C. will enforce a law starting Sunday that bans businesses from refusing cash payments.</p><p><strong>Why it matters: </strong>Cashless policies have been controversial for years — and often deemed discriminatory — so there's finally clarity on how D.C. businesses should proceed.</p><p><strong>Driving the news: </strong>The D.C. Council passed <a data-vars-link-text="legislation" data-vars-click-url="https://code.dccouncil.gov/us/dc/council/laws/23-187#:~:text=Cashless%20Retailers%20Prohibition%20Amendment%20Act%20of%202020.,-AN%20ACT&amp;text=To%20amend%20Title%2028%20of,for%20enforcement%20of%20this%20requirement." data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://code.dccouncil.gov/us/dc/council/laws/23-187#:~:text=Cashless%20Retailers%20Prohibition%20Amendment%20Act%20of%202020.,-AN%20ACT&amp;text=To%20amend%20Title%2028%20of,for%20enforcement%20of%20this%20requirement." target="_blank">legislation</a> in 2020 prohibiting retailers from discriminating against cash payments, but <a data-vars-link-text="funding" data-vars-click-url="https://dcist.com/story/23/09/27/new-laws-dc-md-va-october-2023/" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://dcist.com/story/23/09/27/new-laws-dc-md-va-october-2023/" target="_blank">funding</a> from the city to enforce it didn't kick in until the new fiscal year.</p><ul><li>The law was also unenforceable during a public health emergency — bad timing with the Covid pandemic when a bunch of businesses went digital to limit contact and prevent the spread via dollars (which we know now <a data-vars-link-text="is not a thing" data-vars-click-url="https://news.byu.edu/going-cashless-to-prevent-covid-19-was-useless-new-byu-study-finds" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://news.byu.edu/going-cashless-to-prevent-covid-19-was-useless-new-byu-study-finds" target="_blank">is not a thing</a>).</li></ul><p><strong>Zoom in: </strong>The law <a data-vars-link-text="stipulates" data-vars-click-url="https://www.lawhelp.org/dc/resource/cashless-retailer-legal-alert" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.lawhelp.org/dc/resource/cashless-retailer-legal-alert" target="_blank">stipulates</a> that in direct transactions, businesses cannot refuse cash, charge a higher price for cash payments, or post signs that cash isn't accepted. That applies to restaurants, bars, retail shops, and more.</p><p><strong>Yes, but: </strong>Businesses can offer cash-to-card services for in-store purchases with <a data-vars-link-text="a lot of stipulations" data-vars-click-url="https://www.lawhelp.org/dc/resource/cashless-retailer-legal-alert" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.lawhelp.org/dc/resource/cashless-retailer-legal-alert" target="_blank">a lot of stipulations</a> including no associated fees or time limits.</p><ul><li>Internet, mail, and phone transactions are exempt from the law. As are parking facilities that did not accept cash payments prior to Dec. 1, 2020.</li></ul><p><strong>Flashback: </strong>The cashless trend gained momentum in the pandemic, but it started well before. Salad mega-chain <a data-vars-link-text="Sweetgreen" data-vars-click-url="https://medium.com/sweetgreen/cashless-49f64f24dd0f" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://medium.com/sweetgreen/cashless-49f64f24dd0f" target="_blank">Sweetgreen</a> led the all-digital charge in 2016, citing a decline in cash payments and the time and labor costs of processing them.</p><ul><li>It didn't last long. In 2019, Sweetgreen <a data-vars-link-text="reversed the policy" data-vars-click-url="https://www.nytimes.com/2019/04/25/business/cashless-stores-sweetgreen-amazon-go.html" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.nytimes.com/2019/04/25/business/cashless-stores-sweetgreen-amazon-go.html" target="_blank">reversed the policy</a> after facing backlash that it was discriminatory against people without bank accounts or credit cards — a demographic dominated by the elderly, low-income, and people of color.</li></ul><p><strong>Zoom out:</strong> Several states and cities have already banned cashless policies, including Massachusetts, California, and New York City.</p><p><strong>Reality check: </strong>Cashless laws don't<strong> </strong>mean the digital trend is slowing down, thanks in large part to changes in consumer behavior during the pandemic. Businesses like Sweetgreen may take cash, but mobile ordering and app-based rewards programs are still huge, especially in the fast-casual sphere.</p><ul><li>Financial services company <a data-vars-link-text="Square" data-vars-click-url="https://streetsensemedia.org/article/cashless-covid-discrimination-dc-council-mayor/" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://streetsensemedia.org/article/cashless-covid-discrimination-dc-council-mayor/" target="_blank">Square</a> reported that their cashless D.C. clients jumped from 15% in 2019 to nearly 40% in 2021.</li></ul><p><strong>Between the lines:</strong> D.C. businesses have found ways — some more helpful than others — around the cash question, and it's still unclear if they'll have to change their policies. Nationals Park went cashless in 2021, but fans can exchange cash for "Nats Bucks" in the stadium to use at concessions.</p><ul><li>Meanwhile, local chain <a data-vars-link-text="Compass Coffee" data-vars-click-url="https://www.compasscoffee.com/pages/frequently-asked-questions" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.compasscoffee.com/pages/frequently-asked-questions" target="_blank">Compass Coffee</a> limits cash by accepting only exact change or cash payments for gift cards.</li></ul><p><strong>The other side:</strong> Some businesses have sworn off cash to prevent burglaries, especially as crime has spiked in the city. Penn Quarter's <a data-vars-link-text="Hill Country Barbecue" data-vars-click-url="https://www.washingtonian.com/2018/08/03/after-four-break-ins-hill-country-barbecue-market-is-going-cashless/" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://www.washingtonian.com/2018/08/03/after-four-break-ins-hill-country-barbecue-market-is-going-cashless/" target="_blank">Hill Country Barbecue</a> went cashless years ago after back-to-back robberies.</p><p><strong>What we're watching: </strong>Enforcement, of which even D.C. Council chair <a data-vars-link-text="Phil Mendelson" data-vars-click-url="https://dcist.com/story/23/09/27/new-laws-dc-md-va-october-2023/" data-vars-content-id="2c477b8f-ecd6-497f-bfaf-6f753394cea0" data-vars-headline="D.C.'s ban on cashless businesses takes effect" data-vars-event-category="story" data-vars-sub-category="story" data-vars-item="in_content_link" href="https://dcist.com/story/23/09/27/new-laws-dc-md-va-october-2023/" target="_blank">Phil Mendelson</a> seemed unsure.</p></div></div><div data-vars-category="cta" data-vars-experiment="side_rail_membership" data-vars-experiment-variant="washington-dc" data-vars-label="cta/view/side_rail_membership/washington-dc"><p>🌱</p><p>Support local journalism by becoming a member.</p><a href="https://www.axios.com/local/washington-dc/membership"><p>Learn more</p></a></div><h2>More <!-- -->Washington D.C.<!-- --> stories</h2><p>No <!-- -->stories<!-- --> could be found</p></div><div><div data-vars-category="cta" data-vars-experiment="side_rail" data-vars-experiment-variant="washington-dc" data-vars-label="cta/view/side_rail/washington-dc"><p><img alt="Washington D.C.postcard" loading="lazy" width="80" height="80" decoding="async" data-nimg="1" srcset="https://www.axios.com/_next/image?url=%2Fimages%2Fpostcard%2Fwashington-dc.png&amp;w=320&amp;q=75 1x" src="https://www.axios.com/_next/image?url=%2Fimages%2Fpostcard%2Fwashington-dc.png&amp;w=320&amp;q=75"></p><p>Get a free daily digest of the most important news in your backyard with Axios Washington D.C..</p></div><div data-vars-category="cta" data-vars-experiment="side_rail_membership" data-vars-experiment-variant="washington-dc" data-vars-label="cta/view/side_rail_membership/washington-dc"><p>🌱</p><p>Support local journalism by becoming a member.</p><a href="https://www.axios.com/local/washington-dc/membership"><p>Learn more</p></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Daisugi, the Japanese technique of growing trees out of other trees (2020) (225 pts)]]></title>
            <link>https://www.openculture.com/2020/10/daisugi.html</link>
            <guid>37759366</guid>
            <pubDate>Wed, 04 Oct 2023 00:18:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.openculture.com/2020/10/daisugi.html">https://www.openculture.com/2020/10/daisugi.html</a>, See on <a href="https://news.ycombinator.com/item?id=37759366">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p><img loading="lazy" decoding="async" fetchpriority="high" src="https://cdn8.openculture.com/2020/10/22214022/Daisugi-3.jpg" alt="" width="640" height="427" srcset="https://cdn8.openculture.com/2020/10/22214022/Daisugi-3.jpg 640w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-360x240.jpg 360w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-240x160.jpg 240w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-300x200.jpg 300w" sizes="(max-width: 640px) 100vw, 640px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2020/10/22214022/Daisugi-3.jpg" data-srcset="https://cdn8.openculture.com/2020/10/22214022/Daisugi-3.jpg 640w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-360x240.jpg 360w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-240x160.jpg 240w, https://cdn8.openculture.com/2020/10/22214022/Daisugi-3-300x200.jpg 300w"></p>
<p><small><em><a href="https://twitter.com/wrathofgnon/status/1250287741247426565">Image by Wrath of Gnon</a></em></small></p>
<p>We’ve all admired the elegance of Japan’s traditional styles of architecture. Their development required the kind of dedicated craftsmanship that takes generations to cultivate — but also, more practically speaking, no small amount of wood. By the 15th century, Japan already faced a shortage of seedlings, as well as land on which to properly cultivate the trees in the first place. Necessity being the mother of invention, this led to the creation of an ingenious solution: <em>daisugi</em>, the growing of additional trees, in effect, out of existing trees — creating, in other words, a kind of giant <a href="http://www.openculture.com/2020/01/the-art-philosophy-of-bonsai.html">bonsai</a>.</p>
<p>“Written as 台杉 and literally meaning <em>platform cedar</em>, the technique resulted in a tree that resembled an open palm with multiple trees growing out if it, perfectly vertical,” <a href="https://www.spoon-tamago.com/2020/10/20/daisugi-japanese-forestry-technique/">writes Spoon and Tamago’s Johnny Waldman</a>. “Done right, the technique can prevent deforestation and result in perfectly round and straight timber known as <em>taruki</em>, which are used in the roofs of Japanese teahouses.”</p>


<p>These teahouses are still prominent in Kyoto, a city still known for its traditional cultural heritage, and not coincidentally where&nbsp;<em>daisugi</em> first developed. “It’s said that it was Kyoto’s preeminent tea master, <a href="http://thekyotoproject.org/english/sen-no-rikyu-the-greatest-tea-master/">Sen-no-rikyu</a>, who demanded perfection in the Kitayama cedar during the 16th century,” <a href="https://mymodernmet.com/kitayama-cedar-daisugi/">writes My Modern Met’s Jessica Stewart</a>.</p>
<p><img loading="lazy" decoding="async" src="https://cdn8.openculture.com/2020/10/22223540/daisugi-2.jpeg" alt="" width="640" height="797" srcset="https://cdn8.openculture.com/2020/10/22223540/daisugi-2.jpeg 640w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-289x360.jpeg 289w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-193x240.jpeg 193w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-300x374.jpeg 300w" sizes="(max-width: 640px) 100vw, 640px" data-old-src="https://www.openculture.com/wp-content/plugins/native-lazyload/assets/images/placeholder.svg" data-src="https://cdn8.openculture.com/2020/10/22223540/daisugi-2.jpeg" data-srcset="https://cdn8.openculture.com/2020/10/22223540/daisugi-2.jpeg 640w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-289x360.jpeg 289w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-193x240.jpeg 193w, https://cdn8.openculture.com/2020/10/22223540/daisugi-2-300x374.jpeg 300w"></p>
<p>At the time “a form of very straight and stylized <a href="https://www.rethinktokyo.com/sukiya-zukuri-japanese-architecture"><em>sukiya-zukuri</em></a> architecture was high fashion, but there simply weren’t nearly enough raw materials to build these homes for every noble or samurai who wanted one,” says <a href="https://twitter.com/wrathofgnon/status/1250287741247426565">a thread by Twitter account Wrath of Gnon</a>, which includes these and other photos of <em>daisugi</em> in action. “Hence this clever solution of using bonsai techniques on trees.” Aesthetics aside — as far aside as they ever get in Japan, at any rate — “the lumber produced in this method is 140% as flexible as standard cedar and 200% as dense/strong,” making it “absolutely perfect for rafters and roof timber.” And not only is&nbsp;<em>daisugi</em>‘s product straight, slender, and typhoon-resistant, it’s&nbsp;marveled at around the world 600 years later. Of how many forestry techniques can we say the same?</p>
<p>via <a href="https://www.spoon-tamago.com/2020/10/20/daisugi-japanese-forestry-technique/">Spoon and Tamago</a></p>
<p><strong>Related Content:</strong></p>
<p><a href="http://www.openculture.com/2020/01/the-art-philosophy-of-bonsai.html">The Art &amp; Philosophy of Bonsai</a></p>
<p><a href="http://www.openculture.com/2017/02/this-392-year-old-bonsai-tree-survived-the-hiroshima-atomic-blast-still-flourishes-today.html">This 392-Year-Old Bonsai Tree Survived the Hiroshima Atomic Blast &amp; Still Flourishes Today: The Power of Resilience</a></p>
<p><a href="http://www.openculture.com/2017/10/the-philosophical-appreciation-of-rocks-in-china-japan.html">The Philosophical Appreciation of Rocks in China &amp; Japan: A Short Introduction to an Ancient Tradition</a></p>
<p><a href="http://www.openculture.com/2019/07/the-secret-language-of-trees.html">The Secret Language of Trees: A Charming Animated Lesson Explains How Trees Share Information with Each Other</a></p>
<p><a href="http://www.openculture.com/2017/10/the-social-lives-of-trees.html">The Social Lives of Trees: Science Reveals How Trees Mysteriously Talk to Each Other, Work Together &amp; Form Nurturing Families</a></p>
<p><a href="http://www.openculture.com/2019/08/a-digital-animation-compares-the-size-of-trees-from-the-3-inch-bonsai-to-the-300-foot-sequoia.html">A Digital Animation Compares the Size of Trees: From the 3-Inch Bonsai, to the 300-Foot Sequoia</a></p>
<p><em>Based in Seoul,&nbsp;<a href="http://blog.colinmarshall.org/">Colin Marshall</a>&nbsp;writes and broadcasts on cities, language, and culture. His projects include the Substack newsletter</em>&nbsp;<a href="https://colinmarshall.substack.com/">Books on Cities</a>,<em>&nbsp;the book&nbsp;</em>The Stateless City: a Walk through 21st-Century Los Angeles&nbsp;<em>and the video series&nbsp;</em><a href="https://vimeo.com/channels/thecityincinema">The City in Cinema</a><em>. Follow him on Twitter at&nbsp;<a href="https://twitter.com/#%21/colinmarshall">@colinmarshall</a>, on&nbsp;<a href="https://www.facebook.com/colinmarshallessayist">Facebook</a>, or on&nbsp;<a href="https://www.instagram.com/colinmarshallseoul/">Instagram</a>.</em></p>
<br>		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Press Reviews of the AMD Ryzen 7040 Framework Laptop 13 (115 pts)]]></title>
            <link>https://frame.work/blog/reviews-of-framework-laptop-13-amd-ryzen-7040-series-are-live</link>
            <guid>37758911</guid>
            <pubDate>Tue, 03 Oct 2023 23:23:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://frame.work/blog/reviews-of-framework-laptop-13-amd-ryzen-7040-series-are-live">https://frame.work/blog/reviews-of-framework-laptop-13-amd-ryzen-7040-series-are-live</a>, See on <a href="https://news.ycombinator.com/item?id=37758911">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>The first reviews of <a href="https://frame.work/products/laptop-diy-13-gen-amd">Framework Laptop 13 (AMD Ryzen 7040 Series)</a> are live, and the results are everything we hoped for when we kicked off the product. Reviewers called out the massive jump in graphics performance, increases in battery life, and improvements in multi-core workloads. The integrated graphics capabilities are especially astounding, putting a wide range of recent game titles within reach in a thin, light, portable system. Check out some of what reviewers have to say:</p>
<h3><strong>“It immediately jumps to my number one recommendation for software developers looking for a small and portable laptop. But, given how insanely good this laptop is, I’d also strongly recommend those looking for a laptop for school or home or office use, to really consider this one."</strong></h3>
<p>– <a href="https://www.youtube.com/watch?v=uwr14Q4C9gY" target="_blank" rel="noopener">JustJosh</a></p>
<h3><strong>“It’s worth saying, as usual, that this is, first and foremost, a productivity machine, but with AMD on board, it’ll play just as hard as it works."</strong></h3>
<p>– <a href="https://www.engadget.com/framework-brings-amd-mainboards-to-its-13-inch-laptop-140050567.html" target="_blank" rel="noopener">Daniel Cooper, Engadget</a></p>
<h3><strong>“All upgrades should be this easy. Taking the Intel board out and putting the AMD board in didn't take me very long at all, especially with Framework's instructions handy.”</strong></h3>
<p>– <a href="https://www.tomshardware.com/news/amd-7040-framework-laptop-mainboard-transfer-performance-test-review" target="_blank" rel="noopener">Andrew Freedman, Tom's Hardware</a></p>
<p>Soon, you’ll be able to see real customer feedback too, with the first Batch 1 shipments going out shortly. With recent increases in our factory capacity and logistics throughput, we’re working through the rest of the pre-order backlog as quickly as we can. We’re also continuing to work with AMD and other silicon providers on further firmware and driver updates to continue to optimize the product, especially as it comes to Linux support.</p>
<h2>EU warehouse for Framework Marketplace items</h2>
<p>We have one more exciting, long awaited announcement for you. This week, we activated a new warehouse in the Netherlands, enabling substantially faster and cheaper shipment of <a href="https://frame.work/marketplace">Framework Marketplace</a> items to customers in EU and UK. Note that new laptops will continue to ship from our Taiwan warehouse. This is a project we’ve been working on for some time, finding the right partner, building integrations with our systems, and getting inventory into place. With this, we reduced our shipping prices to €12 to countries we ship to in EU and £10 to UK.</p>
<p>Our philosophy is to charge as close as possible to our actual costs for picking, packing, and shipping out orders. In addition to being transparent, we believe this better aligns incentives. Rather than hiding shipping costs within our product pricing and setting artificial free shipping thresholds to try to get you to buy more, you can choose how and when to group items that you need into orders to maximize value. This also means that when we optimize our infrastructure like we did this week, we get to pass the savings directly on to you.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Passive Solar Water Desalination (148 pts)]]></title>
            <link>https://theness.com/neurologicablog/index.php/passive-solar-water-desalination/</link>
            <guid>37758788</guid>
            <pubDate>Tue, 03 Oct 2023 23:07:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theness.com/neurologicablog/index.php/passive-solar-water-desalination/">https://theness.com/neurologicablog/index.php/passive-solar-water-desalination/</a>, See on <a href="https://news.ycombinator.com/item?id=37758788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <p><img decoding="async" fetchpriority="high" src="https://theness.com/neurologicablog/wp-content/uploads/sites/3/2023/09/MIT-Solar-Desalinization-01-PRESS_0-700x467.jpg" alt="" width="300" height="200" srcset="https://theness.com/neurologicablog/wp-content/uploads/sites/3/2023/09/MIT-Solar-Desalinization-01-PRESS_0-700x467.jpg 700w, https://theness.com/neurologicablog/wp-content/uploads/sites/3/2023/09/MIT-Solar-Desalinization-01-PRESS_0-768x512.jpg 768w, https://theness.com/neurologicablog/wp-content/uploads/sites/3/2023/09/MIT-Solar-Desalinization-01-PRESS_0.jpg 900w" sizes="(max-width: 300px) 100vw, 300px">I know we are supposed to be worried about the world supply of fresh water. I have been hearing that at least for the last 40 years, and the statistics are alarming. According to the <a href="https://turningthetide.watercommission.org/">Global Commission on the Economics of Water</a>:</p>
<blockquote><p>“We are seeing the consequences not of freak events, nor of population growth and economic development, but of having mismanaged water globally for decades. As the science and evidence show, we now face a systemic crisis that is both local and global.”</p></blockquote>
<p>Sounds about right. We are not very good at this sort of large-scale management. Everyone just does their thing, oblivious to the big picture, until we have a crisis. Then experts point out the looming crisis which everyone at first ignores. Then we have meetings, summits, and a lot of hand-wringing but next to nothing gets done. Eventually we mostly technology our way out of the problem, but not after significant negative consequences, especially for the world’s poor. The water crisis seems to be following the same playbook.</p>
<p>Now experts are predicting that by 2030 world demand for fresh water will <a href="https://www.theguardian.com/environment/2023/mar/17/global-fresh-water-demand-outstrip-supply-by-2030">outstrip supply by 40%</a>. This shortage will affect everyone, including people in wealthy developed nations. So now it’s a real crisis. To be clear, I am not trying to minimize this problem at all. The Commission outlines a seven-point plan for properly and fairly managing the world’s fresh water supply, and it all sound very reasonable. It feels like we are in a phase of human history when we are collectively realizing that billions of people have a global effect on the entire planet, and we need to seriously start transitioning from a local focus on securing resources, to globally managing those limited resources.</p>
<p><span id="more-14155"></span>But I have to admit that my level of worry about the fresh water problem was always mitigated by the sense that we would technology our way out of this one, eventually. There is a lot of water in the world. Water itself is not a limited resource, only fresh water, and we know how to make fresh water out of salt water. It looks like a problem that can be solved by incremental advances in technology. I also admit I know this perspective is a bit naive. I know that <a href="https://www.unicef.org/wash/water-scarcity">4 billion people</a> already experience water scarcity in the world at least part of the time. This is largely a political problem – a problem of mismanagement and lack of resource equity. Technology can never completely compensate for political malpractice or malfeasance. We need to do both – advance the technology while pushing for proper management.</p>
<p>The primary technology we are talking about is desalination – removing salt from salt water to make fresh water. There are already many desalination plants around the world, but like with everything, it’s ultimately a matter of economics. How much does it cost to desalinate water? It’s also a matter of scale – how much fresh water can we make? (These issues all sound very similar to global warming and energy production.) A recent incremental advance may move us closer to economical fresh water on demand. MIT engineers and their collaborators have improved on the design of <a href="https://news.mit.edu/2023/desalination-system-could-produce-freshwater-cheaper-0927">passive solar powered water desalination</a>.</p>
<p>The basic technology uses sunlight to evaporate salt water, which will then condense into drinkable fresh water, leaving the salt behind. The problem with this technology is that the salt left behind quickly clogs the system, meaning it has to be cleaned out every few days. This makes the devices labor and cost intensive. What the researchers did was add a circulating current to the water and the salt which is left behind. This removes the salt from the system and keeping it from clogging up. However, the initial design desalinated water at a very low rate. So they tweaked the design into a multi-stage system of evaporation and condensing that removes the salt while maintaining a high water output.</p>
<p>The result is a portable passive system, entirely run by sunlight and physics. Here is how it works:</p>
<blockquote><p>The heart of the team’s new design is a single stage that resembles a thin box, topped with a dark material that efficiently absorbs the heat of the sun. Inside, the box is separated into a top and bottom section. Water can flow through the top half, where the ceiling is lined with an evaporator layer that uses the sun’s heat to warm up and evaporate any water in direct contact. The water vapor is then funneled to the bottom half of the box, where a condensing layer air-cools the vapor into salt-free, drinkable liquid. The researchers set the entire box at a tilt within a larger, empty vessel, then attached a tube from the top half of the box down through the bottom of the vessel, and floated the vessel in saltwater.</p>
<p>In this configuration, water can naturally push up through the tube and into the box, where the tilt of the box, combined with the thermal energy from the sun, induces the water to swirl as it flows through. The small eddies help to bring water in contact with the upper evaporating layer while keeping salt circulating, rather than settling and clogging.</p></blockquote>
<p>For a box a square meter in size it can produce 5 liters of drinkable water per hour. The system can also last for several years. Perhaps more importantly, the total price for the water is less than the average price for tap water in the US. The system should also be scalable.</p>
<p>Assuming the best case scenario, that this system works, is scalable, and can be economically mass-produced, it may go a long way to mitigate water insecurity. I doubt something like this will be a solution all by itself. We still need to globally manage our water properly. But at least technology offers us new tools.</p>
                    			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[StreamingLLM: Efficient streaming technique enable infinite sequence lengths (104 pts)]]></title>
            <link>https://arxiv.org/abs/2309.17453</link>
            <guid>37756895</guid>
            <pubDate>Tue, 03 Oct 2023 20:10:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2309.17453">https://arxiv.org/abs/2309.17453</a>, See on <a href="https://news.ycombinator.com/item?id=37756895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2309.17453.pdf">Download PDF</a></p><blockquote>
            <span>Abstract:</span>Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at <a href="https://github.com/mit-han-lab/streaming-llm" rel="external noopener nofollow">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Guangxuan Xiao [<a href="https://arxiv.org/show-email/a1f20a90/2309.17453">view email</a>]      <br>    <strong>[v1]</strong>
        Fri, 29 Sep 2023 17:59:56 UTC (9,548 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lumber prices down 11% YoY (170 pts)]]></title>
            <link>https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html</link>
            <guid>37756714</guid>
            <pubDate>Tue, 03 Oct 2023 19:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html">https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html</a>, See on <a href="https://news.ycombinator.com/item?id=37756714">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="main-wrap1">
<h3>
<a href="https://www.calculatedriskblog.com/2023/10/update-lumber-prices-down-11-yoy.html">Update: Lumber Prices Down 11% YoY</a>
</h3>
<p><span size="-1">by <span>Calculated Risk on <abbr title="2023-10-03T14:42:00-04:00">10/03/2023 02:42:00 PM</abbr></span></span></p>


<div>
<p>Here is another monthly update on lumber prices.</p><p><b>SPECIAL NOTE:</b> The <a href="https://www.cmegroup.com/content/dam/cmegroup/notices/ser/2023/04/SER-9183.pdf">CME group discontinued</a> the Random Length Lumber Futures contract on May 16th.  I've now switched to a new physically-delivered Lumber Futures contract that was started in August 2022.&nbsp;</p><div><p>Unfortunately, this impacts long term price comparisons since the new contract was priced about 24% higher than the old random length contract for the period both contracts were available.</p><p>

This graph shows CME random length framing futures through last August (blue), and the new physically-delivered Lumber Futures (LBR) contract starting in August 2022 (Red).</p><p>LBR is currently at $491.50 per 1000 board feet, down 11% from $550.00 a year ago.</p><div>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-nrnFu0Tnc6yBc5n7GnpBs5C9ywhzj5fhhBH7QmZ7pP9b3aapUJPgQahl3g0BSQWAzcmSmPL-qF_uNxg_zLlR1bR0K0lwscZYMAGhm4OQHKU5qUwznyerW6ywaEAtd1jLa5EKJ7XDNQBKqRT6Bmuey_Q7VBu_VghRkwsQlcV7z4nKm18_tBZb/s1001/LumberOct32023.PNG"><img alt="Lumber Prices" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh-nrnFu0Tnc6yBc5n7GnpBs5C9ywhzj5fhhBH7QmZ7pP9b3aapUJPgQahl3g0BSQWAzcmSmPL-qF_uNxg_zLlR1bR0K0lwscZYMAGhm4OQHKU5qUwznyerW6ywaEAtd1jLa5EKJ7XDNQBKqRT6Bmuey_Q7VBu_VghRkwsQlcV7z4nKm18_tBZb/s320/LumberOct32023.PNG"></a><i><b><span>Click on graph for larger image.</span></b></i></p><p>There is somewhat of a seasonal demand for lumber, and lumber prices usually peak in April or May.</p><div><p>
We didn't see a significant runup in prices this Spring due to the housing slowdown.</p></div></div></div>


</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debunking NIST's calculation of the Kyber-512 security level (416 pts)]]></title>
            <link>https://blog.cr.yp.to/20231003-countcorrectly.html</link>
            <guid>37756656</guid>
            <pubDate>Tue, 03 Oct 2023 19:49:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cr.yp.to/20231003-countcorrectly.html">https://blog.cr.yp.to/20231003-countcorrectly.html</a>, See on <a href="https://news.ycombinator.com/item?id=37756656">Hacker News</a></p>
<div id="readability-page-1" class="page">
<h2>The cr.yp.to <a href="https://blog.cr.yp.to/index.html" accesskey="i">blog</a></h2>
<hr>
<div>
<p>Older (Access-J): <a href="https://blog.cr.yp.to/20230609-turboboost.html" accesskey="j"><b>2023.06.09: Turbo Boost:</b></a> <span>How to perpetuate security problems. #overclocking #performancehype #power #timing #hertzbleed #riskmanagement #environment</span></p>
<details><summary>Table of contents (Access-I for index page)</summary>
<table>
<tbody><tr><td><b>2023.10.03: The inability to count correctly:</b> Debunking NIST's calculation of the Kyber-512 security level. #nist #addition #multiplication #ntru #kyber #fiasco</td></tr>
<tr><td><a href="https://blog.cr.yp.to/20230609-turboboost.html"><b>2023.06.09: Turbo Boost:</b></a> <span>How to perpetuate security problems. #overclocking #performancehype #power #timing #hertzbleed #riskmanagement #environment</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20220805-nsa.html"><b>2022.08.05: NSA, NIST, and post-quantum cryptography:</b></a> <span>Announcing my second lawsuit against the U.S. government. #nsa #nist #des #dsa #dualec #sigintenablingproject #nistpqc #foia</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20220129-plagiarism.html"><b>2022.01.29: Plagiarism as a patent amplifier:</b></a> <span>Understanding the delayed rollout of post-quantum cryptography. #pqcrypto #patents #ntru #lpr #ding #peikert #newhope</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20201206-msword.html"><b>2020.12.06: Optimizing for the wrong metric, part 1: Microsoft Word:</b></a> <span>Review of "An Efficiency Comparison of Document Preparation Systems Used in Academic Research and Development" by Knauff and Nejasmic. #latex #word #efficiency #metrics</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20191024-eddsa.html"><b>2019.10.24: Why EdDSA held up better than ECDSA against Minerva:</b></a> <span>Cryptosystem designers successfully predicting, and protecting against, implementation failures. #ecdsa #eddsa #hnp #lwe #bleichenbacher #bkw</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20190430-vectorize.html"><b>2019.04.30: An introduction to vectorization:</b></a> <span>Understanding one of the most important changes in the high-speed-software ecosystem. #vectorization #sse #avx #avx512 #antivectors</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20171105-infineon.html"><b>2017.11.05: Reconstructing ROCA:</b></a> <span>A case study of how quickly an attack can be developed from a limited disclosure. #infineon #roca #rsa</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20171017-collisions.html"><b>2017.10.17: Quantum algorithms to find collisions:</b></a> <span>Analysis of several algorithms for the collision problem, and for the related multi-target preimage problem. #collision #preimage #pqcrypto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20170723-random.html"><b>2017.07.23: Fast-key-erasure random-number generators:</b></a> <span>An effort to clean up several messes simultaneously. #rng #forwardsecrecy #urandom #cascade #hmac #rekeying #proofs</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20170719-pqbench.html"><b>2017.07.19: Benchmarking post-quantum cryptography:</b></a> <span>News regarding the SUPERCOP benchmarking system, and more recommendations to NIST. #benchmarking #supercop #nist #pqcrypto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20161030-pqnist.html"><b>2016.10.30: Some challenges in post-quantum standardization:</b></a> <span>My comments to NIST on the first draft of their call for submissions. #standardization #nist #pqcrypto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20160607-dueprocess.html"><b>2016.06.07: The death of due process:</b></a> <span>A few notes on technology-fueled normalization of lynch mobs targeting both the accuser and the accused. #ethics #crime #punishment</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20160516-quantum.html"><b>2016.05.16: Security fraud in Europe's "Quantum Manifesto":</b></a> <span>How quantum cryptographers are stealing a quarter of a billion Euros from the European Commission. #qkd #quantumcrypto #quantummanifesto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20160315-jefferson.html"><b>2016.03.15: Thomas Jefferson and Apple versus the FBI:</b></a> <span>Can the government censor how-to books? What if some of the readers are criminals? What if the books can be understood by a computer? An introduction to freedom of speech for software publishers. #censorship #firstamendment #instructions #software #encryption</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20151120-batchattacks.html"><b>2015.11.20: Break a dozen secret keys, get a million more for free:</b></a> <span>Batch attacks are often much more cost-effective than single-target attacks. #batching #economics #keysizes #aes #ecc #rsa #dh #logjam</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20150314-optimizing.html"><b>2015.03.14: The death of optimizing compilers:</b></a> <span>Abstract of my tutorial at ETAPS 2015. #etaps #compilers #cpuevolution #hotspots #optimization #domainspecific #returnofthejedi</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20150218-printing.html"><b>2015.02.18: Follow-You Printing:</b></a> <span>How Equitrac's marketing department misrepresents and interferes with your work. #equitrac #followyouprinting #dilbert #officespaceprinter</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140602-saber.html"><b>2014.06.02: The Saber cluster:</b></a> <span>How we built a cluster capable of computing 3000000000000000000000 multiplications per year for just 50000 EUR. #nvidia #linux #howto</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140517-insns.html"><b>2014.05.17: Some small suggestions for the Intel instruction set:</b></a> <span>Low-cost changes to CPU architecture would make cryptography much safer and much faster. #constanttimecommitment #vmul53 #vcarry #pipelinedocumentation</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140411-nist.html"><b>2014.04.11: NIST's cryptographic standardization process:</b></a> <span>The first step towards improvement is to admit previous failures. #standardization #nist #des #dsa #dualec #nsa</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140323-ecdsa.html"><b>2014.03.23: How to design an elliptic-curve signature system:</b></a> <span>There are many choices of elliptic-curve signature systems. The standard choice, ECDSA, is reasonable if you don't care about simplicity, speed, and security. #signatures #ecc #elgamal #schnorr #ecdsa #eddsa #ed25519</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140213-ideal.html"><b>2014.02.13: A subfield-logarithm attack against ideal lattices:</b></a> <span>Computational algebraic number theory tackles lattice-based cryptography.</span></td></tr>
<tr><td><a href="https://blog.cr.yp.to/20140205-entropy.html"><b>2014.02.05: Entropy Attacks!</b></a> <span>The conventional wisdom says that hash outputs can't be controlled; the conventional wisdom is simply wrong.</span></td></tr>
</tbody></table></details></div><hr>
<h2>2023.10.03: The inability to count correctly: <span>Debunking NIST's calculation of the Kyber-512 security level. #nist #addition #multiplication #ntru #kyber #fiasco</span></h2>
<p><img src="https://blog.cr.yp.to/20231003/longevity.png"></p>
<p>[Sidney Harris cartoon used with permission.
Copyright holder: <a href="http://sciencecartoonsplus.com/">ScienceCartoonsPlus.com</a>.]</p>
<p>Quick, what's 2<sup>40</sup> plus 2<sup>40</sup>?
It's 2<sup>80</sup>, right?</p>
<p>No, obviously not.
40 plus 40 is 80,
and
2<sup>40</sup> <em>times</em> 2<sup>40</sup> is 2<sup>80</sup>,
but
2<sup>40</sup> <em>plus</em> 2<sup>40</sup> is only 2<sup>41</sup>.</p>
<p><strong>Take a deep breath and relax.</strong>
When cryptographers
are analyzing the security of
cryptographic systems,
<em>of course</em>
they don't make stupid mistakes such as
multiplying numbers that should have been added.</p>
<p>If such an error somehow managed to appear,
<em>of course</em> it would immediately be caught
by the robust procedures that cryptographers follow
to thoroughly review security analyses.</p>
<p>Furthermore,
in the context of standardization processes
such as the NIST Post-Quantum Cryptography Standardization Project (NISTPQC),
<em>of course</em> the review procedures are even more stringent.</p>
<p>The only way
for the security claims for modern cryptographic standards
to turn out to fail
would be because of some unpredictable new discovery
revolutionizing the field.</p>
<p><strong>Oops, wait, maybe not.</strong>
In 2022,
NIST announced plans to standardize
a particular cryptosystem, Kyber-512.
As justification, NIST issued claims
regarding the security level of Kyber-512.
In 2023,
NIST issued a draft standard for Kyber-512.</p>
<p>NIST's underlying calculation of the security level
was a severe and indefensible miscalculation.
NIST's primary error is exposed in this blog post,
and boils down to nonsensically <em>multiplying</em> two costs that should have been <em>added</em>.</p>
<p>How did such a serious error
slip past NIST's review process?
Do we dismiss this as an isolated incident?
Or do we conclude that something is fundamentally broken
in the procedures that NIST is following?</p>
<p><strong>Discovering the secret workings of NISTPQC.</strong> 
I filed a FOIA request
<a href="https://blog.cr.yp.to/20220805-nsa.html">"NSA, NIST, and post-quantum cryptography"</a>
in March 2022.
NIST stonewalled, in violation of the law.
Civil-rights firm
<a href="https://loevy.com/">Loevy &amp; Loevy</a>
filed a lawsuit on my behalf.</p>
<p>That lawsuit has been gradually
<a href="https://nist.pqcrypto.org/foia/index.html">revealing</a>
secret NIST documents,
shedding some light on what was actually going on behind the scenes,
including much heavier NSA involvement than indicated by NIST's public narrative.
Compare, for example, the following documents:</p>
<ul>
<li>
<p>A <a href="https://web.archive.org/web/20230910091944/https://csrc.nist.gov/CSRC/media/Events/ISPAB-MARCH-2014-MEETING/documents/a_quantum_world_v1_ispab_march_2014.pdf">public 2014 document</a>
says that its author is
"Post Quantum Cryptography Team, National Institute of Standards and Technology
(NIST), pqc@nist.gov".</p>
</li>
<li>
<p>A <a href="https://nist.pqcrypto.org/foia/index.html#20230815/Re_%20pqc%20mailing%20list(1)-3.pdf">secret 2016 document</a>
listed the actual pqc@nist.gov team members,
with more NSA people
(Nick Gajcowski; David Hubbard; Daniel Kirkwood; Brad Lackey; Laurie Law; John McVey;
Mark Motley; Scott Simon; Jerry Solinas; David Tuller)
than NIST people.
(Another Department of Defense representative on the list
was Jacob Farinholt, Naval Surface Warfare Center, US Navy.
I'm not sure about Evan Bullock.)</p>
</li>
<li>
<p>Another <a href="https://nist.pqcrypto.org/foia/index.html#20230619/Re_%20Your%20visit%20to%20NIST%20.pdf">secret 2016 document</a>
shows that NSA's Scott Simon was scheduled to visit NIST on 12 January 2016.</p>
</li>
<li>
<p>Another <a href="https://nist.pqcrypto.org/foia/index.html#20230508/RE_%20Outline%20for%20PQC%20announcement.pdf">secret 2016 document</a>
shows that NIST's "next meeting with the NSA PQC folks" was scheduled for 26 January 2016.</p>
</li>
<li>
<p>Another <a href="https://nist.pqcrypto.org/foia/index.html#20230508/pqc%20stuff.pdf">secret 2016 document</a>
shows that Michael Groves from NSA's UK partner
was scheduled to visit NIST on 2 February 2016.</p>
</li>
<li>
<p>Another <a href="https://nist.pqcrypto.org/foia/index.html#20230915/Foreign%20Trip%20Report-09232016dm-ykl.doc">secret 2016 document</a>
lists Colin Whorlow from NSA's UK partner
as someone that NIST visited in 2016,
in particular discussing
"confidence and developments for each of the primary PQC families".</p>
</li>
<li>
<p>A <a href="https://web.archive.org/web/20230316130702/https://csrc.nist.gov/CSRC/media/Presentations/pqc-update-round-2-and-beyond/images-media/pqcrypto-sept2020-moody.pdf">public 2020 document</a>
says "Engagement with community and stakeholders.
This includes feedback we received from many, including the NSA.
We keep everyone out of our internal standardization meetings and the decision process.
The feedback received (from the NSA) did not change any of our decisions ...
NIST encouraged the NSA to provide comments publicly.
NIST alone makes the PQC standardization decisions, based on publicly available information, and stands by those decisions".</p>
</li>
</ul>
<p>I filed a new FOIA request in January 2023,
after NIST issued its claims regarding the security level of Kyber-512.
NIST again stonewalled.
Loevy &amp; Loevy has now filed a new lawsuit regarding that FOIA request.</p>
<p>Public material regarding Kyber-512 already shows
how NIST multiplied costs that should have been added,
how NIST sabotaged public review of this calculation,
and how important this calculation was for NIST's narrative of Kyber outperforming NTRU,
filling a critical gap left by other steps
that NIST took to promote the same narrative.
This blog post goes carefully through the details.</p>
<p><strong>Alice and Bob paint a fence.</strong>
At this point you might be thinking
something like this:
"Sorry, no, it's not plausible
that anyone could have mixed up
a formula saying 2<sup>x</sup>+2<sup>y</sup> with a formula saying 2<sup>x+y</sup>,
whatever the motivations might have been."</p>
<p>As a starting point for understanding what happened,
think about schoolchildren in math class facing a word problem:</p>
<blockquote>
<p>There is a fence to paint.
Alice would take 120 minutes to paint the fence.
Bob would take 240 minutes to paint the fence.
How long would it take Alice and Bob
to paint the fence together?</p>
</blockquote>
<p>The approved answer in school
says that Alice paints 1/120 of the fence per minute,
and Bob paints 1/240 of the fence per minute,
so together they paint 1/120 + 1/240 = 1/80 of the fence per minute,
so it takes them 80 minutes to paint the fence.</p>
<p>The real answer could be more complicated
because of second-order effects.
Probably Alice and Bob working together
are getting less tired
than Alice or Bob working alone for longer would have.
In the opposite direction,
maybe there's a slowdown because Alice and Bob
enjoy each other's company
and pause for a coffee.</p>
<p>Schoolchildren often give answers such as
240 − 120 = 120,
or 120 + 240 = 360,
or (120 + 240)/2 = 180.
These children are just manipulating numbers,
not thinking through what the numbers <em>mean</em>.</p>
<p><strong>Two disciplines for catching errors.</strong>
In later years of education,
physics classes
teach students a type-checking discipline
of tracking <em>units</em> with each number.</p>
<p>Here are examples of calculations following this discipline:</p>
<ul>
<li>
<p>Dividing "1 fence" by "120 min"
  gives "0.00833 fence/min".</p>
</li>
<li>
<p>Adding "0.00833 fence/min"
  to "0.00417 fence/min"
  gives "0.01250 fence/min".</p>
</li>
<li>
<p>Taking the reciprocal gives "80.0 min/fence".</p>
</li>
</ul>
<p>The same discipline wouldn't let you add,
for example, "1 fence" to "120 min":
the units don't match.</p>
<p>This discipline avoids many basic errors.
On the other hand,
it still allows, e.g., the mistake of adding
"120 min" to "240 min" to obtain "360 min".</p>
<p>What catches this mistake
is a discipline stronger than tracking units:
namely, tracking <em>semantics</em>.</p>
<p>The numbers have meanings.
They're quantitative features of real objects.
For example,
80 minutes
is the total time for Alice and Bob
to paint the fence
when Alice is painting part of the fence
and Bob is painting part of the fence.
That's what
the question asked us to calculate.</p>
<p>A different question would be
the total time for Alice to paint the fence
and then for Bob to repaint the same fence.
This would be 120 minutes plus 240 minutes.</p>
<p>Yet another question would be
the total time
for Alice to paint the fence,
and then for Bob to wait for the coat
of paint to dry,
and then for Bob to apply a second coat.
Answering this would require more information,
namely the waiting time.</p>
<p>All of these questions make sense.
They pass type-checking.
But their semantics are different.</p>
<p><strong>Alice and Bob tally the costs of an attack.</strong>
Alice and Bob have finished painting
and are now discussing the merits
of different encryption systems.</p>
<p><img src="https://blog.cr.yp.to/20231003/alice-bob.png">
They'd like to make sure that
breaking whichever system they pick
is <em>at least</em> as hard as searching for an AES-128 key.
They've agreed that searching for an AES-128 key
is slightly above 2<sup>140</sup> bit operations.</p>
<p>Alice and Bob are broadcasting their discussion
for anyone who's interested.
Let's listen to what they're saying:</p>
<ul>
<li>
<p>Alice:
"Hmmm,
there are a bunch of sources saying
that the XYZZY attack algorithm uses 2<sup>80</sup> iterations
to break this particular cryptosystem.
It's worrisome that this number is so low.
What else do we know
about the cost of the attack?"</p>
</li>
<li>
<p>Bob:
"I found a source saying
that there are actually extra factors
in the iteration count,
and estimating that
the XYZZY attack uses 2<sup>95</sup> iterations."</p>
</li>
<li>
<p>Alice:
"Here's another source
looking at the details
of the computations inside each iteration,
and estimating that those computations
cost 2<sup>25</sup> bit operations."</p>
</li>
<li>
<p>Bob:
"There's also a gigantic array being accessed.
Here's a source
estimating that the memory access
inside each iteration
is as expensive as 2<sup>35</sup> bit operations."</p>
</li>
<li>
<p>Alice:
"Okay, let's review.
The best estimate available
for the total cost of each iteration in the XYZZY attack
is around 2<sup>35</sup> bit operations.
A tiny part of that is
2<sup>25</sup> bit operations for computation.
The main cost is the equivalent of
2<sup>35</sup> bit operations for the memory access."</p>
</li>
<li>
<p>Bob:
"Agreed.
Multiplying
2<sup>95</sup> iterations
by
2<sup>35</sup> bit operations per iteration
gives us a total of 2<sup>130</sup> bit operations.
Doesn't meet the security target."</p>
</li>
<li>
<p>Alice:
"Right,
that's a thousand times easier than AES-128 key search.
Let's move on to the next cryptosystem."</p>
</li>
</ul>
<p><strong>How to botch the tally of costs.</strong>
Imagine a government agency
that has also
been looking at this particular cryptosystem,
but with one critical difference:
the agency is desperate to say
that this cryptosystem is okay.</p>
<p>How does the agency deal with the XYZZY attack?</p>
<p>One answer is to aim
for a lower security goal,
hyping the cost of carrying out 2<sup>130</sup> bit operations.
For comparison,
Bitcoin mining
did only about 2<sup>111</sup> bit operations in 2022.
("Only"!)</p>
<p>But let's assume that
the agency has promised the world
that it will reach <em>at least</em>
the AES-128 security level.</p>
<p>What does the agency do?</p>
<p>Here's an idea.
For the costs per iteration,
<strong>instead of <em>adding</em> 2<sup>25</sup> for computation to 2<sup>35</sup> for memory access,
how about <em>multiplying</em> 2<sup>25</sup> for computation by 2<sup>35</sup> for memory access?</strong></p>
<p>The product is 2<sup>60</sup>.
Multiplying this by 2<sup>95</sup> iterations
gives 2<sup>155</sup>, solidly above 2<sup>143</sup>.
Problem solved!</p>
<p><strong>How discipline catches the error.</strong>
Alice and Bob are correctly tracking
the semantics of each number.
The agency isn't.</p>
<p>The total attack cost is the number of iterations
times the cost per iteration.
Each iteration incurs</p>
<ul>
<li>
<p>cost for computation, estimated as 2<sup>25</sup> bit operations, and</p>
</li>
<li>
<p>cost for memory access, estimated to be as expensive as 2<sup>35</sup> bit operations.</p>
</li>
</ul>
<p>The agency's multiplication of these two costs
makes no sense,
and produces a claimed per-iteration cost that's millions of times larger
than the properly estimated per-iteration cost.</p>
<p>This multiplication is so glaringly wrong
that it doesn't even pass
physics-style type-checking.
Specifically,
multiplying
"2<sup>25</sup> bitops/iter"
by
"2<sup>35</sup> bitops/iter"
doesn't give
"2<sup>60</sup> bitops/iter".
It gives
"2<sup>60</sup> bitops<sup>2</sup>/iter<sup>2</sup>".
Multiplying further by "2<sup>95</sup> iter"
doesn't give
"2<sup>155</sup> bitops";
it gives
"2<sup>155</sup> bitops<sup>2</sup>/iter".</p>
<p><strong>Agency desperation strikes back.</strong>
How can the agency
phrase this nonsensical calculation of a severely inflated security estimate
in a way that will pass superficial review?</p>
<p>The goal here is for the 155 to sound
as if it's simply putting together
numbers from existing sources.
For example:</p>
<ul>
<li>
<p><span color="red">Here's a source estimating
  an iteration count of 2<sup>95</sup>.</span></p>
</li>
<li>
<p><span color="red">Here's a source estimating
  2<sup>25</sup> bit operations per iteration.</span></p>
</li>
<li>
<p><span color="red">Here's a source estimating
  that accounting for memory
  multiplies costs by 2<sup>35</sup>.</span></p>
</li>
<li>
<p><span color="red">95 plus 25 plus 35 is 155,
  solidly above 143.</span></p>
</li>
</ul>
<p>The deception here is in the third step,
the step that leaps from
cost 2<sup>25</sup> per iteration
to cost 2<sup>60</sup> per iteration.</p>
<p>How many readers are going to check
the third source
and see that it was actually estimating
cost 2<sup>35</sup> per iteration?</p>
<p><strong>Streamlining the marketing.</strong>
The wrong calculation sounds even simpler
if there's a previous source
that has already put the 2<sup>95</sup>
and the 2<sup>25</sup> together:</p>
<ul>
<li>
<p><span color="red">Here's a source estimating
  2<sup>120</sup> bit operations.</span></p>
</li>
<li>
<p><span color="red">Here's a source estimating
  that accounting for memory
  multiplies costs by 2<sup>35</sup>.</span></p>
</li>
<li>
<p><span color="red">120 plus 35 is 155,
  solidly above 143.</span></p>
</li>
</ul>
<p>At this point the agency
has completely suppressed
any mention of iterations,
despite the central role of iterations
in the attack and in any competent analysis of the attack.</p>
<p>How many readers are going
to check <em>both</em> sources,
see that
the second source
estimates cost 2<sup>35</sup> per iteration,
and see that
the iteration count in the first source
is far below 2<sup>120</sup>?</p>
<p><strong>Kyber's limited selection of security levels.</strong>
You might be thinking something like this:
"Okay, sure, I see how it would be possible for a desperate agency
to replace cost addition with a nonsensical multiplication,
replacing 2<sup>130</sup> with a fake 2<sup>155</sup>,
while at the same time making this hard for people to see.
But <strong>why would anyone have wanted to play this risky game?</strong>
If Kyber-512 was around 2<sup>130</sup>,
and the target was a little above 2<sup>140</sup>,
why didn't they just bump up the parameters to 10% higher security,
something like Kyber-576?"</p>
<p>This is an obvious question given that
RSA and ECC and (to take some post-quantum examples) McEliece and NTRU
naturally support whatever size you want.</p>
<p>A long, long time ago,
I wrote
<a href="https://cr.yp.to/nistp224.html">fast software</a>
for the NSA/NIST P-224 elliptic curve,
and then found a
<a href="https://cr.yp.to/talks.html#2001.10.29">better curve</a>
at that security level,
namely
y<sup>2</sup> = x<sup>3</sup> + 7530x<sup>2</sup> + x mod 2<sup>226</sup>−5.
But then I decided that bumping the size up
to <a href="https://lib25519.cr.yp.to/">2<sup>255</sup>−19</a>
would be much more comfortable, so I did.</p>
<p>Kyber is different.
You <em>can't</em> just bump up Kyber's parameters to 10% higher security:</p>
<ul>
<li>
<p>Kyber-576 doesn't exist.
  If you want something stronger than Kyber-512
  then you have to increase the "dimension" by 50%,
  jumping all the way up to Kyber-768.</p>
</li>
<li>
<p>If you want something stronger than Kyber-768
  then you have to jump all the way up to Kyber-1024.</p>
</li>
<li>
<p>If you want something stronger than Kyber-1024
  then, sorry, tough luck.</p>
</li>
</ul>
<p>One of the "unique advantages of Kyber"
specifically advertised in the
<a href="https://web.archive.org/web/20230310174959/https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf#subsection.6.1">official Kyber documentation</a>
is that implementing a "dimension-256 NTT"
handles "<em>all parameter sets</em>" for Kyber
(emphasis in original).
This "NTT" isn't something optional for Kyber implementors;
it's baked into the structure of Kyber's public keys and ciphertexts.
Using dimensions that aren't multiples of 256
would require changing the core Kyber design.</p>
<p>The same Kyber "advantage" also means that going beyond 1024
would lead to performance issues and,
more importantly,
security issues surrounding occasional
"decryption failures" forced by the prime baked into the NTT.
Avoiding this would again
require changing the core Kyber design.</p>
<p>For comparison,
NTRU options targeting higher security levels—including
simple proofs that there are no decryption failures—are readily available.
For example, one of the
<a href="https://ntruprime.cr.yp.to/">NTRU Prime</a>
options is <code>sntrup1277</code>.</p>
<p>But let's assume that NIST doesn't care about Kyber's limitations at the high end.
Let's instead focus on the low end,
specifically on applications that
have limited sizes for public keys and/or ciphertexts
and thus can't use the highest available security levels.</p>
<p>An application limited to 1KB
can't use Kyber-768 (1184-byte public keys, 1088-byte ciphertexts).
The highest-security Kyber option for that application
is Kyber-512 (800-byte keys, 768-byte ciphertexts).</p>
<p>The same application obtains
higher security with NTRU,
according to a security-estimation mechanism called "Core-SVP".
For example, the application can use </p>
<ul>
<li>
<p><code>sntrup653</code> (994-byte keys, 897-byte ciphertexts),
  where the Core-SVP security estimate is 2<sup>129</sup>,
  or</p>
</li>
<li>
<p>NTRU-677 (<code>ntruhps2048677</code>, 931-byte keys, 931-byte ciphertexts),
  where Core-SVP is 2<sup>145</sup>,</p>
</li>
</ul>
<p>while the current version of Kyber-512,
starting with the round-3 version from 2020,
has Core-SVP just 2<sup>118</sup>.</p>
<p>Is this "Core-SVP" something I made up to make Kyber look bad?
Absolutely not:</p>
<ul>
<li>
<p>Core-SVP is the security-estimation mechanism
that was <em>chosen by the Kyber team</em>
to estimate security levels
in its round-1 and round-2 submissions.
The mechanism was introduced by Kyber's predecessor, NewHope.</p>
</li>
<li>
<p>In 2020,
after I expressed
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/iyvpfk0hAQAJ">skepticism</a>
about whether Core-SVP
"gets the right ordering of security levels",
NIST
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/69c5Ph9vCAAJ">stated</a> that
"we feel that the CoreSVP metric does
indicate which lattice schemes are being more and less
aggressive in setting their parameters".
NIST's official
<a href="https://web.archive.org/web/20230903180546/https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8309.pdf">round-2 report</a>
in 2020
used Core-SVP for comparisons.</p>
</li>
<li>
<p>The original definition of Core-SVP assigns
2<sup>112</sup> to the round-3 version of Kyber-512.
Round-3 Kyber
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/NSe0wAzKJtA/m/dLP05gv7BgAJ">switched</a>
to a new definition of Core-SVP
that increases Kyber's Core-SVP
(without changing anything for NTRU).</p>
</li>
</ul>
<p>This blog post has bigger fish to fry,
so let's blindly accept Kyber's
claim that the new definition is better,
meaning that Kyber-512 has Core-SVP 2<sup>118</sup>.
That's still clearly worse than the
2<sup>129</sup> for <code>sntrup653</code> and the 2<sup>145</sup> for NTRU-677.</p>
<p>It's not that Kyber's competitors
<em>always</em> beat Kyber in size-security tradeoffs.
For example,
if an application instead has a limit of 1184 bytes,
then it can use Kyber-768, which has Core-SVP 2<sup>181</sup>,
while <code>ntruhps</code> needs 1230 bytes to reach Core-SVP 2<sup>179</sup>.</p>
<p>But Kyber's competitors
<em>often</em> beat Kyber in size-security tradeoffs.
Throwing away Kyber-512,
leaving just Kyber-768 and Kyber-1024,
means that Kyber has nothing as small as the 931 bytes for NTRU-677.</p>
<p><img src="https://blog.cr.yp.to/20231003/latticerisks-sizegraph.png">
The normal way for scientists to present quantitative tradeoffs is with scatterplots,
such as Figure 3.5 in my 2019 paper
<a href="https://cr.yp.to/papers.html#paretoviz">"Visualizing size-security tradeoffs for lattice-based encryption"</a>.
The particular scatterplot shown here is
Figure 7.3 in the 2021 paper
<a href="https://ntruprime.cr.yp.to/warnings.html">"Risks of lattice KEMs"</a>
from the NTRU Prime Risk-Management Team.
The vertical axis is the Core-SVP security estimate,
and the horizontal axis is ciphertext bytes.</p>
<p>The scatterplot shows that
Kyber has a higher Core-SVP than NTRU
for applications with a size limit of,
e.g., 768 bytes or 1088 bytes.
But NTRU has a higher Core-SVP than Kyber
for applications with a size limit of,
e.g., 700 bytes or 1024 bytes or 2048 bytes.
Kyber has nothing as small as the 699-byte option for NTRU.
Kyber also has nothing as strong as the 1842-byte option for NTRU.
NTRU is also trivially capable of adding further options
between and beyond what's shown in the graph,
whereas for Kyber this is more problematic.</p>
<p><strong>Official evaluation criteria for the competition.</strong>
NIST had issued an
<a href="https://web.archive.org/web/20220119113311/https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf">official call</a>
for post-quantum proposals in 2016.
One of the evaluation criteria in the call was as follows:</p>
<blockquote>
<p>Assuming good overall security and performance, schemes with greater
flexibility will meet the needs of more users than less flexible schemes,
and therefore, are preferable.</p>
</blockquote>
<p>One of the official examples given for "flexibility"
was that it is
“straightforward to customize the scheme's parameters to meet a
range of security targets and performance goals".</p>
<p>The call proposed five broad security "categories",
and said that submitters could specify even more than
five parameter sets to demonstrate flexibility:</p>
<blockquote>
<p>Submitters may also provide more than one parameter set in the same
category, in order to demonstrate how parameters can be tuned to offer
better performance or higher security margins.</p>
</blockquote>
<p>In 2020,
NIST eliminated NewHope.
One of the reasons stated
in the aforementioned
<a href="https://web.archive.org/web/20230903180546/https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8309.pdf">round-2 report</a>
was that
"KYBER naturally supports a category 3 security strength parameter set,
whereas NewHope does not".
NewHope offered only NewHope-512 and NewHope-1024.</p>
<p>Imagine Kyber similarly offering only Kyber-768 and Kyber-1024,
acknowledging that Kyber-512 doesn't meet the minimum security level specified by NIST.
It's then very easy to see how limited Kyber's flexibility is
compared to NTRU's broader, denser spectrum of security levels.
How, then, would NIST argue that Kyber is the best option?</p>
<p>One answer is that the evaluation criteria say more flexibility is preferable
only assuming "good overall security and performance".
But how would NIST argue that NTRU doesn't have "good overall security and performance"?</p>
<p>Regarding the security of Kyber and NTRU,
NIST's official 2022
<a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">selection report</a>
says that NIST is
"confident in the security that each provides".
The report describes MLWE, the problem inside Kyber,
as "marginally more convincing" than the problem inside NTRU.
There's much more that could and should have been said about
the security comparison between Kyber and NTRU:</p>
<ul>
<li>
<p>Kyber's use of modules,
  despite being portrayed as purely having a (marginal) security benefit,
  also introduces
  <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/BfnzYM8emOw/m/WHHQzM78AQAJ">extra subfields</a>
  into the cryptosystem structure,
  creating security risks
  analogous to the risks of taking extra subfields in pre-quantum DH.
  Fewer extra subfields appear in NTRU (depending on parameters) than in Kyber.
  NTRU Prime completely avoids extra subfields.</p>
</li>
<li>
<p>Kyber's QROM IND-CCA2 proof assuming MLWE hardness is much looser
  than NTRU's QROM IND-CCA2 proof assuming hardness of the problem inside NTRU.
  In other words,
  even under the assumption that MLWE is as strong as the problem inside NTRU,
  Kyber could be much weaker than NTRU.</p>
</li>
<li>
<p>NIST could have told people to use NTRU
  shortly after its deadline for NISTPQC input in 2021.
  Instead it delayed for three quarters of a year to carry out patent negotiations,
  and ended up telling people
  to wait for its Kyber patent license to activate in 2024,
  <strong>giving away three years of user data to attackers</strong>.
  Picking Kyber was doing obvious damage to security
  given the patent situation.</p>
</li>
</ul>
<p>The situation isn't that NTRU avoids <em>every</em> security risk of Kyber.
A <a href="https://ntruprime.cr.yp.to/warnings.html">careful comparison</a>
finds mathematical security risks in both directions.
Maybe there's a way to argue that the mathematical security risks for NTRU
should be given higher weight than the mathematical security risks for Kyber.
But the immediate choice that NIST was facing in 2021 between NTRU and Kyber,
assuming that the attackers currently recording user data
will have quantum computers in the future, was between</p>
<ul>
<li>
<p>the security risks of NTRU and</p>
</li>
<li>
<p>the guaranteed security failure of not yet deploying anything.</p>
</li>
</ul>
<p>The call for submissions said
"NIST believes it is critical that this process leads to cryptographic
standards that can be freely implemented in security technologies and products".
Nothing else in the call was labeled as "critical".
How could NIST ignore the damage that it was doing in not going ahead with NTRU?
NIST knew it didn't have a patent license signed for Kyber yet,
let alone an <em>activated</em> patent license.</p>
<p>Anyway,
let's get back to the question of how NIST might be able to argue
that NTRU doesn't have "good overall security and performance".
A report saying that NIST is
"confident in the security that each provides"
is obviously not claiming that NTRU doesn't have "good overall security".
What about performance?</p>
<p>The same selection report admits that
"the overall performance of any of these KEMs
would be acceptable for general-use applications".
If the objective is to use performance differences as a deciding factor
between two acceptable options,
let's see how Kyber would stack up without Kyber-512:
<img src="https://blog.cr.yp.to/20231003/latticerisks-sizegraph-no512.png"></p>
<ul>
<li>
<p>Kyber-768 and Kyber-1024 provide size-security tradeoffs that NTRU doesn't match.</p>
</li>
<li>
<p>NTRU-677 and NTRU-1229
  provide size-security tradeoffs that Kyber doesn't match.
  Even more options are already implemented for NTRU Prime.</p>
</li>
<li>
<p>The smallest options are from NTRU, not Kyber.</p>
</li>
<li>
<p>The highest-security options are from NTRU, not Kyber.</p>
</li>
</ul>
<p>This is a solid case for eliminating Kyber in favor of NTRU,
given NIST's declaration that there can be only one.</p>
<p>(If NIST thought that performance differences at this scale matter,
and if the best performance comes from Kyber at some security levels
and NTRU at other security levels,
then why wasn't NIST allowing both?
Answer:
The movie says there can be only one!
STOP ASKING QUESTIONS!)</p>
<p><strong>Tilting the competition, part 1: ignoring NTRU's extra flexibility.</strong>
Keeping Kyber-512 changes the competition.
Having three options, Kyber-512 and Kyber-768 and Kyber-1024,
looks a lot better than having just two.</p>
<p>There are four NTRU circles in the first scatterplot above,
namely NTRU-509 and NTRU-677 and NTRU-821 and NTRU-1229.
But NTRU-821 isn't a winner,
and earlier in NISTPQC there wasn't an NTRU-1229.</p>
<p>Wait a minute.
The NTRU literature has always made clear that NTRU supports many more options.
For example,
here's a scatterplot
from John Schanck's 2018 paper
<a href="https://eprint.iacr.org/2018/1174">"A comparison of NTRU variants"</a>.
<img src="https://blog.cr.yp.to/20231003/schanck-ntru.png">
There are a huge number of dots;
each dot is showing another NTRU option.</p>
<p>One of the bizarre twists in NISTPQC
was the following <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/LPuZKGNyQJ0/m/ZUoZZss5AwAJ">announcement</a>
from NIST in 2020:
"NIST believes that too many parameter sets make
evaluation and analysis more difficult."
I asked
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/LPuZKGNyQJ0/m/XchLDA3HAwAJ">various questions</a>
about this, starting as follows:</p>
<blockquote>
<p>How many is "too many"? How did flexibility, which was portrayed as
purely positive in the call for proposals, turn into a bad thing for
NIST? The call for proposals explicitly allowed multiple parameter sets
<em>per category</em>, never suggesting that this would be penalized!</p>
<p>NIST's latest report complains about NewHope's lack of flexibility to
use dimensions strictly between 512 and 1024. If a submission team is
thinking "Aha, Kyber similarly suffers from its lack of flexibility to
target security levels strictly between maybe-2<sup>128</sup> and maybe-2<sup>192</sup>, and
we can clearly show this to NIST by selecting parameter sets at several
intermediate security levels", then isn't this something NIST should be
interested in, rather than discouraging by making submitters worry that
this is "too many parameter sets"?</p>
</blockquote>
<p>NIST never replied.</p>
<p>Think about what this is like for
submitters trying to figure out what to do:</p>
<ul>
<li>
<p>The official evaluation criteria say flexibility is good.</p>
</li>
<li>
<p>A high-profile submission has just been eliminated,
  in part for having only two parameter sets.</p>
</li>
<li>
<p>So, okay, implement more parameter sets
  to demonstrate flexibility.</p>
</li>
<li>
<p>But, yikes, NIST is suddenly going out of its way
  to criticize "too many" parameter sets.
  They won't say what "too many" means
  and where this criticism came from.</p>
</li>
</ul>
<p>NTRU Prime
moved up to selecting six <code>sntrup</code> parameter sets
(plus six <code>ntrulpr</code> parameter sets, which, compared to <code>sntrup</code>,
have larger ciphertexts but smaller public keys),
enough that the flexibility advantage over Kyber should have been impossible to ignore.
NIST ignored it.</p>
<p><strong>Tilting the competition, part 2: exaggerating and hyping key-generation costs.</strong>
For Intel's recent Golden Cove microarchitecture
(the "performance" cores in Alder Lake CPUs),
<a href="https://bench.cr.yp.to/"><code>https://bench.cr.yp.to</code></a>
reports that</p>
<ul>
<li>
<p>Kyber-512 takes 25829 cycles for encapsulation
  and 20847 cycles for decapsulation,
  while</p>
</li>
<li>
<p>NTRU-509 takes just 15759 cycles for encapsulation
  and 25134 cycles for decapsulation.</p>
</li>
</ul>
<p>The total cycle count for handling a ciphertext,
the total of encapsulation and decapsulation,
is 13% smaller for NTRU-509 than for Kyber-512.</p>
<p>NTRU-509 also beats Kyber-512 in ciphertext size.
NTRU-509 is the leftmost dot in the first scatterplot above,
meaning smallest ciphertexts.</p>
<p>On the other hand,
NTRU-509 takes 112866 cycles for key generation
while Kyber-512 takes only 17777 cycles.
The total of key generation plus encapsulation plus decapsulation
is more than twice as large for NTRU-509 as for Kyber-512.</p>
<p>When some factors favor one option and some factors favor another option,
someone objectively searching for the best option
will think about what weight to put on each factor.
Here are three reasons that a careful performance analysis
will put very low weight on Kyber's key-generation speedup:</p>
<ul>
<li>
<p>There's overwhelming evidence that these cycle counts
  are far less important than byte counts.
  A useful rule of thumb is that sending or receiving a byte
  has similar cost to 1000 cycles;
  see Section 6.6 of the aforementioned paper
  <a href="https://ntruprime.cr.yp.to/warnings.html">"Risks of lattice KEMs"</a>.
  Sending a key, receiving a key, sending a ciphertext, and receiving a ciphertext
  involves thousands of bytes, similar cost to millions of cycles.</p>
</li>
<li>
<p>All of these KEMs are designed to allow a key to be reused for many ciphertexts.
  If an application actually cares about the cost of key generation
  then this reuse is an obvious step to take.
  NIST's
  <a href="https://web.archive.org/web/20220119113311/https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf">official evaluation criteria</a>
  already acknowledged the possibility
  that "applications can cache public keys,
  or otherwise avoid transmitting them frequently".
  Many applications are naturally reusing keys in any case.</p>
</li>
<li>
<p>Even in the extreme case of an application that structurally has to use
  a new key for each ciphertext,
  there's a trick due to Montgomery that makes NTRU key generation much faster.
  Billy Bob Brumley, Ming-Shing Chen, Nicola Tuveri, and I
  have a paper
  <a href="https://cr.yp.to/papers.html#opensslntru">"OpenSSLNTRU: Faster post-quantum TLS key exchange"</a>
  at USENIX Security 2022
  giving a web-browsing demo on top of TLS 1.3 using <code>sntrup761</code>
  with Montgomery's trick for key generation.
  We already had the paper and code online in 2021,
  before NIST's deadline for input regarding NISTPQC decisions.</p>
</li>
</ul>
<p>In other words:
If an average key is used for just 100 ciphertexts
then Kyber-512 saving 95089 Golden Cove cycles in key generation is</p>
<ul>
<li>
<p>of similar importance to changing ciphertext size by <em>a fraction of a byte</em>;</p>
</li>
<li>
<p>6x less important
  than NTRU-509 saving 5783 cycles per ciphertext;
  and</p>
</li>
<li>
<p>not what will happen in applications trying to optimize key-generation time,
  since in NTRU's case those applications will use Montgomery's trick.</p>
</li>
</ul>
<p>With this in mind,
let's look at the "Kyber vs NTRU vs Saber" slide
from NIST's March 2022 talk
<a href="https://web.archive.org/web/20220811213554/https://csrc.nist.gov/csrc/media/Presentations/2022/the-beginning-of-the-end-the-first-nist-pqc-standa/images-media/pkc2022-march2022-moody.pdf">"The beginning of the end: the first NIST PQC standards"</a>.</p>
<p><img src="https://blog.cr.yp.to/20231003/how-to-lie-with-graphs.png">
The eye is immediately drawn to the larger red bars on the right.
NTRU appears in two of the groups of bars,
in both cases with clearly larger bars,
meaning worse performance.</p>
<p>The main message NIST is communicating here is that
NTRU costs strikingly more than Kyber and Saber.
Only a small part of the audience
will go to the effort of checking the numbers
and seeing how NIST manipulated
the choices in its presentation to favor Kyber over NTRU:</p>
<ul>
<li>
<p>The graph gives 100% weight to key generation,
  utterly failing to account for key reuse.</p>
</li>
<li>
<p>The graph also
  utterly fails to account for Montgomery's trick.</p>
</li>
<li>
<p>The graph does include some recognition of communication costs,
  but even here NIST couldn't resist tweaking the numbers:
  "1000*(PK+CT)" counts Alice's cost while omitting Bob's cost.</p>
</li>
</ul>
<p>Regarding the last point:
1000 is just a rule of thumb.
NIST could have posted a rationale for a proposal to use 500
and asked for public comments.
But it didn't.</p>
<p>NIST's
<a href="https://nist.pqcrypto.org/foia/index.html#20230105/KSN%20Document.docx">secret October 2021 Kyber-SABER-NTRU comparison</a>
claimed, without citation, that I had said 1000*(PK+CT) was reasonable.
Compare this to what I had
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/VK9dROwgY0Y/m/PIXc9wTtCAAJ">actually written</a>
in 2019
about the costs of sending and receiving a ciphertext,
after various NTRU Prime documents had given examples backing up the first sentence:</p>
<blockquote>
<p>Typically sending or receiving a byte costs at least three orders of
magnitude more than a clock cycle. Taking bytes+cycles/1000 for
sntrup4591761 gives 1047+45 = 1092 for the sender, 1047+94 = 1141 for
the receiver, which is better than 1248 no matter how few cycles you
use.</p>
</blockquote>
<p>The numbers here account for Alice sending a 1047-byte <code>sntrup4591761</code> ciphertext
and Bob receiving a 1047-byte ciphertext,
on top of about 45000 Haswell cycles for Alice's enc
and about 94000 cycles for Bob's dec
(which was later sped up a lot,
but this barely matters next to the ciphertext sizes).
See also the more detailed NTRU examples in Section 6.6 of <a href="https://ntruprime.cr.yp.to/warnings.html">"Risks of lattice KEMs"</a>,
filed before NIST's deadline for input at the end of October 2021.</p>
<p>NIST's secret comparison continued by saying "David suggests 2000?",
apparently referring to a
<a href="https://nist.pqcrypto.org/foia/index.html#20221213/PQC%20KEM%20Benchmarks%2020200407.pdf">secret performance comparison in 2020</a>
where NIST used "bandwidth cost of 2000 cycles/byte".
Evidently NIST was considering multiple options for this number.
Maybe more FOIA results
will shed more light on how exactly NIST ended up with a NIST-fabricated option
that—<em>quelle surprise!</em>—is better for Kyber.</p>
<p>As for key reuse,
NIST might try to defend itself by saying, look, there's
a separate PK+CT bandwidth graph on the left,
which for these KEMs is visually close to a 2000*CT+enc+dec graph.
However:</p>
<ul>
<li>
<p>NIST chose to deemphasize the bandwidth graph by using thinner red bars for it.</p>
<p>The graph isn't invisible,
so together the two graphs don't give exactly 100% weight to key-generation time.
But a key used for 100 ciphertexts
incurs 1 keygen, 100 enc, and 100 dec,
meaning only 1% weight for key-generation time,
which is <em>very</em> far from the weight
conveyed by NIST's slide.</p>
</li>
<li>
<p>NIST chose to use smaller (and non-log)
  vertical scales for the bandwidth graph.
  This further deemphasizes that graph <em>and</em>
  makes it hard for the audience to notice
    the size advantage of
    NTRU-509 (699-byte keys and 699-byte ciphertexts)
    over Kyber-512 (800-byte keys and 768-byte ciphertexts).</p>
<p>NTRU-509's savings of 170 bytes in key+ciphertext size
compared to Kyber-512
is comparable to saving 340000 cycles in total for Alice and Bob.
This easily outweighs the cost of NTRU-509 key generation,
even in the extreme case of one ciphertext per key,
even <em>without</em> Montgomery's trick,
even if one rewinds a decade from Alder Lake to Haswell.</p>
<p>In short, NTRU-509's size advantage
is more important than Kyber-512's keygen-time advantage.
But NIST chose to give more vertical space to Kyber's keygen-time advantage
than to NTRU-509's size advantage.</p>
</li>
<li>
<p>NIST applied a <a href="https://cr.yp.to/papers.html#categories">discretization attack</a>
  to both graphs
  to conceal the security advantages of the larger NTRU options.</p>
<p>If NIST had provided an honest size-vs.-Core-SVP scatterplot,
then readers would have seen
that NTRU-677 has much higher Core-SVP than Kyber-512
and much better size than Kyber-768.
NIST would never have been able to get away with its
<a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">claim</a>
that NTRU has "somewhat larger public keys and ciphertexts" than Kyber:
a scatterplot immediately shows that,
no, this depends on the target security level,
with NTRU smaller at some security levels
and Kyber smaller at others.</p>
<p>Instead NIST started with the options in Core-SVP order
and then <em>grouped the options according to "category"</em>.
Because of this grouping,
the options <em>look like</em> they have some arbitrary order within each "category".
People looking at the graph
have no idea that NTRU's placement farther to the right in each "category"
reflects NTRU's higher security levels.
A different choice of "category" cutoffs
would have reversed the visual comparison.</p>
</li>
</ul>
<p>As for the failure to account for Montgomery's trick,
NIST might try to defend itself
by saying that the OpenSSLNTRU software focused on NTRU Prime,
so NIST's only choice was to presume that there's no speedup for NTRU beyond NTRU Prime.
In fact, the OpenSSLNTRU paper had already explained why there will be about a 2x speedup.</p>
<p><strong>Tilting the competition, part 3: concealing the fact that NTRU offers the highest security levels.</strong>
The official call for submissions in 2016
recommended focusing on "categories 1, 2 and/or 3".
See below for a full quote.</p>
<p>The call also recommended that submitters
"specify some other level of security that demonstrates the ability of
their cryptosystem to scale up beyond category 3".
NTRU (and NTRU Prime) did this,
specifying parameters across a wide range of security levels.
See, e.g., the 2018 scatterplot shown above.</p>
<p>In the aforementioned
<a href="https://web.archive.org/web/20230903180546/https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8309.pdf">round-2 report</a>
from 2020, NIST suddenly</p>
<ul>
<li>
<p>said that it
  "strongly encourages the
  submitters to provide at least one parameter set that meets category 5",</p>
</li>
<li>
<p>complained that
  "the NTRU submission lacks a category 5 parameter set proposal"
  when the costs of memory are ignored,
  and</p>
</li>
<li>
<p>complained that NTRU Prime provided
  "a narrower range of CoreSVP
  values than other lattice submissions targeting security strengths 1, 3, and 5".</p>
</li>
</ul>
<p>This wasn't following the official evaluation criteria.
NIST was retroactively changing "recommend" to "strongly encourage",
was retroactively changing "beyond category 3" to "category 5",
and was ignoring all of the existing documentation of NTRU's flexibility.</p>
<p>Submissions that provided "category 4",
or provided higher security within "category 3",
were fully meeting the recommendation in the official evaluation criteria:</p>
<blockquote>
<p>Submitters may also provide more than one parameter set in the same
category, in order to demonstrate how parameters can be tuned to offer better
performance or higher security margins.</p>
<p>NIST recommends that submitters primarily focus on parameters meeting the
requirements for categories 1, 2 and/or 3, since these are likely to provide sufficient
security for the foreseeable future. To hedge against future breakthroughs in cryptanalysis
or computing technology, NIST also recommends that submitters provide at least one
parameter set that provides a substantially higher level of security, above category 3.
Submitters can try to meet the requirements of categories 4 or 5, or they can specify some
other level of security that demonstrates the ability of their cryptosystem to scale up
beyond category 3.</p>
</blockquote>
<p>But, in 2020,
NIST wasn't even trying to follow the official evaluation criteria.
It was inventing new evaluation criteria, with no warning,
and retroactively applying
those criteria to criticize the NTRU and NTRU Prime submissions.</p>
<p>Unsurprisingly,
those submissions responded with software for higher security levels:</p>
<ul>
<li>
<p>NTRU responded with reference implementations of NTRU-1229 and NTRU-HRSS-1373.
  The NTRU team didn't provide optimized implementations
  (maybe it ran out of time, which is NIST's fault
  for not having asked for category 5
  in the official call four years earlier),
  but it reported that NTRU-1229 has Core-SVP 2<sup>301</sup>
  and that NTRU-HRSS-1373 has Core-SVP 2<sup>310</sup>.
  Both of these are solidly above Kyber-1024's 2<sup>254</sup>.</p>
</li>
<li>
<p>NTRU Prime responded with reference and optimized implementations of various options,
  such as <code>sntrup1277</code> and <code>ntrulpr1277</code>,
  which have Core-SVP 2<sup>270</sup> and 2<sup>271</sup> respectively,
  again above anything Kyber offers.
  (There's a code generator automatically
  producing all of the official NTRU Prime implementations;
  the generator is
  easily extensible to cover further parameter sets.)</p>
</li>
</ul>
<p>After insisting on higher security levels
(and adopting Core-SVP)
in its 2020 round-2 report,
NIST praised NTRU
for responding with higher security levels
(as measured by Core-SVP)
than Kyber, right?</p>
<p>Of course not.
NIST concealed the fact
that NTRU was offering higher security levels than Kyber:</p>
<ul>
<li>
<p>NIST's big graph
  doesn't show any NTRU options in the top "category".
  (The cover story writes itself:
  The NTRU submission didn't provide optimized software for the new options!
  Reporting reference speeds would have been unfair!
  NIST is just trying to protect readers from being misled!)</p>
</li>
<li>
<p>For readers who go to the effort of looking at the small graph,
  the discretization attack
  makes NTRU's higher security levels
  look just like Kyber's lower security levels.</p>
</li>
</ul>
<p>Readers looking at NIST's graphs are left with the impression
that NTRU is <em>less</em> flexible than Kyber
and, in particular,
has <em>more</em> trouble reaching high security levels.
This is exactly the opposite of the facts.</p>
<p><strong>Tilting the competition, part 4: throwing away the highest-performance option.</strong>
NTRU-1229 and NTRU-HRSS-1373 aren't the only options
that NIST excluded from its big graph.
Let's again look at the low end,
the top-performance end,
where NIST chose to exclude NTRU-509.</p>
<p>Optimized NTRU-509 software was already available.
If NIST had included NTRU-509 in the big graph
then that graph would have shown NTRU-509 as the best performer,
better than Kyber-512.</p>
<p>Accounting for key reuse
would have further favored NTRU-509.
Accounting for Montgomery's trick
would have further favored NTRU-509.
Upgrading from Haswell
would have further favored NTRU-509.
Counting 1000 cycles per byte for Alice <em>and</em> for Bob
would have further favored NTRU-509.</p>
<p>But NIST simply removed NTRU-509 from the big graph,
making NTRU look strictly worse than Kyber in that graph.</p>
<p>NIST went even further in its subsequent report
selecting Kyber for standardization:
<a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">the report</a>
didn't show NTRU-509 in any of the figures or tables.
The report's descriptions of Kyber's performance
were visibly more positive
than its descriptions of NTRU's performance,
as illustrated by
NIST's claim that NTRU has "somewhat larger public keys and ciphertexts" than Kyber.</p>
<p>How does NIST stop people from quickly spotting the errors in
this "somewhat larger public keys and ciphertexts" claim?</p>
<p>A discretization attack
easily hides the fact that NTRU has smaller sizes than Kyber at intermediate security levels,
but it doesn't hide NTRU-509 being smaller than Kyber-512.
NIST's narrative also relied on kicking out NTRU-509.</p>
<p>How can NIST justify throwing NIST-509 away?</p>
<p>The only possible answer
is claiming that NTRU-509 doesn't reach the minimum allowed NISTPQC security level,
the security level of AES-128.
But, at the same time, NIST is including Kyber-512,
so NIST is claiming that Kyber-512
<em>does</em> reach the security level of AES-128.</p>
<p>NTRU-509 has Core-SVP 2<sup>106</sup>, just
6 bits below Kyber-512's original Core-SVP (2<sup>112</sup>)
or
12 bits below Kyber-512's revised Core-SVP (2<sup>118</sup>).
Evidently NIST is claiming that AES-128 is inside this narrow margin:
in other words, that
NTRU-509 has <em>slightly</em> lower security than AES-128
while Kyber-512 has <em>slightly</em> higher security than AES-128.</p>
<p><img src="https://blog.cr.yp.to/20231003/shattering.png">
Let's take a moment to admire how spectacularly fragile this is:</p>
<ul>
<li>
<p>If some effect slightly increases lattice security levels
  compared to what NIST is claiming,
  then NTRU-509 is back in the game,
  outperforming all of the Kyber options.</p>
</li>
<li>
<p>If some effect slightly reduces lattice security levels
  compared to what NIST is claiming,
  then Kyber-512 is gone,
  and NTRU-677 outperforms all of the Kyber options.</p>
</li>
<li>
<p><em>If</em> security levels are measured in a way that
<em>just</em> manages to have Kyber-512 retained while NTRU-509 isn't retained,
then NTRU's superior flexibility still provides the highest security level
and wins at various intermediate levels,
but Kyber wins at other intermediate levels
and provides the highest performance level,
so putting enough weight on the highest performance level favors Kyber.</p>
</li>
</ul>
<p>See how important it is
for Kyber-512 to reach the AES-128 security level?
Without that, Kyber is in big trouble:
NTRU provides the highest level of security
<em>and</em> the highest level of performance
<em>and</em> the best flexibility.</p>
<p><strong>The chaos beyond Core-SVP.</strong>
How is Kyber-512 supposed to reach the AES-128 security level
if AES-128 needs more than 2<sup>140</sup> bit operations to break
while the Core-SVP security estimate for Kyber-512 is only 2<sup>118</sup>?</p>
<p>This question was briefly addressed in the
<a href="https://web.archive.org/web/20230323095809/https://pq-crystals.org/kyber/data/kyber-specification.pdf">round-1 Kyber submission</a>
in 2017.
That submission said that the 2017 version of Kyber-512 had Core-SVP 2<sup>112</sup>,
falling short of the target by 30 bits,
but gave a five-line list of reasons that "it seems clear"
that Kyber-512 has at least 30 bits more security than Core-SVP indicates.</p>
<p>The
<a href="https://web.archive.org/web/20230919163511/https://pq-crystals.org/kyber/data/kyber-specification-round2.pdf">round-2 Kyber submission</a>
in 2019
made the same claim regarding the 2019 version of Kyber-512.
In 2020,
I <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/EHW9h87kAAAJ">disproved</a>
the stated rationale.
To summarize:</p>
<ul>
<li>
<p>Kyber's argument that it was gaining security from
  "the additional rounding noise (the LWR problem, see [13, 8]), i.e. the
  deterministic, uniformly distributed noise introduced in ciphertexts via [rounding]" was simply wrong.
  Attackers could freely target Kyber's keys, and the keys didn't have any rounding.</p>
</li>
<li>
<p>Kyber's argument that it was gaining security from the
  "additional cost of sieving with asymptotically subexponential complexity"
  was <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/5ORleAt4AQAJ">unfounded and probably wrong</a>:
  as far as I could tell (and as far as we know today),
  the actual asymptotics are subexponentially <em>faster</em> than Core-SVP,
  not subexponentially slower.
  It was still plausible that the costs for specific sizes such as Kyber-512
  were higher than Core-SVP,
  but this required an analysis that Kyber hadn't carried out.</p>
</li>
<li>
<p>Kyber's arguments that it was gaining security from
  "the (polynomial) number of calls to the SVP oracle that are
  required to solve the MLWE problem"
  and "the gate count required for one 'operation' "
  were plausible, but didn't seem to be enough to rescue Kyber-512 without further help.</p>
</li>
<li>
<p>Kyber's argument that it was gaining security from
  "the cost of access into exponentially large memory"
  was plausible as a matter of real-world attack costs.
  NTRU Prime had already proposed a particular way to quantify this cost.</p>
<p>However,
the
<a href="https://web.archive.org/web/20220119113311/https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf">official call for submissions</a>
had asked for a security level of at least 2<sup>143</sup> "classical gates"
without regard to memory-access costs.
So this argument was useless for rescuing Kyber-512:
it wasn't what the official evaluation criteria were asking for.</p>
</li>
</ul>
<p>To try to rescue Kyber-512,
the
<a href="https://web.archive.org/web/20230310174959/https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf">round-3 Kyber submission</a></p>
<ul>
<li>
<p>changed Kyber-512 and (as noted above) redefined Core-SVP
  to obtain Core-SVP 2<sup>118</sup> rather than 2<sup>112</sup>;</p>
</li>
<li>
<p>took (without credit) my <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/5ORleAt4AQAJ">preliminary analysis</a>
  of the gaps between Core-SVP and reality;</p>
</li>
<li>
<p>added further numerical estimates regarding the gaps
  and the "known unknowns";</p>
</li>
<li>
<p>concluded that this preliminary analysis
  gave a <strong>32-bit range of security estimates</strong>,
  specifically 151 bits plus or minus 16 "in either direction";
  and</p>
</li>
<li>
<p>claimed that dropping to 135 wouldn't be "catastrophic,
  in particular given the massive memory requirements
  that are ignored in the gate-count metric".</p>
</li>
</ul>
<p>The memory argument again wasn't relevant,
given the official evaluation criteria asking for 2<sup>143</sup> "classical gates".
Kyber-512 wasn't claiming to require 2<sup>143</sup> "classical gates" to break;
it was claiming some undetermined number between 2<sup>135</sup> and 2<sup>167</sup>.</p>
<p>Various papers then appeared
claiming to cut further bits out of lattice security in various ways,
such as a <a href="https://eprint.iacr.org/2022/239">2022 paper</a>
reporting an order-of-magnitude speedup
from tweaking the "BKZ" layer inside attacks.
Many of the papers made the analyses of lattice security
even more complicated and even less stable than before.
For example, for one line of "dual attacks":</p>
<ul>
<li>
<p>there's an Asiacrypt paper and a paper from Israel's Matzov organization
  with complicated analyses claiming to reduce Kyber-512's 151 to 137;</p>
</li>
<li>
<p>but then there's a Crypto paper
  <a href="https://eprint.iacr.org/2023/302">"Does the dual-sieve attack on learning with errors even work?"</a>
  giving the impression that, no, this whole line of attacks fails;</p>
</li>
<li>
<p>but then the actual contents of the Crypto paper
  are merely saying that there's a "presumably significant" change in the improvements without <em>quantifying</em> the change;</p>
</li>
<li>
<p>but then there's a new paper
  <a href="https://eprint.iacr.org/2023/1238">"A remark on the independence heuristic in the dual attack"</a>
  that sounds like it's helping quantify the change;</p>
</li>
<li>
<p>but that paper still doesn't get all the way to
  claiming any particular attack cost for Kyber-512;</p>
</li>
<li>
<p>but then there's another new paper
  <a href="https://eprint.iacr.org/2023/1460">"Rigorous foundations for dual attacks in coding theory"</a>
  that, for a dual attack against an analogous low-rate decoding problem,
  says that a "slight modification of this algorithm"
  avoids the issue raised in the Crypto paper;</p>
</li>
<li>
<p>but that paper doesn't analyze what the idea means for lattices;</p>
</li>
<li>
<p>but then there's another new paper
  <a href="https://eprint.iacr.org/2023/1508">"Provable dual attacks on learning with errors"</a>
  that says it proves the correctness of a simplified dual attack for lattices;</p>
</li>
<li>
<p>but that paper also doesn't quantify consequences for Kyber-512.</p>
</li>
</ul>
<p>And this is just one small piece of a giant unholy mess
that some cryptographers say we should trust.</p>
<p>How, back in 2022, did NIST end up concluding that Kyber-512 is as hard to break as AES-128?
Time to look at some quotes.
I'll go through the quotes in two parts:
first, looking at what NIST said its notion of hardness was;
second, going line by line through what NIST said about Kyber-512's security level.</p>
<p><strong>NIST rescuing Kyber-512, part 1: manipulating the qualification criteria.</strong>
In the call for submissions,
it was crystal clear that cryptosystems had to be at least as hard to break as AES-128
in <em>every</em> "potentially relevant" cost metric:</p>
<blockquote>
<p>Each category will be defined by a comparatively easy-to-analyze reference primitive,
whose security will serve as a floor for a wide variety of metrics that NIST deems
potentially relevant to practical security. ...</p>
<p>In order for a
cryptosystem to satisfy one of the above security requirements, any attack must require
computational resources comparable to or greater than the stated threshold, with respect
to <em>all</em> metrics that NIST deems to be potentially relevant to practical security.</p>
</blockquote>
<p>(Emphasis in original.)</p>
<p>The call commented on the "classical gates" to break AES-128 etc.
Obviously "classical gates" were a "potentially relevant" cost metric.</p>
<p>What exactly is this metric?
The literature defines many different gate sets.
NIST dodged years of requests to define exactly which gates
it was including as "classical gates".
NIST's 2022
<a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">selection report</a>
finally pinned down one part of this,
allowing "each one-bit memory read or write" as a cost-1 gate.</p>
<p>Here's an illustration of how important definitions of cost metrics are:</p>
<ul>
<li>
<p>Kyber's security analysis relies on
  an <a href="https://eprint.iacr.org/2019/1161">Asiacrypt 2020 paper</a>
  for counting the number of "gates" inside the most important attack step
  inside "primal" attacks.</p>
</li>
<li>
<p>Tung Chou and I have a new paper
  <a href="https://cat.cr.yp.to/papers.html">"CryptAttackTester: formalizing attack analyses"</a>
  including an appendix that, for Kyber-512,
  <strong>cuts almost 10 bits out of the "gate" count</strong>
  for the "primary optimisation target" in the Asiacrypt 2020 paper,
  exploiting the fact that the Asiacrypt 2020 paper counts a memory-access "gate" as cost 1.
  (The Asiacrypt 2020 paper also relies on this; it's not a typo in that paper.)</p>
</li>
<li>
<p>The same appendix also disproves the claim
  that an "optimal" AES-128 key search requires 2<sup>143</sup> "gates",
  but the reduction in AES-128 "gate" counts
  isn't as large as the reduction in Kyber-512 "gate" counts.</p>
</li>
</ul>
<p>Keep this in mind if you hear people claiming that the costs of lattice attacks have been thoroughly analyzed.</p>
<p>Anyway,
being able to access arbitrarily large amounts of memory for cost 1 isn't realistic:
the actual costs of data communication grow with distance.
But NIST said in 2020
that anyone proposing a replacement metric
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/o2roJXAlsUk/m/9oeKbY5MAQAJ">"must at minimum convince NIST that the metric meets the following criteria"</a>,
which "seems to us like a fairly tall order":</p>
<ul>
<li>
<p>"The value of the proposed metric can be accurately measured (or at least lower
  bounded) for all known attacks (accurately mere means at least as accurately as for
  gate count.)"</p>
</li>
<li>
<p>"We can be reasonably confident that all known attacks have been
  optimized with respect to the proposed metric. (at least as confident
  as we currently are for gate count.)"</p>
</li>
<li>
<p>"The proposed metric will more accurately reflect the real-world
  feasibility of implementing attacks with future technology than gate
  count -- in particular, in cases where gate count underestimates the
  real-world difficulty of an attack relative to the attacks on AES or
  SHA3 that define the security strength categories."</p>
</li>
<li>
<p>"The proposed metric will not replace these underestimates with overestimates."</p>
</li>
</ul>
<p>There have been no announcements on the NISTPQC mailing list
of anyone claiming to have met these minimum criteria,
never mind the question of whether such a claim could survive public scrutiny.</p>
<p>Recall that
NIST excluded NTRU-509
from the figures and tables in its selection report,
the report announcing the selection of Kyber over NTRU.
If you look for the report's explanation of <em>why</em> NIST excluded NTRU-509,
you'll find the following quote:</p>
<blockquote>
<p>The submission specification uses both local and non-local cost models for determining
the security category of their parameter sets. For a more direct comparison with the other
KEM finalists, the assignment of security categories according to the non-local cost model
is appropriate. This is what NIST used for NTRU in the figures and tables in this report.</p>
</blockquote>
<p>The underlying definition of "local" accounts for long-distance communication costs,
whereas "non-local" allows accessing arbitrarily large amounts of memory for free.</p>
<p>Everything I've been describing from NIST above, and more,
sounds consistent with the official call for submissions asking for
2<sup>143</sup> "classical gates",
<strong>not counting the costs of memory access</strong>:</p>
<ul>
<li>
<p>To try to avoid overestimating security levels,
  NIST was insisting on counting <em>just</em> bit operations for computation,
  ignoring the costs of communication.</p>
</li>
<li>
<p>In response to the round-1 NTRU Prime submission,
  which provided a
  <a href="https://ntruprime.cr.yp.to/nist/ntruprime-20171130.pdf#subsection.6.7">detailed rationale</a>
  for including the costs of memory access,
  NIST complained in its
  <a href="https://web.archive.org/web/20230920201351/https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8240.pdf">round-1 report</a>
  that the submission
  "uses a cost model for lattice attacks
  with higher complexity than many of the other lattice-based candidates".
  (NTRU Prime started reporting Core-SVP in round 2.)</p>
</li>
<li>
<p>In its <a href="https://web.archive.org/web/20230903180546/https://nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8309.pdf">round-2 report</a>,
  as noted above,
  NIST complained that
  "the NTRU submission lacks a category 5 parameter set proposal"
  <em>when memory-access costs are ignored</em>.</p>
</li>
<li>
<p>In its
  <a href="https://web.archive.org/web/20230824124130/https://nvlpubs.nist.gov/nistpubs/ir/2022/NIST.IR.8413-upd1.pdf">selection report</a>,
  NIST kicked out NTRU-509
  because NTRU-509's "category 1" claim
  relied on a "local cost model", i.e., accounting for memory-access costs;
  see above for the full quote.</p>
</li>
</ul>
<p>With this in mind,
consider the fact that NIST was including Kyber-512 in its figures and tables in the same report.
This must mean that NIST was claiming
that breaking Kyber-512 takes at least 2<sup>143</sup> bit operations,
<em>without accounting for memory-access costs</em>, right?</p>
<p>Nope. NIST doesn't ask Kyber
to meet the same criteria as other submissions.</p>
<p>In <a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/-1Ja0ZYyAQAJ">November 2022</a>,
NIST announced a list of parameter sets that it was "planning" to standardize,
including Kyber-512.
NIST's announcement
avoided claiming that Kyber requires as many "classical gates" to break as AES-128.
The announcement
specifically acknowledged the possibility of Kyber being "a few bits" below
(while omitting the possibility of Kyber being many more bits below):</p>
<blockquote>
<p>It is clear that in the gate-count metric it is a very close call and
that in this metric the pre-quantum security of Kyber-512 may be a few
bits below the one of AES-128.</p>
</blockquote>
<p>Instead the announcement relied on accounting for "realistic memory access costs"
to claim that Kyber-512 qualified for "category 1":</p>
<blockquote>
<p>... the best known attacks against Kyber-512 require huge amounts
of memory and the real attack cost will need to take the cost of
(access to) memory into account.  This cost is not easy to calculate,
as it depends on the memory access patterns  of the lattice algorithms
used for cryptanalysis, as well as the physical properties of the
memory hardware.  Nonetheless, barring major improvements in
cryptanalysis, it seems unlikely that the cost of memory access will
ever become small enough to cause Kyber-512 to fall below category 1
security, in realistic models of security that take these costs into
account.  We acknowledge there can be different views on our current
view to include Kyber-512.</p>
<p>As a point of clarification: in this email, we refer to parameter sets
based on the claimed security strength category where those parameter
sets are most recently specified, irrespective of whether those
parameter sets actually meet their claimed security level. That said,
our current assessment is that, <strong>when realistic memory access costs of
known attacks are taken into account, all the parameter sets we plan
to standardize do, in fact, meet their claimed security strength
categories</strong>.</p>
</blockquote>
<p>(Emphasis added.)</p>
<p><img src="https://blog.cr.yp.to/20231003/inconsistency.png">
So NIST used a "non-local" free-memory metric to kick out NTRU-509,
but used a memory-access-is-expensive metric
to claim that Kyber-512 qualifies for "category 1".
Can anyone tell me how these two things make sense together?</p>
<p>(As a side note,
NIST subsequently
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/VcKp223-DQAJ">stated</a>
that its 2022 selection report was merely reflecting
"the submitters' <u>claimed</u> security categories"
and that the report was making no
"assertions about whether or not the parameter sets
actually provided the claimed level of security".
How does NIST reconcile this with the report kicking out NTRU-509 while keeping Kyber-512?
Both of those submissions
were claiming to achieve "category 1" given memory-access costs.)</p>
<p>For anyone who cares about reviewability of security analyses,
NIST's sudden switch to accounting for Kyber's memory-access costs
should be setting off alarm bells.</p>
<p>None of the official Kyber security analyses
had tried to quantify the effects of memory on security levels.
The Kyber documentation had merely pointed at memory as supposedly saving the day
in case there weren't enough "gates".</p>
<p>In the absence of an analysis,
how exactly was NIST concluding that memory-access costs
were enough to close the gap?</p>
<p><strong>NIST rescuing Kyber-512, part 2: NIST's botched security analysis.</strong>
In early December 2022,
I asked how NIST was arriving at its conclusion that Kyber-512
was as hard to break as AES-128.</p>
<p>NIST followed up with an
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/xHojUDCaBAAJ">explanation</a>
on 7 December 2022.
I'll refer to this explanation as
"NIST's botched security analysis of Kyber-512";
for brevity, "<strong>NISTBS</strong>".</p>
<p>One of the complications in NISTBS is
that it considers a large space of scenarios, with analysis steps mixed
into comments on the likelihood of each scenario.
Even worse,
NISTBS doesn't give any confirming end-to-end examples
of the tallies obtained in each particular scenario.
So a security reviewer has to trace carefully
through each step of NISTBS.</p>
<p>Here's one example of a scenario
from within the space that NISTBS specifies.
I'll call this "scenario X" for future reference.
Scenario X makes the following three assumptions:</p>
<ul>
<li>
<p>Assume accuracy of 2<sup>137</sup> from the most recent attack paper taken
  into account (Matzov) regarding the number of "gates". (This is a
  number specifically mentioned in NISTBS as a starting point; see below.
  NISTBS also considers the more complicated possibility of this estimate
  being invalid.)</p>
</li>
<li>
<p>Assume this isn't affected by the "known unknowns". (This is a
  possibility specifically mentioned in NISTBS; see below. NISTBS
  also considers the more complicated possibility of the security level
  being affected by the "known unknowns".)</p>
</li>
<li>
<p>Assume accuracy of the RAM-cost model in the NTRU Prime
  documentation. (This is one of two sources that NISTBS
  repeatedly points to and calculates numbers on the basis of. NISTBS also
  considers other possibilities for the RAM cost.)</p>
</li>
</ul>
<p>Obviously the quantitative conclusions of NISTBS vary depending on the exact
assumptions. Considering scenario X is simpler than considering
the full space of scenarios. I'll use scenario X as an example below.</p>
<p>Without further ado, here's every line of NISTBS,
NIST's botched security analysis of Kyber-512.</p>
<blockquote>
<p><span color="#663319">
We can elaborate a little bit further on our reasoning leading to our
current assessment that Kyber512 likely meets NIST category I (similar
considerations apply to the other parameter sets we plan to
standardize for lattice-based schemes.)
</span></p>
</blockquote>
<p>This is a preliminary statement regarding the importance of the
calculations at hand. See below for the calculations.</p>
<blockquote>
<p><span color="#663319">
That said, beyond this message, we don’t think further elaboration of
our current position will be helpful. While we did consult among
ourselves and with the Kyber team,
</span></p>
</blockquote>
<p>I filed a formal complaint in December 2022 regarding NIST's lack of
transparency for its investigation of Kyber-512's security level. As
noted above, I filed a new FOIA request in January 2023.</p>
<blockquote>
<p><span color="#663319">
it’s basically just our considered
opinion based on the same publicly available information everyone else
has access to.
</span></p>
</blockquote>
<p>This is not true. NISTBS starts from, e.g., the Matzov paper's
2<sup>137</sup> estimate for "gates", but then goes beyond this in quantifying the
impact of memory costs, something the Matzov paper definitely did not
do. What we'll see later is how NISTBS botches its own calculations
<em>starting from</em> the Matzov number.</p>
<blockquote>
<p><span color="#663319">
The point of this thread is to seek a broader range of
perspectives on whether our current plan to standardize Kyber512 is a
good one, and a long back and forth between us and a single researcher
does not serve that purpose.
</span></p>
</blockquote>
<p>Public review of NIST's security evaluations requires transparency and
clarity regarding those evaluations. It is not appropriate for NIST to
be asking for a range of perspectives while concealing information. An
open and transparent process would involve less "back and forth" than
the process that NIST chose.</p>
<blockquote>
<p><span color="#663319">
Here's how we see the situation:
In April this year, “Report on the Security of LWE” was published by
MATZOV (<a href="https://zenodo.org/record/6412487#.Y4-V53bMKUk">https://zenodo.org/record/6412487#.Y4-V53bMKUk</a>), describing an
attack, assessed in the RAM model to bring some parameter sets,
including Kyber512, slightly below their claimed security strength
categories.
</span></p>
</blockquote>
<p>This is the most recent attack paper mentioned in NISTBS. That's
why my definition of scenario X says "the most recent attack paper
taken into account (Matzov)".</p>
<p>It's surprising that NISTBS doesn't mention any of the newer
attack papers. NIST had hypothesized that there are no "major improvements in
cryptanalysis" (see full quote above), but this doesn't justify ignoring
the improvements that have already been published!</p>
<p>Anyway, given that NISTBS is calculating security levels starting from
the Matzov paper, let's look carefully at those calculations.</p>
<p>"Assessed in the RAM model" appears to be referring to the Matzov
paper counting the number of "gates". As a side note, "the" RAM model is
ambiguous; the literature defines many different RAM models, and many
different sets of "gates", as noted above.</p>
<blockquote>
<p><span color="#663319">
In particular, the report estimates the cost of attacking Kyber512
using a classical lattice attack to be 2<sup>137</sup> bit operations, which is
less than the approximately 2<sup>143</sup> bit operations required to
classically attack AES-128.
</span></p>
</blockquote>
<p>NISTBS takes this 137 as the foundation of various calculations below.</p>
<p>This doesn't mean NISTBS is saying Kyber-512 is broken in 2<sup>137</sup> "gates".
NISTBS is saying that Matzov estimated 137, and then NISTBS is calculating
various consequences of the 137. If the 137 is inaccurate then the
details of the NISTBS calculations (see below) go up or down accordingly.</p>
<p>For purposes of putting together the sources available, the simplest
case to consider is that 2<sup>137</sup> accurately counts the number of "gates".
Scenario X explicitly assumes this.</p>
<blockquote>
<p><span color="#663319">
However, like previous lattice attacks, the MATZOV attack is based on
sieving techniques, which require a large amount of (apparently
unstructured) access to a very large memory.
</span></p>
</blockquote>
<p>In announcing its plans to standardize Kyber-512, NIST had said that
"the best known attacks against Kyber-512 require huge amounts of
memory"; here NISTBS is reiterating this.</p>
<blockquote>
<p><span color="#663319">
The RAM model ignores the cost of this memory access,
</span></p>
</blockquote>
<p>Indeed, the "gate" counts in question ignore the cost of memory access.</p>
<blockquote>
<p><span color="#663319">
and while the science of comparing the cost of memory access to other
costs involved in a large cryptanalytic attack is not as mature as we
would like, it seems overwhelmingly likely that, in any realistic
accounting of memory access costs, these will significantly exceed the
costs that are assessed by the RAM model for lattice sieving. 
</span></p>
</blockquote>
<p>Here are three obvious examples of quantitative questions raised by this
part of NISTBS. Quantification is essential for cryptographic security
review.</p>
<p>First, what exactly does "significantly" mean in this context?</p>
<p>Second, how does NISTBS reach its "overwhelmingly likely ... significantly
exceed" conclusion?</p>
<p>Third, how does NISTBS get from "significantly exceed" to its conclusion
that having Kyber-512 fall short of AES-128 is "unlikely"? (Assuming no
"major improvements in cryptanalysis".)</p>
<p>NISTBS does eventually get to some quantified calculations; see below.</p>
<blockquote>
<p><span color="#663319">
The largest practical implementation of sieving techniques we know of,
described in detail in “Advanced Lattice Sieving on GPUs, with Tensor
Cores” by Ducas, Stevens, and van Woerden
(https://eprint.iacr.org/2021/141), was forced by memory access
limitations, to adopt settings for bucket size, that would be
suboptimal according to the RAM model.
</span></p>
</blockquote>
<p>Something else unclear from this part of NISTBS is whether "bucket size
... suboptimal" is supposed to imply NIST's "significantly" claim
regarding "costs", and, from there, NIST's claim that it's "unlikely"
for Kyber-512 to be easier to break than AES-128.</p>
<blockquote>
<p><span color="#663319">
It should be noted that, increasing the scale of the instances being
attacked to near cryptographic scale would probably require extensive
hardware optimization, e.g. by using special purpose ASICs, and these
techniques, being generally acknowledged to be less effective against
memory-intensive tasks, would likely make memory access even more of a
bottleneck.
</span></p>
</blockquote>
<p>Qualitatively, this is a reasonable summary of what the literature on
point is saying. However, at this point the reader still doesn't know
how NISTBS gets from this to the claim that Kyber-512 is "unlikely" to
be below the AES-128 security level.</p>
<blockquote>
<p><span color="#663319">
Additionally,
</span></p>
</blockquote>
<p>This is where NISTBS transitions into quantification.</p>
<blockquote>
<p><span color="#663319">
While the Kyber, Dilithium, and Falcon teams did not give a
quantitative assessment of the practical cost of memory access during
sieving against cryptographic parameters, assessments by the NTRU and
NTRUprime teams gave estimates that would suggest the cost of sieving
against category 1 parameters, in models that account for the cost of
memory access, is something like 20 to 40 bits of security more than
would be suggested by the RAM model.
</span></p>
</blockquote>
<p>Finally some numbers to work with! See below for how NISTBS uses these
numbers.</p>
<p>As a side note, NIST seems to have very low confidence in the numbers
it's citing, saying not just "estimates" but also "suggest" and
"something like". But the question I want to focus on here is <em>not</em> how
confident NIST is in the sources that it cites. The question is simply
what security level NISTBS is calculating for Kyber-512 <em>starting from</em>
the sources it cites.</p>
<p>Scenario X explicitly assumes accuracy of one of the two sources that
NISTBS cites, specifically NTRU Prime. In context, this choice of source
is favorable to Kyber:
NISTBS points to NTRU Prime as giving Kyber a
40-bit bonus, and points to NTRU as giving Kyber only a 20-bit bonus.</p>
<blockquote>
<p><span color="#663319">
(For NTRU’s estimates see section 6.3 of the round 3 specification
document available at <a href="https://ntru.org/index.shtml">https://ntru.org/index.shtml</a> . For NTRUprime’s
estimates see section 6.11 of
<a href="https://ntruprime.cr.yp.to/nist/ntruprime-20201007.pdf">https://ntruprime.cr.yp.to/nist/ntruprime-20201007.pdf</a> .
</span></p>
</blockquote>
<p>Scenario X specifically assumes "accuracy of the RAM-cost model in
the NTRU Prime documentation", one of the two sources that NISTBS relies
upon for its quantification. See below for the numbers that NISTBS
obtains from this source.</p>
<blockquote>
<p><span color="#663319">
The Kyber spec (available at
<a href="https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf">https://pq-crystals.org/kyber/data/kyber-specification-round3-20210804.pdf</a>)
discusses, but does not quantify, memory access costs in section 5.3 (Q6))
</span></p>
</blockquote>
<p>Indeed, what's cited here doesn't quantify this. So let's keep going
with the numbers that NISTBS obtains from other sources.</p>
<blockquote>
<p><span color="#663319">
Taking Matzov's estimates of the attack cost to be accurate,
</span></p>
</blockquote>
<p>This is exactly what scenario X is assuming. Of course, NISTBS also
considers other possibilities, but, as an illustrative example, let's
follow through what NISTBS obtains from this assumption.</p>
<blockquote>
<p><span color="#663319">
only 6 bits of security from memory access costs are required for
Kyber512 to meet category 1,
</span></p>
</blockquote>
<p>Indeed, 137 is "only" 6 bits short of the 143 goal. NIST wants to find 6
bits of security that it can credit to Kyber-512, plus so much security
margin that it can claim not to be worried about the "known unknowns"
etc. The point of NISTBS is to argue that the costs of memory do the job.</p>
<blockquote>
<p><span color="#663319">
so in this case Kyber512 would meet category 1 even if the NTRU and
NTRUprime submission significantly overestimate the cost of memory
access in lattice sieving algorithms.
</span></p>
</blockquote>
<p>Here NIST is finding more than its desired 6 bits of security, by
giving Kyber the aforementioned "20 to 40 bits" coming from "assessments
by the NTRU and NTRUprime teams" of the extra costs coming from memory
access.</p>
<p>For example, if NTRU says 20 and if this is accurate, then NISTBS is
calculating a security level of 137+20 = 157, safely above 143. (Again,
this is explicitly assuming accuracy of the 137 in the first place.)</p>
<p>As another example, if NTRU Prime says 40 and if this is accurate, then
NISTBS is calculating a security level of 137+40 = 177, even farther
above 143. (Once again assuming accuracy of the 137.)</p>
<p>See how simple this calculation is? NISTBS points to its sources as
saying that there are actually "20 to 40 bits of security more than
would be suggested by the RAM model" (in NIST's words). So NISTBS adds
20 or 40 to Matzov's 137, giving 157 or 177.</p>
<p>NIST says that even if those sources have "significantly" overestimated
the memory-access cost then Kyber-512 is still okay. To figure out what
NIST means by "significant" here, simply work backwards from NIST's
desired conclusion: if "20 bits" is overestimated by as many as 14 bits,
then that still leaves 20−14 bits, covering the desired 6 bits. Anyway,
Scenario X simply assumes accuracy of the NTRU Prime RAM-cost model.</p>
<blockquote>
<p><span color="#663319">
Further, since about 5 of the 14 claimed bits of security by Matzov
involved speedups to local computations in AllPairSearch (as described
by section 6 of the MATZOV paper), it is likely that Kyber512 would
not be brought below category 1  by the MATZOV attack, as long as
state of the art lattice cryptanalyses prior to the MATZOV paper were
bottlenecked by memory at all.
</span></p>
</blockquote>
<p>It's of course correct that if there's a bottleneck then speeding up
computations outside the bottleneck has little impact. See below for how
NIST seems to be using this to claim <em>even more</em> security.</p>
<blockquote>
<p><span color="#663319">
However, we acknowledge there is some additional uncertainty in the
exact complexity of the MATZOV attack (and all other sieving-based
lattice attacks) due to the known-unknowns Dan alludes to (described
with quantitative estimates in section 5.3 of the Kyber spec.)
</span></p>
</blockquote>
<p>Three reasons that it might be possible to beat Matzov's 2<sup>137</sup> "gates"
are (1) inaccuracies in Matzov's analysis (of course, these could also
point the other way), (2) missing optimizations covered by the "known
unknowns", and (3) missing optimizations beyond the "known unknowns".</p>
<p>Here NIST is pointing to #2. As a side note, it's disturbing to not see
NIST accounting for #1 and #3. NIST explicitly assumed that there are no
"major" improvements in cryptanalysis; but some of its scenarios have
Kyber with very few bits of security margin, and closing those wouldn't
require "major" improvements.</p>
<p>Scenario X skips this complication: it explicitly assumes that the 137
is accurate, and that there are no improvements from the "known
unknowns".</p>
<blockquote>
<p><span color="#663319">
Nonetheless, even taking the most paranoid values for these
known-unknowns (16 bits of security loss),
</span></p>
</blockquote>
<p>This is what the Kyber documentation says is the worst case, yes.</p>
<blockquote>
<p><span color="#663319">
the cost of memory access and/or algorithmically making memory access
local, would still need to be less than what both the NTRU and
NTRUPrime submissions assume.
</span></p>
</blockquote>
<p>I found this puzzling when I first saw it: if we take 137, and then
subtract a hypothesized 16, then we need to find 22 bits, which is
less than the 40 that NISTBS mentioned but <em>not</em> less than the 20. What's
going on?</p>
<p>The best explanation I could come up with is that NIST thinks the 16
overlap the 5 bits that NISTBS mentioned above from Matzov, so NIST is
actually taking 137−16+5, meaning that NIST has to find only 17 bits,
and then the 20 that NISTBS attributes to NTRU is enough (at least if we
disregard the uncertainties conveyed by "estimate" and "suggest" and
"something like").</p>
<p>Again, Scenario X simply assumes that the 137 is accurate, with no
speedups from the "known unknowns", so this complication doesn't arise
for that scenario.</p>
<blockquote>
<p><span color="#663319">
The low end estimate of approximately 20 bits (from the NTRU
submission) is based on a conjecture by Ducas that a fully local
implementation of the BGJ1 sieving algorithm is possible.
</span></p>
</blockquote>
<p>Here NIST is pointing to a reason to ask whether the NTRU model is too
low. Scenario X explicitly takes the NTRU Prime model, which doesn't
trigger this particular issue.</p>
<blockquote>
<p><span color="#663319">
So, in the case that all known-unknowns take on the most paranoid
values, this would either require a sieving algorithm with local
memory access that is much better than any such published algorithm,
and in fact better than any that has been conjectured (at least as far
as we are aware),
</span></p>
</blockquote>
<p>This is summarizing the NISTBS calculations from the perspective of what
algorithmic improvements would be required to break NIST's conclusions.
This isn't relevant to scenario X.</p>
<blockquote>
<p><span color="#663319">
or it would require the approximately 40 bits of additional security
quoted as the "real cost of memory access" by the NTRUprime submission
to be a massive overestimate.
</span></p>
</blockquote>
<p>This is summarizing the NISTBS calculations from the perspective of what
modeling errors would be required to break NIST's conclusions.</p>
<p>It's concerning to observe deviations between what NISTBS attributes to
its source here and what the source actually says. For example, the
source says that it's <em>estimating</em> the cost of memory access, whereas
NIST incorrectly makes it sound as if the source is mislabeling an
estimate as a fact. Furthermore, contrary to what NISTBS's "quoted as"
claim leads readers to believe, the "40 bits" that NISTBS claims as
memory overhead is <em>not</em> a quote from what the source says on this
topic.</p>
<p>Presumably NIST obtained 40 in the following easy way: look at the
security-level table on page 103 of the source; observe that pre-quantum
sieving for <code>sntrup653</code> at the top is listed as 169 and 129 for "real"
and "free" respectively; subtract the 129 from the 169.</p>
<blockquote>
<p><span color="#663319">
In any event, a lot of things would have to go wrong simultaneously to
push the real-world classical cost of known attacks against Kyber512
below category 1, which is why we don't think it's terribly likely.
</span></p>
</blockquote>
<p>This is going beyond the per-scenario calculations into an overall
probability conclusion.</p>
<blockquote>
<p><span color="#663319">
As a final note, known quantum speedups for lattice sieving are much
less effective than Grover’s algorithm for brute force key search, so
in the likely scenario where the limiting attack on AES128 is Grover’s
algorithm, this would further increase the security margin of Kyber512
over AES128 in practice.
</span></p>
</blockquote>
<p>This is yet another complication, and one with several unquantified
steps. It's also blatantly inconsistent with earlier comments from NIST on
the impact of Grover's algorithm.</p>
<p>For example, in email dated 11 Sep 2017 13:48:59 +0000 to
pqc-forum@nist.gov (before the list moved to Google), NIST wrote that
"even if we assume the sort of quantum technology often suggested to be
possible in 15 years (e.g. ~1GW power requirement and a few hours to
factor a 2048 bit number), current technology can still do brute force
search cheaper than Grover’s algorithm". Where are the numbers backing
up NIST's new claim that Grover's algorithm is "likely" the top threat?</p>
<p>Surely NIST agrees that pre-quantum metrics are at least "potentially"
relevant to the practical security of Kyber-512. Consequently, under the
official evaluation criteria, NIST can't use post-quantum metrics as a
way to rescue Kyber-512 if Kyber-512 is easier to break than AES-128 in
the pre-quantum metrics.</p>
<p>I'll focus below on how NISTBS botched its calculation of the
pre-quantum Kyber-512 security level.</p>
<p><strong>What the underlying numbers actually mean.</strong>
Core-SVP is a rough estimate for the number of iterations
in a particular type of lattice attack.
Each iteration involves large-scale memory access and computation.</p>
<p>Let's look at how the latest versions
of the documentation for two submissions, NTRU Prime and Kyber,
convert their estimates for the number of iterations
into larger security-level estimates.
(Note that both of the documents in question are from 2020,
so the numbers don't include subsequent attack improvements.)</p>
<p>NTRU Prime focuses on the cost of memory access.
In particular,
for the important task of sorting N small items,
a two-dimensional circuit of area essentially N needs time essentially N<sup>1/2</sup>,
whereas a circuit of the same area running for the same time
can carry out essentially N<sup>3/2</sup> bit operations.</p>
<p>To put these two types of costs on the same scale,
the NTRU Prime documentation estimates
"the cost of each access to a bit within N bits of memory
as the cost of N<sup>0.5</sup>/2<sup>5</sup> bit operations",
and explains how the 2<sup>5</sup> comes from analyzing energy numbers reported by Intel.</p>
<p>As a concrete example:</p>
<ul>
<li>
<p>The NTRU Prime documentation reports Core-SVP 2<sup>129</sup> for <code>sntrup653</code>,
  meaning a rough estimate of 2<sup>129</sup> iterations.</p>
</li>
<li>
<p>The documentation also reports a rough estimate
  that memory accesses cost, in total,
  the equivalent of 2<sup>169</sup> bit operations for <code>sntrup653</code>.
  This comes from combining
  the N<sup>0.5</sup>/2<sup>5</sup> formula with estimates for N, for the number of iterations,
  and for the number of bits accessed inside each iteration.</p>
</li>
</ul>
<p>For comparison,
recall that Kyber-512 says Core-SVP 2<sup>118</sup>.
A rough estimate for the cost of memory accesses in this Kyber-512 attack
is the equivalent of 2<sup>154</sup> bit operations.</p>
<p>This might sound similar to the Kyber documentation
estimating 2<sup>151</sup> bit operations ("gates").
But the 2<sup>151</sup> estimate in the Kyber documentation
isn't an estimate of the bit-operation equivalent of memory access.
It's ignoring memory access.
It's instead considering the number of bit operations
used inside the attack's computations,
and estimating that this number is somewhere between 2<sup>135</sup> and 2<sup>167</sup>,
given the "known unknowns".</p>
<p><strong>Agency desperation, revisited.</strong>
With the meaning of the numbers in mind,
let's briefly summarize how NISTBS tries to use computations <em>and</em> memory
to push up the claimed security level of Kyber-512:</p>
<ul>
<li><span color="red">
  Start with 118 bits of security for Core-SVP.
  </span></li>
</ul>
<p>Indeed, Core-SVP estimates 2<sup>118</sup> iterations,
  at least with the round-3 Kyber redefinition of Core-SVP.</p>
<ul>
<li><span color="red">
  Add 33 bits of security, giving Kyber-512's claimed 151 bits of security,
  to account for the bit operations used in computations.
  </span></li>
</ul>
<p>Yes, the Kyber-512 documentation has a preliminary estimate of 2<sup>151</sup> bit operations.</p>
<ul>
<li><span color="red">
  Oh, oops, Kyber says this could be 16 bits too high,
  and Matzov says it reached 137,
  and maybe these could be combined,
  and there are other attack papers too?
  That's okay: memory will come to the rescue!
  </span></li>
</ul>
<p>Will it? Quantification needed.</p>
<ul>
<li><span color="red">
  Add "40 bits of additional security" (NIST's words)
  supposedly estimated by NTRU Prime,
  turning Matzov's 137 bits of security into 177 bits of security.
  </span></li>
</ul>
<p>This is where NISTBS goes horribly wrong.</p>
<p>The calculation here doesn't even pass basic type-checking.
Yes, there's a 2<sup>40</sup> in NTRU Prime for <code>sntrup653</code>,
but that's 2<sup>40</sup> bitops/iter.
Multiplying this by Matzov's bitops,
and portraying the result as bitops,
is nonsense from NIST.</p>
<p>Whatever the cost is for computation per iteration,
you have to <em>add</em> that to the cost for memory access per iteration.
<em>Multiplying</em> is wrong.</p>
<p>In the typical case of both numbers being considerably above 1,
multiplying the numbers—which is exactly what NISTBS is doing when it says
"40 bits of security more than would be suggested by the RAM model"
and "40 bits of additional security"—gives
an embarrassing, indefensible overestimate of attack costs.</p>
<p>To finish this NISTBS recap, let's briefly summarize
the happy conclusions that NISTBS draws:</p>
<ul>
<li>
<p><span color="red">
  Look at how much security margin we have here!
  The critical point is that, starting from 137,
  "only 6 bits of security from memory access costs are required for
  Kyber512 to meet category 1" (NIST's words).
  So we don't have to worry about a few bits here and there,
  such as the possibility of 137 being too high.
  </span></p>
</li>
<li>
<p><span color="red">
  We can even get away with replacing 40 bits of NTRU Prime
  with an attacker-optimistic 20 bits of security from NTRU,
  since that gives 157 bits of security.
  Still way above 143!
  Surely we aren't going to lose <em>all</em> 16 bits from the "known unknowns".
  </span></p>
</li>
<li>
<p><span color="red">
  To summarize,
  "a lot of things would have to go wrong simultaneously to
  push the real-world classical cost of known attacks against Kyber512
  below category 1, which is why we don't think it's terribly likely"
  (NIST's words).
  </span></p>
</li>
</ul>
<p>Yeah, sounds great,
except that it's all based on a botched calculation.</p>
<p><strong>How easy it is to catch the error.</strong>
This blog post is aimed at people who want to understand
the whole picture of what's going on here.
But imagine that you're looking at NISTBS without knowing any of this.
How quickly can you see that NISTBS is wrong?</p>
<p>I think the fastest answer is the following simple sanity check.
If</p>
<ul>
<li>
<p>Kyber estimates that the computations in breaking Kyber-512
cost between 2<sup>135</sup> and 2<sup>167</sup> bit operations,
and</p>
</li>
<li>
<p>NTRU Prime estimates that the memory accesses in breaking <code>sntrup653</code>
(which seems harder to break than Kyber-512)
cost the equivalent of 2<sup>169</sup> bit operations,
and</p>
</li>
<li>
<p>attacks then improve by a factor 2<sup>14</sup>,</p>
</li>
</ul>
<p>how can NIST end up estimating that breaking Kyber-512 costs 2<sup>177</sup> bit operations?</p>
<p>This doesn't tell you <em>where</em> NIST went wrong,
but there's a more basic trick that works for that.
See where NISTBS is claiming
that the NTRU Prime documentation
estimates "40 bits of security more than would be suggested by the RAM model"
(NIST's words),
<em>without giving a full quote from the NTRU Prime documentation</em>?</p>
<p>I'm one of the NTRU Prime submitters.
I already knew that this NISTBS claim was false:
it's misattributing NIST's wishful thinking to the NTRU Prime documentation.
But say you're reading this claim <em>without</em> knowing in advance that it's false.
How do you figure out that it's false?</p>
<p>Here's a hard answer and an easy answer:</p>
<ul>
<li>
<p>Hard answer:
Follow NISTBS's pointer
to Section 6.11 of the documentation.
That section starts on page 68, ends on page 70,
doesn't say "40", and doesn't say "the RAM model".
You can read through all the formulas and comments,
try to match it up to the NISTBS claim,
and see that nothing matches.</p>
</li>
<li>
<p>Easy answer:
As soon as you observe that this citation is hard to check,
simply <em>ask for clarification</em> regarding what exactly the citation is referring to.
Honest authors will be happy to clarify.</p>
</li>
</ul>
<p>As a followup,
let's imagine that
NIST responds by saying
"We calculated the 40 by subtracting 129 from 169 on the top row of Table 2".
NIST is then implicitly claiming that the 129 is an example of
calculating security in "the RAM model".
How do you figure out that this implicit claim is false?</p>
<p>This followup similarly has a hard answer and an easy answer:</p>
<ul>
<li>
<p>Hard answer:
Read through enough material about what NIST calls "the RAM model"
to see that this doesn't match the definition of the 129 in the source document.</p>
</li>
<li>
<p>Easy answer:
Simply <em>ask for clarification</em> of what exactly the rest of the citation,
the part attributing something about "the RAM model" to the NTRU Prime documentation,
is referring to.
Honest authors will again be happy to clarify.</p>
</li>
</ul>
<p>Asking questions
is the normal scientific process
for rapidly reaching clarity—and rapidly fixing errors.
For the particular error at hand,
it takes very few rounds to pinpoint the discrepancy:
the 2<sup>129</sup> in the source document for <code>sntrup653</code> is Core-SVP,
<em>not</em> a gate count in what NIST calls "the RAM model".</p>
<p>Of course,
this clarification process doesn't work when an agency
decides to dodge clarification questions,
for example because it doesn't <em>want</em> errors to be fixed.</p>
<p><strong>The research that would be needed for a correct calculation.</strong>
To fix NIST's calculation,
one needs to carefully distinguish two different effects:</p>
<ul>
<li>
<p>Kyber-512's preliminary estimate of security being 33 bits above Core-SVP
  (151 vs. 118)
  comes partially
  from estimating the number of
  bit operations inside the computations in an iteration inside a "primal" attack;
  see the <a href="https://eprint.iacr.org/2019/1161">Asiacrypt 2020 paper</a> mentioned above.
  The cost for computation per iteration
  has to be <em>added</em> to the cost for memory access per iteration.
  <em>Multiplying</em> these costs, as NIST did,
  is exactly the central mistake highlighted in this blog post.</p>
</li>
<li>
<p>On the other hand,
  the estimate comes partially from saying
  that there's an outer loop
  increasing the number of iterations compared to Core-SVP.
  Multiplying the new iteration count
  by the cost of memory access per iteration
  makes perfect sense.</p>
</li>
</ul>
<p>Quantifying these effects
requires tracing carefully through
hundreds of pages of papers on state-of-the-art lattice attacks
(not just rewriting the Asiacrypt 2020 paper)
to see what would happen if costs of memory access were included.</p>
<p>What makes this <em>really</em> tough is that
a change of cost metric also forces
reoptimization of the entire stack of attack subroutines,
along with all applicable parameters.</p>
<p>Consider, as one of many examples,
the choice between low-memory "enumeration"
and high-memory "sieving" as a subroutine inside BKZ.
The Kyber documentation uses cost metrics that ignore the cost of memory access
to conclude that enumeration is less efficient than sieving.
If NIST is suddenly saying that memory access makes sieving slower
than obviously there's a gap in the Kyber analysis.
Where's the recalculation that accounts for the cost of memory access,
and for the large
<a href="https://eprint.iacr.org/2020/707">recent</a>
<a href="https://eprint.iacr.org/2020/1260">improvements</a>
in enumeration?</p>
<p>Shortly after Matzov's attack appeared in April 2022,
I had sent a message to the NISTPQC mailing list
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/Fm4cDfsx65s/m/0dBVXOcSCAAJ">summarizing</a>
the complicated analysis that needed to be done.
I took, as an example, a less Kyber-favorable scenario
in which the "known unknowns" reduce 137 to 121,
and I said that simply multiplying the bit-operation count by 2<sup>40</sup> would be wrong:</p>
<blockquote>
<p>Does accounting for real RAM costs close the gap between 2<sup>121.5</sup> and
2<sup>143</sup>? One might think that, sure, this is covered by the 2<sup>40</sup> mentioned
above: Kyber-512 previously had security 2<sup>40</sup>*2<sup>135.5</sup> = 2<sup>175.5</sup>, so a
32.5-bit security margin, and the new paper is reducing this to an
18.5-bit security margin: i.e., the new paper is merely cutting out 40%
of the Kyber security margin, rather than breaking Kyber outright.</p>
<p>But let's look more closely at the numbers. As a preliminary point,
round-3 Kyber-512 is starting from Core-SVP just 2<sup>112</sup> and
revised-Core-SVP just 2<sup>118</sup>, with exponent 87% and 91% of 129
respectively, so the obvious estimate is about 2<sup>36</sup> instead of 2<sup>40</sup>.</p>
<p>Furthermore, this 2<sup>36</sup> is accounting for the energy cost of accesses to
a giant RAM array, while it's clear that many of the bits of security
beyond Core-SVP claimed in the round-3 Kyber security analysis are
coming from accounting for the cost of local bit operations. These
effects don't multiply; they add!</p>
<p>Internally, Core-SVP is starting from estimates of the number of
"operations" inside sieving. It makes sense to say that the attacker
needs to pay for the large-scale memory access inside each "operation".
It also makes sense to say that the attacker needs to pay for all the
bit operations inside each "operation". But the local bit operations are
an asymptotically irrelevant extra cost on top of the memory access, and
the best bet is that they don't make much difference for Kyber-512. The
real cost of this type of algorithm is, at a large scale, driven
primarily by data motion, not by local computation. ...</p>
<p>So I don't see how current knowledge can justify suggesting that the
costs of RAM rescue Kyber-512 from the new attack. It seems entirely
possible that the real costs of this Kyber-512 attack are considerably
below the costs of a brute-force AES-128 attack. Deciding this one way
or the other will require much more serious analysis of attack costs.</p>
</blockquote>
<p>An agency desperate to rescue Kyber-512
will take note of the first part of what I had written:
great, memory-access costs bump Kyber's security level up by 40 bits,
giving us a healthy security margin!</p>
<p>The agency won't listen to the subsequent part saying that,
no, this calculation is garbage.</p>
<p>The agency won't even listen to the preliminary adjustment of 40 to 36:
we have a healthy security margin, why worry about a few bits here and there?</p>
<p>Meanwhile,
if there's something that sounds like a few bits <em>favoring</em> Kyber-512,
then the desperate agency happily takes note of that,
as the following example illustrates.</p>
<p>The fact that the cost of memory access in each iteration
adds to the cost of computation in each iteration,
rather than multiplying,
has a silver lining for defenders:
in the common situation of memory access being dominant,
improvements in the cost of computation per iteration
make little difference in total cost.
I mentioned this in my April 2022 message regarding the Matzov paper:</p>
<blockquote>
<p>The new paper seems to have some local speedups to the sieving inner
loop, which similarly should be presumed to make little difference next
to the memory-access bottleneck, but my understanding is that this is
under half of the bits of security loss that the paper is reporting.</p>
</blockquote>
<p>Now look at this from the perspective of the desperate agency.
Aha, some bits of the Matzov speedup
are computation speedups that won't matter next to memory access!
As long as we're willing to switch to counting memory access,
this effect downgrades the Matzov speedup,
which sounds good for Kyber-512!</p>
<p>Sure enough,
NISTBS says that
"about 5 of the 14 claimed bits of security by Matzov
involved speedups to local computations",
and portrays this as a "further" reason for confidence in Kyber-512,
beyond the "40 bits of additional security" supposedly produced by memory access.</p>
<p>This is double-counting the silver lining.
Multiplying the 2<sup>40</sup> cost of memory access per iteration
by Matzov's 2<sup>137</sup> bit operations
is already assuming (implicitly and incorrectly)
that every bit operation has its own iteration,
giving 2<sup>137</sup> iterations.
This leaves no room for multiplying by a "further" 2<sup>5</sup>.
The estimated 2<sup>5</sup> is actually on a completely different axis:
it's an estimate for the Matzov-vs.-previous speedup ratio in one metric
divided by the Matzov-vs.-previous speedup ratio in another metric.</p>
<p><strong>NIST rescuing Kyber-512, part 3: dodging clarification requests.</strong>
When NISTBS appeared in December 2022,
I looked through and saw
that NISTBS was multiplying, rather than adding,
the cost of memory access per iteration
and the cost of computation per iteration,
despite my having already pointed out in April 2022 that this was wrong.</p>
<p>But, hmmm, NIST didn't write NISTBS in a verification-friendly way.
In particular, as noted above,
NIST didn't include any examples of confirming tallies.</p>
<p>It seemed perfectly clear
that NIST was adding "40 bits of additional security"
to 137 in scenario X.
But NIST didn't bother saying, yes,
the security level is 177 in that scenario.
NIST also didn't make clear where exactly it was getting the 40 from.</p>
<p>When I find mistakes in security analyses,
the authors usually say
"Thanks for catching the mistake!"—<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/rJYnyTEi92E/m/l5xBpeTpBQAJ">except</a>
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/_kBMTq3RM28/m/afIJBlpoBAAJ">in</a>
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/8_uKOBN4Srw/m/KoAbiE4TDAAJ">lattice</a>-<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/Yx0wZuZP6ag/m/Q_H9lf_zCAAJ">based</a>
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/SxH_NkLOv9Q/m/iCT1Y2VaAwAJ">cryptography</a>,
where the authors usually claim that they meant something different from what they had written.
This continual evasion is a serious disincentive to security review.
If there was <em>any</em> way that I could have misunderstood what NISTBS was saying,
then I wanted to know that at the outset,
before doing the work of writing up an explanation of the error.</p>
<p>So I posted a
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/KeqHDr2lBAAJ">short clarification question</a>.
Specifically,
I spelled out scenario X
and asked whether, in that scenario, I was
"correctly gathering that you're calculating the Kyber-512
security level as 2<sup>177</sup> (i.e., 34 bits of security margin compared to
2<sup>143</sup> for AES-128), where this 177 comes from the above 137 plus 40,
where 40 comes from 169 minus 129 on page 103 of the NTRU Prime
documentation, specifically 'real' minus 'free' for pre-quantum sieving
for sntrup653".</p>
<p>I was expecting a prompt answer saying "Yes, for that specific scenario we're calculating 177 bits of security,
and we're getting the 40 from the 169 and 129 that you mentioned."</p>
<p>What actually happened is that NIST didn't reply.</p>
<p>Seriously?
<strong>NIST picks a risky, bleeding-edge cryptosystem to standardize for users worldwide,
and then doesn't even bother answering clarification questions
about what NIST claims the security level is?</strong></p>
<p>I mentioned above
that I filed a formal complaint
regarding the lack of transparency.
Here's what the complaint said:</p>
<blockquote>
<p>NIST has publicly claimed that Kyber-512 is as difficult to break as
AES-128 (see, e.g., page 8 and Figure 1 of NISTIR 8413 claiming that
Kyber-512 is "category 1"), at least by known attacks. As you know, this
is the minimum security level allowed by the official evaluation
criteria for the NIST Post-Quantum Cryptography Standardization Project.</p>
<p>However, NIST has concealed many details of the investigation that led
to this claim. NIST admits that "we did consult among ourselves and with
the Kyber team"; NIST still has not published those communications.</p>
<p>I have been trying to review the details of NIST's work on this topic.
NIST's lack of transparency makes this review process unnecessarily
difficult.</p>
<p>Some information was released by Dr. Moody and Dr. Perlner in response
to my requests, but this information is (1) incomplete and (2) unclear.
My email dated 8 Dec 2022 03:10:06 +0100 consisted of an "am I correctly
gathering" clarification question that could have been immediately
answered with a simple "Yes, that's correct" if my understanding of
NIST's calculations was correct; but there was no reply, so presumably
NIST actually meant something else. Surely the communications that NIST
is concealing shed light on how NIST actually reached the above claim.</p>
<p>I am writing to file a formal complaint regarding NIST's failure to
promptly and publicly disclose full details of its investigation of the
security of Kyber-512. This investigation should have been carried out
transparently from the outset, allowing prompt correction of any errors
that NIST failed to detect. The fact that NIST was still concealing the
details in July 2022 prevented the public from seeing how NIST arrived
at NISTIR 8413's claims on the topic. The fact that NIST is continuing
to conceal the details today seems inexplicable except as part of NIST
trying to limit public review of NIST's security evaluations.</p>
<p>Please acknowledge receipt of this message, and please publish full
details of NIST's investigation of the security of Kyber-512.</p>
</blockquote>
<p>I escalated the complaint to NIST's Matthew Scholl on 20 January 2023.
Scholl didn't reply.
The public still hasn't seen the details of
NIST's consultations "among ourselves and with the Kyber team"
regarding Kyber-512.</p>
<p>Maybe Scholl was sending internal email:
"Why is djb asking about this?
Did we screw something up again?"
Maybe NIST looked again at my April 2022 message,
realized how badly it had botched its Kyber-512 security analysis,
and then decided that it could get away with being obstructionist
rather than admitting the error.</p>
<p>Or maybe NIST,
still struggling to catch up on post-quantum cryptography,
simply hasn't had time to figure out the meaning of the numbers
that it's multiplying to obtain its claims regarding Kyber-512.
But this doesn't explain what happened next,
namely NIST spending more time dodging clarification questions
than it would have spent simply answering the questions.</p>
<p>The same day that I escalated
my non-transparency complaint to Scholl,
I publicly noted NIST's non-responsiveness,
and
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/EQXN_5z-AQAJ">asked</a>
if anyone saw another way to interpret NIST's calculations:</p>
<blockquote>
<p>In the absence of such clarity, reviewers have to worry that putting
NIST's stated components together in what <em>seems</em> to be the obvious way,
and then doing the work to disprove what NIST <em>appears</em> to be claiming
about the security margin, will lead to a response claiming that, no,
NIST meant something else. It's natural to ask for clarification.</p>
<p>... I've again gone through NIST's 7 December email, and again concluded
that for this scenario NIST is claiming 34 bits in the way spelled out
below. Is there any way I could be missing something here? Does anyone
see another way to interpret NIST's calculations?</p>
</blockquote>
<p>NIST
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/VcKp223-DQAJ">dodged</a>,
replying that NIST's email "speaks for itself".</p>
<p>Well, yes, I think NISTBS speaks for itself, and is very clearly adding
the "40 bits of additional security" to the 137 postulated in scenario X,
obtaining 177 in that scenario,
i.e., 34 bits more than NIST's 143 target.
I was simply asking for NIST to confirm that, yes,
in that scenario you take the 137 from Matzov,
and add the "40 bits of additional security",
giving 177 bits of security.</p>
<p>NIST also tried to shift attention to the question of
"whether or not our current plan to standardize Kyber512 is a good one",
while downplaying the question of whether NIST had correctly calculated
the Kyber-512 security level:</p>
<blockquote>
<p>While
reviewers are free, as a fun exercise, to attempt to "disprove what NIST
<em>appears</em> to be claiming about the security margin," the results of this
exercise would not be particularly useful to the standardization process.</p>
</blockquote>
<p>Seriously?
NIST</p>
<ul>
<li>
<p>kicks out NTRU-509 as supposedly being easier to break than AES-128,</p>
</li>
<li>
<p>keeps Kyber-512 as supposedly being as hard to break as AES-128,</p>
</li>
<li>
<p>repeatedly, inside its rationale for selecting Kyber, points to Kyber-512's efficiency,</p>
</li>
<li>
<p>says it's planning to standardize Kyber-512 as supposedly being as hard to break as AES-128,
  and then</p>
</li>
<li>
<p>claims that disproving NIST's Kyber-512 security-level calculation wouldn't be useful input?</p>
</li>
</ul>
<p>I
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/0mzp87QiCgAJ">replied</a>,
starting with again asking for clarification:</p>
<blockquote>
<p>I <em>think</em> I understand what NIST is claiming in that message regarding
the quantitative Kyber security level.</p>
<p>I <em>think</em> that my clarification question (focusing on one example, much
shorter than NIST's message) is identifying the obvious interpretation.</p>
<p>But then why hasn't NIST simply said "Yes, that's correct" in response?</p>
<p>If the interpretation I've identified differs from what NIST meant, can
NIST please simply say what the difference is, so that security
reviewers don't have to spend time on the quantitative security claims
that NIST currently <em>seems</em> to be making?</p>
</blockquote>
<p>I also commented on the notion that this wouldn't be useful input:</p>
<blockquote>
<p>If Kyber-512 doesn't meet the minimum security level allowed by the
official call for submissions to the NIST Post-Quantum Cryptography
Standardization Project then Kyber-512 should not be standardized.</p>
<p>NIST's evaluation of the Kyber-512 security level---after various attack
advances newer than the latest version of the Kyber submission---depends
explicitly on NIST's calculations of the impact of memory costs.</p>
<p>With all due respect, is it so hard to imagine that NIST has botched
those calculations? If NIST is so sure that it got the whole sequence of
calculations right, why is it so resistant to clarification questions
that will help reviewers check and confirm that NIST got this right? If
NIST <em>isn't</em> sure, doesn't that make public review even more important?</p>
<p>In any case, there's a strong public interest in having NIST's security
evaluations clearly and promptly explained, to maximize the chance of
having errors corrected before bad decisions are set into stone.</p>
</blockquote>
<p>NIST
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/FaoVfGA6CgAJ">dodged again</a>:</p>
<blockquote>
<p>It would be helpful to redirect discussion to</p>
<p>1)      The question of whether Kyber512 is as hard to break as AES128, (which is a
scientific question that cannot be settled by NIST pronouncements)</p>
<p>2)      The related question of whether Kyber512 should be standardized, (which is a
question where NIST will ultimately need to make a definitive decision, but thus far
we have only signaled we are leaning towards yes.)</p>
<p>With this in mind, I would like to note that the technical point on which Dan has
asked for clarification is effectively "how much additional security does Kyber512
get on account of memory access costs, according to the NTRUprime submission's
memory cost model?" Surely Dan, being on the NTRUPrime team, is in a better position
to answer this question than us.</p>
</blockquote>
<p>Seriously?
NIST</p>
<ul>
<li>
<p>takes NTRU Prime's smallest bitops/iter number,</p>
</li>
<li>
<p>slightly screws up by failing to downscale that number from <code>sntrup653</code> to Kyber-512,</p>
</li>
<li>
<p>massively screws up by multiplying that number by Matzov's 2<sup>137</sup> bitops,</p>
</li>
<li>
<p>claims on this basis that
  "a lot of things would have to go wrong simultaneously to
  push the real-world classical cost of known attacks against Kyber512 below category 1",
  and then</p>
</li>
<li>
<p>says that any questions should be addressed to the NTRU Prime team?</p>
</li>
</ul>
<p>Even if NIST <em>didn't</em> understand by this point that it had screwed up,
it certainly knew that</p>
<ul>
<li>
<p>NISTBS was stating conclusions about the Kyber-512 security level relative to AES-128,
  and</p>
</li>
<li>
<p>those conclusions were not in the source documents that NISTBS was citing.</p>
</li>
</ul>
<p>Those conclusions were the result of <em>calculations announced by NIST</em>.
It's completely inappropriate
for NIST to be trying to deflect clarification questions about those calculations.</p>
<p><a href="https://blog.cr.yp.to/20220129-plagiarism.html">Chris Peikert</a>
had entered the discussion in the meantime
to issue blanket denials that NIST was claiming any particular number of bits of security.
Of course, Peikert didn't propose an alternative interpretation
of NIST's words "40 bits of additional security".</p>
<p>I posted a
<a href="https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/4MBurXr58Rs/m/bpbdpmtICgAJ">line-by-line dissection</a>
of NISTBS,
very similar to the line-by-line dissection shown above,
and asked if anyone could see any alternative interpretation:</p>
<blockquote>
<p>If anyone sees any way that I could be misunderstanding the details of
NIST's posting, please pinpoint which step is at issue and what the
alternative interpretation of NIST's calculation is supposed to be.</p>
</blockquote>
<p>There was no reply.</p>
<p>Perhaps NIST will now claim that,
when it wrote "40 bits of additional security",
it actually meant something different from, um, 40 bits of additional security.
But then why didn't NIST promptly answer my first question
by saying that, no, they didn't mean 40 bits of additional security,
and here's what they did mean?</p>
<p>I went far beyond the call of duty
in informing NIST of my understanding of NISTBS,
asking for confirmation,
and giving them ample time to reply.
By dodging, NIST successfully delayed having NISTBS publicly debunked.</p>
<p>At some point one has to draw a line and say that this has gone too far.
NIST's miscalculation of Kyber-512's security level
is still sitting there misinforming people,
and it has to be corrected.</p>
<p><strong>NIST rescuing Kyber-512, part 4: standards making unreviewable security claims.</strong>
In August 2023,
NIST released a
<a href="https://web.archive.org/web/20230827094905/https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.203.ipd.pdf">draft</a>
of its Kyber standard ("ML-KEM"),
in particular saying
"it is claimed that the computational resources needed to break ML-KEM
are greater than or equal to the computational resources needed to break the block cipher ...
ML-KEM-512 is claimed to be in security category 1, ML-KEM-768 is claimed to be in security category 3, and ML-KEM-1024 is claimed to be in security category 5".</p>
<p>Impressive use of the passive voice.
Is <em>NIST</em> claiming these categories?
Are the <em>designers</em> claiming these categories?
Is someone else claiming these categories?</p>
<p>Citation needed.
Or, really, <em>responsibility</em> needed.</p>
<p>Appendix A of the draft
again says that these "categories" are defined as
matching or surpassing AES-128, AES-192, and AES-256 respectively
in every "potentially relevant" cost metric:</p>
<blockquote>
<p>Each category is defined by a comparatively easy-to-analyze reference primitive, whose security
will serve as a floor for a wide variety of metrics that NIST deems potentially relevant to practical
security. ...</p>
<p>In order for a cryptosystem to satisfy one
of the above security requirements, any attack must require computational resources comparable
to or greater than the stated threshold, with respect to all metrics that NIST deems to be potentially
relevant to practical security.</p>
</blockquote>
<p>The latest Kyber documentation says that
the Kyber-512 attack cost could be as low as 2<sup>135</sup> "classical gates".
That's below NIST's estimate of 2<sup>143</sup> "classical gates" for AES-128,
never mind subsequent attack developments.
Where exactly is the justification for claiming that Kyber-512 reaches the AES-128 floor
in all potentially relevant metrics?</p>
<p>Is NIST now officially declaring that "classical gates" aren't "potentially relevant to practical security"?</p>
<p>If so,
how does NIST reconcile this with NIST's 2022 selection report,
which used gate counts ("the non-local cost model")
as an excuse to kick out the most efficient lattice KEM that NIST was considering,
namely NTRU-509?</p>
<p>What exactly <em>are</em> the metrics that NIST is now using
for the claim that Kyber-512 is as hard to break as AES-128?
When and where were the definitions of those metrics published?
(NISTBS doesn't even pass basic type-checking,
let alone refer to a clearly defined metric.)</p>
<p>Where's the analysis of Kyber-512's security level in NIST's metrics?</p>
<p>For comparison,
where's the analysis of the AES-128 security level in NIST's metrics?</p>
<p>The Kyber documentation concentrates on Kyber-512 for its concrete cost analysis,
but the subexponential "dimensions for free" speedup (and subsequent improvements)
should do more damage to security at larger sizes.
Where are the analyses of the Kyber-768, AES-192, Kyber-1024, and AES-256 security levels in NIST's metrics?</p>
<p>NIST's call for submissions said the following:</p>
<blockquote>
<p>All submitters are advised to
be somewhat conservative in assigning parameters to a given category, but submitters of
algorithms where the complexity of the best known attack has recently decreased
significantly, or is otherwise poorly understood, should be especially conservative.</p>
</blockquote>
<p>How exactly is this being handled for the latest "category" claims?
Are the claims accounting for
the 32-bit range of "known unknowns" in the latest Kyber documentation?
A wider range given the "unknowns" appearing in newer papers?
An even wider range to protect against the likelihood of further attack speedups?</p>
<p>Readers understand the word "claim"
to be asserting that something is true,
not to be merely saying "we don't think it's terribly likely that this is false".
Why does this draft standard
conceal NIST's assessment of the probability of failure?</p>
<p>The official NISTPQC call for submissions said
<a href="https://web.archive.org/web/20220119113311/https://csrc.nist.gov/CSRC/media/Projects/Post-Quantum-Cryptography/documents/call-for-proposals-final-dec-2016.pdf">"NIST will perform a thorough analysis of the submitted algorithms in a manner that is open and transparent to the public"</a>.
Scholl said
<a href="https://web.archive.org/web/20211115191840/https://www.nist.gov/blogs/taking-measure/post-quantum-encryption-qa-nists-matt-scholl">"We operate transparently. We've shown all our work"</a>.
But the reality is that
security reviewers aren't even being given a clear statement of <em>what</em> exactly is being claimed about Kyber's security,
let alone what the justification for that claim is supposed to be.</p>
<p><strong>Next steps.</strong>
Given how unstable and poorly understood the lattice attack surface is,
standardizing Kyber-512 (or NTRU-509) would be reckless.</p>
<p>The poor understanding is a sign of danger.
Contrary to NISTBS,
it's entirely possible that Kyber-512 is substantially easier to break than AES-128
with attacks that have already been published,
even considering the costs of memory access.
The opposite is also possible.
Figuring out the actual status of this bleeding-edge proposal would be a tough research project.</p>
<p>The instability is another sign of danger.
How are we supposed to manage the risks of better attacks wiping out many more bits of security?</p>
<p>("Bad news: It's broken. Good news: Comparing it to AES-128 has become much easier.")</p>
<p>AES-128 isn't some stratospheric security level.
For example,
<a href="https://blog.cr.yp.to/20151120-batchattacks.html">multi-target attacks</a>
against AES-128
take only 2<sup>88</sup> computations to break one of a trillion keys.
That amount of computation is already feasible for large-scale attackers today.
Even if you think this is too expensive to worry about,
what happens if a cryptosystem actually loses 10 or 20 or 30 bits compared to that?</p>
<p>A paper at ACM CCS 2021
claimed to be able to show that one-out-of-many-ciphertext attacks against Kyber
are as hard as single-ciphertext attacks.
But I have a paper
<a href="https://cr.yp.to/papers.html#lprrr">"Multi-ciphertext security degradation for lattices"</a>
that</p>
<ul>
<li>
<p>points out an apparently unfixable flaw in the proof and</p>
</li>
<li>
<p>shows that,
  according to the heuristics used in Kyber's security analysis,
  particular multi-ciphertext attacks are asymptotically more efficient
  than the standard single-ciphertext attacks.</p>
</li>
</ul>
<p>The main theorem of my paper isn't easy but now has a proof
fully verified by <a href="https://www.cl.cam.ac.uk/~jrh13/hol-light/">HOL Light</a>.
"Asymptotically" refers to what happens when sizes grow to infinity;
more research is required to quantify
the impact of these multi-ciphertext attacks—and whatever improved attacks people
find—upon Kyber's limited range of sizes.
This is just one of many unexplored parts of the attack surface.</p>
<p>Some attack avenues have clear quantitative limits:
for example, 2<sup>40</sup>-target attacks can't eliminate more than 40 bits of security.
Replacing Kyber-512 with Kyber-1024 clearly reduces risks
(which is not to say that it eliminates <em>all</em> risks:
look at what happened to SIKE).
There are many previous examples in cryptography
of attacks that would have been stopped
if cryptographic parameters had been chosen just twice as large
as what people had thought was necessary.</p>
<p>Standardizing Kyber-512 means that Kyber-512 will be deployed
in many applications that would easily have been able to afford Kyber-1024 or NTRU-1229
or something even larger.
This is true even if the standard has Kyber-1024 (or Kyber-768)
as an option, even <em>the recommended option</em>.
It's
<a href="https://cr.yp.to/papers.html#competitions">easier</a>
for a manager to take the fastest option
than to investigate whether the fastest option is actually needed.
Why exactly <em>won't</em> a manager take the fastest option
if NIST has declared it to be a standard option?</p>
<p>Security is supposed to be job #1.
So I recommend eliminating Kyber-512.
I also recommend that NIST be honest with the public about what happened here:</p>
<ul>
<li>
<p>Honest NIST: "We were desperate to establish that Kyber-512 is as hard to break as AES-128,
  given the costs of memory access, assuming no attack improvements.
  This desperation led us to botch our security-level calculations. Sorry."</p>
</li>
<li>
<p>Public: "So you're withdrawing the claim that Kyber-512 qualifies for category 1?"</p>
</li>
<li>
<p>Honest NIST: "Correct. We are not making a claim either way.
  Settling this requires future research.
  Given the uncertainties regarding the performance of current attacks
  and the risks of better attacks,
  we are no longer planning to standardize Kyber-512.
  Our apologies to anyone who already invested effort in Kyber-512."</p>
</li>
<li>
<p>Public: "But, wait, doesn't removing Kyber-512
  make NTRU the clear winner in flexibility and performance?"</p>
</li>
<li>
<p>Honest NIST: "Yes.
  We were desperate to create the opposite perception.
  That's why we were desperate to keep Kyber-512.
  That's also why we were manipulating our selection and presentation of data in other ways,
  for example by kicking out NTRU-509 on the basis of gate counts
  while keeping Kyber-512 on the basis of memory-access costs.
  Sorry."</p>
</li>
<li>
<p>Public: "Partway through the competition,
  you suddenly started criticizing submissions that weren't providing category 5.
  NTRU responded with parameters having much higher Core-SVP than Kyber-1024.
  Does Kyber-1024 meet category 5?"</p>
</li>
<li>
<p>Honest NIST: "Figuring that out would be another tough research project.
  The latest versions of Kyber-512, Kyber-768, and Kyber-1024
  report Core-SVP 2<sup>118</sup>, 2<sup>183</sup>, and 2<sup>256</sup>,
  so we extrapolated from saying that Kyber-512 is in category 1
  to saying that Kyber-768 is in category 3 and that Kyber-1024 is in category 5.
  We never looked at the details.
  Sorry."</p>
</li>
<li>
<p>Public: "Doesn't your official report say that you're confident in the security of NTRU?
  Doesn't this mean that NTRU actually scores better than Kyber on all three evaluation factors?"</p>
</li>
<li>
<p>Honest NIST: "Yes.
  The only decisive factor listed in our selection report was that
  Kyber was 'near the top (if not the top) in most benchmarks'.
  Without Kyber-512, Kyber can't compete with NTRU in performance.
  Sorry."</p>
</li>
<li>
<p>Public: "Why were you so desperate to take Kyber over NTRU in the first place?"</p>
</li>
<li>
<p>Honest NIST: "Here are the full records that we were keeping secret,
  and in particular they answer that question.
  These records also show why we weren't meeting our commitment to operate transparently,
  and why we repeatedly lied about this."</p>
</li>
<li>
<p>Public: "You exposed three years of user data to attackers
  by telling people to use Kyber starting when your patent license activates in 2024,
  rather than telling people to use NTRU starting in 2021!"</p>
</li>
<li>
<p>Honest NIST: "Sorry. What's done is done.
  We're locked into standardizing Kyber at this point,
  and deviating from this would produce even more slowdowns.
  We'll standardize Kyber-768 as category 2 and Kyber-1024 as category 4."</p>
</li>
</ul>
<p>After everything that has happened,
I'm skeptical that we're going to suddenly see Honest NIST,
but hope springs eternal.</p><hr><span size="1"><b>Version:</b>
This is version 2023.10.03 of the 20231003-countcorrectly.html web page.
</span>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Working on Multiple Web Projects with Docker Compose and Traefik (163 pts)]]></title>
            <link>https://georgek.github.io/blog/posts/multiple-web-projects-traefik/</link>
            <guid>37756632</guid>
            <pubDate>Tue, 03 Oct 2023 19:46:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://georgek.github.io/blog/posts/multiple-web-projects-traefik/">https://georgek.github.io/blog/posts/multiple-web-projects-traefik/</a>, See on <a href="https://news.ycombinator.com/item?id=37756632">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>Docker Compose is a brilliant tool for bringing up local development environments for
web projects. But working with multiple projects can be a pain due to clashes. For
example, all projects want to listen to port 80 (or perhaps one of the super common
higher ones like 8000 etc.). This forces developers to only bring one project up at a
time, or hack the compose files to change the port numbers.</p><p>Recently I've found a way that makes managing these more enjoyable.</p><div id="outline-container-headline-1"><h2 id="headline-1">A single project with Docker Compose</h2><div id="outline-text-headline-1"><p>I use <a href="https://docs.docker.com/compose/">docker compose</a> to manage local development instances of these projects. A typical
compose file for a web project might look like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># proj/compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>db</span>:
</span></span><span><span>    <span>image</span>: <span>"postgres"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>POSTGRES_DB</span>: <span>"proj"</span>
</span></span><span><span>      <span>POSTGRES_USER</span>: <span>"user"</span>
</span></span><span><span>      <span>POSTGRES_PASSWORD</span>: <span>"pass"</span>
</span></span><span><span>
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>build</span>: .
</span></span><span><span>    <span>depends_on</span>:
</span></span><span><span>      - <span>"db"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>DATABASE_URL</span>: <span>"postgres://user:pass@db/proj"</span>
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"8000:80"</span></span></span></code></pre></td></tr></tbody></table></div><p>Note the very last line. This is where we map port 8000 from the host to port 80 of the
container such that the service can be accessed via <code>http://127.0.0.1:8000</code>.</p><p>This works quite well for a single project, but it suffers from a couple of problems if
you work on multiple projects:</p><ol><li>It doesn't scale. If I want to run another project at the same time, I'll have to
use a different port number, maybe 8001, then 8002 etc.,</li><li>What if that <code>compose.yaml</code> file is checked in as part of the project? Does the whole
team have to agree on a set of port numbers to use for each project?</li></ol></div></div><div id="outline-container-headline-2"><h2 id="headline-2">Using overrides for multiple projects</h2><div id="outline-text-headline-2"><p>Fortunately Docker Compose does have a solution for (2) in the form of the
<code>compose.override.yaml</code> file. This file will be automatically be <a href="https://docs.docker.com/compose/multiple-compose-files/merge/">merged</a> into the
<code>compose.yaml</code> without any extra configuration.</p><p>Unlike some other guides (including the official <a href="https://docs.docker.com/compose/multiple-compose-files/merge/#example">docs</a>) concerning this file, I prefer to
<strong>not</strong> check <code>compose.override.yaml</code> into version control and instead add it to the
<code>.gitignore</code> file. Adding it to version control completely defeats the purpose of it: to
allow individual developers to override the standard compose file.</p><p>So, with this in mind, I no longer expose any ports by default in <code>compose.yaml</code> because
I don't know what will be convenient for each developer. This set up might look like
this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>db</span>:
</span></span><span><span>    <span>image</span>: <span>"postgres"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>POSTGRES_DB</span>: <span>"proj"</span>
</span></span><span><span>      <span>POSTGRES_USER</span>: <span>"user"</span>
</span></span><span><span>      <span>POSTGRES_PASSWORD</span>: <span>"pass"</span>
</span></span><span><span>
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>build</span>: .
</span></span><span><span>    <span>depends_on</span>:
</span></span><span><span>      - <span>"db"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>DATABASE_URL</span>: <span>"postgres://user:pass@db/proj"</span></span></span></code></pre></td></tr></tbody></table></div><div><table><tbody><tr><td><pre tabindex="0"><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># compose.override.yaml (to be created by each developer)</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"8000:80"</span></span></span></code></pre></td></tr></tbody></table></div></div></div><div id="outline-container-headline-3"><h2 id="headline-3">Using Traefik</h2><div id="outline-text-headline-3"><p>So now each developer can pick their own port numbers for each project, but we can still
do better than this. People aren't good at remembering numbers. We are much better at
remembering names. <a href="https://doc.traefik.io/traefik/">Traefik</a> is a free software edge router that can be used as a simple
and super easy to configure reverse-proxy in container-based set ups.</p><p>Using Docker, Traefik can automatically discover services to create routes to. It uses
container labels to further configure these routes. The following tiny example from the
<a href="https://doc.traefik.io/traefik/getting-started/quick-start/">docs</a> is illustrative:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># traefik/compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>reverse-proxy</span>:
</span></span><span><span>    <span>image</span>: traefik:v2.10
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"80:80"</span>
</span></span><span><span>    <span>volumes</span>:
</span></span><span><span>      - /var/run/docker.sock:/var/run/docker.sock
</span></span><span><span>  <span>whoami</span>:
</span></span><span><span>    <span>image</span>: traefik/whoami
</span></span><span><span>    <span>labels</span>:
</span></span><span><span>      - <span>"traefik.http.routers.whoami.rule=Host(`whoami.docker.localhost`)"</span></span></span></code></pre></td></tr></tbody></table></div><p>This starts two containers on the same docker network. The reverse proxy listens on
port 80 and forwards traffic with a host header of "whoami.docker.localhost" to the
<code>whoami</code> service. Traefik guesses which port to send it to <code>whoami</code> based on the ports
exposed by the container.</p><p>If you haven't played with Traefik before it's worth going through the <a href="https://doc.traefik.io/traefik/getting-started/quick-start/">quick-start</a>
properly now then coming back to see how we can make this work for multiple projects.</p></div></div><div id="outline-container-headline-4"><p>This doesn't quite solve our problem yet. We don't want all of our various projects
inside one compose file. Luckily Traefik communicates with the Docker daemon directly
and doesn't really care about the compose file, but you do need to make sure a few
things are in order for this to work.</p><p>Firstly, make a docker network especially for Traefik to communicate with other services
that you want to expose, for example:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># traefik/compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>reverse-proxy</span>:
</span></span><span><span>    <span>image</span>: traefik:v2.10
</span></span><span><span>    <span>restart</span>: unless-stopped
</span></span><span><span>    <span>command</span>: --api.insecure=true --providers.docker
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"80:80"</span>
</span></span><span><span>      - <span>"8080:8080"</span>
</span></span><span><span>    <span>volumes</span>:
</span></span><span><span>      - <span>"/var/run/docker.sock:/var/run/docker.sock"</span>
</span></span><span><span>    <span>networks</span>:
</span></span><span><span>      - traefik
</span></span><span><span>
</span></span><span><span><span>networks</span>:
</span></span><span><span>  <span>traefik</span>:
</span></span><span><span>    <span>attachable</span>: <span>true</span>
</span></span><span><span>    <span>name</span>: traefik</span></span></code></pre></td></tr></tbody></table></div><p>We create the network <code>traefik</code> and give it the name "traefik" (otherwise docker compose
would scope it by project, e.g. "traefik_traefik"). We also allow other containers to
attach to this network.</p><p>Then in our <code>compose.override.yaml</code> file from above, instead of mapping ports, we do the
following:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># proj/compose.override.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>labels</span>:
</span></span><span><span>      - <span>"traefik.http.routers.proj.rule=Host(`proj.traefik.me`)"</span>
</span></span><span><span>      - <span>"traefik.http.services.proj.loadbalancer.server.port=8000"</span>
</span></span><span><span>      - <span>"traefik.docker.network=traefik"</span>
</span></span><span><span>    <span>networks</span>:
</span></span><span><span>      - default
</span></span><span><span>      - traefik
</span></span><span><span>
</span></span><span><span><span>networks</span>:
</span></span><span><span>  <span>traefik</span>:
</span></span><span><span>    <span>external</span>: <span>true</span></span></span></code></pre></td></tr></tbody></table></div><p>Now, after bringing up first the traefik project then your web project, you should be
able to browse to <a href="http://proj.traefik.me/">http://proj.traefik.me/</a> in your web browser.</p><p>There's a few things going on here. First, we have declared the <code>traefik</code> network as an
external network. This means compose won't manage it, but expects it to exist (so you
must start your traefik composition first). Next we override the <code>networks</code> setting of
<code>web</code> to make it part of the <code>traefik</code> network too. Note we also have to add the
<code>default</code> network, otherwise it wouldn't be able to communicate with <code>db</code> and other
services on its own default network.</p><p>The <code>traefik.http.routers.proj.rule</code> label configures Traefik to route HTTP traffic with
the "proj.traefik.me" hostname to the container. The <code>traffic.docker.network</code> label is
necessary because <code>web</code> is on two networks. Finally, we set
<code>traefik.http.services.proj.loadbalancer.server.port</code> for completeness, just in case
your container needs a different port mapping than the port it is set to expose, or if
it exposes multiple ports.</p><p>There is one final piece of magic: the "traefik.me" hostname. What is that? You can
read about it at <a href="http://traefik.me/">http://traefik.me/</a>. Essentially it is a DNS service that resolves to
any IP address that you want, but by default it resolves <code>&lt;xxx&gt;.traefik.me</code> to
<code>127.0.0.1</code>. There are other services like this including <a href="https://sslip.io/">https://sslip.io/</a> and
<a href="https://nip.io/">https://nip.io/</a>.</p><p>Now, because we don't need to define any ports at all, it is possible to take advantage
of a newish compose feature and reinstate the ports in the original <code>compose.yaml</code> file
for those developers who don't want to set up Traefik for themselves. So our final
configuration looks like this:</p><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># compose.yaml</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>db</span>:
</span></span><span><span>    <span>image</span>: <span>"postgres"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>POSTGRES_DB</span>: <span>"proj"</span>
</span></span><span><span>      <span>POSTGRES_USER</span>: <span>"user"</span>
</span></span><span><span>      <span>POSTGRES_PASSWORD</span>: <span>"pass"</span>
</span></span><span><span>
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>build</span>: .
</span></span><span><span>    <span>depends_on</span>:
</span></span><span><span>      - <span>"db"</span>
</span></span><span><span>    <span>environment</span>:
</span></span><span><span>      <span>DATABASE_URL</span>: <span>"postgres://user:pass@db/proj"</span>
</span></span><span><span>    <span>ports</span>:
</span></span><span><span>      - <span>"8000:80"</span></span></span></code></pre></td></tr></tbody></table></div><div><table><tbody><tr><td><pre tabindex="0"><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span></code></pre></td><td><pre tabindex="0"><code data-lang="yaml"><span><span><span># compose.override.yaml (to be created by each developer)</span>
</span></span><span><span><span>services</span>:
</span></span><span><span>  <span>web</span>:
</span></span><span><span>    <span>labels</span>:
</span></span><span><span>      - <span>"traefik.http.routers.proj.rule=Host(`proj.traefik.me`)"</span>
</span></span><span><span>      - <span>"traefik.http.services.proj.loadbalancer.server.port=8000"</span>
</span></span><span><span>      - <span>"traefik.docker.network=traefik"</span>
</span></span><span><span>    <span>networks</span>:
</span></span><span><span>      - default
</span></span><span><span>      - traefik
</span></span><span><span>    <span>ports</span>: !reset []
</span></span><span><span>
</span></span><span><span><span>networks</span>:
</span></span><span><span>  <span>traefik</span>:
</span></span><span><span>    <span>external</span>: <span>true</span></span></span></code></pre></td></tr></tbody></table></div><p>The <code>!reset []</code> tag sets the ports back to empty; you can read about it <a href="https://docs.docker.com/compose/compose-file/13-merge/#reset-value">here</a>. Note that
unfortunately it can't be used to set <em>new</em> ports, only reset them to default (you would
have to use two layers of override file to set new ports). The <code>!reset</code> tag requires a
fairly recent version of docker compose, at least greater than 2.18.0.</p><p>A final note: you can check that these overrides are working correctly by running
<code>docker compose config</code>.</p></div><div id="outline-container-headline-5"><h2 id="headline-5">Conclusion</h2><div id="outline-text-headline-5"><p>By leveraging both the <code>compose.override.yaml</code> file and Traefik it's easy to run
multiple web projects on your development system at the same time and have easy to
remember names to access them all. Each developer is free to run as many as they want
and create their own easily-manageable configurations. Traefik and traefik.me can also
be used to allow other developers on your network to easily access your local
development instances with no DNS configuration required.</p><p>It's a shame that the docs instruct people to use the override file for a distributed
developer config rather than let individual developers use it, but hopefully it's not
too hard to remove this file from repos if already present.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Glibc dynamic loader hit by a nasty local privilege escalation vulnerability (124 pts)]]></title>
            <link>https://www.phoronix.com/news/Glibc-LD-Nasty-Root-Bug</link>
            <guid>37756357</guid>
            <pubDate>Tue, 03 Oct 2023 19:25:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phoronix.com/news/Glibc-LD-Nasty-Root-Bug">https://www.phoronix.com/news/Glibc-LD-Nasty-Root-Bug</a>, See on <a href="https://news.ycombinator.com/item?id=37756357">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="GNU" src="https://www.phoronix.com/assets/categories/gnu.webp" width="100" height="100"></p><p>
A nasty vulnerability has been made public today concerning Glibc's dynamic loader that can lead to full root privileges being obtained by local users. This affects Linux distributions of the past two years with the likes of Ubuntu 22.04 LTS, 23.04, Fedora 38, and others vulnerable to this local privilege escalation issue.
</p><p>
Qualys announced this vulnerability a few minutes ago:
</p><blockquote>"The GNU C Library's dynamic loader "find[s] and load[s] the shared objects (shared libraries) needed by a program, prepare[s] the program to run, and then run[s] it" (man ld.so). The dynamic loader is extremely security sensitive, because its code runs with elevated privileges when a local user executes a set-user-ID program, a set-group-ID program, or a program with capabilities. Historically, the processing of environment variables such as LD_PRELOAD, LD_AUDIT, and LD_LIBRARY_PATH has been a fertile source of vulnerabilities in the dynamic loader.
<p>
Recently, we discovered a vulnerability (a buffer overflow) in the dynamic loader's processing of the GLIBC_TUNABLES environment variable. This vulnerability was introduced in April 2021 (glibc 2.34) by commit 2ed18c ("Fix SXID_ERASE behavior in setuid programs (BZ #27471)").
</p><p>
We successfully exploited this vulnerability and obtained full root privileges on the default installations of Fedora 37 and 38, Ubuntu 22.04 and 23.04, Debian 12 and 13; other distributions are probably also vulnerable and exploitable (one notable exception is Alpine Linux, which uses musl libc, not the glibc). We will not publish our exploit for now; however, this buffer overflow is easily exploitable (by transforming it into a data-only attack), and other researchers might publish working exploits shortly after this coordinated disclosure."</p></blockquote>
<p>See the <a href="https://www.openwall.com/lists/oss-security/2023/10/03/2">oss-security mailing list</a> for more details on this high profile vulnerability.
</p><p><img src="https://www.phoronix.net/image.php?id=854&amp;image=phx_beer_h2_med" alt="A bad day for computers..."><br><em>This glibc dynamic loader vulnerability comes just hours after <a href="https://www.phoronix.com/news/XOrg-Vulnerabilities-Since-1988">new X.Org/X11 vulnerabilities that date back as far as 1988</a> were disclosed. A rough day for computers and a long day for Linux administrators."</em></p><p><em>
<br>Glibc updates to the major Linux distributions should begin rolling out imminently. In the interim we are already seeing <a href="https://lists.debian.org/debian-devel-announce/2023/10/msg00000.html">actions</a> take place such as Debian temporarily restricting access to some of their systems until they are patched against this local privilege escalation vulnerability.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon Used Secret ‘Project Nessie’ Algorithm to Raise Prices (270 pts)]]></title>
            <link>https://www.wsj.com/business/retail/amazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706</link>
            <guid>37755648</guid>
            <pubDate>Tue, 03 Oct 2023 18:23:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/business/retail/amazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706">https://www.wsj.com/business/retail/amazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706</a>, See on <a href="https://news.ycombinator.com/item?id=37755648">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div display="flex"><nav aria-label="breadcrumbs"><ol><li><div><p data-testid="flashline">WSJ News Exclusive</p></div></li><li><a href="https://www.wsj.com/business/retail?mod=breadcrumb" data-testid="breadcrumb-link"><span><span><span>Retail</span></span></span></a></li></ol></nav></div><h2>The strategy, as described in redacted parts of FTC lawsuit, is part of agency’s case that Amazon has outsize influence on consumer prices</h2></div><article><div><div><div> <p>By </p><p><a data-testid="author-link" href="https://www.wsj.com/news/author/dana-mattioli" target="_blank" aria-label="Author page for Dana Mattioli"><span><span>Dana Mattioli</span></span></a></p></div><div><p>Updated Oct. 3, 2023 4:54 pm ET</p></div></div><div aria-label="Utility Bar" role="toolbar" data-block="doNotPrint"></div><section><div data-type="video" data-inset_type="" data-sub_type="" data-layout="inline"><figure><figcaption>Since Lina Khan became Federal Trade Commission chair in 2021, she’s taken on Meta, Microsoft, and Amazon, and that’s made her a lightning rod for controversy. WSJ breaks down the battles she’s picked and why she’s willing to lose. Photo illustration: Xingpei Shen</figcaption></figure></div><p data-type="paragraph">Amazon.com<!-- --> used an algorithm code-named “Project Nessie” to test how much it could raise prices in a way that competitors would follow, according to redacted portions of <a data-type="link" href="https://www.wsj.com/tech/ftc-sues-amazon-alleging-illegal-online-marketplace-monopoly-6bd9af23?mod=article_inline" rel="">the Federal Trade Commission’s monopoly lawsuit</a> against the company.</p></section><p>Copyright ©<!-- -->2023<!-- --> Dow Jones &amp; Company, Inc. All Rights Reserved. 87990cbe856818d5eddac44c7b1cdeb8</p><div><div id="cx-snippet-overlay"><p><img src="https://sts3.wsj.net/iweb/images/wsj-logo-big-black.svg" title="The Wall Street Journal" alt=""></p><p>Continue reading your article with<br>a WSJ subscription</p><p><a href="https://subscribe.wsj.com/wsjsnippet">Subscribe Now</a></p></div><p>Already a subscriber? <a href="https://www.wsj.com/client/login?target=https%3A%2F%2Fwww.wsj.com%2Fbusiness%2Fretail%2Famazon-used-secret-project-nessie-algorithm-to-raise-prices-6c593706">Sign In</a></p></div><div aria-label="What to Read Next" data-block="doNotPrint" data-skip-nav-order="5" data-skip-label="What to Read Next" role="region" tabindex="-1"><p><h2>What to Read Next</h2></p></div><div aria-label="Sponsored Offers" data-block="doNotPrint" data-skip-nav-order="6" data-skip-label="Sponsored Offers" role="region" tabindex="-1"><p>Sponsored Offers</p><ul><li><span>TurboTax<!-- -->: <br></span><a href="https://www.wsj.com/coupons/turbotax" target="_blank" rel="noreferrer">Save up to $15 with TurboTax coupon 2023</a></li><li><span>The Motley Fool<!-- -->: <br></span><a href="https://www.wsj.com/coupons/motley-fool" target="_blank" rel="noreferrer">Epic Bundle - 3x Expert Stock Recommendations</a></li><li><span>H&amp;R Block Tax<!-- -->: <br></span><a href="https://www.wsj.com/coupons/hr-block" target="_blank" rel="noreferrer">15% OFF DIY Online Tax Filing Services | H&amp;R Block Coupon</a></li><li><span>Top Resume<!-- -->: <br></span><a href="https://www.wsj.com/coupons/topresume" target="_blank" rel="noreferrer">Top Resume Coupon: 10% Off professional resume writing</a></li><li><span>eBay<!-- -->: <br></span><a href="https://www.wsj.com/coupons/ebay" target="_blank" rel="noreferrer">Save 25% on designer items using this eBay coupon</a></li><li><span>Groupon<!-- -->: <br></span><a href="https://www.wsj.com/coupons/groupon" target="_blank" rel="noreferrer">Up to $50 off any order with Groupon promo code</a></li></ul></div></div></article><div><div type="news" aria-label="Most Popular News" data-skip-label="Most Popular News" data-skip-nav-order="6" role="complementary" tabindex="-1" width="300px"><h4 type="news">Most Popular news</h4><div></div></div><div aria-label="Most Popular Opinion" data-skip-label="Most Popular Opinion" data-skip-nav-order="7" role="complementary" tabindex="-1" width="300px"><div type="opinion"><h4 type="opinion">Most Popular opinion</h4><div></div></div><div type="opinion_new"><h4 type="opinion_new">Most Popular Opinion</h4><div></div></div></div><div aria-label="Recommended Videos" data-skip-nav-order="8" data-skip-label="Recommended Videos" role="complementary" tabindex="-1" width="300px"><h4>Recommended Videos</h4></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[John Carmack on AI (101 pts)]]></title>
            <link>https://twitter.com/ID_AA_Carmack/status/1708905454544282083</link>
            <guid>37755276</guid>
            <pubDate>Tue, 03 Oct 2023 17:54:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/ID_AA_Carmack/status/1708905454544282083">https://twitter.com/ID_AA_Carmack/status/1708905454544282083</a>, See on <a href="https://news.ycombinator.com/item?id=37755276">Hacker News</a></p>
Couldn't get https://twitter.com/ID_AA_Carmack/status/1708905454544282083: Error [ERR_FR_TOO_MANY_REDIRECTS]: Maximum number of redirects exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Honey, I shrunk the NPM package (144 pts)]]></title>
            <link>https://jamiemagee.co.uk/blog/honey-i-shrunk-the-npm-package/</link>
            <guid>37754489</guid>
            <pubDate>Tue, 03 Oct 2023 16:55:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jamiemagee.co.uk/blog/honey-i-shrunk-the-npm-package/">https://jamiemagee.co.uk/blog/honey-i-shrunk-the-npm-package/</a>, See on <a href="https://news.ycombinator.com/item?id=37754489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><span>Sep 27, 2023 · 11 minute read · <a href="https://jamiemagee.co.uk/blog/honey-i-shrunk-the-npm-package/#disqus_thread">Comments</a></span></p><p>Have you ever wondered what lies beneath the surface of an npm package? At its heart, it’s nothing more than a gzipped tarball. Working in software development, source code and binary artifacts are nearly always shipped as <code>.tar.gz</code> or <code>.tgz</code> files. And gzip compression is supported by every HTTP server and web browser out there. <a href="http://caniuse.com/">caniuse.com</a> doesn’t even give statistics for support, it just says “<a href="https://caniuse.com/sr_content-encoding-gzip">supported in effectively all browsers</a>”. But here’s the kicker: gzip is starting to show its age, making way for newer, more modern compression algorithms like Brotli and ZStandard. Now, imagine a world where npm embraces one of these new algorithms. In this blog post, I’ll dive into the realm of compression and explore the possibilities of moderinising npm’s compression strategy.</p><h2 id="whats-the-competition">What’s the competition?</h2><p>The two major players in this space are Brotli and ZStandard (or zstd for short). Brotli was released by Google in 2013 and zstd was released by Facebook in 2016. They’ve since been standardised, in <a href="https://datatracker.ietf.org/doc/html/rfc7932">RFC 7932</a> and <a href="https://datatracker.ietf.org/doc/html/rfc8478">RFC 8478</a> respectively, and have seen widespread use all over the software industry. It was actually <a href="https://archlinux.org/news/now-using-zstandard-instead-of-xz-for-package-compression/">the announcement</a> by Arch Linux that they were going to start compressing their packages with zstd by default that made think about this in the first place. Arch Linux was by no means the first project, nor is it the only one. But to find out if it makes sense for the Node ecosystem, I need to do some benchmarks. And that means breaking out <code>tar</code>.</p><h2 id="benchmarking-part-1">Benchmarking part 1</h2><figure><img src="https://jamiemagee.co.uk/img/xkcd-1168.png" alt="https://xkcd.com/1168/" title="I don't know what's worse--the fact that after 15 years of using tar I still can't keep the flags straight, or that after 15 years of technological advancement I'm still mucking with tar flags that were 15 years old when I started."><figcaption>https://xkcd.com/1168</figcaption></figure><p>I’m going to start with <code>tar</code> and see what sort of comparisons I can get by switching gzip, Brotli, and zstd. I’ll test with <a href="https://www.npmjs.com/package/npm">the npm package of npm itself</a> as it’s a pretty popular package, averaging over 4 million downloads a week, while also being quite large at around 11MB unpacked.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>$ curl --remote-name https://registry.npmjs.org/npm/-/npm-9.7.1.tgz
</span></span><span><span>2</span><span>$ ls -l --human npm-9.7.1.tgz 
</span></span><span><span>3</span><span>-rw-r--r-- <span>1</span> jamie users 2.6M Jun <span>16</span> 20:30 npm-9.7.1.tgz 
</span></span><span><span>4</span><span>$ tar --extract --gzip --file npm-9.7.1.tgz
</span></span><span><span>5</span><span>$ du --summarize --human --apparent-size package
</span></span><span><span>6</span><span>11M	package
</span></span></code></pre></div><p>gzip is already giving good results, compressing 11MB to 2.6MB for a compression ratio of around 0.24. But what can the contenders do? I’m going to stick with the default options for now:</p><div><pre tabindex="0"><code data-lang="bash"><span><span> 1</span><span>$ brotli --version
</span></span><span><span> 2</span><span>brotli 1.0.9
</span></span><span><span> 3</span><span>$ tar --use-compress-program brotli --create --file npm-9.7.1.tar.br package
</span></span><span><span> 4</span><span>$ zstd --version
</span></span><span><span> 5</span><span>*** Zstandard CLI <span>(</span>64-bit<span>)</span> v1.5.5, by Yann Collet ***
</span></span><span><span> 6</span><span>$ tar --use-compress-program zstd --create --file npm-9.7.1.tar.zst package
</span></span><span><span> 7</span><span>$ ls -l --human npm-9.7.1.tgz npm-9.7.1.tar.br npm-9.7.1.tar.zst 
</span></span><span><span> 8</span><span>-rw-r--r-- <span>1</span> jamie users 1.6M Jun <span>16</span> 21:14 npm-9.7.1.tar.br
</span></span><span><span> 9</span><span>-rw-r--r-- <span>1</span> jamie users 2.3M Jun <span>16</span> 21:14 npm-9.7.1.tar.zst
</span></span><span><span>10</span><span>-rw-r--r-- <span>1</span> jamie users 2.6M Jun <span>16</span> 20:30 npm-9.7.1.tgz 
</span></span></code></pre></div><p>Wow! With no configuration both Brotli and zstd come out ahead of gzip, but Brotli is the clear winner here. It manages a compression ratio of 0.15 versus zstd’s 0.21. In real terms that means a saving of around 1MB. That doesn’t sound like much, but at 4 million weekly downloads, that would save 4TB of bandwidth per week.</p><h2 id="benchmarking-part-2-electric-boogaloo">Benchmarking part 2: Electric boogaloo</h2><p>The compression ratio is only telling half of the story. Actually, it’s a third of the story, but compression speed isn’t really a concern. Compression of a package only happens once, when a package is published, but decompression happens every time you run <code>npm install</code>. So any time saved decompressing packages means quicker install or build steps.</p><p>To test this, I’m going to use <a href="https://github.com/sharkdp/hyperfine">hyperfine</a>, a command-line benchmarking tool. Decompressing each of the packages I created earlier 100 times should give me a good idea of the relative decompression speed.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>$ hyperfine --runs <span>100</span> --export-markdown hyperfine.md <span>\
</span></span></span><span><span>2</span><span><span></span>  <span>'tar --use-compress-program brotli --extract --file npm-9.7.1.tar.br --overwrite'</span> <span>\
</span></span></span><span><span>3</span><span><span></span>  <span>'tar --use-compress-program zstd --extract --file npm-9.7.1.tar.zst --overwrite'</span> <span>\
</span></span></span><span><span>4</span><span><span></span>  <span>'tar --use-compress-program gzip --extract --file npm-9.7.1.tgz --overwrite'</span>
</span></span></code></pre></div><table><thead><tr><th>Command</th><th>Mean [ms]</th><th>Min [ms]</th><th>Max [ms]</th><th>Relative</th></tr></thead><tbody><tr><td>tar –use-compress-program brotli –extract –file npm-9.7.1.tar.br –overwrite</td><td>51.6 ± 3.0</td><td>47.9</td><td>57.3</td><td>1.31 ± 0.12</td></tr><tr><td>tar –use-compress-program zstd –extract –file npm-9.7.1.tar.zst –overwrite</td><td>39.5 ± 3.0</td><td>33.5</td><td>51.8</td><td>1.00</td></tr><tr><td>tar –use-compress-program gzip –extract –file npm-9.7.1.tgz –overwrite</td><td>47.0 ± 1.7</td><td>44.0</td><td>54.9</td><td>1.19 ± 0.10</td></tr></tbody></table><p>This time zstd comes out in front, followed by gzip and Brotli. This makes sense, as “real-time compression” is one of the big features that is touted in <a href="https://facebook.github.io/zstd/">zstd’s documentation.</a> While Brotli is 31% slower compared to zstd, in real terms it’s only 12ms. And compared to gzip, it’s only 5ms slower. To put that into context, you’d need a more than 1Gbps connection to make up for the 5ms loss it has in decompression compared with the 1MB it saves in package size.</p><h2 id="benchmarking-part-3-this-time-its-serious">Benchmarking part 3: This time it’s serious</h2><p>Up until now I’ve just been looking at Brotli and zstd’s default settings, but both have a lot of knobs and dials that you can adjust to change the compression ratio and compression or decompression speed. Thankfully, the industry standard <a href="https://github.com/inikep/lzbench">lzbench</a> has got me covered. It can run through all of the different quality levels for each compressor, and spit out a nice table with all the data at the end.</p><p>But before I dive in, there are a few caveats I should point out. The first is that lzbench isn’t able to compress an entire directory like <code>tar</code> , so I opted to use <code>lib/npm.js</code> for this test. The second is that lzbench doesn’t include the gzip tool. Instead it uses zlib, the underlying gzip library. The last is that the versions of each compressor aren’t quite current. The latest version of zstd is 1.5.5, released on April 4th 2023, whereas lzbench uses version 1.4.5, released on May 22nd 2020. The latest version of Brotli is 1.0.9, released on August 27th 2020, whereas lzbench uses a version released on October 1st 2019.</p><div><pre tabindex="0"><code data-lang="bash"><span><span>1</span><span>$ lzbench -o1 -ezlib/zstd/brotli package/lib/npm.js
</span></span></code></pre></div><details><summary>Click to expand results</summary><table><thead><tr><th>Compressor name</th><th>Compression</th><th>Decompress.</th><th>Compr. size</th><th>Ratio</th><th>Filename</th></tr></thead><tbody><tr><td>memcpy</td><td>117330 MB/s</td><td>121675 MB/s</td><td>13141</td><td>100.00</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -1</td><td>332 MB/s</td><td>950 MB/s</td><td>5000</td><td>38.05</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -2</td><td>382 MB/s</td><td>965 MB/s</td><td>4876</td><td>37.11</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -3</td><td>304 MB/s</td><td>986 MB/s</td><td>4774</td><td>36.33</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -4</td><td>270 MB/s</td><td>1009 MB/s</td><td>4539</td><td>34.54</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -5</td><td>204 MB/s</td><td>982 MB/s</td><td>4452</td><td>33.88</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -6</td><td>150 MB/s</td><td>983 MB/s</td><td>4425</td><td>33.67</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -7</td><td>125 MB/s</td><td>983 MB/s</td><td>4421</td><td>33.64</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -8</td><td>92 MB/s</td><td>989 MB/s</td><td>4419</td><td>33.63</td><td>package/lib/npm.js</td></tr><tr><td>zlib 1.2.11 -9</td><td>95 MB/s</td><td>986 MB/s</td><td>4419</td><td>33.63</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -1</td><td>594 MB/s</td><td>1619 MB/s</td><td>4793</td><td>36.47</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -2</td><td>556 MB/s</td><td>1423 MB/s</td><td>4881</td><td>37.14</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -3</td><td>510 MB/s</td><td>1560 MB/s</td><td>4686</td><td>35.66</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -4</td><td>338 MB/s</td><td>1584 MB/s</td><td>4510</td><td>34.32</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -5</td><td>275 MB/s</td><td>1647 MB/s</td><td>4455</td><td>33.90</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -6</td><td>216 MB/s</td><td>1656 MB/s</td><td>4439</td><td>33.78</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -7</td><td>140 MB/s</td><td>1665 MB/s</td><td>4422</td><td>33.65</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -8</td><td>101 MB/s</td><td>1714 MB/s</td><td>4416</td><td>33.60</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -9</td><td>97 MB/s</td><td>1673 MB/s</td><td>4410</td><td>33.56</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -10</td><td>97 MB/s</td><td>1672 MB/s</td><td>4410</td><td>33.56</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -11</td><td>37 MB/s</td><td>1665 MB/s</td><td>4371</td><td>33.26</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -12</td><td>27 MB/s</td><td>1637 MB/s</td><td>4336</td><td>33.00</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -13</td><td>20 MB/s</td><td>1601 MB/s</td><td>4310</td><td>32.80</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -14</td><td>18 MB/s</td><td>1582 MB/s</td><td>4309</td><td>32.79</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -15</td><td>18 MB/s</td><td>1582 MB/s</td><td>4309</td><td>32.79</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -16</td><td>9.03 MB/s</td><td>1556 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -17</td><td>8.86 MB/s</td><td>1559 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -18</td><td>8.86 MB/s</td><td>1558 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -19</td><td>8.86 MB/s</td><td>1559 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -20</td><td>8.85 MB/s</td><td>1558 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -21</td><td>8.86 MB/s</td><td>1559 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>zstd 1.4.5 -22</td><td>8.86 MB/s</td><td>1589 MB/s</td><td>4305</td><td>32.76</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -0</td><td>604 MB/s</td><td>813 MB/s</td><td>5182</td><td>39.43</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -1</td><td>445 MB/s</td><td>775 MB/s</td><td>5148</td><td>39.18</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -2</td><td>347 MB/s</td><td>947 MB/s</td><td>4727</td><td>35.97</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -3</td><td>266 MB/s</td><td>936 MB/s</td><td>4645</td><td>35.35</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -4</td><td>164 MB/s</td><td>930 MB/s</td><td>4559</td><td>34.69</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -5</td><td>135 MB/s</td><td>944 MB/s</td><td>4276</td><td>32.54</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -6</td><td>129 MB/s</td><td>949 MB/s</td><td>4257</td><td>32.39</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -7</td><td>103 MB/s</td><td>953 MB/s</td><td>4244</td><td>32.30</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -8</td><td>84 MB/s</td><td>919 MB/s</td><td>4240</td><td>32.27</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -9</td><td>7.74 MB/s</td><td>958 MB/s</td><td>4237</td><td>32.24</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -10</td><td>4.35 MB/s</td><td>690 MB/s</td><td>3916</td><td>29.80</td><td>package/lib/npm.js</td></tr><tr><td>brotli 2019-10-01 -11</td><td>1.59 MB/s</td><td>761 MB/s</td><td>3808</td><td>28.98</td><td>package/lib/npm.js</td></tr></tbody></table></details><p>This pretty much confirms what I’ve shown up to now. zstd is able to provide faster decompression speed than either gzip or Brotli, and slightly edge out gzip in compression ratio. Brotli, on the other hand, has comparable decompression speeds and compression ratio with gzip at lower quality levels, but at levels 10 and 11 it’s able to edge out both gzip and zstd’s compression ratio.</p><h2 id="everything-is-derivative">Everything is derivative</h2><p>Now that I’ve finished with benchmarking, I need to step back and look at my original idea of replacing gzip as npm’s compression standard. As it turns out, Evan Hahn had a similar idea in 2022 and proposed <a href="https://github.com/npm/rfcs/pull/595">an npm RFC</a>. He proposed using Zopfli, a backwards-compatible gzip compression library, and Brotli’s older (and cooler 😎) sibling. Zopfli is able to produce smaller artifacts with the trade-off of a much slower compression speed. In theory an easy win for the npm ecosystem. And if you watch <a href="https://www.youtube.com/watch?v=sDsq_i1Q4Lc&amp;t=170s">the RFC meeting recording</a> or read <a href="https://github.com/npm/rfcs/blob/main/meetings/2022-06-01.md#pr-595-propose-backwards-compatible-improvements-to-compression---evanhahn">the meeting notes</a>, everyone seems hugely in favour of the proposal. However, the one big roadblock that prevents this RFC from being immediately accepted, and ultimately results in it being abandoned, is the lack of a native JavaScript implementation.</p><p>Learning from this earlier RFC and my results from benchmarking Brotli and zstd, what would it take to build a strong RFC of my own?</p><h2 id="putting-it-all-together">Putting it all together</h2><p>Both Brotli and zstd’s reference implementations are written in C. And while there are a lot of ports on the npm registry using Emscripten or WASM, Brotli has an implementation in <a href="https://nodejs.org/api/zlib.html">Node.js’s zlib module</a>, and has done <a href="https://nodejs.org/en/blog/release/v10.16.0">since Node.js 10.16.0</a>, released in May 2019. I opened <a href="https://github.com/nodejs/node/issues/48412">an issue in Node.js’s GitHub repo</a> to add support for zstd, but it’ll take a long time to make its way into an LTS release, nevermind the rest of npm’s dependency chain. I was already leaning towards Brotli, but this just seals the deal.</p><p>Deciding on an algorithm is one thing, but implementing it is another. npm’s current support for gzip compression ultimately comes from Node.js itself. But the dependency chain between npm and Node.js is long and slightly different depending on if you’re packing or unpacking a package.</p><p>The dependency chain for packing, as in <code>npm pack</code> or <code>npm publish</code>, is:</p><p><a href="https://www.npmjs.com/package/npm">npm</a> → <a href="https://www.npmjs.com/package/libnpmpack">libnpmpack</a> → <a href="https://www.npmjs.com/package/pacote">pacote</a> → <a href="https://www.npmjs.com/package/tar">tar</a> → <a href="https://www.npmjs.com/package/minizlib">minizlib</a> → <a href="https://nodejs.org/api/zlib.html">zlib</a> (Node.js)</p><p>But the dependency chain for unpacking (or ‘reifying’ as npm calls it), as in <code>npm install</code> or <code>npm ci</code> is:</p><p><a href="https://www.npmjs.com/package/npm">npm</a> → <a href="https://www.npmjs.com/package/@npmcli/arborist">@npmcli/arborist</a> → <a href="https://www.npmjs.com/package/pacote">pacote</a> → <a href="https://www.npmjs.com/package/tar">tar</a> → <a href="https://www.npmjs.com/package/minizlib">minizlib</a> → <a href="https://nodejs.org/api/zlib.html">zlib</a> (Node.js)</p><p>That’s quite a few packages that need to be updated, but thankfully the first steps have already been taken. Support for Brotli was added to minizlib 1.3.0 back in September 2019. I built on top of that and <a href="https://github.com/isaacs/node-tar/pull/391">contributed Brotil support</a> to <code>tar</code>. That is now <a href="https://github.com/isaacs/node-tar/blob/main/CHANGELOG.md#62">available in version 6.2.0</a>. It may take a while, but I can see a clear path forward.</p><p>The final issue is backwards compatibility. This wasn’t a concern with Evan Hahn’s RFC, as Zopfli generates backwards-compatible gzip files. However, Brotli is an entirely new compression format, so I’ll need to propose a very careful adoption plan. The process I can see is:</p><ol><li>Support for packing and unpacking is added in a minor release of the current version of npm<ol><li>Unpacking using Brotli is handled transparently</li><li>Packing using Brotli is disabled by default and only enabled if one of the following are true:<ol><li>The <code>engines</code> field in <code>package.json</code> is set to a version of npm that supports Brotli</li><li>The <code>engines</code> field in <code>package.json</code> is set to a version of node that bundles a version of npm that supports Brotli</li><li>Brotli support is explicitly enabled in <code>.npmrc</code></li></ol></li></ol></li><li>Packing using Brotli is enabled by default in the next major release of npm after the LTS version of Node.js that bundles it goes out of support</li></ol><p>Let’s say that Node.js 22 comes with npm 10, which has Brotli support. Node.js 22 will stop getting LTS updates in April 2027. Then, the next major version of npm after that date should enable Brotli packing by default.</p><p>I admit that this is an <em>incredibly</em> long transition period. However, it will guarantee that if you’re using a version of Node.js that is still being supported, there will be no visible impact to you. And it still allows early adopters to opt-in to Brotli support. But if anyone has other ideas about how to do this transition, I am open to suggestions.</p><h2 id="whats-next">What’s next?</h2><p>As I wrap up my exploration into npm compression, I must admit that my journey has only just begun. To push the boundaries further, there are a lot more steps. First and foremost, I need to do some more extensive benchmarking with <a href="https://socket.dev/npm/category/popular">the top 250 most downloaded npm packages</a>, instead of focusing on a single package. One that’s complete, I need to draft an npm RFC and seek feedback from the wider community. If you’re interested in helping out, or just want to see how it’s going, you can follow me on Mastodon at <a href="https://infosec.exchange/@JamieMagee">@<span data-cfemail="e0aa818d8985ad81878585a0898e868f938583ce85988388818e8785">[email&nbsp;protected]</span></a>, or on Twitter at <a href="https://twitter.com/Jamie_Magee">@Jamie_Magee</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Best books to understand semiconductor business? (121 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37754287</link>
            <guid>37754287</guid>
            <pubDate>Tue, 03 Oct 2023 16:44:23 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37754287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37759135"><td></td></tr>
            <tr id="37756840"><td></td></tr>
                <tr id="37757375"><td></td></tr>
                <tr id="37757735"><td></td></tr>
                <tr id="37757800"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37757800" href="https://news.ycombinator.com/vote?id=37757800&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>Most of the stuff you'll find on the web is pretty old. In most industries it's not really a big issue-- waste management and industrial pump manufacturers don't change that quickly. But obviously in semis it matters more.<p>If you know anyone who works at a hedge fund or investment bank, you can ask them to grab some primers for you. All the big banks produce them for the major industries they cover. But they usually watermark every page of the report with the name of the client to discourage people from sharing more widely.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37755054"><td></td></tr>
                <tr id="37757105"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37757105" href="https://news.ycombinator.com/vote?id=37757105&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>I'm cautious with Asianometry. I watched some about a year ago and he wasn't completely accurate. I don't remember which videos now, but letting people know they should consume more than just Asianometry to corroborate facts.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755294"><td></td></tr>
                <tr id="37757292"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37757292" href="https://news.ycombinator.com/vote?id=37757292&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>Chip War focuses a lot on the personalities and comparatively little on the business/industry itself. It provides an entertaining read, I guess, but especially in the second half, I found myself reading on just to finish the book, rather than out of a desire/expectation to get informed.<p>Some representative paragraphs from the book. Some people probably like this style, but it's not for me.</p><p>&gt; In 1985, Taiwan's powerful minister K. T. Li called Morris Chang into his office in Taipei. Nearly two decades had passed since Li had helped convince Texas Instruments to build its first semiconductor facility on the island. In the twenty years since then, Li had forged close ties with Texas Instrument's leaders, visiting Pat Haggerty and Morris Chang whenever he was in the U.S. and convincing other electronics firms to follow TI and open factories in Taiwan. In 1985, he hired Chang to lead Taiwan's chip industry. "We want to promote a semiconductor industry in Taiwan," he told Chang. "Tell me," he continued, "how much money you need."</p><p>...</p><p>&gt; Lee Byung-Chul could make a profit selling almost anything. Born in 1910, just a year after Jack Simplot, Lee launched his business career in March 1938, a time when his native Korea was part of Japan's empire, at war with China and soon with the United States. Lee's first products were dried fish and vegetables, which he gathered from Korea and shipped to northern China to feed Japan's war machine. Korea was an impoverished backwater, with no industry or technology, but Lee was already dreaming of building a business that would be "big, strong, and eternal," he declared. He would turn Samsung into a semiconductor superpower thanks to two influential allies: America's chip industry and the South Korean state. A key part of Silicon Valley's strategy to outmaneuver the Japanese was to find cheaper sources of supply in Asia. Lee decided this was a role Samsung could easily play.</p><p>...</p><p>&gt; Vladimir Vetrov was a KGB spy, but his life felt more like a Chekhov story than a James Bond film. His KGB work was bureaucratic, his mistress far from a supermodel, and his wife more affectionate toward her shih tzu puppies than toward him. By the end of the 1970s, Vetrov's career, and his life, had hit a dead end. He despised his desk job and was ignored by his bosses. He detested his wife, who was having an affair with one of his friends. For recreation, he escaped to his log cabin in a village north of Moscow, which was so rustic that there was no electricity. Or he'd simply stay in Moscow and get drunk.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37755739"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37755739" href="https://news.ycombinator.com/vote?id=37755739&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>Adding that if you prefer to read (since the question is about best books), asianometry's patreon includes transcripts.  Plus it's always good to support someone doing good work.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755233"><td></td></tr>
                <tr id="37755741"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37755741" href="https://news.ycombinator.com/vote?id=37755741&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>That ARM video is useful because it somewhat ties together lots of things in a short-ish video. But I hate the breathless style and the video confuses and mistakes many things. Also what's the point of that silly [the iPhone is british, Fitbit is british, DJI drones are British...]?<p>In the end it is not about the semiconductor business - just hits several aspects of it at the periphery.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37755608"><td></td></tr>
                <tr id="37756012"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37756012" href="https://news.ycombinator.com/vote?id=37756012&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>2nd this. Great mix of journalism, criticism and deep technical insight while being accessible people not in the industry.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37756391"><td></td></tr>
            <tr id="37754452"><td></td></tr>
                <tr id="37754929"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37754929" href="https://news.ycombinator.com/vote?id=37754929&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>I've read this and it gives a pretty good overview of the history as well as current events surrounding the semiconductor industry. I think it gives a good high-level overview so you'll have the foundation to dig deeper. 
I don't think that this book by itself would be enough to understand the semiconductor business, but it's a great place to start to get acclimated.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37757299"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37757299" href="https://news.ycombinator.com/vote?id=37757299&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>+1. I have only finished around ~20% of the book - but it's been really informative covering a wide range of topics from early technical innovations, to  to the companies involved like Fairchild, TI, Intel, Sony, TSMC etc (and by extension the nations involved US, Japan, Russia, Taiwan etc)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755745"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37755745" href="https://news.ycombinator.com/vote?id=37755745&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>Came here to say this.  It's a fabulous history and easily one of the best and most informative books I read last year.  It's not necessarily going to teach you much about the current "business" per se, but it's a must-read for the history and has quite a bit about the current global state of affairs, especially re PRC/ROC, and state of the technology.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755676"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37755676" href="https://news.ycombinator.com/vote?id=37755676&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>After reading Chip War I read Conquering the Electron by Eric Brach and Derek Cheung. Very good book that goes into a bit more detail than Chip War. Chip War focuses more on the geopolitics than the science.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37755551"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37755551" href="https://news.ycombinator.com/vote?id=37755551&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>I'm about 90% through this book that I picked up based on some other HN comment I had read. I would highly recommend it</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37756702"><td></td></tr>
            <tr id="37757566"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37757566" href="https://news.ycombinator.com/vote?id=37757566&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>-1: dude was wrong about everything, missed the role of software and computer engineering.<p>1. China rapidly catching up</p><p>2. Companies can produce competitive offerings with old manufacturing processes</p><p>He took a manufacturing heavy view, but actually open archs like RISC-V or ARM's china division defecting provided avenues to make competitive offerings with last generation technology.</p><p>fastest chip today runs on 4 year old silicon: <a href="https://www.eetimes.com/groq-demos-fast-llms-on-4-year-old-silicon/" rel="nofollow noreferrer">https://www.eetimes.com/groq-demos-fast-llms-on-4-year-old-s...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37755702"><td></td></tr>
            <tr id="37756043"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37756043" href="https://news.ycombinator.com/vote?id=37756043&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>Andy Grove: The Life and Times of an American<p><a href="https://www.amazon.com/Andy-Grove-Life-Times-American/dp/1591841399/ref=sr_1_1?keywords=Andy+Grove%3A+The+Life+and+Times+of+an+American&amp;s=books&amp;sr=1-1" rel="nofollow noreferrer">https://www.amazon.com/Andy-Grove-Life-Times-American/dp/159...</a></p><p>This appears to be out of print. I read it a long time ago when it was new. I enjoyed the book.</p><p>One of the things that I remember from it was how Intel really overestimated how quickly video calls would take off. They thought that video calls would be the "killer app" for desktop computers in the late 1990s / early 2000s.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37758144"><td></td></tr>
            <tr id="37756527"><td></td></tr>
            <tr id="37754413"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37754413" href="https://news.ycombinator.com/vote?id=37754413&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>If you’re interested in the history then I recently read and mostly liked The Chip by TR Reid. I’d like to find something more up to date than that. Lots of buzz around Chip War by Chris Miller. I haven’t read it.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37756045"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37756045" href="https://news.ycombinator.com/vote?id=37756045&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>The Idea Factory
Book by Jon Gertner<p>The Idea Factory: Bell Labs and the Great Age of American Innovation is a 2012 book by Jon Gertner that describes the history of Bell Labs, the research and development wing of AT&amp;T, as well as many of its eccentric personalities, such as Claude Shannon and William Shockley. It is Gertner's first published book.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37755804"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37755804" href="https://news.ycombinator.com/vote?id=37755804&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><br><div>
                  <p><span>Two that I found really helpful were The Big Score and The Intel Trinity by Michael Malone. Both cover the earlier years, but great history.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37756156"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37756156" href="https://news.ycombinator.com/vote?id=37756156&amp;how=up&amp;goto=item%3Fid%3D37754287"></a></center>    </td><td><p><span>Offering this title for asking opinions of it more than recommendation, as I've not yet read it:<p>'Crystal Fire: The Invention of the Transistor and the Birth of the Information Age', Riordan.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37755757"><td></td></tr>
            <tr id="37755525"><td></td></tr>
            <tr id="37758575"><td></td></tr>
            <tr id="37756022"><td></td></tr>
            <tr id="37755863"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google open-sources their graph mining library (283 pts)]]></title>
            <link>https://github.com/google/graph-mining</link>
            <guid>37753442</guid>
            <pubDate>Tue, 03 Oct 2023 15:46:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google/graph-mining">https://github.com/google/graph-mining</a>, See on <a href="https://news.ycombinator.com/item?id=37753442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-turbo-body="">
      


    <div>
      <p><a href="#start-of-content">Skip to content</a>
      <span data-view-component="true">
    <span data-view-component="true"></span>
</span></p><header role="banner" data-color-mode="light" data-light-theme="light" data-dark-theme="dark">
  

  <div>
    <div>
      <a href="https://github.com/" aria-label="Homepage" data-ga-click="(Logged out) Header, go to homepage, icon:logo-wordmark">
        
      </a>

        <div>
          <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="77262e073ade2e82c16866bbb7be3f8bfd1e2a2aca883e57ee571494d2f12129">
            Sign&nbsp;up
          </a>
        </p></div>

      
    </div>


    <div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
              <p><span id="product-explore-heading">Explore</span></p><ul aria-labelledby="product-explore-heading">
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to All features&quot;,&quot;label&quot;:&quot;ref_cta:All features;&quot;}" href="https://github.com/features">
      All features

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Documentation&quot;,&quot;label&quot;:&quot;ref_cta:Documentation;&quot;}" href="https://docs.github.com/">
      Documentation

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Skills&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Skills;&quot;}" href="https://skills.github.com/">
      GitHub Skills

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Blog&quot;,&quot;label&quot;:&quot;ref_cta:Blog;&quot;}" href="https://github.blog/">
      Blog

    
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li>
      
      <div>
          <div>
              <p><span id="solutions-for-heading">For</span></p><ul aria-labelledby="solutions-for-heading">
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Enterprise&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise;&quot;}" href="https://github.com/enterprise">
      Enterprise

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Teams&quot;,&quot;label&quot;:&quot;ref_cta:Teams;&quot;}" href="https://github.com/team">
      Teams

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Startups&quot;,&quot;label&quot;:&quot;ref_cta:Startups;&quot;}" href="https://github.com/enterprise/startups">
      Startups

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Education&quot;,&quot;label&quot;:&quot;ref_cta:Education;&quot;}" href="https://education.github.com/">
      Education

    
</a></li>

            </ul>
          </div>
          <div>
              <p><span id="solutions-by-solution-heading">By Solution</span></p><ul aria-labelledby="solutions-by-solution-heading">
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to CI/CD &amp;amp; Automation&quot;,&quot;label&quot;:&quot;ref_cta:CI/CD &amp;amp; Automation;&quot;}" href="https://github.com/solutions/ci-cd/">
      CI/CD &amp; Automation

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevOps&quot;,&quot;label&quot;:&quot;ref_cta:DevOps;&quot;}" href="https://resources.github.com/devops/">
      DevOps

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to DevSecOps&quot;,&quot;label&quot;:&quot;ref_cta:DevSecOps;&quot;}" href="https://resources.github.com/devops/fundamentals/devsecops/">
      DevSecOps

    
</a></li>

            </ul>
          </div>
          <div>
              <p><span id="solutions-resources-heading">Resources</span></p><ul aria-labelledby="solutions-resources-heading">
                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Learning Pathways&quot;,&quot;label&quot;:&quot;ref_cta:Learning Pathways;&quot;}" href="https://resources.github.com/learn/pathways/">
      Learning Pathways

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to White papers, Ebooks, Webinars&quot;,&quot;label&quot;:&quot;ref_cta:White papers, Ebooks, Webinars;&quot;}" href="https://resources.github.com/">
      White papers, Ebooks, Webinars

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Customer Stories&quot;,&quot;label&quot;:&quot;ref_cta:Customer Stories;&quot;}" href="https://github.com/customer-stories">
      Customer Stories

    
</a></li>

                <li>
  <a target="_blank" data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Solutions&quot;,&quot;action&quot;:&quot;click to go to Partners&quot;,&quot;label&quot;:&quot;ref_cta:Partners;&quot;}" href="https://partner.github.com/">
      Partners

    
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
              <p><span id="open-source-repositories-heading">Repositories</span></p><ul aria-labelledby="open-source-repositories-heading">
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Topics&quot;,&quot;label&quot;:&quot;ref_cta:Topics;&quot;}" href="https://github.com/topics">
      Topics

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Trending&quot;,&quot;label&quot;:&quot;ref_cta:Trending;&quot;}" href="https://github.com/trending">
      Trending

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to Collections&quot;,&quot;label&quot;:&quot;ref_cta:Collections;&quot;}" href="https://github.com/collections">
      Collections

    
</a></li>

            </ul>
          </div>
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:google/graph-mining" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="TS3QoKfc2IPYUoICFpVo2Vq_OJwEQG4ezUV0KZ15uWK0-Dd-njoo8cNHwfCVEG07GmdfFG_jnsfcHPPLOSjuYw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="google/graph-mining" data-current-org="google" data-current-owner="" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <div data-view-component="true">        <!-- '"` --><!-- </textarea></xmp> --><form id="code-search-feedback-form" data-turbo="false" action="/search/feedback" accept-charset="UTF-8" method="post">
          <p>We read every piece of feedback, and take your input very seriously.</p>
          
          
          <label for="include_email">Include my email address so I can be contacted</label>
</form></div>
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input><div>
            <p><a href="https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2Fgoogle%2Fgraph-mining" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="73e2d2b0de1b008326d5fb468636efc41d9c02ac9911b800d296626ea61b2c86" data-ga-click="(Logged out) Header, clicked Sign in, text:sign-in">
              Sign in
            </a>
          </p></div>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=google%2Fgraph-mining" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="73e2d2b0de1b008326d5fb468636efc41d9c02ac9911b800d296626ea61b2c86" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div>
  </div>
</header>

      
    </div>

  








    


    
    <include-fragment data-base-src="https://github.com/notifications/beta/shelf"></include-fragment>






  <div itemscope="" itemtype="http://schema.org/SoftwareSourceCode" data-commit-hovercards-enabled="" data-discussion-hovercards-enabled="" data-issue-and-pr-hovercards-enabled="">
    <main id="js-repo-pjax-container">
      
      
  





    
    

    






  
  <div id="repository-container-header" data-turbo-replace="">

      <div>

        <div>
      
    
    <p><span itemprop="author">
      <a rel="author" data-hovercard-type="organization" data-hovercard-url="/orgs/google/hovercard" data-octo-click="hovercard-link-click" data-octo-dimensions="link_type:self" href="https://github.com/google">
        google
</a>    </span>
    <span>/</span>
    <strong itemprop="name">
      <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining">graph-mining</a>
    </strong>

    <span></span><span>Public</span>
  </p></div>

        <div id="repository-details-container" data-turbo-replace="">
            <ul>
    
      

  <li>
            <a href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f4edbd2d3226daa305e9b01fff9f3937e084479b0ef9459d3c1829990d34df4e" aria-label="You must be signed in to change notification settings" data-view-component="true">    Notifications
</a>
  </li>

  <li>
          <a icon="repo-forked" id="fork-button" href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;repo details fork button&quot;,&quot;repository_id&quot;:389477745,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="fc861c424dd0f860f742af0ee10fc5bd1ebcdcfcf0017744d9cea0aba8a814a8" data-view-component="true">    Fork
    <span id="repo-network-counter" data-pjax-replace="true" data-turbo-replace="true" title="6" data-view-component="true">6</span>
</a>
  </li>

  <li>
        <div data-view-component="true">
        <a href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:389477745,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="ce4377d60b45e684cca5fab16552ad973cf8447c360d6bcc7091ef616538cb65" aria-label="You must be signed in to star a repository" data-view-component="true">    <span data-view-component="true">
          Star
</span>          <span id="repo-stars-counter-star" aria-label="171 users starred this repository" data-singular-suffix="user starred this repository" data-plural-suffix="users starred this repository" data-turbo-replace="true" title="171" data-view-component="true">171</span>
</a>        </div>
  </li>

</ul>

        </div>
      </div>

        <div id="responsive-meta-container" data-turbo-replace="">

    <h3>License</h3>
  <p>
    <a href="https://github.com/google/graph-mining/blob/main/LICENSE" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}">
      
     Apache-2.0 license
    </a>
  </p>


    <p>
        <a href="https://github.com/google/graph-mining/stargazers">
          
          <span>171</span>
          stars
</a>        <a href="https://github.com/google/graph-mining/forks">
          
          <span>6</span>
          forks
</a>        <a href="https://github.com/google/graph-mining/activity">
          
          <span>Activity</span>
</a>    </p>

      <div>
        <div data-view-component="true">
        <a href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;star button&quot;,&quot;repository_id&quot;:389477745,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="ce4377d60b45e684cca5fab16552ad973cf8447c360d6bcc7091ef616538cb65" aria-label="You must be signed in to star a repository" data-view-component="true">    <span data-view-component="true">
          Star
</span>
</a>        </div>
        <p>
                <a href="https://github.com/login?return_to=%2Fgoogle%2Fgraph-mining" rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;notification subscription menu watch&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;LOG_IN&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="f4edbd2d3226daa305e9b01fff9f3937e084479b0ef9459d3c1829990d34df4e" aria-label="You must be signed in to change notification settings" data-view-component="true">    Notifications
</a>
        </p>
      </div>
  </div>


          <nav data-pjax="#js-repo-pjax-container" aria-label="Repository" data-view-component="true">

  <ul data-view-component="true">
      <li data-view-component="true">
  <a id="code-tab" href="https://github.com/google/graph-mining" data-tab-item="i0code-tab" data-selected-links="repo_source repo_downloads repo_commits repo_releases repo_tags repo_branches repo_packages repo_deployments /google/graph-mining" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g c" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Code&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" aria-current="page" data-view-component="true">
    
              
        <span data-content="Code">Code</span>
          <span id="code-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true"></span>


    
</a></li>
      <li data-view-component="true">
  <a id="issues-tab" href="https://github.com/google/graph-mining/issues" data-tab-item="i1issues-tab" data-selected-links="repo_issues repo_labels repo_milestones /google/graph-mining/issues" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g i" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Issues&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Issues">Issues</span>
          


    
</a></li>
      <li data-view-component="true">
  <a id="pull-requests-tab" href="https://github.com/google/graph-mining/pulls" data-tab-item="i2pull-requests-tab" data-selected-links="repo_pulls checks /google/graph-mining/pulls" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g p" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Pull requests&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Pull requests">Pull requests</span>
          


    
</a></li>
      <li data-view-component="true">
  <a id="actions-tab" href="https://github.com/google/graph-mining/actions" data-tab-item="i3actions-tab" data-selected-links="repo_actions /google/graph-mining/actions" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g a" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Actions&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Actions">Actions</span>
          <span id="actions-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true"></span>


    
</a></li>
      <li data-view-component="true">
  <a id="projects-tab" href="https://github.com/google/graph-mining/projects" data-tab-item="i4projects-tab" data-selected-links="repo_projects new_repo_project repo_project /google/graph-mining/projects" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g b" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Projects&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Projects">Projects</span>
          


    
</a></li>
      <li data-view-component="true">
  <a id="security-tab" href="https://github.com/google/graph-mining/security" data-tab-item="i5security-tab" data-selected-links="security overview alerts policy token_scanning code_scanning /google/graph-mining/security" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-hotkey="g s" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Security&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Security">Security</span>
          <include-fragment src="/google/graph-mining/security/overall-count" accept="text/fragment+html"></include-fragment>

    
</a></li>
      <li data-view-component="true">
  <a id="insights-tab" href="https://github.com/google/graph-mining/pulse" data-tab-item="i6insights-tab" data-selected-links="repo_graphs repo_contributors dependency_graph dependabot_updates pulse people community /google/graph-mining/pulse" data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" data-analytics-event="{&quot;category&quot;:&quot;Underline navbar&quot;,&quot;action&quot;:&quot;Click tab&quot;,&quot;label&quot;:&quot;Insights&quot;,&quot;target&quot;:&quot;UNDERLINE_NAV.TAB&quot;}" data-view-component="true">
    
              
        <span data-content="Insights">Insights</span>
          <span id="insights-repo-tab-count" data-pjax-replace="" data-turbo-replace="" title="Not available" data-view-component="true"></span>


    
</a></li>
</ul>
    <div data-view-component="true">        <details data-view-component="true">
    <summary role="button" data-view-component="true">          <div>
            
            <p><span>More</span>
          </p></div>
</summary>
    <details-menu role="menu" data-view-component="true">
          <ul>
              
              
              
              
              
              
              
          </ul>
</details-menu>
</details></div>
</nav>

  </div>

  



<turbo-frame id="repo-content-turbo-frame" target="_top" data-turbo-action="advance">
    <div id="repo-content-pjax-container">
  


  

  <include-fragment src="/google/graph-mining/spoofed_commit_check/c1c54b486f7a8a5f83a88ead02a211a9372e33b0" data-test-selector="spoofed-commit-check"></include-fragment>

  <div data-view-component="true">
  <div data-view-component="true">        
        
        <div>
  
<div>
  <details id="branch-select-menu" data-hydro-click-payload="{&quot;event_type&quot;:&quot;repository.click&quot;,&quot;payload&quot;:{&quot;target&quot;:&quot;REFS_SELECTOR_MENU&quot;,&quot;repository_id&quot;:389477745,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="793927d0e8b757a802264854cff8e7f6df6255e8df46c96117c067ca25207868">
    <summary data-hotkey="w" title="Switch branches or tags">
      
      <span data-menu-button="">main</span>
      <span></span>
    </summary>

    
<div>
    <header>
      <span>Switch branches/tags</span>
      
    </header>

    <input-demux data-action="tab-container-change:input-demux#storeInput tab-container-changed:input-demux#updateInput">
      <tab-container>
        

        

        <div role="tabpanel" id="ref-list-branches" data-filter-placeholder="Filter branches/tags" tabindex="">
          <ref-selector type="branch" data-targets="input-demux.sinks" data-action="
              input-entered:ref-selector#inputEntered
              tab-selected:ref-selector#tabSelected
              focus-list:ref-selector#focusFirstListMember
            " query-endpoint="/google/graph-mining/refs" cache-key="v0:1627264726.048119" current-committish="bWFpbg==" default-branch="bWFpbg==" name-with-owner="Z29vZ2xlL2dyYXBoLW1pbmluZw==" prefetch-on-mouseover="">

            <template data-target="ref-selector.fetchFailedTemplate">
              <div class="SelectMenu-message" data-index="{{ index }}">Could not load branches</div>
            </template>

              <template data-target="ref-selector.noMatchTemplate">
    <div class="SelectMenu-message">Nothing to show</div>
</template>


            

              

<template data-target="ref-selector.itemTemplate">
  <a href="https://github.com/google/graph-mining/tree/{{ urlEncodedRefName }}" class="SelectMenu-item" role="menuitemradio" rel="nofollow" aria-checked="{{ isCurrent }}" data-index="{{ index }}">
    <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check SelectMenu-icon SelectMenu-icon--check">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    <span class="flex-1 css-truncate css-truncate-overflow {{ isFilteringClass }}">{{ refName }}</span>
    <span hidden="{{ isNotDefault }}" class="Label Label--secondary flex-self-start">default</span>
  </a>
</template>


              
          </ref-selector>

        </div>

        
      </tab-container>
    </input-demux>
  </div>

  </details>

</div>


<div data-modal-dialog-overlay="">
  <modal-dialog role="dialog" id="warn-tag-match-create-branch-dialog" aria-modal="true" aria-labelledby="warn-tag-match-create-branch-dialog-header" data-view-component="true">
      <header>
        <div>
          <p>
            <h2 id="warn-tag-match-create-branch-dialog-header">Name already in use</h2>
          </p>
          
        </div>
      </header>
    <div>
      
          <p>      A tag already exists with the provided branch name. Many Git commands accept both tag and branch names, so creating this branch may cause unexpected behavior. Are you sure you want to create this branch?
</p>

    </div>
      
</modal-dialog></div>



  <p>
    <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/branches">
          
          <strong>1</strong>
          <span>branch</span>
    </a>
    <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/tags">
      
        <strong>0</strong>
        <span>tags</span>
    </a>
  </p>

  

  <include-fragment src="/google/graph-mining/overview_actions/main"></include-fragment>


    <p><span>
        
<get-repo>
    
    <details data-action="
               toggle:get-repo#onDetailsToggle
               keydown:get-repo#onDetailsKeydown">
        <summary data-hydro-click="{&quot;event_type&quot;:&quot;repository.click&quot;,&quot;payload&quot;:{&quot;repository_id&quot;:389477745,&quot;target&quot;:&quot;CLONE_OR_DOWNLOAD_BUTTON&quot;,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="c7dcf1fb8c9b620d1955a014a722ef2adb656b42264d20955ad62fc05b191511" data-view-component="true">    <span>
      <span>Code</span>
    </span>
      <span>
        
      </span>
</summary>  
      <div data-target="get-repo.modal">
    <tab-container data-view-component="true">
  <div with_panel="true" data-view-component="true">
    
    <ul role="tablist" aria-label="Choose where to access your code" data-view-component="true">
        <li role="presentation" data-view-component="true">
  </li>
        <li role="presentation" data-view-component="true">
  </li>
</ul>    
</div>    <div id="local-panel" role="tabpanel" tabindex="0" aria-labelledby="local-tab" data-view-component="true">          <ul>
              <li>
  <a href="https://docs.github.com/articles/which-remote-url-should-i-use" rel="noopener" target="_blank" aria-label="Which remote URL should I use?">
  
</a>

<div>
  <p>
  Clone
</p></div>

<tab-container>

  

  <div role="tabpanel">
    

    <p>
        Use Git or checkout with SVN using the web URL.
    </p>
  </div>


  
</tab-container>

</li>
<li data-platforms="windows,mac">
  <a data-hydro-click="{&quot;event_type&quot;:&quot;clone_or_download.click&quot;,&quot;payload&quot;:{&quot;feature_clicked&quot;:&quot;OPEN_IN_DESKTOP&quot;,&quot;git_repository_type&quot;:&quot;REPOSITORY&quot;,&quot;repository_id&quot;:389477745,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="dbdb1793a8cc4d814be5c72e7a04178035786fce7ec03d49c543906902778098" data-action="click:get-repo#showDownloadMessage" href="https://desktop.github.com/">
    
    Open with GitHub Desktop
</a></li>
<li>
  <a rel="nofollow" data-hydro-click="{&quot;event_type&quot;:&quot;clone_or_download.click&quot;,&quot;payload&quot;:{&quot;feature_clicked&quot;:&quot;DOWNLOAD_ZIP&quot;,&quot;git_repository_type&quot;:&quot;REPOSITORY&quot;,&quot;repository_id&quot;:389477745,&quot;originating_url&quot;:&quot;https://github.com/google/graph-mining&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="4821d2c47444b6f65bd4501e666123a43fff352c949914e0ff1cc083bb57c9cf" data-ga-click="Repository, download zip, location:repo overview" data-open-app="link" data-turbo="false" href="https://github.com/google/graph-mining/archive/refs/heads/main.zip">
    
    Download ZIP
</a></li>

          </ul>
</div>
    
</tab-container>    
</div>
    </details>


</get-repo>

    </span>

    <span>
        

    </span>
</p></div>




        


<div>
  <div>
    <h2>Latest commit</h2>
    <div data-issue-and-pr-hovercards-enabled="">
      <include-fragment src="/google/graph-mining/tree-commit/c1c54b486f7a8a5f83a88ead02a211a9372e33b0" aria-busy="true" aria-label="Loading latest commit">
        
        
</include-fragment>      <div>
        <h2>Git stats</h2>
        <ul>
          <li>
            <a data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/commits/main">
              
              <span>
                    <strong>8</strong>
                    <span aria-label="Commits on main">
                      commits
                    </span>
              </span>
            </a>
          </li>
        </ul>
      </div>
    </div>
  </div>
    <h2 id="files">Files</h2>
    


  <include-fragment src="/google/graph-mining/file-list/main">
        <a data-hotkey="y" href="https://github.com/google/graph-mining/tree/c1c54b486f7a8a5f83a88ead02a211a9372e33b0">Permalink</a>
  <div data-view-component="true">
  <p>
    Failed to load latest commit information.


  
</p></div>  <div role="grid" aria-labelledby="files" data-hpc="">
      <div role="row">
        <p>Type</p>
        <p>Name</p>
        <p>Latest commit message</p>
        <p>Commit time</p>
      </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="docs" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/tree/main/docs">docs</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="in_memory" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/tree/main/in_memory">in_memory</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="utils" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/tree/main/utils">utils</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title=".bazelrc" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/.bazelrc">.bazelrc</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="BUILD.oss" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/BUILD.oss">BUILD.oss</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="CONTRIBUTING.md" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/CONTRIBUTING.md">CONTRIBUTING.md</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="LICENSE" data-turbo-frame="repo-content-turbo-frame" itemprop="license" href="https://github.com/google/graph-mining/blob/main/LICENSE">LICENSE</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="README.md" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/README.md">README.md</a></span>
            </p></div>

            

            

          </div>
          <div role="row">
            

            <div role="rowheader">
              <p><span><a title="WORKSPACE.bazel" data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/blob/main/WORKSPACE.bazel">WORKSPACE.bazel</a></span>
            </p></div>

            

            

          </div>
    </div>

</include-fragment>


</div>

  
    
      <div id="readme" data-tagsearch-path="README.md" data-tagsearch-lang="Markdown">

        <div>
          <p>
            <h2>
              <a href="#readme" data-view-component="true">README.md</a>
            </h2>
          </p>
        </div>

          <div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-the-graph-mining-library" dir="auto"><a href="#the-graph-mining-library">The Graph Mining Library</a></h2>
<p dir="auto">This project includes some of Google's Graph Mining tools, namely in-memory
clustering. Our tools can be used for solving data mining and machine learning
problems that either inherently have a graph structure or can be formalized as
graph problems.</p>
<p dir="auto">For questions/comments, please create an issue on this repository.</p>
</article>
          </div>
      </div>



</div>
  <div data-pjax="" data-view-component="true">
        <div>
            <h2>About</h2>

    <p>
      No description, website, or topics provided.
    </p>


  <h3>Resources</h3>
  <p>
    <a data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:readme&quot;}" href="#readme">
      
      Readme
</a>  </p>

<h3>License</h3>
  <p>
    <a href="https://github.com/google/graph-mining/blob/main/LICENSE" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:license&quot;}">
      
     Apache-2.0 license
    </a>
  </p>


  <h3>Code of conduct</h3>
  <p>
    <a href="https://github.com/google/graph-mining/blob/main/docs/code-of-conduct.md" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:code of conduct&quot;}">
      
      Code of conduct
    </a>
  </p>

  <h3>Security policy</h3>
  <p>
    <a href="https://github.com/google/graph-mining/security/policy" data-analytics-event="{&quot;category&quot;:&quot;Repository Overview&quot;,&quot;action&quot;:&quot;click&quot;,&quot;label&quot;:&quot;location:sidebar;file:security policy&quot;}">
      
      Security policy
    </a>
  </p>

<include-fragment src="/google/graph-mining/hovercards/citation/sidebar_partial?tree_name=main">
</include-fragment>



<p>
  <a data-turbo-frame="repo-content-turbo-frame" href="https://github.com/google/graph-mining/activity" data-view-component="true">
    
    <span>Activity</span>
</a></p>

<h3>Stars</h3>
<p>
  <a href="https://github.com/google/graph-mining/stargazers" data-view-component="true">
    
    <strong>171</strong>
    stars
</a></p>

<h3>Watchers</h3>
<p>
  <a href="https://github.com/google/graph-mining/watchers" data-view-component="true">
    
    <strong>7</strong>
    watching
</a></p>

<h3>Forks</h3>
<p>
  <a href="https://github.com/google/graph-mining/forks" data-view-component="true">
    
    <strong>6</strong>
    forks
</a></p>

  <div>
    <p><a href="https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fgraph-mining&amp;report=google+%28user%29">
        Report repository
</a>  </p></div>

          </div>

        
        
            <div>
                <h2 data-pjax="#repo-content-pjax-container" data-turbo-frame="repo-content-turbo-frame">
  <a href="https://github.com/google/graph-mining/releases" data-view-component="true">
    Releases
</a></h2>

    <p>No releases published</p>

              </div>

        
        
            <div>
                <h2>
  <a href="https://github.com/orgs/google/packages?repo_name=graph-mining" data-view-component="true">
    Packages
      
</a></h2>


      <p>
        No packages published <br>
      </p>



              </div>

        
        
        
        
            <div>
                <h2>Languages</h2>

<ul>
    <li>
        <a href="https://github.com/google/graph-mining/search?l=c%2B%2B" data-ga-click="Repository, language stats search click, location:repo overview">
          
          <span>C++</span>
          <span>93.0%</span>
        </a>
    </li>
    <li>
        <a href="https://github.com/google/graph-mining/search?l=starlark" data-ga-click="Repository, language stats search click, location:repo overview">
          
          <span>Starlark</span>
          <span>6.8%</span>
        </a>
    </li>
    <li>
        <a href="https://github.com/google/graph-mining/search?l=c" data-ga-click="Repository, language stats search click, location:repo overview">
          
          <span>C</span>
          <span>0.2%</span>
        </a>
    </li>
</ul>

              </div>

              </div>
  
</div></div>

</turbo-frame>


    </main>
  </div>

          




  

    <template id="site-details-dialog">
  <details class="details-reset details-overlay details-overlay-dark lh-default color-fg-default hx_rsm" open="">
    <summary role="button" aria-label="Close dialog"></summary>
    <details-dialog class="Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal">
      <button class="Box-btn-octicon m-0 btn-octicon position-absolute right-0 top-0" type="button" aria-label="Close dialog" data-close-dialog="">
        <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-x">
    <path d="M3.72 3.72a.75.75 0 0 1 1.06 0L8 6.94l3.22-3.22a.749.749 0 0 1 1.275.326.749.749 0 0 1-.215.734L9.06 8l3.22 3.22a.749.749 0 0 1-.326 1.275.749.749 0 0 1-.734-.215L8 9.06l-3.22 3.22a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042L6.94 8 3.72 4.78a.75.75 0 0 1 0-1.06Z"></path>
</svg>
      </button>
      <div class="octocat-spinner my-6 js-details-dialog-spinner"></div>
    </details-dialog>
  </details>
</template>

    

    <template id="snippet-clipboard-copy-button">
  <div class="zeroclipboard-container position-absolute right-0 top-0">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn js-clipboard-copy m-2 p-0 tooltipped-no-delay" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon m-2">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none m-2">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>
<template id="snippet-clipboard-copy-button-unpositioned">
  <div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div>
</template>




    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I applied to 250 jobs and timed how long each one took (246 pts)]]></title>
            <link>https://www.careerfair.io/online-maze</link>
            <guid>37753292</guid>
            <pubDate>Tue, 03 Oct 2023 15:36:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.careerfair.io/online-maze">https://www.careerfair.io/online-maze</a>, See on <a href="https://news.ycombinator.com/item?id=37753292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <section id="intro">
        <p>
          Applying to jobs online is like navigating a maze. 
        </p>

        <p>
          Amidst the special torture that is resume parsing software, the inability to reuse information across different application tracking systems (ATS), and the existence of a certain company that rhymes with every day of the week, it can get pretty frustrating.
        </p>

        <p>
          I wanted to explore what factors make a job application more or less frustrating. 
        </p>

        <p>
          For example, what industries have the worst application processes? Do big companies ask for more information than small companies? What is it about websites like Workday that make them really hard to use? 
        </p>

        <p>
          To answer these questions, I applied to 250 jobs. One by one. Click by click. No Linkedin Easy Apply, no shortcuts – just straight from the careers page. 
        </p>

        <p>
          I timed how long it took me to go from “apply to job” to “submit application”.
        </p>

        <center>
          <img src="https://www.careerfair.io/maze_assets/ka_mov_v4.gif" alt="">
        </center>

        <p>
          Make no mistake: I sacrificed my soul for this article. I created over 83 accounts and spent a total of 11 hours scrolling. I was originally going to do this for 500 companies, but wanted to chop my head off halfway. 
        </p>

        <p>
          I did this for a mix of companies – Fortune 500 to early stage startups, spread out across different industries from software to manufacturing. The <i>type</i> of role I applied to was kept constant: engineering / product focused.
        </p>

        <!-- <p>
          I recorded how long it took me to apply. I noted down the ATS that was being used. I recorded whether I had to create an account. And I also recorded if I was redirected to some external page during the application process. 
        </p> -->

        <center>
          <figure>
          <img src="https://www.careerfair.io/maze_assets/spreadsheet_recording_v3.gif" alt="">
            <figcaption>Get the spreadsheet with the full raw data <a href="https://mailchi.mp/1a15a90c4aeb/company_raw_data_leadmagnet" target="_blank">here</a></figcaption>
          </figure>
        </center> 

        <p>
          The outcome? An average of over two and a half minutes per application—162 seconds of your life you'll never get back. But as we dig deeper, you'll discover that these 162 seconds only scratch the surface of an often maddening process. 
        </p>

        <div>
          <p><bold>Key Takeaways</bold></p>
          <ul>
            <li><b>Average Application Time: </b>On average, it took a bit over two and a half minutes to apply to a job.</li>
            <li><b>Company Size Impact: </b>If company size doubles, the application time increases by 5%. If company size increases by a factor of 10, then the app time increases by 20%.</li>
            <li><b>Industry Influence: </b>Being a government company is the single largest determinant of a long application, followed closely by aerospace and consulting firms.</li>
            <li><b>Longest Application: </b>The longest application time went to the United States Postal Service (10 minutes and 12 seconds).</li>
            <li><b>Shortest Application: </b>On the other hand, It took me just 17 seconds to apply to Renaissance Technologies.</li>
            <li><b>ATS Impact: </b>Older ATS like Workday and Taleo make job applications as much as 128% longer.</li>
          </ul> 
        </div>

        <p>
          If you'd like to access the complete raw data, I can send you the Google Spreadsheet via email <a href="https://mailchi.mp/1a15a90c4aeb/company_raw_data_leadmagnet" target="_blank">here.</a> 
        </p>

        <p>
          Let's dive in.
        </p>
      </section>

      <!-- <section class="medium-section med_img_container">
        <header class="fader">
          <h1 id="thesetup">The Setup</h1>
        </header>
      </section> -->
        <section id="thesetup">
          <h2>The Setup</h2>
          <p>
            There’s no real method to the 250 companies I pick. I’m just typing names into Google and trying to vary it up. Where does Trisha work? What was that billboard I saw? It's all up for grabs.
          </p>

          <p>
            Here’s the distribution of the 250 companies by size:
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/employee_breakdown_w2.png" alt="">
          </center>

          <p>
            Some examples of companies in each range: 
          </p>

          <ul>
              <li>1-500 → Glean, Quizlet, Gumroad</li>
              <li>500-5,000 → Notion, Dolby, Moloco </li>
              <li>5,000-50,000 → Airbnb, Genentech, Logitech </li>
              <li>50,000-100,000 → HP, American Express, Pfizer </li>
              <li>100,000+ → Wells Fargo, Lockheed Martin, General Motors </li>
            </ul> 
          


          <p>
            And here’s a look at the different types of industries represented:
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/industry_breakdown.png" alt="">
          </center>      

          <p>
            I used a mix of Linkedin and Crunchbase for categorization. 
          </p>

          <p>
            Before we get started, if you’d like you can read up on my <a href="https://docs.google.com/document/d/1A0I9_WBN9zIqwezM6OXqmOl3LPqaq5704EPmGDTDiYI/edit" target="_blank">methodology</a> for applying to each job (aka assumptions I made, what data I chose to submit, and how much effort I put into each application).
          </p>

          <h3>What makes a job application so frustrating</h3>

          <p>
            Generally speaking, the more frustrating a job application, the longer it takes to complete. 
          </p>

          <p>
            The three main factors that might influence how long a job application is (as measured in my data):
          </p>

          <ol>
            <li><b>Company size</b> → I would expect bigger companies to ask more questions.</li>
            <li><b>The ATS that is being used</b> → I would expect clunkier, older ATS to make job applications longer.</li>
            <li><b>Company industry</b> → I would expect more “traditional” industries to ask more questions.</li>
          </ol> 

          <p>
            We’re going to model the relationship between the above three factors and the amount of time it takes to complete a job application. To do this, we’re going to use a technique called linear regression.           </p>

          <p>
            Regression is about the way two measurements change together. It can help us make predictions.
          </p>

          <p>
            For example, if I add 10 employees to a company, how many seconds will that add to the company’s job application process?
          </p>

          <p>Since we have other factors like ATS and Industry, we will also account for those. For now, though, let’s just focus on each factor one by one. 
          </p>



        </section>
        <section id="size">
          <h2>Company Size</h2>
          <p>
            Let’s first plot the data as is: 
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/size_time_linear_v5.png" alt="">
          </center>   
          <p>
            Yes, I know, this isn’t the most useful graph. I’m going to spruce it up real quick, I promise. 
          </p>

          <p>
            The United States Postal Service has a job application that took over 10 minutes to complete. Navigating their portal felt like using Internet Explorer in 2003: 
          </p>


          <center>
            <img src="https://www.careerfair.io/maze_assets/usps_internet_explorer.png" alt="">
          </center>  

          <p>
            Netflix’s application was just 20 seconds - their only mandatory requirements are your resume and basic info. 
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/netflix_app_basic.png" alt="">
          </center>  

          <p>
            Apple took me 71 seconds, still pretty fast for a company that has over 270,000 employees (PWC, which has a similar number of employees, took me almost six times as long). 
          </p>

          <p>
            Okay, back to the chart. There are a couple of problems with it. 
          </p>

          <p>
            First, the data is not linear. This is a problem if we want to use linear regression.
          </p>

          <p>
            Second, the company size scale is hard to interpret because of the many data points clumped together near zero (representing all the smaller companies). 
          </p>

          <p>
            We can resolve both these issues with the following insight:
          </p>

          <p>
            There is a big difference between going from 10 to 100 employees and, say, 10,000 to 10,100 employees. The first represents major changes in company structure: you might actually hire a proper HR team, a bunch of recruiters, and build out your candidate experience. The second, though, is pretty much just business as usual - think of a multinational opening up a satellite office or a regular month of hiring. 
          </p>

          <p>
            Since we want to account for this, our data is better suited to a log scale than a linear scale. I will also transform our Y-axis, the application time, to a log scale because it helps normalize the data. 
          </p>

          <p>
            If we plot both our variables on a log-log scale, we get the below chart:
          </p>
         
          <center>
            <img src="https://www.careerfair.io/maze_assets/size_time_log_v8.png" alt="">
          </center> 
          <p>
            Better right? This is the same data as the last chart, but with different axes that fits the data better, we observe a linear relationship. 
          </p>

          <p>
            We have the usual suspects in the top right: Government organizations, professional services firms, and some of the tech industry dinosaurs. 
          </p>

          <p>
            The variance in application times across smaller companies, like startups, is interesting. For example, many of the startups with longer application times (e.g OpenAI, Posthog, Comma.AI) reference that they are looking for “exceptional” candidates on their careers page. (Note that OpenAI has changed its application since I last analyzed it - it’s now much faster, but when I went through they asked for a mini essay on why you’re exceptional). 
          </p>

          <p>
            One thing that I was expecting to see was competitors mirroring each other’s application times. This is most closely represented with the consulting firms like Deloitte, E&amp;Y, KPMG, etc all clumped together. McKinsey and Bain, the two most prestigious consulting firms, have applications that take longer to complete. 
          </p>

          <p>
            This doesn’t necessarily seem to be the case with the FAANG companies. 
          </p>

          <p>
            We can also calculate the correlation coefficient for this graph. This is a statistical measure of the strength of a linear relationship between two variables. The closer to 1 the value, the stronger the relationship.
          </p>
          <p>
            For the above data, we get a correlation coefficient of 0.58, which is a moderate to strong association.
          </p>
          <p>
            Note that on its own, this doesn't tell us anything about causation. But it does start to point us in some type of direction.
          </p>

          <p>
            It's not rocket science: big companies ask for more stuff. Sometimes they ask for the last 4 digits of your SSN. 
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/ssn_box_v2.png" alt="">
              <figcaption>Newell Brands</figcaption>
            </figure>
          </center> 
          <p>
            Sometimes they even ask if you’d be okay going through a polygraph:
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/polygraph_v2.png" alt="">
              <figcaption>NASA</figcaption>
            </figure>
          </center> 
          <p>
            And sometimes they even ask you to explain employment gaps:
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/employment_gap_v4.png" alt="">
            </figure>
          </center> 
          
          <p>
            An argument here is that if big companies didn’t have some sort of barriers in their application process, they’d get swarmed with applications.
          </p>

          <p>
            Consider the fact that Google gets 3 million applications every year. Deloitte gets 2 million. Without some sort of initial friction in the application process, those numbers would be even higher. That friction almost serves as a reliable filter for interest. 
          </p>

          <p>
            If you’re an employer, you don’t really care about the people using a shotgun approach to apply. You want the candidates that have a real interest in the position. On the other hand, if you’re a candidate, the reality is such that the shotgun approach to apply is arguably the most efficient. 
          </p>

          <p>
            So we have this inherent tension between companies and candidates. Candidates want the most bang for their buck, companies don’t want thousands of irrelevant resumes. 
          </p>

          <p>
            And in the middle, we have the plethora of application tracking software that can often be quite old and clunky. 
          </p>


          <div>
            <center>
              <h2> Sorry to interrupt </h2>
              <p>
                  I hope you're enjoying the article. In my next article, I investigate the Strange World of Referral Bonuses - enter your email and I'll deliver it straight to your inbox when it's out in 3 weeks.
                </p>
            </center>
            <center>
              
            </center>
          </div>

        </section>

        

        <section id="ats">
          <h2>ATS</h2>
          <p>
            Everytime I came face to face with a company that used Workday as their ATS, I died a bit inside. This is because Workday makes you:
          </p>
          <ol>
            <li>create a new account every single time</li>
            <li>redirects you away from the careers page</li>
          </ol>
          <p>
            I defined a redirect as one when the job description is not listed on the same page as the first input box part of the application. 
          </p>
          <p>
            This isn’t a perfectly accurate measure, but it does allow us to differentiate between the modern ATS like Greenhouse and older ones like Workday. 
          </p>
          <p>
            With every ATS, I implicitly had some type of “how easy is this going to be” metric in my head. 
          </p>
          <p>
            We can try to represent this “how easy is this going to be” metric a bit more concretely using the matrix below. 
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/ats_matrix_v2.png" alt="">
          </center> 
          <p>
            Ideally, you want the ATS to be in the bottom left corner. This creates an experience that is low friction and fast. 
          </p>
          <p>
            If we plot application time versus ATS, this is what we get: 
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/ats_time_v4.png" alt="">
          </center> 

          <p>
            The ATS that don’t make you create an account and don’t redirect you are tied to lower application times than the ones that do. 
          </p>

          <p>
            One possibility is that certain companies are more likely to use certain ATS. Big companies might use Workday for better compliance reporting. Same with the industry - maybe B2C software companies use the newer ATS on the market. These would be confounding variables, meaning that we may misinterpret a relationship between the ATS and the application time when in fact there isn’t one (and the real relationship is tied to the industry or size). 
          </p>
          <p>
            So to properly understand whether the ATS actually has an effect on application time, we need to control for our other variables. We’ll do this in the final section when we run a regression including all our variables. 
          </p>
          <p>
            One of the big frustrations surrounding different ATS is that when you upload your resume, you then need to retype out your experience in the boxes because the ATS resume parser did it incorrectly. For example, I went to UC Berkeley but sometimes got this:
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/pwc_workday.png" alt="">
              <figcaption>PWC / Workday</figcaption>
            </figure>
          </center> 
          <p>
            The only resume parser that didn't seem abysmal was the one from Smart Recruiters. TikTok's resume parser also isn't bad.  
          </p>
          <!-- <center>
            <figure>
            <img
              src="/maze_assets/resume_parser_table_v3.png"
              alt=""
              />
              <figcaption>It's helpful to have two versions of your resume: a normal PDF one, and another TXT version that is easier to parse.</figcaption>
            </figure>
          </center>  -->
          <p>
            Another frustrating experience is tied to inconsistency between the company I'm applying to and the ATS.
          </p>
          <center>
            <figure>
            <img src="https://www.careerfair.io/maze_assets/dual_branding_v5.png" alt="">
              <figcaption>Possible confusion</figcaption>
            </figure>
          </center> 
          <p>
            A company’s application process is often the first touchpoint you have with their brand. Startups competing for the best talent can't afford extra steps in their process. Apple and Facebook can.
          </p>
          <p>
            Whilst the average time to complete a job application may only be 162 seconds, the fact that  many ATS require steps like account creation and authentication can lead to application fatigue.
          </p>
          <p>
            It’s not necessarily the explicit amount of time it takes, it’s the steps involved that drain you of energy and make you want to avoid applying to new jobs.
          </p>
        </section>
        <section id="industry">
          <h2>Industry</h2>
          <p>
            Okay, so far we’ve looked at company size and the ATS as a loose indicator of what might make a job application frustrating. What about the company industry?
          </p>
          <p>
            You would expect industries like banking or professional services to have longer application times, because getting those jobs revolves around having a bunch of credentials which they likely screen for (and ask you to submit) early on in the process. 
          </p>
          <p>
            On the other hand, internet startups I’d expect to be quick and fast. Let’s find out if this is true.
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/industry_time_v4.png" alt="">
          </center> 
          <p>
            Hyped up industries like AI and Crypto have shorter application times. As expected, banks and consulting firms care about your GPA and ask you to submit it.  
          </p>
          <p>
            A government company has to basically verify your identity before they can even receive your application, so the process is entirely different and reflected in the submission time. 
          </p>
          <p>
            For many technology companies, the application process is almost like an extension of the company’s brand itself. For example, Plaid (an API first Fintech company), has a neat option where you can actually apply to the job via API:
          </p>
          <center>
            <img src="https://www.careerfair.io/maze_assets/plaid_api_cropped.png" alt="">
          </center> 

          <p>
            Roblox, a gaming company, allows people to submit job applications from within their <a href="https://gamerant.com/roblox-company-interview-job-applicants-in-game/">games</a>.
          </p>

          <p>
            We also notice differences between legacy companies and their newer competitors. If we compare legacy banks versus neobanks (like Monzo, Mercury, etc), the legacy players averaged around 250 seconds per job application whereas the neobanks averaged less than 60 seconds. 
          </p>

          <p>
            If you can’t compete on prestige, you need to find other ways. One of those ways can be through asking for less information upfront. 
          </p>

          <h3>Putting it together</h3>

          <p>
            Now that we've analyzed each variable - the company size, ATS, and the industry - to understand the separate relationship of each to application time, we can use linear regression to understand the <i>combined</i> relationships.
          </p>

          <p>
            This will allow us to determine what factors actually have an impact on the job application time versus which ones might just have had one when we looked at them in isolation.
          </p>

          <p>
            After some number crunching in R, I get the following results (I’ve only added the statistically significant factors – the ones with the “strongest evidence”):
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/regression_output_v2.png" alt="">
          </center> 

          <p>
            Here’s how you can interpret some of the information above:
          </p>

          <ul>
              <li>When a job app is for a company that is within the Government industry, the submission time goes up by 366% (assuming the size and ATS are constant). For the aerospace industry, this is 249% (and so on).</li>
              <li>When a job app is for a company using the Workday ATS, the submission times goes up by 128% (assuming the size and industry are constant). For the Phenom ATS, this is 110% (and so on). </li>
              <li>Our only (statistically significant) metric which seems to make job applications faster is the Lever ATS (42% shorter). </li>
            </ul> 
          

          <p>
            Okay, now what about company size? 
          </p>

          <p>
            Well, first up: company size is indeed statistically significant. So there is an effect. 
          </p>

          <p>
            However, its effect is not as strong as most of our other variables. To be precise, here are some ways to interpret our company size coefficient:
          </p>

          <ul>
              <li>If company size doubles, the app size increases by 5%</li>
              <li>If company size increases by a factor of 10, then the app time increases by 20%</li>
            </ul> 
          

          <p>
            This is a smaller effect size compared to ATS or industry (a 20% increases in app time for a 10x large company is a qualitatively smaller effect size than e.g. a 100% increase in app time for Taleo ATS). So although company size is statistically significant, it is not as strong of a driver as ATS and industry of app time.
          </p>

          <p>
            Another thing to note is that the effect of company size seems to level off as the company gets large enough (due to the log transformation). The plot below shows this. 
          </p>

          <center>
            <img src="https://www.careerfair.io/maze_assets/company_level.png" alt="">
          </center> 
        </section>
        <section id="conclusion">
          <h2>Wrapping it up</h2>

          <p>
            Two and a half minutes might not be too long, but it can feel like an eternity when you’re forced to answer the same questions and upload the same documents. Over and over again. 
            </p>
            <p>
            Think about catching a flight. All you want is to get on the jet. Hawaii awaits.  
            </p>
            <p>
            But first: the security line. You have to take your shoes off. You get patted down and your bag gets searched. The gate numbers don’t make sense. And then at the end of it, your flight’s delayed. Congrats.
            </p>
            <p>
            Applying to a job can feel similar. All you want to do is say aloha to the hiring manager, a real human being. 
            </p>
            <p>
            To even have the remote possibility of making that happen, you need to create an account and password, check your email, retype your entire resume, tell them the color of your skin, and explain why this company you’ve never heard of before is the greatest thing on Earth. 
            </p>
            <p>
            And for what? Most likely for the privilege of receiving an automated email about two weeks later rejecting you. 
            </p>
            <p>
            If we make it tiring and unappealing to look for new opportunities, then we prevent people from doing their best work. 
            </p>
            <p>
            But what would a world where applying took just a few seconds actually look like? Recruiters would get bombarded with resumes. It's possible to argue that job applications taking so long is a feature, not a bug. You get to filter for intent and narrow down your application pool.
            </p>
            <p>
            Is it fair to shift the burden of screening unqualified candidates onto good candidates that now need to provide so much information? Shouldn’t that burden fall on the recruiter? 
            </p>
            <!-- <p>
            There’s innovation happening in this space. Companies like Simplify help job seekers apply to thousands of jobs from just one platform. ChatGPT will make it faster to complete job applications too. 
            </p> -->
            <p>
            The truth is that applying to a job via the careers page is a bit of a rigged game. The odds are not in your favor.
            </p>
            <p>
             Sometimes, though, all you need is to only be right once.
            </p>

            <p>***</p>

            <p>
              If you made it all the way to the bottom, you're a star. This took a while to write. I hope you enjoyed it. 
            </p>

            <p>
              You might also enjoy reading:
            </p>
            <ul>
                <li><a href="https://www.careerfair.io/company-reviews">The Underground Economy of Glassdoor Reviews</a></li>
                <li><a href="https://www.careerfair.io/reviews/cover-letter-guide">My Guide To Writing A Killer Cover Letter</a></li>
                <li><a href="https://www.careerfair.io/job-hunt-story">My Job Hunt Story</a></li>
              </ul> 
            


        </section>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Predictive Policing Software Is Pretty Terrible at Predicting Crimes (122 pts)]]></title>
            <link>https://gizmodo.com/predictive-policing-cops-law-enforcement-predpol-1850893951</link>
            <guid>37753079</guid>
            <pubDate>Tue, 03 Oct 2023 15:19:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/predictive-policing-cops-law-enforcement-predpol-1850893951">https://gizmodo.com/predictive-policing-cops-law-enforcement-predpol-1850893951</a>, See on <a href="https://news.ycombinator.com/item?id=37753079">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure data-id="f038a16dedea41bd25c57efa7a87ca97" data-recommend-id="image://f038a16dedea41bd25c57efa7a87ca97" data-format="jpg" data-width="6720" data-height="3780" data-lightbox="true" data-recommended="true" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="shutterstock" data-hide="false" data-hidecredit="false"><p><span><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="https://i.kinja-img.com/image/upload/c_fit,f_auto,g_center,q_60,w_645/f038a16dedea41bd25c57efa7a87ca97.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="https://i.kinja-img.com/image/upload/c_fit,f_auto,g_center,q_60,w_1315/f038a16dedea41bd25c57efa7a87ca97.jpg"><img alt="Image for article titled Study Finds Predictive Policing Software Is Actually Pretty Terrible at Predicting Crimes" data-chomp-id="f038a16dedea41bd25c57efa7a87ca97" data-format="jpg" data-alt="Image for article titled Study Finds Predictive Policing Software Is Actually Pretty Terrible at Predicting Crimes" data-anim-src="" src="https://i.kinja-img.com/image/upload/c_fit,f_auto,g_center,q_60,w_645/f038a16dedea41bd25c57efa7a87ca97.jpg"></picture></div></span></p><p><figcaption>Image<!-- -->: <!-- -->zef art<!-- --> (<!-- -->Shutterstock<!-- -->)</figcaption></p></div><span data-id="f038a16dedea41bd25c57efa7a87ca97" data-recommend-id="image://f038a16dedea41bd25c57efa7a87ca97" data-format="jpg" data-width="6720" data-height="3780" data-lightbox="true" data-recommended="true" data-hide="false"></span></figure><div><p>Predictive policing—the trendy law enforcement field that uses data collection and analysis to try <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/crime-prediction-software-promised-to-be-free-of-biases-1848138977&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/crime-prediction-software-promised-to-be-free-of-biases-1848138977">to predict where crimes will occur</a></span>—has been wildly popular with police departments  across the country. Unfortunately, though some cops <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.police1.com/police-leader-playbook/articles/how-a-nebraska-police-chief-has-leveraged-technology-and-predictive-policing-to-decrease-crime-XpUVclO7j3qzXMAy/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.police1.com/police-leader-playbook/articles/how-a-nebraska-police-chief-has-leveraged-technology-and-predictive-policing-to-decrease-crime-XpUVclO7j3qzXMAy/" target="_blank" rel="noopener noreferrer">swear by it</a></span>, there hasn’t always been a ton of evidence <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://thenewstack.io/predictive-policing-real-just-not-effective/&quot;,{&quot;metric25&quot;:1}]]" href="https://thenewstack.io/predictive-policing-real-just-not-effective/" target="_blank" rel="noopener noreferrer">that the tech actually works</a></span> that well. In fact, a new study, released this week, seems to suggest that—for one community at least—it’s been pretty much a total waste of time.</p><div data-video-id="195771" data-monetizable="true" data-position="sidebar" data-video-title="Taylor Lorenz Talks &quot;Extremely Online&quot;" data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="170" data-playlist="195771,195747,195726" data-current="195771"><div><p>Taylor Lorenz Talks "Extremely Online"</p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/195771/195771_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195771/195771_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195771/195771_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/195771/195771_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/21281.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>A new joint investigation by <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes&quot;,{&quot;metric25&quot;:1}]]" href="https://themarkup.org/prediction-bias/2023/10/02/predictive-policing-software-terrible-at-predicting-crimes" target="_blank" rel="noopener noreferrer">The Markup</a></span> and <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/plainfield-geolitica-crime-predictions/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/plainfield-geolitica-crime-predictions/" target="_blank" rel="noopener noreferrer">Wired</a></span> shows that, for the city of Plainfield, New Jersey, predictive policing has been a giant, expensive mess—one that produced almost zero helpful results. </p><p>Both outlets looked into a large dataset provided by Plainfield PD that involved some 23,631 predictions made by  Geolitica, a crime prediction software. Geolitica (which was previously called PredPol <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://blog.predpol.com/geolitica-a-new-name-a-new-focus&quot;,{&quot;metric25&quot;:1}]]" href="https://blog.predpol.com/geolitica-a-new-name-a-new-focus" target="_blank" rel="noopener noreferrer">but rebranded</a></span> two years ago) claims to use “data-driven strategies” to help police identify so-called “hot spots”—places where crime is most likely to occur.  However, the dataset provided by the Plainfield PD showed that—during a roughly ten-month period, between Feb. 25 to Dec. 18, 2018—the software accurately predicted where crimes would occur with a “less than half a percent” success rate. </p><p>Captain David Guarino, of the Plainfield Police Department, seems to have been pretty upfront about the software’s shortcomings. He told The Markup: </p><blockquote data-type="BlockQuote"><p>“Why did we get PredPol? I guess we wanted to be more effective when it came to reducing crime. And having a prediction where we should be would help us to do that. I don’t know that it did that...I don’t believe we really used it that often, if at all. That’s why we ended up getting rid of it.”</p></blockquote><p>Gizmodo and The Markup previously collaborated on an investigation into Predpol’s software, finding that cops used it to <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/how-we-determined-predictive-policing-software-dispropo-1848139456&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/how-we-determined-predictive-policing-software-dispropo-1848139456">disproportionately targeted low-income communities of color</a></span>. <br></p><p>We reached out to Geolitica for comment on the recent study’s findings and will update this story if we receive a response. Wired <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wired.com/story/soundthinking-geolitica-acquisition-predictive-policing/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wired.com/story/soundthinking-geolitica-acquisition-predictive-policing/" target="_blank" rel="noopener noreferrer">previously reported</a></span> that the company is planning to cease operations at the end of this year; some of its team have already been hired by a different law enforcement company. </p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Review: Framework Laptop finally gets an AMD Ryzen config–and it’s pretty good (164 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/10/review-framework-laptop-finally-gets-an-amd-ryzen-config-and-its-pretty-good/</link>
            <guid>37752950</guid>
            <pubDate>Tue, 03 Oct 2023 15:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/10/review-framework-laptop-finally-gets-an-amd-ryzen-config-and-its-pretty-good/">https://arstechnica.com/gadgets/2023/10/review-framework-laptop-finally-gets-an-amd-ryzen-config-and-its-pretty-good/</a>, See on <a href="https://news.ycombinator.com/item?id=37752950">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                                    
  




<!-- cache hit 53:single/related:47534ee0e696b9a8ad1fcc011a863361 --><!-- empty -->
  <div>
    <ul>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1032-150x150.jpeg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1032.jpeg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1032-980x653.jpeg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1032-1440x960.jpeg 2560" data-sub-html="#caption-1972735">
          <figure>
            
                          <figcaption id="caption-1972735">
                <span></span>
                                  <p>
                    This is the Framework Laptop 13. We're using the same pictures as a previous review because it's the exact same laptop.                   </p>
                                                  <p><span></span>
                                          Andrew Cunningham                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1042-150x150.jpeg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1042.jpeg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1042-980x653.jpeg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1042-1440x960.jpeg 2560" data-sub-html="#caption-1972738">
          <figure>
            
                          <figcaption id="caption-1972738">
                <span></span>
                                  <p>
                    And the same keyboard and trackpad.                  </p>
                                                  <p><span></span>
                                          Andrew Cunningham                                      </p>
                              </figcaption>
                      </figure>
        </li>
              <li data-thumb="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1034-150x150.jpeg" data-src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1034.jpeg" data-responsive="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1034-980x653.jpeg 1080, https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_1034-1440x960.jpeg 2560" data-sub-html="#caption-1972737">
          <figure>
            
                          <figcaption id="caption-1972737">
                <span></span>
                                  <p>
                    And the same lid.                  </p>
                                                  <p><span></span>
                                          Andrew Cunningham                                      </p>
                              </figcaption>
                      </figure>
        </li>
          </ul>
  </div>

<table>
<tbody>
<tr>
<th colspan="2">Specs at a glance: Framework Laptop 13 (2023)</th>
</tr>
<tr>
<th>OS</th>
<td>Windows 11 22H2</td>
</tr>
<tr>
<th>CPU</th>
<td>AMD Ryzen 7 7840U (8-cores)</td>
</tr>
<tr>
<th>RAM</th>
<td>32GB DDR5-5600 (upgradeable)</td>
</tr>
<tr>
<th>GPU</th>
<td>AMD Radeon 780M (integrated)</td>
</tr>
<tr>
<th>SSD</th>
<td>1TB Western Digital Black SN770</td>
</tr>
<tr>
<th>Battery</th>
<td>61 WHr</td>
</tr>
<tr>
<th>Display</th>
<td>13.5-inch 2256x1504 non-touchscreen in glossy or matte</td>
</tr>
<tr>
<th>Connectivity</th>
<td>4x recessed USB-C ports (2x USB 4, 2x USB 3.2) with customizable "Expansion Card" dongles, headphone jack</td>
</tr>
<tr>
<th>Price as tested</th>
<td><a href="https://tinyurl.com/2zhubuf7" target="_blank" rel="noopener">$1,679</a> pre-built, $1,523 DIY edition with no OS included</td>
</tr>
</tbody>
</table>
<p>The Framework Laptop 13 is back again.</p>
<p>My third review of this laptop is probably the one that I (and many Framework-curious PC buyers) have been the most interested to test, as the company has finally added <a href="https://tinyurl.com/2zhubuf7" target="_blank" rel="noopener">an AMD Ryzen option</a> to the repair-friendly portable. Updates to the Intel version of the Framework Laptop have boosted CPU performance, but its graphics performance has been at a standstill since the Framework Laptop <a href="https://arstechnica.com/gadgets/2021/07/frameworks-new-lightweight-modular-laptop-delivers-on-its-promises/">originally hit the scene</a> in mid-2021.</p>
<p>Even AMD's latest integrated graphics won't make a thin-and-light laptop a replacement for a gaming PC with dedicated graphics, but a bit more GPU power makes the Framework Laptop that much more versatile, making it easier to play games at reasonable resolutions and settings than it is on Intel's aging Iris Xe graphics hardware.</p>
<p>Whether you hopped on the Framework train early and have been waiting for a motherboard that felt like a true all-around upgrade or you've been on the fence about buying your first Framework Laptop, the new Ryzen version makes a good case for itself. If you want to order one, there's currently a backlog—all versions are shipping at an unspecified date in "Q4."</p>
<h2>Meet the Ryzen-powered Framework 13</h2>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0054.jpeg" data-height="1920" data-width="2560" alt="The Ryzen version of the Framework Laptop's system board has the same shape and layout as the Intel versions, preserving full compatibility with older Framework Laptop 13 enclosures."><img alt="The Ryzen version of the Framework Laptop's system board has the same shape and layout as the Intel versions, preserving full compatibility with older Framework Laptop 13 enclosures." src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0054-980x735.jpeg" width="980" height="735"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0054.jpeg" data-height="1920" data-width="2560">Enlarge</a> <span>/</span> The Ryzen version of the Framework Laptop's system board has the same shape and layout as the Intel versions, preserving full compatibility with older Framework Laptop 13 enclosures.</p><p>Andrew Cunningham</p></figcaption></figure>
<p>I won't spend a lot of time talking about the design of the Framework Laptop 13 again, except to say that it remains a competent ultraportable, and there's nothing that feels dated or clunky about its design now that didn't already feel a little dated and clunky two years ago (the relatively thick display bezel is the main culprit here). Another laptop in this category we generally like, <a href="https://arstechnica.com/gadgets/2023/06/lenovo-thinkpad-x1-carbon-gen-11-review-two-steps-forward-one-step-back/">Lenovo's ThinkPad X1 Carbon</a>, has been using the same basic design for years, so it's not like Framework is in danger of falling behind in a chaotic and fast-paced industry.</p>                                            
                                                        
<p>The Ryzen version of the mainboard looks mostly identical to the Intel version, given that it needs to fit in all the same cases with all the same connectors. It dropped directly into the same case I've also used for the Intel versions of the Framework Laptop, and moving from Intel to AMD is as easy as it is in a desktop tower with standard parts.</p>
<figure><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0056.jpeg" data-height="1920" data-width="2560" alt="The label on the board is one of the few indicators that you're using an AMD board."><img alt="The label on the board is one of the few indicators that you're using an AMD board." src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0056-980x735.jpeg" width="980" height="735"></a><figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/IMG_0056.jpeg" data-height="1920" data-width="2560">Enlarge</a> <span>/</span> The label on the board is one of the few indicators that you're using an AMD board.</p><p>Andrew Cunningham</p></figcaption></figure>

<p>But it wouldn't be a Ryzen system if there weren't a couple of weird, fiddly things about it! All the Intel Framework Laptops have supported the same specifications for all four ports (USB 4 for the 11th-gen, Thunderbolt 4 for the newer ones), allowing you to install the expansion card modules wherever you want them without worrying about the particulars.</p>
<p>The Ryzen laptop supports USB 4 in the rear-left and rear-right ports, USB 3.2 and DisplayPort for the front-right slot, and only USB 3.2 on the front-left slot (all four ports support USB-PD for charging, though). Framework also says the rear ports enter a "high-power mode" when USB-A modules are connected to them, which can reduce battery life.</p>
<p>So yes, the Framework Laptop's ports are still customizable, and you can still have a lot of flexibility when installing expansion modules. But some modules are better fits for specific ports, and you'll have to be a bit more careful about where you put things if you want the best performance and battery life.</p>

                                                </div></div>]]></description>
        </item>
    </channel>
</rss>