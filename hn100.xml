<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 11 Oct 2024 03:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The FBI created a coin to investigate crypto pump-and-dump schemes (102 pts)]]></title>
            <link>https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation</link>
            <guid>41802823</guid>
            <pubDate>Thu, 10 Oct 2024 19:52:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation">https://www.theverge.com/2024/10/10/24267098/fbi-coin-crypto-token-nexgenai-sec-doj-fraud-investigation</a>, See on <a href="https://news.ycombinator.com/item?id=41802823">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The FBI created a cryptocurrency as part of an investigation into price manipulation in crypto markets, the <a href="https://www.justice.gov/usao-ma/pr/eighteen-individuals-and-entities-charged-international-operation-targeting-widespread">government revealed</a> on Wednesday. The FBI’s Ethereum-based token, NexFundAI, was created with the help of “cooperating witnesses.”</p><p>As a result of the investigation, the Securities and Exchange Commission <a href="https://www.sec.gov/newsroom/press-releases/2024-166">charged</a> three “market makers” and nine people for allegedly engaging in schemes to boost the prices of certain crypto assets. The Department of Justice charged 18 people and entities for “widespread fraud and manipulation” in crypto markets.</p><p>The defendants allegedly made false claims about their tokens and executed so-called “wash trades” to create the impression of an active trading market, prosecutors claim. The three market makers — ZMQuant, CLS Global, and MyTrade — allegedly wash traded or conspired to wash trade on behalf of NexFundAI, an Ethereum-based token they didn’t realize was created by the FBI.&nbsp;</p><p>“What the FBI uncovered in this case is essentially a new twist to old-school financial crime,” Jodi Cohen, the special agent in charge of the FBI’s Boston division, said in a statement. “What we uncovered has resulted in charges against the leadership of four cryptocurrency companies, and four crypto ‘market makers’ and their employees who are accused of spearheading a sophisticated trading scheme that allegedly bilked honest investors out of millions of dollars.”</p><p>Liu Zhou, a “market maker” working with MyTrade MM, allegedly told promoters of NexFundAI that MyTrade MM was better than its competitors because they “control the pump and dump” allowing them to “do inside trading easily.”</p><p>An FBI spokesperson <a href="https://www.coindesk.com/policy/2024/10/09/prosecutors-charge-two-crypto-market-makers-employees-with-market-manipulation-fraud/">told <em>CoinDesk</em></a> that there was limited trading activity on the coin but didn’t share additional information. On a Wednesday press call, Joshua Levy, the acting US attorney for the District of Massachusetts, said trading on the token was disabled, according to <em>CoinDesk</em>.</p><p>The DOJ has reportedly secured $25 million from “fraudulent proceeds” that will be returned to investors.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Copenhagen Book: general guideline on implementing auth in web applications (310 pts)]]></title>
            <link>https://thecopenhagenbook.com/</link>
            <guid>41801883</guid>
            <pubDate>Thu, 10 Oct 2024 18:37:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thecopenhagenbook.com/">https://thecopenhagenbook.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41801883">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
      <main>
<p>The Copenhagen Book provides a general guideline on implementing auth in web applications. It is free, open-source, and community-maintained. It may be opinionated or incomplete at times but we hope this fills a certain void in online resources. We recommend using this alongside the <a href="https://cheatsheetseries.owasp.org/index.html">OWASP Cheat Sheet Series</a>.</p>
<p>If you have any suggestions or concerns, consider opening a new issue.</p>
<p><em>Created by <a href="https://github.com/pilcrowOnPaper">Pilcrow</a></em></p>
</main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: HTML for People (267 pts)]]></title>
            <link>https://htmlforpeople.com</link>
            <guid>41801334</guid>
            <pubDate>Thu, 10 Oct 2024 17:47:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmlforpeople.com">https://htmlforpeople.com</a>, See on <a href="https://news.ycombinator.com/item?id=41801334">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
      
  
<p>HTML isn’t only for people working in the tech field. It’s for anybody, the way documents are for anybody. HTML is just another type of document. A very special one—the one the web is built on.</p>
<p>I’m <a href="https://blakewatson.com/">Blake Watson</a>. I’ve been building websites since the early 2000s. Though I work professionally in the field, I feel strongly that <em>anyone</em> should be able to make a website with HTML if they want. This book will teach you how to do just that. It doesn’t require any previous experience making websites or coding. I will cover everything you need to know to get started in an approachable and friendly way.</p>
<p>Ready? Let’s do it!</p>


  <p>
    <a href="https://htmlforpeople.com/intro">Read the introduction</a>
    <a href="https://htmlforpeople.com/zero-to-internet-your-first-website">Start coding already!</a>
  </p>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Recall is now an explorer.exe dependency (189 pts)]]></title>
            <link>https://github.com/ChrisTitusTech/winutil/issues/2697</link>
            <guid>41801331</guid>
            <pubDate>Thu, 10 Oct 2024 17:47:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ChrisTitusTech/winutil/issues/2697">https://github.com/ChrisTitusTech/winutil/issues/2697</a>, See on <a href="https://news.ycombinator.com/item?id=41801331">Hacker News</a></p>
<div id="readability-page-1" class="page"><div disabled="" sortable="">
          <p dir="auto">There are 3 settings you can use to practically disable copilot/recall completely. These will do the following things<br>
1: Remove the package so it doesn't even appear in search or app lists<br>
2: Remove the package so it doesn't activate if you hit the new copilot key on keyboards<br>
3: Remove the package so it doesn't even show up as an app that was installed in apps and features</p>
<p dir="auto">In group policy there are 2 settings</p>
<p dir="auto">"turn off windows copilot"<br>
(UserConfig\AdministrativeTemplates\WindowsComponents\Windows Copilot)</p>
<p dir="auto">"turn off saving snapshots to windows"<br>
(UserConfig\AdministrativeTemplates\WindowsComponents\Windows Ai)</p>
<p dir="auto">These 2 group policy options enter 2 registry keys located here</p>
<p dir="auto">HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\WindowsCopilot<br>
HKEY_CURRENT_USER\Software\Policies\Microsoft\Windows\WindowsCopilot<br>
"TurnOffWindowsCopilot"<br>
Dword (1)</p>
<p dir="auto">HKEY_CURRENT_USER\Software\Policies\Microsoft\Windows\WindowsAI<br>
HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Windows\WindowsCopilot<br>
"DisableAIDataAnalysis"<br>
Dword (1)</p>
<p dir="auto">If you also use an app called "AppXPackagesManager"<br>
<a href="https://github.com/Savitarax/File-Resources">https://github.com/Savitarax/File-Resources</a><br>
(I am NOT the original creator of said app, I am merely providing the software to use)</p>
<p dir="auto">Copilot is an option that you can remove listed in user packages</p>
<p dir="auto">After doing these steps.<br>
I can't get copilot to activate in any way, nor can I find recall.</p>
<p dir="auto">Figured i'd list this to anyone that was looking for a pretty solid solution if they were looking for the template to do so.</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Studios: Please Don't Spoil the Movie We Are Seated to See (199 pts)]]></title>
            <link>http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html</link>
            <guid>41801300</guid>
            <pubDate>Thu, 10 Oct 2024 17:44:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html">http://fxrant.blogspot.com/2024/06/studios-dont-spoil-movie-we-are-seated.html</a>, See on <a href="https://news.ycombinator.com/item?id=41801300">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-8595952586617710821" itemprop="description articleBody">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXlFhEI0V8S6K4imrSag-M5PNYBCJIV-ze_r-r6R06QUqdyY49YhxiO79peJKdwjV8mH06lM5R-JFgSU57mfctbt1eyM52NbW9L0F95gZqMaq8RIsQInxb7Jrkf8wi5V3Zw7TRod8OsP3I72jsP5ZgwGzaNAWfdjERvIgFLBtSSPvfvP9Aq5U/s720/alien-1979.jpg.webp" imageanchor="1"><img data-original-height="480" data-original-width="720" height="265" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXlFhEI0V8S6K4imrSag-M5PNYBCJIV-ze_r-r6R06QUqdyY49YhxiO79peJKdwjV8mH06lM5R-JFgSU57mfctbt1eyM52NbW9L0F95gZqMaq8RIsQInxb7Jrkf8wi5V3Zw7TRod8OsP3I72jsP5ZgwGzaNAWfdjERvIgFLBtSSPvfvP9Aq5U/w400-h265/alien-1979.jpg.webp" width="400"></a></p><p>I tweeted this incredibly non-controversial take and it got a huge reaction, so I thought I'd recycle the content for a blog post. Enjoy.</p><p>We took our kid to see "Alien" (1979) on the big screen during its one-week-only theatrical run. We told him there was a good chance of a pre-show featurette that would spoil the movie, so he needed to be ready to cover his eyes.</p><p>Well, that's exactly what happened.</p><p>My kid threw his hoodie over his eyes while a pre-show interview between Fede Alvarez and Ridley Scott appeared, featuring tons of behind-the-scenes photos of the alien, the chestburster scene, and discussion of the legacy of the classic film.</p><p>Why do this before the movie!??!</p><p>If even one person in that theater hadn't seen the film yet, it puts a huge damper on the surprise and delight that the movie would bring them, which is sad. We WANT to bring first-timers to theaters to see classic movies. Don't ruin it for them.</p><p><b>Play that shit AFTER the movie.</b></p><p>This has happened with several re-releases for me. Fathom did this to "Star Trek II: The Wrath of Khan" (pre-show highlighted a main character's death!) and "Close Encounters" (pre-show showed the effing aliens!). And now "Alien" (Disney/Fox).</p><p>The solution is simple: preserve the wonder for first-timers by putting these featurettes AFTER the movie. Tease it before the feature.</p><p>Anyway, <a href="https://www.fathomevents.com/events/close-encounters-of-the-third-kind-2024-re-release/">"Close Encounters of the Third Kind" is coming to theaters again this summer </a>(Fathom), so get ready to cover the eyes of first-timers before the show.</p><p><a href="https://x.com/tvaziri/status/1796955123337372032">Original tweet thread.</a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Game Programming in Prolog (109 pts)]]></title>
            <link>https://thingspool.net/morsels/page-10.html</link>
            <guid>41800764</guid>
            <pubDate>Thu, 10 Oct 2024 16:50:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thingspool.net/morsels/page-10.html">https://thingspool.net/morsels/page-10.html</a>, See on <a href="https://news.ycombinator.com/item?id=41800764">Hacker News</a></p>
<div id="readability-page-1" class="page">

<a href="https://thingspool.net/morsels/list.html">Back to List</a>

<h3>Author: Youngjin Kang</h3>
<h3>Date: August 25, 2024</h3>

<hr>

<h3><b>Introduction</b></h3>
<p>As a fan of unconventional programming paradigms, I enjoy learning new programming languages which are drastically different from the typical object-oriented ones such as C#, Java, and the like. The most iconic of them are LISP (which is a powerful language for both functional programming as well as metalinguistic patterns in software development) and Prolog (which is one of the most popular languages in logic programming). Learning these languages is quite hard, compared to being acquainted with usual C-style imperative languages such as Ruby and Python, yet it has turned out to be one of the most effective ways of exercising one's brain.</p>
<p>By the time I started learning LISP via MIT's 1986 lecture series called "SICP (Structure and Interpretation of Computer Programs)" back in 2018, I was already quite familiar with some of its core concepts (such as lambda expressions, higher-order functions, etc) because they were already integrated as some of the main features of C#, which was the language I was using all the time as a Unity game developer. Also, my academic background in electrical engineering (signal processing in particular) helped me easily grasp the idea of "stream processing" which appeared in the latter half of the lecture series. Thus, learning LISP and its functional design patterns was not as difficult as I imagined it to be.</p>
<p>A major intellectual challenge, however, struck me when I began to study Prolog - the famous logic programming language which is notorious for its esoteric syntax. The grammar itself did not appear to be complicated at all; it was just as minimal as that of LISP. The way in which programming had to be done in Prolog, though, was stressful enough to fry the engine of my brain. The way it approached data structures (such as lists) and algorithms based upon mathematical relations was something so revolutionarily novel to me, that it seriously opened up a new horizon in my faculty of computational reasoning.</p>
<p>While Prolog's approach in software development was quite alien to me, I managed to notice a number of familiar associations between Prolog and many useful topics in engineering. I discovered, for example, that the so-called "relational databases" (e.g. MySQL) are named so not because they comprise data tables which are related to each other via references, but because each row of a data table can be considered an n-ary predicate (where 'n' is the number of columns in the table) in Prolog's syntax. Besides, I found out that the input/output behavior of each digital circuit component (e.g. logic gate) could be implemented as an n-ary relation (where 'n' is the total number of the input/output ports combined), implying that an "object", whether it be a piece of hardware or a piece of pure data in memory, may as well be defined as a relation in logic programming (just like an object may as well be defined as a function in functional programming). Furthermore, the declarative nature of Prolog strongly convinced me that it must be optimal for data-driven design.</p>
<p>These realizations soon led me to contemplate upon the notion that, maybe, logic programming has a great deal of potential in the design and implementation of highly complex systems, such as a video game's core gameplay mechanics. I began to ask myself, "Will it be possible to develop an entire game using the grammar of logic programming?"</p>
<p>Indeed, there are reasons why most game developers just stick to general-purpose programming languages (such as C#) for making games, aside from purely experimental purposes. Implementing an entire game based on Prolog, for instance, is perhaps too much of a challenge for those who are not hardcore mathematicians. Also, Prolog may not be the best language to use for parts of the project which are not necessarily made of a complex web of relations, such as simple I/O modules, graphics modules, audio modules, physics modules, and the like.</p>
<p>However, I believe that at least the core mechanics of a game can definitely be implemented using the language of Prolog, and that we will be able to solve a plethora of complex design problems by doing so. It is because a gameplay system which is structured in terms of a set of declarative statements will be far more robust, modular, and free of confusing edge cases (e.g. race conditions) than an imperative system.</p>
<p>For this alternative methodology to be successful, one must start by designing the system in terms of logical relations/predicates only, and nothing else (That is, no functions, no structs, no classes, no interfaces, no state variables, etc). This will allow us to construct a gameplay system which is purely driven by the soul of Prolog.</p>
<hr>

<h3><b>World and Actors</b></h3>
<p>The core idea in Prolog-based game programming is to utilize relations as the most primitive building blocks of the system, just like basic circuit components (e.g. resistors, transistors, capacitors, inductors, etc) are the most primitive building blocks of an electric circuit. It is sensible, therefore, to start this journey by considering the most rudimentary relations (e.g. unary and binary) first, and see if these elements can serve as the most essential nuts and bolts of the game.</p>
<img src="https://thingspool.net/morsels/e01.jpg" alt="Game Programming in Prolog - Part 1 (Figure 1)">
<p>Suppose that we are designing a game, and that the game consists of two major parts - world and actors (see the image above). The world is a scene in which everything is supposed to happen, and actors are objects which belong to the world. Examples of actors include "players", "enemies", "obstacles", "items", and pretty much any discrete entities which have their own names and attributes. Actors are able to interact with each other (as well as with themselves), from which various events occur. What we refer to as "gameplay" is a chain of such events.</p>
<p>We will begin formulating a gameplay system based off of this conceptual backbone. All you need to remember is that there is a world, and that the world contains a number of actors, each of which possesses its own state and behavior.</p>
<hr>

<h3><b>Tags</b></h3>
<p>First of all, let us identify each individual actor with a unique name. If there are two actors in the world, for instance, we will simply assume that the name "actor1" and "actor2" will be used to indicate the first and second actors, respectively.</p>
<img src="https://thingspool.net/morsels/e02.jpg" alt="Game Programming in Prolog - Part 1 (Figure 2)">
<p>The first piece of logic I will illustrate is the idea of tags. A tag is a keyword which, when attached to an actor, describes what the actor stands for. When an actor has the tag "bread" attached to it, for example, we should be able to tell that the actor is a piece of bread.</p>
<p>The Prolog code below assigns the tag "bread" to both actor1 and actor2, in the form of unary predicates (The tag "bread" itself is an unary relation, and "bread(actor1)" &amp; "bread(actor2)" are two separate instances of it). This implies that both actor1 and actor2 are pieces of bread.</p>
<div><pre><code>bread(actor1).
bread(actor2).</code></pre></div>
<img src="https://thingspool.net/morsels/e03.jpg" alt="Game Programming in Prolog - Part 1 (Figure 3)">
<p>An actor can have multiple tags as well. However, one may feel that it is a bit too tedious to manually assign a bunch of tags to each individual actor. For example, let us say that every piece of bread must also be labeled as flammable and decomposable. This means that, whenever an actor is associated with the tag "bread", we are obliged to always ensure that it is also associated with the tag "flammable" and "decomposable". Manually attaching these two additional tags to every "bread" actor is way too cumbersome and error-prone. Fortunately, the following pair of horn clauses neatly solve this problem. They enforce the following two rules:</p>
<p>(1) Whenever tag "bread" is assigned to actor X, tag "flammable" will automatically be assigned to actor X.<br>(2) Whenever tag "bread" is assigned to actor X, tag "decomposable" will automatically be assigned to actor X.</p>
<div><pre><code>flammable(X) :- bread(X).
decomposable(X) :- bread(X).</code></pre></div>
<img src="https://thingspool.net/morsels/e04.jpg" alt="Game Programming in Prolog - Part 1 (Figure 4)">
<p>These horn clauses, therefore, serve as part of the game's "config data" - a list of data entries in the game's technical design document (like the ones you would see on a spreadsheet) telling us the characteristics of each individual character type, skill type, mission type, and so forth. The tags called "flammable" and "decomposable" in our case, for instance, are characteristics which belong to the type-specifier called "bread", meaning that any actor which can be identified as "bread" is a composition of two properties called "flammable" and "decomposable".</p>
<p>A decent analogy can be found in Unity game engine, where we may create a prefab called "Bread" with two components in it - "Flammable" and "Decomposable". Or, in a general object-oriented programming environment, "Bread" may stand for the name of a class which implements two interfaces called "IFlammable" and "IDecomposable".</p>
<p>In a way, therefore, horn clauses in Prolog play the role of data type definitions.</p>
<img src="https://thingspool.net/morsels/e05.jpg" alt="Game Programming in Prolog - Part 1 (Figure 5)">
<p>Aside from these pre-configured tags (which all rely on the presence of the tag "bread"), one may as well attach a custom tag to an actor as needed. For example, imagine that a wizard happened to enchant actor2 (i.e. the second piece of bread). This means that, unlike actor1 which is an ordinary piece of bread, actor2 must be an "enchanted" piece of bread which is required to have the tag "enchanted" attached to it for the purpose of showing us that it has been enchanted. The code below ensures that this is the case.</p>

<img src="https://thingspool.net/morsels/e06.jpg" alt="Game Programming in Prolog - Part 1 (Figure 6)">
<p>The tags "flammable" and "decomposable" are characteristics of all pieces of bread, whereas the tag "enchanted" is a characteristic of only special pieces of bread which have been enchanted by a wizard.</p>
<hr>

<h3><b>Relationships</b></h3>
<p>So far, we have been using tags for specifying the characteristics of each individual actor. In a gameplay system, however, we also need to specify relationships between actors, such as ways in which they interact, etc.</p>
<p>In an ecosystem, predators chase preys and preys run from predators. In a dating simulator, a guy tries to flirt with girls and girls reject him. In a social simulator (such as The Sims), people are either friends or enemies of each other, or somewhere in between. In the game of chess, a bishop devours a rook diagonally and a rook devours a bishop orthogonally. These are all relationships out of which the game's dynamics emerge.</p>
<p>Defining actor-to-actor relationships in Prolog is pretty straightforward. Just like an unary predicate can be used to characterize a single actor, a binary predicate can be used to characterize a relationship between a pair of actors. And by means of a horn clause, such a relationship can be dynamically deduced from a set of requisite conditions.</p>
<p>The following code is an example of a relationship. Suppose that there is a third actor called "actor3", and that we have declared it as a human (by attaching the tag "human" to it). Since a human is able to eat a piece of bread, we can confidently assert that "X can eat Y if X is a human and Y is a piece of bread". Here, "X can eat Y" is a relationship which holds whenever X is associated with tag "human" and Y is associated with tag "bread".</p>
<div><pre><code>human(actor3).
canEat(X, Y) :- human(X), bread(Y).</code></pre></div>
<img src="https://thingspool.net/morsels/e07.jpg" alt="Game Programming in Prolog - Part 1 (Figure 7)">
<p>Here is another example. Since a piece of bread is decomposable (because anything which is identified as "bread" must also be identified as "decomposable"), we know that microbes such as fungi are capable of spoiling it. If there is an actor with the tag "fungus" attached to it, therefore, we will be able to tell that it must be able to spoil any other actor which is "decomposable". This is yet another case of a relationship between two types of actors; it is a relationship which says, "X can spoil Y if X is a fungus and Y is decomposable". The following code shows its definition.</p>
<div><pre><code>fungus(actor4).
canSpoil(X, Y) :- fungus(X), decomposable(Y).</code></pre></div>
<img src="https://thingspool.net/morsels/e08.jpg" alt="Game Programming in Prolog - Part 1 (Figure 8)">
<p>There is something still missing here, though. While I have demonstrated that it is possible to assign characteristics to individual actors as well as their mutual connections (i.e. relationships), I have not shown yet how to make these characteristics change over time. They all have been static so far, and the declarative nature of Prolog does not seem to offer an easy solution to make things dynamic.</p>
<p>If we want to create a game rather than a fixed landscape of how things are shaped permanently, we better let them move and interact as time goes by. In the next part of the series, I will explain how the game loop shall be conceptualized in Prolog.</p>
<p>(Will be continued in <a href="https://thingspool.net/morsels/page-11.html">Part 2</a>)</p>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeskPad – A virtual monitor for screen sharing (638 pts)]]></title>
            <link>https://github.com/Stengo/DeskPad</link>
            <guid>41800602</guid>
            <pubDate>Thu, 10 Oct 2024 16:36:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Stengo/DeskPad">https://github.com/Stengo/DeskPad</a>, See on <a href="https://news.ycombinator.com/item?id=41800602">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/DeskPad/Assets.xcassets/AppIcon.appiconset/Icon-256.png">
  <img src="https://github.com/Stengo/DeskPad/raw/main/DeskPad/Assets.xcassets/AppIcon.appiconset/Icon-256.png?raw=true" alt="DeskPad Icon" width="128">
  </a>
</h3><a id="user-content-------" aria-label="Permalink: " href="#------"></a></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">DeskPad</h2><a id="user-content-deskpad" aria-label="Permalink: DeskPad" href="#deskpad"></a></p>
<p dir="auto">A virtual monitor for screen sharing</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/screenshot.jpg">
  <img src="https://github.com/Stengo/DeskPad/raw/main/screenshot.jpg?raw=true" alt="DeskPad Screenshot">
  </a>
</h3><a id="user-content--------1" aria-label="Permalink: " href="#-------1"></a></div>
<p dir="auto">Certain workflows require sharing the entire screen (usually due to switching through multiple applications), but if the presenter has a much larger display than the audience it can be hard to see what is happening.</p>
<p dir="auto">DeskPad creates a virtual display that is mirrored within its application window so that you can create a dedicated, easily shareable workspace.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">You can either download the <a href="https://github.com/Stengo/DeskPad/releases">latest release binary</a> or install via <a href="https://brew.sh/" rel="nofollow">Homebrew</a> by calling <code>brew install deskpad</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">DeskPad behaves like any other display. Launching the app is equivalent to plugging in a monitor, so macOS will take care of properly arranging your windows to their previous configuration.</p>
<p dir="auto">You can change the display resolution through the system preferences and the application window will adjust accordingly.</p>
<p dir="auto">Whenever you move your mouse cursor to the virtual display, DeskPad will highlight its title bar in blue and move the application window to the front to let you know where you are.</p>
<div dir="auto"><h3 tabindex="-1" dir="auto">
  <a href="https://github.com/Stengo/DeskPad/blob/main/demonstration.gif">
  <img src="https://github.com/Stengo/DeskPad/raw/main/demonstration.gif?raw=true" alt="DeskPad Demonstration" data-animated-image="">
  </a>
</h3><a id="user-content--------2" aria-label="Permalink: " href="#-------2"></a></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AAA Gaming on Asahi Linux (479 pts)]]></title>
            <link>https://rosenzweig.io/blog/aaa-gaming-on-m1.html</link>
            <guid>41799068</guid>
            <pubDate>Thu, 10 Oct 2024 14:16:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rosenzweig.io/blog/aaa-gaming-on-m1.html">https://rosenzweig.io/blog/aaa-gaming-on-m1.html</a>, See on <a href="https://news.ycombinator.com/item?id=41799068">Hacker News</a></p>
<div id="readability-page-1" class="page"> <header><p>10 Oct 2024</p></header><p>Gaming on Linux on M1 is here! We’re thrilled to release our Asahi game playing toolkit, which integrates our Vulkan 1.3 drivers with x86 emulation and Windows compatibility. Plus a bonus: conformant OpenCL 3.0.</p> <p>Asahi Linux now ships the only conformant <a href="https://www.khronos.org/conformance/adopters/conformant-products/opengl#submission_3470">OpenGL®</a>,<!--
[OpenGL® ES](https://www.khronos.org/conformance/adopters/conformant-products/opengles#submission_1045),--> <a href="https://www.khronos.org/conformance/adopters/conformant-products/opencl#submission_433">OpenCL™</a>, and <a href="https://www.khronos.org/conformance/adopters/conformant-products#submission_7910">Vulkan®</a> drivers for this hardware. As for gaming… while today’s release is an alpha, <a href="https://store.steampowered.com/app/870780/Control_Ultimate_Edition/"><strong>Control</strong></a> runs well!</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Control-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Control-small.avif" alt="Control"></a> </figure> <h2 id="installation">Installation</h2> <p>First, install <a href="https://asahilinux.org/fedora/">Fedora Asahi Remix</a>. Once installed, get the latest drivers with <code>dnf upgrade --refresh &amp;&amp; reboot</code>. Then just <code>dnf install steam</code> and play. While all M1/M2-series systems work, most games require 16GB of memory due to emulation overhead.</p> <h2 id="the-stack">The stack</h2> <p>Games are typically x86 Windows binaries rendering with DirectX, while our target is Arm Linux with Vulkan. We need to handle each difference:</p> <ul> <li><a href="https://fex-emu.com/">FEX</a> emulates x86 on Arm.</li> <li><a href="https://www.winehq.org/">Wine</a> translates Windows to Linux.</li> <li><a href="https://github.com/doitsujin/dxvk">DXVK</a> and <a href="https://github.com/HansKristian-Work/vkd3d-proton">vkd3d-proton</a> translate DirectX to Vulkan.</li> </ul> <p>There’s one curveball: page size. Operating systems allocate memory in fixed size “pages”. If an application expects smaller pages than the system uses, they will break due to insufficient alignment of allocations. That’s a problem: x86 expects 4K pages but Apple systems use 16K pages.</p> <p>While Linux can’t mix page sizes between processes, it <em>can</em> virtualize another Arm Linux kernel with a different page size. So we run games inside a tiny virtual machine using <a href="https://github.com/AsahiLinux/muvm">muvm</a>, passing through devices like the GPU and game controllers. The hardware is happy because the system is 16K, the game is happy because the virtual machine is 4K, and you’re happy because you can play <a href="https://store.steampowered.com/app/377160/Fallout_4/"><strong>Fallout 4</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Fallout4-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Fallout4-small.avif" alt="Fallout 4"></a> </figure> <h2 id="vulkan">Vulkan</h2> <p>The final piece is an adult-level Vulkan driver, since translating DirectX requires Vulkan 1.3 with many extensions. Back in April, I wrote <a href="https://rosenzweig.io/blog/vk13-on-the-m1-in-1-month.html">Honeykrisp</a>, the only Vulkan 1.3 driver for Apple hardware. I’ve since added DXVK support. Let’s look at some new features.</p> <h3 id="tessellation">Tessellation</h3> <p>Tessellation enables games like <a href="https://store.steampowered.com/app/292030/The_Witcher_3_Wild_Hunt/"><strong>The Witcher 3</strong></a> to generate geometry. The M1 has hardware tessellation, but it is too limited for DirectX, Vulkan, or OpenGL. We must instead tessellate with arcane compute shaders, as detailed in <a href="https://www.youtube.com/live/pDsksRBLXPk">today’s talk at XDC2024</a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Witcher3-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Witcher3-small.avif" alt="The Witcher 3"></a> </figure> <h3 id="geometry-shaders">Geometry shaders</h3> <p>Geometry shaders are an older, cruder method to generate geometry. Like tessellation, the M1 lacks geometry shader hardware so we emulate with compute. Is that fast? No, but geometry shaders are slow <a href="http://www.joshbarczak.com/blog/?p=667">even on desktop GPUs</a>. They don’t need to be fast – just fast enough for games like <a href="https://store.steampowered.com/app/1139900/Ghostrunner/"><strong>Ghostrunner</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Ghostrunner-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Ghostrunner-small.avif" alt="Ghostrunner"></a> </figure> <h3 id="enhanced-robustness">Enhanced robustness</h3> <p>“Robustness” permits an application’s shaders to access buffers out-of-bounds without crashing the hardware. In OpenGL and Vulkan, out-of-bounds loads may return arbitrary elements, and out-of-bounds stores may corrupt the buffer. Our OpenGL driver <a href="https://rosenzweig.io/blog/conformant-gl46-on-the-m1.html">exploits this definition</a> for efficient robustness on the M1.</p> <p>Some games require stronger guarantees. In DirectX, out-of-bounds loads return zero, and out-of-bounds stores are ignored. DXVK therefore requires <a href="https://docs.vulkan.org/guide/latest/robustness.html#_vk_ext_robustness2"><code>VK_EXT_robustness2</code></a>, a Vulkan extension strengthening robustness.</p> <p>Like before, we implement robustness with compare-and-select instructions. A naïve implementation would <em>compare</em> a loaded index with the buffer size and <em>select</em> a zero result if out-of-bounds. However, our GPU loads are vector while arithmetic is scalar. Even if we disabled page faults, we would need up to four compare-and-selects per load.</p> <div><pre><code><span><span>load</span> R, buffer, index * <span>16</span></span>
<span><span>ulesel</span> R[<span>0</span>], index, size, R[<span>0</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>1</span>], index, size, R[<span>1</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>2</span>], index, size, R[<span>2</span>], <span>0</span></span>
<span><span>ulesel</span> R[<span>3</span>], index, size, R[<span>3</span>], <span>0</span></span></code></pre></div> <p>There’s a trick: reserve <em>64 gigabytes</em> of zeroes using virtual memory voodoo. Since every 32-bit index multiplied by 16 fits in 64 gigabytes, any index into this region loads zeroes. For out-of-bounds loads, we simply replace the buffer address with the reserved address while preserving the index. Replacing a 64-bit address costs just two 32-bit compare-and-selects.</p> <div><pre><code><span><span>ulesel</span> buffer.lo, index, size, buffer.lo, RESERVED.lo</span>
<span><span>ulesel</span> buffer.hi, index, size, buffer.hi, RESERVED.hi</span>
<span><span>load</span> R, buffer, index * <span>16</span></span></code></pre></div> <p>Two instructions, not four.</p> <h2 id="next-steps">Next steps</h2> <p>Sparse texturing is next for Honeykrisp, which will unlock more DX12 games. The alpha already runs DX12 games that don’t require sparse, like <a href="https://store.steampowered.com/app/1091500/Cyberpunk_2077/"><strong>Cyberpunk 2077</strong></a>.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Cyberpunk2077-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Cyberpunk2077-small.avif" alt="Cyberpunk 2077"></a> </figure> <p>While many games are playable, newer AAA titles don’t hit 60fps <em>yet</em>. Correctness comes first. Performance improves next. Indie games like <a href="https://store.steampowered.com/app/367520/Hollow_Knight/"><strong>Hollow Knight</strong></a> do run full speed.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/HollowKnight-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/HollowKnight-small.avif" alt="Hollow Knight"></a> </figure> <p>Beyond gaming, we’re adding general purpose x86 emulation based on this stack. For more information, <a href="https://docs.fedoraproject.org/en-US/fedora-asahi-remix/x86-support/">see the FAQ</a>.</p> <p>Today’s alpha is a taste of what’s to come. Not the final form, but enough to enjoy <a href="https://store.steampowered.com/app/620/Portal_2/"><strong>Portal 2</strong></a> while we work towards “1.0”.</p> <figure> <a href="https://rosenzweig.io/blog/Games-Asahi/Portal2-small.png"><img src="https://rosenzweig.io/blog/Games-Asahi/Portal2-small.avif" alt="Portal 2"></a> </figure> <h2 id="acknowledgements">Acknowledgements</h2> <p>This work has been years in the making with major contributions from…</p> <ul> <li><a href="https://rosenzweig.io/">Alyssa Rosenzweig</a></li> <li><a href="https://lina.yt/me">Asahi Lina</a></li> <li><a href="https://social.treehouse.systems/@chaos_princess">chaos_princess</a></li> <li><a href="https://github.com/davide125">Davide Cavalca</a></li> <li><a href="https://mastodon.social/@dougall">Dougall Johnson</a></li> <li><a href="https://ella.gay/">Ella Stanforth</a></li> <li><a href="https://www.gfxstrand.net/faith/welcome/">Faith Ekstrand</a></li> <li><a href="https://social.treehouse.systems/@janne">Janne Grunau</a></li> <li><a href="https://chaos.social/@karolherbst">Karol Herbst</a></li> <li><a href="https://social.treehouse.systems/@marcan">marcan</a></li> <li><a href="https://mary.zone/">Mary Guillemard</a></li> <li><a href="https://neal.gompa.dev/">Neal Gompa</a></li> <li><a href="https://sinrega.org/">Sergio López</a></li> <li><a href="https://github.com/TellowKrinkle">TellowKrinkle</a></li> <li><a href="https://github.com/teohhanhui">Teoh Han Hui</a></li> <li><a href="https://mastodon.gamedev.place/@robclark">Rob Clark</a></li> <li><a href="https://github.com/sonicadvance1">Ryan Houdek</a></li> </ul> <p>… Plus hundreds of developers whose work we build upon, spanning the Linux, Mesa, Wine, and FEX projects. Today’s release is thanks to the magic of open source.</p> <p>We hope you enjoy the magic.</p> <p>Happy gaming.</p> <p><a href="https://rosenzweig.io/">Back to home</a></p> </div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Tenno – Markdown and JavaScript = an Hybrid of Word and Excel (282 pts)]]></title>
            <link>https://tenno.app</link>
            <guid>41798477</guid>
            <pubDate>Thu, 10 Oct 2024 13:15:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tenno.app">https://tenno.app</a>, See on <a href="https://news.ycombinator.com/item?id=41798477">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Owe your banker £1k you are at his mercy; owe him £1m the position is reversed (2019) (136 pts)]]></title>
            <link>https://quoteinvestigator.com/2019/04/23/bank/</link>
            <guid>41798027</guid>
            <pubDate>Thu, 10 Oct 2024 12:08:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quoteinvestigator.com/2019/04/23/bank/">https://quoteinvestigator.com/2019/04/23/bank/</a>, See on <a href="https://news.ycombinator.com/item?id=41798027">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-135737">
		
	
	<div>
		<p><strong>John Maynard Keynes? Paul Bareau? John Paul Getty? Anonymous?</strong></p>
<p><img fetchpriority="high" decoding="async" src="https://77c054.p3cdn1.secureserver.net/wp-content/uploads/2019/04/money08.jpg" alt="" width="540" height="221" srcset="https://77c054.p3cdn1.secureserver.net/wp-content/uploads/2019/04/money08.jpg 540w, https://77c054.p3cdn1.secureserver.net/wp-content/uploads/2019/04/money08-300x123.jpg 300w" sizes="(max-width: 540px) 100vw, 540px"><strong>Dear Quote Investigator:</strong> The relationship between bankers and borrowers is symbiotic and occasionally counter-intuitive. Here is a pertinent adage:</p>
<blockquote><p>If you owe the bank $100, that’s your problem; if you owe the bank $100 million, that’s the bank’s problem.</p></blockquote>
<p>The prominent economist John Maynard Keynes apparently made a similar remark using pounds sterling instead of dollars. Would you please explore this topic?</p>
<p><strong>Quote Investigator:</strong> The earliest strong match known to <strong>QI</strong> occurred in a memo that Keynes circulated to the British War Cabinet in 1945; however, the attribution was anonymous. Emphasis added to excerpts by <strong>QI</strong>:<span><a role="button" tabindex="0" href="#f+135737+1+1"><sup id="footnote_plugin_tooltip_135737_1_1">[1]</sup></a><span><span id="r+135737+1+1"></span></span><span id="footnote_plugin_tooltip_text_135737_1_1">1979, The Collected Writings of John Maynard Keynes: Volume 24: Activities 1944-1946: The Transition to Peace, Edited by Donald Moggridge, Section: Overseas Financial Policy in Stage III (Revised&nbsp;… <a href="#f+135737+1+1">Continue reading</a></span></span></p>
<blockquote><p>On such conditions, by cunning and kindness, we have persuaded the outside world to lend us upwards of the prodigious total of £3,000 million. The very size of these sterling debts is itself a protection. The old saying holds. <strong>Owe your banker £1,000 and you are at his mercy; owe him £1 million and the position is reversed.</strong></p></blockquote>
<p>Below are additional selected citations in chronological order.</p>
<p><span id="more-135737"></span>In 1942 H. L. Mencken included a thematically related adage in his remarkable compendium “A New Dictionary of Quotations on Historical Principles from Ancient and Modern Sources”<span><a role="button" tabindex="0" href="#f+135737+1+2"><sup id="footnote_plugin_tooltip_135737_1_2">[2]</sup></a><span><span id="r+135737+1+2"></span></span><span id="footnote_plugin_tooltip_text_135737_1_2">1942, A New Dictionary of Quotations on Historical Principles from Ancient and Modern Sources, Selected and Edited by H. L. Mencken (Henry Louis Mencken), Topic: Bank, Quote Page 82, Alfred A. Knopf.&nbsp;… <a href="#f+135737+1+2">Continue reading</a></span></span></p>
<blockquote><p><strong>If you owe a bank enough money you own it.</strong><br>
Author unidentified</p></blockquote>
<p>In 1943 “Esar’s Comic Dictionary” by Evan Esar included the same adage listed in Mencken’s work.<span><a role="button" tabindex="0" href="#f+135737+1+3"><sup id="footnote_plugin_tooltip_135737_1_3">[3]</sup></a><span><span id="r+135737+1+3"></span></span><span id="footnote_plugin_tooltip_text_135737_1_3"> 1943, Esar’s Comic Dictionary by Evan Esar, Entry: bank, Quote Page 21, Harvest House, New York. (Verified on paper) </span></span></p>
<p>In 1945 Keynes circulated a memo containing the expression under analysis as mentioned previously. The memo appeared in volume 24 of “The Collected Writings of John Maynard Keynes” which was published in 1979.</p>
<p>In February 1947 “Time” magazine credited Keynes with a slightly different phrasing of the quip:<span><a role="button" tabindex="0" href="#f+135737+1+4"><sup id="footnote_plugin_tooltip_135737_1_4">[4]</sup></a><span><span id="r+135737+1+4"></span></span><span id="footnote_plugin_tooltip_text_135737_1_4"> 1947 February 17, Time, Foreign News: Whose Mercy?, Time Inc, New York. (Verified via online archive at time.com on April 21, 2018)</span></span></p>
<blockquote><p>Before his death, Lord Keynes had spoken his mind about those sterling debts: <strong>“If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at your mercy.”</strong></p></blockquote>
<p>Four days later the saying was printed in the “San Francisco Chronicle” of California with an ascription to Keynes:<span><a role="button" tabindex="0" href="#f+135737+1+5"><sup id="footnote_plugin_tooltip_135737_1_5">[5]</sup></a><span><span id="r+135737+1+5"></span></span><span id="footnote_plugin_tooltip_text_135737_1_5"> 1947 February 21, San Francisco Chronicle, Editorial: Indian Problem Headed For a Showdown, Quote Page 12, Column 1, San Francisco, California. (GenealogyBank)</span></span></p>
<blockquote><p>Britain owes India £1,250,000,000, a sum which must be paid in British products if at all.</p>
<p>The late Lord Keynes said in this connection: <strong>“If you owe your bank manager a thousand pounds, you are at his mercy. If you owe him a million pounds, he is at yours.”</strong></p></blockquote>
<p>In April 1947 the periodical “Nation’s Business” published a version using dollars instead of pounds sterling:<span><a role="button" tabindex="0" href="#f+135737+1+6"><sup id="footnote_plugin_tooltip_135737_1_6">[6]</sup></a><span><span id="r+135737+1+6"></span></span><span id="footnote_plugin_tooltip_text_135737_1_6">1947 April, Nation’s Business, Volume 35, Number 4, On the Lighter Side of the Capital, Journalistic thermostats, Quote Page 94, Column 2, Chamber of Commerce of the U.S., Washington D.C.&nbsp;… <a href="#f+135737+1+6">Continue reading</a></span></span></p>
<blockquote><p>Some one revived a conclusion reached by one of the early Americans:<br>
<strong>“If you owe a banker $100 he’s got you. If you owe him $100,000 you’ve got him.”</strong></p></blockquote>
<p>In 1951 economist Paul Bareau employed the saying while delivering a speech at the Insurance Institute of London:<span><a role="button" tabindex="0" href="#f+135737+1+7"><sup id="footnote_plugin_tooltip_135737_1_7">[7]</sup></a><span><span id="r+135737+1+7"></span></span><span id="footnote_plugin_tooltip_text_135737_1_7">1953, Journal of the Insurance Institute of London, Volume 40, Session 1951-1952, The Place of Sterling in the Modern World by Paul Bareau (Economist, Asst. City Editor, News Chronicle), Description:&nbsp;… <a href="#f+135737+1+7">Continue reading</a></span></span></p>
<blockquote><p>To-day it is the customer, the debtor, who has the upper hand, although that in itself need not be a reason for immediate weakness. I always remember Keynes’s saying: <strong>“When you owe your bank manager £1,000 you are at his mercy. But if you owe him £1,000,000 he is at your mercy.”</strong></p></blockquote>
<p>In 1959 a newspaper in Salt Lake City, Utah published a variant involving the bank’s board of directors:<span><a role="button" tabindex="0" href="#f+135737+1+8"><sup id="footnote_plugin_tooltip_135737_1_8">[8]</sup></a><span><span id="r+135737+1+8"></span></span><span id="footnote_plugin_tooltip_text_135737_1_8"> 1959 August 14, The Salt Lake Tribune, Dan Valentine’s Nothing Serious, Quote Page B1, Column 2, Salt Lake City, Utah. (Newspapers_com) </span></span></p>
<blockquote><p>SAM, THE SAD CYNIC, SAYS:<br>
<strong>Owe a bank $1,000 an it’ll foreclose, owe it a million and it’ll elect you to the board of directors!</strong></p></blockquote>
<p>In 1963 the “Edwardsville Intelligencer” of Illinois printed another instance that did not mention precise amounts of currency:<span><a role="button" tabindex="0" href="#f+135737+1+9"><sup id="footnote_plugin_tooltip_135737_1_9">[9]</sup></a><span><span id="r+135737+1+9"></span></span><span id="footnote_plugin_tooltip_text_135737_1_9"> 1963 December 17, Edwardsville Intelligencer, Credits for the U.S.S.R., Quote Page 4, Column 2, Edwardsville, Illinois. (Newspapers_com) </span></span></p>
<blockquote><p>Keynes once said that <strong>if you owe your bank manager a modest debt you are in his power; if you owe a huge debt, he is in yours.</strong></p></blockquote>
<p>In 1975 a newspaper in Binghamton, New York printed another variant reminiscent of the saying in Mencken’s compendium:<span><a role="button" tabindex="0" href="#f+135737+1+10"><sup id="footnote_plugin_tooltip_135737_1_10">[10]</sup></a><span><span id="r+135737+1+10"></span></span><span id="footnote_plugin_tooltip_text_135737_1_10"> 1975 June 18, The Evening Press, N.Y. City Rescue Has Price by George F. Will, Quote Page 6A, Column 3, Binghamton, New York. (Newspapers_com) </span></span></p>
<blockquote><p>Beame was operating on this principle: <strong>If you owe the bank $100, the bank owns you; if you owe $1 million, you own the bank.</strong></p></blockquote>
<p>In 2000 the fanciful tabloid “Weekly World News” assigned the saying to business titan John Paul Getty:<span><a role="button" tabindex="0" href="#f+135737+1+11"><sup id="footnote_plugin_tooltip_135737_1_11">[11]</sup></a><span><span id="r+135737+1+11"></span></span><span id="footnote_plugin_tooltip_text_135737_1_11"> 2000 May 23, Weekly World News, Just For Laughs, Quote Page 5, Column 4, Weekly World News Inc., Lantana, Florida. (Google Books Full View) <a href="https://books.google.com/books?id=MPADAAAAMBAJ&amp;q=%22owe+the%22#v=snippet&amp;">link</a> </span></span></p>
<blockquote><p><strong>If you owe the bank $100, that’s your problem. If you owe the bank $100 million, that’s the bank’s problem.</strong> — John Paul Getty</p></blockquote>
<p>In conclusion, John Maynard Keynes helped to popularize this expression when he included it in a 1945 memo, but he disclaimed credit by calling it an “old saying”. The originator remains anonymous.</p>
<p>Image Notes: Illustration of money flying through the air from PublicDomainPictures at Pixabay. Image has been cropped and resized.</p>
<p>(Great thanks to George Mannes and Adam Rose whose inquiries led QI to formulate this question and perform this exploration. Both of them noted that the expression had been attributed to J. Paul Getty and John Maynard Keynes. Thanks to the volunteer editors of Wikiquote who presented the important 1945 citation in “The Collected Writings of John Maynard Keynes”. Thanks to Barry Popik for his <a href="https://www.barrypopik.com/index.php/new_york_city/entry/if_you_owe_a_bank_thousands_you_have_a_problem_if_you_owe_a_bank_millions_t">helpful research</a>. Many thanks to Rand Hartsell of the University of Illinois, Urbana-Champaign for precisely locating and verifying the citation in the “Journal of the Insurance Institute of London”.)</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why we're helping more wikis move away from Fandom (868 pts)]]></title>
            <link>https://weirdgloop.org/blog/why-were-helping-more-wikis-move-away-from-fandom</link>
            <guid>41797719</guid>
            <pubDate>Thu, 10 Oct 2024 11:21:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://weirdgloop.org/blog/why-were-helping-more-wikis-move-away-from-fandom">https://weirdgloop.org/blog/why-were-helping-more-wikis-move-away-from-fandom</a>, See on <a href="https://news.ycombinator.com/item?id=41797719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Hi! You may have seen that Weird Gloop is now hosting the <a href="https://wiki.leagueoflegends.com/" target="_blank" rel="noopener noreferrer">official League of Legends Wiki</a>. We’ve spent the last couple months working with the Riot folks and the League wiki editors to move it off of Fandom, and turn it into something the players will (hopefully!) really dig. I also love that it got started because one of the Riot guys plays a ton of Old School RuneScape and thinks <a href="https://oldschool.runescape.wiki/" target="_blank" rel="noopener noreferrer">our wiki</a> is awesome. How cool is that??</p>
<p>I want this to kick off a new era where communities and developers take control from Fandom, and make some really great wikis. We’ve already been doing a bit of this, starting when we helped the <a href="https://minecraft.wiki/" target="_blank" rel="noopener noreferrer">Minecraft Wiki</a> leave Fandom, but I think it’s time for me (and the rest of our group) to be more explicit about what we want to do.
</p>
<p>So if you’re any of these things:</p>
<ul>
<li>A frustrated wiki editor trying to figure out your options</li>
<li>A community manager trying to get internal support for an official wiki</li>
<li>Someone contemplating making a new wiki</li>
</ul>
<p><strong>I will give you (free, very specific) advice on how to get your wiki off Fandom, and make a kickass wiki somewhere else</strong>. We might even be able to host you ourselves.</p>
<p>If you think this sounds cool, <a href="https://weirdgloop.org/contact">come talk to me</a>.</p>
<ul id="markdown-toc">
<li><a href="#why-do-we-actually-care" id="markdown-toc-why-do-we-actually-care">Why do we actually care?</a></li>
<li><a href="#why-ditching-fandom-is-cool-and-based" id="markdown-toc-why-ditching-fandom-is-cool-and-based">Why ditching Fandom is cool and based</a></li>
<li><a href="#what-im-offering" id="markdown-toc-what-im-offering">What I’m offering</a></li>
<li>
<a href="#how-to-not-turn-into-fandom-20-with-these-2-simple-tricks" id="markdown-toc-how-to-not-turn-into-fandom-20-with-these-2-simple-tricks">How to not turn into Fandom 2.0 (with these 2 simple tricks)</a> <ul>
<li><a href="#point-1---wiki-communities-need-to-be-able-to-freely-leave-their-host" id="markdown-toc-point-1---wiki-communities-need-to-be-able-to-freely-leave-their-host">Point 1 - wiki communities need to be able to freely leave their host</a></li>
<li><a href="#point-2---global-branding-is-extremely-negative-value-for-wiki-farms" id="markdown-toc-point-2---global-branding-is-extremely-negative-value-for-wiki-farms">Point 2 - global branding is extremely negative value for wiki farms</a></li>
</ul>
</li>
</ul>
<h2 id="why-do-we-actually-care">Why do we actually care?</h2>
<p><a href="https://archive.ph/kwt1b" target="_blank" rel="noopener noreferrer">This post</a> (and many others) have done a much better job than I could, explaining from a reader’s perspective why Fandom is bad place to host a wiki, but I thought it might be useful to give my take on it as a long-time wiki editor.</p>
<p>I love wikis. I think it’s unbelievably cool that this completely insane idea (“what if we just had a website that anyone can edit?”) doesn’t descend into anarchy, and instead self-organizes into a fun, project-oriented community. I think that despite its flaws, Wikipedia is the single coolest thing the internet has ever done. And wikis on niche topics feel like some of the last remnants of a friendlier, more collaborative, early 2000s web. I loved contributing to wikis, building something with other people, and feeling a sense of ownership (and pride that so many people were using stuff I made).</p>
<p>Which is why it’s so concerning that Fandom has taken this wonderful concept and turned it into one of the most dreadful parts of the internet. Being deeply involved with the RuneScape Wiki on Fandom had a huge psychological cost – what wonderful thing did they add today that made our wiki harder to use? Scammy green link ads? Comically bad videos on the top of our most popular pages? Garbage AI-generated Q&amp;A? Ads that take up literally 100% of the content window?</p>
<p>I (and so many others) had spent countless nights trying to make the best possible resource for RuneScape, and it was brutal to realize that it didn’t matter how hard we worked or creative we were – our wiki was never gonna be that great, because Fandom was in charge. That sense of ownership and pride…slowly turned into feeling like my passion was being exploited by a company that didn’t want the same things I did.</p>
<p>We weren’t the only ones feeling this way, of course – some wiki communities got fed up and moved somewhere independent. But here’s the key thing you need to understand: even when a wiki community unanimously wants to leave, Fandom keeps their copy of the wiki up, even though it no longer has a community. Google remembers years of people searching, linking, and visiting the Fandom wiki URLs, and continues to rank the increasingly stale Fandom results first. Since roughly 85% of a wiki’s traffic comes from Google, it’s nearly impossible for the new wiki to win without fixing this ranking disparity. It’s an extremely draining thing to do – nobody likes to spend their waking hours competing against the thing they helped lovingly build.</p>
<p>Historically, independent wikis have had an extremely hard time winning this battle. Most of the traffic stayed on the Fandom wiki, and the independent wikis often fizzled out. This had a chilling effect on the remaining communities, and emboldened Fandom to further prioritize revenue extraction.</p>
<p>That’s the key takeaway: <strong>if leaving Fandom was easy, they wouldn’t be able to enshittify as much as they have</strong>.</p>
<p>But don’t lose hope! Google has gotten much friendlier to independent wikis over the last decade. With a large, sustained effort, we were able to recover 95% of RuneScape Wiki traffic within the first year.</p>
<h2 id="why-ditching-fandom-is-cool-and-based">Why ditching Fandom is cool and based</h2>
<p>The main advantage of leaving Fandom is likely clear to anyone who’s ever visited one of their wikis without an ad blocker. But there’s more than that! When you have a site that people are happy to go to (instead of something they’re forced to grimace and use), you get all these wonderful secondary effects that are worth mentioning.</p>
<p>For starters: on average, moving away from Fandom doubles the number of people editing. I’ve seen the pattern across every wiki we’ve ever moved off Fandom, but here’s a pretty striking graph from OSRS Wiki:</p>
<p><img src="https://weirdgloop.org/images/posts/osrs-edit-count.png" alt="Graph of OSRS editors by editcount per month" width="650"></p>
<p>It’s incredibly consistent: way more people show up and want to help, when they feel like they’re contributing to something that isn’t taking advantage of them.</p>
<p>It’s not a coincidence that OSRS Wiki got really good once we left Fandom in 2018. Once we had way more people wanting to contribute (and the only objective was “make the best possible wiki for the game”) the wiki magically got way better! Crazy!</p>
<p>Departing from Fandom has also opened the door for a number of custom technical projects that otherwise would have been downright impossible to implement on the old wiki. <a href="https://oldschool.runescape.wiki/w/RuneScape:Lookup" target="_blank" rel="noopener noreferrer">In-game item lookup</a>, <a href="https://oldschool.runescape.wiki/w/RuneScape:WikiSync" target="_blank" rel="noopener noreferrer">WikiSync</a> and <a href="https://prices.runescape.wiki/osrs/" target="_blank" rel="noopener noreferrer">real-time prices</a> are core parts of our offering now, with hundreds of thousands of users. They’re all made possible by the new flexibility we gained when we took control of the hosting.</p>
<h2 id="what-im-offering">What I’m offering</h2>
<p>I think a lot of people would love to get their wiki off Fandom, but it’s extremely not obvious what that even involves, so it’s hard to formulate a plan. <strong>I will help you figure out a viable, detailed strategy for you to get your wiki off Fandom, and bring the traffic along</strong>.</p>
<p>In the next couple weeks, we’ll be posting some general advice on this blog that goes through the main steps and pitfalls involved with leaving Fandom. Most of it should be broadly applicable, but the real power comes from looking at the specifics of your topic (how big is it? does it change frequently? is it a game? are you the rights-holder?) and tailoring the plan to fit.</p>
<p>As far as where you host it…there’s plenty of decent options. Wiki hosting is not nearly as hard as Fandom makes it out to be – for example, if you’re the Path of Exile devs and you already host a bunch of PHP web stuff, then hosting the wiki yourself is objectively a really good option.</p>
<p>Sometimes Weird Gloop will be the good option for your situation, and being totally honest, sometimes it won’t be. And that’s okay! I want to help communities get away from Fandom, regardless of who’s running the servers.</p>
<p>I will say, I don’t think we would ever do a “self-service” thing where you could just sign up and immediately make a wiki. We want to do projects where we get to know the community, and closely support every wiki we host.</p>
<h2 id="how-to-not-turn-into-fandom-20-with-these-2-simple-tricks">How to not turn into Fandom 2.0 (with these 2 simple tricks)</h2>
<p>As we’ve started hosting more wikis besides RuneScape, some people have asked a pretty reasonable question: what’s stopping us from eventually getting enshittified, just like Fandom (or the other wiki farms that eventually sold to Fandom)?</p>
<p>From my perspective, there are two key choices that Fandom made that have had major negative consequences for communities. And we’re just going to do the exact opposite on both points.</p>
<h3 id="point-1---wiki-communities-need-to-be-able-to-freely-leave-their-host">Point 1 - wiki communities need to be able to freely leave their host</h3>
<p>You can probably tell that I think wiki editors (as opposed to hosts) are the ones who create the vast majority of the value on a wiki. So the premise is simple:</p>
<p><strong>If a wiki community is unhappy, and they have a better option somewhere else, they should be able to leave and take their stuff with them</strong>. We won’t prop up the old wiki, Weekend-at-Bernie’s style, abusing the dominant Google position that the wiki editors built up while they were on our platform.</p>
<p>In my opinion, <em>this is really the only rule that matters</em>. If you have the ability to leave (and take your revenue-driving wiki with you) when things go to shit, then your host has an extremely strong incentive to not let things completely go to shit.</p>
<p>There’s a long history of wiki farms vaguely handwaving that they’d agree to something like this, and then backtracking later. So why believe us?</p>
<p>It helps that Weird Gloop literally only exists because we were on the losing end of this sort of situation with Fandom back in 2018, and that we have no outside investors or debt (the company’s owned by wiki nerds)…but I don’t think that’s convincing enough on its own. So we’ve been voluntarily entering into agreements with the wikis we host (<a href="https://meta.minecraft.wiki/w/Memorandum_of_Understanding_with_Weird_Gloop" target="_blank" rel="noopener noreferrer">here’s an example</a>) where we set very clear obligations for what happens if the wiki community wants to go somewhere else (hint: it’s all about the domain). If we ever start going down the same path as Fandom, everyone can just leave! I would love to see other wiki platforms start to do this, because I think it’s the only way you really solve the problem.</p>
<h3 id="point-2---global-branding-is-extremely-negative-value-for-wiki-farms">Point 2 - global branding is extremely negative value for wiki farms</h3>
<p>If you go to any page on a Fandom wiki, even if you’ve got an ad blocker…you’ll be greeted by an absurd amount of Fandom-related branding: a gaudy sidebar that links to Fan Central (whatever that is), a bunch of other links to wikis that aren’t relevant to you, buttons to follow Fandom on Instagram, TikTok, to take “Fan Quizzes”. The brand strategy seems like it was cooked up by a bunch of market researchers who think that people are fans of…media properties in general? It’s super cringey and totally irrelevant to the people who are on Fandom wiki to, say, look up the stats of a new pickaxe they got.</p>
<p>It’s easy to laugh about how bad the branding and identity is, but there’s a bigger issue: the fact that it’s so overwhelmingly branded as “Fandom” (as opposed to, say, the Warframe Wiki) makes it way harder for each of the individual wikis to develop an public identity, because anything they do will be subordinate to the (very loud) global brand. These individual wikis are the only popular thing that Fandom has ever operated, <strong>and the focus on global branding makes each individual wiki worse</strong>.</p>
<p>Our position: the actual wikis should be front and center, because it’s way more important for the wiki itself to have a great reputation, rather than sucking all the oxygen out to make sure people know who owns the platform. We have extremely minimal branding (<a href="https://minecraft.wiki/" target="_blank" rel="noopener noreferrer">can you even find it?</a>), and I can’t imagine ever trying to put wikis on subdomains of weirdgloop.org (or anywhere else) unless there were no decent domain options. We don’t actually get anything out of everyday readers knowing who we are.</p>
<hr>
<p>That’s all I’ve got right now. If you liked this and want to talk to me about wiki things, please <a href="https://weirdgloop.org/contact">come say hi</a> – it doesn’t matter if you have a big wiki or a small wiki (or no wiki at all!) – I really just love talking to people about this stuff.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Let's talk about Animation Quality (211 pts)]]></title>
            <link>https://theorangeduck.com/page/animation-quality</link>
            <guid>41797462</guid>
            <pubDate>Thu, 10 Oct 2024 10:38:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theorangeduck.com/page/animation-quality">https://theorangeduck.com/page/animation-quality</a>, See on <a href="https://news.ycombinator.com/item?id=41797462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <hr>


<h3>02/10/2024</h3>

<p>Attending both CVPR and SIGGRAPH this year it was interesting to observe and discuss the difference between how vision and graphics researchers approach the field of animation. There was a sense from some researchers that the reviews from the graphics communities like SIGGRAPH could be too tough, and rejected papers based on petty irrelevant issues like shadow quality. While on the other side there were feelings that the vision community are willfully ignoring problems in their results while marching forward to develop new models. And I think both of these feelings really come to the forefront on the issue of animation quality, so I thought it might be a good opportunity to examine that subject a bit more.</p>

<hr>

<h2>High Quality Animation Data</h2>

<p>I think one of the best ways to start looking at animation quality is to look at some of the different sources of animation data in the community and compare them. For example, here is a video of some data from the <a href="https://github.com/simonalexanderson/MotoricaDanceDataset" target="blank">Motorica Dance Dataset</a> which is one of the highest quality publicly available datasets of motion capture in the graphics community:</p>



<p>Here is another animation, this time with the skeleton visualized.</p>



<p>This is the kind of data that is often typical in the AAA game and VFX industry. This data is sampled at 120 Hz, with finger and toe motions, captured with an optical system which can triangulate marker positions with sub-mm accuracy and solve onto a skeleton with realistic proportions and carefully positioned joints that accurately fit the actor. It's easy to see all of the detail and nuance that exists in data like this. The foot contacts are perfectly clean, there is no perceptual noise or foot sliding, and when skinned by a talented artist onto a well made character you get something that looks very natural and realistic with all sorts of subtle details.</p>

<p>When I look at this kind of animation data I see it as a target. Whatever animation systems we build should aim to generate animation of this quality. But I also see it as a kind of ceiling - because when we are evaluating our methodologies, the maximum quality of the data is a bit like the sensitivity of our measuring device - once the differences become indistinguishable we can't evaluate them reliably anymore.</p>

<p>So let's start by trying to work out some basic facts about this kind of data. What sort of precision is required when capturing, storing, or generating it?</p>

<hr>

<h2>Temporal Resolution</h2>

<p>I'll start with the temporal dimension... and I'll keep my analysis stupidly simple here... because if you talk to someone who actually knows about signal processing, they will tell you that almost everything we do when it comes to handling the temporal aspect of animation data is already wrong - from up-sampling to down-sampling and interpolation.</p>

<p>(For an accessible intro for graphics people as to what exactly is wrong I highly recommend <a href="https://bartwronski.com/2020/04/14/bilinear-texture-filtering-artifacts-alternatives-and-frequency-domain-analysis/" target="blank">Bart Wronski's blog</a> or the classic <a href="http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf">A Pixel Is Not A Little Square</a>. I'm also committing a bit of a sin here by drawing lines connecting the data-samples in my graphs... but all of that is a subject for another day.)</p>

<p>Anyway, if we take this motion from the dataset:</p>



<p>And plot a one second window of the shoulder rotation over time...</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/rotation.png" alt="shoulder rotation">
</p>

<p>...we can see that even at 60 Hz the signal is not entirely smooth. Once we are down to 15 Hz we have a signal that is just obviously far too coarse to capture the nuances of the motion.</p> 

<p>If we plot the velocity of these signals (computed by finite difference) you can see we can get differences of over 90 degrees per second in the velocity we compute between the 60Hz signal and the 15Hz one.</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/velocity.png" alt="shoulder rotation">
</p>

<p>Also take a second to note how quickly the angular velocities can change in this kind of data. Here we go from a velocity of around +150 degrees per second in one direction to -120 degrees per second in the opposite direction, within the space of 0.2 seconds. In fact, I would argue that even at 120 Hz, when we have these large peaks, the velocities are not really smooth at this sampling frequency:</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/velocity120.png" alt="shoulder rotation">
</p>

<p>The differences in acceleration are therefore obviously even worse, and completely degraded for the 15 Hz signal:</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/acceleration.png" alt="shoulder rotation">
</p>

<p>Here is an overlay of a clip of animation sampled at 60 Hz, and the same clip downsampled to 15 Hz (by taking every 4th frame) and then linearly upsampled again.</p>



<p>At normal playback speed, most of the time the visual difference is fairly minimal. However if we slow the playback down we can see issues: the temporal down-sampling has introduced errors of at least several centimeters in some places.</p>

<p>The fact is this: the reason we often deal with animation data sampled at 60 Hz is not because no higher frequency information exists and this is a sufficient frequency to record losslessly all the information in the data, but because the rate monitors refresh at is 60 Hz. As I described in my article on <a href="https://theorangeduck.com/page/cubic-interpolation-quaternions">cubic interpolation of quaternions</a> - as soon as we start to sample the data at other rates, unless we are careful, sampling frequency artefacts can re-appear very quickly.</p>

<p>And I think we can conclude that although we can sometimes get away with displaying animation at lower sampling frequencies than 60 Hz without much perceptual difference, if we really want to hit the motion quality shown above, and also in terms of storage and processing, 60 Hz is already kind of the minimum we should be aiming for if we want to preserve the temporal information in the data - and that if you are planning on doing any kind of real signal processing on your data (such as frequency transformations, low-pass filters, or even extraction of velocities, accelerations, or torques) you should try to aim for higher resolution if you can.</p>

<p>If you want a ballpark figure, <a href="https://www.sciencedirect.com/science/article/abs/pii/S0021929022000902">this paper</a> provides a more detailed analysis and suggests that to preserve 90% accuracy for the acceleration information at least 180 Hz is required when it comes to human motion.</p>

<hr>

<h2>Numerical Precision</h2>

<p>So that is temporal resolution. What about in terms of the numerical precision of each frame?</p>

<p>Here is a quick experiment that shows how important this is too. This is what our animation data looks like if I round all the quaternion floating point values to three decimal places and then re-normalize before performing the forward kinematics step:</p>



<p>As you can see, with three decimal places of accuracy, the quantization error is pretty obvious (take a look at the head) and we see the character jumping discretely from pose to pose. So numerically we also need a pretty high amount of precision in our floating point values to represent each pose.</p>

<p>Here is another similar experiment. In this case I masked out the bottom 16 bits of the floating point mantissa.</p>



<p>The error is again pretty obvious. So to get perceptually accurate motion we probably need something like 10 of the 23 bits of the floating point mantissa to be correct.</p>

<p>Obviously a huge part of this is the error propagation that we get down the joint chain... but either way, it means if we have any kind of numerical system that processes, generates, or in any way touches the local joint rotations in animation data we should make sure it is numerically accurate to as many decimal places as possible. For example, 16-bit floats are probably a no-go for animation generation if we want any chance of achieving the quality of animation shown at the beginning of this article - they are only accurate to at most five decimal places in the zero to one range (they have a 10-bit mantissa) which leaves extremely little room for error.</p>

<p>Take a moment to consider how different this is to the image-generation domain where images are usually stored with just 8-bits of precision per channel. Not only does this mean they require much less precision as 8-bits means pixel channels can only take one of 256 values, but since humans are so much less sensitive to pixel color differences we don't even really need this - noise, quantization error, and other artefacts are often not even noticeable to the human eye anyway. As a ballpack figure I would say animation generation needs to be at least one hundred times more numerically accurate than image generation if we are dealing with local joint rotations.</p>

<p>We can do a simple experiment to show this. Here I add Gaussian noise to the quaternions of the local joint rotations with a standard deviation of 0.001 and then re-normalize. As you can see the motion it totally broken:</p>



<p>On the other hand, starting with this image...</p>

<p>
    <img src="https://theorangeduck.com/media/uploads/MotionQuality/tomatoes_original.png" alt="tomatoes original">
</p>

<p>Here is what I get if I add noise with a standard deviation of 0.01. The different is barely noticeable...</p>

<p>
    <img src="https://theorangeduck.com/media/uploads/MotionQuality/tomatoes_noisy.png" alt="tomatoes noisy">
</p>

<p>So to summarize: if we want to produce animation with the quality shown at the beginning of this article, 60 Hz is really the minimum frequency we should be dealing with when it comes to the temporal resolution, and we need numerical accuracy of at least five or six decimal places on our local joint rotations. Anything less than that and we can already expect to be producing visible errors.</p>

<hr>

<h2>Data Issues</h2>

<p>Now that we have an idea of what high quality motion capture data looks like and what the constraints are, let's take a look at some of the other data sets in the community.</p>

<p>First a brief disclaimer: my goal is not to be critical of these datasets. They have been incredible workhorses for the vision and graphics communities and I really appreciate all the work that has gone into them from so many people. I've used them extensively in my own papers! My goal here is simply to talk about some of the issues you might face when trying to use them as a researcher.</p>

<p>For example, let's look at some data from the <a href="https://moyo.is.tue.mpg.de/index.html">MOYO</a> yoga part of the <a href="https://amass.is.tue.mpg.de/">AMASS</a> dataset. Here I have extracted the skeleton via the regression method. I'm going to put it on a skinned character too, but first let's look at just the skeletal data itself:</p>

<p>We'll start with the obvious problems and the first one is right there - the rest pose:</p>



<p>I'm unconvinced that the actor had their knees bent outward like that when they were doing a T-pose. That means we already have some kind of constant systematic error in our data when it comes to joint rotations and positioning. Not only is that going to make it difficult to skin and retarget this data but having such a large systematic error right from the start doesn't give us much confidence regarding what else might have gone wrong with the capture and processing.</p>

<p>The next most obvious issue is that the character's feet penetrate the floor. This fix is more simple - we just need to vertically translated the character in all the data. No big deal - but it does mean we can't just naively throw this data into a network along with a bunch of other data without manual correction and cleaning.</p>

<p>Third is that the data is missing hand, finger, and toe movements - although that is pretty common for most publicly available datasets.</p>

<p>If we browse a few of the clips in this dataset we can see there are many places where we have glitches and jitters:</p>



<p>All of these sections should be trimmed from the database before training. These kind of glitches and outliers can be really damaging to training of a neural network and often causing exploding gradients and NaNs to appear in losses. Techniques like gradient clipping often hide/handle these kinds of data issues but even with that, there can be big negative affects on things such as normalization.</p>

<p>Lots of the clips also have more subtle issues like this kind of low level noise that is very hard to spot (take a look at the spine about 5 seconds in):</p>



<p>We can remove this noise by filtering the data, but as discussed earlier, at a sampling rate of 60 Hz we cannot perfectly capture the fastest component of human motion. This means that there is important motion information encoded in the higher frequencies of our signal. Applying any low-pass filter for denoising would also inevitably erase parts of the actual signal.</p>

<p>
<img src="https://theorangeduck.com/media/uploads/MotionQuality/rotation_noise.png" alt="rotation noise">
</p>

<p>For example, if I apply this same Gaussian filter to the Motorica data we can see it can end up modifying the data by several centimeters in places and removes some subtle details. Here look at how the hand motion gets smoothed out:</p>



<p>I've done my best to retarget a medium-sized AMASS skeleton to the UE5 Manny so we can see some of the issues on the same skinned character. This is obviously not a perfect comparison, but nonetheless on a skinned character a whole new domain of artefacts become visible. Obviously we have missing finger and toe motion, but we also can see some joints don't twist properly, sometimes the spine pops in a weird way, we get odd shoulder motion, or the feet slide a lot.</p>



<p>Large parts of the <a href="http://mocap.cs.cmu.edu/">CMU motion capture database</a> (which AMASS contains) and many of the other large databases we use in graphics contain similar issues. Here are a few random clips from the CMU portion of AMASS where we can see all sorts of artefacts. Again, this is not an exactly fair comparison since I struggled to build a retargeting setup which worked well given the variety of rest poses in the data - and of course I have cherry picked some bad examples - but it still gives you an idea of the data.</p>



<p>As I mentioned before - I really appreciate the work that has gone into these kinds of datasets which have been powering the academic community for a long time and led to a huge amount of progress. But these datasets also come with some downsides:</p>

<p>First, data of this quality is simply too far off being practically usable in the video games or VFX industry. This immediately makes it very hard to evaluate fairly the practicality of the proposed method since we simply don't know how it might behave when trained on higher quality data.</p>

<p>Second, when examining the animation generated by a system trained on this kind of data it becomes impossible to distinguish between animation artifacts caused by the method itself, and those which are present in the training data and the system is just faithfully reproducing.</p>

<p>Third, when systems trained on this kind of data report error metrics such as joint position improvements of a few centimeters or small improvements to the FID score, given we know the ground truth data has obvious systematic errors on the order of tens of centimeters as well as other glitches and noise, it brings into question if these kind of metric differences are even meaningful any longer.</p>

<hr>

<h2>Mixed Data</h2>

<p>An argument I've heard often is this: Does it really matter if the quality is variable when there is so much <em>more</em> data in databases like AMASS than there are in other datasets? Can't I train a model on a large dataset of low quality motion and then fine-tune it on the high quality data? The mantra we hear a lot in deep learning for this kind of training regime is "Noise cancels out, signal adds up" - which is absolutely true.</p>

<p>I think this is a very interesting argument, and it's an approach which has worked fantastically in other domains (Speech Synthesis being one example). There are a lot of very smart people in the field betting on this kind of idea and I will be interested to see what kind of results it produces.</p>

<p>While I still don't really know if this will work, my gut feels skeptical. In my experience most of the time when you combine two animation datasets, you end up with a multi-modal dataset made up of two clusters that don't necessarily interact properly with each other.</p>

<p>In fact, we can somewhat see this already, because AMASS is a collection of many different datasets and capture sessions. If we look at the rest pose from part of the CMU portion of the AMASS dataset:</p>



<p>And then look at the rest pose from the Yoga portion:</p>



<p>We can see already just by eyeballing that depending on which session the data comes from the whole rest pose is completely different. If we train a model on this data it is going to need to learn how to switch between different styles of rest pose depending on what kind of motion we are asking it to produce as output. These datasets might be on the same skeleton structure but that does not mean they are "combined".</p>

<p>To me, the idea that taking a system trained on low quality dancing data (for example), training it on high quality locomotion data, and expecting it to produce high quality dancing data (and not low quality dancing data) seems like wishful thinking. To put it bluntly, the common mantra which much more reflects my experience with deep learning is this one: "garbage in, garbage out".</p>

<p>As mentioned at the beginning of the article, I like to think of the training data as your ceiling. It represents the best possible quality you can expect to produce from your model, which in most cases you will only be able to approach exponentially. That is why targeting the highest quality source data and preserving that quality through processing steps is so important.</p>

<p>In fact, I often find that as soon as you put animation data into a neural network in any form you can expect some loss in quality. We can see this by training a very simple auto-encoder on the high quality data from the Motorica dataset we showed before:</p>

<p>(In red is the ground truth and in green is the reproduction.)</p>



<p>Even after some careful training which accounts for errors propagating down the joint chain, we still get some error in the autoencoder's reconstructions on the training set which manifest in small amounts of foot sliding and other posing errors.</p>

<p>In this case I used 8 layers of 512 neurons for both the encoder and decoder and a 256-dimensional latent space. Perhaps my network was just not large enough? More layers generally equals better right? Well in this case the opposite is true. In fact, we <em>can</em> get almost perfect reproductions if we just use 2 layers instead of 8:</p>



<p>So not only can quality degrade as soon as we enter any kind of neural network, but it seems that the more layers that data passes through the worse things are.</p>

<p>All of these sources of error accumulate and propagate and obfuscate our ability to evaluate methods and architectural differences objectively and fairly. Methods that produce foot sliding on top of data (or other methods) that already have foot sliding, <em>hide</em> the foot sliding they introduce, and make the sources of error infinitely more difficult to decouple and attribute to specific methodological decisions.</p>

<p>Also, the models we train end up wasting their capacity learning to emulate the errors, noise, multiple-modes, artefacts, and whatever other undesirable things might be in the data. For example, here is the same 2 layer auto-encoder trained on of our AMASS yoga data:</p>



<p>Now all of the care we took to retain precision is working against us because we're attempting to reproduce the errors and noise. If we're not careful you can see how easy it would be to come to the wrong conclusion about model architectures. Even with this basic example we're in a situation where with good data quality we might conclude "more layers = worse", while with poor data quality "more layers = better" (as, for example, it might remove the noise more).</p>

<p>It might seem like I am nit-picking on tiny issues, and that errors of a couple of centimeters here are there are really just not a big deal compared to the new opportunities that Machine Learning opens up... but a lesson I think we can learn from the rendering community is that hacks, approximations, and other sources of errors might not seem like a big deal - until suddenly they are - because at some point you end up with a final result that looks like a hot mess - and you have no idea what to do - because you have hundreds of different sources of errors, none of which you really understand or can control - compounding together in a deeply complex way. That is the situation the rendering folks have been untying over the last 15 years.</p>

<hr>

<h2>Before Deep Learning</h2>

<p>When talking about animation quality it's a good exercise to go back and look at SIGGRAPH papers from 10 years ago before deep learning started to take off. Here is a selection of my favorites:</p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/G_bLwfzYqF4?si=8ispuSq0TSr-m_M6&amp;start=111" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p><a href="https://grail.cs.washington.edu/projects/motion-fields/">Motion Fields for Interactive Character Animation (2010)</a></p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/rd03xOukcsw?si=8ZhzfydqFcZzULs5&amp;start=161" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p><a href="https://www.cs.ubc.ca/~van/papers/2012-TOG-TerrainRunner/index.html">Terrain Runner: Control, Parameterization, Composition, and Planning for Highly Dynamic Motions (2013)</a></p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/hr3pdDl5IAg?si=gYFULf92ZS1fLoHE&amp;start=126" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p><a href="https://graphics.stanford.edu/projects/ccclde/">Continuous Character Control with Low-Dimensional Embeddings (2012)</a></p>



<p><a href="https://people.computing.clemson.edu/~sjoerg/mocap.html">Data-driven Finger Motion Synthesis for Gesturing Characters (2012)</a></p>

<p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/6rRpKF0pTwI?si=cIn0gjwsLBOViL1e&amp;start=72" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p>

<p><a href="https://web.archive.org/web/20161021195120id_/http://humanmotion.ict.ac.cn:80/papers/2015P1_StyleTransfer/Realtime%20Style%20Transfer%20for%20Unlabeled%20Heterogeneous%20Human%20Motion.pdf">Realtime Style Transfer for Unlabeled Heterogeneous Human Motion (2015)</a></p>

<p>Although all these methods have issues (and there were lots of papers from this time which did not look nearly as good), in many ways, in terms of pure animation quality what they produced is better than some of the results from deep learning methods we see now. In the very least, the animation they produce is almost always clean and artefact free: it does not have noise, or jumps and jitters, and in general it does not even contain much foot sliding.</p>

<p>If our newer methods cannot attain this quality bar then the elephant in the room is that we may not actually be making progress in the field - we might just be trading off one thing for another.</p>

<blockquote><strong>So let's talk more about animation quality - let's work out how to tackle it and not try push it under the rug because it is an inconvenient topic when it comes to publishing animation papers that use deep learning.</strong></blockquote>

<hr>

<h2>Improving Animation Quality</h2>

<p>So what can we do to try and improve animation quality across the board?</p>

<p>I think those of us in the industry have some responsibility here. Almost all games and VFX companies don't share their data with the academic world (with the exception of Ubisoft who have released some very high quality animation datasets), so how can we penalize academics for not reaching a bar they don't even know exists? We need to make more of an effort to share what we can, not just our data, but also our processes and considerations when we capture it.</p>

<p>And because I believe in putting your money where your mouth is I've done a few things in preparation for this blog post that I hope will help:</p>

<!--

<p>1) I've skinned a simple character (to the best of my limited abilities) to the skeleton used in the <a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS">ZeroEGGS</a> dataset to provide a clean and clear way to evaluate overall motion quality including fingers, toes, hand and feet contacts.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_wireframe.m4v" type="video/mp4">
</video>
</p>

<p>2) I've spent some time re-solving in MotionBuilder the <a href="https://github.com/ubisoft/ubisoft-laforge-animation-dataset/tree/master/lafan1">LaFAN dataset</a> from the original marker data and preparing a new version which is better quality and closer to what we used internally: it has toe and finger motion (at least one marker's worth of finger motion), is more carefully retargeted to this new skeleton, and is solved at 60fps instead of 30fps. Given I'm not a MotionBuilder expert, and the limited finger markers in the original data, the result is still far from perfect - but I hope it can provide a reasonable baseline for researchers given the size and diversity of the dataset. You can find it <a href="#">here</a>.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_lafan.m4v" type="video/mp4">
</video>
</p>

<p>3) Epic Games have agreed to release the animation data from their recent <a href="https://dev.epicgames.com/documentation/en-us/unreal-engine/game-animation-sample-project-in-unreal-engine">Gameplay Animation Sample</a> under a non-commercial research license. This is a production quality locomotion dataset of X hours of motion that was lovingly prepared and cleaned, and was used in Fortnite's Motion Matching release. Even if it only contains locomotion, it's still probably the highest quality large animation dataset I have seen that is publicly available for research. So if you have a method which can produce animation data of this quality, I don't think anyone can argue with you that it is not production quality. This dataset I have also retargeted to the same skeleton so that it can be combined easily with data from the LaFAN re-solve. You can find it <a href="#">here</a>.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_gas.m4v" type="video/mp4">
</video>
</p>

<p>4) I've retargeted the Motorica Dance Dataset to this skeleton in MotionBuilder so that it can be combined with the previous two datasets. You can find it <a href="#">here</a>.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_motorica.m4v" type="video/mp4">
</video>
</p>

<p>5) I've also done a basic adjustment of the ZeroEGGS dataset so that it looks correct on this new simple character. You can find it <a href="#">here</a>.</p>

<p style='text-align:center;'>
 <video autoplay loop muted width="640" height="360">
  <source src="/media/uploads/MotionQuality/geno_zeroeggs.m4v" type="video/mp4">
</video>
</p>

<p>That gives us at least four large datasets all on a common character skeleton with fingers, toes, and a skinned mesh, covering Locomotion, Jumping, Climbing, Vaulting, Dance, Speech, Get-ups, Falls, and everything else in LaFAN. I hope that can be considered a pretty good starting point for anyone who is interested in improving the motion quality in their animation research.</p>

-->

<p>1) I've skinned a simple character (to the best of my limited abilities) to the skeleton used in Ubisoft's <a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS">ZeroEGGS</a> dataset to provide a clean and clear way to evaluate overall motion quality including fingers, toes, hand and feet contacts.</p>



<p>2) I've spent some time re-solving in MotionBuilder Ubisoft's <a href="https://github.com/ubisoft/ubisoft-laforge-animation-dataset/tree/master/lafan1">LaFAN dataset</a> from the original marker data and preparing a new version which is better quality and closer to what we used internally: it has toe and finger motion (at least one marker's worth of finger motion), is more carefully retargeted to this new skeleton, and is solved at 60fps instead of 30fps. Given I'm not a MotionBuilder expert, and the limited finger markers in the original data, the result is still far from perfect - but I hope it can provide a reasonable baseline for researchers given the size and diversity of the dataset. You can find it <a href="https://github.com/orangeduck/lafan1-resolved">here</a>.</p>



<p>3) I've retargeted the Motorica Dance Dataset to this skeleton in MotionBuilder so that it can be combined with the previous two datasets. You can find it <a href="https://github.com/orangeduck/motorica-retarget">here</a>.</p>



<p>4) I've also done a basic adjustment of the ZeroEGGS dataset so that it looks correct on this new simple character. You can find it <a href="https://github.com/orangeduck/zeroeggs-retarget">here</a>.</p>



<p>That gives us at least three large datasets all on a common character skeleton with fingers, toes, and a skinned mesh, covering Dance, Speech, and everything in LaFAN. I hope that can be considered a pretty good starting point for anyone who is interested in improving the motion quality in their animation research.</p>

<p>Finally, if anyone else releases simliar quality datasets I promise I will also make a good attempt to retarget it to the same setup.</p>

<p>With that said, I think there are things to be done in the academic world too. We need to strike a balance between rewarding risk-taking, innovation, and unique ideas - but we also need to make sure we don't lower the bar to the point where our results become irrelevant to the industry or we lose the ability to evaluate what works and what doesn't. Most importantly: let us not avoid the discussion of animation quality just because it may be inconvenient for publishing.</p>

<blockquote><em>Friendly Reminder: All of these datasets are NOT licensed for commercial use. They are for research use only. The quickest way to ruin any good-will from the industry in terms of releasing data is to use this data (or models trained on this data) in a commercial product without acquiring a commercial license from the original owners of the datasets.</em></blockquote>

<hr>

<h2>Conclusion</h2>

<p>I'd like to end on something more philosophical.</p>

<p>There has been a huge amount of change in the tech world over the last 10 years and almost all companies (including the one I work for) are shifting their focus away from being content creators, to being storefront owners and algorithmic content curators. The new business model is this: instead of employing the best artists and creators you can find to make content, you instead pay your users to create the content (be that apps, games, videos, recipes, whatever), and that content is then evaluated and filtered algorithmically for other users - not based on some subjective evaluation of quality - but based on how much it engagement it generates.</p>

<p>The automated, engagement based nature of this process has created a race to the bottom. YouTube users are creating hundreds of <a href="https://en.wikipedia.org/wiki/Elsagate">brain-hacking Spiderman-Elsa videos</a> to farm views from undeveloped minds. Game developers are <a href="https://www.youtube.com/watch?v=E8Lhqri8tZk">auto-generating thousands of gambling games</a> for the Apple app store. Everything is being flooded with low-quality, engagement-hacking crap. The people who are actually trying to build quality content are being forced to sink or swim - optimize for engagement or else be forgotten.</p>

<p>There are many people involved in deep learning who are trying very hard to sell you the idea that in this new world of big-data, recommendation algorithms, large language models, and deep learning, quantity is the only thing that matters: more data, more GPUs, more papers, more students, more layers, more neurons, more users, more hours of engagement. Take a second to consider why it might be in the interests of the people shouting this the loudest, to convince everyone it is really true.</p>

<p>But ask almost anyone what they actually want in this world and they will say the opposite. They want less content, of higher quality.</p>

<p>And if that is what all of us want, then that is what we need to try and do: focus on producing quality content ourselves, and reward others when they do so too.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nixiesearch: Running Lucene over S3, and why we're building a new search engine (108 pts)]]></title>
            <link>https://nixiesearch.substack.com/p/nixiesearch-running-lucene-over-s3</link>
            <guid>41797041</guid>
            <pubDate>Thu, 10 Oct 2024 09:11:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nixiesearch.substack.com/p/nixiesearch-running-lucene-over-s3">https://nixiesearch.substack.com/p/nixiesearch-running-lucene-over-s3</a>, See on <a href="https://news.ycombinator.com/item?id=41797041">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><blockquote><p><a href="https://haystackconf.com/eu2024/" rel="nofollow ugc noopener">This is a long-read version of a Haystack EU24 conference talk by Roman Grebennikov. Check out the slides or watch the video [TODO] if you’re a Gen Z.</a></p></blockquote><p>If you’re running a large search engine deployment, you already have a personal list of things that can go wrong on a daily basis.&nbsp;</p><p>In this post we are going to:</p><ul><li><p>Complain about existing search engines ops complexity. I’m looking at you, Elasticsearch and SOLR.</p></li><li><p>Hypothesize about running Lucene-powered search in a stateless mode over S3 block store. Why do you even need a stateless search over a block store?</p></li><li><p>Introduce Nixiesearch, a new stateless search engine, and how we struggled to make it work nicely with S3. And how it ended with RAG, ML inference and hybrid search.</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png" width="793" height="752" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:752,&quot;width&quot;:793,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef5adcc1-e1e6-4969-97ed-ad424acd677c_793x752.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A prize wheel search engineers spin every day. Image by author.</figcaption></figure></div><p>Unlike regular back-end applications, search engines nowadays are considered special and require additional “like a database” handling. The prize wheel above summarises author’s personal incident experience with Elasticsearch, OpenSearch and SOLR — but other modern vector search engines such as Weaviate and Qdrant are not immune.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png" width="1456" height="820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:820,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83ce232d-8d03-4dce-990d-48f3186c55a6_1588x894.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Curse of stateful, blessing of stateless apps. Image by author.</figcaption></figure></div><p><span>Each of your search nodes is stateful and contains a tiny part of a distributed shared mutable index. And if you’ve ever read at least one book on systems design and engineering (such as the </span><a href="https://dataintensive.net/" rel="nofollow ugc noopener">“Designing Data Intensive Applications” by M. Kleppmann</a><span>), you’ll perfectly know that things aren’t going to be smooth sailing when there’s a “</span><strong>distributed shared mutable</strong><span>” in the name:</span></p><ul><li><p><strong>Each node depends on an attached storage</strong><span> — which makes cloud deployments much more fragile with EBS disks, PersistentVolume and PersistentVolumeClaims over DaemonSets.</span></p></li><li><p><span>To perform </span><strong>auto-scaling you need to redistribute state across nodes</strong><span>. Since scaling is usually triggered when the cluster is already under load, and the redistribution process also adds additional load — we usually just over-provision instead.</span></p></li></ul><p>Worse still, this internal state also spawns outside of the search engine itself, as the entire indexing pipeline is also tied to it.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png" width="1456" height="717" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d3480bd1-799e-4912-b409-32f3874d9443_1600x788.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:717,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3480bd1-799e-4912-b409-32f3874d9443_1600x788.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Indexing pipeline as a Rube Goldberg Machine. Image by author.</figcaption></figure></div><p>The idea of stateless search engines is not new, as they already exist in the wild:</p><ul><li><p><a href="https://github.com/Yelp/nrtsearch" rel="nofollow ugc noopener">Yelp NRTSearch</a><span>, built on top of </span><a href="https://blog.mikemccandless.com/2017/09/lucenes-near-real-time-segment-index.html" rel="nofollow ugc noopener">Lucene near-real-time segment replication</a><span>. Works over gRPC and supports only lexical search.</span></p></li><li><p><a href="https://opensearch.org/" rel="nofollow ugc noopener">OpenSearch</a><span> when configured for </span><a href="https://opensearch.org/docs/latest/tuning-your-cluster/availability-and-recovery/segment-replication/index/" rel="nofollow ugc noopener">segment replication</a><span> over </span><a href="https://opensearch.org/docs/latest/tuning-your-cluster/availability-and-recovery/remote-store/index/" rel="nofollow ugc noopener">remote-backed storage</a><span>. With this mode of operation, index is stored externally, but the cluster state is not.</span></p></li><li><p><a href="https://quickwit.io/" rel="nofollow ugc noopener">Quickwit</a><span> for log search over S3 storage. A great alternative to the ELK stack, but not for consumer-facing search.</span></p></li><li><p><a href="https://turbopuffer.com/" rel="nofollow ugc noopener">Turbopuffer</a><span> doing HNSW-like vector search over S3-based block storage. SaaS only, but written in Rust.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png" width="956" height="358" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dfad2822-1e58-4240-b71b-a04b606cb439_956x358.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:358,&quot;width&quot;:956,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdfad2822-1e58-4240-b71b-a04b606cb439_956x358.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Choosing between easy operations and costs. Image by author.</figcaption></figure></div><p>The main point of these search engines is to decouple computation (e.g. the actual search process) from the storage. There are also examples in the industry of large companies migrating to stateless search:</p><ul><li><p><a href="https://www.uber.com/en-NL/blog/lucene-version-upgrade/" rel="nofollow ugc noopener">Lucene: Uber’s Search Platform Version Upgrade.</a></p></li><li><p><a href="https://careers.doordash.com/blog/introducing-doordashs-in-house-search-engine/" rel="nofollow ugc noopener">Introducing DoorDash’s in-house search engine</a><span>.</span></p></li><li><p><span>Amazon: </span><a href="https://www.youtube.com/watch?v=EkkzSLstSAE" rel="nofollow ugc noopener">E-Commerce search at scale on Apache Lucene.</a></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif" width="1456" height="511" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:511,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05872472-5399-48cd-847b-566f0dc839fa_1600x562.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A DoorDash search engine indexing flow. Image taken from </span><a href="https://careers.doordash.com/blog/introducing-doordashs-in-house-search-engine/" rel="nofollow ugc noopener">[1]</a><span>.</span></figcaption></figure></div><p>With all these great in-house search engines being not available for general audience, what should we do?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png" width="1235" height="1235" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1235,&quot;width&quot;:1235,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccf8bb21-0265-46b9-8a7b-e1148e05043d_1235x1235.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Writing a search engine from scratch in 2024 might be considered risky. Image by author.</figcaption></figure></div><p><a href="https://github.com/nixiesearch/nixiesearch" rel="nofollow ugc noopener">Nixiesearch</a><span> is a Lucene-based search stateless search engine, inspired by the Uber, Doordash and Amazon designs from the articles above:</span></p><ul><li><p>Started as a proof-of-concept that Lucene could be run over the S3-compatible block storage.</p></li><li><p>Went further with many modern features implemented on top: hybrid search, RAG and ML inference.</p></li></ul><p><span>There have been a number of experiments back in the days to add S3 support to Lucene, starting from 15 years ago by</span><a href="https://www.elastic.co/de/blog/author/shay-banon" rel="nofollow ugc noopener"> Shay Banon</a><span> (a creator of Elasticsearch, and now a CEO of Elastic):</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png" width="1220" height="742" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:742,&quot;width&quot;:1220,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd519ee0c-53db-496c-8704-c726b0b0a22a_1220x742.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Lucene over AWS S3 in 2007. Screenshot taken from kimchy.github.com.</figcaption></figure></div><p>Lucene is has an IO abstraction called Directory — a high-level API to support almost any data access method:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png" width="795" height="319" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:319,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:55974,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F38a2d2eb-9a40-4d89-99df-c5afa174da0c_795x319.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Simplified Lucene Directory interface.</figcaption></figure></div><p><span>There are many Directory implementations available ranging from practical ones like </span><a href="https://lucene.apache.org/core/9_12_0/core/org/apache/lucene/store/MMapDirectory.html" rel="nofollow ugc noopener">MMapDirectory</a><span> and </span><a href="https://lucene.apache.org/core/9_12_0/core/org/apache/lucene/store/ByteBuffersDirectory.html" rel="nofollow ugc noopener">ByteBuffersDirectory</a><span>, to the much more experimental and obscure </span><a href="https://github.com/unkascrack/lucene-jdbcdirectory" rel="nofollow ugc noopener">JDBCDirectory</a><span>. And the Directory API maps quite nicely to the S3 API operations, so it’s no surprise that there’s already one brave soul who’s made the </span><a href="https://github.com/albogdano/lucene-s3directory" rel="nofollow ugc noopener">S3Directory</a><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png" width="1242" height="805" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:805,&quot;width&quot;:1242,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8026180-2a43-4266-9c20-8451ab85d28b_1242x805.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Experimental S3 Directory. A screenshot from </span><a href="https://github.com/albogdano/lucene-s3directory" rel="nofollow ugc noopener">[2]</a><span>.</span></figcaption></figure></div><p>Lucene itself is a synchronous library built on the main assumption that your index data is stored locally, but this is no longer the case with S3Directory.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png" width="1456" height="723" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:723,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F75b8cbea-82c8-435f-9169-66454dcbd9a2_1600x794.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>S3 access latency is almost 100x bigger. Image by author.</figcaption></figure></div><p><span>The S3Directory itself is implemented by naively mapping Directory calls directly to the S3 API without any intermediate caching, and its </span><a href="https://github.com/albogdano/lucene-s3directory?tab=readme-ov-file#performance" rel="nofollow ugc noopener">README</a><span> mentions that performance is not great:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png" width="866" height="271" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:271,&quot;width&quot;:866,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2dd398a-ea00-4227-bd1d-186ac3724ff7_866x271.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Integration test time for S3Directory. A screenshot from </span><a href="https://github.com/albogdano/lucene-s3directory" rel="nofollow ugc noopener">[2]</a><span>.</span></figcaption></figure></div><p>With 100x higher data access latency, it’s not a surprise to see 100x slower end-to-end times.&nbsp;</p><p>An obvious improvement to this approach is to introduce the read-through caching support — that’s what we did in the first version of Nixiesearch.</p><p>The main assumption behind the caching idea is that the Lucene index may have hot spots, and there is no reason to make an S3 API call for index blocks we have already read in the past:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png" width="1456" height="656" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/af33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:656,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf33f0c3-1674-4acd-85f1-a3c505c41c6c_1600x721.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Cached S3 Directory. Image by author.</figcaption></figure></div><p><span>With this approach, the S3Directory is implemented just as an ephemeral caching layer on top of the regular MMapDirectory, with the main benefit of making the whole </span><strong>search application stateless</strong><span>! But what’s the latency cost?</span></p><p>How slow is S3 in practice?</p><p>Today, S3 has become a protocol, and not just an AWS service, but being implemented by GCP, Minio and many otherproviders. The AWS S3 reference implementation has two API flavors:</p><ul><li><p>Traditional S3: geographically redundant, with higher latency.</p></li><li><p><a href="https://aws.amazon.com/s3/storage-classes/express-one-zone/" rel="nofollow ugc noopener">S3 Express</a><span>: limited to a single AZ, but with much lower latency.</span></p></li></ul><p><span>We implemented a small </span><a href="https://github.com/nixiesearch/s3bench/" rel="nofollow ugc noopener">nixiesearch/s3bench</a><span> tool to measure a first byte latency for </span><a href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html" rel="nofollow ugc noopener">S3 GetObject API</a><span> request across different endpoints:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png" width="1456" height="466" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:466,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5469d502-2ee5-4660-a6ab-8b10cdc42913_1512x484.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>S3 vs S3 express first byte latency. Image by author.</figcaption></figure></div><p>The main observation we made is that the first byte latency of S3 Express is actually only 5ms instead of 20ms for a traditional S3. And this latency is independent of block size.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png" width="692" height="427" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:427,&quot;width&quot;:692,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6061ba97-0b4c-412a-b7ea-caa948e5d3eb_692x427.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Last byte latency for different block sizes. Image by author.</figcaption></figure></div><p><span>For the last byte latency both curves for S3 and S3Express are almost parallel, so we can assume that the only difference between these two implementations is in much lower constant value of initial request latency, due </span><a href="https://quickwit.io/blog/s3-express-speculations" rel="nofollow ugc noopener">to S3Express using pre-signed access tokens</a><span> — so there is no need to perform a full IAM role evaluation on each request.</span></p><p>Synthetic micro-benchmark latencies are nice, but what are the real-life numbers we get when searching through a read-through block cache for S3Directory? Let’s do an experiment:</p><ul><li><p><span>Take a well-known dataset like </span><a href="https://huggingface.co/datasets/BeIR/msmarco" rel="nofollow ugc noopener">MSMARCO</a><span>, with sample sizes of 10k, 100k and 1M documents.</span></p></li><li><p><span>Implement a traditional </span><a href="https://lucene.apache.org/core/9_2_0/core/org/apache/lucene/search/KnnVectorQuery.html" rel="nofollow ugc noopener">HNSW “vector search”</a><span> index on top of S3-backed Lucene. We will use a </span><a href="https://huggingface.co/intfloat/e5-base-v2" rel="nofollow ugc noopener">Microsoft e5-base-v2</a><span> embedding model with 768 dimensions, and a single-segment index.</span></p></li><li><p><span>We will use the default Lucene HNSW index settings of </span><strong>M=16 (so the number of neighbors for each node in the graph — this number is going to hit us hard later)</strong><span> and efConstruction=100.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png" width="532" height="494" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:494,&quot;width&quot;:532,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89cf0e7f-c3ac-4e85-913d-79fa44541a05_532x494.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>First cold request latency for 3 index sizes over S3-backed Lucene. Image by author.</figcaption></figure></div><p>There are a lot of numbers in the table above, but here are two most surprising facts:</p><ul><li><p><span>For a small 10k docs dataset, the first request </span><strong>fetched almost 30% of the entire index</strong><span> while traversing the HNSW graph.</span></p></li><li><p><span>For a larger 1M docs index, we performed over 300 read operations to S3, resulting </span><strong>in a whopping 3 seconds of latency</strong><span>.</span></p></li></ul><p>What is going on here, why do we need so many reads to perform a simple nearest neighbors search?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png" width="1456" height="989" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:989,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22199c71-8edf-4c58-ab5f-8dfcda204c09_1512x1027.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A simplified HNSW graph traversal. Blue = neighbors, Red = jumps. Image by author.</figcaption></figure></div><p><span>Because Lucene uses an </span><a href="https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world" rel="nofollow ugc noopener">HNSW algorightm</a><span> to perform a nearest neighbors search, it needs to traverse a large multi-layer graph:</span></p><ul><li><p><span>Each layer of the HNSW graph contains </span><strong>links to neighbor nodes</strong><span>. Top layers have only long links, and bottom layers have only short links.</span></p></li><li><p><span>At each step of the graph traversal, we </span><strong>pick a node and evaluate a distance between a query and a neighbor </strong><span>— so we need to load this neighbor embedding from the storage.</span></p></li><li><p><span>Each such evaluation is effectively a </span><strong>random cold read from S3</strong><span>, adding yet another +10ms to the request processing latency.</span></p></li></ul><p>You may be wondering, “OK, but this is a first cold request, will it get better over time?”&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png" width="1456" height="466" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:466,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b3b06fa-1620-48c4-a673-7ea5bd99b135_1512x484.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>e2e latency for request #1-#32, and index read rate. Image by author.</figcaption></figure></div><p>Unfortunately we found out that the read distribution over the HNSW index still stays quite random, and the initial hypothesis that there are index hotspots is not true:</p><ul><li><p><span>e2e latency goes down from 3 sec to </span><strong>1 sec on request #32</strong><span>. Things are getting better, but 1 second search latency is still not acceptable for a consumer-facing search.</span></p></li><li><p>On request #32 we are already reading 30% of the entire index.</p></li></ul><p>So the idea of doing HNSW search over S3-backed block storage is nice in theory, but not very practical in reality, isn’t it?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png" width="500" height="348" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:348,&quot;width&quot;:500,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F382015c3-f5b9-4ce8-b6aa-f02b2f560f48_500x348.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A meme from 2005. Based on a drawing by </span><a href="https://www.facebook.com/strangedrawings" rel="nofollow ugc noopener">Borya_Spec</a><span>.</span></figcaption></figure></div><p><span>But it’s not all doom and gloom: the upcoming release of Lucene 10 includes the ongoing initiative </span><a href="https://github.com/apache/lucene/issues/13179" rel="nofollow ugc noopener">“Improve Lucene’s I/O Concurrency”</a><span>:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png" width="1133" height="827" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:827,&quot;width&quot;:1133,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef49f1e-e942-4656-ba35-48ce87b13deb_1133x827.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Lucene-13179 issue. A screenshot from </span><a href="https://github.com/apache/lucene" rel="nofollow ugc noopener">[3].</a></figcaption></figure></div><p>To summarize the idea behind this enhancement:</p><ul><li><p><span>Lucene is not concurrent inside: e.g. full of regular for-loops over data structures. Such loops are CPU efficient, but </span><strong>not always IO </strong><span>efficient</span><strong> because they’re sequential</strong><span>.</span></p></li><li><p><span>Let’s introduce an </span><strong>IndexInput.prefetch</strong><span> method: it hints to the underlying Directory implementation which parts of the data we are going to read really soon, so it can start prefetching in the background.</span></p></li></ul><p>This will dramatically improve the situation for the HNSW search over S3Directory:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png" width="1456" height="1110" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1110,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8cc11d6b-9191-42fd-ba4d-209e77c7fd7f_1600x1220.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Parallel S3 requests instead of sequential. Image by author.</figcaption></figure></div><p><span>This makes all neighbor lookups parallel instead of being sequential! Since Lucene uses M=16 as the default value for its HNSW implementation, then we need only </span><strong>N=num_layers</strong><span> hops to S3 (in practice in the range of 3–5) instead of </span><strong>N*M=num_layers*num_neighbors</strong><span>!</span></p><p>Since S3 has not-so-nice latency but almost unlimited concurrency, this can improve the situation a lot:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png" width="724" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:724,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3bcc179f-528f-4b88-a52e-64df97fd9b72_724x450.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>S3 concurrent throughput for M2/M5/C5 EC2 instances, based on data from</span><a href="https://github.com/dvassallo/s3-benchmark" rel="nofollow ugc noopener"> [4].</a></figcaption></figure></div><p>So in practice you can reach a 1.1 GB/s S3-EC2 throughput even on low-cost EC2 instances with just 16 concurrent requests, maxing out the default 10 Gbps network adapter.</p><p>For Nixiesearch the Lucene 10 approach looks promising and we plan to re-evaluate the numbers when the LUCENE-13179 will get implemented. But as for now, Nixiesearch is using a more traditional segment-based replication approach:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png" width="1365" height="573" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:573,&quot;width&quot;:1365,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F184ce8df-5d75-485e-a2dc-3d152c715fdd_1365x573.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Stateless index. Image by author.</figcaption></figure></div><p>Segment-based replication is not a new thing and has been available in NRTSearch and OpenSearch for years. But the search nodes are still not stateless: there is index schema and cluster metadata that you can access and modify.</p><p>If you come from regular back-end app development, a regular deployment flow looks like this:</p><ol><li><p>Commit to Git, make a PR.</p></li><li><p>PR gets reviewed, and later the CI/CD system takes care of rolling/blue-green deployment.</p></li></ol><p>In contrast to this approach, to change an index configuration, you need to:</p><ol><li><p>Send an HTTP POST request to a prod cluster. Pray.&nbsp;</p></li><li><p><span>Earth shakes, lights go on and off. You get a </span><strong>“hey wtf” </strong><span>Slack message from the CEO.</span></p></li></ol><p>Can the index management be more like we do in a regular backend development?</p><p><span>Nixiesearch has </span><strong>no API to change runtime configuration, nor API to create and modify indexes</strong><span>. Everything is defined in a static config file that the server loads on startup:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png" width="795" height="464" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:464,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:34848,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8b72801-a789-4aa1-a1a0-6ca121fb93b2_795x464.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>A simple Nixiesearch index definition.</figcaption></figure></div><p>In this snippet we:</p><ul><li><p><span>define a “</span><strong>helloworld</strong><span>” index with two “</span><strong>title</strong><span>” and “</span><strong>price</strong><span>” fields</span></p></li><li><p><span>A </span><strong>title</strong><span> field has a text type, intended for semantic search, and uses the “</span><strong>text</strong><span>” model to perform the embedding process.</span></p></li><li><p><span>A </span><strong>price</strong><span> field is numeric and can be filtered, sorted and faceted over.</span></p></li><li><p><span>We also define an </span><a href="https://huggingface.co/intfloat/e5-small-v2" rel="nofollow ugc noopener">e5-small-v2</a><span> ONNX model for ML inference.</span></p></li></ul><p>And after that we can send regular search queries:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png" width="795" height="429" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:429,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:20377,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2b0c160-59fb-4bee-9a6f-29c929811290_795x429.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>But how can we change index settings if it’s immutable?</p><p><span>Things are changed in Nixiesearch the same way as in traditional cloud deployment strategies by doing </span><strong>a rolling (or a blue-green) deployment</strong><span>. You never change the setup, you create a new one aside and gradually switch traffic when the new deployment reports an OK healthcheck.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png" width="1456" height="1026" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1026,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3e82789f-b22a-4bc4-ad55-d3d1f93ac368_1600x1128.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Blue-green deployment with Nixiesearch. Image by author.</figcaption></figure></div><p><span>Since search engine deployment depends not only on configuration, but also on an externally stored index in S3, Nixiesearch also validates if the </span><strong>configuration change is backward compatible</strong><span> and can use the same index.</span></p><ul><li><p>If the change is backward compatible (e.g. change in caching settings), then the service reports OK to the readiness probe and starts accepting traffic.</p></li><li><p><span>If the change is </span><strong>incompatible and requires reindexing</strong><span>, then the readiness probe is not going to be OK until you reindex.</span></p></li></ul><p>But since the entire index is just one directory on S3, reindexing is not as complicated as with traditional search engines.</p><p>Most of traditional search engines like Elastic/OpenSearch/Solr use push-based approach to indexing:</p><ul><li><p>Submit a batch of document updates to the HTTP REST API of the prod cluster,</p></li><li><p>The cluster starts crunching the batch and updates the distributed index.</p></li></ul><p><span>While this is a traditional approach, it still has a number of major drawbacks: you </span><strong>need to control the back-pressure</strong><span> (so the cluster won’t be overwhelmed with indexing) and </span><strong>the same nodes are used for searching and indexing</strong><span> (so your search latency will be affected).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png" width="794" height="575" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:575,&quot;width&quot;:794,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f3572b2-14c1-424a-aff5-ae4e12653b6c_794x575.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Do we really need to do the push based indexing in 2024? Image by author, based on </span><a href="https://de.wikipedia.org/wiki/Black_Panther_%28Film%29" rel="nofollow ugc noopener">Black Panther movie</a><span>.</span></figcaption></figure></div><p>But as the index is not anymore tied to the cluster, the whole reindexing process can happen offline:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif" width="1456" height="899" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:899,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede9281d-b09a-469d-9dfd-0d3b010e01d5_2631x1624.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Offline reindexing in Nixiesearch. Image by author.</figcaption></figure></div><p><span>With offline indexing, prior to starting the search cluster, you run a separate batch task to rebuild the index and </span><strong>publish it to S3 into a separate location</strong><span>. And then perform a rolling deployment as usual.</span></p><p>But does the design decision to have stateless searchers have any drawbacks?</p><p>Stateless searcher architecture also assumes that searchers don’t talk to each other, as otherwise they’re no longer stateless. And sharding is an example of a feature that cannot be easily implemented on top of a stateless architecture.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png" width="1321" height="1259" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1259,&quot;width&quot;:1321,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F658e8460-facc-46d4-9a83-17defa88cc37_1321x1259.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>No-sharding vs with-sharding architecture. Image by author.</figcaption></figure></div><p>This does not mean that there will be no sharding support in the future. We just accept the fact that we still do not yet know the best way to implement it without breaking the promise of remaining stateless.</p><p>From another practical perspective, in a consumer-facing search (e.g. e-commerce and enterprise doc search) you rarely have datasets so large that you cannot do without sharding:</p><ul><li><p><strong>A 1M docs MSMARCO index we used for testing is just 3GB on disk</strong><span>. With </span><strong>int8</strong><span> quantization it’ll be about 800MB.</span></p></li><li><p>Indexes of 1B/1T docs are usually seen in log/APM/trace search tasks, and Nixiesearch will definitely not going be the best solution for this.</p></li></ul><p>On the way to better ops, Nixiesearch tries to have as few dependencies as possible. The same goes for internal ML embedding and LLM models. Besides simpler deployment, this has also the following advantages:</p><ul><li><p><strong>Latency</strong><span>: a </span><a href="https://medium.com/nixiesearch/how-to-compute-llm-embeddings-3x-faster-with-model-quantization-25523d9b4ce5" rel="nofollow ugc noopener">CPU ONNX inference of the e5-base-v2 model is about 5 ms</a><span>. This is much faster than sending a network request to an external SaaS embedding API.</span></p></li><li><p><strong>Privacy</strong><span>: your private data (i.e. documents and queries) does not leave your secure perimeter.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png" width="640" height="627" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:627,&quot;width&quot;:640,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83e49da6-2e35-41c1-a1e3-119a0277453d_640x627.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>When your AI is just your prompt. Image by author.</figcaption></figure></div><p>As of v0.3.0, Nixiesearch supports the following model families:</p><ul><li><p><span>For a vector search, </span><strong>any ONNX-compatible sentence-transformers model should work</strong><span>. The model translation can be done either with </span><a href="https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model" rel="nofollow ugc noopener">Huggingface optimum-cli</a><span>, or with an in-house </span><a href="https://github.com/nixiesearch/onnx-convert" rel="nofollow ugc noopener">onnx-convert tool</a><span> (which can also perform model quantization).</span></p></li><li><p><span>For RAG,</span><strong><span> any GGUF model that works with </span><a href="https://github.com/ggerganov/llama.cpp" rel="nofollow ugc noopener">llama-cpp</a></strong><span> should be compatible.</span></p></li><li><p><span>Of course, you can optionally use SaaS-based embedding and completion models from </span><a href="https://cohere.com/" rel="nofollow ugc noopener">Cohere</a><span>, </span><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings" rel="nofollow ugc noopener">Google</a><span>, </span><a href="https://www.mixedbread.ai/" rel="nofollow ugc noopener">Mixedbread</a><span> and </span><a href="https://openai.com/" rel="nofollow ugc noopener">OpenAI</a><span>.</span></p></li></ul><p>Furthermore, both indexing and serving services can be run on the GPU:&nbsp;</p><pre><code>docker run --gpus=all nixiesearch/nixiesearch:0.3.3-amd64-gpu index </code></pre><p>With ONNX model support for both CPU and GPU inference, an obvious next step would be to implement reranking support:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png" width="1456" height="631" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:631,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F85e37c1c-c85b-4714-a105-d7f9e9744bc7_1600x693.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Modern search pipeline, as seen by author. Image by author.</figcaption></figure></div><p>Since reranking is part of the common retrieval pipeline, we can then:</p><ul><li><p><span>Retrieve the top-N (N=100) documents with hybrid search by combining lexical and semantic retrieval with </span><a href="https://www.nixiesearch.ai/features/search/#hybrid-search-with-reciprocal-rank-fusion" rel="nofollow ugc noopener">RRF</a><span> — Reciprocal Rank Fusion.</span></p></li><li><p><span>Rerank all found documents using a more advanced Cross-Encoder model, such as </span><a href="https://huggingface.co/BAAI/bge-reranker-v2-m3/tree/main" rel="nofollow ugc noopener">BAAI/bge-reranker-v2-m3</a><span>.</span></p></li><li><p>Summarize the top-M (M=10) most relevant documents wusing the RAG approach.</p></li></ul><p>Everything is done with local inference, no data left the searcher node.</p><p>A common pain point of existing search engines is the need for a complicated ML-driven indexing pipeline to compute custom embeddings, do text processing and so on. Can we automate most of the typical ones?</p><p>We see such a pipeline becoming less of a separate application, but more of a feature of the engine itself:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png" width="795" height="294" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a8123bdb-0533-4d68-825b-37382356c3b0_795x294.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:294,&quot;width&quot;:795,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:18623,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa8123bdb-0533-4d68-825b-37382356c3b0_795x294.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>The plan is to introduce a </span><strong>multi-modal search with CLIP-like models</strong><span> — but with the embedding and search process handled completely transparently by the engine itself.</span></p><p><span>We also plan to introduce an </span><strong>LLM-driven DSL for text transformation</strong><span>: so you can handle summarization and classification tasks (e.g. “does this doc belong to category A?”).</span></p><p>Yes, such an approach will require GPU for indexing, but since indexing can be a one-time task, it does not require a GPU powered node running for 24x365. And low-end GPU nodes are not that expensive these days: EC2 g4.large with Nvidia T40 GPU is ~$300/month.</p><p>Nixiesearch is an actively developed project with many rough edges. Some features may be missing. Docs are imperfect — but they do exist!&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png" width="741" height="334" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:334,&quot;width&quot;:741,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c7d5b95-c83f-4d6c-9591-f45fdd7a2d21_741x334.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>And there will definitely be breaking changes in both configuration and index format. But with every Github issue you submit, it’s going to be more and more stable.</p><p><span>For the current </span><a href="https://github.com/nixiesearch/nixiesearch/releases/tag/0.3.3" rel="nofollow ugc noopener">v0.3.3</a><span>, Nixiesearch can already do quite a lot of things:</span></p><ul><li><p>Segment based replication over the S3-compatible block storage.</p></li><li><p>Filters, facets, sorting and autocomplete.</p></li><li><p>Semantic and hybrid retrieval.</p></li><li><p>RAG and local ML inference.</p></li></ul><p>If you want to know more, here are the links:</p><ul><li><p><strong>Github</strong><span>: </span><a href="https://github.com/nixiesearch/nixiesearch" rel="nofollow ugc noopener">nixiesearch/nixiesearch</a><span> (only 78 stars so far at the moment of publication)</span></p></li><li><p><strong>Docs microsite</strong><span>: </span><a href="https://nixiesearch.ai/" rel="nofollow ugc noopener">nixiesearch.ai</a></p></li><li><p><strong>Slack</strong><span>: </span><a href="https://nixiesearch.ai/slack" rel="nofollow ugc noopener">nixiesearch.ai/slack</a></p></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mozilla fixes Firefox zero-day actively exploited in attacks (182 pts)]]></title>
            <link>https://www.bleepingcomputer.com/news/security/mozilla-fixes-firefox-zero-day-actively-exploited-in-attacks/</link>
            <guid>41796030</guid>
            <pubDate>Thu, 10 Oct 2024 06:11:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bleepingcomputer.com/news/security/mozilla-fixes-firefox-zero-day-actively-exploited-in-attacks/">https://www.bleepingcomputer.com/news/security/mozilla-fixes-firefox-zero-day-actively-exploited-in-attacks/</a>, See on <a href="https://news.ycombinator.com/item?id=41796030">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><img alt="Firefox" height="900" src="https://www.bleepstatic.com/content/hl-images/2021/10/06/Firefox_headpic.jpg" width="1600"></p>
<p>Mozilla has issued an emergency security update for the Firefox browser to address a critical use-after-free vulnerability that is currently exploited in attacks.</p>
<p>The vulnerability, tracked as CVE-2024-9680, and discovered by ESET researcher Damien Schaeffer, is a use-after-free in Animation timelines.</p>
<p>This type of flaw occurs when memory that has been freed is still used by the program, allowing malicious actors to add their own malicious data to the memory region to perform code execution.</p>
<p>Animation timelines, part of Firefox's Web Animations API, are a mechanism that controls and synchronizes animations on web pages.</p>
<p>"An attacker was able to achieve code execution in the content process by exploiting a use-after-free in Animation timelines," <a href="https://www.mozilla.org/en-US/security/advisories/mfsa2024-51/" target="_blank" rel="nofollow noopener">reads the security bulletin</a>.</p>
<p>"We have had reports of this vulnerability being exploited in the wild."</p>
<p>The vulnerability impacts the latest Firefox (standard release) and the extended support releases (ESR).</p>
<p>Fixes have been made available in the below versions, which users are recommended to upgrade to immediately:</p>
<ul><li>Firefox 131.0.2</li>
<li>Firefox ESR 115.16.1</li>
<li>Firefox ESR 128.3.1</li>
</ul><p>Given the active exploitation status for CVE-2024-9680 and the lack of any information on how people are targeted, upgrading to the latest versions is essential.</p>
<p>To upgrade to the latest version, launch Firefox and go to <strong>Settings -&gt; Help -&gt; About Firefox</strong>, and the update should start automatically. A restart of the program will be required for the changes to apply.</p>
<div>
<figure><img alt="Updating Firefox" height="404" src="https://www.bleepstatic.com/images/news/u/1220909/2024/Phishing/22/update.png" width="736"><figcaption><strong>Updating Firefox</strong><br><em>Source: BleepingComputer</em></figcaption></figure></div>
<p>BleepingComputer has contacted both Mozilla and ESET to learn more about the vulnerability, how it's being exploited, and against whom, and we will update this post when we receive more information.</p>
<p>Throughout 2024, so far, Mozilla had to fix zero-day vulnerabilities on Firefox <a href="https://www.bleepingcomputer.com/news/security/mozilla-fixes-two-firefox-zero-day-bugs-exploited-at-pwn2own/" target="_blank">only once</a>.</p>
<p>On March 22, the internet company released security updates to address CVE-2024-29943 and CVE-2024-29944, both critical-severity issues discovered and demonstrated by Manfred Paul during the Pwn2Own Vancouver 2024 hacking competition.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WASM Is the New CGI (239 pts)]]></title>
            <link>https://roborooter.com/post/wasm-is-the-new-cgi/</link>
            <guid>41795561</guid>
            <pubDate>Thu, 10 Oct 2024 04:38:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roborooter.com/post/wasm-is-the-new-cgi/">https://roborooter.com/post/wasm-is-the-new-cgi/</a>, See on <a href="https://news.ycombinator.com/item?id=41795561">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I read a wonderful twitter thead about <a href="https://twitter.com/progrium/status/1337074333018189824">CGI and the birth of the web</a> and this triggered a thought I've been kicking around.</p>
<blockquote>
<p>Wasm is the new CGI</p>
</blockquote>
<p>And to be clear I don't mean the <a href="https://en.wikipedia.org/wiki/Common_Gateway_Interface">Common Gateway Interface</a> as a protocol. I mean what CGI and the <code>cgi-bin</code> application model brought to the web. They allowed people to easily write code that makes websites interactive. This shifted the web from an archive of documents to a vast network of applications. It was the first "web application model". I think Wasm (<a href="https://webassembly.org/">WebAssembly</a>) is setup to bring the next "web application model" to the industry.</p>
<p>Every shift in common web application models that I've observed over the last 20 years has been towards one goal; High performance applications that are easier to build and maintain. I almost titled this essay as "Wasm is the new serverless" but "serverless" is just the current hot iteration of web application models and not the goal.</p>
<p>I'm going to give a history of web application models and then I'm going to talk about what Wasm brings to the table. I'm seeing a convergence of Wasm related technologies maturing that I think will change some of the fundamental constraints of web application development today. When you change the constraints in a system you enable things that were impossible before.</p>
<h2>From CGI to Serverless</h2>
<p>There was a time (and honestly still today) where you could throw a script or any executable in a folder named <code>cgi-bin</code> and visit it at a url. The web server would execute the program on demand and return the output of the program to your web browser. This is roughly how the dynamic web worked for it's first decade. Make a request, run a process to respond to the request. It was slow by today's standards as starting a new process (and in some cases parsing scripts too) is a lot of work.</p>
<p><span><img alt="The official CGI logo from the spec announcement" loading="lazy" width="650" height="600" decoding="async" data-nimg="1" src="https://roborooter.com/assets/Common_Gateway_Interface_logo.svg"><figcaption>The official CGI logo from the spec announcement, yes really</figcaption></span></p><p><a href="https://en.wikipedia.org/wiki/FastCGI"><code>FastCGI</code></a> was developed as a response to the performance problems. It was a new web application model where a long lived processes would listen for CGI requests. Your web server would talk to one or more of these processes instead of invoking a new one for every request. It has its own follies as a lot of applications would be naively ported from the previous model and would leak resources like a sieve. They were built to have their processes terminated after each request and now had to stay alive for a long time. This lead to a long period of transition where languages and frameworks adjusted to the new reality.</p>
<p><span><img alt="FastCGI" loading="lazy" width="650" height="600" decoding="async" data-nimg="1" srcset="https://roborooter.com/_next/image/?url=%2Fassets%2Ffastcgi.jpg&amp;w=750&amp;q=75 1x, https://roborooter.com/_next/image/?url=%2Fassets%2Ffastcgi.jpg&amp;w=1920&amp;q=75 2x" src="https://roborooter.com/_next/image/?url=%2Fassets%2Ffastcgi.jpg&amp;w=1920&amp;q=75"><figcaption>FastCGI</figcaption></span></p><p>During this time language based web servers would emerge as a convention. (Possibly inspired by the Apache Tomcat era of Java application servers.) These applications would be built around a request/response model, and have different web servers available for different execution strategies. They were usually designed to go behind a "battle tested" feature rich web server like Apache or nginx that would isolate the application from slow requests and HTTP details.</p>
<p>The general application models were centered around process management with regards to requests. Some of these servers would fork the process on every request, some would use OS or language based threads, others would use an event or reactor models.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Rack_(web_server_interface)">Rack</a> web server interface from the Ruby community eventually made into python via the Flask application server and the <a href="https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface"><code>WSGI</code></a> specification. To simplify the specification, you receive an HTTP method, a headers hashmap, and a string or stream of input bytes. In response you send a status code, a headers object and a string or stream of response bytes.</p>
<p>With all these approaches you have to manage the number of physical or virtual servers you run. With physical servers application servers would often get slow when traffic was high, or have lots of computers sitting idle when traffic is low. With the rise of cloud computing, <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html">autoscaling</a> could allow a number of application servers to be based upon CPU or memory load or even the time of day. This allowed you to adapt to changes in traffic over time, turning on machines when you needed them and off when you didn't. Scaling up new computers can take 2-20 minutes depending on many factors of your application and configuration. Additionally it's dependant on the resources available in your cloud hosting region.</p>
<p>The introduction of "Serverless compute" with <a href="https://docs.aws.amazon.com/lambda/latest/dg/welcome.html">Amazon Lambda</a> changed the game. Instead of managing servers you now managed "functions". When paired with <a href="https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html">API Gateway</a> you now had a web server that would guarantee a single processes, isolated CPU and isolated memory for every request. Processes might be reused for up to a couple of hours but would be suspended or destroyed when not in use. This approach removes the concept of servers from the application management and allows AWS to scale up and down based upon request volume in seconds.</p>
<p><span><img alt="AWS Lambda Logo" loading="lazy" width="650" height="600" decoding="async" data-nimg="1" src="https://roborooter.com/assets/lambda.svg"><figcaption>Amazon started the serverless age of compute with Lambda</figcaption></span></p><p>This of course has its tradeoffs. New processes as we know from CGI are expensive, which lead to a "cold start" that some requests observe while scaling up concurrent requests. Each platform has different strategies to mitigate this penalty to varying success. Additionally since processes can be suspended after a response maintaining persistent tcp connections between requests can be troublesome. In practice you can tune database or cache server connections to stay alive (high timeouts on the server, low timeouts on the client) but you need to be more tolerant of reconnecting to external services, or simply reconnect for every request. HTTP based database APIs (see Azure or DynamoDB) are popular in the serverless application model as they tolerate massive numbers of connections and are easier to scale up and down with functions than traditional RDBMs solutions.</p>
<p>Another tradeoff is the dedicated CPU and memory that you get with each request. Some workloads thrive in this model, you don't have to manage these resources if they're guaranteed. And you might lower costs and mitigate scaling concerns. Other workloads perform horribly in this model as the CPU and memory go to waste as a single process could be leveraged for a considerable number of requests, or possibly the shared memory of a server model allows for batching or caching that make processing significantly more performant.</p>
<p>Anecdotally, I've both brought costs down 90% by moving a CMS based web application to a serverless model, and reduced costs 90% moving an event analytics based service to a server based model.</p>
<p>There are many variations of "Severless" such as <a href="https://cloud.google.com/run/">Google Cloud Run</a> or <a href="https://cloud.google.com/functions?hl=en">Google Cloud Functions</a> which lets you have a single process to take any number of concurrent requests. They model this with a Docker container but the tradeoffs are mostly the same. (Lambda these days now has docker support too but traditionally supported a zip file with an executable or script.)</p>
<p>Lastly, the Rack and WSGI specifications heavily influenced the request/response models we see in serverless environments today. With the request and response tuples becoming function or api signatures. Initially most function services did not provide streaming requests or responses (not to mention websockets or Server Side Events) but this has started to change and is worth exploring as application frameworks are starting to demand it.</p>
<h2>Wasm on the Server</h2>
<p>With that context, you might ask;</p>
<blockquote>
<p>Why on earth are we talking about Wasm? Isn't it for the browser?</p>
</blockquote>
<p>And I really hope even my mention of that question becomes dated, but I still hear this question quite often so it's worth talking about. Wasm was initially developed to run high perfromant code in the web browser. There's a history that traces to <a href="http://asmjs.org/"><code>asm.js</code></a> and other attempts to get code to run really fast in the browser. I'll let <a href="https://webassembly.org/">webassembly.org</a> speak for itself.</p>
<blockquote>
<p>WebAssembly (abbreviated Wasm) is a binary instruction format for a stack-based virtual machine. Wasm is designed as a portable compilation target for programming languages, enabling deployment on the web for client and server applications. The Wasm stack machine is designed to be encoded in a size- and load-time-efficient binary format. WebAssembly aims to execute at native speed by taking advantage of common hardware capabilities available on a wide range of platforms</p>
</blockquote>
<p>What we have today is the ability to compile many languages to Wasm instructions that can be run in both the browser and the server. While running CPU intensive processes in the browser (like <a href="https://silentspacemarine.com/">Doom</a>) is valuable the isolation and security model that the browser demanded is incredible for server side applications. It's now possible to have a significantly lighter weight form of isolation for untrusted code than a VM or docker container.</p>
<p><span><img alt="Web Assembly Logo" loading="lazy" width="650" height="600" decoding="async" data-nimg="1" src="https://roborooter.com/assets/web-assembly-logo.svg"><figcaption>Web Assembly Logo</figcaption></span></p><p>Additionally since V8 based serverless environments (like Node.js, Cloudflare Workers and Deno) are common, we already have some very mature Wasm execution capabilities thanks to the work in the browser. Wasm native environments are few (Fastly, Shopify and Suborbital) I think we'll see many more in the coming years thanks to advances in tooling.</p>
<p>If a Wasm module is instructions for a virtual machine, then you need a virtual machine to execute these instructions. This comes in the form of "runtimes" that will take generic Wasm, compile it for your local architecture, and provide it an execution environment. Some of these environments <a href="https://github.com/bytecodealliance/wasmtime/blob/main/docs/WASI-tutorial.md">look like POSIX APIs</a> that you'd find on any linux system, and some will do nothing but provide specific functions from the "host" system and allow you to execute exported functions in the module itself. These runtimes are available via libraries in many languages and run in process (or threads or anywhere you like).</p>
<p>Regardless of your runtime, WebAssembly programs are organized into <a href="https://webassembly.github.io/spec/core/syntax/modules.html">modules</a> and the VM running the module is called a "host".</p>
<blockquote>
<p>Modules are the unit of deployment, loading, and compilation. A module collects definitions for types, functions, tables, memories, and globals. In addition, it can declare imports and exports and provide initialization in the form of data and element segments, or a start function.</p>
</blockquote>
<p>"Memories" are represented as a uninterrupted contiguous array of bytes; which are allocated by the host at instantiation time, giving each guest module memory isolation. They function as the RAM for your virtual machine. You can provide them empty or pre-fill them with data segments. One of the many effect of how modules are isolated is that you can "pause" a module, and save its memory as a data segment. A similar concept to a Snapshot of a virtual machine. You can then start as many copies of the paused module as you like. (As I tell friends, it's like saving your game in an emulator.)</p>
<p>The snapshotted module has no extra startup time. The leading utility to perform this is called <a href="https://github.com/bytecodealliance/wizer">Wizer</a> and describes the process like so;</p>
<blockquote>
<p>First we instantiate the input Wasm module with Wasmtime and run the initialization function. Then we record the Wasm instance's state: What are the values of its globals? What regions of memory are non-zero?
Then we rewrite the Wasm binary by intializing its globals directly to their recorded state, and removing the module's old data segments and replacing them with data segments for each of the non-zero regions of memory we recorded.</p>
</blockquote>
<p>If we go back to thinking about our Application Server models; this allows us to have a fresh process but without paying the startup costs of a new process. Essentially giving us CGI without the downsides of CGI. Or in more recent terms, serverless without cold starts. This is how Wasm is the new CGI.</p>
<h2>Tradeoffs of Wasm</h2>
<p>Like any tech choice, it does have some tradeoffs.</p>
<p>Threads are not a native construct, this forces any blocking operation to host methods. This could mean the host handles the bulk of IO operations, providing wrappers for reading and writing to files or network interfaces, pausing the module as convenient or providing callback handlers. It is possible to build a reactor model (eg, tokio, nodejs) or a blocking model for your application. Until the <a href="https://webassembly.github.io/threads/core/">thread proposal</a> lands the constraint of not being able to have threads moves a significant amount of this design to your execution environment.</p>
<p><span><img alt="Threading" loading="lazy" width="650" height="600" decoding="async" data-nimg="1" src="https://roborooter.com/assets/Multithreaded_process.svg"><figcaption>A process with two threads of execution, running on one processor from <a href="https://en.wikipedia.org/wiki/Thread_(computing)#Multithreading">Wikipedia</a></figcaption></span></p><p>Just in Time (JIT) compilation is not possible as dynamic Wasm code generation is not allowed for security reasons. In fact, code itself is not addressable at runtime, which required for traditional JIT compilers as they generate code to replace "hot paths" of an interpreted script. This means systems like V8 and CRuby (and many other scripting environments) which rely on a JIT compiler for performance aren't able to run in a Wasm VM or have to abandon their JIT. There are alternative approaches that borrow from JIT. For example a "pre-jit" build step that outputs an optimized runtime for your script as been proposed. But they are not in wide use yet.</p>
<p>Since Wasm runs in a VM there is a simple interface between a module and its host; Memory. As a result moving data between a Wasm module and its host may require a copy. It is possible to share chunks of memory but depending on how the runtime models memory this may or may not be possible or recommended. (<a href="https://docs.rs/wasmtime/0.21.0/wasmtime/struct.Memory.html">Wasmtime in rust</a> for example uses a vector of bytes which may change out from under you.) As far as I can tell with most run times you cannot have zero copy communication between a module and IO operations. This means streaming data into and out of a Wasm module may be slower than doing it on the host.</p>
<p>Wasm VMs however do provide a much higher level of control. Some runtimes can enforce CPU limits by counting CPU instructions (see the <a href="https://docs.rs/wasmtime/0.31.0/wasmtime/struct.Config.html#method.consume_fuel">Wasmtime fuel concept</a>) which is really cool. All of the VMs are able to limit memory and wall clock time. So if you're looking to control usage limits, it's rather trivial to do.</p>
<p>Upcoming features to Wasm such as <a href="https://github.com/WebAssembly/interface-types/blob/main/proposals/interface-types/Explainer.md">Interface Types</a> (usable today but not a ratified standard) and module linking (functioning prototypes but no standard approach) help reduce module size and improve IO speed and ergonomics. But they are not standard yet. Wizer currently can interfere with module linking, and while there are custom approaches to solving that problem, there isn't a clear winner. Interface Types provide langue agnostic objects that can move through the Wasm memory boundary without expensive encoding and decoding, today a common approach is to copy JSON into and out of memory.</p>
<p>I've hand waved over security but by default <a href="https://webassembly.github.io/spec/core/intro/introduction.html#security-considerations">Wasm modules only have access to what they're given</a>. It's generally safe to run untrusted Wasm code and the surface area of the VM is quite small. This is not the case with Docker or other isolation models. Timing attacks are possible since the VM will be running translated instructions on the host's hardware, but there are mitigations for such attacks. It is also possible to compile Wasm to a native binary, the surface for attack there is much higher, but is safer than running untrusted native code. (I'm not up on the tradeoffs of this approach.)</p>
<h2>The Future</h2>
<p>Already we're seeing the rise of Wasm execution environments. As they (and their dev tools) get more popular, they will drive scripting languages to have Wasm runtimes and "Wizer like" preboots. In theory your application could be faster to run in snapshotted copy of your app that resumes on each request than with other models available today. Even on our own computers we could in theory take a CLI written in ruby and ship it in a snapshotted Wasm module that links to a Wasm ruby runtime and have it startup nearly as fast as c++ utility and ensure it only ever operates within the confines of a project directory.</p>
<p>Right now I'm seeing enhancements to our existing models and new platforms that are experimenting with what's possible.</p>
<p><span><img alt="Jacquard machine" loading="lazy" width="650" height="600" decoding="async" data-nimg="1" srcset="https://roborooter.com/_next/image/?url=%2Fassets%2FDeutsches_Technikmuseum_Berlin_February_2008_0013.jpg&amp;w=750&amp;q=75 1x, https://roborooter.com/_next/image/?url=%2Fassets%2FDeutsches_Technikmuseum_Berlin_February_2008_0013.jpg&amp;w=1920&amp;q=75 2x" src="https://roborooter.com/_next/image/?url=%2Fassets%2FDeutsches_Technikmuseum_Berlin_February_2008_0013.jpg&amp;w=1920&amp;q=75"><figcaption>If we go way back this was your application server. The Jacquard machine (from <a href="https://commons.wikimedia.org/w/index.php?curid=3634542">Wikipedia</a>)</figcaption></span></p><p>The first major enhancement I'm observing is moving "functions" to the "edge" allowing for compute near your users instead of near your database. Edge functions are a new primitive that application frameworks can take advantage of. (<a href="https://vercel.com/features/edge-functions">Vercel Edge Functions</a> being the one I'm working on - which is v8 based but shares many principals.) For example <a href="https://next-auth.js.org/configuration/nextjs#middleware">next-auth</a> can control access to pre rendered pages based upon JWT login tokens. You're able to get dynamic personalized content that is composed of data cached at the CDN.</p>
<p>Another enhancement to existing models (and maybe a natural experiment of a new model) is replacing processes based functions with Wasm based functions for serverless applications. <a href="https://suborbital.dev/">Suborbital</a> (which allows you to build your own Wasm based function execution environment) handles function execution but also allows for innovative chaining of Wasm functions into workflows. Most execution platforms seem to encourage a single module per request ignoring the ability to share memory or quickly invoke multiple modules (even from different languages!). As interface types become standard (very soon as they're part of the Wasm 2.0 spec) I expect to see a "middleware" data model (similar to Rack?) emerge.</p>
<p>It's worth noting an interesting confluence of technologies is that you are able to execute Wasm inside a Lambda function. You're able to use a snapshotted version of your app on existing infrastructure. While it is an interesting mix of technologies that shouldn't be ignored, I imagine it will disappear eventually.</p>
<p>I don't know what the next web application model looks like but I think we're at a tipping point. Wasm improves performance, makes process level security much easier, and lowers the cost of building and executing serverless functions. It can run almost any language and with module linking and interface types it lowers the latency between functions incredibly. When you change the constraints in a system you enable things that were impossible before. All this is very exciting to me and I'm quite eager to help us find out where it goes.</p></div></div>]]></description>
        </item>
    </channel>
</rss>