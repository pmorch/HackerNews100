<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 12 Apr 2024 09:00:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A 30-year old Rabbit Telepoint base station at Seven Sisters tube station (122 pts)]]></title>
            <link>https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/</link>
            <guid>40009856</guid>
            <pubDate>Fri, 12 Apr 2024 06:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/">https://www.ianvisits.co.uk/articles/theres-a-dead-rabbit-in-seven-sisters-tube-station-71502/</a>, See on <a href="https://news.ycombinator.com/item?id=40009856">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<!-- Article Start -->
					
<p>For over thirty years, a dead rabbit has hung inside Seven Sisters tube station, and thousands of people walk past it every day without noticing.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg.webp 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg.webp 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg.webp 1800w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img fetchpriority="high" decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg" alt="" width="605" height="336" loading="none" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg 1800w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>The dead rabbit is a legacy of an early form of mobile phone technology, albeit one that lasted less than two years before closing down.</p>
<p>We need to jump back to 1989, when the government awarded four licenses to operate Telepoint services, with the aim that their lower costs would offer competition to the country’s two mobile networks – Cellnet (now O2) and Vodafone.</p>
<p>At a time when the two mobile networks had 500,000 customers between them, it was <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0000540%2F19890127%2F238%2F0015">expected</a> these Telepoint phones would have as many as seven million customers by the middle of the 1990s.</p>
<p>Rabbit was created by the Hong-Kong based conglomerate, Hutchison, which didn’t have a license to operate a telepoint service, so they bought one of the four companies that did. However, by the time the company was ready to launch Rabbit to the world, the other three companies — Mercury Callpoint, Ferranti’s Zonephone and BT’s Phonepoint — had already <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0003740%2F19911003%2F011%2F0011">closed down</a>.</p>
<p>Rabbit pushed on and, with a huge marketing blitz, finally started appearing in the shops in May 1992.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg.webp 611w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-358x600.jpg.webp 358w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-768x1288.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-916x1536.jpg.webp 916w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-100x168.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-150x252.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-200x335.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-300x503.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-450x755.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-600x1006.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-900x1509.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg.webp 1052w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg" alt="" width="605" height="1014" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-611x1024.jpg 611w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-358x600.jpg 358w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-768x1288.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-916x1536.jpg 916w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-100x168.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-150x252.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-200x335.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-300x503.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-450x755.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-600x1006.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02-900x1509.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-02.jpg 1052w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>Despite the fact that it could only make calls within 100 yards of a base station and couldn’t receive calls (you could get a pager telling you to call someone) — in theory, it should have been a success.</p>


<p>At the time, the two mobile networks were for business people and <a href="https://en.wikipedia.org/wiki/Yuppie">Yuppies</a>, and were very expensive to use, so even the restrictive Rabbit phones would have an edge as they were much cheaper to make calls on – typically 50p a minute on a basic cellphone tariff vs 20p a minute on Rabbit. And cheaper monthly rentals.</p>
<p>The other factor is that the Rabbit phones could also be used at home.</p>
<p>Many homes had a <a href="https://www.britishtelephones.com/cordles3.htm">cordless phone</a>, but these domestic analogue radio-based cordless phones were large and frankly pretty poor quality, with calls often dropping out. The Rabbit phones came with a home base station and used early digital phone technologies, so the sound quality was considerably better. And the handset was much smaller than the home cordless phone.</p>
<p>So, when Rabbit hopped into the shops in May 1992, sales were expected to be brisk, and indeed, they were. Initially.</p>
<p>I worked in a shop selling them, and from memory the mobile calling function was only about half the appeal — it was the substantially better home phone function that really appealed to people when they heard about it from friends.</p>
<p>Rabbit launched with base stations on shop fronts and lamposts across Manchester and quickly spread, reaching nationwide coverage by late 1993.</p>
<p>Then it abruptly closed down.</p>
<p>On a day to remember, Friday 5th November, Hutchison Telecom suddenly <a href="https://www.awin1.com/cread.php?awinmid=5895&amp;awinaffid=249893&amp;clickref=rabbit&amp;clickref2=article&amp;ued=https%3A%2F%2Fwww.britishnewspaperarchive.co.uk%2Fviewer%2Fbl%2F0005278%2F19931107%2F714%2F0050">announced</a> that it was shutting down the Rabbit service. Hutchison Telecom had recently been granted a license to build a full mobile network and wasn’t interested in its old Telepoint based service anymore.</p>


<p>Customers who owned a Rabbit phone were given a refund, and a promise of a discount on Hutchison’s replacement, a GSM based mobile phone network — Orange (now EE).</p>
<p>The bunny had flopped. The future was Orange.</p>
<p>The announcement that Rabbit was closing down sparked something totally unexpected — a surge in sales.</p>
<p>Remember how I mentioned that the phones worked at home as a replacement for the clunky cordless phone — well, that wasn’t dependent on the Rabbit base stations, so the handsets would keep on working in the home after the mobile network was shut down.</p>
<p>And people really liked the Rabbit phone for their homes.</p>
<p>Awkwardly, customers offered a refund had to return the handsets, and many chose not to, while shops were told to return unsold stock to the warehouses, and many chose not to.</p>
<p>Hutchison’s attempt to kill the rabbit dead was struggling for life.</p>
<p>In the end, shops ran out of rabbit stock, and in December 1993, the Rabbit base stations switched off forever.</p>
<p>However, like many items of street furniture, what is installed in haste can take ages to remove at leisure. Unless the location is needed for something else, old signs and clutter can linger on for years, decades even.</p>
<p>So, inside <a href="https://www.ianvisits.co.uk/articles/tag/seven-sisters-station/">Seven Sisters tube station</a>, there’s still a Rabbit base station sitting on the wall, more than 30 years after it last broadcast a radio signal.</p>
<p>If you want to find the dead rabbit, just hop down to Seven Sisters tube station’s High Road entrance ticket hall. You can find it right next to the escalators to the Victoria line.</p>
<p><a href="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg"><picture><source srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg.webp 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg.webp 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg.webp 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg.webp 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg.webp 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg.webp 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg.webp 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg.webp 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg.webp 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg.webp 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg.webp 1800w" sizes="(max-width: 605px) 100vw, 605px" type="image/webp"><img decoding="async" src="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg" alt="" width="605" height="336" srcset="https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1024x569.jpg 1024w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-600x333.jpg 600w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-768x427.jpg 768w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-1536x853.jpg 1536w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-100x56.jpg 100w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-150x83.jpg 150w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-200x111.jpg 200w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-300x167.jpg 300w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-450x250.jpg 450w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station-900x500.jpg 900w, https://www.ianvisits.co.uk/articles/wp-content/uploads/2024/04/rabbit-seven-sisters-station.jpg 1800w" sizes="(max-width: 605px) 100vw, 605px" data-eio="p"></picture></a></p>
<p>There’s also another one, in <a href="https://twitter.com/ianvisits/status/1695383779659948112">rather better condition</a>, <span>in the waiting room on platform 7/8 at Watford Junction station.</span></p>




										


								
					



					

									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a Linux Container Using Namespaces: Part – 1 (2020) (134 pts)]]></title>
            <link>https://www.polarsparc.com/xhtml/Containers-1.html</link>
            <guid>40008841</guid>
            <pubDate>Fri, 12 Apr 2024 02:22:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.polarsparc.com/xhtml/Containers-1.html">https://www.polarsparc.com/xhtml/Containers-1.html</a>, See on <a href="https://news.ycombinator.com/item?id=40008841">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <br>
    
    
    <p>Building a Linux Container using Namespaces :: Part - 1</p>
    <br>
    
    <hr> 
    <p>Overview</p>
    <div id="para-div">
      <p>Ever wondered how Linux <span>Container</span>s worked ???</p>
      <p>Currently, <a href="https://www.polarsparc.com/xhtml/Docker.html" target="_blank"><span>Docker</span></a>
        is one of the most popular and prevalent container implementations.</p>
      <p>Containers run on top of the same Operating System kernel, but isolate the application processes running inside them
        from one another. One of the secret sauces behind containers is <a href="http://man7.org/linux/man-pages/man7/namespaces.7.html" target="_blank"><span>Namespaces</span></a>.</p>
      <p>A <span>Namespace</span> abstracts global system resources, such as, host names, user IDs, group IDs,
        process IDs, network ports, etc., in a way that it appears to the processes (within the namespace) as though they have
        their own isolated instance of the global system resources. One of the primary goals of namespaces is to support the
        implementation of containers (lightweight virtualization).</p>
      <p>Currently, in Linux there are <span>6</span> types of namespaces - <span>IPC</span>,
        <span>Network</span>, <span>Mount</span>, <span>PID</span>,
        <span>User</span>, and <span>UTS</span>.</p>
    </div>
    <div id="para-div">
      <p>The following are brief descriptions for each of the namespaces:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span>IPC</span> :: This namespace isolates certain interprocess communication (IPC) resources,
            namely, Message Queues, Semaphores, and Shared Memory</p>
        </li>
        <li>
          <p><span>Network</span> :: This namespace provides isolation of the system resources associated with
            networking, such as, Network devices, IP addresses, IP routing tables, /proc/net directory, port numbers, and so on</p>
        </li>
        <li>
          <p><span>Mount</span> :: This namespace isolates the set of filesystem mount points seen by a group
            of processes. Processes in different mount namespaces can have different views of the filesystem hierarchy</p>
        </li>
        <li>
          <p><span>PID</span> :: This namespace isolates the process ID number space. This allows processes
            in different PID namespaces to have the same PID</p>
        </li>
        <li>
          <p><span>User</span> :: This namespace isolates the user and group ID number spaces, such that, a
            process's user and group IDs can be different inside and outside the user namespace</p>
        </li>
        <li>
          <p><span>UTS</span> :: This namespace isolates two system identifiers — the hostname and the
            domainname. For containers, the UTS namespaces allows each container to have its own hostname and NIS domain name</p>
        </li>
      </ul>
      <p>For the demonstration in this article, we will be using the <span>unshare</span> Linux command
          as well as implement, build, and execute a simple container using <span>golang</span>.</p>
    </div>
    <p>Installation and Setup</p>
    <p>The installation is on a <span>Ubuntu 18.04 LTS</span> based Linux desktop.</p>
    <div id="para-div">
      <p>We will need two commands <span>newuidmap</span> and <span>newgidmap</span>
        to demonstrate <span>User</span> namespace. For this, we need to install the package
        <span>uidmap</span>.</p>
      <p>To install the package <span>uidmap</span>, execute the following command:</p>
    </div>
    <p>$ sudo apt install -y uidmap</p>
    <div id="para-div">
      <p>Next, we will need the <span>brctl</span> command to create a <span>bridge</span>
        network interface. For this, we need to install the package <span>bridge-utils</span>.</p>
      <p>To install the package <span>bridge-utils</span>, execute the following command:</p>
    </div>
    <p>$ sudo apt install -y bridge-utils</p>
    <div id="para-div">
      <p>To develop, build, and execute the simple container in <span>go</span> programming language, we
        need to install the <span>golang</span> package.</p>
      <p>To check the version of <span>golang</span> available to install, execute the following command:</p>
    </div>
    <p>$ sudo apt-cache policy golang</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>golang:
  Installed: (none)
  Candidate: 2:1.13~1ubuntu1ppa1~bionic
  Version table:
 *** 2:1.13~1ubuntu1ppa1~bionic 500
        500 http://ppa.launchpad.net/hnakamur/golang-1.13/ubuntu bionic/main amd64 Packages
        500 http://ppa.launchpad.net/hnakamur/golang-1.13/ubuntu bionic/main i386 Packages
        100 /var/lib/dpkg/status
     2:1.10~4ubuntu1 500
        500 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages</pre>
    </div>
    <p>To install <span>golang</span>, execute the following command:</p>
    <p>$ sudo apt install -y golang</p>
    <p>The above installation procedure installs <span>golang</span> from the official Ubuntu repository.</p>
    <p>Create a directory for developing, building, and running <span>go</span> programs by executing the
        following commands:</p>
    <div id="cmd-div">
      <p>$ mkdir $HOME/projects/go</p>
      <p>$ export GOPATH=$HOME/projects/go</p>
    </div>
    
    <p>We will need one of the popular <span>go</span> packages on <span>netlink</span>
        for networking.</p>
    <p>To download the <span>go</span> package, execute the following command:</p>
    <p>$ go get github.com/vishvananda/netlink</p>
    <p>Open two <span>Terminal</span> windows - we will refer to them as <span>TA</span>
        and <span>TB</span> respectively. <span>TB</span> is where we will demonstrate the
        simple container.</p>
    <div id="para-div">
      <p>We need to download a minimal root filesystem (<span>rootfs</span>) that will be used as the base
        image for the simple container. For our demonstration, we will choose the latest <span>
        <a href="http://cdimage.ubuntu.com/ubuntu-base/releases/18.04.4/release/ubuntu-base-18.04.4-base-amd64.tar.gz" target="_blank"><span>Ubuntu Base 18.04.4 LTS</span></a></span> at the time of this article.</p>
      <p>We will assume the latest Ubuntu Base is downloaded to the directory <span>$HOME/Downloads</span>.</p>
    </div>
    <p>Hands-on with Namespaces</p>
    <p>UTS Namespace</p>
    <p>The <span>unshare</span> command executes the specified program with the indicated namespace(s)
        isolated from the parent process.</p>
    <p>To display the options for the <span>unshare</span> command, execute the following command in
        <span>TA</span>:</p>
    <p>$ unshare -h</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.2</h4>
      <pre>Usage:
 unshare [options] [&lt;program&gt; [&lt;argument&gt;...]]

Run a program with some namespaces unshared from the parent.

Options:
 -m, --mount[=&lt;file&gt;]      unshare mounts namespace
 -u, --uts[=&lt;file&gt;]        unshare UTS namespace (hostname etc)
 -i, --ipc[=&lt;file&gt;]        unshare System V IPC namespace
 -n, --net[=&lt;file&gt;]        unshare network namespace
 -p, --pid[=&lt;file&gt;]        unshare pid namespace
 -U, --user[=&lt;file&gt;]       unshare user namespace
 -C, --cgroup[=&lt;file&gt;]     unshare cgroup namespace
 -f, --fork                fork before launching &lt;program&gt;
     --mount-proc[=&lt;dir&gt;]  mount proc filesystem first (implies --mount)
 -r, --map-root-user       map current user to root (implies --user)
     --propagation slave|shared|private|unchanged
                           modify mount propagation in mount namespace
 -s, --setgroups allow|deny  control the setgroups syscall in user namespaces

 -h, --help                display this help
 -V, --version             display version</pre>
    </div>
    <p>Each process (with [PID]) has associated with it a sub-directory <span>/proc/[PID]/ns</span> that
        contains one entry for each of the namespaces.</p>
    <p>To list all the namespaces associated with a process, execute the following command in <span>TA</span>
        :</p>
    <p>$ ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.3</h4>
      <pre>total 0
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 alice alice 0 Mar  7 20:41 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 user -&gt; 'user:[4026531837]'
lrwxrwxrwx 1 alice alice 0 Mar  7 12:17 uts -&gt; 'uts:[4026531838]'</pre>
    </div>
    <p>To launch a simple container whose host name is isolated from the parent host name, execute the following command in
        <span>TB</span>:</p>
    <p>$ sudo unshare -u /bin/sh</p>
    <div id="para-div">
      <p>The <span>-u</span> option enables the <span>UTS</span> namespace.</p>
      <p>The command prompt will change to a <span>#</span>.</p>
    </div>
    <p>To check the <span>PID</span> of the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># echo $$</p>
    <p>The following would be a typical output:</p>
    
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.5</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar  7 12:36 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 user -&gt; 'user:[4026531837]'
lrwxrwxrwx 1 root root 0 Mar  7 12:36 uts -&gt; 'uts:[4026533064]'</pre>
    </div>
    <p>Comparing Output.5 to Output.3, we see a change in the <span>uts</span> namespace, which is expected
        and correct.</p>
    <p>To change the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p># hostname leopard</p>
    <p>To display the host name of the parent host, execute the following command in <span>TA</span>:</p>
    <p>$ hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p># hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>This demonstrates to us that we have isolated the host name of the simple container from the parent host name.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span> namespace isolation using the following <span>
        go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.1</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
       panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS,
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>The <span>Command</span> function from the <span>exec</span> package allows one
        to run the specified command (1st parameter) with the supplied arguments (2nd parameter). It returns an instance of
        the <span>Cmd</span> struct.</p>
      <p>One can set the standard input (<span>os.Stdin</span>), the standard output <span>
        os.Stdout</span>, the standard error <span>os.Stderr</span>, and some operating system specific
        attributes on the returned <span>Cmd</span> instance. In this case, we specify the <span>
        syscall.CLONE_NEWUTS</span> OS attribute to indicate the command be run in a new <span>UTS</span>
        namespace.</p>
      <p><span>IMPORTANT</span> : When the <span>main</span> process starts, it internally 
        spawns another <span>main</span> process (with the <span>CLONE</span> argument) in a new
        namespace. It is this spawned <span>main</span> process (running in the new namespace) that is overlayed
        (<span>syscall.Exec</span>) with the shell command by invoking the function <span>
        execContainerShell</span>.</p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/uts</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/uts</p>
      <p>$ cd $GOPATH/uts</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.8</h4>
      <pre>2020/03/07 12:49:11 Starting process ./main with args: [./main]
2020/03/07 12:49:11 Ready to run command ...
2020/03/07 12:49:11 Starting process ./main with args: [./main CLONE]
2020/03/07 12:49:11 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the <span>UTS</span> namespace using both
        the <span>unshare</span> command and a simple <span>go</span> program.</p>
    <p>User Namespace</p>
    <div id="para-div">
      <p>Let us layer the <span>User</span> namespace on top of the <span>UTS</span> namespace.</p>
      <p>To launch a simple container whose user/group IDs as well as the host name are isolated from the parent namespace,
        execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ sudo unshare -uU /bin/sh</p>
    <div id="para-div">
      <p>The <span>-U</span> option enables the <span>User</span> namespace.</p>
      <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.10</h4>
      <pre>uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup)</pre>
    </div>
    <p>When a <span>User</span> namespace is created, it starts without a mapping for the user/group IDs in
        the new namespace to the parent user/group IDs. The unmapped user/group ID is assigned the default value of the
        overflow user/group ID. The default value for the overflow user ID is read from
        <span>/proc/sys/kernel/overflowuid</span> (which is 65534). Similarly, the default value for the
        overflow group ID is read from <span>/proc/sys/kernel/overflowgid</span> (which is 65534).</p>
    <p>To fix the mapping for the user/group ID to the parent user/group ID, exit the simple container by executing the
        following command in <span>TB</span>:</p>
    <p>$ exit</p>
    <p>To re-launch the simple container with the current effective user/group ID mapped to the superuser user/group ID
        in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>$ sudo unshare -uUr /bin/sh</p>
    <div id="para-div">
      <p>The <span>-r</span> option enables the mapping of the user/group IDs in the new namespace to the
        parent namespace user/group IDs.</p>
      <p>The command prompt will change to a <span>#</span>.</p>
    </div>
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p># id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.11</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p># ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.12</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 7 13:09 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 user -&gt; 'user:[4026532892]'
lrwxrwxrwx 1 root root 0 Mar 7 13:09 uts -&gt; 'uts:[4026533401]'</pre>
    </div>
    <p>Comparing Output.12 to Output.3, we see a change in both the <span>uts</span> namespace as well as
        the <span>user</span> namespace, which is what is expected and correct.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span> and <span>User</span> namespace isolation
        using the following <span>go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.2</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
        panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
        os.Exit(0)
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWUSER,
        UidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
        GidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>As indicated previously, the <span>Command</span> function returns an instance of the
        <span>Cmd</span> struct.</p>
      <p>In this example, we specify the additional <span>syscall.CLONE_NEWUSER</span> OS attribute
        to indicate the command be run in a new <span>User</span> namespace.</p>
      <p>In addition, we set the user ID map <span>UidMappings</span> as an array of
        <span>syscall.SysProcIDMap</span> struct entries, each consisting of the user ID mapping in
        the container (<span>ContainerID</span>) to the user ID in the host namespace
        (<span>HostID</span>). In this case, we map the <span>root</span> user ID
        <span>0</span> in the container to the <span>root</span> user ID <span>
        0</span> of the host namespace. Similarly, we set the group ID map <span>GidMappings</span></p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/user</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/user</p>
      <p>$ cd $GOPATH/user</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.13</h4>
      <pre>2020/03/07 13:17:02 Starting process ./main with args: [./main]
2020/03/07 13:17:02 Ready to run command ...
2020/03/07 13:17:02 Starting process ./main with args: [./main CLONE]
2020/03/07 13:17:02 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>-&gt; id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.14</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p>-&gt; ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.15</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 13 21:17 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 mnt -&gt; 'mnt:[4026531840]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 pid -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 pid_for_children -&gt; 'pid:[4026531836]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 user -&gt; 'user:[4026532666]'
lrwxrwxrwx 1 root root 0 Mar 13 21:17 uts -&gt; 'uts:[4026532723]'</pre>
    </div>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span> and
        <span>User</span> namespaces using both the <span>unshare</span> command and a simple
        <span>go</span> program.</p>
    <p>PID Namespace</p>
    <div id="para-div">
      <p>Let us now layer the <span>PID</span> namespace on top of the <span>User</span> namespace
        and the <span>UTS</span> namespace.</p>
      <p>To launch a simple container whose process IDs as well as the user/group IDs and the host name are isolated from
        the parent namespace, execute the following command in <span>TB</span>:</p>
    </div>
    <p>$ sudo unshare -uUrpf --mount-proc /bin/sh</p>
    <div id="para-div">
      <p>The <span>-p</span> option enables the <span>PID</span> namespace.</p>
      <p>The <span>-f</span> option enables spawning (or forking) of new processes in the new namespace.</p>
      <p>The <span>--mount-proc</span> option mounts the <span>proc</span> filesystem as
        a private mount at <span>/proc</span> in the new namespace. This means the <span>
        /proc</span> pseudo directory only shows information only about processes within that <span>PID</span>
        namespace.</p>
    </div>
    <div id="error-div">
      <h4>ATTENTION</h4>
      <pre><span>Ensure</span> the option <span>-f</span> is *<span>SPECIFIED</span>*. Else will encounter the following error:<p><span>/bin/sh: 4: Cannot fork</span></p></pre>
    </div>
    <p>The command prompt will change to a <span>#</span>.</p>
    <p>To display all the processes in the new namespace, execute the following command in <span>TB</span>:</p>
    <p># ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.17</h4>
      <pre>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   4628   880 pts/1    S    09:08   0:00 /bin/sh
root         6  0.0  0.0  37368  3340 pts/1    R+   09:12   0:00 ps -fu</pre>
    </div>
    <p>To display all the processes in the parent namespace, execute the following command in <span>TA</span>:</p>
    <p>$ ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.18</h4>
      <pre>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
polarsparc  8695  0.0  0.0  22840  5424 pts/1    Ss   08:43   0:00 bash
polarsparc  8681  0.0  0.0  22708  5096 pts/0    Ss   08:43   0:00 bash
polarsparc  9635  0.0  0.0  37368  3364 pts/0    R+   09:12   0:00  \_ ps -fu</pre>
    </div>
    <p>Comparing Output.17 to Output.18, we see the isolation between the new namespace and the parent namespace, which is
        what is expected and correct.</p>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p>Next, we will mimic the above <span>UTS</span>, <span>User</span>, and <span>
        PID</span> namespace isolation using the following <span>go</span> program:</p>    
    <fieldset id="sc-fieldset">
      <legend>Listing.3</legend>
      <pre>package main

import (
    "log"
    "os"
    "os/exec"
    "syscall"
)

func execContainerShell() {
    log.Printf("Ready to exec container shell ...\n")

    if err := syscall.Sethostname([]byte("leopard")); err != nil {
        panic(err)
    }
    
    if err := syscall.Mount("proc", "/proc", "proc", 0, ""); err != nil {
        panic(err)
    }

    const sh = "/bin/sh"

    env := os.Environ()
    env = append(env, "PS1=-&gt; ")

    if err := syscall.Exec(sh, []string{""}, env); err != nil {
        panic(err)
    }
}

func main() {
    log.Printf("Starting process %s with args: %v\n", os.Args[0], os.Args)

    const clone = "CLONE"

    if len(os.Args) &gt; 1 &amp;&amp; os.Args[1] == clone {
        execContainerShell()
        os.Exit(0)
    }

    log.Printf("Ready to run command ...\n")

    cmd := exec.Command(os.Args[0], []string{clone}...)
    cmd.Stdin = os.Stdin
    cmd.Stdout = os.Stdout
    cmd.Stderr = os.Stderr
    cmd.SysProcAttr = &amp;syscall.SysProcAttr{
        Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWUSER | syscall.CLONE_NEWNS | syscall.CLONE_NEWPID,
        UidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
        GidMappings: []syscall.SysProcIDMap{
            {ContainerID: 0, HostID: 0, Size: 1},
        },
    }

    if err := cmd.Run(); err != nil {
        panic(err)
    }
}</pre>
    </fieldset>
    <div id="para-div">
      <p>As indicated previously, the <span>Command</span> function returns an instance of the
        <span>Cmd</span> struct.</p>
      <p>In this example, we specify the additional <span>syscall.CLONE_NEWNS</span> and
        <span>syscall.CLONE_NEWPID</span> OS attributes to indicate the command be run in a new
        <span>PID</span> namespace.</p>
    </div>
    <p>Create and change to the directory <span>$GOPATH/pid</span> by executing the following commands in
        <span>TB</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p $GOPATH/pid</p>
      <p>$ cd $GOPATH/pid</p>
    </div>
    <p>Copy the above code into the program file <span>main.go</span> in the current directory.</p>
    <p>To compile the program file <span>main.go</span>, execute the following command in <span>
        TB</span>:</p>
    <p>$ go build main.go</p>
    <p>To run program <span>main</span>, execute the following command in <span>TB</span>:</p>
    <p>$ sudo ./main</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.19</h4>
      <pre>2020/03/07 13:38:02 Starting process ./main with args: [./main]
2020/03/07 13:38:02 Ready to run command ...
2020/03/07 13:38:02 Starting process ./main with args: [./main CLONE]
2020/03/07 13:38:02 Ready to exec container shell ...
-&gt;</pre>
    </div>
    <p>The command prompt will change to a <span>-&gt;</span>.</p>
    <p>To display the host name of the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; hostname</p>
    <p>The following would be a typical output:</p>
    
    <p>To display the user ID and group ID in the new namespace, execute the following command in <span>TB</span>:</p>
    <p>-&gt; id</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.21</h4>
      <pre>uid=0(root) gid=0(root) groups=0(root)</pre>
    </div>
    <p>To display all the processes in the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; ps -fu</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.22</h4>
      <pre>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   4628   776 pts/1    S    09:41   0:00 
root         6  0.0  0.0  37368  3400 pts/1    R+   09:41   0:00 ps -fu</pre>
    </div>
    <p>To list all the namespaces associated with the simple container, execute the following command in <span>
        TB</span>:</p>
    <p>-&gt; ls -l /proc/$$/ns</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.23</h4>
      <pre>total 0
lrwxrwxrwx 1 root root 0 Mar 14 09:44 cgroup -&gt; 'cgroup:[4026531835]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 ipc -&gt; 'ipc:[4026531839]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 mnt -&gt; 'mnt:[4026532366]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 net -&gt; 'net:[4026531993]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 pid -&gt; 'pid:[4026532368]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 pid_for_children -&gt; 'pid:[4026532368]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 user -&gt; 'user:[4026532365]'
lrwxrwxrwx 1 root root 0 Mar 14 09:44 uts -&gt; 'uts:[4026532367]'</pre>
    </div>
    <p>To exit the simple container, execute the following command in <span>TB</span>:</p>
    <p>-&gt; exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span>,
        <span>User</span>, and <span>PID</span> namespaces using both the
        <span>unshare</span> command and a simple <span>go</span> program.</p>
    <p>Mount Namespace</p>
    <p>We will now setup the minimal Ubuntu Base image for use in the new namespace in the <span>/tmp</span>
        directory.</p>
    <p>To create and copy the base image to a directory in <span>/tmp</span>, execute the following commands
        in <span>TA</span>:</p>
    <div id="cmd-div">
      <p>$ mkdir -p /tmp/rootfs/.old_root</p>
      <p>$ tar -xvf $HOME/Downloads/ubuntu-base-18.04.4-base-amd64.tar.gz --directory /tmp/rootfs</p>
      <p>cd /tmp</p>
    </div>
    <p>Now let us now layer the <span>Mount</span> namespace on top of the <span>User</span>,
        the <span>UTS</span>, and the <span>PID</span> namespaces.</p>
    <p>To launch a simple container whose mount points as well as the process IDs, the user/group IDs, and the host name
        are isolated from the parent namespace, execute the following command in <span>TB</span>:</p>
    <p>$ sudo unshare -uUrpfm --mount-proc /bin/sh</p>
    <p>The <span>-m</span> option enables the <span>Mount</span> namespace.</p>
    <p>The command prompt will change to a <span>#</span>.</p>
    <p>To list all the mount points in the parent namespace, execute the following command in <span>TA</span>:</p>
    <p>$ cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.24</h4>
      <pre>cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0
cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0
cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0
cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0
cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0
cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0
cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0
cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0
cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0
cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0
cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0
cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0
cgroup /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0
configfs /sys/kernel/config configfs rw,relatime 0 0
debugfs /sys/kernel/debug debugfs rw,relatime 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
/dev/sdb1 /home ext4 rw,relatime,data=ordered 0 0
/dev/sdc1 /home/data ext4 rw,relatime,data=ordered 0 0
fusectl /sys/fs/fuse/connections fusectl rw,relatime 0 0
gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0
hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0
mqueue /dev/mqueue mqueue rw,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0
securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=25,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=28210 0 0
tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0
tmpfs /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0
tmpfs /run tmpfs rw,nosuid,noexec,relatime,size=3293620k,mode=755 0 0
tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=3293616k,mode=700,uid=1000,gid=1000 0 0
tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0
udev /dev devtmpfs rw,nosuid,relatime,size=16402556k,nr_inodes=4100639,mode=755 0 0</pre>
    </div>
    <p>Now, let us list all the mount points in the new namespace by executing the following command in <span>
        TB</span>:</p>
    <p># cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.25</h4>
      <pre>cgroup /sys/fs/cgroup/blkio cgroup rw,nosuid,nodev,noexec,relatime,blkio 0 0
cgroup /sys/fs/cgroup/cpu,cpuacct cgroup rw,nosuid,nodev,noexec,relatime,cpu,cpuacct 0 0
cgroup /sys/fs/cgroup/cpuset cgroup rw,nosuid,nodev,noexec,relatime,cpuset 0 0
cgroup /sys/fs/cgroup/devices cgroup rw,nosuid,nodev,noexec,relatime,devices 0 0
cgroup /sys/fs/cgroup/freezer cgroup rw,nosuid,nodev,noexec,relatime,freezer 0 0
cgroup /sys/fs/cgroup/hugetlb cgroup rw,nosuid,nodev,noexec,relatime,hugetlb 0 0
cgroup /sys/fs/cgroup/memory cgroup rw,nosuid,nodev,noexec,relatime,memory 0 0
cgroup /sys/fs/cgroup/net_cls,net_prio cgroup rw,nosuid,nodev,noexec,relatime,net_cls,net_prio 0 0
cgroup /sys/fs/cgroup/perf_event cgroup rw,nosuid,nodev,noexec,relatime,perf_event 0 0
cgroup /sys/fs/cgroup/pids cgroup rw,nosuid,nodev,noexec,relatime,pids 0 0
cgroup /sys/fs/cgroup/rdma cgroup rw,nosuid,nodev,noexec,relatime,rdma 0 0
cgroup /sys/fs/cgroup/systemd cgroup rw,nosuid,nodev,noexec,relatime,xattr,name=systemd 0 0
cgroup /sys/fs/cgroup/unified cgroup2 rw,nosuid,nodev,noexec,relatime,nsdelegate 0 0
configfs /sys/kernel/config configfs rw,relatime 0 0
debugfs /sys/kernel/debug debugfs rw,relatime 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
/dev/sdb1 /home ext4 rw,relatime,data=ordered 0 0
/dev/sdc1 /home/data ext4 rw,relatime,data=ordered 0 0
fusectl /sys/fs/fuse/connections fusectl rw,relatime 0 0
gvfsd-fuse /run/user/1000/gvfs fuse.gvfsd-fuse rw,nosuid,nodev,relatime,user_id=1000,group_id=1000 0 0
hugetlbfs /dev/hugepages hugetlbfs rw,relatime,pagesize=2M 0 0
mqueue /dev/mqueue mqueue rw,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
pstore /sys/fs/pstore pstore rw,nosuid,nodev,noexec,relatime 0 0
securityfs /sys/kernel/security securityfs rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
systemd-1 /proc/sys/fs/binfmt_misc autofs rw,relatime,fd=25,pgrp=0,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=28210 0 0
tmpfs /dev/shm tmpfs rw,nosuid,nodev 0 0
tmpfs /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0
tmpfs /run tmpfs rw,nosuid,noexec,relatime,size=3293620k,mode=755 0 0
tmpfs /run/user/1000 tmpfs rw,nosuid,nodev,relatime,size=3293616k,mode=700,uid=1000,gid=1000 0 0
tmpfs /sys/fs/cgroup tmpfs ro,nosuid,nodev,noexec,mode=755 0 0
udev /dev devtmpfs rw,nosuid,relatime,size=16402556k,nr_inodes=4100639,mode=755 0 0</pre>
    </div>
    <p>Comparing Output.25 and Output.24, we see the one difference for <span>proc</span>. When a new
        <span>Mount</span> namespace is created, the mount points of the new namespace is a copy of the
        mount points in the parent's namespace.</p>
    <p>We will now demonstrate any changes to the new namespace will not affect the parent namespace.</p>
    <p>To make the mount point <span>/</span> (and its children recursively) to be private to the new
        namespace, execute the following command in <span>TB</span>:</p>
    <p># mount --make-rprivate /</p>
    <p>To recursive bind the mount point <span>rootfs/</span> to <span>rootfs/</span> in the new
        namespace, execute the following command in <span>TB</span>:</p>
    <p># mount --rbind rootfs/ rootfs/</p>
    <p>We need the <span>proc</span> filesystem in the new namespace for making changes to mounts. To mount
        <span>/proc</span> as the proc filesystem <span>proc</span> in the new namespace,
        execute the following command in <span>TB</span>:</p>
    <p># mount -t proc proc rootfs/proc</p>
    <p>Next, we need to make <span>rootfs/</span> the root filesystem in the new namespace and move the parent
        root filesystem to <span>rootfs/.old_root</span> using the <span>pivot_root</span>
        command. To do that, execute the following commands in <span>TB</span>:</p>
    <div id="cmd-div">
      <p># pivot_root rootfs/ rootfs/.old_root</p>
      <p># cd /</p>
    </div>
    <p>To list all the file(s) under <span>/</span> in the parent namespace, execute the following command in
        <span>TA</span>:</p>
    <p>$ ls -l /</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.26</h4>
      <pre>total 96
drwxr-xr-x   2 root root  4096 Mar  1 10:58 bin
drwxr-xr-x   3 root root  4096 Mar 16 21:15 boot
drwxr-xr-x   2 root root  4096 Sep 13  2019 cdrom
drwxr-xr-x  22 root root  4560 Mar 21 06:59 dev
drwxr-xr-x 163 root root 12288 Mar 20 10:01 etc
drwxr-xr-x   5 root root  4096 Sep 13  2019 home
lrwxrwxrwx   1 root root    33 Mar 16 21:15 initrd.img -&gt; boot/initrd.img-4.15.0-91-generic
lrwxrwxrwx   1 root root    33 Feb 17 14:08 initrd.img.old -&gt; boot/initrd.img-4.15.0-88-generic
drwxr-xr-x  25 root root  4096 Mar 16 13:37 lib
drwxr-xr-x   2 root root  4096 Jul 29  2019 lib64
drwx------   2 root root 16384 Sep 13  2019 lost+found
drwxr-xr-x   3 root root  4096 Nov 10 13:00 media
drwxr-xr-x   2 root root  4096 Jul 29  2019 mnt
drwxr-xr-x   7 root root  4096 Mar 13 08:04 opt
dr-xr-xr-x 328 root root     0 Mar 21 06:59 proc
drwx------   9 root root  4096 Feb 23 13:25 root
drwxr-xr-x  36 root root  1140 Mar 21 07:04 run
drwxr-xr-x   2 root root 12288 Mar 16 13:37 sbin
drwxr-xr-x   2 root root  4096 Jul 29  2019 srv
dr-xr-xr-x  13 root root     0 Mar 21 06:59 sys
drwxrwxrwt  20 root root  4096 Mar 21 11:10 tmp
drwxr-xr-x  11 root root  4096 Jul 29  2019 usr
drwxr-xr-x  11 root root  4096 Jul 29  2019 var
lrwxrwxrwx   1 root root    30 Mar 16 21:15 vmlinuz -&gt; boot/vmlinuz-4.15.0-91-generic
lrwxrwxrwx   1 root root    30 Feb 17 14:08 vmlinuz.old -&gt; boot/vmlinuz-4.15.0-88-generic</pre>
    </div>
    <p>To list all the file(s) under <span>/</span> in the new namespace, execute the following command in
        <span>TB</span>:</p>
    <p># ls -l /</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.27</h4>
      <pre>total 72
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 bin
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 boot
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 dev
drwxr-xr-x  29 nobody nogroup 4096 Feb  3 20:24 etc
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 home
drwxr-xr-x   8 nobody nogroup 4096 May 23  2017 lib
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 lib64
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 media
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 mnt
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 opt
dr-xr-xr-x 328 root   root       0 Mar 21 14:10 proc
drwx------   2 nobody nogroup 4096 Feb  3 20:24 root
drwxr-xr-x   4 nobody nogroup 4096 Feb  3 20:23 run
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:24 sbin
drwxr-xr-x   2 nobody nogroup 4096 Feb  3 20:23 srv
drwxr-xr-x   2 nobody nogroup 4096 Apr 24  2018 sys
drwxrwxr-x   2 nobody nogroup 4096 Feb  3 20:24 tmp
drwxr-xr-x  10 nobody nogroup 4096 Feb  3 20:23 usr
drwxr-xr-x  11 nobody nogroup 4096 Feb  3 20:24 var</pre>
    </div>
    <p>Comparing Output.26 and Output.27, we see the root filesystems are totally different.</p>
    <p>To mount <span>/tmp</span> as the temporary filesystem <span>tmpfs</span> in the
        new namespace, execute the following command in <span>TB</span>:</p>
    <p># mount -t tmpfs tmpfs /tmp</p>
    <p>To create a text file <span>/tmp/leopard.txt</span> in the directory <span>/tmp</span>
        of the new namespace, execute the following command in <span>TB</span>:</p>
    <p># echo 'leopard' &gt; /tmp/leopard.txt</p>
    <p>To list the properties of the file <span>/tmp/leopard.txt</span> in the new namespace, execute
        the following command in <span>TB</span>:</p>
    <p># ls -l /tmp/leopard.txt</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.28</h4>
      <pre>-rw-r--r-- 1 root root 7 Mar 14 22:05 /tmp/leopard.txt</pre>
    </div>
    <p>To list the properties of the file <span>/tmp/leopard.txt</span> in the parent namespace, execute
        the following command in <span>TA</span>:</p>
    <p>$ ls -l /tmp/leopard.txt</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.29</h4>
      <pre>ls: cannot access '/tmp/leopard.txt': No such file or directory</pre>
    </div>
    <p>Finally, to completely remove the parent root filesystem <span>rootfs/.old_root</span> from the new
        namespace, execute the following commands in <span>TB</span>:</p>
    <div id="cmd-div">
      <p># mount --make-rprivate /.old_root</p>
      <p># umount -l /.old_root</p>
    </div>
    <p>To list all the mount points in the new namespace by executing the following command in <span>TB</span>
        :</p>
    <p># cat /proc/mounts | sort</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.30</h4>
      <pre>/dev/sda1 / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
proc /proc proc rw,relatime 0 0
tmpfs /tmp tmpfs rw,relatime 0 0</pre>
    </div>
    <p>To exit the new namespace, execute the following command in <span>TB</span>:</p>
    <p># exit</p>
    <p><span>SUCCESS !!!</span> We have demonstrated the combined <span>UTS</span>,
        <span>User</span>, <span>PID</span>, and <span>Mount</span> namespaces
        using the <span>unshare</span> command.</p>
    <p>References</p>
    
    <br>
    <hr>
    
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[DwarFS – Deduplicating Warp-Speed Advanced Read-Only File System (102 pts)]]></title>
            <link>https://github.com/mhx/dwarfs</link>
            <guid>40008755</guid>
            <pubDate>Fri, 12 Apr 2024 02:04:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mhx/dwarfs">https://github.com/mhx/dwarfs</a>, See on <a href="https://news.ycombinator.com/item?id=40008755">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a href="https://github.com/mhx/dwarfs/releases/latest"><img src="https://camo.githubusercontent.com/7c14c3f1b59f09fab877e8fd13d3682f78ce4b9318fbe3cc4572e937200d6e56/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6d68782f6477617266733f6c6162656c3d4c617465737425323052656c65617365" alt="Latest Release" data-canonical-src="https://img.shields.io/github/release/mhx/dwarfs?label=Latest%20Release"></a>
<a href="https://github.com/mhx/dwarfs/releases"><img src="https://camo.githubusercontent.com/2b6c35d2fbd0dac827465bda94342ea83a91f3effafab9c1d117eb790cc936e9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d68782f6477617266732f746f74616c2e7376673f26636f6c6f723d453935343230266c6162656c3d546f74616c253230446f776e6c6f616473" alt="Total Downloads" data-canonical-src="https://img.shields.io/github/downloads/mhx/dwarfs/total.svg?&amp;color=E95420&amp;label=Total%20Downloads"></a>
<a href="https://github.com/mhx/dwarfs/actions/workflows/build.yml"><img src="https://github.com/mhx/dwarfs/actions/workflows/build.yml/badge.svg" alt="DwarFS CI Build"></a>
<a href="https://app.travis-ci.com/github/mhx/dwarfs" rel="nofollow"><img src="https://camo.githubusercontent.com/9d83f6eb2696807dc865c941a7b37c7fc27346db3cbb11595be061b977b8866b/68747470733a2f2f6170702e7472617669732d63692e636f6d2f6d68782f6477617266732e7376673f6272616e63683d6d61696e" alt="Build Status" data-canonical-src="https://app.travis-ci.com/mhx/dwarfs.svg?branch=main"></a>
<a href="https://app.codacy.com/gh/mhx/dwarfs/dashboard" rel="nofollow"><img src="https://camo.githubusercontent.com/6d5dac47834dc9a72470dd868d91391d3a808686e1457628e3022dfb61cf92fd/68747470733a2f2f6170702e636f646163792e636f6d2f70726f6a6563742f62616467652f47726164652f3533343839663737373535323438633939396533383035303032363765383839" alt="Codacy Badge" data-canonical-src="https://app.codacy.com/project/badge/Grade/53489f77755248c999e380500267e889"></a>
<a href="https://codecov.io/github/mhx/dwarfs" rel="nofollow"><img src="https://camo.githubusercontent.com/028b266bc1c1b17af19e188abadf5901e33b4b9972e56631c7a76a0dea82d2e5/68747470733a2f2f636f6465636f762e696f2f6769746875622f6d68782f6477617266732f67726170682f62616467652e7376673f746f6b656e3d424b52344133584b4139" alt="codecov" data-canonical-src="https://codecov.io/github/mhx/dwarfs/graph/badge.svg?token=BKR4A3XKA9"></a>
<a href="https://www.bestpractices.dev/projects/8663" rel="nofollow"><img src="https://camo.githubusercontent.com/9a7cc655c21144482581cbbfc96b3a1d20b4dbb174d99ca64ca27a682d2d9aa1/68747470733a2f2f7777772e626573747072616374696365732e6465762f70726f6a656374732f383636332f6261646765" alt="OpenSSF Best Practices" data-canonical-src="https://www.bestpractices.dev/projects/8663/badge"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">DwarFS</h2><a id="user-content-dwarfs" aria-label="Permalink: DwarFS" href="#dwarfs"></a></p>
<p dir="auto">The <strong>D</strong>eduplicating <strong>W</strong>arp-speed <strong>A</strong>dvanced <strong>R</strong>ead-only <strong>F</strong>ile <strong>S</strong>ystem.</p>
<p dir="auto">A fast high compression read-only file system for Linux and Windows.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#history">History</a></li>
<li><a href="#building-and-installing">Building and Installing</a>
<ul dir="auto">
<li><a href="#prebuilt-binaries">Prebuilt Binaries</a></li>
<li><a href="#universal-binaries">Universal Binaries</a></li>
<li><a href="#dependencies">Dependencies</a></li>
<li><a href="#building">Building</a></li>
<li><a href="#installing">Installing</a></li>
<li><a href="#static-builds">Static Builds</a></li>
</ul>
</li>
<li><a href="#usage">Usage</a></li>
<li><a href="#windows-support">Windows Support</a>
<ul dir="auto">
<li><a href="#building-on-windows">Building on Windows</a></li>
</ul>
</li>
<li><a href="#macos-support">macOS Support</a>
<ul dir="auto">
<li><a href="#building-on-macos">Building on macOS</a></li>
</ul>
</li>
<li><a href="#use-cases">Use Cases</a>
<ul dir="auto">
<li><a href="#astrophotography">Astrophotography</a></li>
</ul>
</li>
<li><a href="#dealing-with-bit-rot">Dealing with Bit Rot</a></li>
<li><a href="#extended-attributes">Extended Attributes</a></li>
<li><a href="#comparison">Comparison</a>
<ul dir="auto">
<li><a href="#with-squashfs">With SquashFS</a></li>
<li><a href="#with-squashfs--xz">With SquashFS &amp; xz</a></li>
<li><a href="#with-lrzip">With lrzip</a></li>
<li><a href="#with-zpaq">With zpaq</a></li>
<li><a href="#with-zpaqfranz">With zpaqfranz</a></li>
<li><a href="#with-wimlib">With wimlib</a></li>
<li><a href="#with-cromfs">With Cromfs</a></li>
<li><a href="#with-erofs">With EROFS</a></li>
<li><a href="#with-fuse-archive">With fuse-archive</a></li>
</ul>
</li>
<li><a href="#performance-monitoring">Performance Monitoring</a></li>
<li><a href="#other-obscure-features">Other Obscure Features</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mhx/dwarfs/blob/main/doc/windows.gif?raw=true"><img src="https://github.com/mhx/dwarfs/raw/main/doc/windows.gif?raw=true" alt="Windows Screen Capture" title="DwarFS Windows" data-animated-image=""></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/mhx/dwarfs/blob/main/doc/screenshot.gif?raw=true"><img src="https://github.com/mhx/dwarfs/raw/main/doc/screenshot.gif?raw=true" alt="Linux Screen Capture" title="DwarFS Linux" data-animated-image=""></a></p>
<p dir="auto">DwarFS is a read-only file system with a focus on achieving <strong>very
high compression ratios</strong> in particular for very redundant data.</p>
<p dir="auto">This probably doesn't sound very exciting, because if it's redundant,
it <em>should</em> compress well. However, I found that other read-only,
compressed file systems don't do a very good job at making use of
this redundancy. See <a href="#comparison">here</a> for a comparison with other
compressed file systems.</p>
<p dir="auto">DwarFS also <strong>doesn't compromise on speed</strong> and for my use cases I've
found it to be on par with or perform better than SquashFS. For my
primary use case, <strong>DwarFS compression is an order of magnitude better
than SquashFS compression</strong>, it's <strong>6 times faster to build the file
system</strong>, it's typically faster to access files on DwarFS and it uses
less CPU resources.</p>
<p dir="auto">To give you an idea of what DwarFS is capable of, here's a quick comparison
of DwarFS and SquashFS on a set of video files with a total size of 39 GiB.
The twist is that each unique video file has two sibling files with a
different set of audio streams (I didn't make this up, this is
<a href="https://github.com/mhx/dwarfs/discussions/63" data-hovercard-type="discussion" data-hovercard-url="/mhx/dwarfs/discussions/63/hovercard">an actual use case</a>). So
there's redundancy in both the video and audio data, but as the streams
are interleaved and identical blocks are typically very far apart, it's
quite challenging to make use of that redundancy for compression. SquashFS
essentially fails to compress the source data at all, whereas DwarFS is
able to reduce the size by almost a factor of 3, which is close to the
theoretical maximum:</p>
<div data-snippet-clipboard-copy-content="$ du -hs dwarfs-video-test
39G     dwarfs-video-test
$ ls -lh dwarfs-video-test.*fs
-rw-r--r-- 1 mhx users 14G Jul  2 13:01 dwarfs-video-test.dwarfs
-rw-r--r-- 1 mhx users 39G Jul 12 09:41 dwarfs-video-test.squashfs"><pre><code>$ du -hs dwarfs-video-test
39G     dwarfs-video-test
$ ls -lh dwarfs-video-test.*fs
-rw-r--r-- 1 mhx users 14G Jul  2 13:01 dwarfs-video-test.dwarfs
-rw-r--r-- 1 mhx users 39G Jul 12 09:41 dwarfs-video-test.squashfs
</code></pre></div>
<p dir="auto">While this is already impressive, it gets even better. When mounting
the SquashFS image and performing a random-read throughput test using
<a href="https://github.com/axboe/fio/">fio</a>-3.34, both <code>squashfuse</code> and
<code>squashfuse_ll</code> top out at around 230 MiB/s:</p>
<div data-snippet-clipboard-copy-content="$ fio --readonly --rw=randread --name=randread --bs=64k --direct=1 \
      --opendir=mnt --numjobs=4 --ioengine=libaio --iodepth=32 \
      --group_reporting --runtime=60 --time_based
[...]
   READ: bw=230MiB/s (241MB/s), 230MiB/s-230MiB/s (241MB/s-241MB/s), io=13.5GiB (14.5GB), run=60004-60004msec"><pre><code>$ fio --readonly --rw=randread --name=randread --bs=64k --direct=1 \
      --opendir=mnt --numjobs=4 --ioengine=libaio --iodepth=32 \
      --group_reporting --runtime=60 --time_based
[...]
   READ: bw=230MiB/s (241MB/s), 230MiB/s-230MiB/s (241MB/s-241MB/s), io=13.5GiB (14.5GB), run=60004-60004msec
</code></pre></div>
<p dir="auto">DwarFS, however, manages to sustain <strong>random read rates of 20 GiB/s</strong>:</p>
<div data-snippet-clipboard-copy-content="  READ: bw=20.2GiB/s (21.7GB/s), 20.2GiB/s-20.2GiB/s (21.7GB/s-21.7GB/s), io=1212GiB (1301GB), run=60001-60001msec"><pre><code>  READ: bw=20.2GiB/s (21.7GB/s), 20.2GiB/s-20.2GiB/s (21.7GB/s-21.7GB/s), io=1212GiB (1301GB), run=60001-60001msec
</code></pre></div>
<p dir="auto">Distinct features of DwarFS are:</p>
<ul dir="auto">
<li>
<p dir="auto">Clustering of files by similarity using a similarity hash function.
This makes it easier to exploit the redundancy across file boundaries.</p>
</li>
<li>
<p dir="auto">Segmentation analysis across file system blocks in order to reduce
the size of the uncompressed file system. This saves memory when
using the compressed file system and thus potentially allows for
higher cache hit rates as more data can be kept in the cache.</p>
</li>
<li>
<p dir="auto"><a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md#categorizers">Categorization framework</a> to categorize
files or even fragments of files and then process individual categories
differently. For example, this allows you to not waste time trying to
compress incompressible files or to compress PCM audio data using FLAC
compression.</p>
</li>
<li>
<p dir="auto">Highly multi-threaded implementation. Both the
<a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md">file system creation tool</a> as well as the
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">FUSE driver</a> are able to make good use of the
many cores of your system.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">I started working on DwarFS in 2013 and my main use case and major
motivation was that I had several hundred different versions of Perl
that were taking up something around 30 gigabytes of disk space, and
I was unwilling to spend more than 10% of my hard drive keeping them
around for when I happened to need them.</p>
<p dir="auto">Up until then, I had been using <a href="https://bisqwit.iki.fi/source/cromfs.html" rel="nofollow">Cromfs</a>
for squeezing them into a manageable size. However, I was getting more
and more annoyed by the time it took to build the filesystem image
and, to make things worse, more often than not it was crashing after
about an hour or so.</p>
<p dir="auto">I had obviously also looked into <a href="https://en.wikipedia.org/wiki/SquashFS" rel="nofollow">SquashFS</a>,
but never got anywhere close to the compression rates of Cromfs.</p>
<p dir="auto">This alone wouldn't have been enough to get me into writing DwarFS,
but at around the same time, I was pretty obsessed with the recent
developments and features of newer C++ standards and really wanted
a C++ hobby project to work on. Also, I've wanted to do something
with <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace" rel="nofollow">FUSE</a>
for quite some time. Last but not least, I had been thinking about
the problem of compressed file systems for a bit and had some ideas
that I definitely wanted to try.</p>
<p dir="auto">The majority of the code was written in 2013, then I did a couple
of cleanups, bugfixes and refactors every once in a while, but I
never really got it to a state where I would feel happy releasing
it. It was too awkward to build with its dependency on Facebook's
(quite awesome) <a href="https://github.com/facebook/folly">folly</a> library
and it didn't have any documentation.</p>
<p dir="auto">Digging out the project again this year, things didn't look as grim
as they used to. Folly now builds with CMake and so I just pulled
it in as a submodule. Most other dependencies can be satisfied
from packages that should be widely available. And I've written
some rudimentary docs as well.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building and Installing</h2><a id="user-content-building-and-installing" aria-label="Permalink: Building and Installing" href="#building-and-installing"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prebuilt Binaries</h3><a id="user-content-prebuilt-binaries" aria-label="Permalink: Prebuilt Binaries" href="#prebuilt-binaries"></a></p>
<p dir="auto"><a href="https://github.com/mhx/dwarfs/releases">Each release</a> has pre-built,
statically linked binaries for <code>Linux-x86_64</code>, <code>Linux-aarch64</code> and
<code>Windows-AMD64</code> available for download. These <em>should</em> run without
any dependencies and can be useful especially on older distributions
where you can't easily build the tools from source.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Universal Binaries</h3><a id="user-content-universal-binaries" aria-label="Permalink: Universal Binaries" href="#universal-binaries"></a></p>
<p dir="auto">In addition to the binary tarballs, there's a <strong>universal binary</strong>
available for each architecture. These universal binaries contain
<em>all</em> tools (<code>mkdwarfs</code>, <code>dwarfsck</code>, <code>dwarfsextract</code> and the <code>dwarfs</code>
FUSE driver) in a single executable. These executables are compressed
using <a href="https://github.com/upx/upx">upx</a>, so they are much smaller than
the individual tools combined. However, it also means the binaries need
to be decompressed each time they are run, which can have a signficant
overhead. If that is an issue, you can either stick to the "classic"
individual binaries or you can decompress the universal binary, e.g.:</p>
<div data-snippet-clipboard-copy-content="upx -d dwarfs-universal-0.7.0-Linux-aarch64"><pre><code>upx -d dwarfs-universal-0.7.0-Linux-aarch64
</code></pre></div>
<p dir="auto">The universal binaries can be run through symbolic links named after
the proper tool. e.g.:</p>
<div data-snippet-clipboard-copy-content="$ ln -s dwarfs-universal-0.7.0-Linux-aarch64 mkdwarfs
$ ./mkdwarfs --help"><pre><code>$ ln -s dwarfs-universal-0.7.0-Linux-aarch64 mkdwarfs
$ ./mkdwarfs --help
</code></pre></div>
<p dir="auto">This also works on Windows if the file system supports symbolic links:</p>
<div data-snippet-clipboard-copy-content="> mklink mkdwarfs.exe dwarfs-universal-0.7.0-Windows-AMD64.exe
> .\mkdwarfs.exe --help"><pre><code>&gt; mklink mkdwarfs.exe dwarfs-universal-0.7.0-Windows-AMD64.exe
&gt; .\mkdwarfs.exe --help
</code></pre></div>
<p dir="auto">Alternatively, you can select the tool by passing <code>--tool=&lt;name&gt;</code> as
the first argument on the command line:</p>
<div data-snippet-clipboard-copy-content="> .\dwarfs-universal-0.7.0-Windows-AMD64.exe --tool=mkdwarfs --help"><pre><code>&gt; .\dwarfs-universal-0.7.0-Windows-AMD64.exe --tool=mkdwarfs --help
</code></pre></div>
<p dir="auto">Note that just like the <code>dwarfs.exe</code> Windows binary, the universal
Windows binary depends on the <code>winfsp-x64.dll</code> from the
<a href="https://github.com/winfsp/winfsp">WinFsp</a> project. However, for the
universal binary, the DLL is loaded lazily, so you can still use all
other tools without the DLL.
See the <a href="#windows-support">Windows Support</a> section for more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dependencies</h3><a id="user-content-dependencies" aria-label="Permalink: Dependencies" href="#dependencies"></a></p>
<p dir="auto">DwarFS uses <a href="https://cmake.org/" rel="nofollow">CMake</a> as a build tool.</p>
<p dir="auto">It uses both <a href="https://www.boost.org/" rel="nofollow">Boost</a> and
<a href="https://github.com/facebook/folly">Folly</a>, though the latter is
included as a submodule since very few distributions actually
offer packages for it. Folly itself has a number of dependencies,
so please check <a href="https://github.com/facebook/folly#dependencies">here</a>
for an up-to-date list.</p>
<p dir="auto">It also uses <a href="https://github.com/facebook/fbthrift">Facebook Thrift</a>,
in particular the <code>frozen</code> library, for storing metadata in a highly
space-efficient, memory-mappable and well defined format. It's also
included as a submodule, and we only build the compiler and a very
reduced library that contains just enough for DwarFS to work.</p>
<p dir="auto">Other than that, DwarFS really only depends on FUSE3 and on a set
of compression libraries that Folly already depends on (namely
<a href="https://github.com/lz4/lz4">lz4</a>, <a href="https://github.com/facebook/zstd">zstd</a>
and <a href="https://github.com/kobolabs/liblzma">liblzma</a>).</p>
<p dir="auto">The dependency on <a href="https://github.com/google/googletest">googletest</a>
will be automatically resolved if you build with tests.</p>
<p dir="auto">A good starting point for apt-based systems is probably:</p>
<div data-snippet-clipboard-copy-content="$ apt install \
    gcc \
    g++ \
    clang \
    git \
    ccache \
    ninja-build \
    cmake \
    make \
    bison \
    flex \
    ronn \
    fuse3 \
    pkg-config \
    binutils-dev \
    libacl1-dev \
    libarchive-dev \
    libbenchmark-dev \
    libboost-chrono-dev \
    libboost-context-dev \
    libboost-filesystem-dev \
    libboost-iostreams-dev \
    libboost-program-options-dev \
    libboost-regex-dev \
    libboost-system-dev \
    libboost-thread-dev \
    libbrotli-dev \
    libevent-dev \
    libhowardhinnant-date-dev \
    libjemalloc-dev \
    libdouble-conversion-dev \
    libiberty-dev \
    liblz4-dev \
    liblzma-dev \
    libmagic-dev \
    librange-v3-dev \
    libssl-dev \
    libunwind-dev \
    libdwarf-dev \
    libelf-dev \
    libfmt-dev \
    libfuse3-dev \
    libgoogle-glog-dev \
    libutfcpp-dev \
    libflac++-dev \
    python3-mistletoe"><pre><code>$ apt install \
    gcc \
    g++ \
    clang \
    git \
    ccache \
    ninja-build \
    cmake \
    make \
    bison \
    flex \
    ronn \
    fuse3 \
    pkg-config \
    binutils-dev \
    libacl1-dev \
    libarchive-dev \
    libbenchmark-dev \
    libboost-chrono-dev \
    libboost-context-dev \
    libboost-filesystem-dev \
    libboost-iostreams-dev \
    libboost-program-options-dev \
    libboost-regex-dev \
    libboost-system-dev \
    libboost-thread-dev \
    libbrotli-dev \
    libevent-dev \
    libhowardhinnant-date-dev \
    libjemalloc-dev \
    libdouble-conversion-dev \
    libiberty-dev \
    liblz4-dev \
    liblzma-dev \
    libmagic-dev \
    librange-v3-dev \
    libssl-dev \
    libunwind-dev \
    libdwarf-dev \
    libelf-dev \
    libfmt-dev \
    libfuse3-dev \
    libgoogle-glog-dev \
    libutfcpp-dev \
    libflac++-dev \
    python3-mistletoe
</code></pre></div>
<p dir="auto">Note that when building with <code>gcc</code>, the optimization level will be
set to <code>-O2</code> instead of the CMake default of <code>-O3</code> for release
builds. At least with versions up to <code>gcc-10</code>, the <code>-O3</code> build is
<a href="https://github.com/mhx/dwarfs/issues/14" data-hovercard-type="issue" data-hovercard-url="/mhx/dwarfs/issues/14/hovercard">up to 70% slower</a> than a
build with <code>-O2</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building</h3><a id="user-content-building" aria-label="Permalink: Building" href="#building"></a></p>
<p dir="auto">Firstly, either clone the repository...</p>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs
</code></pre></div>
<p dir="auto">...or unpack the release archive:</p>
<div data-snippet-clipboard-copy-content="$ tar xvf dwarfs-x.y.z.tar.bz2
$ cd dwarfs-x.y.z"><pre><code>$ tar xvf dwarfs-x.y.z.tar.bz2
$ cd dwarfs-x.y.z
</code></pre></div>
<p dir="auto">Once all dependencies have been installed, you can build DwarFS
using:</p>
<div data-snippet-clipboard-copy-content="$ mkdir build
$ cd build
$ cmake .. -DWITH_TESTS=1
$ make -j$(nproc)"><pre><code>$ mkdir build
$ cd build
$ cmake .. -DWITH_TESTS=1
$ make -j$(nproc)
</code></pre></div>
<p dir="auto">You can then run tests with:</p>

<p dir="auto">All binaries use <a href="https://github.com/jemalloc/jemalloc">jemalloc</a>
as a memory allocator by default, as it is typically uses much less
system memory compared to the <code>glibc</code> or <code>tcmalloc</code> allocators.
To disable the use of <code>jemalloc</code>, pass <code>-DUSE_JEMALLOC=0</code> on the
<code>cmake</code> command line.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installing</h3><a id="user-content-installing" aria-label="Permalink: Installing" href="#installing"></a></p>
<p dir="auto">Installing is as easy as:</p>

<p dir="auto">Though you don't have to install the tools to play with them.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Static Builds</h3><a id="user-content-static-builds" aria-label="Permalink: Static Builds" href="#static-builds"></a></p>
<p dir="auto">Attempting to build statically linked binaries is highly discouraged
and not officially supported. That being said, here's how to set up
an environment where you <em>might</em> be able to build static binaries.</p>
<p dir="auto">This has been tested with <code>ubuntu-22.04-live-server-amd64.iso</code>. First,
install all the packages listed as dependencies above. Also install:</p>
<div data-snippet-clipboard-copy-content="$ apt install ccache ninja libacl1-dev"><pre><code>$ apt install ccache ninja libacl1-dev
</code></pre></div>
<p dir="auto"><code>ccache</code> and <code>ninja</code> are optional, but help with a speedy compile.</p>
<p dir="auto">Depending on your distibution, you'll need to build and install static
versions of some libraries, e.g. <code>libarchive</code> and <code>libmagic</code> for Ubuntu:</p>
<div data-snippet-clipboard-copy-content="$ wget https://github.com/libarchive/libarchive/releases/download/v3.6.2/libarchive-3.6.2.tar.xz
$ tar xf libarchive-3.6.2.tar.xz &amp;&amp; cd libarchive-3.6.2
$ ./configure --prefix=/opt/static-libs --without-iconv --without-xml2 --without-expat
$ make &amp;&amp; sudo make install"><pre><code>$ wget https://github.com/libarchive/libarchive/releases/download/v3.6.2/libarchive-3.6.2.tar.xz
$ tar xf libarchive-3.6.2.tar.xz &amp;&amp; cd libarchive-3.6.2
$ ./configure --prefix=/opt/static-libs --without-iconv --without-xml2 --without-expat
$ make &amp;&amp; sudo make install
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ wget ftp://ftp.astron.com/pub/file/file-5.44.tar.gz
$ tar xf file-5.44.tar.gz &amp;&amp; cd file-5.44
$ ./configure --prefix=/opt/static-libs --enable-static=yes --enable-shared=no
$ make &amp;&amp; make install"><pre><code>$ wget ftp://ftp.astron.com/pub/file/file-5.44.tar.gz
$ tar xf file-5.44.tar.gz &amp;&amp; cd file-5.44
$ ./configure --prefix=/opt/static-libs --enable-static=yes --enable-shared=no
$ make &amp;&amp; make install
</code></pre></div>
<p dir="auto">That's it! Now you can try building static binaries for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs &amp;&amp; mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=1 -DSTATIC_BUILD_DO_NOT_USE=1 \
           -DSTATIC_BUILD_EXTRA_PREFIX=/opt/static-libs
$ ninja
$ ninja test"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
$ cd dwarfs &amp;&amp; mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=1 -DSTATIC_BUILD_DO_NOT_USE=1 \
           -DSTATIC_BUILD_EXTRA_PREFIX=/opt/static-libs
$ ninja
$ ninja test
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">Please check out the manual pages for <a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs.md">mkdwarfs</a>,
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">dwarfs</a>, <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsck.md">dwarfsck</a> and
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsextract.md">dwarfsextract</a>. You can also access the manual
pages using the <code>--man</code> option to each binary, e.g.:</p>

<p dir="auto">The <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs.md">dwarfs</a> manual page also shows an example for setting
up DwarFS with <a href="https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt" rel="nofollow">overlayfs</a>
in order to create a writable file system mount on top a read-only
DwarFS image.</p>
<p dir="auto">A description of the DwarFS filesystem format can be found in
<a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfs-format.md">dwarfs-format</a>.</p>
<p dir="auto">A high-level overview of the internal operation of <code>mkdwarfs</code> is shown
in <a href="https://github.com/mhx/dwarfs/blob/main/doc/mkdwarfs-sequence.svg">this sequence diagram</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Windows Support</h2><a id="user-content-windows-support" aria-label="Permalink: Windows Support" href="#windows-support"></a></p>
<p dir="auto">Support for the Windows operating system is currently experimental.
Having worked pretty much exclusively in a Unix world for the past two
decades, my experience with Windows development is rather limited and
I'd expect there to definitely be bugs and rough edges in the Windows
code.</p>
<p dir="auto">The Windows version of the DwarFS filesystem driver relies on the awesome
<a href="https://github.com/winfsp/winfsp">WinFsp</a> project and its <code>winfsp-x64.dll</code>
must be discoverable by the <code>dwarfs.exe</code> driver.</p>
<p dir="auto">The different tools should behave pretty much the same whether you're
using them on Linux or Windows. The file system images can be copied
between Linux and Windows and images created on one OS should work fine
on the other.</p>
<p dir="auto">There are a few things worth pointing out, though:</p>
<ul dir="auto">
<li>
<p dir="auto">DwarFS supports both hardlinks and symlinks on Windows, just as it
does on Linux. However, creating hardlinks and symlinks seems to
require admin privileges on Windows, so if you want to e.g. extract
a DwarFS image that contains links of some sort, you might run into
errors if you don't have the right privileges.</p>
</li>
<li>
<p dir="auto">Due to a <a href="https://github.com/winfsp/winfsp/issues/454" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/454/hovercard">problem</a> in
WinFsp, symlinks cannot currently point outside of the mounted file
system.  Furthermore, due to another
<a href="https://github.com/winfsp/winfsp/issues/530" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/530/hovercard">problem</a> in WinFsp,
symlinks with a drive letter will appear with a mangled target path.</p>
</li>
<li>
<p dir="auto">The DwarFS driver on Windows correctly reports hardlink counts via
its API, but currently these counts are not correctly propagated
to the Windows file system layer. This is presumably due to a
<a href="https://github.com/winfsp/winfsp/issues/511" data-hovercard-type="issue" data-hovercard-url="/winfsp/winfsp/issues/511/hovercard">problem</a> in WinFsp.</p>
</li>
<li>
<p dir="auto">When mounting a DwarFS image on Windows, the mount point must not
exist. This is different from Linux, where the mount point must
actually exist. Also, it's possible to mount a DwarFS image as a
drive letter, e.g.</p>
<p dir="auto">dwarfs.exe image.dwarfs Z:</p>
</li>
<li>
<p dir="auto">Filter rules for <code>mkdwarfs</code> always require Unix path separators,
regardless of whether it's running on Windows or Linux.</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building on Windows</h3><a id="user-content-building-on-windows" aria-label="Permalink: Building on Windows" href="#building-on-windows"></a></p>
<p dir="auto">Building on Windows is not too complicated thanks to <a href="https://vcpkg.io/" rel="nofollow">vcpkg</a>.
You'll need to install:</p>
<ul dir="auto">
<li>
<p dir="auto"><a href="https://visualstudio.microsoft.com/vs/features/cplusplus/" rel="nofollow">Visual Studio and the MSVC C/C++ compiler</a></p>
</li>
<li>
<p dir="auto"><a href="https://git-scm.com/download/win" rel="nofollow">Git</a></p>
</li>
<li>
<p dir="auto"><a href="https://cmake.org/download/" rel="nofollow">CMake</a></p>
</li>
<li>
<p dir="auto"><a href="https://github.com/ninja-build/ninja/releases">Ninja</a></p>
</li>
<li>
<p dir="auto"><a href="https://github.com/winfsp/winfsp/releases">WinFsp</a></p>
</li>
</ul>
<p dir="auto"><code>WinFsp</code> is expected to be installed in <code>C:\Program Files (x68)\WinFsp</code>;
if it's not, you'll need to set <code>WINFSP_PATH</code> when running CMake via
<code>cmake/win.bat</code>.</p>
<p dir="auto">Now you need to clone <code>vcpkg</code> and <code>dwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="> cd %HOMEPATH%
> mkdir git
> cd git
> git clone https://github.com/Microsoft/vcpkg.git
> git clone https://github.com/mhx/dwarfs"><pre><code>&gt; cd %HOMEPATH%
&gt; mkdir git
&gt; cd git
&gt; git clone https://github.com/Microsoft/vcpkg.git
&gt; git clone https://github.com/mhx/dwarfs
</code></pre></div>
<p dir="auto">Then, bootstrap <code>vcpkg</code>:</p>
<div data-snippet-clipboard-copy-content="> .\vcpkg\bootstrap-vcpkg.bat"><pre><code>&gt; .\vcpkg\bootstrap-vcpkg.bat
</code></pre></div>
<p dir="auto">And build DwarFS:</p>
<div data-snippet-clipboard-copy-content="> cd dwarfs
> mkdir build
> cd build
> ..\cmake\win.bat
> ninja"><pre><code>&gt; cd dwarfs
&gt; mkdir build
&gt; cd build
&gt; ..\cmake\win.bat
&gt; ninja
</code></pre></div>
<p dir="auto">Once that's done, you should be able to run the tests.
Set <code>CTEST_PARALLEL_LEVEL</code> according to the number of CPU cores in
your machine.</p>
<div data-snippet-clipboard-copy-content="> set CTEST_PARALLEL_LEVEL=10
> ninja test"><pre><code>&gt; set CTEST_PARALLEL_LEVEL=10
&gt; ninja test
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">macOS Support</h2><a id="user-content-macos-support" aria-label="Permalink: macOS Support" href="#macos-support"></a></p>
<p dir="auto">Support for the macOS operating system is currently experimental.</p>
<p dir="auto">The macOS version of the DwarFS filesystem driver relies on the awesome
<a href="https://https//osxfuse.github.io/" rel="nofollow">macFUSE</a> project.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building on macOS</h3><a id="user-content-building-on-macos" aria-label="Permalink: Building on macOS" href="#building-on-macos"></a></p>
<p dir="auto">Building on macOS involves a few steps, but should be relatively
straightforward:</p>
<ul dir="auto">
<li>
<p dir="auto">Install <a href="https://brew.sh/" rel="nofollow">Homebrew</a></p>
</li>
<li>
<p dir="auto">Use Homebrew to install the necessary dependencies:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="$ brew install cmake ninja ronn macfuse python3 brotli howard-hinnant-date \
               double-conversion fmt glog libarchive libevent flac openssl \
               pkg-config range-v3 utf8cpp xxhash boost zstd jemalloc"><pre><code>$ brew install cmake ninja ronn macfuse python3 brotli howard-hinnant-date \
               double-conversion fmt glog libarchive libevent flac openssl \
               pkg-config range-v3 utf8cpp xxhash boost zstd jemalloc
</code></pre></div>
<ul dir="auto">
<li>
<p dir="auto">When installing macFUSE for the first time, you'll need to explicitly
allow the sofware in <em>System Preferences</em> / <em>Privacy &amp; Security</em>. It's
quite likely that you'll have to reboot after this.</p>
</li>
<li>
<p dir="auto">Clone the DwarFS repository:</p>
</li>
</ul>
<div data-snippet-clipboard-copy-content="$ git clone --recurse-submodules https://github.com/mhx/dwarfs"><pre><code>$ git clone --recurse-submodules https://github.com/mhx/dwarfs
</code></pre></div>
<ul dir="auto">
<li>Prepare the build by installing the <code>mistletoe</code> python module
in a virtualenv:</li>
</ul>
<div data-snippet-clipboard-copy-content="$ cd dwarfs
$ python3 -m venv @buildenv
$ source ./@buildenv/bin/activate
$ pip3 install mistletoe"><pre><code>$ cd dwarfs
$ python3 -m venv @buildenv
$ source ./@buildenv/bin/activate
$ pip3 install mistletoe
</code></pre></div>
<ul dir="auto">
<li>Build DwarFS and run its tests:</li>
</ul>
<div data-snippet-clipboard-copy-content="$ git checkout v0.9.4
$ git submodule update
$ mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=ON
$ ninja
$ export CTEST_PARALLEL_LEVEL=$(sysctl -n hw.logicalcpu)
$ ninja test"><pre><code>$ git checkout v0.9.4
$ git submodule update
$ mkdir build &amp;&amp; cd build
$ cmake .. -GNinja -DWITH_TESTS=ON
$ ninja
$ export CTEST_PARALLEL_LEVEL=$(sysctl -n hw.logicalcpu)
$ ninja test
</code></pre></div>
<ul dir="auto">
<li>Install DwarFS:</li>
</ul>

<p dir="auto">That's it!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use Cases</h2><a id="user-content-use-cases" aria-label="Permalink: Use Cases" href="#use-cases"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Astrophotography</h3><a id="user-content-astrophotography" aria-label="Permalink: Astrophotography" href="#astrophotography"></a></p>
<p dir="auto">Astrophotography can generate huge amounts of raw image data. During a
single night, it's not unlikely to end up with a few dozens of gigabytes
of data. With most dedicated astrophotography cameras, this data ends up
in the form of FITS images. These are usually uncompressed, don't compress
very well with standard compression algorithms, and while there are certain
compressed FITS formats, these aren't widely supported.</p>
<p dir="auto">One of the compression formats (simply called "Rice") compresses reasonably
well and is really fast. However, its implementation for compressed FITS
has a few drawbacks. The most severe drawbacks are that compression isn't
quite as good as it could be for color sensors and sensors with a less than
16 bits of resolution.</p>
<p dir="auto">DwarFS supports the <code>ricepp</code> (Rice++) compression, which builds on the basic
idea of Rice compression, but makes a few enhancements: it compresses color
and low bit depth images significantly better and always searches for the
optimum solution during compression instead of relying on a heuristic.</p>
<p dir="auto">Let's look at an example using 129 images (darks, flats and lights) taken
with an ASI1600MM camera. Each image is 32 MiB, so a total of 4 GiB of data.
Compressing these with the standard <code>fpack</code> tool takes about 16.6 seconds
and yields a total output size of 2.2 GiB:</p>
<div data-snippet-clipboard-copy-content="$ time fpack */*.fit */*/*.fit

user	14.992
system	1.592
total	16.616

$ find . -name '*.fz' -print0 | xargs -0 cat | wc -c
2369943360"><pre><code>$ time fpack */*.fit */*/*.fit

user	14.992
system	1.592
total	16.616

$ find . -name '*.fz' -print0 | xargs -0 cat | wc -c
2369943360
</code></pre></div>
<p dir="auto">However, this leaves you with <code>*.fz</code> files that not every application can
actually read.</p>
<p dir="auto">Using DwarFS, here's what we get:</p>
<div data-snippet-clipboard-copy-content="$ mkdwarfs -i ASI1600 -o asi1600-20.dwarfs -S 20 --categorize
I 08:47:47.459077 scanning &quot;ASI1600&quot;
I 08:47:47.491492 assigning directory and link inodes...
I 08:47:47.491560 waiting for background scanners...
I 08:47:47.675241 scanning CPU time: 1.051s
I 08:47:47.675271 finalizing file inodes...
I 08:47:47.675330 saved 0 B / 3.941 GiB in 0/258 duplicate files
I 08:47:47.675360 assigning device inodes...
I 08:47:47.675371 assigning pipe/socket inodes...
I 08:47:47.675381 building metadata...
I 08:47:47.675393 building blocks...
I 08:47:47.675398 saving names and symlinks...
I 08:47:47.675514 updating name and link indices...
I 08:47:47.675796 waiting for segmenting/blockifying to finish...
I 08:47:50.274285 total ordering CPU time: 616.3us
I 08:47:50.274329 total segmenting CPU time: 1.132s
I 08:47:50.279476 saving chunks...
I 08:47:50.279622 saving directories...
I 08:47:50.279674 saving shared files table...
I 08:47:50.280745 saving names table... [1.047ms]
I 08:47:50.280768 saving symlinks table... [743ns]
I 08:47:50.282031 waiting for compression to finish...
I 08:47:50.823924 compressed 3.941 GiB to 1.201 GiB (ratio=0.304825)
I 08:47:50.824280 compression CPU time: 17.92s
I 08:47:50.824316 filesystem created without errors [3.366s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
5 dirs, 0/0 soft/hard links, 258/258 files, 0 other
original size: 3.941 GiB, hashed: 315.4 KiB (18 files, 0 B/s)
scanned: 3.941 GiB (258 files, 117.1 GiB/s), categorizing: 0 B/s
saved by deduplication: 0 B (0 files), saved by segmenting: 0 B
filesystem: 3.941 GiB in 4037 blocks (4550 chunks, 516/516 fragments, 258 inodes)
compressed filesystem: 4037 blocks/1.201 GiB written"><pre><code>$ mkdwarfs -i ASI1600 -o asi1600-20.dwarfs -S 20 --categorize
I 08:47:47.459077 scanning "ASI1600"
I 08:47:47.491492 assigning directory and link inodes...
I 08:47:47.491560 waiting for background scanners...
I 08:47:47.675241 scanning CPU time: 1.051s
I 08:47:47.675271 finalizing file inodes...
I 08:47:47.675330 saved 0 B / 3.941 GiB in 0/258 duplicate files
I 08:47:47.675360 assigning device inodes...
I 08:47:47.675371 assigning pipe/socket inodes...
I 08:47:47.675381 building metadata...
I 08:47:47.675393 building blocks...
I 08:47:47.675398 saving names and symlinks...
I 08:47:47.675514 updating name and link indices...
I 08:47:47.675796 waiting for segmenting/blockifying to finish...
I 08:47:50.274285 total ordering CPU time: 616.3us
I 08:47:50.274329 total segmenting CPU time: 1.132s
I 08:47:50.279476 saving chunks...
I 08:47:50.279622 saving directories...
I 08:47:50.279674 saving shared files table...
I 08:47:50.280745 saving names table... [1.047ms]
I 08:47:50.280768 saving symlinks table... [743ns]
I 08:47:50.282031 waiting for compression to finish...
I 08:47:50.823924 compressed 3.941 GiB to 1.201 GiB (ratio=0.304825)
I 08:47:50.824280 compression CPU time: 17.92s
I 08:47:50.824316 filesystem created without errors [3.366s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
5 dirs, 0/0 soft/hard links, 258/258 files, 0 other
original size: 3.941 GiB, hashed: 315.4 KiB (18 files, 0 B/s)
scanned: 3.941 GiB (258 files, 117.1 GiB/s), categorizing: 0 B/s
saved by deduplication: 0 B (0 files), saved by segmenting: 0 B
filesystem: 3.941 GiB in 4037 blocks (4550 chunks, 516/516 fragments, 258 inodes)
compressed filesystem: 4037 blocks/1.201 GiB written
</code></pre></div>
<p dir="auto">In less than 3.4 seconds, it compresses the data down to 1.2 GiB, almost
half the size of the <code>fpack</code> output.</p>
<p dir="auto">In addition to saving a lot of disk space, this can also be useful when your
data is stored on a NAS. Here's a comparison of the same set of data accessed
over a 1 Gb/s network connection, first using the uncompressed raw data:</p>
<div data-snippet-clipboard-copy-content="find /mnt/ASI1600 -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 36.0455 s, 117 MB/s"><pre><code>find /mnt/ASI1600 -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 36.0455 s, 117 MB/s
</code></pre></div>
<p dir="auto">And next using a DwarFS image on the same share:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs /mnt/asi1600-20.dwarfs mnt

$ find mnt -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 14.3681 s, 294 MB/s"><pre><code>$ dwarfs /mnt/asi1600-20.dwarfs mnt

$ find mnt -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress
4229012160 bytes (4.2 GB, 3.9 GiB) copied, 14.3681 s, 294 MB/s
</code></pre></div>
<p dir="auto">That's roughly 2.5 times faster. You can very likely see similar results
with slow external hard drives.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Dealing with Bit Rot</h2><a id="user-content-dealing-with-bit-rot" aria-label="Permalink: Dealing with Bit Rot" href="#dealing-with-bit-rot"></a></p>
<p dir="auto">Currently, DwarFS has no built-in ability to add recovery information to a
file system image. However, for archival purposes, it's a good idea to have
such recovery infomation in order to be able to repair a damaged image.</p>
<p dir="auto">This is fortunately relatively straightforward using something like
<a href="https://github.com/Parchive/par2cmdline">par2cmdline</a>:</p>
<div data-snippet-clipboard-copy-content="$ par2create -n1 asi1600-20.dwarfs"><pre><code>$ par2create -n1 asi1600-20.dwarfs
</code></pre></div>
<p dir="auto">This will create two additional files that you can place alongside the image
(or on a different storage), as you'll only need them if DwarFS has detected
an issue with the file system image. If there's an issue, you can run</p>
<div data-snippet-clipboard-copy-content="$ par2repair asi1600-20.dwarfs"><pre><code>$ par2repair asi1600-20.dwarfs
</code></pre></div>
<p dir="auto">which will very likely be able to recover the image if less than 5% (that's
the default used by <code>par2create</code>) of the image are damaged.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Extended Attributes</h2><a id="user-content-extended-attributes" aria-label="Permalink: Extended Attributes" href="#extended-attributes"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Preserving Extended Attributes in DwarFS Images</h3><a id="user-content-preserving-extended-attributes-in-dwarfs-images" aria-label="Permalink: Preserving Extended Attributes in DwarFS Images" href="#preserving-extended-attributes-in-dwarfs-images"></a></p>
<p dir="auto">Extended attributes are not currently supported. Any extended attributes
stored in the source file system will not currently be preserved when
building a DwarFS image using <code>mkdwarfs</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Extended Attributes exposed by the FUSE Driver</h3><a id="user-content-extended-attributes-exposed-by-the-fuse-driver" aria-label="Permalink: Extended Attributes exposed by the FUSE Driver" href="#extended-attributes-exposed-by-the-fuse-driver"></a></p>
<p dir="auto">That being said, the root inode of a mounted DwarFS image currently exposes
one or two extended attributes on Linux:</p>
<div data-snippet-clipboard-copy-content="$ attr -l mnt
Attribute &quot;dwarfs.driver.pid&quot; has a 4 byte value for mnt
Attribute &quot;dwarfs.driver.perfmon&quot; has a 4849 byte value for mnt"><pre><code>$ attr -l mnt
Attribute "dwarfs.driver.pid" has a 4 byte value for mnt
Attribute "dwarfs.driver.perfmon" has a 4849 byte value for mnt
</code></pre></div>
<p dir="auto">The <code>dwarfs.driver.pid</code> attribute simply contains the PID of the DwarFS
FUSE driver. The <code>dwarfs.driver.perfmon</code> attribute contains the current
results of the <a href="#performance-monitoring">performance monitor</a>.</p>
<p dir="auto">Furthermore, each regular file exposes an attribute <code>dwarfs.inodeinfo</code>
with information about the undelying inode:</p>
<div data-snippet-clipboard-copy-content="$ attr -l &quot;05 Disappear.caf&quot;
Attribute &quot;dwarfs.inodeinfo&quot; has a 448 byte value for 05 Disappear.caf"><pre><code>$ attr -l "05 Disappear.caf"
Attribute "dwarfs.inodeinfo" has a 448 byte value for 05 Disappear.caf
</code></pre></div>
<p dir="auto">The attribute contains a JSON object with information about the
underlying inode:</p>
<div data-snippet-clipboard-copy-content="$ attr -qg dwarfs.inodeinfo &quot;05 Disappear.caf&quot;
{
  &quot;chunks&quot;: [
    {
      &quot;block&quot;: 2,
      &quot;category&quot;: &quot;pcmaudio/metadata&quot;,
      &quot;offset&quot;: 270976,
      &quot;size&quot;: 4096
    },
    {
      &quot;block&quot;: 414,
      &quot;category&quot;: &quot;pcmaudio/waveform&quot;,
      &quot;offset&quot;: 37594368,
      &quot;size&quot;: 29514492
    },
    {
      &quot;block&quot;: 419,
      &quot;category&quot;: &quot;pcmaudio/waveform&quot;,
      &quot;offset&quot;: 0,
      &quot;size&quot;: 29385468
    }
  ],
  &quot;gid&quot;: 100,
  &quot;mode&quot;: 33188,
  &quot;modestring&quot;: &quot;----rw-r--r--&quot;,
  &quot;uid&quot;: 1000
}"><pre><code>$ attr -qg dwarfs.inodeinfo "05 Disappear.caf"
{
  "chunks": [
    {
      "block": 2,
      "category": "pcmaudio/metadata",
      "offset": 270976,
      "size": 4096
    },
    {
      "block": 414,
      "category": "pcmaudio/waveform",
      "offset": 37594368,
      "size": 29514492
    },
    {
      "block": 419,
      "category": "pcmaudio/waveform",
      "offset": 0,
      "size": 29385468
    }
  ],
  "gid": 100,
  "mode": 33188,
  "modestring": "----rw-r--r--",
  "uid": 1000
}
</code></pre></div>
<p dir="auto">This is useful, for example, to check how a particular file is spread
across multiple blocks or which categories have been assigned to the
file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comparison</h2><a id="user-content-comparison" aria-label="Permalink: Comparison" href="#comparison"></a></p>
<p dir="auto">The SquashFS, <code>xz</code>, <code>lrzip</code>, <code>zpaq</code> and <code>wimlib</code> tests were all done on
an 8 core Intel(R) Xeon(R) E-2286M CPU @ 2.40GHz with 64 GiB of RAM.</p>
<p dir="auto">The Cromfs and EROFS tests were done with an older version of DwarFS
on a 6 core Intel(R) Xeon(R) CPU D-1528 @ 1.90GHz with 64 GiB of RAM.</p>
<p dir="auto">The systems were mostly idle during all of the tests.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With SquashFS</h3><a id="user-content-with-squashfs" aria-label="Permalink: With SquashFS" href="#with-squashfs"></a></p>
<p dir="auto">The source directory contained <strong>1139 different Perl installations</strong>
from 284 distinct releases, a total of 47.65 GiB of data in 1,927,501
files and 330,733 directories. The source directory was freshly
unpacked from a tar archive to an XFS partition on a 970 EVO Plus 2TB
NVME drive, so most of its contents were likely cached.</p>
<p dir="auto">I'm using the same compression type and compression level for
SquashFS that is the default setting for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on perl-install-zstd.squashfs, block size 131072.
[=========================================================/] 2107401/2107401 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 4637597.63 Kbytes (4528.90 Mbytes)
        9.29% of uncompressed filesystem size (49922299.04 Kbytes)
Inode table size 19100802 bytes (18653.13 Kbytes)
        26.06% of uncompressed inode table size (73307702 bytes)
Directory table size 19128340 bytes (18680.02 Kbytes)
        46.28% of uncompressed directory table size (41335540 bytes)
Number of duplicate files found 1780387
Number of inodes 2255794
Number of files 1925061
Number of fragments 28713
Number of symbolic links  0
Number of device nodes 0
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 330733
Number of ids (unique uids + gids) 2
Number of uids 1
        mhx (1000)
Number of gids 1
        users (100)

real    32m54.713s
user    501m46.382s
sys     0m58.528s"><pre><code>$ time mksquashfs install perl-install.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on perl-install-zstd.squashfs, block size 131072.
[=========================================================/] 2107401/2107401 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 4637597.63 Kbytes (4528.90 Mbytes)
        9.29% of uncompressed filesystem size (49922299.04 Kbytes)
Inode table size 19100802 bytes (18653.13 Kbytes)
        26.06% of uncompressed inode table size (73307702 bytes)
Directory table size 19128340 bytes (18680.02 Kbytes)
        46.28% of uncompressed directory table size (41335540 bytes)
Number of duplicate files found 1780387
Number of inodes 2255794
Number of files 1925061
Number of fragments 28713
Number of symbolic links  0
Number of device nodes 0
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 330733
Number of ids (unique uids + gids) 2
Number of uids 1
        mhx (1000)
Number of gids 1
        users (100)

real    32m54.713s
user    501m46.382s
sys     0m58.528s
</code></pre></div>
<p dir="auto">For DwarFS, I'm sticking to the defaults:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install.dwarfs
I 11:33:33.310931 scanning install
I 11:33:39.026712 waiting for background scanners...
I 11:33:50.681305 assigning directory and link inodes...
I 11:33:50.888441 finding duplicate files...
I 11:34:01.120800 saved 28.2 GiB / 47.65 GiB in 1782826/1927501 duplicate files
I 11:34:01.122608 waiting for inode scanners...
I 11:34:12.839065 assigning device inodes...
I 11:34:12.875520 assigning pipe/socket inodes...
I 11:34:12.910431 building metadata...
I 11:34:12.910524 building blocks...
I 11:34:12.910594 saving names and links...
I 11:34:12.910691 bloom filter size: 32 KiB
I 11:34:12.910760 ordering 144675 inodes using nilsimsa similarity...
I 11:34:12.915555 nilsimsa: depth=20000 (1000), limit=255
I 11:34:13.052525 updating name and link indices...
I 11:34:13.276233 pre-sorted index (660176 name, 366179 path lookups) [360.6ms]
I 11:35:44.039375 144675 inodes ordered [91.13s]
I 11:35:44.041427 waiting for segmenting/blockifying to finish...
I 11:37:38.823902 bloom filter reject rate: 96.017% (TPR=0.244%, lookups=4740563665)
I 11:37:38.823963 segmentation matches: good=454708, bad=6819, total=464247
I 11:37:38.824005 segmentation collisions: L1=0.008%, L2=0.000% [2233254 hashes]
I 11:37:38.824038 saving chunks...
I 11:37:38.860939 saving directories...
I 11:37:41.318747 waiting for compression to finish...
I 11:38:56.046809 compressed 47.65 GiB to 430.9 MiB (ratio=0.00883101)
I 11:38:56.304922 filesystem created without errors [323s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
330733 dirs, 0/2440 soft/hard links, 1927501/1927501 files, 0 other
original size: 47.65 GiB, dedupe: 28.2 GiB (1782826 files), segment: 15.19 GiB
filesystem: 4.261 GiB in 273 blocks (319178 chunks, 144675/144675 inodes)
compressed filesystem: 273 blocks/430.9 MiB written [depth: 20000]
█████████████████████████████████████████████████████████████████████████████▏100% |

real    5m23.030s
user    78m7.554s
sys     1m47.968s"><pre><code>$ time mkdwarfs -i install -o perl-install.dwarfs
I 11:33:33.310931 scanning install
I 11:33:39.026712 waiting for background scanners...
I 11:33:50.681305 assigning directory and link inodes...
I 11:33:50.888441 finding duplicate files...
I 11:34:01.120800 saved 28.2 GiB / 47.65 GiB in 1782826/1927501 duplicate files
I 11:34:01.122608 waiting for inode scanners...
I 11:34:12.839065 assigning device inodes...
I 11:34:12.875520 assigning pipe/socket inodes...
I 11:34:12.910431 building metadata...
I 11:34:12.910524 building blocks...
I 11:34:12.910594 saving names and links...
I 11:34:12.910691 bloom filter size: 32 KiB
I 11:34:12.910760 ordering 144675 inodes using nilsimsa similarity...
I 11:34:12.915555 nilsimsa: depth=20000 (1000), limit=255
I 11:34:13.052525 updating name and link indices...
I 11:34:13.276233 pre-sorted index (660176 name, 366179 path lookups) [360.6ms]
I 11:35:44.039375 144675 inodes ordered [91.13s]
I 11:35:44.041427 waiting for segmenting/blockifying to finish...
I 11:37:38.823902 bloom filter reject rate: 96.017% (TPR=0.244%, lookups=4740563665)
I 11:37:38.823963 segmentation matches: good=454708, bad=6819, total=464247
I 11:37:38.824005 segmentation collisions: L1=0.008%, L2=0.000% [2233254 hashes]
I 11:37:38.824038 saving chunks...
I 11:37:38.860939 saving directories...
I 11:37:41.318747 waiting for compression to finish...
I 11:38:56.046809 compressed 47.65 GiB to 430.9 MiB (ratio=0.00883101)
I 11:38:56.304922 filesystem created without errors [323s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
330733 dirs, 0/2440 soft/hard links, 1927501/1927501 files, 0 other
original size: 47.65 GiB, dedupe: 28.2 GiB (1782826 files), segment: 15.19 GiB
filesystem: 4.261 GiB in 273 blocks (319178 chunks, 144675/144675 inodes)
compressed filesystem: 273 blocks/430.9 MiB written [depth: 20000]
█████████████████████████████████████████████████████████████████████████████▏100% |

real    5m23.030s
user    78m7.554s
sys     1m47.968s
</code></pre></div>
<p dir="auto">So in this comparison, <code>mkdwarfs</code> is <strong>more than 6 times faster</strong> than <code>mksquashfs</code>,
both in terms of CPU time and wall clock time.</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install.*fs
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs"><pre><code>$ ll perl-install.*fs
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
</code></pre></div>
<p dir="auto">In terms of compression ratio, the <strong>DwarFS file system is more than 10 times
smaller than the SquashFS file system</strong>. With DwarFS, the content has been
<strong>compressed down to less than 0.9% (!) of its original size</strong>. This compression
ratio only considers the data stored in the individual files, not the actual
disk space used. On the original XFS file system, according to <code>du</code>, the
source folder uses 52 GiB, so <strong>the DwarFS image actually only uses 0.8% of
the original space</strong>.</p>
<p dir="auto">Here's another comparison using <code>lzma</code> compression instead of <code>zstd</code>:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install-lzma.squashfs -comp lzma

real    13m42.825s
user    205m40.851s
sys     3m29.088s"><pre><code>$ time mksquashfs install perl-install-lzma.squashfs -comp lzma

real    13m42.825s
user    205m40.851s
sys     3m29.088s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install-lzma.dwarfs -l9

real    3m43.937s
user    49m45.295s
sys     1m44.550s"><pre><code>$ time mkdwarfs -i install -o perl-install-lzma.dwarfs -l9

real    3m43.937s
user    49m45.295s
sys     1m44.550s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install-lzma.*fs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-lzma.dwarfs
-rw-r--r-- 1 mhx users 3838406656 Mar  3 20:50 perl-install-lzma.squashfs"><pre><code>$ ll perl-install-lzma.*fs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-lzma.dwarfs
-rw-r--r-- 1 mhx users 3838406656 Mar  3 20:50 perl-install-lzma.squashfs
</code></pre></div>
<p dir="auto">It's immediately obvious that the runs are significantly faster and the
resulting images are significantly smaller. Still, <code>mkdwarfs</code> is about
<strong>4 times faster</strong> and produces and image that's <strong>12 times smaller</strong> than
the SquashFS image. The DwarFS image is only 0.6% of the original file size.</p>
<p dir="auto">So, why not use <code>lzma</code> instead of <code>zstd</code> by default? The reason is that <code>lzma</code>
is about an order of magnitude slower to decompress than <code>zstd</code>. If you're
only accessing data on your compressed filesystem occasionally, this might
not be a big deal, but if you use it extensively, <code>zstd</code> will result in
better performance.</p>
<p dir="auto">The comparisons above are not completely fair. <code>mksquashfs</code> by default
uses a block size of 128KiB, whereas <code>mkdwarfs</code> uses 16MiB blocks by default,
or even 64MiB blocks with <code>-l9</code>. When using identical block sizes for both
file systems, the difference, quite expectedly, becomes a lot less dramatic:</p>
<div data-snippet-clipboard-copy-content="$ time mksquashfs install perl-install-lzma-1M.squashfs -comp lzma -b 1M

real    15m43.319s
user    139m24.533s
sys     0m45.132s"><pre><code>$ time mksquashfs install perl-install-lzma-1M.squashfs -comp lzma -b 1M

real    15m43.319s
user    139m24.533s
sys     0m45.132s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install -o perl-install-lzma-1M.dwarfs -l9 -S20 -B3

real    4m25.973s
user    52m15.100s
sys     7m41.889s"><pre><code>$ time mkdwarfs -i install -o perl-install-lzma-1M.dwarfs -l9 -S20 -B3

real    4m25.973s
user    52m15.100s
sys     7m41.889s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install*.*fs
-rw-r--r-- 1 mhx users  935953866 Mar 13 12:12 perl-install-lzma-1M.dwarfs
-rw-r--r-- 1 mhx users 3407474688 Mar  3 21:54 perl-install-lzma-1M.squashfs"><pre><code>$ ll perl-install*.*fs
-rw-r--r-- 1 mhx users  935953866 Mar 13 12:12 perl-install-lzma-1M.dwarfs
-rw-r--r-- 1 mhx users 3407474688 Mar  3 21:54 perl-install-lzma-1M.squashfs
</code></pre></div>
<p dir="auto">Even this is <em>still</em> not entirely fair, as it uses a feature (<code>-B3</code>) that allows
DwarFS to reference file chunks from up to two previous filesystem blocks.</p>
<p dir="auto">But the point is that this is really where SquashFS tops out, as it doesn't
support larger block sizes or back-referencing. And as you'll see below, the
larger blocks that DwarFS is using by default don't necessarily negatively
impact performance.</p>
<p dir="auto">DwarFS also features an option to recompress an existing file system with
a different compression algorithm. This can be useful as it allows relatively
fast experimentation with different algorithms and options without requiring
a full rebuild of the file system. For example, recompressing the above file
system with the best possible compression (<code>-l 9</code>):</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs --recompress -i perl-install.dwarfs -o perl-lzma-re.dwarfs -l9
I 20:28:03.246534 filesystem rewrittenwithout errors [148.3s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
filesystem: 4.261 GiB in 273 blocks (0 chunks, 0 inodes)
compressed filesystem: 273/273 blocks/372.7 MiB written
████████████████████████████████████████████████████████████████████▏100% \

real    2m28.279s
user    37m8.825s
sys     0m43.256s"><pre><code>$ time mkdwarfs --recompress -i perl-install.dwarfs -o perl-lzma-re.dwarfs -l9
I 20:28:03.246534 filesystem rewrittenwithout errors [148.3s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
filesystem: 4.261 GiB in 273 blocks (0 chunks, 0 inodes)
compressed filesystem: 273/273 blocks/372.7 MiB written
████████████████████████████████████████████████████████████████████▏100% \

real    2m28.279s
user    37m8.825s
sys     0m43.256s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-*.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 390845518 Mar  4 20:28 perl-lzma-re.dwarfs
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-lzma.dwarfs"><pre><code>$ ll perl-*.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users 390845518 Mar  4 20:28 perl-lzma-re.dwarfs
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-lzma.dwarfs
</code></pre></div>
<p dir="auto">Note that while the recompressed filesystem is smaller than the original image,
it is still a lot bigger than the filesystem we previously build with <code>-l9</code>.
The reason is that the recompressed image still uses the same block size, and
the block size cannot be changed by recompressing.</p>
<p dir="auto">In terms of how fast the file system is when using it, a quick test
I've done is to freshly mount the filesystem created above and run
each of the 1139 <code>perl</code> executables to print their version.</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c &quot;umount mnt&quot; -p &quot;umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1&quot; -P procs 5 20 -D 5 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.810 s ±  0.013 s    [User: 1.847 s, System: 0.623 s]
  Range (min … max):    1.788 s …  1.825 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.333 s ±  0.009 s    [User: 1.993 s, System: 0.656 s]
  Range (min … max):    1.321 s …  1.354 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.181 s ±  0.018 s    [User: 2.086 s, System: 0.712 s]
  Range (min … max):    1.165 s …  1.214 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      1.149 s ±  0.015 s    [User: 2.128 s, System: 0.781 s]
  Range (min … max):    1.136 s …  1.186 s    10 runs"><pre><code>$ hyperfine -c "umount mnt" -p "umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1" -P procs 5 20 -D 5 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.810 s ±  0.013 s    [User: 1.847 s, System: 0.623 s]
  Range (min … max):    1.788 s …  1.825 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.333 s ±  0.009 s    [User: 1.993 s, System: 0.656 s]
  Range (min … max):    1.321 s …  1.354 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.181 s ±  0.018 s    [User: 2.086 s, System: 0.712 s]
  Range (min … max):    1.165 s …  1.214 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      1.149 s ±  0.015 s    [User: 2.128 s, System: 0.781 s]
  Range (min … max):    1.136 s …  1.186 s    10 runs
</code></pre></div>
<p dir="auto">These timings are for <em>initial</em> runs on a freshly mounted file system,
running 5, 10, 15 and 20 processes in parallel. 1.1 seconds means that
it takes only about 1 millisecond per Perl binary.</p>
<p dir="auto">Following are timings for <em>subsequent</em> runs, both on DwarFS (at <code>mnt</code>)
and the original XFS (at <code>install</code>). DwarFS is around 15% slower here:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -P procs 10 20 -D 10 -w1 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot; &quot;ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     347.0 ms ±   7.2 ms    [User: 1.755 s, System: 0.452 s]
  Range (min … max):   341.3 ms … 365.2 ms    10 runs

Benchmark #2: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     302.5 ms ±   3.3 ms    [User: 1.656 s, System: 0.377 s]
  Range (min … max):   297.1 ms … 308.7 ms    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     342.2 ms ±   4.1 ms    [User: 1.766 s, System: 0.451 s]
  Range (min … max):   336.0 ms … 349.7 ms    10 runs

Benchmark #4: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     302.0 ms ±   3.0 ms    [User: 1.659 s, System: 0.374 s]
  Range (min … max):   297.0 ms … 305.4 ms    10 runs

Summary
  'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'' ran
    1.00 ± 0.01 times faster than 'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null''
    1.13 ± 0.02 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null''
    1.15 ± 0.03 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null''"><pre><code>$ hyperfine -P procs 10 20 -D 10 -w1 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'" "ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     347.0 ms ±   7.2 ms    [User: 1.755 s, System: 0.452 s]
  Range (min … max):   341.3 ms … 365.2 ms    10 runs

Benchmark #2: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     302.5 ms ±   3.3 ms    [User: 1.656 s, System: 0.377 s]
  Range (min … max):   297.1 ms … 308.7 ms    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     342.2 ms ±   4.1 ms    [User: 1.766 s, System: 0.451 s]
  Range (min … max):   336.0 ms … 349.7 ms    10 runs

Benchmark #4: ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     302.0 ms ±   3.0 ms    [User: 1.659 s, System: 0.374 s]
  Range (min … max):   297.0 ms … 305.4 ms    10 runs

Summary
  'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'' ran
    1.00 ± 0.01 times faster than 'ls -1 install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null''
    1.13 ± 0.02 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null''
    1.15 ± 0.03 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null''
</code></pre></div>
<p dir="auto">Using the lzma-compressed file system, the metrics for <em>initial</em> runs look
considerably worse (about an order of magnitude):</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c &quot;umount mnt&quot; -p &quot;umount mnt; dwarfs perl-install-lzma.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1&quot; -P procs 5 20 -D 5 &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):     10.660 s ±  0.057 s    [User: 1.952 s, System: 0.729 s]
  Range (min … max):   10.615 s … 10.811 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.092 s ±  0.021 s    [User: 1.979 s, System: 0.680 s]
  Range (min … max):    9.059 s …  9.126 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.012 s ±  0.188 s    [User: 2.077 s, System: 0.702 s]
  Range (min … max):    8.839 s …  9.277 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v >/dev/null'
  Time (mean ± σ):      9.004 s ±  0.298 s    [User: 2.134 s, System: 0.736 s]
  Range (min … max):    8.611 s …  9.555 s    10 runs"><pre><code>$ hyperfine -c "umount mnt" -p "umount mnt; dwarfs perl-install-lzma.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1" -P procs 5 20 -D 5 "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P{procs} sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P5 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):     10.660 s ±  0.057 s    [User: 1.952 s, System: 0.729 s]
  Range (min … max):   10.615 s … 10.811 s    10 runs

Benchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P10 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.092 s ±  0.021 s    [User: 1.979 s, System: 0.680 s]
  Range (min … max):    9.059 s …  9.126 s    10 runs

Benchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P15 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.012 s ±  0.188 s    [User: 2.077 s, System: 0.702 s]
  Range (min … max):    8.839 s …  9.277 s    10 runs

Benchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '$0 -v &gt;/dev/null'
  Time (mean ± σ):      9.004 s ±  0.298 s    [User: 2.134 s, System: 0.736 s]
  Range (min … max):    8.611 s …  9.555 s    10 runs
</code></pre></div>
<p dir="auto">So you might want to consider using <code>zstd</code> instead of <code>lzma</code> if you'd
like to optimize for file system performance. It's also the default
compression used by <code>mkdwarfs</code>.</p>
<p dir="auto">Now here's a comparison with the SquashFS filesystem:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c 'sudo umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs-zstd &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot; -p 'sudo umount mnt; sudo mount -t squashfs perl-install.squashfs mnt; sleep 1' -n squashfs-zstd &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: dwarfs-zstd
  Time (mean ± σ):      1.151 s ±  0.015 s    [User: 2.147 s, System: 0.769 s]
  Range (min … max):    1.118 s …  1.174 s    10 runs

Benchmark #2: squashfs-zstd
  Time (mean ± σ):      6.733 s ±  0.007 s    [User: 3.188 s, System: 17.015 s]
  Range (min … max):    6.721 s …  6.743 s    10 runs

Summary
  'dwarfs-zstd' ran
    5.85 ± 0.08 times faster than 'squashfs-zstd'"><pre><code>$ hyperfine -c 'sudo umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs-zstd "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'" -p 'sudo umount mnt; sudo mount -t squashfs perl-install.squashfs mnt; sleep 1' -n squashfs-zstd "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: dwarfs-zstd
  Time (mean ± σ):      1.151 s ±  0.015 s    [User: 2.147 s, System: 0.769 s]
  Range (min … max):    1.118 s …  1.174 s    10 runs

Benchmark #2: squashfs-zstd
  Time (mean ± σ):      6.733 s ±  0.007 s    [User: 3.188 s, System: 17.015 s]
  Range (min … max):    6.721 s …  6.743 s    10 runs

Summary
  'dwarfs-zstd' ran
    5.85 ± 0.08 times faster than 'squashfs-zstd'
</code></pre></div>
<p dir="auto">So, DwarFS is almost six times faster than SquashFS. But what's more,
SquashFS also uses significantly more CPU power. However, the numbers
shown above for DwarFS obviously don't include the time spent in the
<code>dwarfs</code> process, so I repeated the test outside of hyperfine:</p>
<div data-snippet-clipboard-copy-content="$ time dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4 -f

real    0m4.569s
user    0m2.154s
sys     0m1.846s"><pre><code>$ time dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4 -f

real    0m4.569s
user    0m2.154s
sys     0m1.846s
</code></pre></div>
<p dir="auto">So, in total, DwarFS was using 5.7 seconds of CPU time, whereas
SquashFS was using 20.2 seconds, almost four times as much. Ignore
the 'real' time, this is only how long it took me to unmount the
file system again after mounting it.</p>
<p dir="auto">Another real-life test was to build and test a Perl module with 624
different Perl versions in the compressed file system. The module I've
used, <a href="https://github.com/mhx/Tie-Hash-Indexed">Tie::Hash::Indexed</a>,
has an XS component that requires a C compiler to build. So this really
accesses a lot of different stuff in the file system:</p>
<ul dir="auto">
<li>
<p dir="auto">The <code>perl</code> executables and its shared libraries</p>
</li>
<li>
<p dir="auto">The Perl modules used for writing the Makefile</p>
</li>
<li>
<p dir="auto">Perl's C header files used for building the module</p>
</li>
<li>
<p dir="auto">More Perl modules used for running the tests</p>
</li>
</ul>
<p dir="auto">I wrote a little script to be able to run multiple builds in parallel:</p>
<div dir="auto" data-snippet-clipboard-copy-content="#!/bin/bash
set -eu
perl=$1
dir=$(echo &quot;$perl&quot; | cut -d/ --output-delimiter=- -f5,6)
rsync -a Tie-Hash-Indexed/ $dir/
cd $dir
$1 Makefile.PL >/dev/null 2>&amp;1
make test >/dev/null 2>&amp;1
cd ..
rm -rf $dir
echo $perl"><pre><span><span>#!</span>/bin/bash</span>
<span>set</span> -eu
perl=<span>$1</span>
dir=<span><span>$(</span>echo <span><span>"</span><span>$perl</span><span>"</span></span> <span>|</span> cut -d/ --output-delimiter=- -f5,6<span>)</span></span>
rsync -a Tie-Hash-Indexed/ <span>$dir</span>/
<span>cd</span> <span>$dir</span>
<span>$1</span> Makefile.PL <span>&gt;</span>/dev/null <span>2&gt;&amp;1</span>
make <span>test</span> <span>&gt;</span>/dev/null <span>2&gt;&amp;1</span>
<span>cd</span> ..
rm -rf <span>$dir</span>
<span>echo</span> <span>$perl</span></pre></div>
<p dir="auto">The following command will run up to 16 builds in parallel on the 8 core
Xeon CPU, including debug, optimized and threaded versions of all Perl
releases between 5.10.0 and 5.33.3, a total of 624 <code>perl</code> installations:</p>
<div data-snippet-clipboard-copy-content="$ time ls -1 /tmp/perl/install/*/perl-5.??.?/bin/perl5* | sort -t / -k 8 | xargs -d $'\n' -P 16 -n 1 ./build.sh"><pre><code>$ time ls -1 /tmp/perl/install/*/perl-5.??.?/bin/perl5* | sort -t / -k 8 | xargs -d $'\n' -P 16 -n 1 ./build.sh
</code></pre></div>
<p dir="auto">Tests were done with a cleanly mounted file system to make sure the caches
were empty. <code>ccache</code> was primed to make sure all compiler runs could be
satisfied from the cache. With SquashFS, the timing was:</p>
<div data-snippet-clipboard-copy-content="real    0m52.385s
user    8m10.333s
sys     4m10.056s"><pre><code>real    0m52.385s
user    8m10.333s
sys     4m10.056s
</code></pre></div>
<p dir="auto">And with DwarFS:</p>
<div data-snippet-clipboard-copy-content="real    0m50.469s
user    9m22.597s
sys     1m18.469s"><pre><code>real    0m50.469s
user    9m22.597s
sys     1m18.469s
</code></pre></div>
<p dir="auto">So, frankly, not much of a difference, with DwarFS being just a bit faster.
The <code>dwarfs</code> process itself used:</p>
<div data-snippet-clipboard-copy-content="real    0m56.686s
user    0m18.857s
sys     0m21.058s"><pre><code>real    0m56.686s
user    0m18.857s
sys     0m21.058s
</code></pre></div>
<p dir="auto">So again, DwarFS used less raw CPU power overall, but in terms of wallclock
time, the difference is really marginal.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With SquashFS &amp; xz</h3><a id="user-content-with-squashfs--xz" aria-label="Permalink: With SquashFS &amp; xz" href="#with-squashfs--xz"></a></p>
<p dir="auto">This test uses slightly less pathological input data: the root filesystem of
a recent Raspberry Pi OS release. This file system also contains device inodes,
so in order to preserve those, we pass <code>--with-devices</code> to <code>mkdwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mkdwarfs -i raspbian -o raspbian.dwarfs --with-devices
I 21:30:29.812562 scanning raspbian
I 21:30:29.908984 waiting for background scanners...
I 21:30:30.217446 assigning directory and link inodes...
I 21:30:30.221941 finding duplicate files...
I 21:30:30.288099 saved 31.05 MiB / 1007 MiB in 1617/34582 duplicate files
I 21:30:30.288143 waiting for inode scanners...
I 21:30:31.393710 assigning device inodes...
I 21:30:31.394481 assigning pipe/socket inodes...
I 21:30:31.395196 building metadata...
I 21:30:31.395230 building blocks...
I 21:30:31.395291 saving names and links...
I 21:30:31.395374 ordering 32965 inodes using nilsimsa similarity...
I 21:30:31.396254 nilsimsa: depth=20000 (1000), limit=255
I 21:30:31.407967 pre-sorted index (46431 name, 2206 path lookups) [11.66ms]
I 21:30:31.410089 updating name and link indices...
I 21:30:38.178505 32965 inodes ordered [6.783s]
I 21:30:38.179417 waiting for segmenting/blockifying to finish...
I 21:31:06.248304 saving chunks...
I 21:31:06.251998 saving directories...
I 21:31:06.402559 waiting for compression to finish...
I 21:31:16.425563 compressed 1007 MiB to 287 MiB (ratio=0.285036)
I 21:31:16.464772 filesystem created without errors [46.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
4435 dirs, 5908/0 soft/hard links, 34582/34582 files, 7 other
original size: 1007 MiB, dedupe: 31.05 MiB (1617 files), segment: 47.23 MiB
filesystem: 928.4 MiB in 59 blocks (38890 chunks, 32965/32965 inodes)
compressed filesystem: 59 blocks/287 MiB written [depth: 20000]
████████████████████████████████████████████████████████████████████▏100% |

real    0m46.711s
user    10m39.038s
sys     0m8.123s"><pre><code>$ time sudo mkdwarfs -i raspbian -o raspbian.dwarfs --with-devices
I 21:30:29.812562 scanning raspbian
I 21:30:29.908984 waiting for background scanners...
I 21:30:30.217446 assigning directory and link inodes...
I 21:30:30.221941 finding duplicate files...
I 21:30:30.288099 saved 31.05 MiB / 1007 MiB in 1617/34582 duplicate files
I 21:30:30.288143 waiting for inode scanners...
I 21:30:31.393710 assigning device inodes...
I 21:30:31.394481 assigning pipe/socket inodes...
I 21:30:31.395196 building metadata...
I 21:30:31.395230 building blocks...
I 21:30:31.395291 saving names and links...
I 21:30:31.395374 ordering 32965 inodes using nilsimsa similarity...
I 21:30:31.396254 nilsimsa: depth=20000 (1000), limit=255
I 21:30:31.407967 pre-sorted index (46431 name, 2206 path lookups) [11.66ms]
I 21:30:31.410089 updating name and link indices...
I 21:30:38.178505 32965 inodes ordered [6.783s]
I 21:30:38.179417 waiting for segmenting/blockifying to finish...
I 21:31:06.248304 saving chunks...
I 21:31:06.251998 saving directories...
I 21:31:06.402559 waiting for compression to finish...
I 21:31:16.425563 compressed 1007 MiB to 287 MiB (ratio=0.285036)
I 21:31:16.464772 filesystem created without errors [46.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
4435 dirs, 5908/0 soft/hard links, 34582/34582 files, 7 other
original size: 1007 MiB, dedupe: 31.05 MiB (1617 files), segment: 47.23 MiB
filesystem: 928.4 MiB in 59 blocks (38890 chunks, 32965/32965 inodes)
compressed filesystem: 59 blocks/287 MiB written [depth: 20000]
████████████████████████████████████████████████████████████████████▏100% |

real    0m46.711s
user    10m39.038s
sys     0m8.123s
</code></pre></div>
<p dir="auto">Again, SquashFS uses the same compression options:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mksquashfs raspbian raspbian.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on raspbian.squashfs, block size 131072.
[===============================================================\] 39232/39232 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 371934.50 Kbytes (363.22 Mbytes)
        35.98% of uncompressed filesystem size (1033650.60 Kbytes)
Inode table size 399913 bytes (390.54 Kbytes)
        26.53% of uncompressed inode table size (1507581 bytes)
Directory table size 408749 bytes (399.17 Kbytes)
        42.31% of uncompressed directory table size (966174 bytes)
Number of duplicate files found 1618
Number of inodes 44932
Number of files 34582
Number of fragments 3290
Number of symbolic links  5908
Number of device nodes 7
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 4435
Number of ids (unique uids + gids) 18
Number of uids 5
        root (0)
        mhx (1000)
        unknown (103)
        shutdown (6)
        unknown (106)
Number of gids 15
        root (0)
        unknown (109)
        unknown (42)
        unknown (1000)
        users (100)
        unknown (43)
        tty (5)
        unknown (108)
        unknown (111)
        unknown (110)
        unknown (50)
        mail (12)
        nobody (65534)
        adm (4)
        mem (8)

real    0m50.124s
user    9m41.708s
sys     0m1.727s"><pre><code>$ time sudo mksquashfs raspbian raspbian.squashfs -comp zstd -Xcompression-level 22
Parallel mksquashfs: Using 16 processors
Creating 4.0 filesystem on raspbian.squashfs, block size 131072.
[===============================================================\] 39232/39232 100%

Exportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072
        compressed data, compressed metadata, compressed fragments,
        compressed xattrs, compressed ids
        duplicates are removed
Filesystem size 371934.50 Kbytes (363.22 Mbytes)
        35.98% of uncompressed filesystem size (1033650.60 Kbytes)
Inode table size 399913 bytes (390.54 Kbytes)
        26.53% of uncompressed inode table size (1507581 bytes)
Directory table size 408749 bytes (399.17 Kbytes)
        42.31% of uncompressed directory table size (966174 bytes)
Number of duplicate files found 1618
Number of inodes 44932
Number of files 34582
Number of fragments 3290
Number of symbolic links  5908
Number of device nodes 7
Number of fifo nodes 0
Number of socket nodes 0
Number of directories 4435
Number of ids (unique uids + gids) 18
Number of uids 5
        root (0)
        mhx (1000)
        unknown (103)
        shutdown (6)
        unknown (106)
Number of gids 15
        root (0)
        unknown (109)
        unknown (42)
        unknown (1000)
        users (100)
        unknown (43)
        tty (5)
        unknown (108)
        unknown (111)
        unknown (110)
        unknown (50)
        mail (12)
        nobody (65534)
        adm (4)
        mem (8)

real    0m50.124s
user    9m41.708s
sys     0m1.727s
</code></pre></div>
<p dir="auto">The difference in speed is almost negligible. SquashFS is just a bit
slower here. In terms of compression, the difference also isn't huge:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian.* *.xz
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
-rw-r--r-- 1 root root  287M Mar  4 21:31 raspbian.dwarfs
-rw-r--r-- 1 root root  364M Mar  4 21:33 raspbian.squashfs"><pre><code>$ ls -lh raspbian.* *.xz
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
-rw-r--r-- 1 root root  287M Mar  4 21:31 raspbian.dwarfs
-rw-r--r-- 1 root root  364M Mar  4 21:33 raspbian.squashfs
</code></pre></div>
<p dir="auto">Interestingly, <code>xz</code> actually can't compress the whole original image
better than DwarFS.</p>
<p dir="auto">We can even again try to increase the DwarFS compression level:</p>
<div data-snippet-clipboard-copy-content="$ time sudo mkdwarfs -i raspbian -o raspbian-9.dwarfs --with-devices -l9

real    0m54.161s
user    8m40.109s
sys     0m7.101s"><pre><code>$ time sudo mkdwarfs -i raspbian -o raspbian-9.dwarfs --with-devices -l9

real    0m54.161s
user    8m40.109s
sys     0m7.101s
</code></pre></div>
<p dir="auto">Now that actually gets the DwarFS image size well below that of the
<code>xz</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian-9.dwarfs *.xz
-rw-r--r-- 1 root root  244M Mar  4 21:36 raspbian-9.dwarfs
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz"><pre><code>$ ls -lh raspbian-9.dwarfs *.xz
-rw-r--r-- 1 root root  244M Mar  4 21:36 raspbian-9.dwarfs
-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz
</code></pre></div>
<p dir="auto">Even if you actually build a tarball and compress that (instead of
compressing the EXT4 file system itself), <code>xz</code> isn't quite able to
match the DwarFS image size:</p>
<div data-snippet-clipboard-copy-content="$ time sudo tar cf - raspbian | xz -9 -vT 0 >raspbian.tar.xz
  100 %     246.9 MiB / 1,037.2 MiB = 0.238    13 MiB/s       1:18

real    1m18.226s
user    6m35.381s
sys     0m2.205s"><pre><code>$ time sudo tar cf - raspbian | xz -9 -vT 0 &gt;raspbian.tar.xz
  100 %     246.9 MiB / 1,037.2 MiB = 0.238    13 MiB/s       1:18

real    1m18.226s
user    6m35.381s
sys     0m2.205s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz"><pre><code>$ ls -lh raspbian.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
</code></pre></div>
<p dir="auto">DwarFS also comes with the <a href="https://github.com/mhx/dwarfs/blob/main/doc/dwarfsextract.md">dwarfsextract</a> tool
that allows extraction of a filesystem image without the FUSE driver.
So here's a comparison of the extraction speed:</p>
<div data-snippet-clipboard-copy-content="$ time sudo tar xf raspbian.tar.xz -C out1

real    0m12.846s
user    0m12.313s
sys     0m1.616s"><pre><code>$ time sudo tar xf raspbian.tar.xz -C out1

real    0m12.846s
user    0m12.313s
sys     0m1.616s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time sudo dwarfsextract -i raspbian-9.dwarfs -o out2

real    0m3.825s
user    0m13.234s
sys     0m1.382s"><pre><code>$ time sudo dwarfsextract -i raspbian-9.dwarfs -o out2

real    0m3.825s
user    0m13.234s
sys     0m1.382s
</code></pre></div>
<p dir="auto">So, <code>dwarfsextract</code> is almost 4 times faster thanks to using multiple
worker threads for decompression. It's writing about 300 MiB/s in this
example.</p>
<p dir="auto">Another nice feature of <code>dwarfsextract</code> is that it allows you to directly
output data in an archive format, so you could create a tarball from
your image without extracting the files to disk:</p>
<div data-snippet-clipboard-copy-content="$ dwarfsextract -i raspbian-9.dwarfs -f ustar | xz -9 -T0 >raspbian2.tar.xz"><pre><code>$ dwarfsextract -i raspbian-9.dwarfs -f ustar | xz -9 -T0 &gt;raspbian2.tar.xz
</code></pre></div>
<p dir="auto">This has the interesting side-effect that the resulting tarball will
likely be smaller than the one built straight from the directory:</p>
<div data-snippet-clipboard-copy-content="$ ls -lh raspbian*.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
-rw-r--r-- 1 mhx users 240M Mar  4 23:52 raspbian2.tar.xz"><pre><code>$ ls -lh raspbian*.tar.xz
-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz
-rw-r--r-- 1 mhx users 240M Mar  4 23:52 raspbian2.tar.xz
</code></pre></div>
<p dir="auto">That's because <code>dwarfsextract</code> writes files in inode-order, and by
default inodes are ordered by similarity for the best possible
compression.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With lrzip</h3><a id="user-content-with-lrzip" aria-label="Permalink: With lrzip" href="#with-lrzip"></a></p>
<p dir="auto"><a href="https://github.com/ckolivas/lrzip">lrzip</a> is a compression utility
targeted especially at compressing large files. From its description,
it looks like it does something very similar to DwarFS, i.e. it looks
for duplicate segments before passsing the de-duplicated data on to
an <code>lzma</code> compressor.</p>
<p dir="auto">When I first read about <code>lrzip</code>, I was pretty certain it would easily
beat DwarFS. So let's take a look. <code>lrzip</code> operates on a single file,
so it's necessary to first build a tarball:</p>
<div data-snippet-clipboard-copy-content="$ time tar cf perl-install.tar install

real    2m9.568s
user    0m3.757s
sys     0m26.623s"><pre><code>$ time tar cf perl-install.tar install

real    2m9.568s
user    0m3.757s
sys     0m26.623s
</code></pre></div>
<p dir="auto">Now we can run <code>lrzip</code>:</p>
<div data-snippet-clipboard-copy-content="$ time lrzip -vL9 -o perl-install.tar.lrzip perl-install.tar
The following options are in effect for this COMPRESSION.
Threading is ENABLED. Number of CPUs detected: 16
Detected 67106172928 bytes ram
Compression level 9
Nice Value: 19
Show Progress
Verbose
Output Filename Specified: perl-install.tar.lrzip
Temporary Directory set as: ./
Compression mode is: LZMA. LZO Compressibility testing enabled
Heuristically Computed Compression Window: 426 = 42600MB
File size: 52615639040
Will take 2 passes
Beginning rzip pre-processing phase
Beginning rzip pre-processing phase
perl-install.tar - Compression Ratio: 100.378. Average Compression Speed: 14.536MB/s.
Total time: 00:57:32.47

real    57m32.472s
user    81m44.104s
sys     4m50.221s"><pre><code>$ time lrzip -vL9 -o perl-install.tar.lrzip perl-install.tar
The following options are in effect for this COMPRESSION.
Threading is ENABLED. Number of CPUs detected: 16
Detected 67106172928 bytes ram
Compression level 9
Nice Value: 19
Show Progress
Verbose
Output Filename Specified: perl-install.tar.lrzip
Temporary Directory set as: ./
Compression mode is: LZMA. LZO Compressibility testing enabled
Heuristically Computed Compression Window: 426 = 42600MB
File size: 52615639040
Will take 2 passes
Beginning rzip pre-processing phase
Beginning rzip pre-processing phase
perl-install.tar - Compression Ratio: 100.378. Average Compression Speed: 14.536MB/s.
Total time: 00:57:32.47

real    57m32.472s
user    81m44.104s
sys     4m50.221s
</code></pre></div>
<p dir="auto">That definitely took a while. This is about an order of magnitude
slower than <code>mkdwarfs</code> and it barely makes use of the 8 cores.</p>
<div data-snippet-clipboard-copy-content="$ ll -h perl-install.tar.lrzip
-rw-r--r-- 1 mhx users 500M Mar  6 21:16 perl-install.tar.lrzip"><pre><code>$ ll -h perl-install.tar.lrzip
-rw-r--r-- 1 mhx users 500M Mar  6 21:16 perl-install.tar.lrzip
</code></pre></div>
<p dir="auto">This is a surprisingly disappointing result. The archive is 65% larger
than a DwarFS image at <code>-l9</code> that takes less than 4 minutes to build.
Also, you can't just access the files in the <code>.lrzip</code> without fully
unpacking the archive first.</p>
<p dir="auto">That being said, it <em>is</em> better than just using <code>xz</code> on the tarball:</p>
<div data-snippet-clipboard-copy-content="$ time xz -T0 -v9 -c perl-install.tar >perl-install.tar.xz
perl-install.tar (1/1)
  100 %      4,317.0 MiB / 49.0 GiB = 0.086    24 MiB/s      34:55

real    34m55.450s
user    543m50.810s
sys     0m26.533s"><pre><code>$ time xz -T0 -v9 -c perl-install.tar &gt;perl-install.tar.xz
perl-install.tar (1/1)
  100 %      4,317.0 MiB / 49.0 GiB = 0.086    24 MiB/s      34:55

real    34m55.450s
user    543m50.810s
sys     0m26.533s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install.tar.xz -h
-rw-r--r-- 1 mhx users 4.3G Mar  6 22:59 perl-install.tar.xz"><pre><code>$ ll perl-install.tar.xz -h
-rw-r--r-- 1 mhx users 4.3G Mar  6 22:59 perl-install.tar.xz
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">With zpaq</h3><a id="user-content-with-zpaq" aria-label="Permalink: With zpaq" href="#with-zpaq"></a></p>
<p dir="auto"><a href="http://mattmahoney.net/dc/zpaq.html" rel="nofollow">zpaq</a> is a journaling backup
utility and archiver. Again, it appears to share some of the ideas in
DwarFS, like segmentation analysis, but it also adds some features on
top that make it useful for incremental backups. However, it's also
not usable as a file system, so data needs to be extracted before it
can be used.</p>
<p dir="auto">Anyway, how does it fare in terms of speed and compression performance?</p>
<div data-snippet-clipboard-copy-content="$ time zpaq a perl-install.zpaq install -m5"><pre><code>$ time zpaq a perl-install.zpaq install -m5
</code></pre></div>
<p dir="auto">After a few million lines of output that (I think) cannot be turned off:</p>
<div data-snippet-clipboard-copy-content="2258234 +added, 0 -removed.

0.000000 + (51161.953159 -> 8932.000297 -> 490.227707) = 490.227707 MB
2828.082 seconds (all OK)

real    47m8.104s
user    714m44.286s
sys     3m6.751s"><pre><code>2258234 +added, 0 -removed.

0.000000 + (51161.953159 -&gt; 8932.000297 -&gt; 490.227707) = 490.227707 MB
2828.082 seconds (all OK)

real    47m8.104s
user    714m44.286s
sys     3m6.751s
</code></pre></div>
<p dir="auto">So, it's an order of magnitude slower than <code>mkdwarfs</code> and uses 14 times
as much CPU resources as <code>mkdwarfs -l9</code>. The resulting archive it pretty
close in size to the default configuration DwarFS image, but it's more
than 50% bigger than the image produced by <code>mkdwarfs -l9</code>.</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install*.*
-rw-r--r-- 1 mhx users 490227707 Mar  7 01:38 perl-install.zpaq
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs"><pre><code>$ ll perl-install*.*
-rw-r--r-- 1 mhx users 490227707 Mar  7 01:38 perl-install.zpaq
-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs
</code></pre></div>
<p dir="auto">What's <em>really</em> surprising is how slow it is to extract the <code>zpaq</code>
archive again:</p>
<div data-snippet-clipboard-copy-content="$ time zpaq x perl-install.zpaq
2798.097 seconds (all OK)

real    46m38.117s
user    711m18.734s
sys     3m47.876s"><pre><code>$ time zpaq x perl-install.zpaq
2798.097 seconds (all OK)

real    46m38.117s
user    711m18.734s
sys     3m47.876s
</code></pre></div>
<p dir="auto">That's 700 times slower than extracting the DwarFS image.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With zpaqfranz</h3><a id="user-content-with-zpaqfranz" aria-label="Permalink: With zpaqfranz" href="#with-zpaqfranz"></a></p>
<p dir="auto"><a href="https://github.com/fcorbelli/zpaqfranz">zpaqfranz</a> is a derivative of zpaq.
Much to my delight, it doesn't generate millions of lines of output.
It claims to be multi-threaded and de-duplicating, so definitely worth
taking a look. Like zpaq, it supports incremental backups.</p>
<p dir="auto">We'll use a different input to compare zpaqfranz and DwarFS: The source code
of 670 different releases of the "wine" emulator. That's 73 gigabytes of data
in total, spread across slightly more than 3 million files. It's obviously
highly redundant and should thus be a good data set to compare the tools.
For reference, a <code>.tar.xz</code> of the directory is still 7 GiB in size and a
SquashFS image of the data gets down to around 1.6 GiB. An "optimized"
<code>.tar.xz</code>, where the input files were ordered by similarity, compresses down
to 399 MiB, almost 20 times better than without ordering.</p>
<p dir="auto">Now it's time to try zpaqfranz. The input data is stored on a fast SSD and a
large fraction of it is already in the file system cache from previous runs,
so disk I/O is not a bottleneck.</p>
<div data-snippet-clipboard-copy-content="$ time ./zpaqfranz a winesrc.zpaq winesrc
zpaqfranz v58.8k-JIT-L(2023-08-05)
Creating winesrc.zpaq at offset 0 + 0
Add 2024-01-11 07:25:22 3.117.413     69.632.090.852 (  64.85 GB) 16T (362.904 dirs)
3.480.317 +added, 0 -removed.

0 + (69.632.090.852 -> 8.347.553.798 -> 617.600.892) = 617.600.892 @ 58.38 MB/s

1137.441 seconds (000:18:57) (all OK)

real    18m58.632s
user    11m51.052s
sys     1m3.389s"><pre><code>$ time ./zpaqfranz a winesrc.zpaq winesrc
zpaqfranz v58.8k-JIT-L(2023-08-05)
Creating winesrc.zpaq at offset 0 + 0
Add 2024-01-11 07:25:22 3.117.413     69.632.090.852 (  64.85 GB) 16T (362.904 dirs)
3.480.317 +added, 0 -removed.

0 + (69.632.090.852 -&gt; 8.347.553.798 -&gt; 617.600.892) = 617.600.892 @ 58.38 MB/s

1137.441 seconds (000:18:57) (all OK)

real    18m58.632s
user    11m51.052s
sys     1m3.389s
</code></pre></div>
<p dir="auto">That is considerably faster than the original zpaq, and uses about 60 times
less CPU resources. The output file is 589 MiB, so slightly larger than both
the "optimized" <code>.tar.gz</code> and the zpaq output.</p>
<p dir="auto">How does <code>mkdwarfs</code> do?</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i winesrc -o winesrc.dwarfs -l9
[...]
I 07:55:20.546636 compressed 64.85 GiB to 93.2 MiB (ratio=0.00140344)
I 07:55:20.826699 compression CPU time: 6.726m
I 07:55:20.827338 filesystem created without errors [2.283m]
[...]

real    2m17.100s
user    9m53.633s
sys     2m29.236s"><pre><code>$ time mkdwarfs -i winesrc -o winesrc.dwarfs -l9
[...]
I 07:55:20.546636 compressed 64.85 GiB to 93.2 MiB (ratio=0.00140344)
I 07:55:20.826699 compression CPU time: 6.726m
I 07:55:20.827338 filesystem created without errors [2.283m]
[...]

real    2m17.100s
user    9m53.633s
sys     2m29.236s
</code></pre></div>
<p dir="auto">It uses pretty much the same amount of CPU resources, but finishes more than
8 times faster. The DwarFS output file is more than 6 times smaller.</p>
<p dir="auto">You can actually squeeze a bit more redundancy out of the original data by
tweaking the similarity ordering and switching from lzma to brotli compression,
albeit at a somewhat slower compression speed:</p>
<div data-snippet-clipboard-copy-content="mkdwarfs -i winesrc -o winesrc.dwarfs -l9 -C brotli:quality=11:lgwin=26 --order=nilsimsa:max-cluster-size=200k
[...]
I 08:21:01.138075 compressed 64.85 GiB to 73.52 MiB (ratio=0.00110716)
I 08:21:01.485737 compression CPU time: 36.58m
I 08:21:01.486313 filesystem created without errors [5.501m]
[...]
real    5m30.178s
user    40m59.193s
sys     2m36.234s"><pre><code>mkdwarfs -i winesrc -o winesrc.dwarfs -l9 -C brotli:quality=11:lgwin=26 --order=nilsimsa:max-cluster-size=200k
[...]
I 08:21:01.138075 compressed 64.85 GiB to 73.52 MiB (ratio=0.00110716)
I 08:21:01.485737 compression CPU time: 36.58m
I 08:21:01.486313 filesystem created without errors [5.501m]
[...]
real    5m30.178s
user    40m59.193s
sys     2m36.234s
</code></pre></div>
<p dir="auto">That's almost a 1000x reduction in size.</p>
<p dir="auto">Let's also look at decompression speed:</p>
<div data-snippet-clipboard-copy-content="$ time zpaqfranz x winesrc.zpaq
zpaqfranz v58.8k-JIT-L(2023-08-05)
/home/mhx/winesrc.zpaq:
1 versions, 3.480.317 files, 617.600.892 bytes (588.99 MB)
Extract 69.632.090.852 bytes (64.85 GB) in 3.117.413 files (362.904 folders) / 16 T
        99.18% 00:00:00  (  64.32 GB)=>(  64.85 GB)  548.83 MB/sec

125.636 seconds (000:02:05) (all OK)

real    2m6.968s
user    1m36.177s
sys     1m10.980s"><pre><code>$ time zpaqfranz x winesrc.zpaq
zpaqfranz v58.8k-JIT-L(2023-08-05)
/home/mhx/winesrc.zpaq:
1 versions, 3.480.317 files, 617.600.892 bytes (588.99 MB)
Extract 69.632.090.852 bytes (64.85 GB) in 3.117.413 files (362.904 folders) / 16 T
        99.18% 00:00:00  (  64.32 GB)=&gt;(  64.85 GB)  548.83 MB/sec

125.636 seconds (000:02:05) (all OK)

real    2m6.968s
user    1m36.177s
sys     1m10.980s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ time dwarfsextract -i winesrc.dwarfs

real    1m49.182s
user    0m34.667s
sys     1m28.733s"><pre><code>$ time dwarfsextract -i winesrc.dwarfs

real    1m49.182s
user    0m34.667s
sys     1m28.733s
</code></pre></div>
<p dir="auto">Decompression time is pretty much in the same ballpark, with just slightly
shorter times for the DwarFS image.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With wimlib</h3><a id="user-content-with-wimlib" aria-label="Permalink: With wimlib" href="#with-wimlib"></a></p>
<p dir="auto"><a href="https://wimlib.net/" rel="nofollow">wimlib</a> is a really interesting project that is
a lot more mature than DwarFS. While DwarFS at its core has a library
component that could potentially be ported to other operating systems,
wimlib already is available on many platforms. It also seems to have
quite a rich set of features, so it's definitely worth taking a look at.</p>
<p dir="auto">I first tried <code>wimcapture</code> on the perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time wimcapture --unix-data --solid --solid-chunk-size=16M install perl-install.wim
Scanning &quot;install&quot;
47 GiB scanned (1927501 files, 330733 directories)
Using LZMS compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    15m23.310s
user    174m29.274s
sys     0m42.921s"><pre><code>$ time wimcapture --unix-data --solid --solid-chunk-size=16M install perl-install.wim
Scanning "install"
47 GiB scanned (1927501 files, 330733 directories)
Using LZMS compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    15m23.310s
user    174m29.274s
sys     0m42.921s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ll perl-install.*
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
-rw-r--r-- 1 mhx users 1016981520 Mar  6 21:12 perl-install.wim"><pre><code>$ ll perl-install.*
-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs
-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-l9.dwarfs
-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs
-rw-r--r-- 1 mhx users 1016981520 Mar  6 21:12 perl-install.wim
</code></pre></div>
<p dir="auto">So, wimlib is definitely much better than squashfs, in terms of both
compression ratio and speed. DwarFS is however about 3 times faster to
create the file system and the DwarFS file system less than half the size.
When switching to LZMA compression, the DwarFS file system is more than
3 times smaller (wimlib uses LZMS compression by default).</p>
<p dir="auto">What's a bit surprising is that mounting a <em>wim</em> file takes quite a bit
of time:</p>
<div data-snippet-clipboard-copy-content="$ time wimmount perl-install.wim mnt
[WARNING] Mounting a WIM file containing solid-compressed data; file access may be slow.

real    0m2.038s
user    0m1.764s
sys     0m0.242s"><pre><code>$ time wimmount perl-install.wim mnt
[WARNING] Mounting a WIM file containing solid-compressed data; file access may be slow.

real    0m2.038s
user    0m1.764s
sys     0m0.242s
</code></pre></div>
<p dir="auto">Mounting the DwarFS image takes almost no time in comparison:</p>
<div data-snippet-clipboard-copy-content="$ time git/github/dwarfs/build-clang-11/dwarfs perl-install-default.dwarfs mnt
I 00:23:39.238182 dwarfs (v0.4.0, fuse version 35)

real    0m0.003s
user    0m0.003s
sys     0m0.000s"><pre><code>$ time git/github/dwarfs/build-clang-11/dwarfs perl-install-default.dwarfs mnt
I 00:23:39.238182 dwarfs (v0.4.0, fuse version 35)

real    0m0.003s
user    0m0.003s
sys     0m0.000s
</code></pre></div>
<p dir="auto">That's just because it immediately forks into background by default and
initializes the file system in the background. However, even when
running it in the foreground, initializing the file system takes only
about 60 milliseconds:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs perl-install.dwarfs mnt -f
I 00:25:03.186005 dwarfs (v0.4.0, fuse version 35)
I 00:25:03.248061 file system initialized [60.95ms]"><pre><code>$ dwarfs perl-install.dwarfs mnt -f
I 00:25:03.186005 dwarfs (v0.4.0, fuse version 35)
I 00:25:03.248061 file system initialized [60.95ms]
</code></pre></div>
<p dir="auto">If you actually build the DwarFS file system with uncompressed metadata,
mounting is basically instantaneous:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs perl-install-meta.dwarfs mnt -f
I 00:27:52.667026 dwarfs (v0.4.0, fuse version 35)
I 00:27:52.671066 file system initialized [2.879ms]"><pre><code>$ dwarfs perl-install-meta.dwarfs mnt -f
I 00:27:52.667026 dwarfs (v0.4.0, fuse version 35)
I 00:27:52.671066 file system initialized [2.879ms]
</code></pre></div>
<p dir="auto">I've tried running the benchmark where all 1139 <code>perl</code> executables
print their version with the wimlib image, but after about 10 minutes,
it still hadn't finished the first run (with the DwarFS image, one run
took slightly more than 2 seconds). I then tried the following instead:</p>
<div data-snippet-clipboard-copy-content="$ ls -1 /tmp/perl/install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P1 sh -c 'time $0 -v >/dev/null' 2>&amp;1 | grep ^real
real    0m0.802s
real    0m0.652s
real    0m1.677s
real    0m1.973s
real    0m1.435s
real    0m1.879s
real    0m2.003s
real    0m1.695s
real    0m2.343s
real    0m1.899s
real    0m1.809s
real    0m1.790s
real    0m2.115s"><pre><code>$ ls -1 /tmp/perl/install/*/*/bin/perl5* | xargs -d $'\n' -n1 -P1 sh -c 'time $0 -v &gt;/dev/null' 2&gt;&amp;1 | grep ^real
real    0m0.802s
real    0m0.652s
real    0m1.677s
real    0m1.973s
real    0m1.435s
real    0m1.879s
real    0m2.003s
real    0m1.695s
real    0m2.343s
real    0m1.899s
real    0m1.809s
real    0m1.790s
real    0m2.115s
</code></pre></div>
<p dir="auto">Judging from that, it would have probably taken about half an hour
for a single run, which makes at least the <code>--solid</code> wim image pretty
much unusable for actually working with the file system.</p>
<p dir="auto">The <code>--solid</code> option was suggested to me because it resembles the way
that DwarFS actually organizes data internally. However, judging by the
warning when mounting a solid image, it's probably not ideal when using
the image as a mounted file system. So I tried again without <code>--solid</code>:</p>
<div data-snippet-clipboard-copy-content="$ time wimcapture --unix-data install perl-install-nonsolid.wim
Scanning &quot;install&quot;
47 GiB scanned (1927501 files, 330733 directories)
Using LZX compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    8m39.034s
user    64m58.575s
sys     0m32.003s"><pre><code>$ time wimcapture --unix-data install perl-install-nonsolid.wim
Scanning "install"
47 GiB scanned (1927501 files, 330733 directories)
Using LZX compression with 16 threads
Archiving file data: 19 GiB of 19 GiB (100%) done

real    8m39.034s
user    64m58.575s
sys     0m32.003s
</code></pre></div>
<p dir="auto">This is still more than 3 minutes slower than <code>mkdwarfs</code>. However, it
yields an image that's almost 10 times the size of the DwarFS image
and comparable in size to the SquashFS image:</p>
<div data-snippet-clipboard-copy-content="$ ll perl-install-nonsolid.wim -h
-rw-r--r-- 1 mhx users 4.6G Mar  6 23:24 perl-install-nonsolid.wim"><pre><code>$ ll perl-install-nonsolid.wim -h
-rw-r--r-- 1 mhx users 4.6G Mar  6 23:24 perl-install-nonsolid.wim
</code></pre></div>
<p dir="auto">This <em>still</em> takes surprisingly long to mount:</p>
<div data-snippet-clipboard-copy-content="$ time wimmount perl-install-nonsolid.wim mnt

real    0m1.603s
user    0m1.327s
sys     0m0.275s"><pre><code>$ time wimmount perl-install-nonsolid.wim mnt

real    0m1.603s
user    0m1.327s
sys     0m0.275s
</code></pre></div>
<p dir="auto">However, it's really usable as a file system, even though it's about
4-5 times slower than the DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ hyperfine -c 'umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot; -p 'umount mnt; wimmount perl-install-nonsolid.wim mnt; sleep 1' -n wimlib &quot;ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v >/dev/null'&quot;
Benchmark #1: dwarfs
  Time (mean ± σ):      1.149 s ±  0.019 s    [User: 2.147 s, System: 0.739 s]
  Range (min … max):    1.122 s …  1.187 s    10 runs

Benchmark #2: wimlib
  Time (mean ± σ):      7.542 s ±  0.069 s    [User: 2.787 s, System: 0.694 s]
  Range (min … max):    7.490 s …  7.732 s    10 runs

Summary
  'dwarfs' ran
    6.56 ± 0.12 times faster than 'wimlib'"><pre><code>$ hyperfine -c 'umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'" -p 'umount mnt; wimmount perl-install-nonsolid.wim mnt; sleep 1' -n wimlib "ls -1 mnt/*/*/bin/perl5* | xargs -d $'\n' -n1 -P20 sh -c '\$0 -v &gt;/dev/null'"
Benchmark #1: dwarfs
  Time (mean ± σ):      1.149 s ±  0.019 s    [User: 2.147 s, System: 0.739 s]
  Range (min … max):    1.122 s …  1.187 s    10 runs

Benchmark #2: wimlib
  Time (mean ± σ):      7.542 s ±  0.069 s    [User: 2.787 s, System: 0.694 s]
  Range (min … max):    7.490 s …  7.732 s    10 runs

Summary
  'dwarfs' ran
    6.56 ± 0.12 times faster than 'wimlib'
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">With Cromfs</h3><a id="user-content-with-cromfs" aria-label="Permalink: With Cromfs" href="#with-cromfs"></a></p>
<p dir="auto">I used <a href="https://bisqwit.iki.fi/source/cromfs.html" rel="nofollow">Cromfs</a> in the past
for compressed file systems and remember that it did a pretty good job
in terms of compression ratio. But it was never fast. However, I didn't
quite remember just <em>how</em> slow it was until I tried to set up a test.</p>
<p dir="auto">Here's a run on the Perl dataset, with the block size set to 16 MiB to
match the default of DwarFS, and with additional options suggested to
speed up compression:</p>
<div data-snippet-clipboard-copy-content="$ time mkcromfs -f 16777216 -qq -e -r100000 install perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --24bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f3a18259000..0x7f3a1bfffb9c
Root inode spans 0x7f3a205d2948..0x7f3a205d294c
Beginning task for Files and directories: Finding identical blocks
2163608 reuse opportunities found. 561362 unique blocks. Block table will be 79.4% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Blockifying:  0.04% (140017/2724970) idx(siz=80423,del=0) rawin(20.97 MB)rawout(20.97 MB)diff(1956 bytes)
Termination signalled, cleaning up temporaries

real    29m9.634s
user    201m37.816s
sys     2m15.005s"><pre><code>$ time mkcromfs -f 16777216 -qq -e -r100000 install perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --24bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f3a18259000..0x7f3a1bfffb9c
Root inode spans 0x7f3a205d2948..0x7f3a205d294c
Beginning task for Files and directories: Finding identical blocks
2163608 reuse opportunities found. 561362 unique blocks. Block table will be 79.4% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Blockifying:  0.04% (140017/2724970) idx(siz=80423,del=0) rawin(20.97 MB)rawout(20.97 MB)diff(1956 bytes)
Termination signalled, cleaning up temporaries

real    29m9.634s
user    201m37.816s
sys     2m15.005s
</code></pre></div>
<p dir="auto">So, it processed 21 MiB out of 48 GiB in half an hour, using almost
twice as much CPU resources as DwarFS for the <em>whole</em> file system.
At this point I decided it's likely not worth waiting (presumably)
another month (!) for <code>mkcromfs</code> to finish. I double checked that
I didn't accidentally build a debugging version, <code>mkcromfs</code> was
definitely built with <code>-O3</code>.</p>
<p dir="auto">I then tried once more with a smaller version of the Perl dataset.
This only has 20 versions (instead of 1139) of Perl, and obviously
a lot less redundancy:</p>
<div data-snippet-clipboard-copy-content="$ time mkcromfs -f 16777216 -qq -e -r100000 install-small perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --16bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f00e0774000..0x7f00e08410a8
Root inode spans 0x7f00b40048f8..0x7f00b40048fc
Beginning task for Files and directories: Finding identical blocks
25362 reuse opportunities found. 9815 unique blocks. Block table will be 72.1% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Compressing raw rootdir inode (28 bytes)z=982370,del=2) rawin(641.56 MB)rawout(252.72 MB)diff(388.84 MB)
 compressed into 35 bytes
INOTAB pseudo file is 839.85 kB
Inotab inode spans 0x7f00bc036ed8..0x7f00bc036ef4
Beginning task for INOTAB: Finding identical blocks
0 reuse opportunities found. 13 unique blocks. Block table will be 0.0% smaller than without the index search.
Beginning task for INOTAB: Blockifying
mkcromfs: Automatically enabling --packedblocks because it is possible for this filesystem.
Compressing raw inotab inode (52 bytes)
 compressed into 58 bytes
Compressing 9828 block records (4 bytes each, total 39312 bytes)
 compressed into 15890 bytes
Compressing and writing 16 fblocks...

16 fblocks were written: 35.31 MB = 13.90 % of 254.01 MB
Filesystem size: 35.33 MB = 5.50 % of original 642.22 MB
End

real    27m38.833s
user    277m36.208s
sys     11m36.945s"><pre><code>$ time mkcromfs -f 16777216 -qq -e -r100000 install-small perl-install.cromfs
Writing perl-install.cromfs...
mkcromfs: Automatically enabling --16bitblocknums because it seems possible for this filesystem.
Root pseudo file is 108 bytes
Inotab spans 0x7f00e0774000..0x7f00e08410a8
Root inode spans 0x7f00b40048f8..0x7f00b40048fc
Beginning task for Files and directories: Finding identical blocks
25362 reuse opportunities found. 9815 unique blocks. Block table will be 72.1% smaller than without the index search.
Beginning task for Files and directories: Blockifying
Compressing raw rootdir inode (28 bytes)z=982370,del=2) rawin(641.56 MB)rawout(252.72 MB)diff(388.84 MB)
 compressed into 35 bytes
INOTAB pseudo file is 839.85 kB
Inotab inode spans 0x7f00bc036ed8..0x7f00bc036ef4
Beginning task for INOTAB: Finding identical blocks
0 reuse opportunities found. 13 unique blocks. Block table will be 0.0% smaller than without the index search.
Beginning task for INOTAB: Blockifying
mkcromfs: Automatically enabling --packedblocks because it is possible for this filesystem.
Compressing raw inotab inode (52 bytes)
 compressed into 58 bytes
Compressing 9828 block records (4 bytes each, total 39312 bytes)
 compressed into 15890 bytes
Compressing and writing 16 fblocks...

16 fblocks were written: 35.31 MB = 13.90 % of 254.01 MB
Filesystem size: 35.33 MB = 5.50 % of original 642.22 MB
End

real    27m38.833s
user    277m36.208s
sys     11m36.945s
</code></pre></div>
<p dir="auto">And repeating the same task with <code>mkdwarfs</code>:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small.dwarfs
21:13:38.131724 scanning install-small
21:13:38.320139 waiting for background scanners...
21:13:38.727024 assigning directory and link inodes...
21:13:38.731807 finding duplicate files...
21:13:38.832524 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:13:38.832598 waiting for inode scanners...
21:13:39.619963 assigning device inodes...
21:13:39.620855 assigning pipe/socket inodes...
21:13:39.621356 building metadata...
21:13:39.621453 building blocks...
21:13:39.621472 saving names and links...
21:13:39.621655 ordering 3559 inodes using nilsimsa similarity...
21:13:39.622031 nilsimsa: depth=20000, limit=255
21:13:39.629206 updating name and link indices...
21:13:39.630142 pre-sorted index (3360 name, 2127 path lookups) [8.014ms]
21:13:39.752051 3559 inodes ordered [130.3ms]
21:13:39.752101 waiting for segmenting/blockifying to finish...
21:13:53.250951 saving chunks...
21:13:53.251581 saving directories...
21:13:53.303862 waiting for compression to finish...
21:14:11.073273 compressed 611.8 MiB to 24.01 MiB (ratio=0.0392411)
21:14:11.091099 filesystem created without errors [32.96s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 121.5 MiB
filesystem: 222.5 MiB in 14 blocks (7177 chunks, 3559/3559 inodes)
compressed filesystem: 14 blocks/24.01 MiB written
██████████████████████████████████████████████████████████████████████▏100% \

real    0m33.007s
user    3m43.324s
sys     0m4.015s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small.dwarfs
21:13:38.131724 scanning install-small
21:13:38.320139 waiting for background scanners...
21:13:38.727024 assigning directory and link inodes...
21:13:38.731807 finding duplicate files...
21:13:38.832524 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:13:38.832598 waiting for inode scanners...
21:13:39.619963 assigning device inodes...
21:13:39.620855 assigning pipe/socket inodes...
21:13:39.621356 building metadata...
21:13:39.621453 building blocks...
21:13:39.621472 saving names and links...
21:13:39.621655 ordering 3559 inodes using nilsimsa similarity...
21:13:39.622031 nilsimsa: depth=20000, limit=255
21:13:39.629206 updating name and link indices...
21:13:39.630142 pre-sorted index (3360 name, 2127 path lookups) [8.014ms]
21:13:39.752051 3559 inodes ordered [130.3ms]
21:13:39.752101 waiting for segmenting/blockifying to finish...
21:13:53.250951 saving chunks...
21:13:53.251581 saving directories...
21:13:53.303862 waiting for compression to finish...
21:14:11.073273 compressed 611.8 MiB to 24.01 MiB (ratio=0.0392411)
21:14:11.091099 filesystem created without errors [32.96s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 121.5 MiB
filesystem: 222.5 MiB in 14 blocks (7177 chunks, 3559/3559 inodes)
compressed filesystem: 14 blocks/24.01 MiB written
██████████████████████████████████████████████████████████████████████▏100% \

real    0m33.007s
user    3m43.324s
sys     0m4.015s
</code></pre></div>
<p dir="auto">So, <code>mkdwarfs</code> is about 50 times faster than <code>mkcromfs</code> and uses 75 times
less CPU resources. At the same time, the DwarFS file system is 30% smaller:</p>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs"><pre><code>$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs
</code></pre></div>
<p dir="auto">I noticed that the <code>blockifying</code> step that took ages for the full dataset
with <code>mkcromfs</code> ran substantially faster (in terms of MiB/second) on the
smaller dataset, which makes me wonder if there's some quadratic complexity
behaviour that's slowing down <code>mkcromfs</code>.</p>
<p dir="auto">In order to be completely fair, I also ran <code>mkdwarfs</code> with <code>-l 9</code> to enable
LZMA compression (which is what <code>mkcromfs</code> uses by default):</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small-l9.dwarfs -l 9
21:16:21.874975 scanning install-small
21:16:22.092201 waiting for background scanners...
21:16:22.489470 assigning directory and link inodes...
21:16:22.495216 finding duplicate files...
21:16:22.611221 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:16:22.611314 waiting for inode scanners...
21:16:23.394332 assigning device inodes...
21:16:23.395184 assigning pipe/socket inodes...
21:16:23.395616 building metadata...
21:16:23.395676 building blocks...
21:16:23.395685 saving names and links...
21:16:23.395830 ordering 3559 inodes using nilsimsa similarity...
21:16:23.396097 nilsimsa: depth=50000, limit=255
21:16:23.401042 updating name and link indices...
21:16:23.403127 pre-sorted index (3360 name, 2127 path lookups) [6.936ms]
21:16:23.524914 3559 inodes ordered [129ms]
21:16:23.525006 waiting for segmenting/blockifying to finish...
21:16:33.865023 saving chunks...
21:16:33.865883 saving directories...
21:16:33.900140 waiting for compression to finish...
21:17:10.505779 compressed 611.8 MiB to 17.44 MiB (ratio=0.0284969)
21:17:10.526171 filesystem created without errors [48.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 122.2 MiB
filesystem: 221.8 MiB in 4 blocks (7304 chunks, 3559/3559 inodes)
compressed filesystem: 4 blocks/17.44 MiB written
██████████████████████████████████████████████████████████████████████▏100% /

real    0m48.683s
user    2m24.905s
sys     0m3.292s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small-l9.dwarfs -l 9
21:16:21.874975 scanning install-small
21:16:22.092201 waiting for background scanners...
21:16:22.489470 assigning directory and link inodes...
21:16:22.495216 finding duplicate files...
21:16:22.611221 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:16:22.611314 waiting for inode scanners...
21:16:23.394332 assigning device inodes...
21:16:23.395184 assigning pipe/socket inodes...
21:16:23.395616 building metadata...
21:16:23.395676 building blocks...
21:16:23.395685 saving names and links...
21:16:23.395830 ordering 3559 inodes using nilsimsa similarity...
21:16:23.396097 nilsimsa: depth=50000, limit=255
21:16:23.401042 updating name and link indices...
21:16:23.403127 pre-sorted index (3360 name, 2127 path lookups) [6.936ms]
21:16:23.524914 3559 inodes ordered [129ms]
21:16:23.525006 waiting for segmenting/blockifying to finish...
21:16:33.865023 saving chunks...
21:16:33.865883 saving directories...
21:16:33.900140 waiting for compression to finish...
21:17:10.505779 compressed 611.8 MiB to 17.44 MiB (ratio=0.0284969)
21:17:10.526171 filesystem created without errors [48.65s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 122.2 MiB
filesystem: 221.8 MiB in 4 blocks (7304 chunks, 3559/3559 inodes)
compressed filesystem: 4 blocks/17.44 MiB written
██████████████████████████████████████████████████████████████████████▏100% /

real    0m48.683s
user    2m24.905s
sys     0m3.292s
</code></pre></div>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small*.*fs
-rw-r--r-- 1 mhx users 18282075 Dec 10 21:17 perl-install-small-l9.dwarfs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs"><pre><code>$ ls -l perl-install-small*.*fs
-rw-r--r-- 1 mhx users 18282075 Dec 10 21:17 perl-install-small-l9.dwarfs
-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs
-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs
</code></pre></div>
<p dir="auto">It takes about 15 seconds longer to build the DwarFS file system with LZMA
compression (this is still 35 times faster than Cromfs), but reduces the
size even further to make it almost half the size of the Cromfs file system.</p>
<p dir="auto">I would have added some benchmarks with the Cromfs FUSE driver, but sadly
it crashed right upon trying to list the directory after mounting.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With EROFS</h3><a id="user-content-with-erofs" aria-label="Permalink: With EROFS" href="#with-erofs"></a></p>
<p dir="auto"><a href="https://github.com/hsiangkao/erofs-utils">EROFS</a> is a new read-only
compressed file system that has recently been added to the Linux kernel.
Its goals are quite different from those of DwarFS, though. It is
designed to be lightweight (which DwarFS is definitely not) and to run
on constrained hardware like embedded devices or smartphones. It only
supports LZ4 compression.</p>
<p dir="auto">I was feeling lucky and decided to run it on the full Perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time mkfs.erofs perl-install.erofs install -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]
^C

real    912m42.601s
user    903m2.777s
sys     1m52.812s"><pre><code>$ time mkfs.erofs perl-install.erofs install -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]
^C

real    912m42.601s
user    903m2.777s
sys     1m52.812s
</code></pre></div>
<p dir="auto">As you can tell, after more than 15 hours I just gave up. In those
15 hours, <code>mkfs.erofs</code> had produced a 13 GiB output file:</p>
<div data-snippet-clipboard-copy-content="$ ll -h perl-install.erofs
-rw-r--r-- 1 mhx users 13G Dec  9 14:42 perl-install.erofs"><pre><code>$ ll -h perl-install.erofs
-rw-r--r-- 1 mhx users 13G Dec  9 14:42 perl-install.erofs
</code></pre></div>
<p dir="auto">I don't think this would have been very useful to compare with DwarFS.</p>
<p dir="auto">Just as for Cromfs, I re-ran with the smaller Perl dataset:</p>
<div data-snippet-clipboard-copy-content="$ time mkfs.erofs perl-install-small.erofs install-small -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]

real    0m27.844s
user    0m20.570s
sys     0m1.848s"><pre><code>$ time mkfs.erofs perl-install-small.erofs install-small -zlz4hc,9 -d2
mkfs.erofs 1.2
        c_version:           [     1.2]
        c_dbg_lvl:           [       2]
        c_dry_run:           [       0]

real    0m27.844s
user    0m20.570s
sys     0m1.848s
</code></pre></div>
<p dir="auto">That was surprisingly quick, which makes me think that, again, there
might be some accidentally quadratic complexity hiding in <code>mkfs.erofs</code>.
The output file it produced is an order of magnitude larger than the
DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users  26928161 Dec  8 15:05 perl-install-small.dwarfs
-rw-r--r-- 1 mhx users 296488960 Dec  9 14:45 perl-install-small.erofs"><pre><code>$ ls -l perl-install-small.*fs
-rw-r--r-- 1 mhx users  26928161 Dec  8 15:05 perl-install-small.dwarfs
-rw-r--r-- 1 mhx users 296488960 Dec  9 14:45 perl-install-small.erofs
</code></pre></div>
<p dir="auto">Admittedly, this isn't a fair comparison. EROFS has a fixed block size
of 4 KiB, and it uses LZ4 compression. If we tweak DwarFS to the same
parameters, we get:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i install-small -o perl-install-small-lz4.dwarfs -C lz4hc:level=9 -S 12
21:21:18.136796 scanning install-small
21:21:18.376998 waiting for background scanners...
21:21:18.770703 assigning directory and link inodes...
21:21:18.776422 finding duplicate files...
21:21:18.903505 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:21:18.903621 waiting for inode scanners...
21:21:19.676420 assigning device inodes...
21:21:19.677400 assigning pipe/socket inodes...
21:21:19.678014 building metadata...
21:21:19.678101 building blocks...
21:21:19.678116 saving names and links...
21:21:19.678306 ordering 3559 inodes using nilsimsa similarity...
21:21:19.678566 nilsimsa: depth=20000, limit=255
21:21:19.684227 pre-sorted index (3360 name, 2127 path lookups) [5.592ms]
21:21:19.685550 updating name and link indices...
21:21:19.810401 3559 inodes ordered [132ms]
21:21:19.810519 waiting for segmenting/blockifying to finish...
21:21:26.773913 saving chunks...
21:21:26.776832 saving directories...
21:21:26.821085 waiting for compression to finish...
21:21:27.020929 compressed 611.8 MiB to 140.7 MiB (ratio=0.230025)
21:21:27.036202 filesystem created without errors [8.899s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 0 B
filesystem: 344 MiB in 88073 blocks (91628 chunks, 3559/3559 inodes)
compressed filesystem: 88073 blocks/140.7 MiB written
████████████████████████████████████████████████████████████████▏100% |

real    0m9.075s
user    0m37.718s
sys     0m2.427s"><pre><code>$ time mkdwarfs -i install-small -o perl-install-small-lz4.dwarfs -C lz4hc:level=9 -S 12
21:21:18.136796 scanning install-small
21:21:18.376998 waiting for background scanners...
21:21:18.770703 assigning directory and link inodes...
21:21:18.776422 finding duplicate files...
21:21:18.903505 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files
21:21:18.903621 waiting for inode scanners...
21:21:19.676420 assigning device inodes...
21:21:19.677400 assigning pipe/socket inodes...
21:21:19.678014 building metadata...
21:21:19.678101 building blocks...
21:21:19.678116 saving names and links...
21:21:19.678306 ordering 3559 inodes using nilsimsa similarity...
21:21:19.678566 nilsimsa: depth=20000, limit=255
21:21:19.684227 pre-sorted index (3360 name, 2127 path lookups) [5.592ms]
21:21:19.685550 updating name and link indices...
21:21:19.810401 3559 inodes ordered [132ms]
21:21:19.810519 waiting for segmenting/blockifying to finish...
21:21:26.773913 saving chunks...
21:21:26.776832 saving directories...
21:21:26.821085 waiting for compression to finish...
21:21:27.020929 compressed 611.8 MiB to 140.7 MiB (ratio=0.230025)
21:21:27.036202 filesystem created without errors [8.899s]
⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
waiting for block compression to finish
3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other
original size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 0 B
filesystem: 344 MiB in 88073 blocks (91628 chunks, 3559/3559 inodes)
compressed filesystem: 88073 blocks/140.7 MiB written
████████████████████████████████████████████████████████████████▏100% |

real    0m9.075s
user    0m37.718s
sys     0m2.427s
</code></pre></div>
<p dir="auto">It finishes in less than half the time and produces an output image
that's half the size of the EROFS image.</p>
<p dir="auto">I'm going to stop the comparison here, as it's pretty obvious that the
domains in which EROFS and DwarFS are being used have extremely little
overlap. DwarFS will likely never be able to run on embedded devices
and EROFS will likely never be able to achieve the compression ratios
of DwarFS.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">With fuse-archive</h3><a id="user-content-with-fuse-archive" aria-label="Permalink: With fuse-archive" href="#with-fuse-archive"></a></p>
<p dir="auto">I came across <a href="https://github.com/google/fuse-archive">fuse-archive</a>
while looking for FUSE drivers to mount archives and it seems to be
the most versatile of the alternatives (and the one that actually
compiles out of the box).</p>
<p dir="auto">An interesting test case straight from fuse-archive's README is in
the <a href="https://github.com/google/fuse-archive#performance">Performance</a>
section: an archive with a single huge file full of zeroes. Let's
make the example a bit more extreme and use a 1 GiB file instead of
just 256 MiB:</p>
<div data-snippet-clipboard-copy-content="$ mkdir zerotest
$ truncate --size=1G zerotest/zeroes"><pre><code>$ mkdir zerotest
$ truncate --size=1G zerotest/zeroes
</code></pre></div>
<p dir="auto">Now, we build several different archives and a DwarFS image:</p>
<div data-snippet-clipboard-copy-content="$ time mkdwarfs -i zerotest -o zerotest.dwarfs -W16 --log-level=warn --progress=none

real    0m7.604s
user    0m7.521s
sys     0m0.083s

$ time zip -9 zerotest.zip zerotest/zeroes
  adding: zerotest/zeroes (deflated 100%)

real    0m4.923s
user    0m4.840s
sys     0m0.080s

$ time 7z a -bb0 -bd zerotest.7z zerotest/zeroes

7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs Intel(R) Xeon(R) E-2286M  CPU @ 2.40GHz (906ED),ASM,AES-NI)

Scanning the drive:
1 file, 1073741824 bytes (1024 MiB)

Creating archive: zerotest.7z

Items to compress: 1

Files read from disk: 1
Archive size: 157819 bytes (155 KiB)
Everything is Ok

real    0m5.535s
user    0m48.281s
sys     0m1.116s

$ time tar --zstd -cf zerotest.tar.zstd zerotest/zeroes

real    0m0.449s
user    0m0.510s
sys     0m0.610s"><pre><code>$ time mkdwarfs -i zerotest -o zerotest.dwarfs -W16 --log-level=warn --progress=none

real    0m7.604s
user    0m7.521s
sys     0m0.083s

$ time zip -9 zerotest.zip zerotest/zeroes
  adding: zerotest/zeroes (deflated 100%)

real    0m4.923s
user    0m4.840s
sys     0m0.080s

$ time 7z a -bb0 -bd zerotest.7z zerotest/zeroes

7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs Intel(R) Xeon(R) E-2286M  CPU @ 2.40GHz (906ED),ASM,AES-NI)

Scanning the drive:
1 file, 1073741824 bytes (1024 MiB)

Creating archive: zerotest.7z

Items to compress: 1

Files read from disk: 1
Archive size: 157819 bytes (155 KiB)
Everything is Ok

real    0m5.535s
user    0m48.281s
sys     0m1.116s

$ time tar --zstd -cf zerotest.tar.zstd zerotest/zeroes

real    0m0.449s
user    0m0.510s
sys     0m0.610s
</code></pre></div>
<p dir="auto">Turns out that <code>tar --zstd</code> is easily winning the compression speed
test. Looking at the file sizes did actually blow my mind just a bit:</p>
<div data-snippet-clipboard-copy-content="$ ll zerotest.* --sort=size
-rw-r--r-- 1 mhx users 1042231 Jul  1 15:24 zerotest.zip
-rw-r--r-- 1 mhx users  157819 Jul  1 15:26 zerotest.7z
-rw-r--r-- 1 mhx users   33762 Jul  1 15:28 zerotest.tar.zstd
-rw-r--r-- 1 mhx users     848 Jul  1 15:23 zerotest.dwarfs"><pre><code>$ ll zerotest.* --sort=size
-rw-r--r-- 1 mhx users 1042231 Jul  1 15:24 zerotest.zip
-rw-r--r-- 1 mhx users  157819 Jul  1 15:26 zerotest.7z
-rw-r--r-- 1 mhx users   33762 Jul  1 15:28 zerotest.tar.zstd
-rw-r--r-- 1 mhx users     848 Jul  1 15:23 zerotest.dwarfs
</code></pre></div>
<p dir="auto">I definitely didn't expect the DwarFS image to be <em>that</em> small.
Dropping the section index would actually save another 100 bytes.
So, if you want to archive lots of zeroes, DwarFS is your friend.</p>
<p dir="auto">Anyway, let's look at how fast and efficiently the zeroes can
be read from the different archives. First, the <code>zip</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
1020117504 bytes (1.0 GB, 973 MiB) copied, 2 s, 510 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.10309 s, 511 MB/s

real    0m2.104s
user    0m0.264s
sys     0m0.486s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
1020117504 bytes (1.0 GB, 973 MiB) copied, 2 s, 510 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.10309 s, 511 MB/s

real    0m2.104s
user    0m0.264s
sys     0m0.486s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 1.8 seconds and mount time
was in the milliseconds.</p>
<p dir="auto">Now, the <code>7z</code> archive:</p>
<div data-snippet-clipboard-copy-content=" $ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
594759168 bytes (595 MB, 567 MiB) copied, 1 s, 595 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.76904 s, 607 MB/s

real    0m1.772s
user    0m0.229s
sys     0m0.572s"><pre><code> $ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
594759168 bytes (595 MB, 567 MiB) copied, 1 s, 595 MB/s
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.76904 s, 607 MB/s

real    0m1.772s
user    0m0.229s
sys     0m0.572s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 2.9 seconds and mount time
was just over 1.0 seconds.</p>
<p dir="auto">Now, the <code>.tar.zstd</code> archive:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.799409 s, 1.3 GB/s

real    0m0.801s
user    0m0.262s
sys     0m0.537s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.799409 s, 1.3 GB/s

real    0m0.801s
user    0m0.262s
sys     0m0.537s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 0.53 seconds and mount time
was 0.13 seconds.</p>
<p dir="auto">Last but not least, let's look at DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.753 s, 1.4 GB/s

real    0m0.757s
user    0m0.220s
sys     0m0.534s"><pre><code>$ time dd if=mnt/zeroes of=/dev/null status=progress
2097152+0 records in
2097152+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.753 s, 1.4 GB/s

real    0m0.757s
user    0m0.220s
sys     0m0.534s
</code></pre></div>
<p dir="auto">CPU time used by the FUSE driver was 0.17 seconds and mount time
was less than a millisecond.</p>
<p dir="auto">If we increase the block size for the <code>dd</code> command, we can get
even higher throughput. For fuse-archive with the <code>.tar.zstd</code>:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.318682 s, 3.4 GB/s

real    0m0.323s
user    0m0.005s
sys     0m0.154s"><pre><code>$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.318682 s, 3.4 GB/s

real    0m0.323s
user    0m0.005s
sys     0m0.154s
</code></pre></div>
<p dir="auto">And for DwarFS:</p>
<div data-snippet-clipboard-copy-content="$ time dd if=mnt/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.172226 s, 6.2 GB/s

real    0m0.176s
user    0m0.020s
sys     0m0.141s"><pre><code>$ time dd if=mnt/zeroes of=/dev/null status=progress bs=16384
65536+0 records in
65536+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.172226 s, 6.2 GB/s

real    0m0.176s
user    0m0.020s
sys     0m0.141s
</code></pre></div>
<p dir="auto">This is all nice, but what about a more real-life use case?
Let's take the 1.82.0 boost release archives:</p>
<div data-snippet-clipboard-copy-content="$ ll --sort=size boost_1_82_0.*
-rw-r--r-- 1 mhx users 208188085 Apr 10 14:25 boost_1_82_0.zip
-rw-r--r-- 1 mhx users 142580547 Apr 10 14:23 boost_1_82_0.tar.gz
-rw-r--r-- 1 mhx users 121325129 Apr 10 14:23 boost_1_82_0.tar.bz2
-rw-r--r-- 1 mhx users 105901369 Jun 28 12:47 boost_1_82_0.dwarfs
-rw-r--r-- 1 mhx users 103710551 Apr 10 14:25 boost_1_82_0.7z"><pre><code>$ ll --sort=size boost_1_82_0.*
-rw-r--r-- 1 mhx users 208188085 Apr 10 14:25 boost_1_82_0.zip
-rw-r--r-- 1 mhx users 142580547 Apr 10 14:23 boost_1_82_0.tar.gz
-rw-r--r-- 1 mhx users 121325129 Apr 10 14:23 boost_1_82_0.tar.bz2
-rw-r--r-- 1 mhx users 105901369 Jun 28 12:47 boost_1_82_0.dwarfs
-rw-r--r-- 1 mhx users 103710551 Apr 10 14:25 boost_1_82_0.7z
</code></pre></div>
<p dir="auto">Here are the timings for mounting each archive and then using
<code>tar</code> to build another archive from the mountpoint and just counting
the number of bytes in that archive, e.g.:</p>
<div data-snippet-clipboard-copy-content="$ time tar cf - mnt | wc -c
803614720

real    0m4.602s
user    0m0.156s
sys     0m1.123s"><pre><code>$ time tar cf - mnt | wc -c
803614720

real    0m4.602s
user    0m0.156s
sys     0m1.123s
</code></pre></div>
<p dir="auto">Here are the results in terms of wallclock time and FUSE driver
CPU time:</p>
<table>
<thead>
<tr>
<th>Archive</th>
<th>Mount Time</th>
<th><code>tar</code> Wallclock Time</th>
<th>FUSE Driver CPU Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.zip</code></td>
<td>0.458s</td>
<td>5.073s</td>
<td>4.418s</td>
</tr>
<tr>
<td><code>.tar.gz</code></td>
<td>1.391s</td>
<td>3.483s</td>
<td>3.943s</td>
</tr>
<tr>
<td><code>.tar.bz2</code></td>
<td>15.663s</td>
<td>17.942s</td>
<td>32.040s</td>
</tr>
<tr>
<td><code>.7z</code></td>
<td>0.321s</td>
<td>32.554s</td>
<td>31.625s</td>
</tr>
<tr>
<td><code>.dwarfs</code></td>
<td>0.013s</td>
<td>2.974s</td>
<td>1.984s</td>
</tr>
</tbody>
</table>
<p dir="auto">DwarFS easily wins all categories while still compressing the data
almost as well as <code>7z</code>.</p>
<p dir="auto">What about accessing files more randomly?</p>
<div data-snippet-clipboard-copy-content="$ find mnt -type f -print0 | xargs -0 -P32 -n32 cat | dd of=/dev/null status=progress"><pre><code>$ find mnt -type f -print0 | xargs -0 -P32 -n32 cat | dd of=/dev/null status=progress
</code></pre></div>
<p dir="auto">It turns out that fuse-archive grinds to a halt in this case, so I had
to run the test on a subset (the <code>boost</code> subdirectory) of the data.
The <code>.tar.bz2</code> and <code>.7z</code> archives were so slow to read that I stopped
them after a few minutes.</p>
<table>
<thead>
<tr>
<th>Archive</th>
<th>Throughput</th>
<th>Wallclock Time</th>
<th>FUSE Driver CPU Time</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.zip</code></td>
<td>1.8 MB/s</td>
<td>83.245s</td>
<td>83.669s</td>
</tr>
<tr>
<td><code>.tar.gz</code></td>
<td>1.2 MB/s</td>
<td>121.377s</td>
<td>122.711s</td>
</tr>
<tr>
<td><code>.tar.bz2</code></td>
<td>0.2 MB/s</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>.7z</code></td>
<td>0.3 MB/s</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>.dwarfs</code></td>
<td>598.0 MB/s</td>
<td>0.249s</td>
<td>1.099s</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance Monitoring</h2><a id="user-content-performance-monitoring" aria-label="Permalink: Performance Monitoring" href="#performance-monitoring"></a></p>
<p dir="auto">Both the FUSE driver and <code>dwarfsextract</code> by default have support for
simple performance monitoring. You can build binaries without this
feature (<code>-DENABLE_PERFMON=OFF</code>), but impact should be negligible even
if performance monitoring is enabled at run-time.</p>
<p dir="auto">To enable the performance monitor, you pass a list of components for which
you want to collect latency metrics, e.g.:</p>
<div data-snippet-clipboard-copy-content="$ dwarfs test.dwarfs mnt -f -operfmon=fuse"><pre><code>$ dwarfs test.dwarfs mnt -f -operfmon=fuse
</code></pre></div>
<p dir="auto">When the driver exits, you will see output like this:</p>
<div data-snippet-clipboard-copy-content="[fuse.op_read]
      samples: 45145
      overall: 3.214s
  avg latency: 71.2us
  p50 latency: 131.1us
  p90 latency: 131.1us
  p99 latency: 262.1us

[fuse.op_readdir]
      samples: 2
      overall: 51.31ms
  avg latency: 25.65ms
  p50 latency: 32.77us
  p90 latency: 67.11ms
  p99 latency: 67.11ms

[fuse.op_lookup]
      samples: 16
      overall: 19.98ms
  avg latency: 1.249ms
  p50 latency: 2.097ms
  p90 latency: 4.194ms
  p99 latency: 4.194ms

[fuse.op_init]
      samples: 1
      overall: 199.4us
  avg latency: 199.4us
  p50 latency: 262.1us
  p90 latency: 262.1us
  p99 latency: 262.1us

[fuse.op_open]
      samples: 16
      overall: 122.2us
  avg latency: 7.641us
  p50 latency: 4.096us
  p90 latency: 32.77us
  p99 latency: 32.77us

[fuse.op_getattr]
      samples: 1
      overall: 5.786us
  avg latency: 5.786us
  p50 latency: 8.192us
  p90 latency: 8.192us
  p99 latency: 8.192us"><pre><code>[fuse.op_read]
      samples: 45145
      overall: 3.214s
  avg latency: 71.2us
  p50 latency: 131.1us
  p90 latency: 131.1us
  p99 latency: 262.1us

[fuse.op_readdir]
      samples: 2
      overall: 51.31ms
  avg latency: 25.65ms
  p50 latency: 32.77us
  p90 latency: 67.11ms
  p99 latency: 67.11ms

[fuse.op_lookup]
      samples: 16
      overall: 19.98ms
  avg latency: 1.249ms
  p50 latency: 2.097ms
  p90 latency: 4.194ms
  p99 latency: 4.194ms

[fuse.op_init]
      samples: 1
      overall: 199.4us
  avg latency: 199.4us
  p50 latency: 262.1us
  p90 latency: 262.1us
  p99 latency: 262.1us

[fuse.op_open]
      samples: 16
      overall: 122.2us
  avg latency: 7.641us
  p50 latency: 4.096us
  p90 latency: 32.77us
  p99 latency: 32.77us

[fuse.op_getattr]
      samples: 1
      overall: 5.786us
  avg latency: 5.786us
  p50 latency: 8.192us
  p90 latency: 8.192us
  p99 latency: 8.192us
</code></pre></div>
<p dir="auto">The metrics should be self-explanatory. However, note that the
percentile metrics are logarithmically quantized in order to use
as little resources as possible. As a result, you will only see
values that look an awful lot like powers of two.</p>
<p dir="auto">Currently, the supported components are <code>fuse</code> for the FUSE
operations, <code>filesystem_v2</code> for the DwarFS file system component
and <code>inode_reader_v2</code> for the component that handles all <code>read()</code>
system calls.</p>
<p dir="auto">The FUSE driver also exposes the performance monitor metrics via
an <a href="#extended-attributes">extended attribute</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other Obscure Features</h2><a id="user-content-other-obscure-features" aria-label="Permalink: Other Obscure Features" href="#other-obscure-features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Setting Worker Thread CPU Affinity</h3><a id="user-content-setting-worker-thread-cpu-affinity" aria-label="Permalink: Setting Worker Thread CPU Affinity" href="#setting-worker-thread-cpu-affinity"></a></p>
<p dir="auto">This only works on Linux and usually only makes sense if you have CPUs
with different types of cores (e.g. "performance" vs "efficiency" cores)
and are <em>really</em> trying to squeeze the last ounce of speed out of DwarFS.</p>
<p dir="auto">By setting the environment variable <code>DWARFS_WORKER_GROUP_AFFINITY</code>, you
can set the CPU affinity of different worker thread groups, e.g.:</p>
<div data-snippet-clipboard-copy-content="export DWARFS_WORKER_GROUP_AFFINITY=blockify=3:compress=6,7"><pre><code>export DWARFS_WORKER_GROUP_AFFINITY=blockify=3:compress=6,7
</code></pre></div>
<p dir="auto">This will set the affinity of the <code>blockify</code> worker group to CPU 3 and
the affinity of the <code>compress</code> worker group to CPUs 6 and 7.</p>
<p dir="auto">You can use this feature for all tools that use one or more worker thread
groups. For example, the FUSE driver <code>dwarfs</code> and <code>dwarfsextract</code> use a
worker group <code>blkcache</code> that the block cache (i.e. block decompression and
lookup) runs on. <code>mkdwarfs</code> uses a whole array of different worker groups,
namely <code>compress</code> for compression, <code>scanner</code> for scanning, <code>ordering</code> for
input ordering, and <code>blockify</code> for segmenting. <code>blockify</code> is what you would
typically want to run on your "performance" cores.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNS over Wikipedia (117 pts)]]></title>
            <link>https://github.com/aaronjanse/dns-over-wikipedia</link>
            <guid>40008383</guid>
            <pubDate>Fri, 12 Apr 2024 00:52:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/aaronjanse/dns-over-wikipedia">https://github.com/aaronjanse/dns-over-wikipedia</a>, See on <a href="https://news.ycombinator.com/item?id=40008383">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DNS over Wikipedia</h2><a id="user-content-dns-over-wikipedia" aria-label="Permalink: DNS over Wikipedia" href="#dns-over-wikipedia"></a></p>
<p dir="auto">Wikipedia keeps track of official URLs for popular websites. With DNS over Wikipedia installed, domains ending with <code>.idk</code> are redirected by searching Wikipedia and extracting the relevant URL from the infobox.</p>
<p dir="auto">Example:</p>
<ol dir="auto">
<li>Type <code>scihub.idk/</code> in browser address bar</li>
<li>Observe redirect to <code>https://sci-hub.tw</code> (at the time of writing)</li>
</ol>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/aaronjanse/dns-over-wikipedia/blob/master/demo.gif"><img src="https://github.com/aaronjanse/dns-over-wikipedia/raw/master/demo.gif" width="600" data-animated-image=""></a></p>
<blockquote>
<p dir="auto">Instead of googling for the site, I google for the site's Wikipedia article ("schihub wiki") which usually has an up-to-date link to the site in the sidebar, whereas Google is forced to censor their results.</p>
<p dir="auto">If you Google "Piratebay", the first search result is a fake "thepirate-bay.org" (with a dash) but the Wikipedia article lists the right one.
— <a href="https://news.ycombinator.com/item?id=22414031" rel="nofollow">shpx</a></p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation Options</h2><a id="user-content-installation-options" aria-label="Permalink: Installation Options" href="#installation-options"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://chrome.google.com/webstore/detail/mjmjpfncapfopnommmngnmjalkopljji/" rel="nofollow">Chrome Extension</a></h4><a id="user-content-chrome-extension" aria-label="Permalink: Chrome Extension" href="#chrome-extension"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://addons.mozilla.org/en-US/firefox/addon/dns-over-wikipedia/" rel="nofollow">Firefox Extension</a></h4><a id="user-content-firefox-extension" aria-label="Permalink: Firefox Extension" href="#firefox-extension"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/aaronjanse/dns-over-wikipedia/blob/master/hosts-file">(optional) Rust Redirect Script</a></h4><a id="user-content-optional-rust-redirect-script" aria-label="Permalink: (optional) Rust Redirect Script" href="#optional-rust-redirect-script"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Descent: A classic 3-D first-person shooter (2012) (111 pts)]]></title>
            <link>http://insectoid.budwin.net/dos/descent/descent.html</link>
            <guid>40006697</guid>
            <pubDate>Thu, 11 Apr 2024 20:52:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://insectoid.budwin.net/dos/descent/descent.html">http://insectoid.budwin.net/dos/descent/descent.html</a>, See on <a href="https://news.ycombinator.com/item?id=40006697">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="rdescm">
<p>DESCENT is a classic 3-D first-person shooter for DOS that was 
released in 1995, developed by Parallax Software and published by Interplay 
Productions.&nbsp; DESCENT is a contemporary of the other very popular FPS of 
the time, DOOM by id Software, released just over a year before.&nbsp; However, 
it is <em>not</em> based on DOOM; the DESCENT engine works quite differently, 
using 3-D models (rather than sprites) to render enemy robots, and has 
"six degrees of freedom".&nbsp; DESCENT also has very little of the 
blood or gore of DOOM.</p>

<p>There were two games made for the DOS platform, <b>DESCENT</b> (1995) and 
<b>DESCENT II</b> (1996, which was also made for Windows 95), as well a sequel 
for Windows 9x, <b>DESCENT<sup>3</sup></b> (1999).&nbsp; There have been many 
expansions for and re-releases of each; the Game Info pages detail as many of 
those that I know about.</p>

<p>For the moment, the rest of the pages will be concentrating on the first 
mission, <span>DESCENT: First Strike</span>.&nbsp; In the future I 
may make <span>DESCENT II</span> pages; but that remains to be 
seen.</p>

<p><img src="http://insectoid.budwin.net/Images/email.png" alt="Email">Questions or comments about the pages, a specific level, or Descent 
in general may be directed to <b>insectoid</b> <i>(at)</i> <b>budwin</b> 
<i>(dot)</i> <b>net</b>; please put <b>IWP:</b> in the subject line.&nbsp; I do 
appreciate the feedback, folks, both good and bad.&nbsp; However, <em>any spam 
mail will be subject to immediate de-resolution!</em>&nbsp; Requests for copies 
of the full-version games are firmly discouraged and will be treated in like 
manner.</p>

<h2><span><img alt="DESCENT" src="http://insectoid.budwin.net/Images/DOS/Descent/d1-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d1gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and ports to other systems.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1basics.html">Basics</a> – A mini-guide to 
	DESCENT.&nbsp; Under construction.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1levels.html">Walkthrough</a> – A walkthrough of 
	each level.&nbsp; Currently has only a few levels completed.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1robots.html">Robots</a> – Every robot in D1, 
	including the rarely-seen Red Triangle.</li>

	<li><a href="http://insectoid.budwin.net/dos/descent/d1weapons.html">Weapons and Items</a> – All of 
	the power-ups and weapons in D1.</li>
</ul>

<h2><span><img alt="DESCENT II" src="http://insectoid.budwin.net/Images/DOS/Descent/d2-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d2gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and ports to other systems.</li>
</ul>

<h2><span><img alt="DESCENT 3" src="http://insectoid.budwin.net/Images/DOS/Descent/d3-logo.png"></span></h2>

<ul>
	<li><a href="http://insectoid.budwin.net/dos/descent/d3gminfo.html">Game Info</a> – Release data, game 
	features, add-ons, and mods.</li>
</ul>

<hr>

<h4>LEGAL STUFF:</h4>

<p>The beveled panels, buttons, and page design are ©2011 
Thomas Keith / Insectoid.<br>

The typeface for most of the banners and the dark grey sidebar buttons are 
TrueType versions of the fonts from DESCENT II; ©1997 Harald Koenigsperger 
(Wild Style GraphX).<br>

The typeface for the game logos is a reproduction of the font used on the discs 
and packaging for all three games; I'm attributing this to Interplay 
Productions.<br>

DESCENT is a registered trademark of Interplay Productions.<br> 

DESCENT, DESCENT II, and all related content and images are ©1995, 1996 
Parallax Software Corporation.<br>

DXX-Rebirth, D1X-Rebirth and D2X-Rebirth are ©Christian Beckhaeuser.<br>

DESCENT<sup>3</sup> and all related content and images are ©1999 Outrage 
Entertainment.<br>

All other copyrights and trademarks are property of their respective owners.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using ClickHouse to scale an events engine (197 pts)]]></title>
            <link>https://github.com/getlago/lago/wiki/Using-Clickhouse-to-scale-an-events-engine</link>
            <guid>40005005</guid>
            <pubDate>Thu, 11 Apr 2024 18:02:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/getlago/lago/wiki/Using-Clickhouse-to-scale-an-events-engine">https://github.com/getlago/lago/wiki/Using-Clickhouse-to-scale-an-events-engine</a>, See on <a href="https://news.ycombinator.com/item?id=40005005">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wiki-body" data-view-component="true">
                <p>Like many companies, we had to change our database stack midway while scaling our core product Lago, an open-source usage-based billing platform. As we grew more popular, we began ingesting millions of events every minute. And our rudimentary Postgres-only stack wasn’t cutting it. We were suffering heavy load times, impacting our entire app’s performance.</p>
<p>After some exploration, we decided to use a distributed ClickHouse instance strictly for our streamed events. Our analytics services were now able to directly query ClickHouse, an OLAP database. For all other data needs, we kept Postgres.</p>
<p>The strategy was successful. Since the refactor, we haven’t looked back.</p>
<p>Today, we’re going to explore that decision for a hybrid database stack, and more specifically, why we decided to go with ClickHouse.</p>
<p><h2>OLTP versus OLAP databases</h2><a id="user-content-oltp-versus-olap-databases" aria-label="Permalink: OLTP versus OLAP databases" href="#oltp-versus-olap-databases"></a></p>
<p>Most developers, including junior developers, have experience using OLTP (online transactional processing) databases such as Postgres. As the name implies, OLTP databases are designed for processing <em>transactions</em>. A transaction is one of many different types of instructions that software might invoke to a database. The most common are: (i) read, (ii) insert, (iii) update and (iv) delete.</p>
<p>OLTP databases are typically general-purpose databases. Because they support every type of data processing, they could be used for any data problem <em>within limits</em>. And, even at a large scale, they are fantastic for software that require:</p>
<ul>
<li>
<strong>atomic transactions</strong>, where a set of grouped transactions either all occur or don’t occur at all</li>
<li>
<strong>consistency</strong>, where queries in-between writes and updates are deterministic and predictable</li>
</ul>
<p>For most problems, these are important qualities. For some, they are crucial. A banking application can’t have discrepancies whenever money is transferred between accounts. For those problems, an OLTP database is needed for cents-level accuracy.
Today, we still use Postgres as our <strong>primary</strong> database, configured [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/config/database.yml#L5">via our database.yml file</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/config/database.yml#L5">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/config/database.yml#L5</a>). And given that we use Ruby on Rail’s, [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/schema.rb">our Postgres schema</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/schema.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/schema.rb</a>) is automatically generated by Rail’s [<a href="https://guides.rubyonrails.org/active_record_basics.html" rel="nofollow">Active Record</a>](<a href="https://guides.rubyonrails.org/active_record_basics.html" rel="nofollow">https://guides.rubyonrails.org/active_record_basics.html</a>), an ORM that manages our various models such as [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/charge_spec.rb">charges</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/charge_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/charge_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/credit_note_spec.rb">credit notes</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/credit_note_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/credit_note_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invoice_spec.rb">invoices</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invoice_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invoice_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invite_spec.rb">invites</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invite_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invite_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/fee_spec.rb">fees</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/fee_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/fee_spec.rb</a>), [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/coupon_spec.rb">coupons</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/coupon_spec.rb">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/coupon_spec.rb</a>), and [<a href="https://github.com/getlago/lago-api/tree/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models">much, much more</a>](<a href="https://github.com/getlago/lago-api/tree/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models">https://github.com/getlago/lago-api/tree/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models</a>). We write some custom queries given [<a href="https://github.com/getlago/lago/wiki/Is-ORM-still-an-%27anti-pattern%27%3F">the performance limits of the ORM</a>](<a href="https://github.com/getlago/lago/wiki/Is-ORM-still-an-%27anti-pattern%27%3F">https://github.com/getlago/lago/wiki/Is-ORM-still-an-%27anti-pattern%27%3F</a>), but otherwise lean heavily on Active Record for most transactions.</p>
<p>So where do OLAP (online analytical processing) databases like ClickHouse come in? Well, Postgres was designed to be <em>strictly</em> atomic and consistent; two properties that require for data to be fully <strong>ingested</strong> before any query that might process them is run. This creates a problem for tables where entries are ingested in the millions per minutes (e.g. billable events, especially those for infrastructure services like managed servers). Specifically, the issue <em><strong>isn’t</strong></em> ingesting data, but rather simultaneously handling expensive analytical queries without locking up the queue. These data-summarizing problems are where OLAP databases like ClickHouse shine.</p>
<p>OLAP databases are designed for two primary problems—(i) efficiently answering complex read queries with <em><strong>approximate</strong></em> accuracy and (ii) batch processing a large number of write queries. However, OLAP databases are <strong>terrible</strong> for mutating data (where the <strong>entire</strong> database often needs to be re-written) or deleting data.</p>
<p>Different OLAP solutions (e.g. ClickHouse, QuestDB, Druid) have different strengths, and we’ll dive into the specific strain of traits that made ClickHouse a winning solution in the next section. But all OLAP solutions share a common quality—data is stored in an inverted layout relative to OLTP databases like Postgres.
<img width="778" alt="Storage types" src="https://private-user-images.githubusercontent.com/85290767/289697112-9b014845-d4f4-4f54-ac71-bd88af75060c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI4NzMxMDUsIm5iZiI6MTcxMjg3MjgwNSwicGF0aCI6Ii84NTI5MDc2Ny8yODk2OTcxMTItOWIwMTQ4NDUtZDRmNC00ZjU0LWFjNzEtYmQ4OGFmNzUwNjBjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDExVDIyMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWM2ZGQ3Y2E5ODU4MGU2YjAxZGE3OWU0NTdlZWVjN2NkNDRkYjEwNTY1MDYzYzM2NGRlZmI0M2YzY2Q2ZmFhMTEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.HNwTuxD94gYZRubVYuKu0CkxCAKzKPqFo4oekhKT7is" content-type-secured-asset="image/png"></p>
<p>Now, from the user’s standpoint, the table’s columns and rows are still just columns and rows. But, physically in memory, data is scanned column-by-column, not row-by-row.  This makes aggregations—such as adding every value in a certain field—very, very fast, as the relevant data is read sequentially.</p>
<p><h2>Enter ClickHouse, our chosen OLAP solution</h2><a id="user-content-enter-clickhouse-our-chosen-olap-solution" aria-label="Permalink: Enter ClickHouse, our chosen OLAP solution" href="#enter-clickhouse-our-chosen-olap-solution"></a></p>
<p>[<a href="https://clickhouse.com/" rel="nofollow">ClickHouse</a>](<a href="https://clickhouse.com/" rel="nofollow">https://clickhouse.com</a>) is an open-source tool spun out from a closed-source algorithm used by Yandex’s website analytics product. Today, ClickHouse is shepherded by [<a href="https://clickhouse.com/company/our-story" rel="nofollow">ClickHouse Inc</a>](<a href="https://clickhouse.com/company/our-story" rel="nofollow">https://clickhouse.com/company/our-story</a>) with notable contributions by [<a href="https://altinity.com/" rel="nofollow">Altinity</a>](<a href="https://altinity.com/" rel="nofollow">https://altinity.com</a>). To date, it is one of the most successful OLAP databases, both commercially and qualitatively.</p>
<p>ClickHouse has three notable features that make it an analytics powerhouse—(i) dynamic materialized views, (ii) specialized engines, and (iii) vectorized query execution.</p>
<p>To summarize each:</p>
<ul>
<li>
<strong>Dynamic</strong> <strong>Materialized Views.</strong> Materialized Views are query-able views that are generated from raw data in underlying tables. While many databases <strong>do</strong> support materialized views, including Postgres, ClickHouse’s materialized views are dynamic, efficiently refreshing content whenever new content is ingested. These contrasts with ordinary materialized views which are just snapshots of a specific point of time, and are very expensive to refresh.</li>
<li>
<strong>Specialized Engines</strong>. Many databases have a single engine for utilizing hardware to process queries / transactions. ClickHouse, however, has dedicated engines for specific mathematical functions, such as summing or averaging numbers.</li>
<li>
<strong>Vectorized Query Execution</strong>. ClickHouse’s specialized engines leverage vectorized query execution, where the hardware uses multiple units in parallel to achieve a communal result (known as SIMD—Single Instruction, Multiple Data).</li>
</ul>
<p>Combined with its columnar storage, these traits allow ClickHouse to easily sum, average, or generally aggregate database values.</p>
<p>As a caveat, Postgres isn’t <em>entirely</em> incapable of achieving similar results, but only via a bastion of optimizations. For instance, there is a third-party [<a href="https://github.com/citusdata/postgres_vectorization_test">vectorized executor</a>](<a href="https://github.com/citusdata/postgres_vectorization_test">https://github.com/citusdata/postgres_vectorization_test</a>) designed for Postgres that imitates ClickHouse’s native support. There is also [<a href="https://aws.amazon.com/blogs/database/building-fast-refresh-capability-in-amazon-rds-for-postgresql/" rel="nofollow">a Fast Refresh Module</a>](<a href="https://aws.amazon.com/blogs/database/building-fast-refresh-capability-in-amazon-rds-for-postgresql/" rel="nofollow">https://aws.amazon.com/blogs/database/building-fast-refresh-capability-in-amazon-rds-for-postgresql/</a>) that uses Postgres’s log to dynamically update materialized views. Coupled with Postgres triggers, developers could create a ClickHouse-like set-up. But all of these techniques require <em>significant</em> set-up work and additional columns to reach any efficiency that is even comparable to ClickHouse’s.</p>
<p><img width="631" alt="Untitled (2)" src="https://private-user-images.githubusercontent.com/85290767/289697301-27120962-d6a1-46e6-b0bf-fe3ec3a65b0a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI4NzMxMDUsIm5iZiI6MTcxMjg3MjgwNSwicGF0aCI6Ii84NTI5MDc2Ny8yODk2OTczMDEtMjcxMjA5NjItZDZhMS00NmU2LWIwYmYtZmUzZWMzYTY1YjBhLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTElMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDExVDIyMDAwNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWVkNDhkYjRlMGI5NTcwODIzNDcxNTQ1YzZhNmQzMzE0NWJlYWY4ZjAwOTRjNmEyYjM3ZjVjY2Y3NWRiMTNlMDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CGGCPxMP95NweE8DLnqEzLft_Fo_8ppc09C6cOA9m4c" content-type-secured-asset="image/png"></p><p>A relevant <a href="https://posthog.com/blog/clickhouse-vs-postgres" rel="nofollow">meme</a> from my Postgres vs Clickhouse guide for PostHog</p>
<p>Recently, the most interesting rift in the Postgres vs OLAP space is [<a href="https://www.hydra.so/" rel="nofollow">Hydra</a>](<a href="https://www.hydra.so/" rel="nofollow">https://www.hydra.so</a>), an open-source, column-oriented distribution of Postgres that was <em><strong>very</strong></em> recently launched (after our migration to ClickHouse). Had Hydra been available during our decision-making time period, we might’ve made a different choice. However, ClickHouse remains an incredible pick, given its mature product, large community, hardware optimizations, and ease of use side-by-side with Postgres.</p>
<p>Of course, migrating analytics processes to ClickHouse is only half the battle. The next is actually deploying ClickHouse to production—where a few strategies exist.</p>
<p><h2>How we utilize ClickHouse</h2><a id="user-content-how-we-utilize-clickhouse" aria-label="Permalink: How we utilize ClickHouse" href="#how-we-utilize-clickhouse"></a></p>
<p>When discussing our ClickHouse implementation, there are fundamentally two different topics—what we use ClickHouse <em><strong>for</strong></em>, and how our ClickHouse instance is deployed and maintained.</p>
<p><h3>What we query ClickHouse for</h3><a id="user-content-what-we-query-clickhouse-for" aria-label="Permalink: What we query ClickHouse for" href="#what-we-query-clickhouse-for"></a></p>
<p>Our ClickHouse instance [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/models/clickhouse/events_raw.rb#L3">ingests raw billable events</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/models/clickhouse/events_raw.rb#L3">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/models/clickhouse/events_raw.rb#L3</a>) dispatched by our users. While we don’t write our own ClickHouse schema (as it is auto-generated by ActiveRecord), it is written to a file, [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_schema.rb#L4">available in our open-source repository</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_schema.rb#L4">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_schema.rb#L4</a>). Our ClickHouse instance only has two tables—<code>raw_events</code> and <code>raw_events_queue</code>—alongside one materialized view, <code>events_raw_mv</code> . That’s it. We don’t store any of the other “business-critical” data on ClickHouse because they aren’t analytical queries.</p>
<p>In detail, our <code>[raw_events_queue](https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231026124912_create_events_raw_queue.rb)</code> is where events are initially streamed to via [<a href="https://kafka.apache.org/" rel="nofollow">Apache Kafka</a>](<a href="https://kafka.apache.org/" rel="nofollow">https://kafka.apache.org</a>), open-source event streaming software. From it, the <code>[events_raw_mv](https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231030163703_create_events_raw_mv.rb)</code> is generated with ClickHouse’s <code>[cast()](https://clickhouse.com/docs/en/sql-reference/functions/type-conversion-functions)</code> function, which maps the event’s metadata from a JSON blob to a string array. Finally, this materialized view pushes data to the <code>[raw_events](https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231024084411_create_events_raw.rb)</code> table. This is a [<a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree" rel="nofollow">MergeTree</a>](<a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree" rel="nofollow">https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree</a>) table that is apt for a large number of writes.</p>
<p><code>raw_events</code> is what Lago’s general codebase interfaces with via our <code>[ClickHouseStores](https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/events/stores/clickhouse_store.rb#L14)</code> class, which is tapped when [<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/billable_metrics/aggregation_factory.rb#L18">aggregating billable metrics</a>](<a href="https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/billable_metrics/aggregation_factory.rb#L18">https://github.com/getlago/lago-api/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/billable_metrics/aggregation_factory.rb#L18</a>). <code>raw_events</code> uses a tuple of <code>organization_id</code>, <code>external_subscription_id</code>, <code>code</code>, and a timestamp as primary keys; given ClickHouse’s [<a href="https://medium.com/datadenys/how-clickhouse-primary-key-works-and-how-to-choose-it-4aaf3bf4a8b9" rel="nofollow">sophisticated support for primary key tuples</a>](<a href="https://medium.com/datadenys/how-clickhouse-primary-key-works-and-how-to-choose-it-4aaf3bf4a8b9" rel="nofollow">https://medium.com/datadenys/how-clickhouse-primary-key-works-and-how-to-choose-it-4aaf3bf4a8b9</a>), this helps ClickHouse locate rows <strong>very</strong> quickly.</p>
<p><h3>How we deploy ClickHouse</h3><a id="user-content-how-we-deploy-clickhouse" aria-label="Permalink: How we deploy ClickHouse" href="#how-we-deploy-clickhouse"></a></p>
<p>Because ClickHouse is an open-source database, it could be self-hosted on any ordinary Linux server. However, many companies trust managed database solutions because they (i) often reduce overall costs, (ii) make scaling databases easier, and (iii) take care of safe replication/backups.</p>
<p>One of the most popular options is ClickHouse Inc’s ClickHouse Cloud offering, which offers a serverless ClickHouse instance with decoupled compute and storage.</p>
<p>However, we instead opted for Altinity Operator, which deploys and manages ClickHouse in a Kubernetes cluster in our existing cloud offering. We preferred this approach given more flexibility due to custom definitions, efficiency on cost, and ease of maintenance.</p>
<p><h2>Other notable open-source projects that use ClickHouse</h2><a id="user-content-other-notable-open-source-projects-that-use-clickhouse" aria-label="Permalink: Other notable open-source projects that use ClickHouse" href="#other-notable-open-source-projects-that-use-clickhouse"></a></p>
<p>We aren’t the only open-source project that uses ClickHouse; in fact, we aren’t even the only open-source project that migrated from Postgres to ClickHouse. A notable example is [<a href="https://posthog.com/" rel="nofollow">PostHog</a>](<a href="https://posthog.com/" rel="nofollow">https://posthog.com</a>), an open-source analytics suite that switched from [<a href="https://posthog.com/blog/clickhouse-announcement" rel="nofollow">Postgres to ClickHouse</a>](<a href="https://posthog.com/blog/clickhouse-announcement" rel="nofollow">https://posthog.com/blog/clickhouse-announcement</a>) given the sheer amount of web events they were processing per second.</p>
<p>Another great example is Gitlab, which used ClickHouse to store data of streamed events [<a href="https://docs.gitlab.com/ee/architecture/blueprints/clickhouse_usage/" rel="nofollow">in their observability suite</a>](<a href="https://docs.gitlab.com/ee/architecture/blueprints/clickhouse_usage/" rel="nofollow">https://docs.gitlab.com/ee/architecture/blueprints/clickhouse_usage/</a>). In general, it’s common for open-source companies (and closed-source projects alike) to find their general-purpose database like Postgres or mySQL ill-suited as they start to scale.</p>
<p>Even some closed-source solutions, like the HTTP data-streaming product TinyBird, have made [<a href="https://www.tinybird.co/blog-posts/we-launched-an-open-source-clickhouse-knowledge-base" rel="nofollow">open-source contributions to ClickHouse</a>](<a href="https://www.tinybird.co/blog-posts/we-launched-an-open-source-clickhouse-knowledge-base" rel="nofollow">https://www.tinybird.co/blog-posts/we-launched-an-open-source-clickhouse-knowledge-base</a>) given their dependence on it. Slowly, ClickHouse is building the same level of success in the OLAP world as Postgres is achieving in the OLTP space.</p>
<p><h3>Closing Thoughts</h3><a id="user-content-closing-thoughts" aria-label="Permalink: Closing Thoughts" href="#closing-thoughts"></a></p>
<p>Due to the hardware optimizations of inverting table layouts, there is no one-size-fits-all database as applications scale. We ran into that problem fairly early in our journey given the event-heavy nature of our product. However, that doesn’t meant that every team needs to start with an OLTP + OLAP stack—just to be ready for it when the moment arrives.</p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Storm: LLM system that researches a topic and generates full-length wiki article (111 pts)]]></title>
            <link>https://github.com/stanford-oval/storm</link>
            <guid>40004887</guid>
            <pubDate>Thu, 11 Apr 2024 17:53:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/stanford-oval/storm">https://github.com/stanford-oval/storm</a>, See on <a href="https://news.ycombinator.com/item?id=40004887">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking</h2><a id="user-content-storm-synthesis-of-topic-outlines-through-retrieval-and-multi-perspective-question-asking" aria-label="Permalink: STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking" href="#storm-synthesis-of-topic-outlines-through-retrieval-and-multi-perspective-question-asking"></a></p>
<p dir="auto">This repository contains the code for our NAACL 2024 paper <a href="https://arxiv.org/abs/2402.14207" rel="nofollow">Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models</a> by <a href="https://cs.stanford.edu/~shaoyj" rel="nofollow">Yijia Shao</a>, <a href="https://yucheng-jiang.github.io/" rel="nofollow">Yucheng Jiang</a>, Theodore A. Kanell, Peter Xu, <a href="https://omarkhattab.com/" rel="nofollow">Omar Khattab</a>, and <a href="https://suif.stanford.edu/~lam/" rel="nofollow">Monica S. Lam</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview <a href="https://storm.genie.stanford.edu/" rel="nofollow">(Try STORM now!)</a></h2><a id="user-content-overview-try-storm-now" aria-label="Permalink: Overview (Try STORM now!)" href="#overview-try-storm-now"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/stanford-oval/storm/blob/main/assets/overview.png"><img src="https://github.com/stanford-oval/storm/raw/main/assets/overview.png"></a>
</p>
STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search.
<p dir="auto">While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.</p>
<p dir="auto"><strong>Try out our <a href="https://storm.genie.stanford.edu/" rel="nofollow">live demo</a> to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system 🙏!</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Research Before Writing</h2><a id="user-content-research-before-writing" aria-label="Permalink: Research Before Writing" href="#research-before-writing"></a></p>
<p dir="auto">STORM breaks down generating long articles with citations into two steps:</p>
<ol dir="auto">
<li><strong>Pre-writing stage</strong>: The system conducts Internet-based research to collect references and generates an outline.</li>
<li><strong>Writing stage</strong>: The system uses the outline and references to generate the full-length article with citations.</li>
</ol>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/stanford-oval/storm/blob/main/assets/two_stages.jpg"><img src="https://github.com/stanford-oval/storm/raw/main/assets/two_stages.jpg"></a>
</p>
<p dir="auto">STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:</p>
<ol dir="auto">
<li><strong>Perspective-Guided Question Asking</strong>: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.</li>
<li><strong>Simulated Conversation</strong>: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.</li>
</ol>
<p dir="auto">Based on the separation of the two stages, STORM is implemented in a highly modular way (see <a href="https://github.com/stanford-oval/storm/blob/main/src/engine.py">engine.py</a>) using <a href="https://github.com/stanfordnlp/dspy">dspy</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto"><strong>We view STORM as an example of automated knowledge curation. We are working on enhancing our codebase to increase its extensibility. Stay tuned!</strong></p>
<p dir="auto">Below, we provide a quick start guide to run STORM locally to reproduce our experiments.</p>
<ol dir="auto">
<li>Install the required packages.
<div dir="auto" data-snippet-clipboard-copy-content="conda create -n storm python=3.11
conda activate storm
pip install -r requirements.txt"><pre>conda create -n storm python=3.11
conda activate storm
pip install -r requirements.txt</pre></div>
</li>
<li>Set up OpenAI API key and <a href="https://api.you.com/" rel="nofollow">You.com search API</a> key. Create a file <code>secrets.toml</code> under the root directory and add the following content:
<div dir="auto" data-snippet-clipboard-copy-content="# Set up OpenAI API key.
OPENAI_API_KEY=<your_openai_api_key>
# If you are using the API service provided by OpenAI, include the following line:
OPENAI_API_TYPE=openai
# If you are using the API service provided by Microsoft Azure, include the following lines:
OPENAI_API_TYPE=azure
AZURE_API_BASE=<your_azure_api_base_url>
AZURE_API_VERSION=<your_azure_api_version>
# Set up You.com search API key.
YOU_API_KEY=<your_youcom_api_key>"><pre><span><span>#</span> Set up OpenAI API key.</span>
OPENAI_API_KEY=<span>&lt;</span>your_openai_api_key<span>&gt;</span>
<span><span>#</span> If you are using the API service provided by OpenAI, include the following line:</span>
OPENAI_API_TYPE=openai
<span><span>#</span> If you are using the API service provided by Microsoft Azure, include the following lines:</span>
OPENAI_API_TYPE=azure
AZURE_API_BASE=<span>&lt;</span>your_azure_api_base_url<span>&gt;</span>
AZURE_API_VERSION=<span>&lt;</span>your_azure_api_version<span>&gt;</span>
<span><span>#</span> Set up You.com search API key.</span>
YOU_API_KEY=<span>&lt;</span>your_youcom_api_key<span>&gt;</span></pre></div>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Paper Experiments</h2><a id="user-content-paper-experiments" aria-label="Permalink: Paper Experiments" href="#paper-experiments"></a></p>
<p dir="auto">The FreshWiki dataset used in our experiments can be found in <a href="https://github.com/stanford-oval/storm/blob/main/FreshWiki">./FreshWiki</a>.</p>
<p dir="auto">Run the following commands under <a href="https://github.com/stanford-oval/storm/blob/main/src">./src</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pre-writing Stage</h3><a id="user-content-pre-writing-stage" aria-label="Permalink: Pre-writing Stage" href="#pre-writing-stage"></a></p>
<p dir="auto">For batch experiment on FreshWiki dataset:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv  --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5"><pre>python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv  --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5</pre></div>
<ul dir="auto">
<li><code>--engine</code> (choices=[<code>gpt-4</code>, <code>gpt-35-turbo</code>]): the LLM engine used for generating the outline</li>
<li><code>--do-research</code>: if True, simulate conversation to research the topic; otherwise, load the results.</li>
<li><code>--max-conv-turn</code>: the maximum number of questions for each information-seeking conversation</li>
<li><code>--max-perspective</code>: the maximum number of perspectives to be considered, each perspective corresponds to an information-seeking conversation.
<ul dir="auto">
<li>STORM also uses a general conversation to collect basic information about the topic. So, the maximum number of QA pairs is <code>max_turn * (max_perspective + 1)</code>. 💡 Reducing <code>max_turn</code> or <code>max_perspective</code> can speed up the process and reduce the cost but may result in less comprehensive outline.</li>
<li>The parameter will not have any effect if <code>--disable-perspective</code> is set (the perspective-driven question asking is disabled).</li>
</ul>
</li>
</ul>
<p dir="auto">To run the experiment on a single topic:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5"><pre>python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5</pre></div>
<ul dir="auto">
<li>The script will ask you to enter the <code>Topic</code> and the <code>Ground truth url</code> that will be excluded. If you do not have any url to exclude, leave that field empty.</li>
</ul>
<p dir="auto">The generated outline will be saved in <code>{output_dir}/{topic}/storm_gen_outline.txt</code> and the collected references will be saved in <code>{output_dir}/{topic}/raw_search_results.json</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Writing Stage</h3><a id="user-content-writing-stage" aria-label="Permalink: Writing Stage" href="#writing-stage"></a></p>
<p dir="auto">For batch experiment on FreshWiki dataset:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate"><pre>python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate</pre></div>
<ul dir="auto">
<li><code>--do-polish-article</code>: if True, polish the article by adding a summarization section and removing duplicate content if <code>--remove-duplicate</code> is set True.</li>
</ul>
<p dir="auto">To run the experiment on a single topic:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate"><pre>python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate</pre></div>
<ul dir="auto">
<li>The script will ask you to enter the <code>Topic</code>. Please enter the same topic as the one used in the pre-writing stage.</li>
</ul>
<p dir="auto">The generated article will be saved in <code>{output_dir}/{topic}/storm_gen_article.txt</code> and the references corresponding to citation index will be saved in <code>{output_dir}/{topic}/url_to_info.json</code>. If <code>--do-polish-article</code> is set, the polished article will be saved in <code>{output_dir}/{topic}/storm_gen_article_polished.txt</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Customize the STORM Configurations</h2><a id="user-content-customize-the-storm-configurations" aria-label="Permalink: Customize the STORM Configurations" href="#customize-the-storm-configurations"></a></p>
<p dir="auto">We set up the default LLM configuration in <code>LLMConfigs</code> in <a href="https://github.com/stanford-oval/storm/blob/main/src/modules/utils.py">src/modules/utils.py</a>. You can use <code>set_conv_simulator_lm()</code>,<code>set_question_asker_lm()</code>, <code>set_outline_gen_lm()</code>, <code>set_article_gen_lm()</code>, <code>set_article_polish_lm()</code> to override the default configuration. These functions take in an instance from <code>dspy.dsp.LM</code> or <code>dspy.dsp.HFModel</code>.</p>
<p dir="auto">💡 <strong>For a good practice,</strong></p>
<ul dir="auto">
<li>choose a cheaper/faster model for <code>conv_simulator_lm</code> which is used to split queries, synthesize answers in the conversation.</li>
<li>if you need to conduct the actual writing step, choose a more powerful model for <code>article_gen_lm</code>. Based on our experiments, weak models are bad at generating text with citations.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Automatic Evaluation</h2><a id="user-content-automatic-evaluation" aria-label="Permalink: Automatic Evaluation" href="#automatic-evaluation"></a></p>
<p dir="auto">In our paper, we break down the evaluation into two parts: outline quality and full-length article quality.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Outline Quality</h3><a id="user-content-outline-quality" aria-label="Permalink: Outline Quality" href="#outline-quality"></a></p>
<p dir="auto">We introduce <em>heading soft recall</em> and <em>heading entity recall</em> to evaluate the outline quality. This makes it easier to prototype methods for pre-writing.</p>
<p dir="auto">Run the following command under <a href="https://github.com/stanford-oval/storm/blob/main/eval">./eval</a> to compute the metrics on FreshWiki dataset:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv"><pre>python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Full-length Article Quality</h3><a id="user-content-full-length-article-quality" aria-label="Permalink: Full-length Article Quality" href="#full-length-article-quality"></a></p>
<p dir="auto"><a href="https://github.com/stanford-oval/storm/blob/main/eval/eval_article_quality.py">eval/eval_article_quality.py</a> provides the entry point of evaluating full-length article quality using ROUGE, entity recall, and rubric grading. Run the following command under <code>eval</code> to compute the metrics:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt"><pre>python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use the Metric Yourself</h3><a id="user-content-use-the-metric-yourself" aria-label="Permalink: Use the Metric Yourself" href="#use-the-metric-yourself"></a></p>
<p dir="auto">The similarity-based metrics (i.e., ROUGE, entity recall, and heading entity recall) are implemented in <a href="https://github.com/stanford-oval/storm/blob/main/eval/metrics.py">eval/metrics.py</a>.</p>
<p dir="auto">For rubric grading, we use the <a href="https://huggingface.co/kaist-ai/prometheus-13b-v1.0" rel="nofollow">prometheus-13b-v1.0</a> introduced in <a href="https://arxiv.org/abs/2310.08491" rel="nofollow">this paper</a>. <a href="https://github.com/stanford-oval/storm/blob/main/eval/evaluation_prometheus.py">eval/evaluation_prometheus.py</a> provides the entry point of using the metric.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions</h2><a id="user-content-contributions" aria-label="Permalink: Contributions" href="#contributions"></a></p>
<p dir="auto">If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!</p>
<p dir="auto">Contact person: <a href="mailto:shaoyj@stanford.edu">Yijia Shao</a> and <a href="mailto:yuchengj@stanford.edu">Yucheng Jiang</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">Please cite our paper if you use this code or part of it in your work:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@inproceedings{shao2024assisting,
      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, 
      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},
      year={2024},
      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}
}"><pre><span>@inproceedings</span>{<span>shao2024assisting</span>,
      <span>title</span>=<span><span>{</span>{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}<span>}</span></span>, 
      <span>author</span>=<span><span>{</span>Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2024<span>}</span></span>,
      <span>booktitle</span>=<span><span>{</span>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)<span>}</span></span>
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vortex: OpenCL compatible RISC-V GPGPU (132 pts)]]></title>
            <link>https://vortex.cc.gatech.edu/</link>
            <guid>40003868</guid>
            <pubDate>Thu, 11 Apr 2024 16:19:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vortex.cc.gatech.edu/">https://vortex.cc.gatech.edu/</a>, See on <a href="https://news.ycombinator.com/item?id=40003868">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__next"><div><p><h2>Vortex: OpenCL Compatible RISC-V GPGPU</h2><h2>Vortex is an open-source hardware and software project to support GPGPU based on RISC-V ISA extensions. Currently Vortex supports OpenCL and it runs on FPGA. The vortex platform is highly customizable and scalable with a complete open-source compiler, driver and runtime software stack to enable research in GPU architectures.</h2></p></div><section><h2>Recent Publications<!-- --> <a href="https://vortex.cc.gatech.edu/publications/">See All</a></h2><div><div><h4>Skybox: Open-Source Graphic Rendering on Programmable RISC-V GPUs</h4><p>Authors: Blaise Tine, Varun Saxena, Santosh Srivatsan, Joshua R. Simpson, Fadi Alzammar, Liam Cooper, and Hyesoon Kim</p><p><a href="https://dl.acm.org/doi/10.1145/3582016.3582024">See Publication</a></p></div><div><h4>Implementing Hardware Extensions for Multicore RISC-V GPUs</h4><p>Authors: Blaise Tine, Hyesoon Kim</p><p><a href="https://carrv.github.io/2022/papers/CARRV2022_paper_11_Blaise.pdf">See Publication</a></p></div></div></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Readme.txt vs. README.txt (2015) (131 pts)]]></title>
            <link>https://softwareengineering.stackexchange.com/questions/301691/readme-txt-vs-readme-txt</link>
            <guid>40003743</guid>
            <pubDate>Thu, 11 Apr 2024 16:08:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://softwareengineering.stackexchange.com/questions/301691/readme-txt-vs-readme-txt">https://softwareengineering.stackexchange.com/questions/301691/readme-txt-vs-readme-txt</a>, See on <a href="https://news.ycombinator.com/item?id=40003743">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="text">
<p>All-uppercase letters stand out and make the file easily visible which makes sense because it is probably the first thing a new user would want to look at.  (Or, at least, <em>should</em> have looked at…) As others have already said, file names starting with a capital letter will be listed before lower-case names in <a href="http://www.catb.org/~esr/jargon/html/A/ASCIIbetical-order.html">ASCIIbetical</a> sorting (<code>LC_COLLATE=C</code>) which helps make the file visible at a first glance.</p>

<p>The <code>README</code> file is part of a bunch of files a user of a free software package would normally expect to find. Others are <code>INSTALL</code> (instructions for building and installing the software), <code>AUTHORS</code> (list of contributors), <code>COPYING</code> (license text), <code>HACKING</code> (how to get started for contributing, maybe including a TODO list of starting points), <code>NEWS</code> (recent changes) or <code>ChangeLog</code> (mostly redundant with version control systems).</p>

<p>This is what the <a href="https://www.gnu.org/prep/standards/html_node/Releases.html#Releases"><em>GNU Coding Standards</em></a> have to say about the <code>README</code> file.</p>

<blockquote>
  <p>The distribution should contain a file named <code>README</code> with a general overview of the package:</p>
  
  <ul>
  <li>the name of the package;</li>
  <li>the version number of the package, or refer to where in the package the version can be found;</li>
  <li>a general description of what the package does;</li>
  <li>a reference to the file <code>INSTALL</code>, which should in turn contain an explanation of the installation procedure;</li>
  <li>a brief explanation of any unusual top-level directories or files, or other hints for readers to find their way around the source;</li>
  <li>a reference to the file which contains the copying conditions. The GNU GPL, if used, should be in a file called <code>COPYING</code>. If the GNU LGPL is used, it should be in a file called <code>COPYING.LESSER</code>.</li>
  </ul>
</blockquote>

<p>Since it is always good to strive for the least surprise of your users, you should follow this convention unless there are compelling reasons for a deviation. In the UNIX world, file name extensions were traditionally used sparingly so the canonical name of the file is <code>README</code> without any suffix. But most users probably would have no troubles understanding that a file named <code>README.txt</code> has the same meaning. If the file is written in <a href="https://daringfireball.net/projects/markdown/"><em>Markdown</em></a>, a file name like <code>README.md</code> might also be reasonable. Avoid using more complicated markup languages like HTML in the <code>README</code> file, however, because it should be convenient to read on a text-only terminal. You can point users to the manual of the software or its on-line documentation, that might be written in a more sophisticated format, for details from the <code>README</code> file.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a new sensor out of 3D printer filament for my PhD (711 pts)]]></title>
            <link>https://paulbupejr.com/developing-the-optigap-sensor-system/</link>
            <guid>40003710</guid>
            <pubDate>Thu, 11 Apr 2024 16:06:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulbupejr.com/developing-the-optigap-sensor-system/">https://paulbupejr.com/developing-the-optigap-sensor-system/</a>, See on <a href="https://news.ycombinator.com/item?id=40003710">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-890">

<div>
<p><span><span>Reading Time: </span> <span>9</span> <span>minutes</span></span></p><p>This article explores the research and development journey behind my new sensor system, <a href="https://ieeexplore.ieee.org/document/10161357" target="_blank" rel="noreferrer noopener">OptiGap</a>, a key component of my PhD research. I’m writing this in a storytelling format to offer insights into my decision-making process and the evolution leading to the final implementation. It should hopefully provide a glimpse into the sometimes-shrouded world of PhD research and may appeal to those curious about the process. For a deeper dive into technical specifics, simulations, and existing research on this subject, my dissertation is <a href="https://ir.library.louisville.edu/etd/4213/" target="_blank" rel="noreferrer noopener">available online here</a>.</p>
<h3>What does it do?</h3>
<p>In very general terms, this sensor is basically a rope that if bent can tell you where along its length you bent it. The fancy term for that is “bend localization.”</p>
<p>OptiGap’s application is mainly within the realm of soft robotics, which typically involves compliant (or ‘squishy’) systems, where the use of<a href="https://paulbupejr.com/autonomous-robot-design/"> traditional sensors</a> is often not practical. The name OptiGap, a fusion of “optical” and “gap,” reflects its core principle of utilizing air gaps within flexible optical light pipes to generate coded patterns essential for bend localization. </p>
<h2>How the OptiGap Sensor System Started</h2>
<p>The idea for OptiGap came about while I was experimenting with light transmission through various light pipes (optical cables) for use as a bend detection sensor. I was initially trying to see how I could effectively “slow down” light through the fiber…a seemingly straightforward task, right?</p>
<p>During this process, I attached a section of clear 3D printer filament (1.75mm TPU) to a piece of tape measure for an experiment and incidentally discovered that when I bent the tape measure (and filament) at the spot where the electrical tape was attached, there was a significant drop in light transmission. I hypothesized that this was because the sticky residue of the electrical tape was causing the filament to stretch, which in turn reduced the light transmission.</p>
<p>To verify this hypothesis, I attached a longer piece of TPU to a tape measure and began bending it at various points to observe how light transmission would change.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/black_tape-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-891&quot;,&quot;imgStyles&quot;:&quot;aspect-ratio:4\/3;object-fit:cover&quot;,&quot;targetWidth&quot;:1920,&quot;targetHeight&quot;:2560,&quot;scaleAttr&quot;:&quot;cover&quot;,&quot;ariaLabel&quot;:&quot;Enlarge image: Tape measure experiment for OptiGap&quot;,&quot;alt&quot;:&quot;Tape measure experiment for OptiGap&quot;}" data-wp-interactive="core/image"><img decoding="async" width="768" height="1024" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-768x1024.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-768x1024.jpg" alt="Tape measure experiment for OptiGap" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-768x1024.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-225x300.jpg 225w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-1152x1536.jpg 1152w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-1536x2048.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-scaled.jpg 1920w" data-sizes="(max-width: 768px) 100vw, 768px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-768x1024.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-225x300.jpg 225w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-1152x1536.jpg 1152w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-1536x2048.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/black_tape-scaled.jpg 1920w"><figcaption>Tape measure experiment with clear TPU filament.</figcaption></figure></div>
<p>I wrote a small Linux I2C driver for the <a href="https://www.st.com/en/imaging-and-photonics-solutions/vl53l0x.html">VL53L0X </a>ToF sensor to run on a <a href="https://zeromq.org/" target="_blank" rel="noreferrer noopener">Raspberry Pi</a> and push the data to a socket using <a href="https://zeromq.org/">ZeroMQ</a>. I then created a rough GUI in Python to pull the sensor data from the socket and visualize the light transmission data in realtime, shown in the GIF below, which very quickly validated my hypothesis. This validation marked the “Eureka!” moment that sparked the eventual development of the OptiGap sensor.</p>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/tape.gif" alt="Initial OptiGap discovery"><figcaption>My excited face while validating my discovery.</figcaption></figure>
<h2>The OptiGap Realization</h2>
<p>I realized that since I could control where the light was being attenuated, I could use this to encode information about the position of the bend on the sensor. Using electrical tape was not a practical solution, so I started looking for a more reliable and consistent way to create these attenuations. This led me to the idea of cutting the filament and then reattaching it together using a flexible rubber (silicone) sleeve, leaving a small air gap, as shown in the image below.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/tpu_gap.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-895&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1600,&quot;targetHeight&quot;:526,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Proof-of-concept showing a light pipe with an air in a silicone sleeve.&quot;,&quot;alt&quot;:&quot;Proof-of-concept showing a light pipe with an air in a silicone sleeve.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="337" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1024x337.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1024x337.jpg" alt="Proof-of-concept showing a light pipe with an air in a silicone sleeve." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1024x337.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-300x99.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-768x252.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1536x505.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap.jpg 1600w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1024x337.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-300x99.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-768x252.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap-1536x505.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_gap.jpg 1600w"><figcaption>Proof-of-concept showing a light pipe with an air in a silicone sleeve.</figcaption></figure>
<p>The main working principle of the air gap is that translation and/or rotation of one light pipe face relative to the other changes the fraction of light transmitted across the gap. The greater the bend angle, the more light escapes across the gap. The resulting change in intensity of the optical signal can then be correlated with known patterns for use as a sensor.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/operating_principle.png&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-896&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1693,&quot;targetHeight&quot;:1046,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: OptiGap operating principle&quot;,&quot;alt&quot;:&quot;OptiGap operating principle&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="633" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1024x633.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1024x633.png" alt="OptiGap operating principle" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1024x633.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-300x185.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-768x474.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1536x949.png 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-825x510.png 825w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle.png 1693w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1024x633.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-300x185.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-768x474.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-1536x949.png 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle-825x510.png 825w, https://paulbupejr.com/wp-content/uploads/2024/04/operating_principle.png 1693w"><figcaption>This image is from a COMSOL simulation I made.</figcaption></figure>
<h2>The Big Idea</h2>
<p>I then proceeded to test this idea by creating multiple air gaps in a row and bending the filament to measure the attenuation.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/tpu_tof-1-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large is-style-default&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-898&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1578,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Multiple air gaps along a single TPU lightpipe&quot;,&quot;alt&quot;:&quot;Multiple air gaps along a single TPU lightpipe&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="631" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1024x631.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1024x631.jpg" alt="Multiple air gaps along a single TPU lightpipe" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1024x631.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-300x185.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-768x474.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1536x947.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-2048x1263.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1024x631.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-300x185.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-768x474.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-1536x947.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/tpu_tof-1-2048x1263.jpg 2048w"><figcaption>Multiple air gaps along a single TPU lightpipe.</figcaption></figure>
<p>As depicted in the GIF below, the optical intensity decreases at each air gap, with a more noticeable decrease as the bend angle increases. This initial experimentation served as proof of concept, demonstrating the feasibility of the idea. It led to the formulation of my final hypothesis of <strong>utilizing a pattern of these air gaps to encode information regarding the sensor’s bending and employing a naive Bayes classifier on a microcontroller to decode the bend location.</strong></p>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/gaps.gif" alt="Validating the attenuation at the air gaps."><figcaption>Validating the attenuation at the air gaps.</figcaption></figure>
<p>This concept resembles the functionality of a linear encoder. Linear encoders gauge an object’s linear movement, typically comprising a slider rail with a coded scale akin to a measuring ruler and a sensing head that moves across this scale to read it. Linear (absolute) encoders emit a distinct code at each position, ensuring consistent identification of displacement.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/block_diagram.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-900&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:519,&quot;targetHeight&quot;:417,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: OptiGap system overview.&quot;,&quot;alt&quot;:&quot;OptiGap system overview.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="519" height="417" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram.png" alt="OptiGap system overview." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram-300x241.png 300w" data-sizes="(max-width: 519px) 100vw, 519px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/block_diagram-300x241.png 300w"><figcaption>OptiGap system overview.</figcaption></figure></div>
<p>The OptiGap system, functioning like an absolute encoder, would encode absolute positions using patterns of bend-sensitive air gaps along parallel light pipes, effectively serving as a singular fiber optic sensor.</p>
<h3><strong>Encoding the Bend Location using Inverse Gray Code</strong></h3>
<p>Absolute encoders commonly employ Gray code, a binary system where two successive values differ in only one bit. This property allows for various applications, including error checking. However, Gray code isn’t optimal for the OptiGap sensor system. Here, we aim for consecutive values to differ by the maximum number of bits to facilitate easier differentiation. This necessity gave rise to Inverse Gray code.</p>
<div>
<figure><img decoding="async" width="300" height="180" src="https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-300x180.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-300x180.jpg" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-300x180.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-1024x615.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-768x461.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code.jpg 1176w" data-sizes="(max-width: 300px) 100vw, 300px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-300x180.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-1024x615.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code-768x461.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/Gray-Code.jpg 1176w"></figure></div>
<p>Inverse Gray code is a binary code where two successive values differ by the maximum (n-1) number of bits. To implement this, I simply create cuts in the filament wherever there’s a “1” in the Inverse Gray code sequence. This approach can scale to any bit number. For the prototype, I utilized 3 bits, providing 8 possible positions.</p>
<h3>Visualization of the OptiGap Sensor System</h3>
<p>The illustration below depicts the signal patterns of the OptiGap sensor system for each bend position using three fibers. By employing a naive Bayes classifier, the sensor system can discern bend positions based on signal patterns. The third graph represents actual sensor data from the prototype system, utilized for training the classifier on the microcontroller.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/signal_patterns.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-903&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:519,&quot;targetHeight&quot;:448,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: OptiGap bending patterns.&quot;,&quot;alt&quot;:&quot;OptiGap bending patterns.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="519" height="448" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns.png" alt="OptiGap bending patterns." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns-300x259.png 300w" data-sizes="(max-width: 519px) 100vw, 519px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/signal_patterns-300x259.png 300w"><figcaption>OptiGap bending patterns.</figcaption></figure></div>
<h2>The OptiGap Prototype</h2>
<p>I proceeded to construct a prototype of the OptiGap sensor system, utilizing 3 strands of clear TPU 3D printer filament, each featuring a distinct pattern of air gaps. The image below showcases the filament just before cutting, with the cut pattern indicated on a piece of tape.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/making_cuts-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-905&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:389,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Beginning stages of an OptiGap sensor prototype.&quot;,&quot;alt&quot;:&quot;Beginning stages of an OptiGap sensor prototype.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="156" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1024x156.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1024x156.jpg" alt="Beginning stages of an OptiGap sensor prototype." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1024x156.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-300x46.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-768x117.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1536x234.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-2048x312.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1024x156.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-300x46.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-768x117.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-1536x234.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/making_cuts-2048x312.jpg 2048w"><figcaption>Beginning stages of an OptiGap sensor prototype.</figcaption></figure>
<p>For the prototype, I employed a commercial 3:1 fiber optic coupler to merge the light from the 3 strands into a single fiber optic cable, resulting in the completion of the sensor prototype, as depicted below.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/first_assembled_prototype-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-906&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:2106,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Assembled sensing head of an OptiGap sensor.&quot;,&quot;alt&quot;:&quot;Assembled sensing head of an OptiGap sensor.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="842" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1024x842.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1024x842.jpg" alt="Assembled sensing head of an OptiGap sensor." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1024x842.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-300x247.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-768x632.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1536x1264.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-2048x1685.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1024x842.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-300x247.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-768x632.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-1536x1264.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/first_assembled_prototype-2048x1685.jpg 2048w"><figcaption>Assembled sensing head of an OptiGap sensor.</figcaption></figure>
<p>This marked the final phase of validating the hypothesis and operational theory behind the OptiGap sensor.</p>
<h3>Reducing the Physical Size</h3>
<p>The initial prototype proved to be large and bulky, primarily due to the size of the 3D printer filament used. Drawing from previous experience, I recognized that PMMA (plastic) optical fiber offered a smaller and more flexible alternative suitable for this application. Consequently, I assessed 500, 750, and 1000 micron unjacketed PMMA optical fibers from Industrial Fiber Optics, Inc. for the sensor strands, resulting in a significant reduction in sensor size.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/fiber_label-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-907&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1359,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: 500 micron PMMA fiber spool.&quot;,&quot;alt&quot;:&quot;500 micron PMMA fiber spool.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="544" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1024x544.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1024x544.jpg" alt="500 micron PMMA fiber spool." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1024x544.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-300x159.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-768x408.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1536x815.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-2048x1087.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1024x544.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-300x159.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-768x408.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-1536x815.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/fiber_label-2048x1087.jpg 2048w"><figcaption>500 micron PMMA fiber spool.</figcaption></figure>
<p>I conducted tests on all three types of fibers to evaluate their light transmission and flexibility. Among them, the 500 micron fiber emerged as the optimal choice overall, although all three exhibited sufficient flexibility for this application.</p>
<h3><strong>Reducing the Optical Transceiver Complexity</strong></h3>
<p>I decided to switch from using the complex VL53L0X ToF sensor to a simple photodiode and IR LED setup to reduce the complexity of the system and to increase modularity. This also allowed me to use a &nbsp;microcontroller to read the sensor data, which was a significant improvement over the initial prototype.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/new_fiber_tx-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-908&quot;,&quot;imgStyles&quot;:&quot;aspect-ratio:4\/3;object-fit:cover&quot;,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:1980,&quot;scaleAttr&quot;:&quot;cover&quot;,&quot;ariaLabel&quot;:&quot;Enlarge image: IR LED prototype board.&quot;,&quot;alt&quot;:&quot;IR LED prototype board.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="792" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1024x792.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1024x792.jpg" alt="IR LED prototype board." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1024x792.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-300x232.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-768x594.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1536x1188.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-2048x1584.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1024x792.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-300x232.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-768x594.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-1536x1188.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/new_fiber_tx-2048x1584.jpg 2048w"><figcaption>IR LED prototype board with 1000 micron PMMA fiber.</figcaption></figure>
<p>I then created a demo system for the sensor based around an STM32 microcontroller and a photodiode/IR LED setup.</p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/demo_system.png&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-909&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:519,&quot;targetHeight&quot;:298,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Full OptiGap demo system using 500 micron PMMA fiber.&quot;,&quot;alt&quot;:&quot;Full OptiGap demo system using 500 micron PMMA fiber.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="519" height="298" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/demo_system.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/demo_system.png" alt="Full OptiGap demo system using 500 micron PMMA fiber." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/demo_system.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/demo_system-300x172.png 300w" data-sizes="(max-width: 519px) 100vw, 519px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/demo_system.png 519w, https://paulbupejr.com/wp-content/uploads/2024/04/demo_system-300x172.png 300w"><figcaption>Full OptiGap demo system using 500 micron PMMA fiber.</figcaption></figure></div>
<h2><strong>Realtime Machine Learning on a Microcontroller</strong></h2>
<p>The final stage in developing the OptiGap sensor system involved integrating a naive Bayes classifier onto the STM32 microcontroller to decode the bend location from the sensor data. <strong><em>I opted for a naive Bayes classifier due to its efficiency compared to if-statements or lookup tables, its capability to handle new or previously unseen data, and its potential for increased accuracy by considering relationships between multiple input variables.</em></strong></p>
<p>Implementing the naive Bayes classifier proved to be relatively straightforward. This classifier is a probabilistic model based on applying Bayes’ theorem to determine how a measurement can be assigned to a particular class, with the class representing the bend location in this context. I utilized the <a href="https://www.arm.com/technologies/cmsis">Arm CMSIS-DSP library</a> for the classifier implementation.</p>
<h3>Fitting the Sensor Data</h3>
<p>The initial step in integrating the classifier was to fit the sensor data to a Gaussian distribution for each air gap pattern. To expedite this process, I developed a Python GUI for rapid labeling and fitting of the data using GNB (Gaussian Naive Bayes) from the scikit-learn library.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/ui_2.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-910&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1431,&quot;targetHeight&quot;:632,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Initial data labeling and fitting UI.&quot;,&quot;alt&quot;:&quot;Initial data labeling and fitting UI.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="452" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-1024x452.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-1024x452.jpg" alt="Initial data labeling and fitting UI." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-1024x452.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-300x132.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-768x339.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2.jpg 1431w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-1024x452.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-300x132.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2-768x339.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/ui_2.jpg 1431w"><figcaption>Initial data labeling and fitting UI.</figcaption></figure>
<p>I later improved this UI to be more general and to allow for more complex data fitting.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/ui.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-911&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1453,&quot;targetHeight&quot;:725,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Improved UI.&quot;,&quot;alt&quot;:&quot;Improved UI.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="511" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/ui-1024x511.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/ui-1024x511.jpg" alt="Improved UI." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/ui-1024x511.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/ui-300x150.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/ui-768x383.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/ui.jpg 1453w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/ui-1024x511.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/ui-300x150.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/ui-768x383.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/ui.jpg 1453w"><figcaption>Improved UI.</figcaption></figure>
<p>The probabilities for each class were computed and saved as a header for use on the microcontroller.</p>
<h3>Filtering the Sensor Data</h3>
<p>To enhance the accuracy of the classifier, I implemented a two-stage filtering process on the STM32 . The initial stage involved a basic moving average filter, followed by a Kalman filter in the second stage. </p>
<div>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/dsp.jpg&quot;,&quot;figureClassNames&quot;:&quot;aligncenter size-full&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-913&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:554,&quot;targetHeight&quot;:190,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image: Signal filtering stages.&quot;,&quot;alt&quot;:&quot;Signal filtering stages.&quot;}" data-wp-interactive="core/image"><img decoding="async" width="554" height="190" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/dsp.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/dsp.jpg" alt="Signal filtering stages." data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/dsp.jpg 554w, https://paulbupejr.com/wp-content/uploads/2024/04/dsp-300x103.jpg 300w" data-sizes="(max-width: 554px) 100vw, 554px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/dsp.jpg 554w, https://paulbupejr.com/wp-content/uploads/2024/04/dsp-300x103.jpg 300w"><figcaption>Signal filtering stages. Noise reduction relative to input signal.</figcaption></figure></div>

<p>The GIFs provided below illustrate various stages of the OptiGap sensor system, encompassing assembly and the operational demonstration of the final sensor system.</p>
<h4>System Overview</h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/System_Overview.gif" alt=""></figure>
<h4>Assembly of an OptiGap Sensor using TPU Filament</h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Assembly.gif" alt=""></figure>
<h4>Attenuation of Light through the OptiGap Sensor</h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Attenuation.gif" alt=""></figure>
<h4><strong>Fitting of the Sensor Data</strong></h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Training.gif" alt=""></figure>
<h4><strong>Segment Classification using PMMA Optical Fiber</strong></h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Segment_Classification.gif" alt=""></figure>
<h4><strong>Segment Classification using TPU Filament</strong></h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Validation.gif" alt=""></figure>
<h4><strong>Underwater Operation</strong></h4>
<figure><img decoding="async" width="800" height="450" src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20450'%3E%3C/svg%3E" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/Underwater_Validation.gif" alt=""></figure>
<h2>OptiGap Design Specifications</h2>
<h3>Key Properties &amp; Parameters</h3>
<div>
<figure><img decoding="async" width="1024" height="191" src="https://paulbupejr.com/wp-content/uploads/2024/04/properties-1024x191.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/properties-1024x191.png" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/properties-1024x191.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/properties-300x56.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/properties-768x143.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/properties.png 1176w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/properties-1024x191.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/properties-300x56.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/properties-768x143.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/properties.png 1176w"></figure></div>
<h3>Material Recommendations</h3>
<div>
<figure><img decoding="async" width="1024" height="636" src="https://paulbupejr.com/wp-content/uploads/2024/04/materails-1024x636.png" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/materails-1024x636.png" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/materails-1024x636.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/materails-300x186.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/materails-768x477.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/materails.png 1174w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/materails-1024x636.png 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/materails-300x186.png 300w, https://paulbupejr.com/wp-content/uploads/2024/04/materails-768x477.png 768w, https://paulbupejr.com/wp-content/uploads/2024/04/materails.png 1174w"></figure></div>
<h2>Next Steps</h2>
<p>I’ve made significant progress on the OptiGap system beyond what’s documented here, including its integration into another modular actuation and sensing system I developed called EneGate.</p>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/enegate_optigap-scaled.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-922&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:2560,&quot;targetHeight&quot;:2035,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="814" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" src="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1024x814.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1024x814.jpg" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1024x814.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-300x239.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-768x611.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1536x1221.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-2048x1628.jpg 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1024x814.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-300x239.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-768x611.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-1536x1221.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap-2048x1628.jpg 2048w"><figcaption>My EneGate PCB integrating an OptiGap sensor.</figcaption></figure>
<p>This has involved custom PCB design and systems integration, detailed in my dissertation. Additionally, I’ve prototyped miniature PCB versions of the optics to interface with the PCBs for the EneGate system.</p>
<figure>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/daughterboard.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-924&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:1600,&quot;targetHeight&quot;:1200,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="1024" height="768" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" data-id="924" src="https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1024x768.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1024x768.jpg" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1024x768.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-300x225.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-768x576.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1536x1152.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard.jpg 1600w" data-sizes="(max-width: 1024px) 100vw, 1024px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1024x768.jpg 1024w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-300x225.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-768x576.jpg 768w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard-1536x1152.jpg 1536w, https://paulbupejr.com/wp-content/uploads/2024/04/daughterboard.jpg 1600w"><figcaption>Mini OptiGap PCB</figcaption></figure>
<figure data-wp-context="{&quot;uploadedSrc&quot;:&quot;https:\/\/paulbupejr.com\/wp-content\/uploads\/2024\/04\/enegate_optigap_pcb.jpg&quot;,&quot;figureClassNames&quot;:&quot;wp-block-image size-large&quot;,&quot;figureStyles&quot;:null,&quot;imgClassNames&quot;:&quot;wp-image-923&quot;,&quot;imgStyles&quot;:null,&quot;targetWidth&quot;:907,&quot;targetHeight&quot;:599,&quot;scaleAttr&quot;:false,&quot;ariaLabel&quot;:&quot;Enlarge image&quot;,&quot;alt&quot;:&quot;&quot;}" data-wp-interactive="core/image"><img decoding="async" width="907" height="599" data-wp-init="callbacks.setButtonStyles" data-wp-on--click="actions.showLightbox" data-wp-on--load="callbacks.setButtonStyles" data-wp-on-window--resize="callbacks.setButtonStyles" data-id="923" src="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb.jpg" data-src="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb.jpg" alt="" data-srcset="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb.jpg 907w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb-300x198.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb-768x507.jpg 768w" data-sizes="(max-width: 907px) 100vw, 907px" srcset="https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb.jpg 907w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb-300x198.jpg 300w, https://paulbupejr.com/wp-content/uploads/2024/04/enegate_optigap_pcb-768x507.jpg 768w"><figcaption>Another mini OptiGap PCB</figcaption></figure>
</figure>
<p>I’ve also validated OptiGap on a real-world soft robotic system, with full details set to be presented in an upcoming RoboSoft paper titled “<strong><em>Embedded Optical Waveguide Sensors for Dynamic Behavior Monitoring in Twisted-Beam Structures.</em></strong>“</p>
<h3><strong>Commercialization</strong></h3>
<p>There’s an ongoing commercialization aspect to this research as well. Feel free to reach out if you’re interested in further details.</p>
<h2>That’s it for now!</h2>
<p>I don’t want to make this too long so I’ll end here. I hope this provided some insight into the research and development process involved in something like this. If you have any questions or would like to learn more, don’t hesitate to contact me!</p>
</div>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenTofu Response to HashiCorp's Cease and Desist Letter (118 pts)]]></title>
            <link>https://opentofu.org/blog/our-response-to-hashicorps-cease-and-desist/</link>
            <guid>40003692</guid>
            <pubDate>Thu, 11 Apr 2024 16:04:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://opentofu.org/blog/our-response-to-hashicorps-cease-and-desist/">https://opentofu.org/blog/our-response-to-hashicorps-cease-and-desist/</a>, See on <a href="https://news.ycombinator.com/item?id=40003692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container" itemprop="articleBody"><p>On April 3rd, we received a Cease and Desist letter from HashiCorp regarding our implementation of the "removed" block in OpenTofu, claiming copyright infringement on the part of one of our core developers. We were also made aware of an article posted that same day with the same accusations. We have investigated these claims and are publishing the C&amp;D letter, our response and the source code origin document resulting from our investigation.</p>
<p><strong>The OpenTofu team vehemently disagrees with any suggestion that it misappropriated, mis-sourced, or otherwise misused HashiCorp’s BSL code. All such statements have zero basis in facts.</strong></p>
<p>HashiCorp has made claims of copyright infringement in a cease &amp; desist letter. These claims are <strong>completely unsubstantiated</strong>.</p>
<p>The code in question can be clearly shown to have been copied from older code under the MPL-2.0 license. HashiCorp seems to have copied the same code itself when they implemented their version of this feature. All of this is easily visible in our detailed SCO analysis, as well as their own comments which indicate this.</p>

<ul>
<li><a href="https://opentofu.github.io/legal-documents/2024-04-03%20HashiCorp%20C%26D/OpenTofu%20C&amp;D%20-%20Redacted.pdf" target="_blank" rel="noopener noreferrer">HashiCorp's C&amp;D Letter</a></li>
<li><a href="https://opentofu.github.io/legal-documents/2024-04-03%20HashiCorp%20C%26D/OpenTofu%20C&amp;D%20Response%20-%20Redacted.pdf" target="_blank" rel="noopener noreferrer">Our Response</a></li>
<li>Source Code Origin Document: [<a href="https://opentofu.github.io/legal-documents/2024-04-03%20HashiCorp%20C%26D/SCO.html" target="_blank" rel="noopener noreferrer">HTML</a>, <a href="https://opentofu.github.io/legal-documents/2024-04-03%20HashiCorp%20C%26D/SCO.pdf" target="_blank" rel="noopener noreferrer">PDF</a>] <strong>⇐ For the detailed code analysis, see here.</strong></li>
</ul>
<p><em>To prevent further harassment of individual people, we have redacted any personal information from these documents.</em></p>

<p>Despite these events, we have managed to carry out significant development on OpenTofu 1.7, including state encryption, “for_each” implementation for “import” blocks, as well as the all-new <strong>provider-defined functions</strong> supported by the recently released provider plugin protocol.</p>
<p>On that note, we will be releasing a new pre-release version next week, and we are eager to gather feedback from the community.</p>
<p>— The OpenTofu Team</p>
<hr>
<p><small><p><em>The image in this blog post contains code licensed under the BUSL-1.1 by HashiCorp. However, for the purposes of this post we are making non-commercial, transformative fair use under <a href="https://www.govinfo.gov/content/pkg/USCODE-2022-title17/html/USCODE-2022-title17-chap1-sec107.htm" target="_blank" rel="noopener noreferrer">17 U.S. Code § 107</a>. <br> You can read more about fair use on the <a href="https://www.copyright.gov/fair-use/" target="_blank" rel="noopener noreferrer">website of the US Copyright Office</a>.</em></p></small></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New paintings found at Pompeii (168 pts)]]></title>
            <link>https://www.bbc.com/news/science-environment-68777741</link>
            <guid>40003138</guid>
            <pubDate>Thu, 11 Apr 2024 15:19:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/science-environment-68777741">https://www.bbc.com/news/science-environment-68777741</a>, See on <a href="https://news.ycombinator.com/item?id=40003138">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Apple alerts users in 92 nations to mercenary spyware attacks (433 pts)]]></title>
            <link>https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/</link>
            <guid>40002987</guid>
            <pubDate>Thu, 11 Apr 2024 15:06:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/">https://techcrunch.com/2024/04/10/apple-warning-mercenary-spyware-attacks/</a>, See on <a href="https://news.ycombinator.com/item?id=40002987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Apple sent threat notifications to iPhone users in 92 countries on Wednesday, warning them that they may have been targeted by mercenary spyware attacks.</p>
<p>The company said it sent the alerts to individuals in 92 nations at 12 p.m. Pacific Time Wednesday. The notification, which TechCrunch has seen, did not disclose the attackers’ identities or the countries where users received notifications.</p>
<p>“Apple detected that you are being targeted by a mercenary spyware attack that is trying to remotely compromise the iPhone associated with your Apple ID -xxx-,” it wrote in the warning to affected customers.</p>
<p>“This attack is likely targeting you specifically because of who you are or what you do. Although it’s never possible to achieve absolute certainty when detecting such attacks, Apple has high confidence in this warning — please take it seriously,” Apple added in the text.</p>
<p>The iPhone maker sends these kind of notifications <a href="https://techcrunch.com/2023/10/30/indian-opposition-leaders-says-apple-has-warned-them-of-state-sponsored-iphone-attacks/" target="_blank" rel="noopener">multiple times a year</a> and has notified users to such threats in over 150 countries since 2021, per an updated Apple <a href="https://support.apple.com/en-in/102174" target="_blank" rel="noopener">support page</a>.</p>
<p>Apple also sent an identical warning to a number of journalists and politicians in India in October last year. Later, nonprofit advocacy group Amnesty International <a href="https://techcrunch.com/2023/12/27/india-pressed-apple-on-state-sponsored-warnings-report-says/" target="_blank" rel="noopener">reported</a> that it had found Israeli spyware maker NSO Group’s invasive spyware Pegasus on the iPhones of prominent journalists in India. (Users in India are among those who have received Apple’s latest threat notifications, according to people familiar with the matter.)</p>
<p>The spyware alerts arrive at a time when many nations are preparing for elections. In recent months, many tech firms have cautioned about rising state-sponsored efforts to sway certain electoral outcomes. Apple’s alerts, however, did not remark on their timing.</p>
<p>“We are unable to provide more information about what caused us to send you this notification, as that may help mercenary spyware attackers adapt their behavior to evade detection in the future,” Apple told affected customers.</p>
<p>Apple <a href="https://web.archive.org/web/20240101053644/https://support.apple.com/en-in/102174" target="_blank" rel="noopener">previously</a> described the attackers as “state-sponsored” but has replaced all such references with “mercenary spyware attacks.”</p>
<p>The warning to customers adds: “Mercenary spyware attacks, such as those using Pegasus from the NSO Group, are exceptionally rare and vastly more sophisticated than regular cybercriminal activity or consumer malware.”</p>
<p>Apple said it relies solely on “internal threat-intelligence information and investigations to detect such attacks.”</p>
<p>“Although our investigations can never achieve absolute certainty, Apple threat notifications are high-confidence alerts that a user has been individually targeted by a mercenary spyware attack and should be taken very seriously,” it added.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vietnamese property tycoon sentenced to death in $27B fraud case (213 pts)]]></title>
            <link>https://www.theguardian.com/world/2024/apr/11/vietnamese-property-tycoon-sentenced-to-death-in-27bn-case</link>
            <guid>40002851</guid>
            <pubDate>Thu, 11 Apr 2024 14:53:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/world/2024/apr/11/vietnamese-property-tycoon-sentenced-to-death-in-27bn-case">https://www.theguardian.com/world/2024/apr/11/vietnamese-property-tycoon-sentenced-to-death-in-27bn-case</a>, See on <a href="https://news.ycombinator.com/item?id=40002851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>A prominent property tycoon has been sentenced to death for her role in Vietnam’s biggest-ever fraud case.</p><p>Truong My Lan, the chair of the developer Van Thinh Phat, was found guilty of embezzlement, bribery and violations of banking rules on Thursday, in a case that has shocked the country. A total of $12.5bn (£10bn) was embezzled, the equivalent of almost 3% of Vietnamese gross domestic product, but prosecutors said on Thursday the total damages caused by the scam now amounted to $27bn.</p><p>“The defendant’s actions … eroded people’s trust in the leadership of the [Communist] party and state,” read the verdict at the trial in Ho Chi Minh City.</p><p>Lan was found guilty of swindling money from Saigon Commercial Bank (SCB) over a decade. She had been on tried alongside 85 others, including former central bankers and government officials, as well as previous SCB executives.</p><p>The trial is part of a national corruption crackdown led by the secretary general of the Communist party of Vietnam, Nguyễn Phú Trọng. The campaign, which is also known as “Blazing Furnace” and has increased in recent years, has led to the indictment of thousands of people, as well as the <a href="https://www.theguardian.com/world/2024/mar/21/vietnam-president-vo-van-thuong-resignation" data-link-name="in body link">resignation of two presidents</a> and two deputy prime ministers.</p><p>Lan, who was arrested in October 2022, had denied the charges. A relative told Reuters before the verdict that she would appeal. The death sentence is an unusually severe punishment for a corruption case.</p><p>State media reported last week that Lan told the court she had joined the banking industry without sufficient experience and blamed a “lack of understanding of legal matters”. She said she had “thought of death” in desperation, and asked the court for leniency for her husband, a Hong Kong businessman, and niece who were on trial as accomplices.</p><p>The verdicts announced on Thursday followed a five-week trial that has been covered in great detail in Vietnam’s tightly controlled state media.</p><p>Documents related to the trial, kept in 105 boxes, weighed 6 tonnes, according to VN Express, which reported the authorities had installed security cameras and fire safety equipment to protect the evidence ahead of the hearings. More than 1,000 properties belonging to Lan have been seized, and nearly 2,700 individuals were summoned for the trial, which included 200 lawyers.</p><p>Dr Nguyen Khac Giang, a visiting fellow at the Vietnam studies programme at the Iseas-Yusof Ishak Institute, said the case was unprecedented, and a milestone in Vietnam’s anti-corruption crackdown. It was, he said, “the biggest case against a private business, with the biggest number of defendants, the biggest number of evidence and of course, the biggest number of money involved”.</p><p>Although Lan did not directly hold executive power at SCB, she owned 91.5% of the bank’s shares through third parties and shell companies.</p><p>She was accused of setting up fake loan applications to withdraw money from the bank over a period of 11 years, from 2012 to 2022. The loans accounted for 93% of the total credit the bank has issued, according to state media.</p><p>To cover up the fraud, Lan and other SCB bankers were accused of giving state officials $5.2m, the largest bribe recorded in Vietnam. The money was handed over in Styrofoam boxes, Do Thi Nhan, a former chief banking inspector at the State Bank of Vietnam, said during the trial. Nhan said that after realising the boxes contained money, she refused the boxes but that Lan declined to take them back, state media reported.</p><p>Since 2021, thousands of people have been indicted for corruption in the country, in what analysts have described as the most comprehensive anti-corruption effort in the history of the Communist party of Vietnam.</p><p>Last month, the Vietnamese government <a href="https://www.theguardian.com/world/2024/mar/21/vietnam-president-vo-van-thuong-resignation" data-link-name="in body link">announced the resignation of its second president in as many years, Vo Van Thuong</a>, over alleged “violations and flaws” that had “negatively affected public perception, as well as the reputation of the party and the state”. He had been in power for just over a year after his predecessor, <a href="https://www.theguardian.com/world/2023/jan/17/vietnam-president-nguyen-xuan-phuc-quits" data-link-name="in body link">Nguyen Xuan Phuc, was forced out</a> because of corruption scandals involving officials under his control.</p><p>Evaluating public sentiment in Vietnam, a one-party state, is challenging. However, social media comments suggested many were shocked by the scale of the scandal, said Giang. While some welcomed the state crackdown, others questions how corruption on such a huge scale could go unchecked for so long.</p><p>“[The case] might indirectly signal that the state hasn’t really been doing well in terms of managing the system in terms of the increasingly complex market economy and also the state is incapable of controlling their own public officials,” said Giang.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Anyone got a contact at OpenAI. They have a spider problem (658 pts)]]></title>
            <link>https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html</link>
            <guid>40001971</guid>
            <pubDate>Thu, 11 Apr 2024 13:34:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html">https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html</a>, See on <a href="https://news.ycombinator.com/item?id=40001971">Hacker News</a></p>
Couldn't get https://mailman.nanog.org/pipermail/nanog/2024-April/225407.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Stacking triangles for fun and profit (178 pts)]]></title>
            <link>https://www.oranlooney.com/post/angle-addition/</link>
            <guid>40001597</guid>
            <pubDate>Thu, 11 Apr 2024 12:57:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.oranlooney.com/post/angle-addition/">https://www.oranlooney.com/post/angle-addition/</a>, See on <a href="https://news.ycombinator.com/item?id=40001597">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
      <img src="https://www.oranlooney.com/post/angle-addition_files/lead.jpg">
      
      
      
      <hr>
      <ul>
        <li><time>April 8, 2024</time></li>
        <li>
          <a href="https://www.oranlooney.com/tags/math/">
            <i></i>
            Math
          </a>&nbsp;
        </li>
        <li>
          <a href="https://www.oranlooney.com/tags/visualization/">
            <i></i>
            Visualization
          </a>&nbsp;
        </li>
      </ul>
      

      

<p>One thing you may have noticed about the trigonometric functions <a href="https://en.wikipedia.org/wiki/Sine_and_cosine">sine and
cosine</a> is that they seem to have no agreed upon definition. Or rather,
different authors choose different definitions as the starting point, mainly
based on convenience. This isn’t problematic, or even particularly unusual in
mathematics - as long as we can derive any of the other forms from any starting
point, it makes little theoretical difference which we start from as they’re
all equivalent anyway.</p>

<p>The most common starting points are the series definitions, the solution to an
initial value problem involving ordinary differential equations, or using
complex numbers as Euler’s formula. You can find detailed descriptions of these
on the <a href="https://en.wikipedia.org/wiki/Sine_and_cosine">Wikipedia page</a>. These are all fine starting points as far as they
go, and as I said before they are all equivalent.</p>

<p>What struck me as odd when I was an undergraduate, and still strikes me to this
day, is that none of these are the obvious trigonometric definitions about the
opposite and adjacent sides of a right triangle. Aren’t axioms and definitions
supposed to be obvious, so obvious and self-evident they can’t be doubted? So
why are we using a highly non-obvious formulation as our definition, and then
backing into the intuitive form as a theorem? The answer is actually pretty
simple - the proofs are slightly shorter and more elegant if we do it that way.</p>

<p>However, I never liked this approach because it’s very much like pulling a
rabbit out of a hat, or perhaps more like pulling a “previously prepared”
turkey out of the oven on a cooking show. It gives students and completely
backward impression of how mathematics is done. We don’t start from intuitive
definitions and work on them until we can understand them more deeply or
connect them to other structures; no, we simply write down a bizarre and
unmotivated equation and show it has the desired properties, with no mention of
how anyone thought it up in the first place. This often leads to shorter, more
elegant proofs but at the cost of completely failing to teach the student how
to actually “do” mathematics.</p>

<p>I mean, look at this thing:</p>

<p>\[
\sin(x) = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{2n+1}
\]</p>

<p>Anyone who looks at that and says, “yes, that’s a self-evident definition” is
either lying or Ramanujan.</p>

<p>The differential equation definition is almost as bad. While the equations
themselves are fairly simple:</p>

<p>\[
\begin{align}
\frac{d^2y}{dt^2} &amp;= -y \\\<br>
y(0) &amp;= 0 ,&amp;
y’(0) &amp;= 1
\end{align}
\]</p>

<p>The problem is that we have to rely on the <a href="https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem">ODE existence and uniqueness
theorem</a> which is non-trivial to prove; the most common proof involves
invoking the <a href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem">Banach fixed point theorem</a>. That seems like a weirdly
technical approach to defining what should be an elementary concept.</p>

<p>What would be nice would be to start from the intuitive, geometric definition:</p>

<p>\[
\begin{align}
\sin(\theta) &amp;= \frac{\text{opposite}}{\text{hypotenuse}} \\\<br>
\cos(\theta) &amp;= \frac{\text{adjacent}}{\text{hypotenuse}}
\end{align}
\]</p>

<p>and derive the analytic definitions by working forward. I think I’ve found a
clear and unambiguous way to explain this at the undergraduate level. Briefly,
we’ll prove the angle addition formulas using geometric methods, then show how
this leads immediately to the other results.</p>

<p>Since we’re interested in establishing a <em>foundation</em> for sine and
cosine from geometric principles, we have to establish some ground rules. It’s
of crucial importance that you ignore everything you already know about these
functions. Yes, I know you can prove this stuff more easily from Euler’s
formula. Yes, I know there a dozen different ways to prove any of these. For
the moment, pretend like you don’t know <em>anything</em> about sine and cosine
except the geometric definitions, and we won’t use anything unless we’ve proved
it earlier. The structure of the proofs will be as follows:</p>

<p><img src="https://www.oranlooney.com/post/angle-addition_files/proof_structure.png"></p>

<p>As the goal is to ground everything on the geometric definition in an
easy-to-follow way, jumping ahead and using theorems before we’ve established
that foundation defeats the purpose.</p>

<h2 id="notation">Notation</h2>

<p>In the first draft of this proof, I left points unlabeled and simply referred
to “the middle triangle” or the “right angle in the top triangle” and so on.
The idea was to keep things simple for a broad audience but I quickly found it
was very hard to follow. Instead, we’ll use the conventional notation, which
I’ll briefly review here.</p>

<p>We use upper case roman letters such as $A$ or $B$ to label points. The line
segment connecting two points is written $\overline{AB}$. The triangle with
vertices $A$, $B$, and $C$ is written $\triangle ABC$. Even though the notation
symbol shows an equilateral triangle, the triangle in question doesn’t have to
be. I’ll write “The right triangle $\triangle ABC$” if it’s important to
specify that the triangle is a right triangle.</p>

<p>The above are all fairly self-explanatory, but the last bit of notation can be
confusing if you don’t know exactly what it means. To describe the angle at $B$
between $\overline{AB}$ and $\overline{BC}$, we write $\angle ABC$.</p>

<p><img src="https://www.oranlooney.com/post/angle-addition_files/angle.png"></p>

<p>$\angle ABC$ is always equal to $\angle CBA$, but $\angle BAC$ or $\angle ACB$
refer different angles located at a different part of the diagram. To locate an
angle $\angle ABC$ in the diagram, first look for the point labeled with the
middle letter $B$. Then imagine lines connecting $B$ to $A$ and $C$ - the angle
referred to is the angle between those lines.</p>

<p>I know this notation takes some getting used to, but it allows us to
unambiguously refer to angles on a cluttered diagram.</p>

<h2 id="geometric-proof-of-angle-addition-formulas">Geometric Proof of Angle Addition Formulas</h2>

<p>We set up the problem by simply stacking two right triangles on top of each
other.</p>

<p><img src="https://www.oranlooney.com/post/angle-addition_files/aa_01.png"></p>

<p>Let’s define $\overline{AB}$ to have a length of 1. $\overline{AB}$ is
hypotenuse of the right triangle $\triangle ABC$. Since the hypotenuse is
length 1, the lengths of opposite and adjacent sides are simple $\sin(\beta)$
and $\cos(\beta)$ respectively.</p>

<p><img src="https://www.oranlooney.com/post/angle-addition_files/aa_02.png"></p>

<p>We can do the same thing for the triangle $\triangle ACD$ and angle $\alpha$
but with the twist that hypotenuse of this triangle is no longer 1 but instead
$\cos(\beta)$. Therefore, the lengths of the opposite and adjacent sides have
to each be multiplied by $\cos(\beta)$.</p>

<p><img src="https://www.oranlooney.com/post/angle-addition_files/aa_03.png"></p>

<p>Let’s add another triangle to our diagram by extending $\overline{CD}$ out in a
straight line and drawing a perpendicular line through the point $B$. This
gives us the triangle BCE. Since $\overline{BE}$ is perpendicular to
$\overline{EC}$ it is in fact a right triangle.</p>

<p>Now, the line $\overline{EC}$ is perpendicular to $\overline{AD}$, and the line
$\overline{BC}$ is perpendicular is $\overline{AC}$, so the angle $\angle BCE$
is the same as the angle $\angle CAD$ which we called $\alpha$.</p>

<p>Now that we know the hypotenuse and one angle of the right triangle $\triangle
BCE$ we can once again use the definitions of sine and cosine to label the
lengths of the opposite and adjacent sides.</p>

<p><img src="https://www.oranlooney.com/post/angle-addition_files/aa_04.png"></p>

<p>We’ll do something similar for the last triangle. We’ll draw a line
perpendicular to $\overline{AD}$ through $A$ and extend it up to point $F$
where it intersects the extension of $\overline{BE}$. $\overline{FE}$ and
$\overline{AD}$ are parallel (because they are both perpendicular to
$\overline{AF}$) therefore $\angle FBA$ is equal to $\angle BAD$ which is the
sum of $\alpha$ and $\beta$. So we can label this angle $\alpha+\beta$. For the
fourth and final time, we’ll use the definition of sine and cosine to label
opposite and adjacent sides of the right triangle $\triangle FAB$. Note that this is where
the expressions $\cos(\alpha + \beta)$ and $\sin(\alpha + \beta)$ enter the
proof.</p>

<p><img src="https://www.oranlooney.com/post/angle-addition_files/aa_05.png"></p>

<p>So far, everything has been construction - adding triangles and chasing angles
to label edge lengths. But now the diagram is complete, and we can easily read
off the angle addition formulas by equating opposite sides.  Note that $FEAD$
forms a rectangle; $\overline{FE}$ and $\overline{AD}$ are parallel which
proves $\overline{AF}$ and $\overline{DE}$ are equal; likewise $\overline{AF}$
and $\overline{ED}$ are parallel, which proves $\overline{FE}$ and
$\overline{AD}$ are equal. All we have to do now is equate opposite pairs of sides:</p>

<p>\[
\begin{align}
    \cos(\alpha+\beta) + \sin(\alpha) \sin(\beta) = \cos(\alpha) \cos(\beta)  \\\<br>
    \sin(\alpha+\beta) = \sin(\alpha) \cos(\beta) +  \cos(\alpha) \sin(\beta)
\end{align}
\]</p>

<p>Finally, we rearrange these slightly to put them in the canonical form of the angle addition
formulas:</p>

<p>\[
\begin{align}
    \cos(\alpha+\beta) &amp;= \cos(\alpha) \cos(\beta) -  \sin(\alpha) \sin(\beta)  \\\<br>
    \sin(\alpha+\beta) &amp;= \sin(\alpha) \cos(\beta) +  \cos(\alpha) \sin(\beta)
\end{align}
\]</p>

<h2 id="pythagorean-theorem">Pythagorean Theorem</h2>

<p>OK, now that we have the angle addition formulas, let’s put them to work.</p>

<p>First, we’d like sine and cosine to trace out a unit circle; in other words, we
want to make sure that $\sin^2(\theta) + \cos^2(\theta) = 1$ for all angles
$\theta$.</p>

<p>There are lots of ways to prove this, but the angle addition formula provides
one of the neatest approaches. Instead of adding two separate angles $\alpha$
and $\beta$, we’ll use $\theta$ and $-\theta$. These two sum to zero so we
have:</p>

<p>\[
\begin{align}
    1  &amp;= \cos(0) \\\<br>
       &amp;= \cos(\theta - \theta) \\\<br>
       &amp;= \cos(\theta)\cos(-\theta) - \sin(\theta)\sin(-\theta) \\\<br>
       &amp;= \cos(\theta)\cos(\theta) + \sin(\theta)\sin(\theta) \\\<br>
       &amp;= \cos^2(\theta) + \sin^2(\theta)
\end{align}
\]</p>

<p>Here, we additionally used the fact that sine and cosine are odd and even
functions respectively, so $\cos(-x) = \cos(x)$ and $\sin(-x) = - \sin(x)$.</p>

<p>That means that the Pythagorean theorem is actually an immediate corollary of
the angle addition formula for cosine.</p>

<h2 id="derivatives">Derivatives</h2>

<p>Another neat thing we can do with the angle addition formulas is calculate the derivatives
of sine and cosine. This is because the limit definition of derivative includes a term with $f(x+h)$
which we can handle with the formulas.</p>

<p>\[
\begin{align}
    \frac{d}{dx} \sin(x) &amp;= \lim_{h \to 0} \frac{\sin(x + h) - \sin(x)}{h} \\\<br>
\end{align}
\]</p>

<p>\[
\begin{align}
     &amp;= \lim_{h \to 0} \frac{\sin(x)\cos(h) + \cos(x)\sin(h) - \sin(x)}{h}
\end{align}
\]</p>

<p>By considering a triangle with hypotenuse 1 and a very small “opposite” side,
it’s not hard to see geometrically that $\sin(h) \approx h$ and $\cos(h) \approx 1$
when $h$ is close to zero. Therefore we have:</p>

<p>\[
\begin{align}
\frac{d}{dx} \sin(x) &amp;= \lim_{h \to 0} \frac{\sin(x) + \cos(x) h - \sin(x)}{h}
\end{align}
\]</p>

<p>\[
\begin{align}
                     &amp;= \lim_{h \to 0} \frac{\cos(x) h}{h}
\end{align}
\]</p>

<p>\[
\begin{align}
                     &amp;= \cos(x)
\end{align}
\]</p>

<p>The equivalent argument for $\cos(x)$ is:</p>

<p>\[
\begin{align}
\frac{d}{dx} \cos(x) &amp;= \lim_{h \to 0} \frac{\cos(x + h) - \cos(x)}{h}
\end{align}
\]</p>

<p>\[
\begin{align}
                     &amp;= \lim_{h \to 0} \frac{\cos(x)\cos(h) - \sin(x)\sin(h) - \cos(x)}{h}
\end{align}
\]</p>

<p>\[
\begin{align}
                     &amp;= \lim_{h \to 0} \frac{\cos(x) - \sin(x) h - \cos(x)}{h}
\end{align}
\]</p>

<p>\[
\begin{align}
                     &amp;= \lim_{h \to 0} \frac{-\sin(x) h}{h}
\end{align}
\]</p>

<p>\[
\begin{align}
                     &amp;= -\sin(x)
\end{align}
\]</p>

<p>Once we know these first derivatives, computing higher derivates is simple, for example:</p>

<p>\[
\frac{d^2}{dx^2} \sin(x) = \frac{d}{dx} \cos(x) = -\sin(x)
\]</p>

<p>\[
\frac{d^2}{dx^2} \cos(x) = \frac{d}{dx} -\sin(x) = -\cos(x)
\]</p>

<p>With just these second derivatives, we can already motivate the initial value problem ODE
definition of sine and cosine.</p>

<p>It’s equally obvious that we can continue the process indefinitely, alternating between
sine and cosine. Since we know $\sin(0) = 0$ and $\cos(0) = 1$, we can evaluate all derivatives
of sine and cosine at zero, allowing us to calculate the Maclaurin series. This gives us
the series form.</p>

<h2 id="arc-length">Arc Length</h2>

<p>The above shows that sine and cosine trace out a unit circle, but there is one
final thing we need to show to fully connect the geometric and analytic
definitions.</p>

<p>The formula for arc length is:</p>

<p>\[
L = \int_{a}^{b} \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2} \, dt
\]</p>

<p>Let’s write down the equations for the unit circle in parametric form. Luckily,
we already worked out the first derivatives:
\[
\begin{align}
    x(t) = \cos(t), &amp; \frac{dx}{dt} = -\sin(t)
\end{align}<br>
\]</p>

<p>\[
\begin{align}
y(t) = \sin(t), &amp; \frac{dy}{dt} = \cos(t)
\end{align}
\]</p>

<p>Then, substituting these into the arc length formula:
\[
L = \int_{0}^{\theta} \sqrt{(-\sin(t))^2 + (\cos(t))^2} \, dt
\]</p>

<p>Simplifying inside the square root:
\[
L = \int_{0}^{\theta} \sqrt{\sin^2(t) + \cos^2(t)} \, dt
\]</p>

<p>Since we showed above that $\sin^2(t) + \cos^2(t) = 1$ for all $t$, this
further simplifies to:
\[
L = \int_{0}^{\theta} 1 \, dt
\]</p>

<p>Integrating from $0$ to $\theta$:
\[
L = \left. t \right|_{0}^{\theta} = \theta - 0 = \theta
\]</p>

<!-- x_ -->

<p>This tells us that the parameter $t$ we used is equal to the arc length along
the unit circle; in other words, the angle expressed in radians.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We’ve shown that all of usual definitions of sine and cosine can be derived
from the geometric definition, and that this can be made elementary if we start
by proving the angle addition formulas using geometric arguments. The geometric
proof itself is quite beautiful and easy to remember as it is simply a matter
of stacking two triangles, building a rectangle around them, and equating the
opposing sides. We offer this as a more pedagogically sound and historical
accurate way to motivate the various definitions of sine and cosine.</p>

    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Transformers.js –  Run Transformers directly in the browser (229 pts)]]></title>
            <link>https://github.com/xenova/transformers.js</link>
            <guid>40001193</guid>
            <pubDate>Thu, 11 Apr 2024 11:57:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/xenova/transformers.js">https://github.com/xenova/transformers.js</a>, See on <a href="https://news.ycombinator.com/item?id=40001193">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
    <br>
    <themed-picture data-catalyst-inline="true"><picture> 
        <source media="(prefers-color-scheme: dark)" srcset="https://camo.githubusercontent.com/e55d794af470c9b405283b492e6a07aee928185a752fc6e613679fa6bde93478/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f58656e6f76612f7472616e73666f726d6572732e6a732d646f63732f7261772f6d61696e2f7472616e73666f726d6572736a732d6461726b2e737667" width="500" data-canonical-src="https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-dark.svg">
        <source media="(prefers-color-scheme: light)" srcset="https://camo.githubusercontent.com/10bd3c5284d4104279c2b977506ea5e9bebf83fbd45715830a39d4a44def28b3/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f58656e6f76612f7472616e73666f726d6572732e6a732d646f63732f7261772f6d61696e2f7472616e73666f726d6572736a732d6c696768742e737667" width="500" data-canonical-src="https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-light.svg">
        <img alt="transformers.js javascript library logo" src="https://camo.githubusercontent.com/10bd3c5284d4104279c2b977506ea5e9bebf83fbd45715830a39d4a44def28b3/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f58656e6f76612f7472616e73666f726d6572732e6a732d646f63732f7261772f6d61696e2f7472616e73666f726d6572736a732d6c696768742e737667" width="500" data-canonical-src="https://huggingface.co/datasets/Xenova/transformers.js-docs/raw/main/transformersjs-light.svg">
    </picture></themed-picture>
    <br>
</p>
<p dir="auto">
    <a href="https://www.npmjs.com/package/@xenova/transformers" rel="nofollow">
        <img alt="NPM" src="https://camo.githubusercontent.com/4f705dd79f06a166012e671df1d8456cdb17bc1ba7c7410ca1dce062e758b1a7/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f4078656e6f76612f7472616e73666f726d657273" data-canonical-src="https://img.shields.io/npm/v/@xenova/transformers">
    </a>
    <a href="https://www.npmjs.com/package/@xenova/transformers" rel="nofollow">
        <img alt="NPM Downloads" src="https://camo.githubusercontent.com/a4c5b6ffd4d92bf1b8f68b1446c397649a2acdbeb4f360a4144e7cf73175ef0d/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f64772f4078656e6f76612f7472616e73666f726d657273" data-canonical-src="https://img.shields.io/npm/dw/@xenova/transformers">
    </a>
    <a href="https://www.jsdelivr.com/package/npm/@xenova/transformers" rel="nofollow">
        <img alt="jsDelivr Hits" src="https://camo.githubusercontent.com/3b81259823a5e4da5cd7d8dd46b9a962e99547ccb610a18c59621a4a329fd670/68747470733a2f2f696d672e736869656c64732e696f2f6a7364656c6976722f6e706d2f68772f4078656e6f76612f7472616e73666f726d657273" data-canonical-src="https://img.shields.io/jsdelivr/npm/hw/@xenova/transformers">
    </a>
    <a href="https://github.com/xenova/transformers.js/blob/main/LICENSE">
        <img alt="License" src="https://camo.githubusercontent.com/a1df49724bc051547675d8e544f351f71a4ab2382771ca46af9d471107333050/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f78656e6f76612f7472616e73666f726d6572732e6a733f636f6c6f723d626c7565" data-canonical-src="https://img.shields.io/github/license/xenova/transformers.js?color=blue">
    </a>
    <a href="https://huggingface.co/docs/transformers.js/index" rel="nofollow">
        <img alt="Documentation" src="https://camo.githubusercontent.com/652b71cd0fafb5f0c2cec2d463539708b4e8b875772134430935ff3127655d47/68747470733a2f2f696d672e736869656c64732e696f2f776562736974652f687474702f68756767696e67666163652e636f2f646f63732f7472616e73666f726d6572732e6a732f696e6465782e7376673f646f776e5f636f6c6f723d72656426646f776e5f6d6573736167653d6f66666c696e652675705f6d6573736167653d6f6e6c696e65" data-canonical-src="https://img.shields.io/website/http/huggingface.co/docs/transformers.js/index.svg?down_color=red&amp;down_message=offline&amp;up_message=online">
    </a>
</p>
<p dir="auto">State-of-the-art Machine Learning for the web. Run 🤗 Transformers directly in your browser, with no need for a server!</p>
<p dir="auto">Transformers.js is designed to be functionally equivalent to Hugging Face's <a href="https://github.com/huggingface/transformers">transformers</a> python library, meaning you can run the same pretrained models using a very similar API. These models support common tasks in different modalities, such as:</p>
<ul dir="auto">
<li>📝 <strong>Natural Language Processing</strong>: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.</li>
<li>🖼️ <strong>Computer Vision</strong>: image classification, object detection, and segmentation.</li>
<li>🗣️ <strong>Audio</strong>: automatic speech recognition and audio classification.</li>
<li>🐙 <strong>Multimodal</strong>: zero-shot image classification.</li>
</ul>
<p dir="auto">Transformers.js uses <a href="https://onnxruntime.ai/" rel="nofollow">ONNX Runtime</a> to run models in the browser. The best part about it, is that you can easily <a href="#convert-your-models-to-onnx">convert</a> your pretrained PyTorch, TensorFlow, or JAX models to ONNX using <a href="https://github.com/huggingface/optimum#onnx--onnx-runtime">🤗 Optimum</a>.</p>
<p dir="auto">For more information, check out the full <a href="https://huggingface.co/docs/transformers.js" rel="nofollow">documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick tour</h2><a id="user-content-quick-tour" aria-label="Permalink: Quick tour" href="#quick-tour"></a></p>
<p dir="auto">It's super simple to translate from existing code! Just like the python library, we support the <code>pipeline</code> API. Pipelines group together a pretrained model with preprocessing of inputs and postprocessing of outputs, making it the easiest way to run models with the library.</p>
<table>
<tbody><tr>
<th><b>Python (original)</b></th>
<th><b>Javascript (ours)</b></th>
</tr>
<tr>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="from transformers import pipeline

# Allocate a pipeline for sentiment-analysis
pipe = pipeline('sentiment-analysis')

out = pipe('I love transformers!')
# [{'label': 'POSITIVE', 'score': 0.999806941}]"><pre><span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>

<span># Allocate a pipeline for sentiment-analysis</span>
<span>pipe</span> <span>=</span> <span>pipeline</span>(<span>'sentiment-analysis'</span>)

<span>out</span> <span>=</span> <span>pipe</span>(<span>'I love transformers!'</span>)
<span># [{'label': 'POSITIVE', 'score': 0.999806941}]</span></pre></div>
</td>
<td>
<div dir="auto" data-snippet-clipboard-copy-content="import { pipeline } from '@xenova/transformers';

// Allocate a pipeline for sentiment-analysis
let pipe = await pipeline('sentiment-analysis');

let out = await pipe('I love transformers!');
// [{'label': 'POSITIVE', 'score': 0.999817686}]"><pre><span>import</span> <span>{</span> <span>pipeline</span> <span>}</span> <span>from</span> <span>'@xenova/transformers'</span><span>;</span>

<span>// Allocate a pipeline for sentiment-analysis</span>
<span>let</span> <span>pipe</span> <span>=</span> <span>await</span> <span>pipeline</span><span>(</span><span>'sentiment-analysis'</span><span>)</span><span>;</span>

<span>let</span> <span>out</span> <span>=</span> <span>await</span> <span>pipe</span><span>(</span><span>'I love transformers!'</span><span>)</span><span>;</span>
<span>// [{'label': 'POSITIVE', 'score': 0.999817686}]</span></pre></div>
</td>
</tr>
</tbody></table>
<p dir="auto">You can also use a different model by specifying the model id or path as the second argument to the <code>pipeline</code> function. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="// Use a different model for sentiment-analysis
let pipe = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment');"><pre><span>// Use a different model for sentiment-analysis</span>
<span>let</span> <span>pipe</span> <span>=</span> <span>await</span> <span>pipeline</span><span>(</span><span>'sentiment-analysis'</span><span>,</span> <span>'Xenova/bert-base-multilingual-uncased-sentiment'</span><span>)</span><span>;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto">To install via <a href="https://www.npmjs.com/package/@xenova/transformers" rel="nofollow">NPM</a>, run:</p>
<div dir="auto" data-snippet-clipboard-copy-content="npm i @xenova/transformers"><pre>npm i @xenova/transformers</pre></div>
<p dir="auto">Alternatively, you can use it in vanilla JS, without any bundler, by using a CDN or static hosting. For example, using <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules" rel="nofollow">ES Modules</a>, you can import the library with:</p>
<div dir="auto" data-snippet-clipboard-copy-content="<script type=&quot;module&quot;>
    import { pipeline } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.0';
</script>"><pre><span>&lt;</span><span>script</span> <span>type</span>="<span>module</span>"<span>&gt;</span>
    <span>import</span> <span>{</span> <span>pipeline</span> <span>}</span> <span>from</span> <span>'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.0'</span><span>;</span>
<span>&lt;/</span><span>script</span><span>&gt;</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Want to jump straight in? Get started with one of our sample applications/templates:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
<th>Links</th>
</tr>
</thead>
<tbody>
<tr>
<td>Whisper Web</td>
<td>Speech recognition w/ Whisper</td>
<td><a href="https://github.com/xenova/whisper-web">code</a>, <a href="https://huggingface.co/spaces/Xenova/whisper-web" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Doodle Dash</td>
<td>Real-time sketch-recognition game</td>
<td><a href="https://huggingface.co/blog/ml-web-games" rel="nofollow">blog</a>, <a href="https://github.com/xenova/doodle-dash">code</a>, <a href="https://huggingface.co/spaces/Xenova/doodle-dash" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Code Playground</td>
<td>In-browser code completion website</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/code-completion/">code</a>, <a href="https://huggingface.co/spaces/Xenova/ai-code-playground" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Semantic Image Search (client-side)</td>
<td>Search for images with text</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/semantic-image-search-client/">code</a>, <a href="https://huggingface.co/spaces/Xenova/semantic-image-search-client" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Semantic Image Search (server-side)</td>
<td>Search for images with text (Supabase)</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/semantic-image-search/">code</a>, <a href="https://huggingface.co/spaces/Xenova/semantic-image-search" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Vanilla JavaScript</td>
<td>In-browser object detection</td>
<td><a href="https://scrimba.com/scrim/cKm9bDAg" rel="nofollow">video</a>, <a href="https://github.com/xenova/transformers.js/tree/main/examples/vanilla-js/">code</a>, <a href="https://huggingface.co/spaces/Scrimba/vanilla-js-object-detector" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>React</td>
<td>Multilingual translation website</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/react-translator/">code</a>, <a href="https://huggingface.co/spaces/Xenova/react-translator" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Text to speech (client-side)</td>
<td>In-browser speech synthesis</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/text-to-speech-client/">code</a>, <a href="https://huggingface.co/spaces/Xenova/text-to-speech-client" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Browser extension</td>
<td>Text classification extension</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/extension/">code</a></td>
</tr>
<tr>
<td>Electron</td>
<td>Text classification application</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/electron/">code</a></td>
</tr>
<tr>
<td>Next.js (client-side)</td>
<td>Sentiment analysis (in-browser inference)</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/next-client/">code</a>, <a href="https://huggingface.co/spaces/Xenova/next-example-app" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Next.js (server-side)</td>
<td>Sentiment analysis (Node.js inference)</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/next-server/">code</a>, <a href="https://huggingface.co/spaces/Xenova/next-server-example-app" rel="nofollow">demo</a></td>
</tr>
<tr>
<td>Node.js</td>
<td>Sentiment analysis API</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/node/">code</a></td>
</tr>
<tr>
<td>Demo site</td>
<td>A collection of demos</td>
<td><a href="https://github.com/xenova/transformers.js/tree/main/examples/demo-site/">code</a>, <a href="https://xenova.github.io/transformers.js/" rel="nofollow">demo</a></td>
</tr>
</tbody>
</table>
<p dir="auto">Check out the Transformers.js <a href="https://huggingface.co/new-space?template=static-templates%2Ftransformers.js" rel="nofollow">template</a> on Hugging Face to get started in one click!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Custom usage</h2><a id="user-content-custom-usage" aria-label="Permalink: Custom usage" href="#custom-usage"></a></p>
<p dir="auto">By default, Transformers.js uses <a href="https://huggingface.co/models?library=transformers.js" rel="nofollow">hosted pretrained models</a> and <a href="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.0/dist/" rel="nofollow">precompiled WASM binaries</a>, which should work out-of-the-box. You can customize this as follows:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Settings</h3><a id="user-content-settings" aria-label="Permalink: Settings" href="#settings"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { env } from '@xenova/transformers';

// Specify a custom location for models (defaults to '/models/').
env.localModelPath = '/path/to/models/';

// Disable the loading of remote models from the Hugging Face Hub:
env.allowRemoteModels = false;

// Set location of .wasm files. Defaults to use a CDN.
env.backends.onnx.wasm.wasmPaths = '/path/to/files/';"><pre><span>import</span> <span>{</span> <span>env</span> <span>}</span> <span>from</span> <span>'@xenova/transformers'</span><span>;</span>

<span>// Specify a custom location for models (defaults to '/models/').</span>
<span>env</span><span>.</span><span>localModelPath</span> <span>=</span> <span>'/path/to/models/'</span><span>;</span>

<span>// Disable the loading of remote models from the Hugging Face Hub:</span>
<span>env</span><span>.</span><span>allowRemoteModels</span> <span>=</span> <span>false</span><span>;</span>

<span>// Set location of .wasm files. Defaults to use a CDN.</span>
<span>env</span><span>.</span><span>backends</span><span>.</span><span>onnx</span><span>.</span><span>wasm</span><span>.</span><span>wasmPaths</span> <span>=</span> <span>'/path/to/files/'</span><span>;</span></pre></div>
<p dir="auto">For a full list of available settings, check out the <a href="https://huggingface.co/docs/transformers.js/api/env" rel="nofollow">API Reference</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Convert your models to ONNX</h3><a id="user-content-convert-your-models-to-onnx" aria-label="Permalink: Convert your models to ONNX" href="#convert-your-models-to-onnx"></a></p>
<p dir="auto">We recommend using our <a href="https://github.com/xenova/transformers.js/blob/main/scripts/convert.py">conversion script</a> to convert your PyTorch, TensorFlow, or JAX models to ONNX in a single command. Behind the scenes, it uses <a href="https://huggingface.co/docs/optimum" rel="nofollow">🤗 Optimum</a> to perform conversion and quantization of your model.</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.convert --quantize --model_id <model_name_or_path>"><pre>python -m scripts.convert --quantize --model_id <span>&lt;</span>model_name_or_path<span>&gt;</span></pre></div>
<p dir="auto">For example, convert and quantize <a href="https://huggingface.co/bert-base-uncased" rel="nofollow">bert-base-uncased</a> using:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python -m scripts.convert --quantize --model_id bert-base-uncased"><pre>python -m scripts.convert --quantize --model_id bert-base-uncased</pre></div>
<p dir="auto">This will save the following files to <code>./models/</code>:</p>
<div data-snippet-clipboard-copy-content="bert-base-uncased/
├── config.json
├── tokenizer.json
├── tokenizer_config.json
└── onnx/
    ├── model.onnx
    └── model_quantized.onnx"><pre><code>bert-base-uncased/
├── config.json
├── tokenizer.json
├── tokenizer_config.json
└── onnx/
    ├── model.onnx
    └── model_quantized.onnx
</code></pre></div>
<p dir="auto">For the full list of supported architectures, see the <a href="https://huggingface.co/docs/optimum/main/en/exporters/onnx/overview" rel="nofollow">Optimum documentation</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported tasks/models</h2><a id="user-content-supported-tasksmodels" aria-label="Permalink: Supported tasks/models" href="#supported-tasksmodels"></a></p>
<p dir="auto">Here is the list of all tasks and architectures currently supported by Transformers.js.
If you don't see your task/model listed here or it is not yet supported, feel free
to open up a feature request <a href="https://github.com/xenova/transformers.js/issues/new/choose">here</a>.</p>
<p dir="auto">To find compatible models on the Hub, select the "transformers.js" library tag in the filter menu (or visit <a href="https://huggingface.co/models?library=transformers.js" rel="nofollow">this link</a>).
You can refine your search by selecting the task you're interested in (e.g., <a href="https://huggingface.co/models?pipeline_tag=text-classification&amp;library=transformers.js" rel="nofollow">text-classification</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Tasks</h3><a id="user-content-tasks" aria-label="Permalink: Tasks" href="#tasks"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Natural Language Processing</h4><a id="user-content-natural-language-processing" aria-label="Permalink: Natural Language Processing" href="#natural-language-processing"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/fill-mask" rel="nofollow">Fill-Mask</a></td>
<td><code>fill-mask</code></td>
<td>Masking some of the words in a sentence and predicting which words should replace those masks.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FillMaskPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/question-answering" rel="nofollow">Question Answering</a></td>
<td><code>question-answering</code></td>
<td>Retrieve the answer to a question from a given text.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.QuestionAnsweringPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=question-answering&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/sentence-similarity" rel="nofollow">Sentence Similarity</a></td>
<td><code>sentence-similarity</code></td>
<td>Determining how similar two texts are.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=feature-extraction&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/summarization" rel="nofollow">Summarization</a></td>
<td><code>summarization</code></td>
<td>Producing a shorter version of a document while preserving its important information.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.SummarizationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=summarization&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/table-question-answering" rel="nofollow">Table Question Answering</a></td>
<td><code>table-question-answering</code></td>
<td>Answering a question about information from a given table.</td>
<td>❌</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-classification" rel="nofollow">Text Classification</a></td>
<td><code>text-classification</code> or <code>sentiment-analysis</code></td>
<td>Assigning a label or class to a given text.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=text-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-generation#completion-generation-models" rel="nofollow">Text Generation</a></td>
<td><code>text-generation</code></td>
<td>Producing new text by predicting the next word in a sequence.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextGenerationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-generation#text-to-text-generation-models" rel="nofollow">Text-to-text Generation</a></td>
<td><code>text2text-generation</code></td>
<td>Converting one text sequence into another text sequence.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.Text2TextGenerationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=text2text-generation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/token-classification" rel="nofollow">Token Classification</a></td>
<td><code>token-classification</code> or <code>ner</code></td>
<td>Assigning a label to each token in a text.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TokenClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=token-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/translation" rel="nofollow">Translation</a></td>
<td><code>translation</code></td>
<td>Converting text from one language to another.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TranslationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=translation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/zero-shot-classification" rel="nofollow">Zero-Shot Classification</a></td>
<td><code>zero-shot-classification</code></td>
<td>Classifying text into classes that are unseen during training.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=zero-shot-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/feature-extraction" rel="nofollow">Feature Extraction</a></td>
<td><code>feature-extraction</code></td>
<td>Transforming raw data into numerical features that can be processed while preserving the information in the original dataset.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=feature-extraction&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Vision</h4><a id="user-content-vision" aria-label="Permalink: Vision" href="#vision"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/depth-estimation" rel="nofollow">Depth Estimation</a></td>
<td><code>depth-estimation</code></td>
<td>Predicting the depth of objects present in an image.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DepthEstimationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=depth-estimation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-classification" rel="nofollow">Image Classification</a></td>
<td><code>image-classification</code></td>
<td>Assigning a label or class to an entire image.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-segmentation" rel="nofollow">Image Segmentation</a></td>
<td><code>image-segmentation</code></td>
<td>Divides an image into segments where each pixel is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageSegmentationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-segmentation&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-to-image" rel="nofollow">Image-to-Image</a></td>
<td><code>image-to-image</code></td>
<td>Transforming a source image to match the characteristics of a target image or a target image domain.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToImagePipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-to-image&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/mask-generation" rel="nofollow">Mask Generation</a></td>
<td><code>mask-generation</code></td>
<td>Generate masks for the objects in an image.</td>
<td>❌</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/object-detection" rel="nofollow">Object Detection</a></td>
<td><code>object-detection</code></td>
<td>Identify objects of certain defined classes within an image.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ObjectDetectionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=object-detection&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/video-classification" rel="nofollow">Video Classification</a></td>
<td>n/a</td>
<td>Assigning a label or class to an entire video.</td>
<td>❌</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/unconditional-image-generation" rel="nofollow">Unconditional Image Generation</a></td>
<td>n/a</td>
<td>Generating images with no condition in any context (like a prompt text or another image).</td>
<td>❌</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-feature-extraction" rel="nofollow">Image Feature Extraction</a></td>
<td><code>image-feature-extraction</code></td>
<td>Transforming raw data into numerical features that can be processed while preserving the information in the original image.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageFeatureExtractionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-feature-extraction&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Audio</h4><a id="user-content-audio" aria-label="Permalink: Audio" href="#audio"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/audio-classification" rel="nofollow">Audio Classification</a></td>
<td><code>audio-classification</code></td>
<td>Assigning a label or class to a given audio.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AudioClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=audio-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/audio-to-audio" rel="nofollow">Audio-to-Audio</a></td>
<td>n/a</td>
<td>Generating audio from an input audio source.</td>
<td>❌</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/automatic-speech-recognition" rel="nofollow">Automatic Speech Recognition</a></td>
<td><code>automatic-speech-recognition</code></td>
<td>Transcribing a given audio into text.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AutomaticSpeechRecognitionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-to-speech" rel="nofollow">Text-to-Speech</a></td>
<td><code>text-to-speech</code> or <code>text-to-audio</code></td>
<td>Generating natural-sounding speech given text input.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextToAudioPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=text-to-audio&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Tabular</h4><a id="user-content-tabular" aria-label="Permalink: Tabular" href="#tabular"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/tabular-classification" rel="nofollow">Tabular Classification</a></td>
<td>n/a</td>
<td>Classifying a target category (a group) based on set of attributes.</td>
<td>❌</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/tabular-regression" rel="nofollow">Tabular Regression</a></td>
<td>n/a</td>
<td>Predicting a numerical value given a set of attributes.</td>
<td>❌</td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Multimodal</h4><a id="user-content-multimodal" aria-label="Permalink: Multimodal" href="#multimodal"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/document-question-answering" rel="nofollow">Document Question Answering</a></td>
<td><code>document-question-answering</code></td>
<td>Answering questions on document images.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DocumentQuestionAnsweringPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=document-question-answering&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/image-to-text" rel="nofollow">Image-to-Text</a></td>
<td><code>image-to-text</code></td>
<td>Output text from a given image.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToTextPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=image-to-text&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/text-to-image" rel="nofollow">Text-to-Image</a></td>
<td><code>text-to-image</code></td>
<td>Generates images from input text.</td>
<td>❌</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/visual-question-answering" rel="nofollow">Visual Question Answering</a></td>
<td><code>visual-question-answering</code></td>
<td>Answering open-ended questions based on an image.</td>
<td>❌</td>
</tr>
<tr>
<td><a href="https://huggingface.co/learn/audio-course/chapter4/classification_models#zero-shot-audio-classification" rel="nofollow">Zero-Shot Audio Classification</a></td>
<td><code>zero-shot-audio-classification</code></td>
<td>Classifying audios into classes that are unseen during training.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotAudioClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?other=zero-shot-audio-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/zero-shot-image-classification" rel="nofollow">Zero-Shot Image Classification</a></td>
<td><code>zero-shot-image-classification</code></td>
<td>Classifying images into classes that are unseen during training.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotImageClassificationPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
<tr>
<td><a href="https://huggingface.co/tasks/zero-shot-object-detection" rel="nofollow">Zero-Shot Object Detection</a></td>
<td><code>zero-shot-object-detection</code></td>
<td>Identify objects of classes that are unseen during training.</td>
<td>✅ <a href="https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotObjectDetectionPipeline" rel="nofollow">(docs)</a><br><a href="https://huggingface.co/models?other=zero-shot-object-detection&amp;library=transformers.js" rel="nofollow">(models)</a></td>
</tr>
</tbody>
</table>
<p dir="auto"><h4 tabindex="-1" dir="auto">Reinforcement Learning</h4><a id="user-content-reinforcement-learning" aria-label="Permalink: Reinforcement Learning" href="#reinforcement-learning"></a></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>ID</th>
<th>Description</th>
<th>Supported?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/tasks/reinforcement-learning" rel="nofollow">Reinforcement Learning</a></td>
<td>n/a</td>
<td>Learning from actions by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback.</td>
<td>❌</td>
</tr>
</tbody>
</table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Models</h3><a id="user-content-models" aria-label="Permalink: Models" href="#models"></a></p>
<ol dir="auto">
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/albert" rel="nofollow">ALBERT</a></strong> (from Google Research and the Toyota Technological Institute at Chicago) released with the paper <a href="https://arxiv.org/abs/1909.11942" rel="nofollow">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer" rel="nofollow">Audio Spectrogram Transformer</a></strong> (from MIT) released with the paper <a href="https://arxiv.org/abs/2104.01778" rel="nofollow">AST: Audio Spectrogram Transformer</a> by Yuan Gong, Yu-An Chung, James Glass.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/bart" rel="nofollow">BART</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/beit" rel="nofollow">BEiT</a></strong> (from Microsoft) released with the paper <a href="https://arxiv.org/abs/2106.08254" rel="nofollow">BEiT: BERT Pre-Training of Image Transformers</a> by Hangbo Bao, Li Dong, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/bert" rel="nofollow">BERT</a></strong> (from Google) released with the paper <a href="https://arxiv.org/abs/1810.04805" rel="nofollow">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/blenderbot" rel="nofollow">Blenderbot</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2004.13637" rel="nofollow">Recipes for building an open-domain chatbot</a> by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/blenderbot-small" rel="nofollow">BlenderbotSmall</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2004.13637" rel="nofollow">Recipes for building an open-domain chatbot</a> by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/bloom" rel="nofollow">BLOOM</a></strong> (from BigScience workshop) released by the <a href="https://bigscience.huggingface.co/" rel="nofollow">BigScience Workshop</a>.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/camembert" rel="nofollow">CamemBERT</a></strong> (from Inria/Facebook/Sorbonne) released with the paper <a href="https://arxiv.org/abs/1911.03894" rel="nofollow">CamemBERT: a Tasty French Language Model</a> by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/chinese_clip" rel="nofollow">Chinese-CLIP</a></strong> (from OFA-Sys) released with the paper <a href="https://arxiv.org/abs/2211.01335" rel="nofollow">Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</a> by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/clap" rel="nofollow">CLAP</a></strong> (from LAION-AI) released with the paper <a href="https://arxiv.org/abs/2211.06687" rel="nofollow">Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</a> by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/clip" rel="nofollow">CLIP</a></strong> (from OpenAI) released with the paper <a href="https://arxiv.org/abs/2103.00020" rel="nofollow">Learning Transferable Visual Models From Natural Language Supervision</a> by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/clipseg" rel="nofollow">CLIPSeg</a></strong> (from University of Göttingen) released with the paper <a href="https://arxiv.org/abs/2112.10003" rel="nofollow">Image Segmentation Using Text and Image Prompts</a> by Timo Lüddecke and Alexander Ecker.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/codegen" rel="nofollow">CodeGen</a></strong> (from Salesforce) released with the paper <a href="https://arxiv.org/abs/2203.13474" rel="nofollow">A Conversational Paradigm for Program Synthesis</a> by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/llama_code" rel="nofollow">CodeLlama</a></strong> (from MetaAI) released with the paper <a href="https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/" rel="nofollow">Code Llama: Open Foundation Models for Code</a> by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/convbert" rel="nofollow">ConvBERT</a></strong> (from YituTech) released with the paper <a href="https://arxiv.org/abs/2008.02496" rel="nofollow">ConvBERT: Improving BERT with Span-based Dynamic Convolution</a> by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/convnext" rel="nofollow">ConvNeXT</a></strong> (from Facebook AI) released with the paper <a href="https://arxiv.org/abs/2201.03545" rel="nofollow">A ConvNet for the 2020s</a> by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/convnextv2" rel="nofollow">ConvNeXTV2</a></strong> (from Facebook AI) released with the paper <a href="https://arxiv.org/abs/2301.00808" rel="nofollow">ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</a> by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/deberta" rel="nofollow">DeBERTa</a></strong> (from Microsoft) released with the paper <a href="https://arxiv.org/abs/2006.03654" rel="nofollow">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a> by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/deberta-v2" rel="nofollow">DeBERTa-v2</a></strong> (from Microsoft) released with the paper <a href="https://arxiv.org/abs/2006.03654" rel="nofollow">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a> by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/deit" rel="nofollow">DeiT</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2012.12877" rel="nofollow">Training data-efficient image transformers &amp; distillation through attention</a> by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/depth_anything" rel="nofollow">Depth Anything</a></strong> (from University of Hong Kong and TikTok) released with the paper <a href="https://arxiv.org/abs/2401.10891" rel="nofollow">Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</a> by Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/detr" rel="nofollow">DETR</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2005.12872" rel="nofollow">End-to-End Object Detection with Transformers</a> by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/dinov2" rel="nofollow">DINOv2</a></strong> (from Meta AI) released with the paper <a href="https://arxiv.org/abs/2304.07193" rel="nofollow">DINOv2: Learning Robust Visual Features without Supervision</a> by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/distilbert" rel="nofollow">DistilBERT</a></strong> (from HuggingFace), released together with the paper <a href="https://arxiv.org/abs/1910.01108" rel="nofollow">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into <a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation">DistilGPT2</a>, RoBERTa into <a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation">DistilRoBERTa</a>, Multilingual BERT into <a href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation">DistilmBERT</a> and a German version of DistilBERT.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/dit" rel="nofollow">DiT</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2203.02378" rel="nofollow">DiT: Self-supervised Pre-training for Document Image Transformer</a> by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/donut" rel="nofollow">Donut</a></strong> (from NAVER), released together with the paper <a href="https://arxiv.org/abs/2111.15664" rel="nofollow">OCR-free Document Understanding Transformer</a> by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/master/model_doc/dpt" rel="nofollow">DPT</a></strong> (from Intel Labs) released with the paper <a href="https://arxiv.org/abs/2103.13413" rel="nofollow">Vision Transformers for Dense Prediction</a> by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/efficientnet" rel="nofollow">EfficientNet</a></strong> (from Google Brain) released with the paper <a href="https://arxiv.org/abs/1905.11946" rel="nofollow">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a> by Mingxing Tan, Quoc V. Le.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/electra" rel="nofollow">ELECTRA</a></strong> (from Google Research/Stanford University) released with the paper <a href="https://arxiv.org/abs/2003.10555" rel="nofollow">ELECTRA: Pre-training text encoders as discriminators rather than generators</a> by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/esm" rel="nofollow">ESM</a></strong> (from Meta AI) are transformer protein language models.  <strong>ESM-1b</strong> was released with the paper <a href="https://www.pnas.org/content/118/15/e2016239118" rel="nofollow">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</a> by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. <strong>ESM-1v</strong> was released with the paper <a href="https://doi.org/10.1101/2021.07.09.450648" rel="nofollow">Language models enable zero-shot prediction of the effects of mutations on protein function</a> by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. <strong>ESM-2 and ESMFold</strong> were released with the paper <a href="https://doi.org/10.1101/2022.07.20.500902" rel="nofollow">Language models of protein sequences at the scale of evolution enable accurate structure prediction</a> by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/falcon" rel="nofollow">Falcon</a></strong> (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/flan-t5" rel="nofollow">FLAN-T5</a></strong> (from Google AI) released in the repository <a href="https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints">google-research/t5x</a> by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/glpn" rel="nofollow">GLPN</a></strong> (from KAIST) released with the paper <a href="https://arxiv.org/abs/2201.07436" rel="nofollow">Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth</a> by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gpt_neo" rel="nofollow">GPT Neo</a></strong> (from EleutherAI) released in the repository <a href="https://github.com/EleutherAI/gpt-neo">EleutherAI/gpt-neo</a> by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gpt_neox" rel="nofollow">GPT NeoX</a></strong> (from EleutherAI) released with the paper <a href="https://arxiv.org/abs/2204.06745" rel="nofollow">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a> by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gpt2" rel="nofollow">GPT-2</a></strong> (from OpenAI) released with the paper <a href="https://blog.openai.com/better-language-models/" rel="nofollow">Language Models are Unsupervised Multitask Learners</a> by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gptj" rel="nofollow">GPT-J</a></strong> (from EleutherAI) released in the repository <a href="https://github.com/kingoflolz/mesh-transformer-jax/">kingoflolz/mesh-transformer-jax</a> by Ben Wang and Aran Komatsuzaki.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/gpt_bigcode" rel="nofollow">GPTBigCode</a></strong> (from BigCode) released with the paper <a href="https://arxiv.org/abs/2301.03988" rel="nofollow">SantaCoder: don't reach for the stars!</a> by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/herbert" rel="nofollow">HerBERT</a></strong> (from Allegro.pl, AGH University of Science and Technology) released with the paper <a href="https://www.aclweb.org/anthology/2020.acl-main.111.pdf" rel="nofollow">KLEJ: Comprehensive Benchmark for Polish Language Understanding</a> by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/hubert" rel="nofollow">Hubert</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2106.07447" rel="nofollow">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a> by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/longt5" rel="nofollow">LongT5</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2112.07916" rel="nofollow">LongT5: Efficient Text-To-Text Transformer for Long Sequences</a> by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/llama" rel="nofollow">LLaMA</a></strong> (from The FAIR team of Meta AI) released with the paper <a href="https://arxiv.org/abs/2302.13971" rel="nofollow">LLaMA: Open and Efficient Foundation Language Models</a> by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/llama2" rel="nofollow">Llama2</a></strong> (from The FAIR team of Meta AI) released with the paper <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX" rel="nofollow">Llama2: Open Foundation and Fine-Tuned Chat Models</a> by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/m2m_100" rel="nofollow">M2M100</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2010.11125" rel="nofollow">Beyond English-Centric Multilingual Machine Translation</a> by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/marian" rel="nofollow">MarianMT</a></strong> Machine translation models trained using <a href="http://opus.nlpl.eu/" rel="nofollow">OPUS</a> data by Jörg Tiedemann. The <a href="https://marian-nmt.github.io/" rel="nofollow">Marian Framework</a> is being developed by the Microsoft Translator Team.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mbart" rel="nofollow">mBART</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2001.08210" rel="nofollow">Multilingual Denoising Pre-training for Neural Machine Translation</a> by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mbart" rel="nofollow">mBART-50</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2008.00401" rel="nofollow">Multilingual Translation with Extensible Multilingual Pretraining and Finetuning</a> by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mistral" rel="nofollow">Mistral</a></strong> (from Mistral AI) by The <a href="https://mistral.ai/" rel="nofollow">Mistral AI</a> team: Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mms" rel="nofollow">MMS</a></strong> (from Facebook) released with the paper <a href="https://arxiv.org/abs/2305.13516" rel="nofollow">Scaling Speech Technology to 1,000+ Languages</a> by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mobilebert" rel="nofollow">MobileBERT</a></strong> (from CMU/Google Brain) released with the paper <a href="https://arxiv.org/abs/2004.02984" rel="nofollow">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a> by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mobilevit" rel="nofollow">MobileViT</a></strong> (from Apple) released with the paper <a href="https://arxiv.org/abs/2110.02178" rel="nofollow">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a> by Sachin Mehta and Mohammad Rastegari.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mpnet" rel="nofollow">MPNet</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2004.09297" rel="nofollow">MPNet: Masked and Permuted Pre-training for Language Understanding</a> by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mpt" rel="nofollow">MPT</a></strong> (from MosaiML) released with the repository <a href="https://github.com/mosaicml/llm-foundry/">llm-foundry</a> by the MosaicML NLP Team.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/mt5" rel="nofollow">MT5</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2010.11934" rel="nofollow">mT5: A massively multilingual pre-trained text-to-text transformer</a> by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/nllb" rel="nofollow">NLLB</a></strong> (from Meta) released with the paper <a href="https://arxiv.org/abs/2207.04672" rel="nofollow">No Language Left Behind: Scaling Human-Centered Machine Translation</a> by the NLLB team.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/nougat" rel="nofollow">Nougat</a></strong> (from Meta AI) released with the paper <a href="https://arxiv.org/abs/2308.13418" rel="nofollow">Nougat: Neural Optical Understanding for Academic Documents</a> by Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/master/model_doc/opt" rel="nofollow">OPT</a></strong> (from Meta AI) released with the paper <a href="https://arxiv.org/abs/2205.01068" rel="nofollow">OPT: Open Pre-trained Transformer Language Models</a> by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/owlvit" rel="nofollow">OWL-ViT</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2205.06230" rel="nofollow">Simple Open-Vocabulary Object Detection with Vision Transformers</a> by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/owlv2" rel="nofollow">OWLv2</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2306.09683" rel="nofollow">Scaling Open-Vocabulary Object Detection</a> by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/phi" rel="nofollow">Phi</a></strong> (from Microsoft) released with the papers - <a href="https://arxiv.org/abs/2306.11644" rel="nofollow">Textbooks Are All You Need</a> by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, <a href="https://arxiv.org/abs/2309.05463" rel="nofollow">Textbooks Are All You Need II: phi-1.5 technical report</a> by Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/qwen2" rel="nofollow">Qwen2</a></strong> (from the Qwen team, Alibaba Group) released with the paper <a href="https://arxiv.org/abs/2309.16609" rel="nofollow">Qwen Technical Report</a> by Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou and Tianhang Zhu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/resnet" rel="nofollow">ResNet</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/1512.03385" rel="nofollow">Deep Residual Learning for Image Recognition</a> by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/roberta" rel="nofollow">RoBERTa</a></strong> (from Facebook), released together with the paper <a href="https://arxiv.org/abs/1907.11692" rel="nofollow">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/roformer" rel="nofollow">RoFormer</a></strong> (from ZhuiyiTechnology), released together with the paper <a href="https://arxiv.org/abs/2104.09864" rel="nofollow">RoFormer: Enhanced Transformer with Rotary Position Embedding</a> by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/segformer" rel="nofollow">SegFormer</a></strong> (from NVIDIA) released with the paper <a href="https://arxiv.org/abs/2105.15203" rel="nofollow">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</a> by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/sam" rel="nofollow">Segment Anything</a></strong> (from Meta AI) released with the paper <a href="https://arxiv.org/pdf/2304.02643v1.pdf" rel="nofollow">Segment Anything</a> by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/siglip" rel="nofollow">SigLIP</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2303.15343" rel="nofollow">Sigmoid Loss for Language Image Pre-Training</a> by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/speecht5" rel="nofollow">SpeechT5</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2110.07205" rel="nofollow">SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</a> by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/squeezebert" rel="nofollow">SqueezeBERT</a></strong> (from Berkeley) released with the paper <a href="https://arxiv.org/abs/2006.11316" rel="nofollow">SqueezeBERT: What can computer vision teach NLP about efficient neural networks?</a> by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/stablelm" rel="nofollow">StableLm</a></strong> (from Stability AI) released with the paper <a href="https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo" rel="nofollow">StableLM 3B 4E1T (Technical Report)</a> by Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz, Duy Phung, Maksym Zhuravinskyi, Nathan Cooper, Nikhil Pinnaparaju, Reshinth Adithyan, and James Baicoianu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/starcoder2" rel="nofollow">Starcoder2</a></strong> (from BigCode team) released with the paper <a href="https://arxiv.org/abs/2402.19173" rel="nofollow">StarCoder 2 and The Stack v2: The Next Generation</a> by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/swin" rel="nofollow">Swin Transformer</a></strong> (from Microsoft) released with the paper <a href="https://arxiv.org/abs/2103.14030" rel="nofollow">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a> by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/swin2sr" rel="nofollow">Swin2SR</a></strong> (from University of Würzburg) released with the paper <a href="https://arxiv.org/abs/2209.11345" rel="nofollow">Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration</a> by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/t5" rel="nofollow">T5</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/t5v1.1" rel="nofollow">T5v1.1</a></strong> (from Google AI) released in the repository <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511">google-research/text-to-text-transfer-transformer</a> by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/table-transformer" rel="nofollow">Table Transformer</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2110.00061" rel="nofollow">PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents</a> by Brandon Smock, Rohith Pesala, Robin Abraham.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/trocr" rel="nofollow">TrOCR</a></strong> (from Microsoft), released together with the paper <a href="https://arxiv.org/abs/2109.10282" rel="nofollow">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models</a> by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/unispeech" rel="nofollow">UniSpeech</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2101.07597" rel="nofollow">UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data</a> by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/unispeech-sat" rel="nofollow">UniSpeechSat</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2110.05752" rel="nofollow">UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING</a> by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/vit" rel="nofollow">Vision Transformer (ViT)</a></strong> (from Google AI) released with the paper <a href="https://arxiv.org/abs/2010.11929" rel="nofollow">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/vitmatte" rel="nofollow">ViTMatte</a></strong> (from HUST-VL) released with the paper <a href="https://arxiv.org/abs/2305.15272" rel="nofollow">ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers</a> by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/vits" rel="nofollow">VITS</a></strong> (from Kakao Enterprise) released with the paper <a href="https://arxiv.org/abs/2106.06103" rel="nofollow">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a> by Jaehyeon Kim, Jungil Kong, Juhee Son.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/wav2vec2" rel="nofollow">Wav2Vec2</a></strong> (from Facebook AI) released with the paper <a href="https://arxiv.org/abs/2006.11477" rel="nofollow">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a> by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/main/model_doc/wav2vec2-bert" rel="nofollow">Wav2Vec2-BERT</a></strong> (from Meta AI) released with the paper <a href="https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/" rel="nofollow">Seamless: Multilingual Expressive and Streaming Speech Translation</a> by the Seamless Communication team.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/wavlm" rel="nofollow">WavLM</a></strong> (from Microsoft Research) released with the paper <a href="https://arxiv.org/abs/2110.13900" rel="nofollow">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</a> by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/whisper" rel="nofollow">Whisper</a></strong> (from OpenAI) released with the paper <a href="https://cdn.openai.com/papers/whisper.pdf" rel="nofollow">Robust Speech Recognition via Large-Scale Weak Supervision</a> by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/xlm" rel="nofollow">XLM</a></strong> (from Facebook) released together with the paper <a href="https://arxiv.org/abs/1901.07291" rel="nofollow">Cross-lingual Language Model Pretraining</a> by Guillaume Lample and Alexis Conneau.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/xlm-roberta" rel="nofollow">XLM-RoBERTa</a></strong> (from Facebook AI), released together with the paper <a href="https://arxiv.org/abs/1911.02116" rel="nofollow">Unsupervised Cross-lingual Representation Learning at Scale</a> by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.</li>
<li><strong><a href="https://huggingface.co/docs/transformers/model_doc/yolos" rel="nofollow">YOLOS</a></strong> (from Huazhong University of Science &amp; Technology) released with the paper <a href="https://arxiv.org/abs/2106.00666" rel="nofollow">You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection</a> by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.</li>
</ol>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mental Health in Software Engineering (280 pts)]]></title>
            <link>https://vadimkravcenko.com/shorts/mental-health-in-software-engineering/</link>
            <guid>40001150</guid>
            <pubDate>Thu, 11 Apr 2024 11:50:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vadimkravcenko.com/shorts/mental-health-in-software-engineering/">https://vadimkravcenko.com/shorts/mental-health-in-software-engineering/</a>, See on <a href="https://news.ycombinator.com/item?id=40001150">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">


<p>I want to talk about something we don't discuss enough in our field: the mental health of software engineers, especially those of us who've taken on the challenge of leadership. As a technical co-founder, I've had my struggles with anxiety. It's something that comes with the territory but isn't part of the job description.</p>


<p>If we rewind back to 2017, it was not a pleasant year for me — my days consisted of panic attacks, constant use of relaxing supplements, and trying to code while being severely under pressure with all the deadlines and new <a href="https://vadimkravcenko.com/shorts/what-cto-does/" title="What does a CTO actually do?">responsibilities</a>. During that time, I inherited the position of Head of IT from my predecessor. I was now in charge of a small team of developers, and our startup has made many promises to many partners. It was my job to make it happen. I was either going to break or deliver. I did both.</p>


<p>Mental health issues still carry a bit of stigma with them. You cannot take a sick day by telling your team, “I have mental issues and need a day off.” Not many people understand what panic attacks feel like and why would you need to take a day off because of them? I understand those people completely. Right until the moment I had my burnout (or a mental breakdown), I didn’t have any idea why you need to take drugs to manage your anxiety. I thought it was all in your head; can’t you control your thoughts? Apparently, sometimes, you can’t.</p>


<figure><img alt="" data-src="https://vadimkravcenko.com/wp-content/uploads/2024/03/panic-attacks-in-it.jpeg" src="https://vadimkravcenko.com/wp-content/uploads/2024/03/panic-attacks-in-it.jpeg"><figcaption>Author: <a href="https://sow-ay.tumblr.com/" rel="nofollow">Sow Ay</a></figcaption></figure>


<p>When I was first diagnosed with anxiety, I didn’t understand it, so I went to Reddit to read similar stories of people all around the world dealing with the same symptoms, trying to survive the same way I did. It calmed me down during panic episodes. Imagine sitting at your workplace, coding, and then a wave of primal fear running through you, an all-enveloping sense of doom starts consuming you, and you turn to Reddit to read stories about people experiencing the same things. That was my coping mechanism. And it worked.</p>


<p>My story is, sadly, not unique. It’s a story that any software engineer can relate to. I was a perfectionist, a high-achiever who liked to have things under control and in a specific way to ensure everything was running as efficiently as possible. I have my keyboard bindings set up just as I need them, I have my dotfiles organized in a specific way, and I have my automation that makes my life easier. I think most software developers are like that. We strive to be efficient.</p>


<p>It’s easy to have things under control when coding is the only thing that you do, for example, at the junior level. You have a clearly defined task, which your senior colleague refined based on some vague description that your product owner brought them. You debug it, have fun building things, get your next task, and solve some bugs. You have zero worries; your only job now is to get better and learn. Life is good.</p>


<p>As you grow, you understand the realities of business. In business, there’s no place for perfection. There’s no space for having everything under control. In fact, not only can’t you influence most of the things around you, but most of the things are uncertain. Thinking about your next month becomes a math problem with probability variables.</p>


<pre>🥸 An example of uncertainty in business is when your CEO tells you they promised a feature to your biggest client and it needs to be built ASAP as highest priority, so all hands on deck. Then a day later they tell you another feature, completely contradictory to the first one, needs to be built as well and is also highest priority. When you tell them they both can't be highest priority, the answer is: make it happen.</pre>


<p>This was my perfect storm in 2017 — I was trying to control all of the uncertainty around me:</p>


<ul>
<li><em>Trying to control the looming unrealistic deadlines.</em></li>


<li><em>Writing a lot of the code myself to ensure we uphold our promises to stakeholders and none of our developers burn out. Which led to me working more and sleeping less.</em></li>


<li><em>Worrying about next month’s payroll and trying to control our runway.</em></li>


<li><em>Maintaining developer velocity and tight budgets, juggling future growth and current issues.</em></li>


<li><em>Trying to control our developer turnover and making sure our juniors grow.</em></li>


<li><em>There were days I'd be coding non-stop or in a series of back-to-back <a href="https://vadimkravcenko.com/shorts/proper-documentation/" title="The Surprising Power of Documentation">meetings</a>, forgetting meals, sleeping, and even what it felt like to relax.</em></li>
</ul>


<p>There wasn’t one big thing that led to the burnout. It’s more of a combination of factors contributing to my perpetual state of stress. (Side note: I remember waking up on the weekends, feeling great the first few minutes in my bed, and then anxiety taking over).</p>


<p>Eventually, I saw a doctor and am doing much better now. It took me a long while, though.</p>


<h2 id="h-not-all-deadlines-are-equal">Not all deadlines are equal</h2>


<p>I’m not going to philosophize about how you need to maintain a good work-life balance and how not doing so may negatively impact your non-job-related aspects of life. But I can tell you for a fact I did not have a good work-life balance before. I think I’m getting better, but it’s only due to the fact that my company is doing better — we have better cash flow, more loyal clients, and a great team. Before that? I thought work-life balance was a myth. I thought every deadline was sacred, every project was critical, and if you’re not online 24/7, it all fell apart. Was that true? It was, sometimes.</p>


<pre>💡 Some people asked me — why are you working so much? What’s the worst that can happen? People not getting their salaries and the company going bankrupt. I think that’s a big enough reason to risk burning out. At least, I thought it was. Now? Probably still is a good reason, but I would approach it differently. </pre>


<p>This kind of situation isn't rare. You're always on edge, thinking the whole company's fate rests on your next move. It's like being in a constant state of alert, where slowing down feels like you're letting everyone down. As a CTO or any other tech leadership position, you're making calls that could either make or break the whole operation. And yeah, it's thrilling, but it's also a breeding ground for anxiety.</p>


<p>This one time, for example, when our <a href="https://vadimkravcenko.com/shorts/database-migrations/" title="Database Migrations">deployment</a> crashed halfway through right before a major release. The CEO emphasized how important this project was, so we were all hands on deck, trying to get it back up, fearing the worst, that the client would go ballistic if he found out we were delaying the release. I was stressing big time, thinking we had to pull off a miracle, and of course we did.</p>


<p>But you know what? After all that chaos, it turned out the relevant stakeholders were away on vacation that week, and the release wasn't even checked for many days after that. All that stress, the mad rush, and for what? We often stress ourselves over deadlines as if they're set in stone. But are they really? We push ourselves to the limit, thinking we're doing what's best for the business. What if we just… didn't? Not all deadlines are equal. That release could've waited. That deadline was one we could’ve let slip. Our health, our sanity, can't always take a backseat.</p>


<figure><img alt="" data-src="https://vadimkravcenko.com/wp-content/uploads/2024/03/deadline-stress-721x1024.png" src="https://vadimkravcenko.com/wp-content/uploads/2024/03/deadline-stress-721x1024.png"><figcaption>Deadlines aren't created equal. Source: <a href="https://monkeyuser.com/">Monkeyuser.com</a></figcaption></figure>


<p>In tech, there's this unspoken rule that you've gotta be all in, all the time. But that's just not sustainable. I've learned the hard way that not every deadline is do-or-die. Sometimes, pushing a release back a week is the best call you can make—for you and your team.</p>


<h2 id="h-what-worked-for-me">What worked for me</h2>


<p>So, as you might guess, I’ve picked up a few tricks along the way that have seriously helped me out. I’m not going to tell you I’m an expert on mental health, and I’m not going to be giving out advice. I just thought I'd share the things that have helped me, just in case you find yourself in a similar boat.</p>


<p>First off, anxiety and burnout are real, and they don't just go away on their own. I learned this the hard way. So, recognizing when you're starting to burn out or get anxious is crucial. For me, it was about noticing when I started to dread work I usually enjoyed and the random sense of apocalyptic doom or when my sleep went sideways. I mean, I don’t think anyone can miss those signs, but it took me half a year to recognize them and go to a psychotherapist. I thought I had the winter blues. Go figure…</p>


<p>Saying “No” to anything non-critical in your off time, as well as setting boundaries between work and the rest of your life. I’m not very good at this yet, but I am learning to shut off after work hours. No reading emails, no "quick checks" on projects, no MacBook even. It's still challenging, but it’s getting better. Also, sometimes my partners call me on the weekends to do something; unless it’s something critical, I tell them I will handle it on Monday. Trust me, the world won't end if you tell someone you’ll do it 24 or 48 hours later.</p>


<figure><img alt="" data-src="https://vadimkravcenko.com/wp-content/uploads/2024/03/new-kid-on-the-block-work-life-integration.png" src="https://vadimkravcenko.com/wp-content/uploads/2024/03/new-kid-on-the-block-work-life-integration.png"><figcaption>I hope this doesn't become the norm. Source: <a href="https://todoist.com/inspiration/work-life-integration-comic" rel="nofollow">Todoist</a></figcaption></figure>


<p>I swapped my coffee for decaf and cut out alcohol. It wasn't easy, but it helped me feel less jittery and sleep better. I also got into walking. I aim for 12,500 steps a day, usually with a podcast in my ears. It's my new (old) obsession. The steps clear my head and get me out of the work <a href="https://vadimkravcenko.com/shorts/engineering-scarcity-mindset/" title="🤝 Engineering Scarcity Mindset">mindset</a>. Plus, hitting my step goal feels like a win every day.</p>


<p>Putting things in perspective has been another big one for me. I always ask myself, "Will this matter in two years?" You'd be surprised how often the answer is no. It takes the edge off the pressure. I allow myself to miss a deadline every now and then. As I mentioned above, not every deadline is do-or-die. Sometimes, it’s entirely reasonable to push things back to worry less. </p>


<p>Notifications on my phone? Almost all gone. If something's truly urgent, it'll find its way to me. This alone has cut down on a lot of unnecessary stress. It's like I've reclaimed my attention span. I don’t get constant DING! DING!</p>


<p>But here's something I didn't expect to have such a big impact: educating myself on mental health and emotional intelligence. Understanding that someone might be fighting a battle I know nothing about made me a more empathetic leader and colleague. It's changed how I interact with my team. I've learned to listen more, assume less, and approach every situation with a bit more kindness and understanding. You never know when a little compassion during those stressful times will help someone avoid burnout.</p>


<p>When I felt overwhelmed, I didn’t immediately seek professional help. I thought I could handle it myself; I was ashamed of going to a psychotherapist, as if my seeking help was admitting that I was broken. Since then, I’ve understood many things. First of all, it’s totally fine to be vulnerable. If you're feeling overwhelmed, don't hesitate to seek professional help. We’re all human. There's no shame in it. In fact, it's one of the bravest things you can do. We go to the doctor for a physical ailment; why should our mental health be any different?</p>


<p>In short, for me, it's been about making small, sustainable changes and being kinder to myself. It's not always easy, and I'm still learning, but these steps have helped me find a healthier balance.</p>


<h2 id="h-our-greatest-asset">Our greatest asset</h2>


<p>You've heard it before, but let me repeat it — our greatest asset isn't the code we write. It's us. Our health, our minds, our ability to be present and enjoy life outside of the terminal window. Software Engineers and Tech co-founders, like us, are more prone to hitting the lows. Depression doesn't care about your GitHub stars or how scalable you managed to build your Kubernetes Cluster.</p>


<p>I've learned the hard way that not every problem at work is mine to solve. I used to take every customer issue personally, letting my stress levels hit the roof. But I've gotten better at recognizing what's within my control and what's not. Can't help a customer because of a time difference or because it's outside my expertise? That's okay. There's a team for that, and it's not all on me.</p>


<p>I’ve been burnt out and stressed out, and it took a toll on my work, my relationships, everything. Only after I finally started prioritizing my well-being did things change for the better. I won’t say it’s all rainbows and ponies now, but things have changed. I became a better engineer, a better leader, a better friend, and a happier person than I was.</p>


<p>If you’ve read this far — know that you’re not alone. Things do get better. If you're overwhelmed, ask for help; there’s no shame. Find what makes you tick outside of work — be it with your family, a new hobby, or just chilling with your pets — and give it the time it deserves.</p>


<p>Any company that measures your worth by how quickly you burn out isn't worth your time or talent. <strong>So, I will repeat it again: our greatest asset isn't the code we write. It’s us, alive, and living the life.</strong></p>


<p><strong>UPDATE April 2024:</strong> If you prefer watching videos rather than reading, or if you just want to see my face — I recorded a video version of this article. In the podcast-style video I talk more in-depth about the stuff that I wrote here. <a href="https://www.youtube.com/watch?v=anPb6X-sXxI">Here's the link</a>. </p>


<iframe height="415" width="100%" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" data-src="https://www.youtube.com/embed/anPb6X-sXxI" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe>


<p>Other <a href="https://vadimkravcenko.com/newsletter/">Newsletter</a> Issues:</p>


<ul>
<li><a href="https://vadimkravcenko.com/shorts/why-software-projects-fail/">Why software projects fail</a></li>


<li><a href="https://vadimkravcenko.com/shorts/falsehoods-junior-developers-believe-about-becoming-senior/">Falsehoods Junior Developers believe about becoming Senior</a></li>


<li><a href="https://vadimkravcenko.com/shorts/habits-of-great-software-engineers/">Habits of great software engineers</a></li>


<li><a href="https://vadimkravcenko.com/shorts/project-estimates/">Proper Software Development Estimations</a></li>
</ul>
 <div>
<h3>Reactions</h3>

<div>
<p><span>Hot!</span> The last couple of years I've been writing about CTO / Tech lead job. I've compiled all my knowledge into a printable PDF. I called it <a href="https://vadimkravcenko.com/technical-manager-guide/" data-analytics="&quot;CTOGuideButton&quot;, {&quot;props&quot;:{&quot;page&quot;:&quot;single&quot;}}">"256 Pages of No Bullshit Guide for CTOs"</a>. So if you're interested, take a look.
</p><p>
<span>Hot!</span> If you're a software engineer looking for a job, I started a <a href="https://vadimkravcenko.com/roast-my-resume/" data-analytics="&quot;RoastMyResume&quot;, {&quot;props&quot;:{&quot;page&quot;:&quot;single&quot;}}">Roast my Resume</a> service, where I record a personalized video of me "roasting" your CV, which basically means taking a hard look at your resume as a CTO and commenting on all the good and the bad parts.
</p></div>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ESA satellites to test razor-sharp formation flying (140 pts)]]></title>
            <link>https://spectrum.ieee.org/satellite-constellation-formation-flying-esa</link>
            <guid>40000443</guid>
            <pubDate>Thu, 11 Apr 2024 10:05:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/satellite-constellation-formation-flying-esa">https://spectrum.ieee.org/satellite-constellation-formation-flying-esa</a>, See on <a href="https://news.ycombinator.com/item?id=40000443">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="ESA Satellites to Test Razor-Sharp Formation Flying" data-elid="2667720269" data-post-url="https://spectrum.ieee.org/satellite-constellation-formation-flying-esa" data-authors="Andrew Jones" data-page-title="ESA Satellites to Test Razor-Sharp Formation Flying - IEEE Spectrum"><p>The European Space Agency will launch a mission late this year to demonstrate precision formation flying in orbit to create artificial solar eclipses. In a press conference last week, the agency announced details of the mission and the technology the orbiters will use to pull off its exquisitely-choreographed maneuvers.</p><p>ESA’s <a href="https://www.esa.int/Enabling_Support/Space_Engineering_Technology/Proba_Missions/About_Proba-3" target="_blank">Proba-3</a> (PRoject for On-Board Autonomy) consists of a pair of spacecraft: a 300-kilogram Coronagraph spacecraft and a 250-kilogram <a href="https://www.esa.int/Enabling_Support/Space_Engineering_Technology/Proba_Missions/Face_to_face_with_Sun-eclipsing_Proba-3" target="_blank">Occulter</a>. The pair are now slated to launch on an Indian <a href="https://en.wikipedia.org/wiki/Polar_Satellite_Launch_Vehicle" target="_blank">PSLV rocket</a> in September and ultimately enter a highly elliptical, 600-by-60,530-kilometer orbit. The aim, the agency says, is to move the separate spacecraft to some 144 meters apart, with the Occulter, as a disc, blocking out the sun. </p><p>Achieving this formation will allow the Coronagraph to study our star’s highly ionized, extremely hot atmosphere—but also demonstrate the technology as a precursor for more ambitious, future, formation-flying endeavors.</p><p><img alt="A large circular aerospace instrument with equipment and panels." data-rm-shortcode-id="47256ef0593d80c3a91f9dea63a1a9f3" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-large-circular-aerospace-instrument-with-equipment-and-panels.jpg?id=51954036&amp;width=980" height="1250" id="f9a75" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-large-circular-aerospace-instrument-with-equipment-and-panels.jpg?id=51954036&amp;width=980" width="2000"><small data-gramm="false" data-lt-tmp-id="lt-63803" placeholder="Add Photo Caption..." spellcheck="false">Proba-3’s disk-shaped Occulter spacecraft was designed to block the Sun for the Coronagraph spacecraft. </small><small placeholder="Add Photo Credit...">P. Sebirot/ESA</small></p><p>Solar eclipses represent both spectacular alignments of celestial bodies and <a href="https://www.livescience.com/space/the-best-photos-and-videos-of-the-april-8-total-solar-eclipse-over-north-america" target="_blank">moments of stark beauty</a>. They are also fleeting opportunities to conduct science. The element helium was <a href="https://www.smithsonianmag.com/history/how-scientists-discovered-helium-first-alien-element-1868-180970057/" target="_blank">first detected</a> in the Sun’s chromosphere by a French astronomer in India during a 1868 total solar eclipse in India. Fifty years later, British astronomers Frank Dyson and Arthur Eddington <a href="https://en.wikipedia.org/wiki/Eddington_experiment" target="_blank">performed measurements</a> of the apparent shift of stars during a 1919 solar eclipse as an early test of Einstein’s general theory of relativity. </p><p>ESA has science objectives for Proba-3, using observations made in space to study solar astrophysics without any intervention of the Earth’s atmosphere. The agency’s Association of Spacecraft for Polarimetric and Imaging Investigation of the Corona of the Sun (<a href="https://www.mps.mpg.de/6884329/proba-3" target="_blank">ASPIICS</a>) coronagraph will help to discern why the solar corona is significantly hotter than the Sun itself. This could further our understanding of the Sun and assist solar weather predictions.</p><p>However, it is the precision formation flying that Proba-3 aims to demonstrate which could help unlock future breakthroughs. </p><p><img alt="A large aerospace instrument attached to machinery in a clean room." data-rm-shortcode-id="4d1cd2e9fd294d174fa2e85ff8377e64" data-rm-shortcode-name="rebelmouse-image" data-runner-src="https://spectrum.ieee.org/media-library/a-large-aerospace-instrument-attached-to-machinery-in-a-clean-room.jpg?id=51954042&amp;width=980" height="1250" id="497e3" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/a-large-aerospace-instrument-attached-to-machinery-in-a-clean-room.jpg?id=51954042&amp;width=980" width="2000"><small data-gramm="false" data-lt-tmp-id="lt-736241" placeholder="Add Photo Caption..." spellcheck="false">Proba-3’s Coronagraph spacecraft hosts the mission’s ASPIICS instrument to observe the solar corona.</small><small placeholder="Add Photo Credit...">P. Sebirot/ESA</small></p><p>The two spacecraft will align to create eclipses for around six hours per orbit, starting on approach to its farthest point from Earth (apogee). The two spacecraft will use radio inter-satellite links to communicate, and star trackers will be used by both craft for determining their attitude.</p><p>Global navigation satellite system (GNSS) receivers will pick up GPS signals around perigee, or the closest point to Earth, which, when combined with a dedicated relative navigation algorithm, will allow regular determination of their relative positions. Optical sensors on the Occulter will view pulsing LEDs on the Coronagraph to provide data for finer measurements. But greater, millimeter-scale precision requires more technological wizardry still. </p><h3>How Proba-3 can achieve millimeter-scale orbital precision</h3><p>For this, the Occulter will ping a laser at a corner cube retro-reflector mounted on the Coronagraph spacecraft, which will bounce the beam back. This act of <a href="https://en.wikipedia.org/wiki/Metrology" target="_blank">metrology</a> will allow precise tracking of the relative position and orientation of the two spacecraft. Using the data gathered, the craft can, the ESA says, control and maintain millimeter-level accuracy using 10 millinewton-scale cold gas thrusters aboard the Occulter.</p><p>“Guidance, navigation and control has undergone a lot of development and this is what we all want to demonstrate,” <a href="https://ieeexplore.ieee.org/author/37088738884" rel="noopener noreferrer" target="_blank">Damien Galano</a>, Proba-3 Project Manager, said during a <a href="https://www.youtube.com/watch?v=nEztx11vC8o" rel="noopener noreferrer" target="_blank">press briefing</a> on 3 April.</p><p>“We have an actual application which is observation of the corona. So, by achieving really good observations of the corona, we would definitely demonstrate that all these equipment are working and that the technology is delivering actual science data,” he said, adding that Proba-3 formation flying control algorithms and <a href="https://spectrum.ieee.org/%5C%22https://www.sciencedirect.com/science/article/abs/pii/S0273117720305147%5C%22" noreferrer="" target="\&quot;_blank\&quot;">metrology systems</a> that could be applied to future missions. </p><p>Precisely-controlled Occulter spacecraft could be used with space telescopes to block light from a star in order to directly detect potential orbiting planets, while a constellation of spacecraft can, through interferometry, create large-scale observatories, achieving large apertures and long focal lengths than possible with large solo satellites.<br></p><p>Further applications include Earth observation, space-based gravitational wave detection, and a range of missions in which two or more spacecraft need to interact, such as rendezvous, docking, and in-orbit servicing. </p><p>Before such complex projects can be undertaken, Proba-3 needs to prove it can nail the basics. “That’s clearly the operational challenge,” said <a href="https://www.esa.int/About_Us/Corporate_news/Dietmar_Pilz_Director_of_Technology_Engineering_and_Quality" target="_blank">Dietmar Pilz</a>, ESA Director of Technology, Engineering and Quality. “To see how far we can get the formation flying, what are the distances that we can achieve. This needs a lot of operational expertise and software from all the partners that are in this project.”</p><p>If successful, Proba-3 could both surpass the work of previous space coronagraphs and illuminate paths for future complex space operations.</p></div></div>]]></description>
        </item>
    </channel>
</rss>