<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 20 Mar 2024 08:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Long Covid brain fog may be due to damaged blood vessels in the brain (139 pts)]]></title>
            <link>https://www.sciencenews.org/article/long-covid-brain-fog-blood-brain-barrier-damage</link>
            <guid>39762776</guid>
            <pubDate>Wed, 20 Mar 2024 04:06:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencenews.org/article/long-covid-brain-fog-blood-brain-barrier-damage">https://www.sciencenews.org/article/long-covid-brain-fog-blood-brain-barrier-damage</a>, See on <a href="https://news.ycombinator.com/item?id=39762776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
		

<article id="post-3137509">
	<header>
	<div>
			<h2>Long COVID brain fog may be due to damaged blood vessels in the brain</h2>

							<p>
					The result suggests there is a biological basis for this symptom				</p>
					</div>

		<figure>
		<p><img width="1030" height="580" src="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?fit=1030%2C580&amp;ssl=1" alt="A foggy image of a woman with long, dark hair swept to the left." decoding="async" srcset="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?w=1440&amp;ssl=1 1440w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=680%2C383&amp;ssl=1 680w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=800%2C450&amp;ssl=1 800w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=330%2C186&amp;ssl=1 330w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=1030%2C580&amp;ssl=1 1030w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?resize=1380%2C776&amp;ssl=1 1380w" sizes="(max-width: 1030px) 100vw, 1030px" data-attachment-id="3137511" data-permalink="https://www.sciencenews.org/030624_mr_covid-brainfog_feat" data-orig-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?fit=1440%2C810&amp;ssl=1" data-orig-size="1440,810" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="030624_mr_covid-brainfog_feat" data-image-description="" data-image-caption="<div>
<p><span class=&quot;None&quot;>Brain fog is a debilitating symptom commonly reported by people with long COVID. Now, scientists have linked the symptom to leaky boundaries in the brain. </span></p>
</div>
" data-medium-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?fit=680%2C383&amp;ssl=1" data-large-file="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_feat.jpg?fit=800%2C450&amp;ssl=1">		</p>

					<figcaption>
									<span>
						
<p><span>Brain fog is a debilitating symptom commonly reported by people with long COVID. Now, scientists have linked the symptom to leaky boundaries in the brain. </span></p>

					</span>
				
									<span>
						<span>baona/iStock/Getty Images Plus</span>
					</span>
							</figcaption>
			</figure>
	</header>

	

		
		
	<div data-component="video-embed">
				




<p>Leakiness in the brain could explain the memory and concentration problems linked to long COVID.&nbsp;</p>



<p>In patients with brain fog,&nbsp;<a href="https://www.nature.com/articles/s41593-024-01576-9" target="_blank" rel="noopener">MRI scans revealed signs of damaged blood vessels in their brains</a>, researchers reported February 22 in&nbsp;<em>Nature Neuroscience</em>. In these people, dye injected into the bloodstream leaked into their brains and pooled in regions that play roles in language, memory, mood and vision.&nbsp;</p>



<p>It’s the first time anyone’s shown that long COVID patients can have leaky blood brain barriers, says study coauthor Matthew Campbell, a geneticist at Trinity College Dublin in Ireland. That barrier, tightly knit cells lining blood vessels, typically keeps&nbsp;riffraff&nbsp;out of the brain, like bouncers guarding a nightclub.&nbsp;</p>





<p>If the barrier breaks down, bloodborne viruses, cells and other interlopers can sneak into the brain’s tissues and wreak havoc, says&nbsp;Avindra Nath,&nbsp;a&nbsp;neurologist&nbsp;at the&nbsp;National Institutes of Health in Bethesda, Md. It’s too early to say definitively whether that’s happening in people with long COVID, but the new study provides evidence that “brain fog has a biological basis,” says Nath, who wasn’t involved with the work. That alone is important for&nbsp;patients, he says, because their symptoms may be otherwise discounted by physicians.&nbsp;</p>



<p>For some people, brain fog can feel like a slowdown in thinking or difficulty recalling short-term memories, Campbell says. For example,&nbsp;“patients&nbsp;will go for a drive, and forget where they’re driving to.” That might sound trivial, he says, but it actually pushes people into panic mode.&nbsp;</p>



<p>Campbell’s team studies repetitive head trauma. They knew that traumatic brain injuries can disrupt the blood brain barrier — and that people with these injuries sometimes report having brain fog. That mental muddling reminded the team of what people with long COVID can experience. Maybe the blood brain barrier disruption seen in some concussion patients applies to long COVID brain fog, too, the researchers surmised.</p>



<p>Evidence for SARS-CoV-2’s damaging effects on the brain has been mounting for years. Studies in cells and animals suggest the virus can crumble components of the blood brain barrier. And autopsies of people who have died from COVID-19 reveal barrier breakdowns, Nath and others have shown.</p>


<div>
<figure><img decoding="async" width="680" height="398" src="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_inline.jpg?resize=680%2C398&amp;ssl=1" alt="Side-by-side black and white brain scans. The scan on the left contains a few colored speckles. The scan on the right contains many more colored speckles." srcset="https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_inline.jpg?w=680&amp;ssl=1 680w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_inline.jpg?resize=654%2C383&amp;ssl=1 654w, https://i0.wp.com/www.sciencenews.org/wp-content/uploads/2024/03/030624_mr_covid-brainfog_inline.jpg?resize=318%2C186&amp;ssl=1 318w" sizes="(max-width: 680px) 100vw, 680px" data-recalc-dims="1"><figcaption><span><p><span>In long COVID patients with brain fog (brain scan at right), dye injected into the bloodstream tends to leak into the brain (see colored speckles) more so than in people without brain fog (left).</span></p></span><span><p><span><span lang="DE">C. Greene <i>et al.</i>/<i>Nature Neuroscience</i> 2024</span></span></p></span></figcaption></figure></div>


<p>But until now, no one knew if this kind of damage persisted long after the initial infection subsided. The team scanned the brains of 32 people, 10 of whom had recovered from COVID-19, and 22 with long COVID. Of those with long COVID,&nbsp;&nbsp;half reported having brain fog.&nbsp;</p>



<p>An injected dye lit up all the participants’&nbsp;brains during MRI brain scans. In people recovered from COVID, the dye had trouble crossing the blood brain barrier. Likewise, in long COVID patients without brain fog, the dye mostly stayed put, confined within blood vessels. But in eight of 11 participants with brain fog, the dye tended to escape from blood vessels and enter brain tissue.&nbsp;</p>



<p>“It was just so clear,” Campbell says. He remembers one of the first people scanned, someone with severe brain fog. Their temporal lobes, brain regions that sit behind the eyes, were&nbsp;“just flooded with this dye,” he says. The researchers’&nbsp;work suggests that&nbsp;“brain fog wasn’t just a figment of [patients’] imagination,” Campbell says.&nbsp;“It was a very, very real thing that they were reporting.”</p>



<p>The new findings offer an opportunity to think about potential therapies, Nath says. Perhaps researchers can find a way to slow down the blood brain barrier’s breakdown — or reverse it.</p>



			</div>
</article><!-- #post-## -->


<section>
	<h3>
		More Stories from Science News on <a href="https://www.sciencenews.org/topic/health-medicine">Health &amp; Medicine</a>
	</h3>
	

</section>
<div>
		<h3>From the Nature Index</h3>
		<p><span>Paid Content</span>
	</p></div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Study Puts Fermented Foods, Not Fire, as Pivotal Moment in Human Brain Growth (118 pts)]]></title>
            <link>https://plantbasednews.org/news/science/fermented-foods-human-brain-growth/</link>
            <guid>39762588</guid>
            <pubDate>Wed, 20 Mar 2024 03:20:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://plantbasednews.org/news/science/fermented-foods-human-brain-growth/">https://plantbasednews.org/news/science/fermented-foods-human-brain-growth/</a>, See on <a href="https://news.ycombinator.com/item?id=39762588">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="316538">
<div>


<p>Fermented foods may have helped evolution of large brains in humans, according to a recent <a href="https://www.nature.com/articles/s42003-023-05517-3" target="_blank" rel="noreferrer noopener">study</a>.</p>
<p>The human brain began increasing in size around 2.5 million years ago. But scientists have been unsure of what mechanism drove that change. Fire and the invention of cooking has often been thought to have been the key, by enabling our ancestors to get enough nourishment to spur our evolution.</p>
<p>But the new study notes that the archaeological evidence shows that human brain expansion predated fire use by up a million years.</p>
<p>As brains need a lot of calories to keep functioning, the researchers believe another dietary change helped to kickstart the growth of early humans’ brains. They posit that fermented foods, as a dietary option accessible to our ancestors, were responsible.</p>
<h2 id="h-external-fermentation-hypothesis">External Fermentation Hypothesis</h2>
<div>
<figure><img decoding="async" width="1200" height="767" src="https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-1200x767.jpeg" alt="Jars of various fermented vegetables" srcset="https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-1200x767.jpeg 1200w, https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-600x384.jpeg 600w, https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-768x491.jpeg 768w, https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-1536x982.jpeg 1536w, https://plantbasednews.org/app/uploads/2024/03/plant-based-news-fermented-foods-brain-growth-2048x1310.jpeg 2048w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption><span>Adobe Stock</span> Fermented foods are popular in the modern world, and they may have aided brain growth of our ancestors</figcaption></figure></div>
<p>The researchers propose the External Fermentation Hypothesis to explain what helped our brains grow. Food ferments inside our guts, but the researchers believe that the food must have been fermented before being eaten.&nbsp;</p>
<p>According to the study, fermentation makes it easier for humans to absorb macronutrients and micronutrients. It also makes carbohydrates and proteins more digestible.</p>
<p>Backing up this hypothesis is the fact that humans have relatively smaller large intestines than other primates. This indicates that our ancestors were eating food that was already partly broken down by fermentation.&nbsp;</p>
<p>“Reduced gut sizes could only evolve if our ancestors were able to exploit a more nutrient-dense and easily digestible food source,” explain the researchers in the study. As a result, less energy would have been needed to support digestion, freeing it up for use by the brain instead.</p>
<h2>A happy accident</h2>
<p>Our ancestors probably didn’t choose fermented foods for their brain health, but fermented foods by accident. The study suggests that our early ancestors may stored food in common locations, intermittently eating some and adding more. Using the same storage spots could have helped a stable microbial ecosystem to develop that would aid fermentation.</p>
<p>“This was not necessarily an intentional endeavor,” Erin Hecht, co-author on the study and Assistant Professor of Human Evolutionary Biology at Harvard University, <a href="https://news.harvard.edu/gazette/story/2024/02/did-fermented-foods-fuel-brain-growth/" target="_blank" rel="noreferrer noopener">told <em>The Harvard Gazette</em></a>. “It may have been an accidental side effect of caching food. And maybe, over time, traditions or superstitions could have led to practices that promoted fermentation or made fermentation more stable or more reliable.”</p>
<h2>Supporting mental health</h2>
<p>Katherine Bryant, lead author and researcher at Aix-Marseille University, suggests that the External Fermentation Hypothesis could have implications for research into modern diets.</p>
<p>“This hypothesis also gives us as scientists even more reasons to explore the role of fermented foods on human health and the maintenance of a healthy gut microbiome,” she told<em> The Harvard Gazette</em>. “There have been a number of studies in recent years linking gut microbiome to not only physical but mental health.”</p>
<p>Indeed, fermented foods such as kimchi and tempeh are becoming increasingly popular for their benefits to gut health. Gut health expert Professor Tim Spector <a href="https://plantbasednews.org/lifestyle/health-and-fitness/benefits-of-fermented-foods/">recommends</a> eating a small amount of fermented foods every day. This encourages diversity in the gut microbiome.</p>
<h6>More like this:</h6>
<ul>
<li><a href="https://plantbasednews.org/lifestyle/food/sprouting-healthy-food/">What Is Sprouting? How To Grow Healthy Food ‘For Pennies’</a></li>
<li><a href="https://plantbasednews.org/lifestyle/health-and-fitness/sleep-apnoea-plant-based/">Healthy Plant-Based Diets Cut Sleep Apnoea Risk, Study Finds</a></li>
<li><a href="https://plantbasednews.org/lifestyle/health-and-fitness/protein-rich-vegetables/">8 Protein-Rich Vegetables To Add To Your Meals</a></li>
</ul>
</div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HIV in cell culture can be completely eliminated using CRISPR-Cas gene editing [pdf] (142 pts)]]></title>
            <link>https://www.escmid.org/fileadmin/src/media/PDFs/2News_Discussions/Press_activities/2024/HIVCRISPRV4_1_.pdf</link>
            <guid>39761283</guid>
            <pubDate>Tue, 19 Mar 2024 23:12:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.escmid.org/fileadmin/src/media/PDFs/2News_Discussions/Press_activities/2024/HIVCRISPRV4_1_.pdf">https://www.escmid.org/fileadmin/src/media/PDFs/2News_Discussions/Press_activities/2024/HIVCRISPRV4_1_.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=39761283">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[It's official: Europe turns to the Falcon 9 to launch its navigation satellites (120 pts)]]></title>
            <link>https://arstechnica.com/space/2024/03/its-official-europe-turns-to-the-falcon-9-to-launch-its-navigation-satellites/</link>
            <guid>39761179</guid>
            <pubDate>Tue, 19 Mar 2024 22:56:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/space/2024/03/its-official-europe-turns-to-the-falcon-9-to-launch-its-navigation-satellites/">https://arstechnica.com/space/2024/03/its-official-europe-turns-to-the-falcon-9-to-launch-its-navigation-satellites/</a>, See on <a href="https://news.ycombinator.com/item?id=39761179">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      And yet it moves    —
</h4>
            
            <h2 itemprop="description">The European Union agreed to pay a 30 percent premium for Falcon 9 launches.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2020/02/getty-spacex-starlink-falcon-9-800x499.jpg" alt="A SpaceX Falcon 9 rocket launching into the sky.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2020/02/getty-spacex-starlink-falcon-9.jpg" data-height="1310" data-width="2100">Enlarge</a> <span>/</span> A SpaceX Falcon 9 rocket carrying 60 Starlink satellites launches from Cape Canaveral Air Force Station on January 29, 2020. </p></figcaption>  </figure>

  




<!-- cache hit 1:single/related:66bcf73f5ea75b9b0d95f4523be283ce --><!-- empty -->
<p>The European Union has reached an agreement with the United States that will allow for the launch of four Galileo navigation satellites on SpaceX's Falcon 9 rocket.</p>
<p>According to Politico, the security agreement permits staff working for the EU and European Space Agency to have access to the launch pad at all times and, should there be a mishap with the mission, the first opportunity to retrieve debris.</p>
<p>With the agreement, final preparations can begin for two launches of two satellites each, on the Falcon 9 rocket from Florida. These Galileo missions will occur later this year. The satellites, which each weigh about 700 kg, will be launched into an orbit about 22,000 km above the planet.</p>
<p>The heightened security measures are due to the proprietary technology incorporated into the satellites, which cost hundreds of millions of euros to build; they perform a similar function to US-manufactured Global Positioning System satellites. The Florida launches will be the first time Galileo satellites, which are used for civilian and military purposes, have been exported outside of European territory.</p>
<p>Due to the extra overhead related to the national security mission, the European Union agreed to pay 180 million euros for the two launches, or about $196 million. This represents about a 30 percent premium over the standard launch price of $67 million for a Falcon 9 launch.</p>                                            
                                                        
<h2>A launcher crisis</h2>
<p>Somewhat to the ESA's embarrassment, the continent has had to purchase several launches from its direct competitor in launch, SpaceX, during the last two years. In 2023, Europe launched its Euclid space telescope on a Falcon 9 rocket, and later this year, an ESA Earth observation satellite and an ESA asteroid probe will launch on Falcon 9 missions.</p>
<p>The reasons are twofold. First, the ESA broke off work with the Russian space corporation Roscosmos after the Russian invasion of Ukraine. After this conflict began, Europe stopped flying its missions on the medium-lift Soyuz rocket. A modified version of this booster had been launching from Europe's spaceport in French Guiana.</p>
<p>The other reason is due to ongoing delays with the development of the Ariane 6 rocket. This booster was originally due to make its debut four years ago, but the new rocket has undergone several development and technical delays. Europe's launcher crisis became acute last year when the continent retired its long-flying Ariane 5 rocket, leaving it without a ready replacement.</p>
<p>However, this lack of access to space should come to an end soon. The ESA has shipped stages of the first flight hardware for the Ariane 6 rocket to its French Guiana spaceport. While the ESA has not set a specific launch date, it is working toward a window that extends from June 15 through July 31.</p>
<p>Assuming this test flight of the Ariane 6 goes well, the vehicle has a lengthy manifest of missions, including future Galileo satellites, other European spacecraft, as well as Project Kuiper satellites for its primary commercial customer, Amazon.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Root cause of Alzheimer's may be fat buildup in brain cells, research suggests (222 pts)]]></title>
            <link>https://medicalxpress.com/news/2024-03-root-alzheimer-fat-buildup-brain.html</link>
            <guid>39760333</guid>
            <pubDate>Tue, 19 Mar 2024 21:19:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicalxpress.com/news/2024-03-root-alzheimer-fat-buildup-brain.html">https://medicalxpress.com/news/2024-03-root-alzheimer-fat-buildup-brain.html</a>, See on <a href="https://news.ycombinator.com/item?id=39760333">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
									    
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2024/root-cause-of-alzheime.jpg" data-src="https://scx2.b-cdn.net/gfx/news/2024/root-cause-of-alzheime.jpg" data-sub-html="Representative immunofluorescence images of human frontal cortex adjacent to the tissue used in snRNA-seq experiments stained for microglia marker IBA1 (green), ACSL1 (red) and DAPI (blue) in an aged-matched healthy control subject (left), an AD-APOE3/3 subject (middle) and an AD-APOE4/4 subject. Credit: <i>Nature</i> (2024). DOI: 10.1038/s41586-024-07185-7">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2024/root-cause-of-alzheime.jpg" alt="Root cause of Alzheimer's may be fat buildup in brain cells" title="Representative immunofluorescence images of human frontal cortex adjacent to the tissue used in snRNA-seq experiments stained for microglia marker IBA1 (green), ACSL1 (red) and DAPI (blue) in an aged-matched healthy control subject (left), an AD-APOE3/3 subject (middle) and an AD-APOE4/4 subject. Credit: Nature (2024). DOI: 10.1038/s41586-024-07185-7" width="800" height="462">
             <figcaption>
                Representative immunofluorescence images of human frontal cortex adjacent to the tissue used in snRNA-seq experiments stained for microglia marker IBA1 (green), ACSL1 (red) and DAPI (blue) in an aged-matched healthy control subject (left), an AD-APOE3/3 subject (middle) and an AD-APOE4/4 subject. Credit: <i>Nature</i> (2024). DOI: 10.1038/s41586-024-07185-7
            </figcaption>        </figure>
    </div><p>A team of neurologists, stem cell specialists and molecular biologists affiliated with several institutions in the U.S. and led by a group at Stanford University School of Medicine has found evidence that the root cause of Alzheimer's disease may be fat buildup in brain cells. The <a href="https://www.nature.com/articles/s41586-024-07185-7" target="_blank">study</a> is published in the journal <i>Nature</i>.</p>


                                        
                                                                                        
                                                                                    <p>Prior research has suggested that Alzheimer's disease is caused by a buildup of beta-amyloid in plaques that grow between <a href="https://medicalxpress.com/tags/nerve+cells/" rel="tag">nerve cells</a>. Other work has also implicated a protein called tau, which can build up in <a href="https://medicalxpress.com/tags/brain+cells/" rel="tag">brain cells</a>. Thus, most work involved in developing ways to prevent, slow or stop the disease is based on reducing or eliminating such buildups. But as the researchers with this new effort have found, there may be something else at the root of the development of the disease.</p>
<p>Back when Alzheimer's disease was first identified by Alois Alzheimer, he noted that in addition to the plaques and tau buildup, there was also a buildup of fat droplets in brain cells. Since that time, little effort has been made to determine whether they might be the cause of the disease.</p>
<p>The research team therefore focused on the function of the APOE gene—prior research has shown that it encodes for a protein involved in transporting fat droplets into nerve cells. Prior research has also shown that there are four APOE variants, numbered 1 through 4, and that one of them, APOE4, carries the most fat into brain cells, while APOE2 brings the least.</p>
<p>The team wondered if the APOE variants carried different risks for developing Alzheimer's disease. To find out, they conducted a few experiments.</p>
<p>In the first experiment, the researchers used single cell RNA sequencing to identify the proteins inside of a test nerve cell. They applied what they found to <a href="https://medicalxpress.com/tags/tissue+samples/" rel="tag">tissue samples</a> collected from people who died of Alzheimer's disease who had dual copies of APOE4 or APOE3.</p>
<p>They found that the brains of people with the APOE4 gene had more <a href="https://medicalxpress.com/tags/immune+cells/" rel="tag">immune cells</a> with a type of enzyme that boosted movement of fat <a href="https://medicalxpress.com/tags/droplets/" rel="tag">droplets</a> into brain cells. In another experiment, they found that applying amyloid to brain cells of people with the APOE4 or APOE3 variants made the cells accumulate more fat.</p>
<p>According to the researchers, the results indicate that buildup of amyloid in the brain triggers the push of fat into brain cells, leading to Alzheimer's disease.</p>

                                                                                
                                        											<div>
												                                                    <p><strong>More information:</strong>
                                                    Michael S. Haney et al, APOE4/4 is linked to damaging lipid droplets in Alzheimer's disease microglia, <i>Nature</i> (2024). <a data-doi="1" href="https://dx.doi.org/10.1038/s41586-024-07185-7" target="_blank">DOI: 10.1038/s41586-024-07185-7</a>
																								
																								</p>
																							</div>
                                        											
										                                                                                    <p>
                                                © 2024 Science X Network
                                            </p>
                                                                                
                                        <!-- print only -->
                                        <div>
                                            <p><strong>Citation</strong>:
                                                 Root cause of Alzheimer's may be fat buildup in brain cells, research suggests (2024, March 19)
                                                 retrieved 19 March 2024
                                                 from https://medicalxpress.com/news/2024-03-root-alzheimer-fat-buildup-brain.html
                                            </p>
                                            <p>
                                            This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
                                            part may be reproduced without the written permission. The content is provided for information purposes only.
                                            </p>
                                        </div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New algorithm unlocks high-resolution insights for computer vision (127 pts)]]></title>
            <link>https://news.mit.edu/2024/featup-algorithm-unlocks-high-resolution-insights-computer-vision-0318</link>
            <guid>39759906</guid>
            <pubDate>Tue, 19 Mar 2024 20:28:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.mit.edu/2024/featup-algorithm-unlocks-high-resolution-insights-computer-vision-0318">https://news.mit.edu/2024/featup-algorithm-unlocks-high-resolution-insights-computer-vision-0318</a>, See on <a href="https://news.ycombinator.com/item?id=39759906">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          

            <p>Imagine yourself glancing at a busy street for a few moments, then trying to sketch the scene you saw from memory. Most people could draw the rough positions of the major objects like cars, people, and crosswalks, but almost no one can draw every detail with pixel-perfect accuracy. The same is true for most modern computer vision algorithms: They are fantastic at capturing high-level details of a scene, but they lose fine-grained details as they process information.</p>

<p>Now, MIT researchers have created a system called “<a href="https://mhamilton.net/featup.html" target="_blank">FeatUp</a>” that lets algorithms capture all of the high- and low-level details of a scene at the same time — almost like Lasik eye surgery for computer vision.</p>

<p>When computers learn to “see” from looking at images and videos, they build up “ideas” of what's in a scene through something called “features.” To create these features, deep networks and visual foundation models break down images into a grid of tiny squares and process these squares as a group to determine what's going on in a photo. Each tiny square is usually made up of anywhere from 16 to 32 pixels, so the resolution of these algorithms is dramatically smaller than the images they work with. In trying to summarize and understand photos, algorithms lose a ton of pixel clarity.&nbsp;</p>

<p>The FeatUp algorithm can stop this loss of information and boost the resolution of any deep network without compromising on speed or quality. This allows researchers to quickly and easily improve the resolution of any new or existing algorithm. For example, imagine trying to interpret the predictions of a lung cancer detection algorithm with the goal of localizing the tumor. Applying FeatUp before interpreting the algorithm using a method like class activation maps (CAM) can yield a dramatically more detailed (16-32x) view of where the tumor might be located according to the model.</p>        

      </div><div>
          

            <p>FeatUp not only helps practitioners understand their models, but also can improve a panoply of different tasks like object detection, semantic segmentation (assigning labels to pixels in an image with object labels), and depth estimation. It achieves this by providing more accurate, high-resolution features, which are crucial for building vision applications ranging from autonomous driving to medical imaging.</p>

<p>“The essence of all computer vision lies in these deep, intelligent features that emerge from the depths of deep learning architectures. The big challenge of modern algorithms is that they reduce large images to&nbsp; very small grids of 'smart' features, gaining intelligent insights but losing the finer details,” says Mark Hamilton, an MIT PhD student in electrical engineering and computer science, MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) affiliate, and a co-lead author on a <a href="https://marhamilresearch4.blob.core.windows.net/feature-upsampling-public/FeatUp_ICLR_2024.pdf">paper</a> about the project. “FeatUp helps enable the best of both worlds: highly intelligent representations with the original image’s resolution. These high-resolution features significantly boost performance across a spectrum of computer vision tasks, from enhancing object detection and improving depth prediction to providing a deeper understanding of your network's decision-making process through high-resolution analysis.”&nbsp;</p>

<p><strong>Resolution renaissance&nbsp;</strong></p>

<p>As these large AI models become more and more prevalent, there’s an increasing need to explain what they’re doing, what they’re looking at, and what they’re thinking.&nbsp;</p>

<p>But how exactly can FeatUp discover these fine-grained details? Curiously, the secret lies in wiggling and jiggling images.&nbsp;</p>

<p>In particular, FeatUp applies minor adjustments (like moving the image a few pixels to the left or right) and watches how an algorithm responds to these slight movements of the image. This results in hundreds of deep-feature maps that are all slightly different, which can be combined into a single crisp, high-resolution, set of deep features. “We imagine that some high-resolution features exist, and that when we wiggle them and blur them, they will match all of the original, lower-resolution features from the wiggled images. Our goal is to learn how to refine the low-resolution features into high-resolution features using this 'game' that lets us know how well we are doing,” says Hamilton. This methodology is analogous to how algorithms can create a 3D model from multiple 2D images by ensuring that the predicted 3D object matches all of the 2D photos used to create it. In FeatUp’s case, they predict a high-resolution feature map that’s consistent with all of the low-resolution feature maps formed by jittering the original image.</p>

<p>The team notes that standard tools available in PyTorch were insufficient for their needs, and introduced a new type of deep network layer in their quest for a speedy and efficient solution. Their custom layer, a special joint bilateral upsampling operation, was over 100 times more efficient than a naive implementation in PyTorch. The team also showed this new layer could improve a wide variety of different algorithms including semantic segmentation and depth prediction. This layer improved the network’s ability to process and understand high-resolution details, giving any algorithm that used it a substantial performance boost.&nbsp;</p>

<p>“Another application is something called small object retrieval, where our algorithm allows for precise localization of objects. For example, even in cluttered road scenes algorithms enriched with FeatUp can see tiny objects like traffic cones, reflectors, lights, and potholes where their low-resolution cousins fail. This demonstrates its capability to enhance coarse features into finely detailed signals,” says Stephanie Fu ’22, MNG ’23, a PhD student at the University of California at Berkeley and another co-lead author on the new FeatUp paper. “This is especially critical for time-sensitive tasks, like pinpointing a traffic sign on a cluttered expressway in a driverless car. This can not only improve the accuracy of such tasks by turning broad guesses into exact localizations, but might also make these systems more reliable, interpretable, and trustworthy.”</p>

<p><strong>What next?</strong></p>

<p>Regarding future aspirations, the team emphasizes FeatUp’s potential widespread adoption within the research community and beyond, akin to data augmentation practices. “The goal is to make this method a fundamental tool in deep learning, enriching models to perceive the world in greater detail without the computational inefficiency of traditional high-resolution processing,” says Fu.</p>

<p>“FeatUp represents a wonderful advance towards making visual representations really useful, by producing them at full image resolutions,” says Cornell University computer science professor Noah Snavely, who was not involved in the research. “Learned visual representations have become really good in the last few years, but they are almost always produced at very low resolution — you might put in a nice full-resolution photo, and get back a tiny, postage stamp-sized grid of features. That’s a problem if you want to use those features in applications that produce full-resolution outputs. FeatUp solves this problem in a creative way by combining classic ideas in super-resolution with modern learning approaches, leading to beautiful, high-resolution feature maps.”</p>

<div><p>“We hope this simple idea can have broad application. It provides high-resolution versions of image analytics that we’d thought before could only be low-resolution,” says senior author William T. Freeman, an MIT professor of electrical engineering and computer science professor and CSAIL member.</p><p>

Lead authors Fu and Hamilton are accompanied by MIT PhD students Laura Brandt SM ’21 and Axel Feldmann SM ’21, as well as Zhoutong Zhang SM ’21, PhD ’22, all current or former affiliates of MIT CSAIL. Their research is supported, in part, by a National Science Foundation Graduate Research Fellowship<strong>,</strong> by the National Science Foundation and Office of the Director of National Intelligence, by the U.S. Air Force Research Laboratory, and by the U.S. Air Force Artificial Intelligence Accelerator. The group will present their work in May at the International Conference on Learning Representations.</p></div>        

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lego price per part over the years (288 pts)]]></title>
            <link>https://brickinsights.com/statistics/ppp</link>
            <guid>39759693</guid>
            <pubDate>Tue, 19 Mar 2024 20:01:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brickinsights.com/statistics/ppp">https://brickinsights.com/statistics/ppp</a>, See on <a href="https://news.ycombinator.com/item?id=39759693">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <h2>Can we really compare Price Per Part (PPP) between sets?</h2>
      <p>A couple of weeks ago I received this question by <a href="https://rebrickable.com/users/jaredhinton/profile/" target="_blank" rel="noopener">Jared Hinton</a>:</p>
      <p><em>"Just a quick question on the PPP data. Most years have a average PPP of 20c+ per part. This sounds very high, especially when most sets come way under that (which are then marked as good value). Do Duplo sets get included in the years average PPP figure? Because they tend to be alot higher than LEGO sets. This is the only thing I could think of why the year average is almost double what I'd expect it to be."</em></p>
      <p>This is a very good observation, and something I hadn't really thought about. I built the PPP comparison when Brick Insights first launched in 2018, almost as an afterthought. I wanted to see if a set was worth buying from a MOCing point of view, and threw it in to help guide me. It was definitely time to take another look at those calculations.</p>
      <p>All numbers in this article are adjusted according to inflation to make them comparable over the years. This isn't something we did before this investigation, so thank you for that too, Jared! <a href="https://brickinsights.com/faq">Check the FAQ</a> to learn the details about this. All categories referenced in this article are collected from Brickset, where we get most of our set data. That's also in the FAQ.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Retina – eBPF distributed networking observability tool for Kubernetes (128 pts)]]></title>
            <link>https://github.com/microsoft/retina</link>
            <guid>39759627</guid>
            <pubDate>Tue, 19 Mar 2024 19:54:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/retina">https://github.com/microsoft/retina</a>, See on <a href="https://news.ycombinator.com/item?id=39759627">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Retina</h2><a id="user-content-retina" aria-label="Permalink: Retina" href="#retina"></a></p>
<p dir="auto"><a href="https://goreportcard.com/report/github.com/microsoft/retina" rel="nofollow"><img src="https://camo.githubusercontent.com/3fec060b6ca3b8336abfe76f0d699a59a942324d5c0e8fbe791b3c710303d053/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f6d6963726f736f66742f726574696e61" alt="goreport" data-canonical-src="https://goreportcard.com/badge/github.com/microsoft/retina"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/abfcc83487967c2e2ee70aa26df8d74ea9b70998fd383b491ae2f8dadf9e8019/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6d6963726f736f66742f726574696e612e737667"><img src="https://camo.githubusercontent.com/abfcc83487967c2e2ee70aa26df8d74ea9b70998fd383b491ae2f8dadf9e8019/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6d6963726f736f66742f726574696e612e737667" alt="GitHub release" data-canonical-src="https://img.shields.io/github/v/release/microsoft/retina.svg"></a> <a href="https://godoc.org/github.com/microsoft/retina" rel="nofollow"><img src="https://camo.githubusercontent.com/a7a2adf339bfb1b7442f42660598da0a83ba3b450d3fa5e28cb4d236ee170bb4/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f6d6963726f736f66742f726574696e613f7374617475732e737667" alt="retina-publish" data-canonical-src="https://godoc.org/github.com/microsoft/retina?status.svg"></a> <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8461bcd16ae5629435cd3547ca2d5e47206d6a739fdba55f08ea007f2703fe61/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75653f6c696e6b3d68747470732533412532462532466769746875622e636f6d2532466d6963726f736f6674253246726574696e61253246626c6f622532466d61696e2532464c4943454e5345"><img src="https://camo.githubusercontent.com/8461bcd16ae5629435cd3547ca2d5e47206d6a739fdba55f08ea007f2703fe61/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75653f6c696e6b3d68747470732533412532462532466769746875622e636f6d2532466d6963726f736f6674253246726574696e61253246626c6f622532466d61696e2532464c4943454e5345" alt="license" data-canonical-src="https://img.shields.io/badge/license-MIT-blue?link=https%3A%2F%2Fgithub.com%2Fmicrosoft%2Fretina%2Fblob%2Fmain%2FLICENSE"></a></p>
<p dir="auto"><a href="https://github.com/microsoft/retina/actions/workflows/test.yaml?query=branch%3Amain"><img src="https://github.com/microsoft/retina/actions/workflows/test.yaml/badge.svg?branch=main" alt="retina-test"></a> <a href="https://retina.sh/" rel="nofollow"><img src="https://github.com/microsoft/retina/actions/workflows/docs.yaml/badge.svg?branch=main" alt="retinash"></a> <a href="https://github.com/microsoft/retina/actions/workflows/images.yaml?query=branch%3Amain"><img src="https://github.com/microsoft/retina/actions/workflows/images.yaml/badge.svg?branch=main" alt="retina-publish"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/retina/actions/workflows/codeql.yaml/badge.svg?branch=main"><img src="https://github.com/microsoft/retina/actions/workflows/codeql.yaml/badge.svg?branch=main" alt="retina-codeql-img"></a> <a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/retina/actions/workflows/golangci-lint.yaml/badge.svg?branch=main"><img src="https://github.com/microsoft/retina/actions/workflows/golangci-lint.yaml/badge.svg?branch=main" alt="retina-golangci-lint-img"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">Retina is a cloud-agnostic, open-source <strong>Kubernetes network observability platform</strong> that provides a <strong>centralized hub for monitoring application health, network health, and security</strong>. It provides actionable insights to cluster network administrators, cluster security administrators, and DevOps engineers navigating DevOps, SecOps, and compliance use cases.</p>
<p dir="auto">Retina <strong>collects customizable telemetry</strong>, which can be exported to <strong>multiple storage options</strong> (such as Prometheus, Azure Monitor, and other vendors) and <strong>visualized in a variety of ways</strong> (like Grafana, Azure Log Analytics, and other vendors).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong><a href="https://ebpf.io/what-is-ebpf#what-is-ebpf" rel="nofollow">eBPF</a>-based</strong> Network Observability platform for Kubernetes workloads.</li>
<li><strong>On-Demand</strong> and <strong>Configurable</strong>.</li>
<li>Actionable, industry-standard <strong>Prometheus metrics</strong>.</li>
<li>Streamlined <strong>Packet Captures</strong> for deep dives.</li>
<li><strong>Cloud-agnostic</strong>, supporting multiple OS (like Linux, Windows, Azure Linux).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why Retina?</h2><a id="user-content-why-retina" aria-label="Permalink: Why Retina?" href="#why-retina"></a></p>
<p dir="auto">Retina lets you <strong>investigate network issues on-demand</strong> and <strong>continuously monitor your clusters</strong>. For scenarios where Retina shines, see the intro docs <a href="https://retina.sh/docs/intro" rel="nofollow">here</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">See <a href="http://retina.sh/" rel="nofollow">retina.sh</a> for documentation and examples.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Capabilities</h2><a id="user-content-capabilities" aria-label="Permalink: Capabilities" href="#capabilities"></a></p>
<p dir="auto">Retina has two major features:</p>
<ul dir="auto">
<li><a href="https://retina.sh/docs/metrics/modes" rel="nofollow">Metrics</a></li>
<li><a href="https://retina.sh/docs/captures" rel="nofollow">Captures</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Metrics Quick Install Guide</h3><a id="user-content-metrics-quick-install-guide" aria-label="Permalink: Metrics Quick Install Guide" href="#metrics-quick-install-guide"></a></p>
<p dir="auto">Prerequisites: Go, Helm</p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repo, then install Retina on your Kubernetes cluster</p>

</li>
<li>
<p dir="auto">Follow steps in <a href="https://retina.sh/docs/installation/prometheus-unmanaged" rel="nofollow">Using Prometheus and Grafana</a> to set up metrics collection and visualization.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Captures Quick Start Guide</h3><a id="user-content-captures-quick-start-guide" aria-label="Permalink: Captures Quick Start Guide" href="#captures-quick-start-guide"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Captures via CLI</h4><a id="user-content-captures-via-cli" aria-label="Permalink: Captures via CLI" href="#captures-via-cli"></a></p>
<p dir="auto">Currently, Retina CLI only supports Linux.</p>
<ul dir="auto">
<li>
<p dir="auto">Option 1: Download from Release</p>
<p dir="auto">Download <code>kubectl-retina</code> from the latest <a href="https://github.com/microsoft/retina/releases">Retina release</a>.
Feel free to move the binary to <code>/usr/local/bin/</code>, or add it to your <code>PATH</code> otherwise.</p>
</li>
<li>
<p dir="auto">Option 2: Build from source</p>
<p dir="auto">Requirements:</p>
<ul dir="auto">
<li>go 1.21 or newer</li>
<li>GNU make</li>
</ul>
<p dir="auto">Clone the Retina repo and execute:</p>
<div dir="auto" data-snippet-clipboard-copy-content="make install-kubectl-retina"><pre>make install-kubectl-retina</pre></div>
</li>
</ul>
<p dir="auto">Execute Retina:</p>
<div dir="auto" data-snippet-clipboard-copy-content="kubectl-retina capture create --help"><pre>kubectl-retina capture create --help</pre></div>
<p dir="auto">For further CLI documentation, see <a href="https://github.com/microsoft/retina/blob/captures/cli.md">Capture with Retina CLI</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Captures via CRD</h4><a id="user-content-captures-via-crd" aria-label="Permalink: Captures via CRD" href="#captures-via-crd"></a></p>
<p dir="auto">Prerequisites: Go, Helm</p>
<ol dir="auto">
<li>
<p dir="auto">Clone the repo, then install Retina with Capture operator support on your Kubernetes cluster</p>
<div dir="auto" data-snippet-clipboard-copy-content="make helm-install-with-operator"><pre>make helm-install-with-operator</pre></div>
</li>
<li>
<p dir="auto">Follow steps in <a href="https://retina.sh/docs/captures/#option-2-capture-crd-custom-resource-definition" rel="nofollow">Capture CRD</a> for documentation of the CRD and examples for setting up Captures.</p>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit <a href="https://cla.opensource.microsoft.com/" rel="nofollow">https://cla.opensource.microsoft.com</a>.</p>
<p dir="auto">When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.</p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>.
For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or
contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><a href="https://retina.sh/docs/contributing" rel="nofollow">Read more about how to begin contributing here.</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Office Hours and Community Meetings</h3><a id="user-content-office-hours-and-community-meetings" aria-label="Permalink: Office Hours and Community Meetings" href="#office-hours-and-community-meetings"></a></p>
<p dir="auto">We host a periodic open community meeting. <a href="https://retina.sh/docs/contributing/#office-hours-and-community-meetings" rel="nofollow">Find the details here.</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Trademarks</h2><a id="user-content-trademarks" aria-label="Permalink: Trademarks" href="#trademarks"></a></p>
<p dir="auto">This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general" rel="nofollow">Microsoft's Trademark &amp; Brand Guidelines</a>.
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">See the <a href="https://github.com/microsoft/retina/blob/main/LICENSE">LICENSE</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code of Conduct</h2><a id="user-content-code-of-conduct" aria-label="Permalink: Code of Conduct" href="#code-of-conduct"></a></p>
<p dir="auto">This project has adopted the <a href="https://opensource.microsoft.com/codeofconduct/" rel="nofollow">Microsoft Open Source Code of Conduct</a>. For more information see the <a href="https://opensource.microsoft.com/codeofconduct/faq/" rel="nofollow">Code of Conduct FAQ</a> or contact <a href="mailto:opencode@microsoft.com">opencode@microsoft.com</a> with any additional questions or comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">For bugs or feature requests, open an <a href="https://github.com/microsoft/retina/issues">issue</a>.<br>
For security or vulnerability concerns, see <a href="https://github.com/microsoft/retina/blob/main/SECURITY.md">SECURITY.md</a>.<br>
For other communication, contact the maintainers at <a href="mailto:retina@microsoft.com">retina@microsoft.com</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: jnv: interactive JSON filter using jq (229 pts)]]></title>
            <link>https://github.com/ynqa/jnv</link>
            <guid>39759325</guid>
            <pubDate>Tue, 19 Mar 2024 19:23:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ynqa/jnv">https://github.com/ynqa/jnv</a>, See on <a href="https://news.ycombinator.com/item?id=39759325">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">jnv</h2><a id="user-content-jnv" aria-label="Permalink: jnv" href="#jnv"></a></p>
<p dir="auto"><em>jnv</em> is designed for navigating JSON,
offering an interactive JSON viewer and <code>jq</code> filter editor.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/6745370/313697735-1d1495e8-5755-487f-bbf3-03e1d4edab08.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA4OTY3MDQsIm5iZiI6MTcxMDg5NjQwNCwicGF0aCI6Ii82NzQ1MzcwLzMxMzY5NzczNS0xZDE0OTVlOC01NzU1LTQ4N2YtYmJmMy0wM2UxZDRlZGFiMDguZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMyMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMjBUMDEwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTZiMzgxYWE5MGMyMzcwYmM0Y2M4ZWIzZjgxNzFjMDQwYWIzMWU1YzhkZGE3ODliOTE0ZjUxODhhNjJjYTUyOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.wj1eUtrbL2DLjx7_gtVeMrEINa6vQcrOQtNKHb9MVho"><img src="https://private-user-images.githubusercontent.com/6745370/313697735-1d1495e8-5755-487f-bbf3-03e1d4edab08.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTA4OTY3MDQsIm5iZiI6MTcxMDg5NjQwNCwicGF0aCI6Ii82NzQ1MzcwLzMxMzY5NzczNS0xZDE0OTVlOC01NzU1LTQ4N2YtYmJmMy0wM2UxZDRlZGFiMDguZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDMyMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDAzMjBUMDEwMDA0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTZiMzgxYWE5MGMyMzcwYmM0Y2M4ZWIzZjgxNzFjMDQwYWIzMWU1YzhkZGE3ODliOTE0ZjUxODhhNjJjYTUyOCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.wj1eUtrbL2DLjx7_gtVeMrEINa6vQcrOQtNKHb9MVho" alt="demo" data-animated-image=""></a></p>
<p dir="auto">Inspired by <a href="https://github.com/simeji/jid">jid</a>
and <a href="https://github.com/fiatjaf/jiq">jiq</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Interactive JSON viewer and <code>jq</code> filter editor
<ul dir="auto">
<li>Syntax highlighting for JSON</li>
</ul>
</li>
<li>Accept JSON from stdin, file, URL</li>
<li>Auto-completion for the filter
<ul dir="auto">
<li>Only supports:
<ul dir="auto">
<li><a href="https://jqlang.github.io/jq/manual/#identity" rel="nofollow">Identity</a></li>
<li><a href="https://jqlang.github.io/jq/manual/#object-identifier-index" rel="nofollow">Object Identifier-Index</a></li>
<li><a href="https://jqlang.github.io/jq/manual/#array-index" rel="nofollow">Array Index</a></li>
</ul>
</li>
</ul>
</li>
<li>Hint message to evaluate the filter</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Homebrew</h3><a id="user-content-homebrew" aria-label="Permalink: Homebrew" href="#homebrew"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install ynqa/tap/jnv"><pre>brew install ynqa/tap/jnv</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cargo</h3><a id="user-content-cargo" aria-label="Permalink: Cargo" href="#cargo"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Requirements</h4><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li><a href="https://www.gnu.org/software/automake/" rel="nofollow">automake</a></li>
</ul>

<div dir="auto"><p dir="auto">Note</p><p dir="auto"><em>jnv</em> does not require users to install <code>jq</code> on their system,
because it utilizes <a href="https://github.com/ynqa/j9">j9</a> Rust bindings.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>

<p dir="auto">Or</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Keymap</h2><a id="user-content-keymap" aria-label="Permalink: Keymap" href="#keymap"></a></p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><kbd>Ctrl + C</kbd></td>
<td>Exit <code>jnv</code></td>
</tr>
<tr>
<td><kbd>Tab</kbd></td>
<td>jq filter auto-completion</td>
</tr>
<tr>
<td><kbd>←</kbd></td>
<td>Move the cursor one character to the left</td>
</tr>
<tr>
<td><kbd>→</kbd></td>
<td>Move the cursor one character to the right</td>
</tr>
<tr>
<td><kbd>Ctrl + A</kbd></td>
<td>Move the cursor to the start of the filter</td>
</tr>
<tr>
<td><kbd>Ctrl + E</kbd></td>
<td>Move the cursor to the end of the filter</td>
</tr>
<tr>
<td><kbd>Backspace</kbd></td>
<td>Delete a character of filter at the cursor position</td>
</tr>
<tr>
<td><kbd>Ctrl + U</kbd></td>
<td>Delete all characters of filter</td>
</tr>
<tr>
<td><kbd>↑</kbd>, <kbd>Ctrl + K</kbd></td>
<td>Move the cursor one entry up in JSON viewer</td>
</tr>
<tr>
<td><kbd>↓</kbd>, <kbd>Ctrl + J</kbd></td>
<td>Move the cursor one entry down in JSON viewer</td>
</tr>
<tr>
<td><kbd>Ctrl + H</kbd></td>
<td>Move to the last entry in JSON viewer</td>
</tr>
<tr>
<td><kbd>Ctrl + L</kbd></td>
<td>Move to the first entry in JSON viewer</td>
</tr>
<tr>
<td><kbd>Enter</kbd></td>
<td>Toggle expand/collapse in JSON viewer</td>
</tr>
<tr>
<td><kbd>Ctrl + P</kbd></td>
<td>Expand all folds in JSON viewer</td>
</tr>
<tr>
<td><kbd>Ctrl + N</kbd></td>
<td>Collapse all folds in JSON viewer</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="JSON navigator and interactive filter leveraging jq

Usage: jnv [OPTIONS] [INPUT]

Examples:
- Read from a file:
        jnv data.json

- Read from standard input:
        cat data.json | jnv

Arguments:
  [INPUT]
          Optional path to a JSON file. If not provided or if &quot;-&quot; is specified, reads from standard input

Options:
  -e, --edit-mode <EDIT_MODE>
                  Specifies the edit mode for the interface.
                  Acceptable values are &quot;insert&quot; or &quot;overwrite&quot;.
                  - &quot;insert&quot; inserts a new input at the cursor's position.
                  - &quot;overwrite&quot; mode replaces existing characters with new input at the cursor's position.
          [default: insert]

  -i, --indent <INDENT>
                  Affect the formatting of the displayed JSON,
                  making it more readable by adjusting the indentation level.
          [default: 2]

  -n, --no-hint
                  When this option is enabled, it prevents the display of
                  hints that typically guide or offer suggestions to the user.

  -d, --expand-depth <EXPAND_DEPTH>
                  Specifies the initial depth to which JSON nodes are expanded in the visualization.
                  Note: Increasing this depth can significantly slow down the display for large datasets.
          [default: 3]

  -l, --suggestion-list-length <SUGGESTION_LIST_LENGTH>
                  Controls the number of suggestions displayed in the list,
                  aiding users in making selections more efficiently.
          [default: 3]

  -h, --help
          Print help (see a summary with '-h')

  -V, --version
          Print version"><pre>JSON navigator and interactive filter leveraging jq

Usage: jnv [OPTIONS] [INPUT]

Examples:
- Read from a file:
        jnv data.json

- Read from standard input:
        cat data.json <span>|</span> jnv

Arguments:
  [INPUT]
          Optional path to a JSON file. If not provided or <span>if</span> <span><span>"</span>-<span>"</span></span> is specified, reads from standard input

Options:
  -e, --edit-mode <span>&lt;</span>EDIT_MODE<span>&gt;</span>
                  Specifies the edit mode <span>for</span> the interface.
                  Acceptable values are <span><span>"</span>insert<span>"</span></span> or <span><span>"</span>overwrite<span>"</span></span>.
                  - <span><span>"</span>insert<span>"</span></span> inserts a new input at the cursor<span><span>'</span>s position.</span>
<span>                  - "overwrite" mode replaces existing characters with new input at the cursor<span>'</span></span>s position.
          [default: insert]

  -i, --indent <span>&lt;</span>INDENT<span>&gt;</span>
                  Affect the formatting of the displayed JSON,
                  making it more readable by adjusting the indentation level.
          [default: 2]

  -n, --no-hint
                  When this option is enabled, it prevents the display of
                  hints that typically guide or offer suggestions to the user.

  -d, --expand-depth <span>&lt;</span>EXPAND_DEPTH<span>&gt;</span>
                  Specifies the initial depth to which JSON nodes are expanded <span>in</span> the visualization.
                  Note: Increasing this depth can significantly slow down the display <span>for</span> large datasets.
          [default: 3]

  -l, --suggestion-list-length <span>&lt;</span>SUGGESTION_LIST_LENGTH<span>&gt;</span>
                  Controls the number of suggestions displayed <span>in</span> the list,
                  aiding users <span>in</span> making selections more efficiently.
          [default: 3]

  -h, --help
          Print <span>help</span> (see a summary with <span><span>'</span>-h<span>'</span></span>)

  -V, --version
          Print version</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intel 8080 emulator. 19th IOCCC. Best of Show (208 pts)]]></title>
            <link>https://nanochess.org/emulator.html</link>
            <guid>39758667</guid>
            <pubDate>Tue, 19 Mar 2024 18:15:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nanochess.org/emulator.html">https://nanochess.org/emulator.html</a>, See on <a href="https://news.ycombinator.com/item?id=39758667">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>Intel 8080 emulator. 19th IOCCC. Best of Show.</h2>
<div><p>Sections:
</p><ul>
<li><a href="#sour">Source code</a></li>
<li><a href="#howc">How to compile it</a></li>
<li><a href="#litt">A little course on CP/M</a></li>
<li><a href="#what">What is an 8080?</a></li>
<li><a href="#port">Porting it</a></li>
<li><a href="#howi">How it works</a></li>
<li><a href="#othe">Other notes</a></li>
<li><a href="#usef">Useful links</a></li>
</ul>
</div>
<p>
After winning the IOCCC for the <a href="https://nanochess.org/chess1.html">first time</a>, I had
the idea of writing an emulator of the 8080 processor in 2000 characters of C,
after patterning experimentally the more than 200 instructions and doing
measures of byte count, I realized that it was possible and I made it.
Then I added CP/M support as a extra feature. I was completely astonished when
I won Best of Show of 19th IOCCC :).
</p>
<div><p>
This program was one of three winning entries I sent for the 19th edition, the
other two were a <a href="https://nanochess.org/chess2.html">graphical chess program</a> and a
<a href="https://nanochess.org/knight.html">knight's tour solver</a>.
</p></div>
<h2>Source code</h2>
<p>This program emulates a complete Intel® 8080
processor, along with a teletype and a disk
controller, just like at the start of the personal
computers revolution (circa 1975).</p>
<p>Here is the source code, written in C language:</p>
<pre><small>                               #include &lt;stdio.h&gt;
           #define n(o,p,e)=y=(z=a(e)%16 p x%16 p o,a(e)p x p o),h(
                                #define s 6[o]
             #define p z=l[d(9)]|l[d(9)+1]&lt;&lt;8,1&lt;(9[o]+=2)||++8[o]
                                #define Q a(7)
           #define w 254&gt;(9[o]-=2)||--8[o],l[d(9)]=z,l[1+d(9)]=z&gt;&gt;8
                               #define O )):((
                  #define b (y&amp;1?~s:s)&gt;&gt;"\6\0\2\7"[y/2]&amp;1?0:(
                               #define S )?(z-=
                    #define a(f)*((7&amp;f)-6?&amp;o[f&amp;7]:&amp;l[d(5)])
                               #define C S 5 S 3
                       #define D(E)x/8!=16+E&amp;198+E*8!=x?
                             #define B(C)fclose((C))
                       #define q (c+=2,0[c-2]|1[c-2]&lt;&lt;8)
                          #define m x=64&amp;x?*c++:a(x),
                         #define A(F)=fopen((F),"rb+")
                    unsigned char o[10],l[78114],*c=l,*k=l
                          #define d(e)o[e]+256*o[e-1]
#define h(l)s=l&gt;&gt;8&amp;1|128&amp;y|!(y&amp;255)*64|16&amp;z|2,y^=y&gt;&gt;4,y^=y&lt;&lt;2,y^=~y&gt;&gt;1,s|=y&amp;4
+64506; FILE *u, *v, *e, *V; int x,y,z,Z; main(r,U)char**U;{

     { { { } } }       { { { } } }       { { { } } }       { { { } } }
    { { {   } } }     { { {   } } }     { { {   } } }     { { {   } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
    { { {   } } }    { { {     } } }    { { {   } } }    { { {     } } }
      { { ; } }      { { {     } } }      { { ; } }      { { {     } } }
    { { {   } } }    { { {     } } }    { { {   } } }    { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
   { { {     } } }   { { {     } } }   { { {     } } }   { { {     } } }
    { { {   } } }     { { {   } } }     { { {   } } }     { { {   } } }
     { { { } } }       { { { } } }       { { { } } }       { { { } } }

                                   for(v A((u A((e A((r-2?0:(V A(1[U])),"C")
),system("stty raw -echo min 0"),fread(l,78114,1,e),B(e),"B")),"A")); 118-(x
=*c++); (y=x/8%8,z=(x&amp;199)-4 S 1 S 1 S 186 S 2 S 2 S 3 S 0,r=(y&gt;5)*2+y,z=(x&amp;
207)-1 S 2 S 6 S 2 S 182 S 4)?D(0)D(1)D(2)D(3)D(4)D(5)D(6)D(7)(z=x-2 C C C C
C C C C+129 S 6 S 4 S 6 S 8 S 8 S 6 S 2 S 2 S 12)?x/64-1?((0 O a(y)=a(x) O 9
[o]=a(5),8[o]=a(4) O 237==*c++?((int (*)())(2-*c++?fwrite:fread))(l+*k+1[k]*
256,128,1,(fseek(e=5[k]-1?u:v,((3[k]|4[k]&lt;&lt;8)&lt;&lt;7|2[k])&lt;&lt;7,Q=0),e)):0 O y=a(5
),z=a(4),a(5)=a(3),a(4)=a(2),a(3)=y,a(2)=z O c=l+d(5) O y=l[x=d(9)],z=l[++x]
,x[l]=a(4),l[--x]=a(5),a(5)=y,a(4)=z O 2-*c?Z||read(0,&amp;Z,1),1&amp;*c++?Q=Z,Z=0:(
Q=!!Z):(c++,Q=r=V?fgetc(V):-1,s=s&amp;~1|r&lt;0) O++c,write(1,&amp;7[o],1) O z=c+2-l,w,
c=l+q O p,c=l+z O c=l+q O s^=1 O Q=q[l] O s|=1 O q[l]=Q O Q=~Q O a(5)=l[x=q]
,a(4)=l[++x] O s|=s&amp;16|9&lt;Q%16?Q+=6,16:0,z=s|=1&amp;s|Q&gt;159?Q+=96,1:0,y=Q,h(s&lt;&lt;8)
O l[x=q]=a(5),l[++x]=a(4) O x=Q%2,Q=Q/2+s%2*128,s=s&amp;~1|x O Q=l[d(3)]O x=Q  /
128,Q=Q*2+s%2,s=s&amp;~1|x O l[d(3)]=Q O s=s&amp;~1|1&amp;Q,Q=Q/2|Q&lt;&lt;7 O Q=l[d(1)]O s=~1
&amp;s|Q&gt;&gt;7,Q=Q*2|Q&gt;&gt;7 O l[d(1)]=Q O m y n(0,-,7)y) O m z=0,y=Q|=x,h(y) O m z=0,
y=Q^=x,h(y) O m z=Q*2|2*x,y=Q&amp;=x,h(y) O m Q n(s%2,-,7)y) O m Q n(0,-,7)y)  O
m Q n(s%2,+,7)y) O m Q n(0,+,7)y) O z=r-8?d(r+1):s|Q&lt;&lt;8,w O p,r-8?o[r+1]=z,r
[o]=z&gt;&gt;8:(s=~40&amp;z|2,Q=z&gt;&gt;8) O r[o]--||--o[r-1]O a(5)=z=a(5)+r[o],a(4)=z=a(4)
+o[r-1]+z/256,s=~1&amp;s|z&gt;&gt;8 O ++o[r+1]||r[o]++O o[r+1]=*c++,r[o]=*c++O z=c-l,w
,c=y*8+l O x=q,b z=c-l,w,c=l+x) O x=q,b c=l+x) O b p,c=l+z) O a(y)=*c++O r=y
,x=0,a(r)n(1,-,y)s&lt;&lt;8) O r=y,x=0,a(r)n(1,+,y)s&lt;&lt;8))));
system("stty cooked echo"); B((B((V?B(V):0,u)),v)); }</small></pre>
<h2>How to compile it</h2>
<p>First, download the source code from <a href="https://nanochess.org/toledo2.c">here</a>.
It requires an *NIX compatible system (find porting notes below).</p>
<p>To compile use:</p>
<pre><small>    cc toledo2.c -o toledo2</small></pre>
<div><p>The emulator needs an initial memory image to do something
usable, so it will need two files
(</p><tt><a href="https://nanochess.org/c_basic.bin">c_basic.bin</a></tt><p> and
</p><tt><a href="https://nanochess.org/c_bios.bin">c_bios.bin</a></tt><p>).
Rename </p><tt>c_basic.bin</tt><p> to C and run
the emulator, and et voila! you have the public
domain Palo Alto Tiny BASIC (by Li-Chen Wang),
published on the very first volume of the now extinct Dr. Dobb's
Journal magazine.</p></div>
<p>Type using uppercase letters, here are three example
programs, press Enter after each line:</p>
<pre><small>   10 PRINT "Hello, world!"
   LIST
   RUN

   10 FOR A=1 TO 10       10 INPUT A
   20 PRINT A             20 INPUT B
   30 NEXT A              30 PRINT A+B
   LIST                   LIST
   RUN                    RUN</small></pre>
<p>Press Ctrl+Z to quit, by the way, the segmentation
fault is normal at this point.</p>
<p>All good programmers started learning BASIC, now,
what about a CP/M emulator?</p>
<p>Download the following file (not included because
of possible copyright and blah, blah):</p>
<pre><small>  <a href="http://www.retroarchive.org/cpm/os/KAYPROII.ZIP">http://www.retroarchive.org/cpm/os/KAYPROII.ZIP</a></small></pre>
<p>Extract CPM64.COM from the SOURCE directory, and
copy it to files named A and B (these will be the
disk drives). Now rename the provided c_bios.bin to C
and run the emulator.</p>
<p>Now you have a running CP/M operating system!, with two
files on A: drive, HALT.COM to stop the emulator
(so it closes drives) and IMPORT.COM to introduce
new files.</p>
<p>To get a complete CP/M system, you will need the
following files from the KAYPROII.ZIP, SOURCE
directory:</p>
<pre><small>  ASM.COM  DDT.COM   DUMP.COM   ED.COM   LOAD.COM
  PIP.COM  STAT.COM  SUBMIT.COM XSUB.COM</small></pre>
<p>To import them, you must run the emulator with an
argument, by example:</p>
<pre><small>  prog DDT.COM</small></pre>
<p>When the A&gt; prompt appears, do:</p>
<pre><small>  IMPORT DDT.COM</small></pre>
<p>When it ends, do HALT, so the file is saved, and
you can start the same process with another file.</p>
<p>The WS33-KPR directory of the KAYPROII.ZIP file also
contains a Wordstar version that works with this emulator. There is also
an interesting game, the classical Adventure by Crowther and Woods in the
ADVENTUR directory, at that time was amazing that a microcomputer could
contain such big game.</p>
<p>At this time I have tested successfully the
following software from
<a href="http://www.retroarchive.org/">www.retroarchive.org</a>:</p>
<pre>Languages
<small>  <a href="http://www.retroarchive.org/cpm/lang/c80v30.zip">http://www.retroarchive.org/cpm/lang/c80v30.zip</a>
  <a href="http://www.retroarchive.org/cpm/lang/Mbasic.com">http://www.retroarchive.org/cpm/lang/Mbasic.com</a>
</small>
Spreadsheet
<small>  <a href="http://www.retroarchive.org/cpm/business/MULTPLAN.ZIP">http://www.retroarchive.org/cpm/business/MULTPLAN.ZIP</a>
</small>
Games
<small>  <a href="http://www.retroarchive.org/cpm/games/zork123_80.zip">http://www.retroarchive.org/cpm/games/zork123_80.zip</a>
</small>
Utilities
<small>  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/UNARC16.ARK">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/UNARC16.ARK</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/UNARC16.MSG">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/UNARC16.MSG</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/DELBR12.ARK">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/ARC-LBR/DELBR12.ARK</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/SQUSQ/USQ-20.COM">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/SQUSQ/USQ-20.COM</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/SQUSQ/UNCR8080.LBR">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/SQUSQ/UNCR8080.LBR</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/DIRUTL/XDIR3-12.LBR">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/DIRUTL/XDIR3-12.LBR</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/DIRUTL/CU.LBR">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/DIRUTL/CU.LBR</a>
  <a href="http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/FILCPY/SWEEP40.LBR">http://www.retroarchive.org/cpm/cdrom/CPM/UTILS/FILCPY/SWEEP40.LBR</a></small></pre>
<p>Some programs require installation to configure
the terminal, locate ANSI or VT-100.</p>
<h2>A little course on CP/M</h2>
<p>The CP/M user's manuals are available on
<a href="http://www.retroarchive.org/">www.retroarchive.org</a>. But
if you remember how to use MS-DOS then you can use CP/M very easily. Here is a
little reference of the command line:</p>
<pre><b>Internal commands:</b>
A:              Change current drive to A
B:              Change current drive to B
DIR             List files in drive
DIR *.TXT       List all files with TXT extension
TYPE FILE.TXT   Shows content of FILE.TXT
ERA FILE.TXT    Erases file FILE.TXT
USER 1          Change to user 1 (0-15 available)
                It is something as subdirectories.
                So you can separate your files.

<b>External commands:</b>
STAT            Show used/free space on drive
STAT *.*        Show file sizes.
DDT PROG.COM    Debug PROG.COM.
                To quit use Ctrl+C

                Dump address 0100 (hex):
                  D0100</pre>
<p><img src="https://nanochess.org/img/captura1.gif" alt="Emulator running Wordstar under CP/M"><br>Emulator running Wordstar
under CP/M.</p>
<h2>What is an 8080?</h2>
<p>It is simply the little brother of the Zilog Z80,
it has no extended registers (AF', BC', DE', HL',
IX or IY), no relative jumps, and every instruction
beginning with CB, DD, ED or FD doesn't exist.</p>
<p>The flags are only S (Sign, bit 7), Z (Zero, bit 6),
P (Parity, bit 2) and C (Carry, bit 0).</p>
<p>The 8080 processor was created by Federico Faggin and
Masatoshi Shima in 1974, afterwards both would design the Zilog Z80 in 1976,
these two processors were pretty important and influential for the rise of
microcomputers.</p>
<h2>Porting it</h2>
<div><p>It is easy if your platform has </p><tt>getch</tt><p>/</p><tt>kbhit</tt><p>
and ANSI terminal</p></div>
<pre><small>    read    --&gt;  Z=kbhit()?getch():0
    write   --&gt;  putchar(7[o])
    system  --&gt;  nothing</small></pre>
<p>Also add the following to trap Ctrl-C:</p>
<pre><small>    #include &lt;signal.h&gt;
    signal(SIGINT, SIG_IGN);</small></pre>
<div><p>On PC/DOS you will need to add </p><tt>ANSI.SYS</tt><p> to
</p><tt>CONFIG.SYS</tt></div>
<div><p>In *NIX the </p><tt>min 0</tt><p> for </p><tt>stty</tt><p> is required,
circa 2001 it was not required.</p></div>
<h2>How it works (SPOILER)</h2>
<p>The l array contains the 64K memory, it is
initialized with a boot image loaded from the 'C'
file, the program counter is the c pointer, and
registers are on o[]. The main loops reads every
op-code and separates it in one of three common
forms, a lot of trinary operators selects the
instruction.</p>
<p>Execution starts at 0000, you can write your own boot
or monitor program, or even your own operating system playing with the
'C' file.</p>
<pre><small>   o[0] = B register   o[1] = C register
   o[2] = D register   o[3] = E register
   o[4] = H register   o[5] = L register
   o[6] = Flags        o[7] = A or accumulator</small></pre>
<p>The following instructions do peripheral operation:</p>
<pre><small>   76           Quits emulator
   DB 00        Reads key pressed status
   DB 01        Reads key
   DB 02        Reads byte from file (Carry=EOF)
   D3 xx        Writes byte from acc. to console
   ED ED 02     Reads sector (128 bytes)
   ED ED 03     Writes sector (128 bytes)

   Memory addresses:

   FBFA = Low source/target address
   FBFB - High source/target address
   FBFC - Sector (0-127)
   FBFD - Cylinder (low byte)
   FBFE - Cylinder (high byte)
   FBFF - Drive (1/2)</small></pre>
<p>The BIOS is tailor made for this emulator and
helps to simplify it. Though the size of each virtual hard disk drive can
reach 1 GB, CP/M only handles up to 8 MB.</p>
<h2>Other notes:</h2>
<ul>
<li>The 8080 runs at your computer speed divided
between a number that I have not calculated.
</li><li>This obfuscated processor was created using
obfuscated code produced by an obfuscated mind,
no brains were harmed during its production,
except those that tried to read the code.
</li><li>The original version of this code was eaten
by my C++ dog.
</li><li>I intended to make it simple to understand,
it uses only four C keywords.
</li><li>Also I discovered that braces are very useful
for commenting.
</li><li>Why to bother with prototypes?, every good C
programmer can develop its C programs using
only one function.
</li></ul>
<h2>And now it is 2024</h2>
<p>This emulator was developed eighteen years ago when the computers had 32-bit processors and it used a hole in the C language syntax where you could pass a pointer on an integer. In fact, this is the IOCCC objective: make C compilers to do things these shouldn't be supposed to do.</p>
<p>However, the C compilers for 64-bit processors don't allow it anymore as pointers are 64-bit and the int types are 32-bit, so compilers stop with an error (in especial in macOS because clang).</p>
<p>The source code in this webpage is already updated to use FILE * instead of int, and this way we brought CP/M to year 2024, hehe.</p>
<p>If you are curious about it, the previous version of the emulator can be downloaded <a href="https://nanochess.org/toledo2_orig.c">here</a> or in the IOCCC website. In 2020, Mark Abene sent me a workaround for x86-64 computers:</p>
<pre><small>    gcc -static toledo2.c -o toledo2</small></pre>
<p>But this doesn't work in macOS.</p>
<h2>Useful links:</h2>
<ul>
<li><a href="http://www.nicholson.com/rhn/basic/">Palo Alto Tiny BASIC source
code</a>, this is a modified version of the original.</li>
<li><a href="http://www.retroarchive.org/cpm/archive/unofficial/source.html">CP/M
source code</a>, the foundation of an operating system: monotask,
no memory manager, only console and file services provided.</li>
<li><a href="http://stardot.org.uk/forums/viewtopic.php?f=3&amp;t=9821&amp;p=116643">Someone received an 8080 computer as a gift</a>,
dumped the ROM, and someone
else found a way to test them with my 8080 emulator. :)</li>
</ul>
<p>Last modified: Feb/06/2024</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What does Alan Kay think about LLMs? (173 pts)]]></title>
            <link>https://www.quora.com/What-does-Alan-Kay-think-about-programming-and-teaching-programming-with-copilots-and-LLMs-of-today</link>
            <guid>39758391</guid>
            <pubDate>Tue, 19 Mar 2024 17:48:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quora.com/What-does-Alan-Kay-think-about-programming-and-teaching-programming-with-copilots-and-LLMs-of-today">https://www.quora.com/What-does-Alan-Kay-think-about-programming-and-teaching-programming-with-copilots-and-LLMs-of-today</a>, See on <a href="https://news.ycombinator.com/item?id=39758391">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Natural language instructions induce generalization in networks of neurons (159 pts)]]></title>
            <link>https://www.nature.com/articles/s41593-024-01607-5</link>
            <guid>39757665</guid>
            <pubDate>Tue, 19 Mar 2024 16:47:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/s41593-024-01607-5">https://www.nature.com/articles/s41593-024-01607-5</a>, See on <a href="https://news.ycombinator.com/item?id=39757665">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div id="Sec1-section" data-title="Main"><h2 id="Sec1">Main</h2><div id="Sec1-content"><p>In a laboratory setting, animals require numerous trials in order to acquire a new behavioral task. This is in part because the only means of communication with nonlinguistic animals is simple positive and negative reinforcement signals. By contrast, it is common to give written or verbal instructions to humans, which allows them to perform new tasks relatively quickly. Further, once humans have learned a task, they can typically describe the solution with natural language. The dual ability to use an instruction to perform a novel task and, conversely, produce a linguistic description of the demands of a task once it has been learned are two unique cornerstones of human communication. Yet, the computational principles that underlie these abilities remain poorly understood.</p><p>One influential systems-level explanation posits that flexible interregional connectivity in the prefrontal cortex allows for the reuse of practiced sensorimotor representations in novel settings<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Cole, M. W. et al. Multi-task connectivity reveals flexible hubs for adaptive task control. Nature Neurosci. 16, 1348–1355 (2013)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR1" id="ref-link-section-d167182994e339">1</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Miller, E. K. &amp; Cohen, J. D. An integrative theory of prefrontal cortex function. Annu. Rev. Neurosci. 24, 167–202 (2001)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR2" id="ref-link-section-d167182994e342">2</a></sup>. More recently, multiple studies have observed that when subjects are required to flexibly recruit different stimulus-response patterns, neural representations are organized according to the abstract structure of the task set<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Bernardi, S. et al. The geometry of abstraction in the hippocampus and prefrontal cortex. Cell 183, 954–967 (2020)." href="#ref-CR3" id="ref-link-section-d167182994e346">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Minxha, J., Adolphs, R., Fusi, S., Mamelak, A. N. &amp; Rutishauser, U. Flexible recruitment of memory-based choice representations by the human medial frontal cortex. Science 368, eaba3313 (2020)." href="#ref-CR4" id="ref-link-section-d167182994e346_1">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Takuya, I. et al. Compositional generalization through abstract representations in human and artificial neural networks. In Proc. 36th Conference on Neural Information Processing Systems (eds Koyejo, S. et al.) 32225–32239 (Curran Associates, Inc., 2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR5" id="ref-link-section-d167182994e349">5</a></sup>. Lastly, recent modeling work has shown that a multitasking recurrent neural network (RNN) will share dynamical motifs across tasks with similar demands<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Driscoll, L., Shenoy, K. &amp; Sussillo, D. Flexible multitask computation in recurrent networks utilizes shared dynamical motifs. Preprint at bioRxiv 
                https://doi.org/10.1101/2022.08.15.503870
                
               (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR6" id="ref-link-section-d167182994e353">6</a></sup>. This work forms a strong basis for explanations of flexible cognition in humans but leaves open the question of how linguistic information can reconfigure a sensorimotor network so that it performs a novel task well on the first attempt. Overall, it remains unclear what representational structure we should expect from brain areas that are responsible for integrating linguistic information in order to reorganize sensorimotor mappings on the fly.</p><p>These questions become all the more pressing given that recent advances in machine learning have led to artificial systems that exhibit human-like language skills<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Brown, Tom, et al. Language models are few-shot learners. In Proc. 34th International Conference on Neural Information Processing Systems 1877–1901 (Curran Associates Inc., 2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR7" id="ref-link-section-d167182994e360">7</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ramesh, A. et al. Zero-shot text-to-image generation. In Proc. 38th International Conference on Machine Learning (eds Marina, M. &amp; Tong, Z.) 8821–8831 (PMLR, 2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR8" id="ref-link-section-d167182994e363">8</a></sup>. Recent works have matched neural data recorded during passive listening and reading tasks to activations in autoregressive language models (that is, GPT<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Radford, A. et al. Language models are unsupervised multitask learners. OpenAI 1, 9 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR9" id="ref-link-section-d167182994e367">9</a></sup>), arguing that there is a fundamentally predictive component to language comprehension<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Schrimpf, M. et al. The neural architecture of language: integrative modeling converges on predictive processing. Proc. Natl Acad. Sci. USA 
                https://doi.org/10.1073/pnas.2105646118
                
               (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR10" id="ref-link-section-d167182994e371">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Goldstein, A. et al. Shared computational principles for language processing in humans and deep language models. Nature Neurosci. 25, 369–380 (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR11" id="ref-link-section-d167182994e374">11</a></sup>. Additionally, some high-profile machine learning models do show the ability to use natural language as a prompt to perform a linguistic task or render an image, but the outputs of these models are difficult to interpret in terms of a sensorimotor mapping that we might expect to occur in a biological system<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Chowdhery, A. et al. Palm: scaling language modeling with pathways. J. Mach. Learn. Res. 24, 11324–11436 (2023)." href="#ref-CR12" id="ref-link-section-d167182994e378">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Thoppilan, R. et al. Lamda: language models for dialog applications. Preprint at 
                https://arxiv.org/abs/2201.08239
                
               (2022)." href="#ref-CR13" id="ref-link-section-d167182994e378_1">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Rombach, R. et al. High-resolution image synthesis with latent diffusion models. In Proc. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 10674–10685 (IEEE, 2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR14" id="ref-link-section-d167182994e381">14</a></sup>. Alternatively, recent work on multimodal interactive agents may be more interpretable in terms of the actions they take, but utilize a perceptual hierarchy that fuses vision and language at early stages of processing, making them difficult to map onto functionally and anatomically distinct language and vision areas in human brains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zitkovich, B. et al. Rt-2: vision-language-action models transfer web knowledge to robotic control. In Proc. 7th Conference on Robot Learning (eds Tan, J. et al.) 2165-2183 (PMLR, 2023)." href="#ref-CR15" id="ref-link-section-d167182994e385">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Abramson, J. et al. Imitating interactive intelligence. Preprint at 
                https://arxiv.org/abs/2012.05672
                
               (2021)." href="#ref-CR16" id="ref-link-section-d167182994e385_1">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="DeepMind Interactive Agents Team. Creating multimodal interactive agents with imitation and self-supervised learning. Preprint at 
                https://arxiv.org/abs/2112.03763
                
               (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR17" id="ref-link-section-d167182994e388">17</a></sup>.</p><p>We, therefore, seek to leverage the power of language models in a way that results in testable neural predictions detailing how the human brain processes natural language in order to generalize across sensorimotor tasks.</p><p>To that end, we train an RNN (sensorimotor-RNN) model on a set of simple psychophysical tasks where models process instructions for each task using a pretrained language model. We find that embedding instructions with models tuned to sentence-level semantics allow sensorimotor-RNNs to perform a novel task at 83% correct, on average. Generalization in our models is supported by a representational geometry that captures task subcomponents and is shared between instruction embeddings and sensorimotor activity, thereby allowing a composition of practice skills in a novel setting. We also find that individual neurons modulate their tuning based on the semantics of instructions. We demonstrate how a network trained to interpret linguistic instructions can invert this understanding and produce a linguistic description of a previously unseen task based on the information in motor feedback signals. We end by discussing how these results can guide research on the neural basis of language-based generalization in the human brain.</p></div></div><div id="Sec2-section" data-title="Results"><h2 id="Sec2">Results</h2><div id="Sec2-content"><h3 id="Sec3">Instructed models and task set</h3><p>We train sensorimotor-RNNs on a set of 50 interrelated psychophysical tasks that require various cognitive capacities that are well studied in the literature<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e411">18</a></sup>. Two example tasks are presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig1">1a,b</a> as they might appear in a laboratory setting. For all tasks, models receive a sensory input and task-identifying information and must output motor response activity (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig1">1c</a>). Input stimuli are encoded by two one-dimensional maps of neurons, each representing a different input modality, with periodic Gaussian tuning curves to angles (over (0, 2<i>π</i>)). Output responses are encoded in the same way. Inputs also include a single fixation unit. After the input fixation is off, the model can respond to the input stimuli. Our 50 tasks are roughly divided into 5 groups, ‘Go’, ‘Decision-making’, ‘Comparison’, ‘Duration’ And ‘Matching’, where within-group tasks share similar sensory input structures but may require divergent responses. For instance, in the decision-making (DM) task, the network must respond in the direction of the stimulus with the highest contrast, whereas in the anti-decision-making (AntiDM) task, the network responds to the stimulus with the weakest contrast (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig1">1a</a>). Thus, networks must properly infer the task demands for a given trial from task-identifying information in order to perform all tasks simultaneously (see Methods for task details; see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">13</a> for example trials of all tasks).</p><div data-test="figure" data-container-section="figure" id="figure-1" data-title="Tasks and models."><figure><figcaption><b id="Fig1" data-test="figure-caption-text">Fig. 1: Tasks and models.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="684"></picture></a></div><p><b>a</b>,<b>b</b>, Illustrations of example trials as they might appear in a laboratory setting. The trial is instructed, then stimuli are presented with different angles and strengths of contrast. The agent must then respond with the proper angle during the response period. <b>a</b>, An example AntiDM trial where the agent must respond to the angle presented with the least intensity. <b>b</b>, An example COMP1 trial where the agent must respond to the first angle if it is presented with higher intensity than the second angle otherwise repress response. <b>c</b>, Diagram of model inputs and outputs. Sensory inputs (fixation unit, modality 1, modality 2) are shown in red and model outputs (fixation output, motor output) are shown in green. Models also receive a rule vector (blue) or the embedding that results from passing task instructions through a pretrained language model (gray). A list of models tested is provided in the inset.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>In our models, task-identifying input is either nonlinguistic or linguistic. We use two nonlinguistic control models. First, in SIMPLENET, the identity of a task is represented by one of 50 orthogonal rule vectors. Second, STRUCTURENET uses a set of 10 orthogonal structure vectors, each representing a dimension of the task set (that is, respond weakest versus strongest direction), and tasks are encoded using combinations of these vectors (see Supplementary Notes <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a> for the full set of structure combinations). As a result, STRUCTURENET fully captures all the relevant relationships among tasks, whereas SIMPLENET encodes none of this structure.</p><p>Instructed models use a pretrained transformer architecture<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Vaswani, A. et al. Attention is all you need. In Proc. 31st International Conference on Neural Information Processing Systems 6000–6010 (Curran Associates Inc., 2017)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR19" id="ref-link-section-d167182994e474">19</a></sup> to embed natural language instructions for the tasks at hand. For each task, there is a corresponding set of 20 unique instructions (15 training, 5 validation; see Supplementary Notes <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">2</a> for the full instruction set). We test various types of language models that share the same basic architecture but differ in their size and also their pretraining objective. We tested two autoregressive models, a standard and a large version of GPT2, which we call GPT and GPT (XL), respectively. Previous work has demonstrated that GPT activations can account for various neural signatures of reading and listening<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Goldstein, A. et al. Shared computational principles for language processing in humans and deep language models. Nature Neurosci. 25, 369–380 (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR11" id="ref-link-section-d167182994e481">11</a></sup>. BERT is trained to identify masked words within a piece of text<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Devlin, J., Chang, M., Lee, K. &amp; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. Preprint at 
                http://arxiv.org/abs/1810.04805
                
               (2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR20" id="ref-link-section-d167182994e485">20</a></sup>, but it also uses an unsupervised sentence-level objective, in which the network is given two sentences and must determine whether they follow each other in the original text. SBERT is trained like BERT but receives additional tuning on the Stanford Natural Language Inference task, a hand-labeled dataset detailing the logical relationship between two candidate sentences (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Reimers, N. &amp; Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint at 
                https://arxiv.org/abs/1908.10084
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR21" id="ref-link-section-d167182994e493">21</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Bowman, S. R., Angeli, G., Potts, C. &amp; Manning, C. D. A large annotated corpus for learning natural language inference. Preprint at 
                http://arxiv.org/abs/1508.05326
                
               (2015)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR22" id="ref-link-section-d167182994e496">22</a></sup>. Lastly, we use the language embedder from CLIP, a multimodal model that learns a joint embedding space of images and text captions<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Radford, A. et al. &quot;Learning transferable visual models from natural language supervision. In Proc. 38th International Conference on Machine Learning (eds Marina, M. &amp; Tong, Z.) 8748–8763 (PMLR, 2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR23" id="ref-link-section-d167182994e500">23</a></sup>. We call a sensorimotor-RNN using a given language model LANGUAGEMODELNET and append a letter indicating its size. The various sizes of models are given in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig1">1c</a>. For each language model, we apply a pooling method to the last hidden state of the transformer and pass this fixed-length representation through a set of linear weights that are trained during task learning. This results in a 64-dimensional instruction embedding across all models (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>). Language model weights are frozen unless otherwise specified. Finally, as a control, we also test a bag-of-words (BoW) embedding scheme that only uses word count statistics to embed each instruction.</p><p>First, we verify our models can perform all tasks simultaneously. For instructed models to perform well, they must infer the common semantic content between 15 distinct instruction formulations for each task. We find that all our instructed models can learn all tasks simultaneously except for GPTNET, where performance asymptotes are below the 95% threshold for some tasks. Hence, we relax the performance threshold to 85% for models that use GPT (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">1</a>; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a> for training details). We additionally tested all architectures on validation instructions (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">2</a>). SBERTNET (L) and SBERTNET are our best-performing models, achieving an average performance of 97% and 94%, respectively, on validation instructions, demonstrating that these networks infer the proper semantic content even for entirely novel instructions.</p><h3 id="Sec4">Generalization to novel tasks</h3><p>We next examined the extent to which different language models aided generalization to novel tasks. We trained individual networks on 45 tasks and then tested performance when exposed to the five held-out tasks. We use unequal-variance <i>t</i>-tests to make comparisons among the performance of different models. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2</a> shows results with <i>P</i> values for the most relevant comparisons (a full matrix of comparisons across all models can be found in Supplementary Figs. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a> and <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>)</p><div data-test="figure" data-container-section="figure" id="figure-2" data-title="Model performance on novel tasks."><figure><figcaption><b id="Fig2" data-test="figure-caption-text">Fig. 2: Model performance on novel tasks.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="520"></picture></a></div><p><b>a</b>, Learning curves for the first 100 exposures to held-out tasks averaged over all tasks. Data are presented as the mean ± s.d. across different <i>n</i> = 5 random initializations of sensorimotor-RNN weights. For all subplots, asterisks indicate significant differences among performance according to a two-sided unequal-variance <i>t</i>-test. Most relevant comparisons are presented in plots (for all subplots, not significant (NS), <i>P</i> &gt; 0.05, *<i>P</i> &lt; 0.05, **<i>P</i> &lt; 0.01, ***<i>P</i> &lt; 0.001; STRUCTURENET versus SBERTNET (L): <i>t</i> = 3.761, <i>P</i> = 1.89 × 10<sup>−4</sup>; SBERTNET (L) versus SBERTNET: <i>t</i> = 2.19, <i>P</i> = 0.029; SBERTNET versus CLIPNET: <i>t</i> = 6.22, <i>P</i> = 1.02 × 10<sup>−9</sup>; CLIPNET versus BERTNET: <i>t</i> = 1.037, <i>P</i> = 0.300; BERTNET versus GPTNET (XL): <i>t</i> = −1.122, <i>P</i> = 0.262; GPTNET (XL) versus GPTNET: <i>t</i> = 6.22, <i>P</i> = 1.04 × 10<sup>−9</sup>; GPTNET versus BOWNET: <i>t</i> = −3.346, <i>P</i> = 8.85 × 10<sup>−</sup><sup>4</sup>; BOWNET versus SIMPLENET: <i>t</i> = 10.25, <i>P</i> = 2.091 × 10<sup>−22</sup>). A full table of pairwise comparisons can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a>. <b>b</b>, Distribution of generalization performance (that is, first exposure to novel task) across models. <b>c</b>–<b>f</b>, Performance across different test conditions for <i>n</i> = 5 different random initialization of sensorimotor-RNN weights where each point indicates average performance across tasks for a given initialization. <b>c</b>, Generalization performance for tasks where instructions are swapped at test time (STRUCTURENET versus SBERTNET (L): <i>t</i> = −0.15, <i>P</i> = 0.875; SBERTNET (L) versus SBERTNET: <i>t</i> = −2.102, <i>P</i> = 0.036; SBERTNET versus CLIPNET: <i>t</i> = −0.162, <i>P</i> = 0.871; CLIPNET versus BERTNET: <i>t</i> = 0.315, <i>P</i> = 0.752; BERTNET versus GPTNET (XL): <i>t</i> = 0.781, <i>P</i> = 0.435; GPTNET (XL) versus GPTNET: <i>t</i> = 1.071, <i>P</i> = 0.285; GPTNET versus BOWNET: <i>t</i> = −2.702, <i>P</i> = 0.007; BOWNET versus SIMPLENET: <i>t</i> = −3.471, <i>P</i> = 5.633<sup>−4</sup>). A full table of pairwise comparisons can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>. <b>d</b>, Generalization performance for models where tasks from the same family are held out during training (STRUCTURENET versus SBERTNET (L): <i>t</i> = 0.629, <i>P</i> = 0.530; SBERTNET (L) versus SBERTNET: <i>t</i> = −0.668, <i>P</i> = 0.504; SBERTNET versus CLIPNET: <i>t</i> = 8.043, <i>P</i> = 7.757 × 10<sup>−15</sup>; CLIPNET versus BERTNET: <i>t</i> = −0.306, <i>P</i> = 0.759; BERTNET versus GPTNET (XL): <i>t</i> = 0.163, <i>P</i> = 0.869; GPTNET (XL) versus GPTNET: <i>t</i> = 1.534, <i>P</i> = 0.126; GPTNET versus BOWNET: <i>t</i> = −6.418, <i>P</i> = 3.26 × 10<sup>−10</sup>; BOWNET versus SIMPLENET: <i>t</i> = 14.23, <i>P</i> = 8.561<sup>−39</sup>). A full table of pairwise comparisons can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>. <b>e</b>, Generalization performance for models where the last layers of language models are allowed to fine-tune to the loss from sensorimotor tasks (STRUCTURENET versus SBERTNET (L): <i>t</i> = 1.203, <i>P</i> = 0.229; SBERTNET (L) versus SBERTNET: <i>t</i> = 2.399, <i>P</i> = 0.016; SBERTNET versus CLIPNET: <i>t</i> = 5.186, <i>P</i> = 3.251 × 10<sup>−7</sup>; CLIPNET versus BERTNET: <i>t</i> = −3.002, <i>P</i> = 0.002; BERTNET versus GPTNET (XL): <i>t</i> = 0.522, <i>P</i> = 0.601; GPTNET (XL) versus GPTNET: <i>t</i> = 2.631, <i>P</i> = 0.009; GPTNET versus BOWNET: <i>t</i> = 4.440, <i>P</i> = 1.134 × 10<sup>−5</sup>; BOWNET versus SIMPLENET: <i>t</i> = 10.255, <i>P</i> = 2.091 × 10<sup>−22</sup>). A full table of pairwise comparisons can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>. <b>f</b>, Average difference in performance between tasks that use standard imperative instructions and those that use instructions with conditional clauses and require a simple deductive reasoning component. Colored asterisks at the bottom of the plot show <i>P</i> values for a two-sided, unequal-variance <i>t</i>-test between the null distribution constructed using random splits of the task set (transparent points represent mean differences for random splits; STRUCTURENET: <i>t</i> = −36.46, <i>P</i> = 4.34 × 10<sup>−23</sup>; SBERTNET (L): <i>t</i> = −16.38, <i>P</i> = 3.02 × 10<sup>−5</sup>; SBERTNET: <i>t</i> = −15.35, <i>P</i> = 3.920 × 10<sup>−5</sup>; CLIPNET: <i>t</i> = −44.68, <i>P</i> = 5.32 × 10<sup>−</sup><sup>13</sup>; BERTNET: <i>t</i> = −25.51, <i>P</i> = 3.14 × 10<sup>−8</sup>; GPTNET (XL): <i>t</i> = −16.99, <i>P</i> = 3.61 × 10<sup>−6</sup>; GPTNET: <i>t</i> = −9.150, <i>P</i> = 0.0002; BOWNET: <i>t</i> = −70.99, <i>P</i> = 4.566 × 10<sup>−35</sup>; SIMPLENET: <i>t</i> = 19.60, <i>P</i> = 5.82 × 10<sup>−6</sup>), and asterisks at the top of plot indicate <i>P</i>-value results from a <i>t</i>-test comparing differences with STRUCTURENET and our other instructed models (versus SBERTNET (L): <i>t</i> = 3.702, <i>P</i> = 0.0168; versus SBERTNET: <i>t</i> = 6.592, <i>P</i> = 0.002; versus CLIPNET: <i>t</i> = 30.35, <i>P</i> = 2.367 × 10<sup>−7</sup>; versus BERTNET: <i>t</i> = 7.234, <i>P</i> = 0.0007; versus GPTNET (XL): <i>t</i> = 5.282, <i>P</i> = 0.004; versus GPTNET: <i>t</i> = −1.745, <i>P</i> = 0.149; versus BOWNET: <i>t</i> = 75.04, <i>P</i> = 9.96 × 10<sup>−11</sup>; versus SIMPLENET: <i>t</i> = −30.95, <i>P</i> = 2.86 × 10<sup>−</sup><sup>6</sup>; see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a> and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">6</a>. for full comparisons).</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Our uninstructed control model SIMPLENET performs at 39%, on average, on the first presentation of a novel task (zero-shot generalization). This serves as a baseline for generalization. Note that despite the orthogonality of task rules provided to SIMPLENET, exposure to the task set allows models to learn patterns that are common to all tasks (for example, always repress response during fixation). Therefore, 39% is not chance-level performance per se, but rather performance achieved by a network trained and tested on a task set with some common requirements for responding. GPTNET, exhibits a zero-shot generalization of 57%. This is a significant improvement over SIMPLENET (<i>t</i> = 8.32, <i>P</i> = 8.24 × 10<sup>−16</sup>). Strikingly, increasing the size of GPT by an order of magnitude to the 1.5 billion parameters used by GPT (XL) only resulted in modest gains over BOWNET (64%), with GPTNET (XL) achieving 68% on held-out tasks (<i>t</i> = 2.04, <i>P</i> = 0.047). By contrast, CLIPNET (S), which uses 4% of the number of parameters utilized by GPTNET (XL), is nonetheless able to achieve the same performance (68% correct, <i>t</i> = 0.146, <i>P</i> = 0.88). Likewise, BERTNET achieves a generalization performance that lags only 2% behind GPTNETXL in the mean (<i>t</i> = −1.122, <i>P</i> = 0.262). By contrast, models with knowledge of sentence-level semantics show marked improvements in generalization, with SBERTNET performing an unseen task at 79% correct on average. Finally, our best-performing model, SBERTNET (L), can execute a never-before-seen task with a performance of 83% correct, on average, lagging just a few percentage points behind STRUCTURENET (88% correct), which receives the structure of the task set hand-coded in its rule vectors.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2b</a> shows a histogram of the number of tasks for which each model achieves a given level of performance. Again, SBERTNET (L) manages to perform over 20 tasks set nearly perfectly in the zero-shot setting (for individual task performance for all models across tasks, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a>).</p><p>To validate that our best-performing models leveraged the semantics of instructions, we presented the sensory input for one held-out task while providing the linguistic instructions for a different held-out task. Models that truly rely on linguistic information should be most penalized by this manipulation and, as predicted, we saw the largest decrease in performance for our best models (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2c</a>).</p><p>We also tested a more stringent hold-out procedure where we purposefully chose 4–6 tasks from the same family of tasks to hold out during training (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2d</a>). Overall, performance decreased in this more difficult setting, although our best-performing models still showed strong generalization, with SBERTNET (L) and SBERTNET achieving 71% and 72% correct on novel tasks, respectively, which was not significantly different from STRUCTURENET at 72% (<i>t</i> = 0.629, <i>P</i> = 0.529; <i>t</i> = 0.064, <i>P</i> = 0.948; for SBERTNET (L) and SBERTNET, respectively).</p><p>In addition, we tested models in a setting where we allow the weights of language models to tune according to the loss experienced during sensorimotor training (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a> for tuning details). This manipulation improved the generalization performance across all models, and for our best-performing model, SBERTNET (L), we see that generalization is as strong as for STRUCTURENET (86%, <i>t</i> = 1.204, <i>P</i> = 0.229).</p><p>Following ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e1086">18</a></sup>, we tested models in a setting where task-type information for a given task was represented as a composition of information for related tasks in the training set (that is, AntiDMMod1 = (rule(AntiDMMod2) − rule(DMMod2)) + rule(DMMod1)). In this setting, we did find that the performance of SIMPLENET improved (60% correct). However, when we combined embedded instructions according to the same compositional rules, our linguistic models dramatically outperformed SIMPLENET. This suggests that training in the context of language more readily allows a simple compositional scheme to successfully configure task responses (see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">5</a> for full results and compositional encodings).</p><p>Finally, we tested a version of each model where outputs of language models are passed through a set of nonlinear layers, as opposed to the linear mapping used in the preceding results. We found that this manipulation reduced performance, suggesting that this added power leads to overfitting on training tasks, and that a simpler linear mapping is better suited to generalization (see <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a> for details and Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a> for full results).</p><p>The discrepancy in performance between our instructed models suggests that in order to represent linguistic information such that it can successfully configure sensorimotor networks, it is not sufficient to simply use any very powerful language processing system. Rather, model success can be delineated by the extent to which they are exposed to sentence-level semantics during pretraining. Our best-performing models SBERTNET (L) and SBERTNET are explicitly trained to produce good sentence embeddings, whereas our worst-performing model, GPTNET, is only tuned to the statistics of upcoming words. Both CLIPNET (S) and BERTNET are exposed to some form of sentence-level knowledge. CLIPNET (S) is interested in sentence-level representations, but trains these representations using the statistics of corresponding vision representations. BERTNET performs a two-way classification of whether or not input sentences are adjacent in the training corpus. That the 1.5 billion parameters of GPTNET (XL) doesn’t markedly improve performance relative to these comparatively small models speaks to the fact that model size isn’t the determining factor. Lastly, although BoW removes key elements of linguistic meaning (that is, syntax), the simple use of word occurrences encodes information primarily about the similarities and differences between the sentences. For instance, simply representing the inclusion or exclusion of the words ‘stronger’ or ‘weaker’ is highly informative about the meaning of the instruction.</p><p>We also investigated which features of language make it difficult for our models to generalize. Thirty of our tasks require processing instructions with a conditional clause structure (for example, COMP1) as opposed to a simple imperative (for example, AntiDM). Tasks that are instructed using conditional clauses also require a simple form of deductive reasoning (if <i>p</i> then <i>q</i> else <i>s</i>). Neuroimaging literature exploring the relationship between such deductive processes and language areas has reached differing conclusions, with some early studies showing that deduction recruits regions that are thought to support syntactic computations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goel, V., Gold, B., Kapur, S. &amp; Houle, S. Neuroanatomical correlates of human reasoning. J. Cogn. Neurosci. 10, 293–302 (1998)." href="#ref-CR24" id="ref-link-section-d167182994e1117">24</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goel, V., Buchel, C., Frith, C. &amp; Dolan, R. J. Dissociation of mechanisms underlying syllogistic reasoning. Neuroimage 12, 504–514 (2000)." href="#ref-CR25" id="ref-link-section-d167182994e1117_1">25</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Reverberi, C. et al. Neural basis of generation of conclusions in elementary deduction. Neuroimage 38, 752–762 (2007)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR26" id="ref-link-section-d167182994e1120">26</a></sup> and follow-up studies claiming that deduction can be reliably dissociated from language areas<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Noveck, I. A., Goel, V. &amp; Smith, K. W. The neural basis of conditional reasoning with arbitrary content. Cortex 40, 613–622 (2004)." href="#ref-CR27" id="ref-link-section-d167182994e1124">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Monti, M. M., Osherson, D. N., Martinez, M. J. &amp; Parsons, L. M. Functional neuroanatomy of deductive inference: a language-independent distributed network. Neuroimage 37, 1005–1016 (2007)." href="#ref-CR28" id="ref-link-section-d167182994e1124_1">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Monti, M. M., Parsons, L. M. &amp; Osherson, D. N. The boundaries of language and thought in deductive inference. Proc. Natl Acad. Sci. USA 106, 12554–12559 (2009)." href="#ref-CR29" id="ref-link-section-d167182994e1124_2">29</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Coetzee, J. P. &amp; Monti, M. M. At the core of reasoning: dissociating deductive and non-deductive load. Hum. Brain Mapp. 39, 1850–1861 (2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR30" id="ref-link-section-d167182994e1127">30</a></sup>. One theory for this variation in results is that baseline tasks used to isolate deductive reasoning in earlier studies used linguistic stimuli that required only superficial processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Monti, M. M. &amp; Osherson, D. N. Logic, language and the brain. Brain Res. 1428, 33–42 (2012)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR31" id="ref-link-section-d167182994e1132">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Prado, J. The relationship between deductive reasoning and the syntax of language in broca’s area: a review of the neuroimaging literature. L’année Psychol. 118, 289–315 (2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR32" id="ref-link-section-d167182994e1135">32</a></sup>.</p><p>To explore this issue, we calculated the average difference in performance between tasks with and without conditional clauses/deductive reasoning requirements (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2f</a>). All our models performed worse on these tasks relative to a set of random shuffles. However, we also saw an additional effect between STRUCTURENET and our instructed models, which performed worse than STRUCTURENET by a statistically significant margin (see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">6</a> for full comparisons). This is a crucial comparison because STRUCTURENET performs deductive tasks without relying on language. Hence, the decrease in performance between STRUCTURENET and instructed models is in part due to the difficulty inherent in parsing syntactically more complicated language. The implication is that we may see engagement of linguistic areas in deductive reasoning tasks, but this may simply be due to the increased syntactic demands of corresponding instructions (rather than processes that recruit linguistic areas to explicitly aid in the deduction). This result largely agrees with two reviews of the deductive reasoning literature, which concluded that the effects in language areas seen in early studies were likely due to the syntactic complexity of test stimuli<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Monti, M. M. &amp; Osherson, D. N. Logic, language and the brain. Brain Res. 1428, 33–42 (2012)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR31" id="ref-link-section-d167182994e1149">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Prado, J. The relationship between deductive reasoning and the syntax of language in broca’s area: a review of the neuroimaging literature. L’année Psychol. 118, 289–315 (2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR32" id="ref-link-section-d167182994e1152">32</a></sup>.</p><h3 id="Sec5">Shared structure in language and sensorimotor networks</h3><p>We then turned to an investigation of the representational scheme that supports generalization. First, we note that like in other multitasking models, units in our sensorimotor-RNNs exhibited functional clustering, where similar subsets of neurons show high variance across similar sets of tasks (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">7</a>). Moreover, we found that models can learn unseen tasks by only training sensorimotor-RNN input weights and keeping the recurrent dynamics constant (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">8</a>). Past work has shown that these properties are characteristic of networks that can reuse the same set of underlying neural resources across different settings<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Driscoll, L., Shenoy, K. &amp; Sussillo, D. Flexible multitask computation in recurrent networks utilizes shared dynamical motifs. Preprint at bioRxiv 
                https://doi.org/10.1101/2022.08.15.503870
                
               (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR6" id="ref-link-section-d167182994e1170">6</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e1173">18</a></sup>. We then examined the geometry that exists between the neural representations of related tasks. We plotted the first three principal components (PCs) of sensorimotor-RNN hidden activity at stimulus onset in SIMPLENET, GPTNETXL, SBERTNET (L) and STRUCTURENET performing modality-specific DM and AntiDM tasks. Here, models receive input for a decision-making task in both modalities but must only attend to the stimuli in the modality relevant for the current task. Importantly, AntiDMMod1 is held out of training in the following examples. In addition, we plotted the PCs of either the rule vectors or the instruction embeddings in each task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3</a>).</p><div data-test="figure" data-container-section="figure" id="figure-3" data-title="Structured representations in instructed models."><figure><figcaption><b id="Fig3" data-test="figure-caption-text">Fig. 3: Structured representations in instructed models.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="602"></picture></a></div><p><b>a</b>–<b>d</b>, The first three PCs of sensorimotor hidden activity and task-info representations for models trained with AntiDMMod1 held out. Solid arrows represent an abstract ‘Pro’ versus ‘Anti’ axis, and dashed arrows represent an abstract ‘Mod1’ versus ‘Mod2’ axis. <b>a</b>, STRUCTURENET. <b>b</b>, SBERTNET (L). <b>c</b>, GPTNET (XL). <b>d</b>, SIMPLENET. <b>e</b>, Correlation between held-out task CCGP and zero-shot performance (Pearson’s <i>r</i> = 0.606, <i>P</i> = 1.57 × 10<sup>−46</sup>). <b>f</b>, CCGP scores for held-out tasks for each layer in the model hierarchy. Significance scores indicate <i>P-</i>value results from pairwise two-sided unequal-variance <i>t</i>-tests performed among model distributions of CCGP scores on held-out tasks for sensorimotor-RNN (NS <i>P</i> &gt; 0.05, *<i>P</i> &lt; 0.05, **<i>P</i> &lt; 0.01, ***<i>P</i> &lt; 0.001; STRUCTURENET versus SBERTNET (L): <i>t</i> = 13.67, <i>P</i> = 2.44 × 10<sup>−36</sup>; SBERTNET (L) versus SBERTNET: <i>t</i> = 5.061, <i>P</i> = 5.84 × 10<sup>−7</sup>; SBERTNET versus CLIPNET: <i>t</i> = 2.809, <i>P</i> = 0.005; CLIPNET versus BERTNET: <i>t</i> = 0.278, <i>P</i> = 0.780; BERTNET versus GPTNET (XL): <i>t</i> = 2.505, <i>P</i> = 0.012; GPTNET (XL) versus GPTNET: <i>t</i> = 3.180, <i>P</i> = 0.001; GPTNET versus BOWNET: <i>t</i> = −4.176, <i>P</i> = 3.50 × 10<sup>−5</sup>; BOWNET versus SIMPLENET: <i>t</i> = 23.0.8, <i>P</i> = 1.10<sup>−80</sup>; see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">9</a> for full comparisons as well as <i>t</i>-test results for embedding layer CCGP scores).</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>For STRUCTURENET, hidden activity is factorized along task-relevant axes, namely a consistent ‘Pro’ versus ‘Anti’ direction in activity space (solid arrows), and a ‘Mod1’ versus ‘Mod2’ direction (dashed arrows). Importantly, this structure is maintained even for AntiDMMod1, which has been held out of training, allowing STRUCTURENET to achieve a performance of 92% correct on this unseen task. This factorization is also reflected in the PCs of rule embeddings. Strikingly, SBERTNET (L) also organizes its representations in a way that captures the essential compositional nature of the task set using only the structure that it has inferred from the semantics of instructions. This is the case for language embeddings, which maintain abstract axes across AntiDMMod1 instructions (again, held out of training). As a result, SBERTNET (L) is able to use these relevant axes for AntiDMMod1 sensorimotor-RNN representations, leading to a generalization performance of 82%. By contrast, GPTNET (XL) fails to properly infer a distinct ‘Pro’ versus ‘Anti’ axes in either sensorimotor-RNN representations or language embeddings leading to a zero-shot performance of 6% on AntiDMMod1 (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3b</a>). Finally, we find that the orthogonal rule vectors used by simpleNet preclude any structure between practiced and held-out tasks, resulting in a performance of 22%.</p><p>To more precisely quantify this structure, we measure the cross-conditional generalization performance (CCGP) of these representations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Bernardi, S. et al. The geometry of abstraction in the hippocampus and prefrontal cortex. Cell 183, 954–967 (2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR3" id="ref-link-section-d167182994e1326">3</a></sup>. CCGP measures the ability of a linear decoder trained to differentiate one set of conditions (that is, DMMod2 and AntiDMMod2) to generalize to an analogous set of test conditions (that is, DMMod1 and AntiDMMod1). Intuitively, this captures the extent to which models have learned to place sensorimotor activity along abstract task axes (that is, the ‘Anti’ dimension). Notably, high CCGP scores and related measures have been observed in experiments that required human participants to flexibly switch between different interrelated tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Minxha, J., Adolphs, R., Fusi, S., Mamelak, A. N. &amp; Rutishauser, U. Flexible recruitment of memory-based choice representations by the human medial frontal cortex. Science 368, eaba3313 (2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR4" id="ref-link-section-d167182994e1330">4</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Ito, T., Yang, G. R., Laurent, P., Schultz, D. H. &amp; Cole, M. W. Constructing neural network models from brain data reveals representational transformations linked to adaptive behavior. Nat. Commun. 13, 673 (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR33" id="ref-link-section-d167182994e1333">33</a></sup>.</p><p>We measured CCGP scores among representations in sensorimotor-RNNs for tasks that have been held out of training (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>) and found a strong correlation between CCGP scores and zero-shot performance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3e</a>). Additionally, we find that swapping task instructions for held-out tasks dramatically reduces CCGP scores for all our instructed models, indicating that the semantic of instructions is crucial for maintaining structured representations (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">9</a>).</p><p>We then looked at how structure emerges in the language processing hierarchy. CCGP decoding scores for different layers in our model are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3f</a>. For each instructed model, scores for 12 transformer layers (or the last 12 layers for SBERTNET (L) and GPTNET (XL)), the 64-dimensional embedding layer and the Sensorimotor-RNN task representations are plotted. We also plotted CCGP scores for the rule embeddings used in our nonlinguistic models. Among models, there was a notable discrepancy in how abstract structure emerges. Autoregressive models (GPTNETXL, GPTNET), BERTNET and CLIPNET (S), showed a low CCGP throughout language model layers followed by a jump in the embedding layer. This is because weights feeding into the embedding layer are tuned during sensorimotor training. The implication of this spike is that most of the useful representational processing in these models actually does not occur in the pretrained language model per se, but rather in the linear readout, which is exposed to task structure via training. By contrast, our best-performing models SBERTNET and SBERTNET (L) use language representations where high CCGP scores emerge gradually in the intermediate layers of their respective language models. Because semantic representations already have such a structure, most of the compositional inference involved in generalization can occur in the comparatively powerful language processing hierarchy. As a result, representations are already well organized in the last layer of language models, and a linear readout in the embedding layer is sufficient for the sensorimotor-RNN to correctly infer the geometry of the task set and generalize well.</p><p>This analysis strongly suggests that models exhibiting generalization do so by leveraging structured semantic representations to properly relate practiced and novel tasks in sensorimotor space, thereby allowing a composition of practiced behaviors in an unseen setting.</p><h3 id="Sec6">Semantic modulation of single-unit tuning properties</h3><p>Next, we examined tuning profiles of individual units in our sensorimotor-RNNs. We found that individual neurons are tuned to a variety of task-relevant variables. Critically, however, we find neurons where this tuning varies predictably within a task group and is modulated by the semantic content of instructions in a way that reflects task demands.</p><p>For instance, in the ‘Go’ family of tasks, unit 42 shows direction selectivity that shifts by <i>π</i> between ‘Pro’ and ‘Anti’ tasks, reflecting the relationship of task demands in each context (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig4">4a</a>). This flip in selectivity is observed even for the AntiGo task, which was held out during training.</p><div data-test="figure" data-container-section="figure" id="figure-4" data-title="Semantic modulation of single-unit tuning properties."><figure><figcaption><b id="Fig4" data-test="figure-caption-text">Fig. 4: Semantic modulation of single-unit tuning properties.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="564"></picture></a></div><p><b>a</b>, Tuning curves for a SBERTNET (L) sensorimotor-RNN unit that modulates tuning according to task demands in the ‘Go’ family. <b>b</b>, Tuning curves, for a SBERTNET (L) sensorimotor-RNN unit in the ‘matching’ family of tasks plotted in terms of difference in angle between two stimuli. <b>c</b>, Full activity traces for modality-specific ‘DM’ and ‘AntiDM’ tasks for different levels of relative stimulus strength. <b>d</b>, Full activity traces for tasks in the ‘comparison’ family of tasks for different levels of relative stimulus strength.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>For the ‘Matching’ family of tasks, unit 14 modulates activity between ‘match’ (DMS, DMC) and ‘non-match’ (DNMS, DNMC) conditions. In ‘non-match’ trials, the activity of this unit increases as the distance between the two stimuli increases. By contrast, for ‘matching’ tasks, this neuron is most active when the relative distance between the two stimuli is small. Hence, in both cases this neuron modulates its activity to represent when the model should respond, changing selectivity to reflect opposing task demands between ‘match’ and ‘non-match’ trials. This is true even for DMS, which has been held out of training.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig4">4c</a> shows traces of unit 3 activity in modality-specific versions of DM and AntiDM tasks (AntiDMMod1 is held out of training) for different levels of contrast (contrast = <i>s</i><i>t</i><i>r</i><sub>stim1</sub> − <i>s</i><i>t</i><i>r</i><sub>stim2</sub>). In all tasks, we observed ramping activity where the rate of ramping is relative to the strength of contrast. This motif of activity has been reported in previous studies<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Shadlen, M. N. &amp; Newsome, W. T. Neural basis of a perceptual decision in the parietal cortex (area lip) of the rhesus monkey. J. Neurophysiol. 86, 1916–1936 (2001)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR34" id="ref-link-section-d167182994e1433">34</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Huk, A. C. &amp; Shadlen, M. N. Neural activity in macaque parietal cortex reflects temporal integration of visual motion signals during perceptual decision making. J. Neurosci. 25, 10420–10436 (2005)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR35" id="ref-link-section-d167182994e1436">35</a></sup>. However, in our models, we observe that an evidence-accumulating neuron can swap the sign of its integration in response to a change in linguistic instructions, which allows models to meet opposing demands of ‘Pro’ and ‘Anti’ versions of the task, even for previously unseen tasks.</p><p>Interestingly, we also found that unsuccessful models failed to properly modulate tuning preferences. For example, with GPTNET (XL), which failed to factorize along a ‘Pro’ versus ‘Anti’ axis (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3b</a>) and had poor generalization on AntiDMMod1, we also find neurons that failed to swap their sign of integration in the held-out setting (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">10</a>).</p><p>Finally, we see a similar pattern in the time course of activity for trials in the ‘Comparison’ family of tasks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig4">4d</a>). In the COMP1 task, the network must respond in the direction of the first stimulus if it has higher intensity than the second stimulus, and must not respond otherwise. In COMP2, it must only respond to the second stimulus if the second stimulus is higher intensity. For ‘Anti’ versions, the demands of stimulus ordering are the same except the model has to choose the stimuli with the weakest contrast. Even with this added complexity, we found individual neurons that modulate their tuning with respect to task demands, even for held-out tasks (in this case COMP2). For example, unit 82 is active when the network should repress response. For ‘COMP1’, this unit is highly active with negative contrast (that is, <i>s</i><i>t</i><i>r</i><sub>stim2</sub> &gt; <i>s</i><i>t</i><i>r</i><sub>stim1</sub>), but flips this sensitivity in COMP2 and is highly active with positive contrast (that is, <i>s</i><i>t</i><i>r</i><sub>stim1</sub> &gt; <i>s</i><i>t</i><i>r</i><sub>stim2</sub>). Importantly, this relation is reversed when the goal is to select the weakest stimuli. Hence, despite these subtle syntactic differences in instruction sets, the language embedding can reverse the tuning of this unit in a task-appropriate manner.</p><h3 id="Sec7">Linguistic communication between networks</h3><p>We now seek to model the complementary human ability to describe a particular sensorimotor skill with words once it has been acquired. To do this, we inverted the language-to-sensorimotor mapping our models learn during training so that they can provide a linguistic description of a task based only on the state of sensorimotor units. First, we constructed an output channel (production-RNN; Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5a–c</a>), which is trained to map sensorimotor-RNN states to input instructions. We then present the network with a series of example trials while withholding instructions for a specific task. During this phase all model weights are frozen, and models receive motor feedback in order to update the embedding layer activity in order to reduce the error of the output (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5b</a>). Once the activity in the embedding layer drives sensorimotor units to achieve a performance criterion, we used the production-RNN to decode a linguistic description of the current task. Finally, to evaluate the quality of these instructions, we input them into a partner model and measure performance across tasks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5c</a>). All instructing and partner models used in this section are instances of SBERTNET (L) (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>).</p><div data-test="figure" data-container-section="figure" id="figure-5" data-title="Communication between networks."><figure><figcaption><b id="Fig5" data-test="figure-caption-text">Fig. 5: Communication between networks.</b></figcaption><div><div><a data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41593-024-01607-5/MediaObjects/41593_2024_1607_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="735"></picture></a></div><p><b>a</b>, Illustration of self-supervised training procedure for the language production network (blue). The red dashed line indicates gradient flow. <b>b</b>, Illustration of motor feedback used to drive task performance in the absence of linguistic instructions. <b>c</b>, Illustration of the partner model evaluation procedure used to evaluate the quality of instructions generated from the instructing model. <b>d</b>, Three example instructions produced from sensorimotor activity evoked by embeddings inferred in <b>b</b> for an AntiDMMod1 task. <b>e</b>, Confusion matrix of instructions produced again using the method described in <b>b</b>. <i>y</i> axis indicates input–output task used to infer an embedding, and <i>x</i> axis indicates whether the instruction produced from the resulting sensorimotor activity was included in one of the instruction sets used during self-supervised training or else was a ‘novel’ formulation. <b>f</b>, Performance of partner models in different training regimes given produced instructions or direct input of embedding vectors. Each point represents the average performance of a partner model across tasks using instructions from decoders train with different random initializations. Dots indicate the partner model was trained on all tasks, whereas diamonds indicate performance on held-out tasks. Axes indicate the training regime of the instructing model. Full statistical comparisons of performance can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">12</a>.</p></div><p><a data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="https://www.nature.com/articles/s41593-024-01607-5/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span></a></p></figure></div><p>Some example decoded instructions for the AntiDMMod1 task (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5d</a>; see Supplementary Notes <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a> for all decoded instructions). To visualize decoded instructions across the task set, we plotted a confusion matrix where both sensorimotor-RNN and production-RNN are trained on all tasks (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5e</a>). Note that many decoded instructions were entirely ‘novel’, that is, they were not included in the training set for the production-RNN (<a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec9">Methods</a>). Novel instructions made up 53% of decoded instructions across all tasks.</p><p>To test the quality of these novel instructions, we evaluated a partner model’s performance on instructions generated by the first network (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5c</a>; results are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5f</a>). When the partner model is trained on all tasks, performance on all decoded instructions was 93% on average across tasks. Communicating instructions to partner models with tasks held out of training also resulted in good performance (78%). Importantly, performance was maintained even for ‘novel’ instructions, where average performance was 88% for partner models trained on all tasks and 75% for partner models with hold-out tasks. Given that the instructing and partner models share the same architecture, one might expect that it is more efficient to forgo the language component of communication and simply copy the embedding inferred by one model into the input of the partner model. This resulted in only 31% correct performance on average and 28% performance when testing partner models on held-out tasks. Although both instructing and partner networks share the same architecture and the same competencies, they nonetheless have different synaptic weights. Hence, using a neural representation tuned for the set of weights within the one agent won’t necessarily produce good performance in the other.</p><p>We also tested an instructing model using a sensorimotor-RNN with tasks held out of training. We emphasize here that during training the production-RNN attempts to decode from sensorimotor hidden states induced by instructions for tasks the network has never experienced before (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5a</a>), whereas during test time, instructions are produced from sensorimotor states that emerge entirely as a result of minimizing a motor error (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5b,c</a>). We nonetheless find that, in this setting, a partner model trained on all tasks performs at 82% correct, while partner models with tasks held out of training perform at 73%. Here, 77% of produced instructions are novel, so we see a very small decrease of 1% when we test the same partner models only on novel instructions. Like above, context representations induce a relatively low performance of 30% and 37% correct for partners trained on all tasks and with tasks held out, respectively.</p><p>Lastly, we tested our most extreme setting where tasks have been held out for both sensorimotor-RNNs and production-RNNs (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5f</a>). We find that produced instructions induce a performance of 71% and 63% for partner models trained on all tasks and with tasks held out, respectively. Although this is a decrease in performance from our previous set-ups, the fact that models can produce sensible instructions at all in this double held-out setting is striking. The fact that the system succeeds to any extent speaks to strong inductive biases introduced by training in the context of rich, compositionally structured semantic representations.</p></div></div><div id="Sec8-section" data-title="Discussion"><h2 id="Sec8">Discussion</h2><div id="Sec8-content"><p>In this study, we use the latest advances in natural language processing to build tractable models of the ability to interpret instructions to guide actions in novel settings and the ability to produce a description of a task once it has been learned. RNNs can learn to perform a set of psychophysical tasks simultaneously using a pretrained language transformer to embed a natural language instruction for the current task. Our best-performing models can leverage these embeddings to perform a brand-new model with an average performance of 83% correct. Instructed models that generalize performance do so by leveraging the shared compositional structure of instruction embeddings and task representations, such that an inference about the relations between practiced and novel instructions leads to a good inference about what sensorimotor transformation is required for the unseen task. Finally, we show a network can invert this information and provide a linguistic description for a task based only on the sensorimotor contingency it observes.</p><p>Our models make several predictions for what neural representations to expect in brain areas that integrate linguistic information in order to exert control over sensorimotor areas. Firstly, the CCGP analysis of our model hierarchy suggests that when humans must generalize across (or switch between) a set of related tasks based on instructions, the neural geometry observed among sensorimotor mappings should also be present in semantic representations of instructions. This prediction is well grounded in the existing experimental literature where multiple studies have observed the type of abstract structure we find in our sensorimotor-RNNs also exists in sensorimotor areas of biological brains<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Bernardi, S. et al. The geometry of abstraction in the hippocampus and prefrontal cortex. Cell 183, 954–967 (2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR3" id="ref-link-section-d167182994e1615">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Panichello, M. F. &amp; Buschman, T. J. Shared mechanisms underlie the control of working memory and attention. Nature 592, 601–605 (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR36" id="ref-link-section-d167182994e1618">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Nieh, E. H. et al. Geometry of abstract learned knowledge in the hippocampus. Nature 595, 80–84 (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR37" id="ref-link-section-d167182994e1621">37</a></sup>. Our models theorize that the emergence of an equivalent task-related structure in language areas is essential to instructed action in humans. One intriguing candidate for an area that may support such representations is the language selective subregion of the left inferior frontal gyrus. This area is sensitive to both lexico-semantic and syntactic aspects of sentence comprehension, is implicated in tasks that require semantic control and lies anatomically adjacent to another functional subregion of the left inferior frontal gyrus, which is implicated in flexible cognition<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Fedorenko, E. &amp; Blank, I. A. Broca’s area is not a natural kind. Trends Cogn. Sci. 24, 270–284 (2020)." href="#ref-CR38" id="ref-link-section-d167182994e1625">38</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Fedorenko, E., Duncan, J. &amp; Kanwisher, N. Language-selective and domain-general regions lie side by side within broca’s area. Curr. Biol. 22, 2059–2062 (2012)." href="#ref-CR39" id="ref-link-section-d167182994e1625_1">39</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Gao, Z. et al. Distinct and common neural coding of semantic and non-semantic control demands. NeuroImage 236, 118230 (2021)." href="#ref-CR40" id="ref-link-section-d167182994e1625_2">40</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Duncan, J. The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour. Trends Cogn. Sci. 14, 172–179 (2010)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR41" id="ref-link-section-d167182994e1628">41</a></sup>. We also predict that individual units involved in implementing sensorimotor mappings should modulate their tuning properties on a trial-by-trial basis according to the semantics of the input instructions, and that failure to modulate tuning in the expected way should lead to poor generalization. This prediction may be especially useful to interpret multiunit recordings in humans. Finally, given that grounding linguistic knowledge in the sensorimotor demands of the task set improved performance across models (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2e</a>), we predict that during learning the highest level of the language processing hierarchy should likewise be shaped by the embodied processes that accompany linguistic inputs, for example, motor planning or affordance evaluation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Buccino, G., Colagé, I., Gobbi, N. &amp; Bonaccorso, G. Grounding meaning in experience: a broad perspective on embodied language. Neurosci. Biobehav. Rev. 69, 69–78 (2016)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR42" id="ref-link-section-d167182994e1635">42</a></sup>.</p><p>One notable negative result of our study is the relatively poor generalization performance of GPTNET (XL), which used at least an order of magnitude more parameters than other models. This is particularly striking given that activity in these models is predictive of many behavioral and neural signatures of human language processing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Schrimpf, M. et al. The neural architecture of language: integrative modeling converges on predictive processing. Proc. Natl Acad. Sci. USA 
                https://doi.org/10.1073/pnas.2105646118
                
               (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR10" id="ref-link-section-d167182994e1642">10</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Goldstein, A. et al. Shared computational principles for language processing in humans and deep language models. Nature Neurosci. 25, 369–380 (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR11" id="ref-link-section-d167182994e1645">11</a></sup>. Given this, future imaging studies may be guided by the representations in both autoregressive models and our best-performing models to delineate a full gradient of brain areas involved in each stage of instruction following, from low-level next-word prediction to higher-level structured-sentence representations to the sensorimotor control that language informs.</p><p>Our models may guide future work comparing compositional representations in nonlinguistic subjects like nonhuman primates. Comparison of task switching (without linguistic instructions) between humans and nonhuman primates indicates that both use abstract rule representations, although humans can make switches much more rapidly<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Mansouri, F. A., Freedman, D. J. &amp; Buckley, M. J. Emergence of abstract rules in the primate brain. Nat. Rev. Neurosci. 21, 595–610 (2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR43" id="ref-link-section-d167182994e1652">43</a></sup>. One intriguing parallel in our analyses is the use of compositional rules vectors (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">5</a>). Even in the case of nonlinguistic SIMPLENET, using these vectors boosted generalization. Importantly, however, this compositionality is much stronger for our best-performing instructed models. This suggests that language endows agents with a more flexible organization of task subcomponents, which can be recombined in a broader variety of contexts.</p><p>Our results also highlight the advantages of linguistic communication. Networks can compress the information they have gained through experience of motor feedback and transfer that knowledge to a partner network via natural language. Although rudimentary in our example, the ability to endogenously produce a description of how to accomplish a task after a period of practice is a hallmark human language skill. The failure to transfer performance by sharing latent representations demonstrates that to communicate information in a group of independent networks of neurons, it needs to pass through a representational medium that is equally interpretable by all members of the group. In humans and for our best-performing instructed models, this medium is language.</p><p>A series of works in reinforcement learning has investigated using language and language-like schemes to aid agent performance. Agents receive language information through step-by-step descriptions of action sequences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Oh, J. Singh, S., Lee, H. &amp; Kohli, P. Zero-shot task generalization with multi-task deep reinforcement learning. In Proc. 34th International Conference on Machine Learning 2661–2670 (JMLR.org, 2017)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR44" id="ref-link-section-d167182994e1666">44</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Chaplot, D. S., Mysore Sathyendra, K., Pasumarthi, R. K., Rajagopal, D., &amp; Salakhutdinov, R. Gated-attention architectures for task-oriented language grounding. In Proc. 32nd AAAI Conference on Artificial Intelligence Vol. 32 (AAAI Press, 2018)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR45" id="ref-link-section-d167182994e1669">45</a></sup>, or by learning policies conditioned on a language goal<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Sharma, P., Torralba, A. &amp; Andreas, J. Skill induction and planning with latent language. Preprint at 
                https://arxiv.org/abs/2110.01517
                
              (2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR46" id="ref-link-section-d167182994e1673">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Jiang, Y., Gu, S., Murphy, K. &amp; Finn, C. Language as an abstraction for hierarchical deep reinforcement learning. In Proc. 33rd International Conference on Neural Information Processing Systems 9419–943132 (Curran Associates Inc., 2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR47" id="ref-link-section-d167182994e1676">47</a></sup>. These studies often deviate from natural language and receive linguistic inputs that are parsed or simply refer directly to environmental objects. Some larger versions of the pretrained language models we use to embed instructions also display instructions following behavior, that is, GPT-3 (ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Brown, Tom, et al. Language models are few-shot learners. In Proc. 34th International Conference on Neural Information Processing Systems 1877–1901 (Curran Associates Inc., 2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR7" id="ref-link-section-d167182994e1680">7</a></sup>), PALM<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Chowdhery, A. et al. Palm: scaling language modeling with pathways. J. Mach. Learn. Res. 24, 11324–11436 (2023)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR12" id="ref-link-section-d167182994e1684">12</a></sup>, LaMDA<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Thoppilan, R. et al. Lamda: language models for dialog applications. Preprint at 
                https://arxiv.org/abs/2201.08239
                
               (2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR13" id="ref-link-section-d167182994e1688">13</a></sup> and InstructGPT<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Ouyang, L. et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 27730–27744 (Curran Associates, Inc., 2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR48" id="ref-link-section-d167182994e1693">48</a></sup> in the modality of language and DALL-E<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Ramesh, A. et al. Zero-shot text-to-image generation. In Proc. 38th International Conference on Machine Learning (eds Marina, M. &amp; Tong, Z.) 8821–8831 (PMLR, 2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR8" id="ref-link-section-d167182994e1697">8</a></sup> and Stable Diffusion<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Rombach, R. et al. High-resolution image synthesis with latent diffusion models. In Proc. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 10674–10685 (IEEE, 2022)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR14" id="ref-link-section-d167182994e1701">14</a></sup> in a language to image modality. The semantic and syntactic understanding displayed in these models is impressive. However, the outputs of these models are difficult to interpret in terms of guiding the dynamics of a downstream action plan. Finally, recent work has sought to engineer instruction following agents that can function in complex or even real-world environments<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Abramson, J. et al. Imitating interactive intelligence. Preprint at 
                https://arxiv.org/abs/2012.05672
                
               (2021)." href="#ref-CR16" id="ref-link-section-d167182994e1705">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="DeepMind Interactive Agents Team. Creating multimodal interactive agents with imitation and self-supervised learning. Preprint at 
                https://arxiv.org/abs/2112.03763
                
               (2022)." href="#ref-CR17" id="ref-link-section-d167182994e1705_1">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e1708">18</a></sup>. While these models exhibit impressive behavioral repertoires, they rely on perceptual systems that fuse linguistic and visual information making them difficult to compare to language representations in human brains, which emerge from a set of areas specialized for processing language. In all, none of these models offer a testable representational account of how language might be used to induce generalization over sensorimotor mappings in the brain.</p><p>Our models by contrast make tractable predictions for what population and single-unit neural representations are required to support compositional generalization and can guide future experimental work examining the interplay of linguistic and sensorimotor skills in humans. By developing interpretable models that can both understand instructions as guiding a particular sensorimotor response, and communicate the results of sensorimotor learning as an intelligible linguistic instruction, we have begun to explain the power of language in encoding and transferring knowledge in networks of neurons.</p></div></div><div id="Sec9-section" data-title="Methods"><h2 id="Sec9">Methods</h2><div id="Sec9-content"><h3 id="Sec10">Model architecture</h3><h4 id="Sec11">Sensorimotor-RNN</h4><p>The base model architecture and task structure used in this paper follows<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e1731">18</a></sup>. All networks of sensorimotor units denoted sensorimotor-RNN are gated recurrent units (GRU)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Chung, J., Gulcehre, C., Cho, K. &amp; Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. Preprint at 
                https://arxiv.org/abs/1412.3555
                
               (2014)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR49" id="ref-link-section-d167182994e1735">49</a></sup> using rectified linear unit (ReLU) nonlinearities with 256 hidden units each. Inputs to the networks consist of (1) sensory inputs, <i>X</i><sub><i>t</i></sub> and (2) task-identifying information, <i>I</i><sub><i>t</i></sub>. We initialize hidden activity in the GRU as <span>\({h}^{0}\in {{\mathbb{R}}}^{256}\)</span> with values set to 0.1. All networks of sensorimotor units use the same hidden state initialization, so we omit <i>h</i><sup>0</sup> in network equations. At each time step, a readout layer Linear<sub>out</sub> decodes motor activity, <span>\(\hat{{y}_{t}}\)</span>, from the activity of recurrent hidden units, <i>h</i><sub><i>t</i></sub>, according to:</p><div id="Equa"><p><span>$$\begin{array}{ll}{h}_{t}={{{\rm{SensorimotorRNN}}}}\Big({X}_{t},{I}_{t};{h}_{t-1}\Big)\qquad\qquad{h}_{t}\in {{\mathbb{R}}}^{256}\\ {\hat{y}}_{t}=\sigma \Big({{{{\rm{Linear}}}}}_{{{{\rm{out}}}}}({h}_{t})\Big)\qquad\qquad\qquad\qquad\qquad\quad{\hat{y}}_{t}\in {{\mathbb{R}}}^{33}\end{array}$$</span></p></div><p>where <i>σ</i> denotes the sigmoid function. Sensory inputs <i>X</i><sub><i>t</i></sub> are made up of three channels, two sensory modalities <span>\({x}_{{{\mathrm{mod}}}\,1,t}\)</span> and <span>\({x}_{{{\mathrm{mod}}}\,2,t}\)</span>, and a fixation channel <i>x</i><sub>fix,<i>t</i></sub>. Both <span>\({x}_{{{\mathrm{mod}}}\,1,t},{x}_{{{\mathrm{mod}}}\,2,t}\in {{\mathbb{R}}}^{32}\)</span> and stimuli in these modalities are represented as hills of activity with peaks determined by units’ preferred directions around a one-dimensional circular variable. For an input at direction <i>θ</i>, the activity of a given input unit <i>u</i><sub><i>i</i></sub> with preferred direction <i>θ</i><sub><i>i</i></sub> is</p><div id="Equb"><p><span>$${u}_{i}=str \times 0.8\exp \left[-0.5 \times {\left(\frac{8| \theta -{\theta }_{i}| }{\pi }\right)}^{2}\right]$$</span></p></div><p>where <i>s</i><i>t</i><i>r</i> is the coefficient describing stimulus strength. The fixation channel <span>\({x}_{{{{\rm{fix}}}},t}\in {{\mathbb{R}}}^{1}\)</span> is a single unit simulating a fixation cue for the network. In all, sensory input <span>\({X}_{t}=({x}_{mod1,t},{x}_{mod2,t},{x}_{fix,t})\in {{\mathbb{R}}}^{65}\)</span>. Motor output, <span>\({\hat{{y}}_{t}}\)</span> consists of both a 32-dimensional ring representing directional responses to the input stimulus as well as a single unit representing model fixation, so that <span>\({\hat{{y}}_{t}}\in {{\mathbb{R}}}^{33}\)</span>.</p><p>For all models, task-identifying information <span>\({I}_{t}\in {{\mathbb{R}}}^{64}\)</span>. Task-identifying information is presented throughout the duration of a trial and remains constant such that <span>\({I}_{t}={I}_{t{\prime} }\forall t,t{\prime}\)</span>. For all models, task-identifying info <i>I</i><sub><i>t</i></sub> and sensory input <i>X</i><sub><i>t</i></sub> are concatenated as inputs to the sensorimotor-RNN.</p><h4 id="Sec12">Nonlinguistic models</h4><p>For SIMPLENET, we generate a set of 64-dimensional orthogonal task rules by constructing an orthogonal matrix using the Python package scipy.stats.ortho_group, and assign rows of this matrix to each task type. For STRUCTURENET, we generate a set of ten orthogonal, 64-dimensional vectors in the same manner, and each of these represents a dimension of the task set (that is, respond weakest versus strongest direction, respond in the same versus opposite direction, pay attention only to stimuli in the first modality, and so on). Rule vectors for tasks are then simple combinations of each of these ten basis vectors. For a full description of structure rule vectors, see Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">3</a>.</p><p>We also test SIMPLENETPLUS and STRUCTURENETPLUS, which use an additional hidden layer with 128 units and ReLU nonlinearities to process orthogonal tasks rules <i>I</i><sub><i>t</i></sub> into a vector <span>\(\bar{{I}_{t}}\)</span> which is used by sensorimotor-RNN as task-identifying information.</p><div id="Equc"><p><span>$$\begin{array}{ll}{\bar{{I}_{t}}}^{{\prime} }=\rm{ReLU}({{{{\rm{Linear}}}}}_{{{{\rm{RuleEmb}}}}1}({I}_{t}))&amp;{\bar{{I}_{t}}}^{{\prime} }\in {{\mathbb{R}}}^{128}\\ {\bar{{I}_{t}}}^{{\prime} }=\rm{ReLU}({{{{\rm{Linear}}}}}_{{{{\rm{RuleEmb}}}}2}({I}_{t}^{{\prime} }))&amp;{\bar{{I}_{t}}}^{{\prime} }\in {{\mathbb{R}}}^{128}\\ \bar{{I}_{t}}=\rm{ReLU}({{{{\rm{Linear}}}}}_{{{{\rm{RuleEmb}}}}3}({\bar{{I}_{t}}}^{{\prime} }))&amp;\bar{{I}_{t}}\in {{\mathbb{R}}}^{64}\end{array}$$</span></p></div><p>Full results for these models are included in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4</a>.</p><h4 id="Sec13">Pretrained transformers</h4><p>The main language models we test use pretrained transformer architectures to produce <i>I</i>. Importantly, transformers differ in the type of pretraining objective used to tune the model parameters. GPT is trained to predict the next word given a context of words<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Radford, A. et al. Language models are unsupervised multitask learners. OpenAI 1, 9 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR9" id="ref-link-section-d167182994e3362">9</a></sup>. GPT (XL) follows the same objective but trains for longer on a larger dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Radford, A. et al. Better language models and their implications. 
                https://openai.com/blog/better-language-models/
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR50" id="ref-link-section-d167182994e3366">50</a></sup>. Both models are fully autoregressive. BERT, by contrast, takes bidirectional language inputs and is tasked with predicting masked words that appear in the middle of input phrases. Additionally, BERT is trained on a simple sentence prediction task where the model must determine if input sentence 1 is followed by input sentence 2 in the training corpus. Extending this principle, SBERT is explicitly trained to produce fixed-length embeddings of whole sentences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Reimers, N. &amp; Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint at 
                https://arxiv.org/abs/1908.10084
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR21" id="ref-link-section-d167182994e3370">21</a></sup>. It takes pretrained BERT networks and uses them in a siamese architecture<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Bromley, J. et al. Signature verification using a ‘siamese’ time delay neural network. Int. J. Pattern Recognit. Artif. Intell. 7, 669–688 (1993)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR51" id="ref-link-section-d167182994e3374">51</a></sup>, which allows the weights of the model to be tuned in a supervised fashion according to the Stanford Natural Language Inference dataset<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Bowman, S. R., Angeli, G., Potts, C. &amp; Manning, C. D. A large annotated corpus for learning natural language inference. Preprint at 
                http://arxiv.org/abs/1508.05326
                
               (2015)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR22" id="ref-link-section-d167182994e3379">22</a></sup>. Natural language inference is a three-way categorization task where the network must infer the logical relationship between sentences: whether a premise sentence implies, contradicts or is unrelated to a hypothesis sentence. Finally, CLIP is trained to jointly embed images and language<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Radford, A. et al. &quot;Learning transferable visual models from natural language supervision. In Proc. 38th International Conference on Machine Learning (eds Marina, M. &amp; Tong, Z.) 8748–8763 (PMLR, 2021)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR23" id="ref-link-section-d167182994e3383">23</a></sup>. It uses data from captioned images and is asked to properly categorize which text and images pairs match or are mismatched in the dataset via a contrastive loss.</p><p>Importantly, the natural output of a transformer is a matrix of size <span>\({\dim }_{{{{\rm{trans}}}}.}\times {{{\mathcal{T}}}}\)</span>, the inherent dimensionality of the transformer by the length of the input sequence. To create an embedding space for sentences it is standard practice to apply a pooling method to the transformer output, which produces a fixed-length representation for each instruction.</p><p>For GPT, GPT (XL), BERT and SBERT, we use an average pooling method. Suppose we have an input instruction <span>\({w}_{1}\ldots {w}_{{{{\mathcal{T}}}}}\)</span>. Following standard practice with pretrained language models, the input to our transformers is tokenized with special ‘cls’ and ‘eos’ tokens at the beginning and end of the input sequence. We then compute <i>I</i> as follows:</p><div id="Equd"><p><span>$$\begin{array}{ll}{h}^{\rm{tran.}}={{{\rm{transformer}}}}\Big({{\mbox{[cls]}}}\,,{w}_{1}\ldots {w}_{{{{\mathcal{T}}}}},\,{{\mbox{[eos]}}}\Big),\qquad{h}^{\rm{tran.}}\in {{\mathbb{R}}}^{{\dim }_{{{{\rm{trans}}}}.}\times {{{\mathcal{T}}}}+2}\\ {h}^{I}={{{\rm{mean}}}}({h}^{\rm{tran.}}),\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad{h}^{I}\in {{\mathbb{R}}}^{{\dim }_{{{{{\rm{trans}}}}}.}}\\ I={{{{\rm{Linear}}}}}_{{{{\rm{embed}}}}}({h}^{I})\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad I\in {{\mathbb{R}}}^{64}\end{array}$$</span></p></div><p>We chose this average pooling method primarily because a previous study<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Reimers, N. &amp; Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint at 
                https://arxiv.org/abs/1908.10084
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR21" id="ref-link-section-d167182994e3845">21</a></sup> found that this resulted in the highest-performing SBERT embeddings. Another alternative would be to simply use the final hidden representation of the ‘cls’ token as a summary of the information in the entire sequence (given that BERT architectures are bidirectional, this token will have access to the whole sequence).</p><div id="Eque"><p><span>$$\begin{array}{ll}{h}^{\rm{tran.}}={{{\rm{transformer}}}}\Big(\,{{\mbox{[cls]}}}\,,{w}_{1}\ldots {w}_{{{{\mathcal{T}}}}},\,{{\mbox{[eos]}}}\,\Big),\qquad{h}^{\rm{tran.}}\in {{\mathbb{R}}}^{{\dim }_{{{{{\rm{trans}}}}}.}\times {{\,{\mathcal{T}}}}+2}\\ {h}^{I}=({h}_{{{{\rm{cls}}}}}^{\rm{tran.}})\qquad\qquad\qquad\qquad\qquad\qquad\quad\qquad\quad\;\;{h}^{I}\in {{\mathbb{R}}}^{{\dim }_{{{{{\rm{trans}}}}}.}}\end{array}$$</span></p></div><p>Where <span>\({h}_{{{{\rm{cls}}}}}^{\rm{tran.}}\)</span> denote the last hidden representation for the ‘cls’ token. Ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Reimers, N. &amp; Gurevych, I. Sentence-bert: Sentence embeddings using siamese bert-networks. Preprint at 
                https://arxiv.org/abs/1908.10084
                
               (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR21" id="ref-link-section-d167182994e4176">21</a></sup> found this pooling method performed worse than average pooling, so we don’t include these alternatives in our results. For GPT and GPT (XL), we also tested a pooling method where the fixed-length representation for a sequence was taken from the transformer output of the ‘eos’ token. In this case:</p><div id="Equf"><p><span>$$\begin{array}{ll}{h}^{\rm{tran.}}={{{\rm{transformer}}}}\Big(\,{{\mbox{[cls]}}}\,,{w}_{1}\ldots {w}_{{{{\mathcal{T}}}}},\,{{\mbox{[eos]}}}\,\Big),\qquad{h}^{\rm{tran.}}\in {{\mathbb{R}}}^{{\dim }_{{{{\rm{trans}}}}.}\times {{\;{\mathcal{T}}}}+2}\\ {h}^{I}=({h}_{{{{\rm{eos}}}}}^{\rm{tran.}}),\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\quad{h}^{I}\in {{\mathbb{R}}}^{{\dim }_{{{{\rm{trans}}}}.}}\\ I={{{{\rm{Linear}}}}}_{{{{\rm{embed}}}}}({h}^{I}),\qquad\qquad\qquad\qquad\qquad\qquad\quad I\in {{\mathbb{R}}}^{64}\end{array}$$</span></p></div><p>We found that GPT failed to achieve even a relaxed performance criterion of 85% across tasks using this pooling method, and GPT (XL) performed worse than with average pooling, so we omitted these models from the main results (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">11</a>). For CLIP models we use the same pooling method as in the original multiModal training procedure, which takes the outputs of the [cls] token as described above.</p><p>For all the above models, we also tested a version where the information from the pretrained transformers is passed through a multilayer perceptron with a single hidden layer of 256 hidden units and ReLU nonlinearities. We found that this manipulation reduced performance across all models, verifying that a simple linear embedding is beneficial to generalization performance.</p><p>For GPT, BERT and SBERT, <span>\({\dim }_{{{{\rm{trans}}}}.}=768\)</span> and each model uses a total of ~100 million parameters; for SBERT (L) <span>\({\dim }_{{{{\rm{trans}}}}.}=1,024\)</span> and the model uses ~300 million parameters; GPT (XL) <span>\({\dim }_{{{{\rm{trans}}}}.}=1,600\)</span> and the model uses ~1.5 billion parameters; for CLIP, <span>\({\dim }_{{{{\rm{trans}}}}.}=512\)</span> and the model uses ~60 million parameters. Full PyTorch implementations, including all pretrained weights and model hyperparameters, can be accessed at the Huggingface library (<a href="https://huggingface.co/docs/transformers/">https://huggingface.co/docs/transformers/</a>)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Wolf, T. et al. Transformers: state-of-the-art natural language processing. In Proc. 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (eds Liu, Q. &amp; Schlangen, D.) 38–45 (Association for Computational Linguistics, 2020)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR52" id="ref-link-section-d167182994e4731">52</a></sup>.</p><h4 id="Sec14">BoW model</h4><p>For our BoW model, instructions are represented as a vector of binary activations the size of the instruction vocabulary, where each unit indicates the inclusion or exclusion of the associated word in the current instruction. For our instruction set, <span>∣</span>vocab<span>∣</span> = 181. This vector is then projected through a linear layer into 64-dimensional space.</p><div id="Equg"><p><span>$$\begin{array}{ll}{h}_{i}^{{{{\rm{BoW}}}}}=\left\{\begin{array}{ll}1\quad\,{{\mbox{if}}}\,\,{w}_{i}\in ({w}_{1}\ldots {w}_{{{{\mathcal{T}}}}})\\ 0\quad\,{{\mbox{otherwise}}}\,\end{array}\right.\qquad\qquad{h}^{{{{\rm{BoW}}}}}\in {{\mathbb{R}}}^{| \rm{vocab}| }\\ I={{{{\rm{Linear}}}}}_{{{{\rm{embed}}}}}({h}^{{{{\rm{BoW}}}}}),\qquad\qquad\qquad\qquad\qquad\quad I\in {{\mathbb{R}}}^{64}\end{array}$$</span></p></div><h4 id="Sec15">Blank slate language models</h4><p>Given that tuning the last layers of language models resulted in improved performance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2e</a>), we tested two additional models to determine if training a blank slate language model trained exclusively on the loss from sensorimotor tasks would improve performance. These models consist of passing BoW representations through a multilayer perceptron and passing pretrained BERT word embeddings through one layer of a randomly initialized BERT encoder. Both models performed poorly compared to pretrained models (Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">4.5</a>), confirming that language pretraining is essential to generalization.</p><h3 id="Sec16">Tasks sets</h3><p>Tasks were divided into five interrelated subgroups: ‘go’, ‘decision-making’, ‘matching’, and ‘comparison’ and ‘duration’. Depending on the task, multiple stimuli may appear during the stimulus epoch. Also, depending on the task, models may be required to respond in a particular direction or repress response altogether. Unless otherwise specified, zero-mean Gaussian noise is added independently at each time step and to each input unit and the variance of this noise is drawn randomly from <span>\({\mathbb{U}}[0.1,0.15]\)</span>. The timing of stimuli differs among the tasks type. However, for all tasks, trials can be divided into preparatory, stimulus and response epochs. The stimulus epoch can be subdivided into three parts—stim1, delay and stim23—although these distinct parts aren’t used by all tasks. A trial lasts for a total of <i>T</i> = 150 time steps. Let <i>d</i><i>u</i><i>r</i><sub>epoch</sub> denote the duration in simulated time steps of a given epoch. Then</p><div id="Equh"><p><span>$$\begin{array}{rcl}&amp;&amp;du{r}_{{{{\rm{response}}}}} \sim\Big\{i| 20 &lt; i\le 25;i\in {\mathbb{N}}\Big\}\\ &amp;&amp;du{r}_{{{{\rm{stim}}}}1},du{r}_{{{{\rm{stim}}}}2} \sim\Big\{i| 37 &lt; i\le 50;i\in {\mathbb{N}}\Big\}\\ &amp;&amp;du{r}_{{{{\rm{delay}}}}} \sim\Big\{i| 15 &lt; i\le 25;i\in {\mathbb{N}}\Big\}\\ &amp;&amp;du{r}_{{{{\rm{prep}}}}.}=150-\Big(du{r}_{{{{\rm{response}}}}}+du{r}_{{{{\rm{stim}}}}1}+du{r}_{{{{\rm{stim}}}}2}+du{r}_{{{{\rm{delay}}}}}\Big)\end{array}$$</span></p></div><p>For tasks that don’t utilize a delay structure, stim1, stim2 and delay epochs are grouped together in a single stimulus epoch where <span>\(du{r}_{{{{\rm{stimulus}}}}}=du{r}_{{{{\rm{stim}}}}1}+du{r}_{{{{\rm{stim}}}}2}+du{r}_{{{{\rm{delay}}}}}\)</span>. Unless otherwise specified, a fixation cue with a constant strength <i>s</i><i>t</i><i>r</i><sub>fix</sub> = 1 is activated throughout the preparatory and stimulus epochs. For example trials of each task, see Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">13</a>.</p><h4 id="Sec17">‘Go’ tasks</h4><p>The ‘Go’ family of tasks includes ‘Go’, ‘RTGo’, ‘AntiGo’, ‘AntiRTGo’ and modality-specific versions of each task denoted with either ‘Mod1’ and ‘Mod2’. In both the ‘Go’ and ‘AntiGo’ tasks, a single stimulus is presented at the beginning of the stimulus epoch. The direction of the presented stimulus is generated by drawing from a uniform distribution between 0 and 2<i>π</i>, that is, <span>\({\theta }_{{{{\rm{stim}}}}} \sim {\mathbb{U}}[0,2\pi ]\)</span>. The stimulus will appear in either modality 1 or modality 2 with equal probability. The strength of the stimulus is given by <span>\(st{r}_{{{{\rm{stim}}}}} \sim {\mathbb{U}}[1.0,1.2]\)</span>. In the ‘Go’ task, the target response is in the same direction as the presented stimulus, that is, <span>\({\theta }_{{{{\rm{stim}}}}}={\theta }_{{{{\rm{target}}}}}\)</span>, while in the ‘AntiGo’ task the direction of the response should be in the opposite of the stimulus direction, <span>\({\theta }_{{{{\rm{stim}}}}}+\pi ={\theta }_{{{{\rm{target}}}}}\)</span>. For modality-specific versions of each task, a stimulus direction is drawn in each modality <span>\({\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}2} \sim {\mathbb{U}}[0,2\pi ]\)</span> and for modality-specific Go-type tasks</p><div id="Equi"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}1} &amp;{{\mbox{if}}}\,\,\,{{\mbox{Mod1 task}}}\\ {\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}2} &amp;{{\mbox{if}}}\,\,\,{{\mbox{Mod2 task}}}\end{array}\right.$$</span></p></div><p>while for modality-specific AntiGo-type tasks</p><div id="Equj"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}1}+\pi &amp;{{\mbox{if}}}\,\,\,{{\mbox{Mod1 task}}}\,\\ {\theta }_{{{{\rm{stim}}}},{{{\rm{mod}}}}2}+\pi &amp;{{\mbox{if}}}\,\,\,{{\mbox{Mod2 task}}}\end{array}\right.$$</span></p></div><p>For ‘RT’ versions of the ‘Go’ tasks, stimuli are only presented during the response epoch and the fixation cue is never extinguished. Thus, the presence of the stimulus itself serves as the response cue and the model must respond as quickly as possible. Otherwise, stimuli persist through the duration of the stimulus epoch.</p><h4 id="Sec18">‘Decision-making’ tasks</h4><p>The ‘decision-making’ family of tasks includes ‘DM’ (decision-making), ‘AntiDM’, ‘MultiDM’ (multisensory decision-making), ‘AntiMultiDM,’ modality-specific versions of each of these tasks and, finally, confidence-based versions of ‘DM’ and ‘AntiDM.’ For all tasks in this group, two stimuli are presented simultaneously and persist throughout the duration of the stimulus epoch. They are drawn according to <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}\)</span><span>\([({\theta }_{{{{\rm{stim}}}}1}-0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}-0.6\pi )\cup ({\theta }_{{{{\rm{stim}}}}1}+0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}+0.6\pi )]\)</span>. A base strength applied to both stimuli is drawn such that <span>\(st{r}_{\rm{base}} \sim {\mathbb{U}}[1.0,1.2]\)</span>. A contrast is drawn from a discrete distribution such that <i>c</i> ~ {−0.175, −0.15, −0.1, 0.1, 0.15, 0.175} so the stimulus strength associated with each direction in a trial are given by <span>\(st{r}_{{{{\rm{stim}}}}1}=st{r}_{\rm{base}}+c\)</span> and <span>\(st{r}_{{{{\rm{stim}}}}2}=\)</span> <span>\({str}_{\rm{base}}-c\)</span>.</p><p>For the ‘DM’ task,</p><div id="Equk"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1} &gt; st{r}_{{{{\rm{stim}}}}2}\\ {\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for the the ‘AntiDM’ task,</p><div id="Equl"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1} &lt; st{r}_{{{{\rm{stim}}}}2}\\ {\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>For these versions of the tasks, the stimuli are presented in either modality 1 or modality 2 with equal probability. For the multisensory versions of each task, stimuli directions are drawn in the same manner and presented across both modalities so that <span>\({\theta }_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}1}={\theta }_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}2}\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}={\theta }_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}2}\)</span>. Base strengths are drawn independently for each modality. Contrasts for both modalities are drawn from a discrete distribution such that <span>\({c}_{{{\mathrm{mod}}}\,1},{c}_{{{\mathrm{mod}}}\,2} \sim \left\{0.2,0.175,\right.\)</span><span>\(\left.0.15,0.125,-0.125,-0.15,-0.175,-0.2\right\}\)</span>. If both <span>\(| {c}_{{{\mathrm{mod}}}\,1}| -| {c}_{{{\mathrm{mod}}}\,2}| =0\)</span> then contrasts are redrawn to avoid zero-contrast trials during training. If both <span>\({c}_{{{\mathrm{mod}}}\,1}\)</span> and <span>\({c}_{{{\mathrm{mod}}}\,2}\)</span> have the same sign, then contrasts are redrawn to ensure that the trial requires integrating over both modalities as opposed to simply performing a ‘DM’ task in a single modality. Criteria for target responses are measured as the strength of a given direction summed over both modalities. So, for ‘MultiDM’</p><div id="Equm"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1,{{\mathrm{mod}}}\,1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}1}+st{r}_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}2} &gt; st{r}_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}\\&amp;+st{r}_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}2}\\ {\theta }_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘AntiMultiDM’</p><div id="Equn"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1,{{\mathrm{mod}}}\,1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}1}+st{r}_{{{{\rm{stim}}}}1,{{{\rm{mod}}}}2} &lt; st{r}_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}\\&amp;+st{r}_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}2}\\ {\theta }_{{{{\rm{stim}}}}2,{{{\rm{mod}}}}1}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>Stimuli for modality-specific versions of each task are generated in the same way as multisensory versions of the task. Criteria for target response are the same as standard versions of ‘DM’ and ‘AntiDM’ tasks applied only to stimuli in the relevant modality.</p><p>In confidence-based decision-making tasks (‘ConDM’ and ‘ConAntiDM’), the stimuli directions are drawn in the same way as above. Stimuli are shown in either modality 1 or modality 2 with equal probability. In each trial, <i>s</i><i>t</i><i>r</i><sub>base</sub> = 1. The contrast and noise for each trial is based on the thresholded performance of a SIMPLENET model trained on all tasks except ‘ConDM’ and ‘ConAntiDM’. Once this model has been trained, we establish a threshold across levels of noise and contrasts for which the model can perform a ‘DM’ or an ‘AntiDM’ task at 95% correct. We then draw contrasts and noises for trials from above and below this threshold with equal probability during training. In trials where the noise and contrast levels fell below the 95% correct threshold, the model must repress response, and otherwise perform the decision-making task (either ‘DM’ or ‘AntiDM’).</p><h4 id="Sec19">‘Comparison’ tasks</h4><p>Our comparison task group includes ‘COMP1’, ‘COMP2’, ‘MultiCOMP1’, ‘MultiCOMP2’, ‘Anti’ versions of each of these tasks, as well as modality-specific versions of ‘COMP1’ and ‘COMP2’ tasks. This group of tasks is designed to extend the basic decision-making framework into a setting with more complex control demands. These tasks utilize the delay structure in the stimulus epoch so that stim1 appears only during the stim1 epoch, followed by a delay, and finally stim2. This provides a temporal ordering on the stimuli. In ‘COMP1’, the model must respond to the first stimulus only if it has greater strength than the second and otherwise repress a response that is</p><div id="Equo"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}1} &gt; st{r}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>Likewise, in ‘COMP2’, the model must respond to the second direction if it presented with greater strength than the first otherwise repress response that is</p><div id="Equp"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,st{r}_{{{{\rm{stim}}}}2} &gt; {{{{\rm{str}}}}}_{{{{\rm{stim}}}}1}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>In ‘Anti’ versions of the task the ordering criteria is the same except for stimuli with least strength, that is, for ‘AntiCOMP1’</p><div id="Equq"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,{{{{\rm{str}}}}}_{{{{\rm{stim}}}}1} &lt; {{{{\rm{str}}}}}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘AntiCOMP2’</p><div id="Equr"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,{{{{\rm{str}}}}}_{{{{\rm{stim}}}}2} &lt; {{{{\rm{str}}}}}_{{{{\rm{stim}}}}1}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>In multisensory settings, the criteria for target direction are analogous to the multisensory decision-making tasks where strength is integrated across modalities. Likewise, for modality-specific versions, the criteria are only applied to stimuli in the relevant modality. Stimuli directions and strength for each of these tasks are drawn from the same distributions as the analogous task in the ‘decision-making’ family. However, during training, we make sure to balance trials where responses are required and trials where models must repress response.</p><h4 id="Sec20">‘Duration’ tasks</h4><p>The ‘duration’ family of tasks includes ‘Dur1’, ‘Dur2’, ‘MultiDur1’, ‘MultiDur2’, ‘Anti’ versions of each of these tasks and modality-specific versions of ‘Dur1’ and ‘Dur2’ tasks. These tasks require models to perform a time estimation task with the added demand or stimuli ordering determining relevance for response. Like in ‘comparison’ tasks, stim1 is presented followed by a delay and then stim2. For ‘Dur1’ trials</p><div id="Equs"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,du{r}_{{{{\rm{stim}}}}1} &gt; du{r}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>Likewise, for ‘Dur2’</p><div id="Equt"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,du{r}_{{{{\rm{stim}}}}2} &gt; du{r}_{{{{\rm{stim}}}}1}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>In ‘Anti’ versions of these tasks, the correct response is in the direction of the stimulus with the shortest duration given the ordering criteria is met. Hence, for ‘AntiDur1’</p><div id="Equu"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,du{r}_{{{{\rm{stim}}}}1} &lt; du{r}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘AntiDur2’</p><div id="Equv"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,du{r}_{{{{\rm{stim}}}}2} &lt; du{r}_{{{{\rm{stim}}}}1}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>Across these tasks directions are drawn according to <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}[({\theta }_{{{{\rm{stim}}}}1}-0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}-0.6\pi )\cup ({\theta }_{{{{\rm{stim}}}}1}+0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}+0.6\pi )]\)</span>. Stimulus strengths are drawn according to <span>\(st{r}_{{{{\rm{stim}}}}1},st{r}_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}[0.8,1.2]\)</span>. To set the duration of each stimulus, we first draw <span>\(du{r}_{{{{\rm{long}}}}} \sim\)</span> <span>\(\{i| 35 &lt; i\le 50,i\in {\mathbb{N}}\}\)</span> and <span>\(du{r}_{{{{\rm{short}}}}} \sim \{i| 25 &lt; i\le (du{r}_{{{{\rm{long}}}}}-8),i\in {\mathbb{N}}\}\)</span>. During training, we determine which trials for a given task should and should not require a response in order to evenly balance repress and respond trials. We then assign <i>d</i><i>u</i><i>r</i><sub>long</sub> and <i>d</i><i>u</i><i>r</i><sub>short</sub> to either stim1 or stim2 so that the trial requires the appropriate response given the particular task type.</p><p>Again, criteria for correct response in the multisensory and modality-specific versions of each tasks follow analogous tasks in the ‘decision-making’ and ‘comparison’ groups where multisensory versions of the task require integrating total duration over each modality, and modality-specific tasks require only considering durations in the given task modality. For multisensory tasks, we draw duration value <span>\(du{r}_{{{{\rm{long}}}}} \sim \{i| 75 &lt; i\le 100,i\in {\mathbb{N}}\}\)</span> and then split this value <i>d</i><i>u</i><i>r</i><sub>long0</sub> = <i>d</i><i>u</i><i>r</i><sub>long </sub>× 0.55 and <i>d</i><i>u</i><i>r</i><sub>long1</sub> = <i>d</i><i>u</i><i>r</i><sub>long </sub>× 0.45. We also draw a value <i>d</i><i>u</i><i>r</i><sub>short</sub> = <i>d</i><i>u</i><i>r</i><sub>long</sub> − Δ<i>d</i><i>u</i><i>r</i> where <span>\(\Delta dur \sim \{i| 15 &lt; i\le 25,i\in {\mathbb{N}}\}\)</span>. This value is then subdivided further into <i>d</i><i>u</i><i>r</i><sub>short0</sub> = <i>d</i><i>u</i><i>r</i><sub>long1</sub> + Δ<i>d</i><i>u</i><i>r</i><sub>short</sub> where <span>\(\Delta du{r}_{{{{\rm{short}}}}} \sim\)</span> <span>\(\{i| 19 &lt; i\le 15,i\in {\mathbb{N}}\}\)</span> and <i>d</i><i>u</i><i>r</i><sub>short1</sub> = <i>d</i><i>u</i><i>r</i><sub>Short</sub> − <i>d</i><i>u</i><i>r</i><sub>short0</sub>. Short and long durations can then be allocated to the ordered stimuli according to task type. Drawing durations in this manner ensures that, like in ‘decision-making’ and ‘comparison’ groups, correct answers truly require models to integrate durations over both modalities, rather than simply performing the task in a given modality to achieve correct responses.</p><h4 id="Sec21">‘Matching’ tasks</h4><p>The ‘matching’ family of tasks consists of ‘DMS’ (delay match to stimulus), ‘DNMS’ (delay non-match to stimulus), ‘DMC’ (delay match to category) and ‘DMNC’ (delay non-match to category) tasks. For all tasks, stim1 is presented at the beginning of the stimulus epoch, followed by a delay, and the presentation of stim2. The stimulus strength is drawn according to <span>\(st{r}_{{{{\rm{stim}}}}1},st{r}_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}[0.8,1.2]\)</span>. The input modality for any given trial is chosen at random with equal probability. In both ‘DMS’ and ‘DNMS’ tasks, trials are constructed as ‘matching stim’ trials or ‘mismatching stim’ trials with equal probability. In ‘matching stim’ trials <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2}={\theta }_{{{{\rm{stim}}}}1}\)</span>. In ‘mismatch stim’ trials, <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and</p><div id="Equw"><p><span>$${\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}[({\theta }_{{{{\rm{stim}}}}1}-0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}-0.6\pi )\cup ({\theta }_{{{{\rm{stim}}}}1}+0.2\pi ,{\theta }_{{{{\rm{stim}}}}1}+0.6\pi )].$$</span></p></div><p>For ‘DMS’, models must respond in the displayed direction if the stimuli match, otherwise repress response,</p><div id="Equx"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,{\theta }_{{{{\rm{stim}}}}1}={\theta }_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘DNMS’, models must respond to the second direction if both directions are mismatched,</p><div id="Equy"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,{\theta }_{{{{\rm{stim}}}}1}\ne {\theta }_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>‘DMC’ and ‘DNMC’ tasks are organized in a similar manner. The stimulus input space is divided evenly into two categories such that cat1 = {<i>θ</i>: 0 &lt; <i>θ</i>≤<i>π</i>} and cat2 = {<i>θ</i>: <i>π</i> &lt; <i>θ</i>≤2<i>π</i>}. For ‘DMC’ and ‘DNMC’ tasks, trials are constructed as ‘matching cat.’ trials or ‘mismatching cat.’ trials with equal probability. In ‘matching cat.’ trials <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}({{{\mbox{cat}}}}_{{{{\rm{stim}}}}1})\)</span>, where <span>\({\mathbb{U}}({{{\mbox{cat}}}}_{{{{\rm{stim}}}}1})\)</span> is a uniform draw from the category of stim1. In ‘mismatch stim’ trials, <span>\({\theta }_{{{{\rm{stim}}}}1} \sim {\mathbb{U}}[0,2\pi ]\)</span> and <span>\({\theta }_{{{{\rm{stim}}}}2} \sim {\mathbb{U}}(-{{{\mbox{cat}}}}_{{{{\rm{stim}}}}1})\)</span> where <span>\(-{{{\mbox{cat}}}}_{{{{\rm{stim}}}}1}\)</span> is the opposite category as stim1. For ‘DMC’, the model must respond in the first direction if both stimuli are presented in the same category otherwise repress response,</p><div id="Equz"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}1}\quad &amp;\,{{\mbox{if}}}\,\,{{{\mbox{cat}}}}_{{{{\rm{stim}}}}1}={{{\mbox{cat}}}}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><p>and for ‘DNMC’, the model should respond to the second direction if both stimuli are presented in opposite categories otherwise repress response,</p><div id="Equaa"><p><span>$${\theta }_{{{{\rm{target}}}}}=\left\{\begin{array}{ll}{\theta }_{{{{\rm{stim}}}}2}\quad &amp;\,{{\mbox{if}}}\,\,{{{\mbox{cat}}}}_{{{{\rm{stim}}}}1}\ne {{{\mbox{cat}}}}_{{{{\rm{stim}}}}2}\\ {{{\rm{repress}}}}\quad &amp;\,{{\mbox{otherwise}}}\,\end{array}\right.$$</span></p></div><h3 id="Sec22">Target output and correct criteria</h3><p>The target output <span>\(y\in {{\mathbb{R}}}^{33\times T}\)</span> for a trial entails maintaining fixation in <i>y</i><sub>1</sub> = <i>y</i><sub>fix</sub> during the stimulus epoch, and then either responding in the correct direction or repressing activity in the remaining target response units <i>y</i><sub>2…33</sub> in the response epoch. Since the model should maintain fixation until response, target for fixation is set at <i>y</i><sub>fix</sub> = 0.85 during preparatory and stimulus epochs and <i>y</i><sub>fix</sub> = 0.05 in the response epoch. When a response is not required, as in the preparatory and stimulus epochs and with repressed activity in the response epoch, unit <i>i</i> takes on a target activity of <i>y</i><sub><i>i</i></sub> = 0.05. Alternatively, when there is a target direction for response,</p><div id="Equab"><p><span>$${y}_{i}=0.8\exp \left[-0.5 \times {\left(\frac{8| {\theta }_{{{{\rm{target}}}}}-{\theta }_{i}| }{\pi }\right)}^{2}\right]+0.05$$</span></p></div><p>where <i>θ</i><sub><i>i</i></sub> is the preferred direction for unit <i>i</i>. Like in sensory stimuli, preferred directions for target units are evenly spaced values from [0, 2<i>π</i>] allocated to the 32 response units.</p><p>For a model response to count as correct, it must maintain fixation, that is, <span>\({\hat{y}}_{{{{\rm{fix}}}}} &gt; 0.5\)</span> during preparatory and stimulus epochs. When no response is required <span>\({\hat{y}}_{i} &lt; 0.15\)</span>. When a response is required, response activity is decoded using a population vector method and <span>\({\theta }_{{{{\rm{resp}}}}.}\in ({\theta }_{{{{\rm{target}}}}}-\frac{\pi }{10},{\theta }_{{{{\rm{target}}}}}+\frac{\pi }{10})\)</span>. If the model fails to meet any of these criteria, the trial response is incorrect.</p><h3 id="Sec23">Model training</h3><p>Again following ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e11643">18</a></sup>, model parameters are updated in a supervised fashion according to a masked mean squared error loss (mMSE) computed between the model motor response, <span>\({\hat{y}}_{1\ldots T}=\hat{y}\)</span>, and the target, <i>y</i><sub>1…<i>T</i></sub> = <i>y</i>, for each trial.</p><div id="Equac"><p><span>$$L={{{\rm{mMSE}}}}(\,y,\hat{y})={\rm{mask}} \times {\Big\langle {\left({\,y}_{t}-{{\hat{y}_{t}}}\right)}^{2}\Big\rangle }_{t}$$</span></p></div><p>Here, the multiplication sign denotes element-wise multiplication. Masks weigh the importance of different trial epochs. During preparatory and stimulus epochs, mask weights are set to 1; during the first five time steps of the response epoch, the mask value is set to 0; and during the remainder of the response epoch, the mask weight is set to 5. The mask value for the fixation is twice that of other values at all time steps.</p><p>For all models, we update Θ = {sensorimotor-RNN, Linear<sub>out</sub>} during training on our task set. For instructed models, we additionally update Linear<sub>embed</sub> in the process of normal training. We train models using standard PyTorch machinery and an Adam optimizer. An epoch consists of 2,400 mini-batches, with each mini-batch consisting of 64 trials. For all models, we use the same initial learning rate as in ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e11872">18</a></sup>, <i>l</i><i>r</i> = 0.001. We found that in the later phases of training, model performance oscillated based on which latest task presented during training, so we decayed the learning rate for each epoch by a factor of <i>γ</i> = 0.95, which allowed performance to converge smoothly. Following ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e11885">18</a></sup>, models train until they reach a threshold performance of 95% across all tasks (and train for a minimum of 35 epochs). We found that training for GPTNET tended to asymptote below performance threshold for multisensory versions of comparison tasks. This held true over a variety of training hyperparameters and learning rate scheduler regimes. Hence, we relax the performance threshold of GPTNET to 85%. For each model type, we train five models that start from five different random initializations. Where applicable, results are averaged over these initializations.</p><h4 id="Sec24">Language model fine-tuning</h4><p>When fine-tuning models, we allow the gradient from the motor loss experienced during sensorimotor training to fine-tune the weights in the final layers of the transformer language models. During normal training, we checkpoint a copy of our instructed models after training for 30 epochs. We then add the last three transformer layers to the set of trainable parameters, and reset the learning rates to <i>l</i><i>r</i> = 1 × 10<sup>−</sup><sup>4</sup> for Θ = {sensorimotor-RNN, Linear<sub>out</sub>} and <i>l</i><i>r</i><sup>lang</sup> = 3 × 10<sup>−4</sup> for Θ<sup>lang</sup> = {Linear<sub>embed</sub>, transformer<sub>−3,−2,−1</sub>} where transformer<sub>−3,−2,−1</sub> denotes the parameters of the last three layers of the relevant transformer architecture. We used these reduced learning rates to avoid completely erasing preexisting linguistic knowledge. Similarly for RNN parameters, we found the above learning rate avoided catastrophic forgetting of sensorimotor knowledge while also allowing the RNN to adapt to updated language embeddings across all models. Autoregressive models were much more sensitive to this procedure, often collapsing at the beginning of fine-tuning. Hence, for GPTNETXL and GPTNET, we used <i>l</i><i>r</i><sup>lang</sup> = 5 × 10<sup>−5</sup>, which resulted in robust learning. Models train until they reach a threshold performance of 95% across training tasks or 85% correct for GPTNET.</p><h3 id="Sec25">Hold-out testing</h3><p>During hold-out testing, we present models with 100 batches of one of the tasks that had been held out of training. For the instructed model, the only weights allowed to update during this phase are Θ = {sensorimotor-RNN, Linear<sub>out</sub>, Linear<sub>embed</sub>}. All weights of SIMPLENET and STRUCTURENET are trainable in this context. In this hold-out setting, we found that in more difficult tasks for some of our more poorly performing models, the standard hyperparameters we used during training resulted in unstable learning curves for novel tasks. To stabilize performance and thereby create fair comparisons across models, we used an increased batch size of 256. We then began with the standard learning rate of 0.001 and decreased this by increments of 0.0005 until all models showed robust learning curves. This resulted in a learning rate of 8 × 10<sup>−4</sup>. All additional results shown in the <a data-track="click" data-track-label="link" data-track-action="section anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Sec36">Supplementary Information</a> section 4 follow this procedure.</p><h3 id="Sec26">CCGP calculation</h3><p>To calculate CCGP, we trained a linear decoder on a pair of tasks and then tested that decoder on alternative pairs of tasks that have an analogous relationship. We grouped tasks into eight dichotomies: ‘Go’ versus ‘Anti’, ‘Standard’ versus ‘RT’, ‘Weakest’ versus ‘Strongest’, ‘Longest’ versus ‘Shortest’, ‘First Stim.’ versus ‘Second Stim’, ‘Stim Match’ versus ‘Category Match’, ‘Matching’ versus ‘Non-Matching’ and ‘Mod1’ versus ‘Mod2’. As an example, the ‘Go’ versus ‘Anti’ dichotomy includes (‘Go’, ‘AntiGo’), (‘GoMod1’, ‘AntiGoMod1’), (‘GoMod2’, ‘AntiGoMod2’), (‘RTGo’, ‘AntiRTGo’), (‘RTGoMod1’, ‘AntiRTGoMod1’) and (‘RTGoMod2’, ‘AntiRTGoMod2’) task pairs. For ‘RNN’ task representations, we extracted activity at the time of stimulus onset for 250 example trials. For language representations, we input the instruction sets for relevant tasks to our language model and directly analyze activity in the ‘embedding’ layer or take the sequence-averaged activity in each transformer layer. For nonlinguistic models, we simply analyze the space of rule vectors. Train and test conditions for decoders were determined by dichotomies identified across the task set (Supplementary Note <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">1</a>). To train and test decoders, we used sklearn.svm.LinearSVC Python package. The CCGP score for a given task is the average decoding score achieved across all dichotomies where the task in question was part of either the train set or the test set. For model scores reported in the main text, we only calculate CCGP scores for models where the task in question has been held out of training. In Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">9</a>, we report scores on tasks where models have been trained on all tasks, and for models where instructions have been switched for the hold-out task.</p><p>For Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3e</a>, we calculated Pearson’s <i>r</i> correlation coefficient between performance on held-out tasks and CCGP scores per task, as well as a <i>P</i>-value testing against the null hypothesis that these metrics are uncorrelated and normally distributed (using the scipy.stats.pearsonr function). Full statistical tests for CCGP scores of both RNN and embedding layers from Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3f</a> can be found in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">9</a>. Note that transformer language models use the same set of pretrained weights among random initialization of Sensorimotor-RNNs, thus for language model layers, the Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig3">3f</a> plots show the absolute scores of those language models.</p><h3 id="Sec27">Conditional clause/deduction task analysis</h3><p>We first split our task set into two groups (listed below): tasks that included conditional clauses and simple deductive reasoning components (30 tasks) and those where instructions include simple imperatives (20 tasks). We computed the difference in performance across the mean of generalization performance for each group across random initialization for each model (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig2">2f</a>). We compared these differences to a null distribution constructed by performing a set of 50 random shuffles of the task set into groups of 30 and 20 tasks and computing differences in the same way, again using two-sided unequal-variance <i>t</i>-tests. Because STRUCUTRENET is a nonlinguistic model, we then compared performance of STRUCUTRENET to our instructed models to disassociate the effects of performing tasks with a deductive reasoning component versus processing instructions with more complicated conditional clause structure. Results of all statistical tests are reported in Supplementary Fig. <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM1">6</a>).</p><p>Simple imperative tasks include: ‘Go’, ‘AntiGo’, ‘RTGo’, ‘AntiRTGo’, ‘GoMod1’, ‘GoMod2’, ‘AntiGoMod1’, ‘AntiGoMod2’, ‘RTGoMod1’, ‘AntiRTGoMod2’, ‘RTGoMod2’, ‘AntiRTGoMod2’, ‘DM’, ‘AntiDM’, ‘MultiDM’, ‘AntiMultiDM’, ‘DMMod1’, ‘DMMod2’, ‘AntiDMMod1’ and ‘AntiDMMod2’.</p><p>Conditional clause/deduction tasks include: ‘ConDM’, ‘ConAntiDM’, ‘Dur1’, ‘Dur2’, ‘MultiDur1’, ‘MultiDur2’, ‘AntiDur1’, ‘AntiDur2’, ‘AntiMultiDur1’, ‘AntiMultiDur2’, ‘Dur1Mod1’, ‘Dur1Mod2’, ‘Dur2Mod1’, ‘Dur2Mod2’, ‘COMP1’, ‘COMP2’, ‘MultiCOMP1’, ‘MultiCOMP2’, ‘AntiCOMP1’, ‘AntiCOMP2’, ‘AntiMultiCOMP1’, ‘AntiMultiCOMP2’, ‘COMP1Mod1’, ‘COMP1Mod2’, ‘COMP2Mod1’, ‘COMP2Mod2’, ‘DMS’, ‘DNMS’, ‘DMC’ and ‘DMNC’.</p><h3 id="Sec28">Language production training</h3><h4 id="Sec29">Self-supervised language production network training</h4><p>Our language production framework is inspired by classic sequence-to-sequence modeling using RNNs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Sutskever, I., Vinyals, O. &amp; Le., Q. V. Sequence to sequence learning with neural networks. In Proc. 27th International Conference on Neural Information Processing Systems 3104–3112 (MIT Press, 2014)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR53" id="ref-link-section-d167182994e12022">53</a></sup>. Our Production-RNN is a GRU with 256 hidden units using ReLU nonlinearities. At each step in the sequence, a set of decoder weights, Linear<sub>words</sub>, attempts to decode the next token, <i>w</i><sub><i>τ</i>+1</sub>, from the hidden state of the recurrent units. The hidden state of the Production-RNN is initialized by concatenating the time average and maximum sensorimotor activity of a SBERTNET (L) and passing that through weights Linear<sub>sm</sub>. The linguistic instruction used to drive the initializing sensorimotor activity is in turn used as the target set of tokens for the Production-RNN outputs. The first input to the Production-RNN is always a special start-of-sentence token, and the decoder runs until an end-of-sentence token is decoded or until input reaches a length of 30 tokens. Suppose <span>\({w}_{1,k}\ldots {w}_{{{{\mathcal{T}}}},k}\in {\rm{Instruc{t}}}_{k}^{i}\)</span> is the sequence of tokens in instruction <i>k</i> where <i>k</i> is in the instruction set for task <i>i</i> and <i>X</i><sup><i>i</i></sup> is sensory input for a trial of task <i>i</i>. For brevity, we denote the process by which language models embed instructions as Embed() (see ‘Pretrained transformers’). The decoded token at the <i>τ</i><sup>th</sup> position, <span>\({\hat{w}}_{\tau ,k}\)</span>, is then given by</p><div id="Equad"><p><span>$$\begin{array}{ll}{h}_{T}^{sm}={{{\rm{SensorimotorRNN}}}}\left({X}^{i},Embed\left({w}_{1,k}\ldots {w}_{{{{\mathcal{T}}}},k}\right)\right)\quad\quad{h}_{T}^{sm}\in {{\mathbb{R}}}^{T\times 256}\\ sm\_out=\left.\right({{{{\rm{mean}}}}}_{T}\left({h}_{T}^{sm}\right),\mathop{\max }\limits_{T}\left({h}_{T}^{sm}\right)\quad\quad\quad\quad\quad\quad\quad\quad\quad\;\;{sm}\_{out}\in {{\mathbb{R}}}^{512}\\ \overline{{h}_{0}^{{{{\rm{decoder}}}}}}={{{\rm{relu}}}}\left({{{{\rm{Linear}}}}}_{{{{\rm{sm}}}}}(sm\_out)\right)\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\;\overline{{h}_{0}^{{{{\rm{decoder}}}}}}\in {{\mathbb{R}}}^{256}\\ {h}_{0}^{{{{\rm{decoder}}}}}={{{\rm{Dropout}}}}\left(\overline{{h}_{0}^{{{{\rm{decoder}}}}}}\right)\;\;\;\quad\quad\quad\quad\quad\quad\quad\quad\quad\qquad\quad{h}_{0}^{{{{\rm{decoder}}}}}\in {{\mathbb{R}}}^{256}\\ {h}_{\tau }^{{{{\rm{decoder}}}}}={{{\rm{ProductionRNN}}}}\left({\hat{w}}_{1,k}\ldots {\hat{w}}_{\tau -1,k};{h}_{0}^{{{{\rm{decoder}}}}}\right),\quad\quad\quad{h}_{\tau }^{{{{\rm{decoder}}}}}\in {{\mathbb{R}}}^{256}\\ {p}_{{\hat{w}}_{\tau ,k}}={{{\rm{softmax}}}}\left({{{{\rm{Linear}}}}}_{{{{\rm{words}}}}}\left({h}_{\tau ,k}^{{{{\rm{decoder}}}}}\right)\right)\quad\quad\quad\quad\quad\quad\quad\quad\quad{p}_{{\hat{w}}_{\tau ,k}}\in {{\mathbb{R}}}^{| vocab| },\\ {\hat{w}}_{\tau ,k}={{{\rm{argmax}}}}\left({p}_{{\hat{w}}_{\tau ,k}}\right)\end{array}$$</span></p></div><p>The model parameters Θ<sup>production</sup> = {Linear<sub>sm</sub>, Linear<sub>words</sub>, Production-RNN} are trained using cross-entropy loss between the <span>\({p}_{{\hat{w}}_{\tau ,i}}\)</span> and the instruction token <i>w</i><sub><i>τ</i>,<i>k</i></sub> provided to the sensorimotor-RNN as input. We train for 80 epochs of 2,400 batches with 64 trials per batch and with task type randomly interleaved. We found that using an initial learning rate of 0.001 sometimes caused models to diverge in early phases of training, so we opted for a learning rate of 1× 10<sup>−4</sup>, which led to stable early training. To alleviate similar oscillation problems detected in sensorimotor training, we also decayed the learning rate by <i>γ</i> = 0.99 per epoch. Additionally, the use of a dropout layer with a dropout rate of 0.05 improved performance. We also used a teacher forcing curriculum, where for some ratio of training batches, we input the ground truth instruction token <i>w</i><sub><i>τ</i>,<i>k</i></sub> at each time step instead of the models decoded word <span>\({\hat{w}}_{\tau ,k}\)</span>. At each epoch, <span>\({\rm{teacher}}\,{{\mbox{\_}}}{\rm{forcing}}{{\mbox{\_}}}\)</span> <span>\({\rm{ratio}}=0.5 \times \frac{80-{{{\rm{epoch}}}}}{80}\)</span>.</p><h4 id="Sec30">Obtaining embedding layer activity using motor feedback</h4><p>For a task, <i>i</i>, we seek to optimize a set of embedding activity vectors <span>\({E}^{i}\in {{\mathbb{R}}}^{64}\)</span> such that when they are input as task-identifying information, the model will perform the task in question. Crucially, we freeze all model weights Θ = {sensorimotor-RNN, Linear<sub>out</sub>, Linear<sub>embedding</sub>} and only update <i>E</i><sup><i>i</i></sup> according to the standard supervised loss on the motor output. For notional clarity, GRU dependence on the previous hidden state <i>h</i><sub><i>t</i>−1</sub> has been made implicit in the following equations.</p><div id="Equae"><p><span>$$\begin{array}{rcl}{\hat{y}}{\,}^{i}&amp;=&amp;\sigma \Big({{{{\rm{Linear}}}}}_{{{{\rm{out}}}}}\left({{{\rm{SensorimotorRNN}}}}({X}^{\,i},{E}^{i})\right)\Big)\\ L&amp;=&amp;{\rm{mMSE}}(\;y,\hat{y})\end{array}$$</span></p></div><p>We optimized a set of 25 embedding vectors for each task, again using an Adam optimizer. Here the optimization space has many suboptimal local minimums corresponding to embeddings for related tasks. Hence, we used a high initial learning rate of <i>l</i><i>r</i> = 0.05, which we decayed by <i>γ</i> = 0.8 for each epoch. This resulted in more robust learning than lower learning rates. An epoch lasts for 800 batches with a batch length of 64, and we train for a minimum of 1 epoch or until we reach a threshold performance of 90% or 85% on ‘DMC’ and ‘DNMC’ tasks.</p><h4 id="Sec31">Producing task instructions</h4><p>To produce task instructions, we simply use the set <i>E</i><sup><i>i</i></sup> as task-identifying information in the input of the sensorimotor-RNN and use the Production-RNN to output instructions based on the sensorimotor activity driven by <i>E</i><sup><i>i</i></sup>. For each task, we use the set of embedding vectors to produce 50 instructions per task. We repeat this process for each of the 5 initializations of sensorimotor-RNN, resulting in 5 distinct language production networks, and 5 distinct sets of learned embedding vectors. Reported results for each task are averaged over these 5 networks. For the confusion matrix (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5d</a>), we report the average percentage that decoded instructions are in the training instruction set for a given task or a novel instruction. Partner model performance (Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="https://www.nature.com/articles/s41593-024-01607-5#Fig5">5e</a>) for each network initialization is computed by testing each of the 4 possible partner networks and averaging over these results.</p><h3 id="Sec32">Sample sizes/randomization</h3><p>No statistical methods were used to predetermine sample sizes but following ref. <sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T. &amp; Wang, X.-J. Task representations in neural networks trained to perform many cognitive tasks. Nat. Neurosci. 22, 297–306 (2019)." href="https://www.nature.com/articles/s41593-024-01607-5#ref-CR18" id="ref-link-section-d167182994e13740">18</a></sup> we used five different random weight initializations per language model tested. Randomization of weights was carried out automatically in Python and PyTorch software packages. Given this automated randomization of weights, we did not use any blinding procedures in our study. No data were excluded from analyses.</p><h3 id="Sec33">Software</h3><p>All simulation and data analysis was performed in Python 3.7.11. PyTorch 1.10 was used to implement and train models (this includes Adam optimizer implementation). Transformers 4.16.2 was used to implement language models and all pretrained weights for language models were taken from the Huggingface repository (<a href="https://huggingface.co/docs/transformers/">https://huggingface.co/docs/transformers/</a>). We also used scikit-learn 0.24.1 and scipy 1.7.3 to perform analyses.</p><h3 id="Sec34">Reporting summary</h3><p>Further information on research design is available in the <a data-track="click" data-track-label="link" data-track-action="supplementary material anchor" href="https://www.nature.com/articles/s41593-024-01607-5#MOESM2">Nature Portfolio Reporting Summary</a> linked to this article.</p></div></div>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The New Inflection (181 pts)]]></title>
            <link>https://inflection.ai/the-new-inflection</link>
            <guid>39757368</guid>
            <pubDate>Tue, 19 Mar 2024 16:23:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://inflection.ai/the-new-inflection">https://inflection.ai/the-new-inflection</a>, See on <a href="https://news.ycombinator.com/item?id=39757368">Hacker News</a></p>
<div id="readability-page-1" class="page"><div justify="center" id=""><p>From day one at Inflection, we’ve been driven by a simple mission: to create a personal intelligence for everyone. To do this, we trained one of the <a href="https://inflection.ai/inflection-2-5" target="_blank" rel="noopener noreferrer">best LLMs in the world</a>, and created our first personal AI, Pi, which couples extraordinary EQ with industry leading IQ, and is now used by millions of people a week.</p>
<p>As an AI studio we have <a href="https://inflection.ai/company" target="_blank" rel="noopener noreferrer">long planned</a> to make our technology available to developers and enterprises. And over the last year, we’ve heard countless times that people haven’t been able to replicate the unique conversational style of Pi with publicly available models, and would love to get access to our model and fine tuning infrastructure. There is a huge opportunity for Inflection here.</p>
<p>Our plan going forward is to lean into our AI studio business, where custom generative AI models are crafted, tested and fine tuned for commercial customers. Our success at training, tailoring and improving the performance of large AI models makes us uniquely well placed to be the AI platform for businesses around the world.</p>
<p>As part of this, we’re thrilled to announce that we will now host Inflection-2.5 on Microsoft Azure helping us get it into the hands of creators everywhere. We’ll also be ensuring it comes to other cloud hosting platforms in the near future. The API itself isn’t available today, but will be up and running very soon. To sign up for early access and help us test it, please register your interest <a href="https://docs.google.com/forms/d/e/1FAIpQLScM9Iz1KzaRlfgDrYrldoPDnXbhO5LW3-hqmQCd56YpheEN7g/viewform" target="_blank" rel="noopener noreferrer">here</a>. Between API access and select high-level partnerships with great customers, our AIs can now spread to even larger new user bases while helping put cutting-edge AI capabilities in the hands of thousands of developers.</p>
<p>This renewed emphasis on our API also comes with some important changes in the company. Today we are also announcing that two of our three co-founders, Mustafa and Karén, will be leaving Inflection to start Microsoft AI, a new division at Microsoft that will bring together their consumer AI efforts, as well as Copilot, Bing and Edge. We’re grateful for all their amazing work in getting Inflection to this stage, and wish them luck for this new chapter.</p>
<p>We are delighted to welcome a new CEO, <a href="https://www.linkedin.com/in/seanwhite/" target="_blank" rel="noopener noreferrer">Sean White</a>, who has decades of experience working at the cutting edge of technology, research and business. He is a visionary leader poised to take Inflection into this new era. Our third co-founder, Reid Hoffman will continue on our board and remains excited to take these next steps in building personal intelligence for everyone.</p>
<p>We are hugely proud of what we’ve achieved with Pi. There will be no immediate changes to the service and we’re committed to ensuring that users get ongoing access to great AI experiences in the future. Our privacy and data policies to ensure users are protected remain in place and unchanged. As ever, no data will be shared with any third parties without users' explicit consent.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to Start Google (582 pts)]]></title>
            <link>https://paulgraham.com/google.html</link>
            <guid>39756865</guid>
            <pubDate>Tue, 19 Mar 2024 15:36:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulgraham.com/google.html">https://paulgraham.com/google.html</a>, See on <a href="https://news.ycombinator.com/item?id=39756865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div width="435"><tbody><tr><td><img src="https://s.turbifycdn.com/aah/paulgraham/how-to-start-google-1.gif" width="171" height="18" alt="How to Start Google"><span size="2" face="verdana">March 2024<p><i>(This is a talk I gave to 14 and 15 year olds about what to do now
if they might want to start a startup later. Lots of schools think
they should tell students something about startups. This is what I
think they should tell them.)</i></p><p>Most of you probably think that when you're released into the
so-called real world you'll eventually have to get some kind of
job. That's not true, and today I'm going to talk about a trick you
can use to avoid ever having to get a job.</p><p>The trick is to start your own company. So it's not a trick for
avoiding <i>work</i>, because if you start your own company you'll
work harder than you would if you had an ordinary job. But you will
avoid many of the annoying things that come with a job, including
a boss telling you what to do.</p><p>It's more exciting to work on your own project than someone else's.
And you can also get a lot richer. In fact, this is the standard
way to get 
</p><a href="https://paulgraham.com/richnow.html"><u>really rich</u></a>. If you look at the lists of the richest
people that occasionally get published in the press, nearly all of
them did it by starting their own companies.<p>Starting your own company can mean anything from starting a barber
shop to starting Google. I'm here to talk about one extreme end of
that continuum. I'm going to tell you how to start Google.</p><p>The companies at the Google end of the continuum are called startups
when they're young. The reason I know about them is that my wife
Jessica and I started something called Y Combinator that is basically
a startup factory. Since 2005, Y Combinator has funded over 4000
startups. So we know exactly what you need to start a startup,
because we've helped people do it for the last 19 years.</p><p>You might have thought I was joking when I said I was going to tell
you how to start Google. You might be thinking "How could <i>we</i>
start Google?" But that's effectively what the people who did start
Google were thinking before they started it. If you'd told Larry
Page and Sergey Brin, the founders of Google, that the company they
were about to start would one day be worth over a trillion dollars,
their heads would have exploded.</p><p>All you can know when you start working on a startup is that it
seems worth pursuing. You can't know whether it will turn into
a company worth billions or one that goes out of business. So when I
say I'm going to tell you how to start Google, I mean I'm going to
tell you how to get to the point where you can start a company that
has as much chance of being Google as Google had of being Google.
</p><span color="#dddddd">[<a href="#f1n"><span color="#dddddd">1</span></a>]</span><p>How do you get from where you are now to the point where you can
start a successful startup? You need three things. You need to be
good at some kind of technology, you need an idea for what you're
going to build, and you need cofounders to start the company with.</p><p>How do you get good at technology? And how do you choose which
technology to get good at? Both of those questions turn out to have
the same answer: work on your own projects. Don't try to guess
whether gene editing or LLMs or rockets will turn out to be the
most valuable technology to know about. No one can predict that.
Just work on whatever interests you the most. You'll work much
harder on something you're interested in than something you're doing
because you think you're supposed to.</p><p>If you're not sure what technology to get good at, get good at
programming. That has been the source of the median startup for the
last 30 years, and this is probably not going to change in the next
10.</p><p>Those of you who are taking computer science classes in school may
at this point be thinking, ok, we've got this sorted. We're already
being taught all about programming. But sorry, this is not enough.
You have to be working on your own projects, not just learning stuff
in classes. You can do well in computer science classes without
ever really learning to program. In fact you can graduate with a
degree in computer science from a top university and still not be
any good at programming. That's why tech companies all make you
take a coding test before they'll hire you, regardless of where you
went to university or how well you did there. They know grades and
exam results prove nothing.</p><p>If you really want to learn to program, you have to work on your
own projects. You learn so much faster that way. Imagine you're
writing a game and there's something you want to do in it, and you
don't know how. You're going to figure out how a lot faster than
you'd learn anything in a class.</p><p>You don't have to learn programming, though. If you're wondering
what counts as technology, it includes practically everything you
could describe using the words "make" or "build." So welding would
count, or making clothes, or making videos. Whatever you're most
interested in. The critical distinction is whether you're producing
or just consuming. Are you writing computer games, or just playing
them? That's the cutoff.</p><p>Steve Jobs, the founder of Apple, spent time when he was a teenager
studying calligraphy — the sort of beautiful writing that
you see in medieval manuscripts. No one, including him, thought
that this would help him in his career. He was just doing it because
he was interested in it. But it turned out to help him a lot. The
computer that made Apple really big, the Macintosh, came out at
just the moment when computers got powerful enough to make letters
like the ones in printed books instead of the computery-looking
letters you see in 8 bit games. Apple destroyed everyone else at
this, and one reason was that Steve was one of the few people in
the computer business who really got graphic design.</p><p>Don't feel like your projects have to be <i>serious</i>. They can
be as frivolous as you like, so long as you're building things
you're excited about. Probably 90% of programmers start out building
games. They and their friends like to play games. So they build
the kind of things they and their friends want. And that's exactly
what you should be doing at 15 if you want to start a startup one
day.</p><p>You don't have to do just one project. In fact it's good to learn
about multiple things. Steve Jobs didn't just learn calligraphy.
He also learned about electronics, which was even more valuable.
Whatever you're interested in. (Do you notice a theme here?)</p><p>So that's the first of the three things you need, to get good at
some kind or kinds of technology. You do it the same way you get
good at the violin or football: practice. If you start a startup
at 22, and you start writing your own programs now, then by the
time you start the company you'll have spent at least 7 years
practicing writing code, and you can get pretty good at anything
after practicing it for 7 years.</p><p>Let's suppose you're 22 and you've succeeded: You're now really
good at some technology. How do you get 
</p><a href="https://paulgraham.com/startupideas.html"><u>startup ideas</u></a>? It might
seem like that's the hard part. Even if you are a good programmer,
how do you get the idea to start Google?<p>Actually it's easy to get startup ideas once you're good at technology.
Once you're good at some technology, when you look at the world you
see dotted outlines around the things that are missing. You start
to be able to see both the things that are missing from the technology
itself, and all the broken things that could be fixed using it, and
each one of these is a potential startup.</p><p>In the town near our house there's a shop with a sign warning that
the door is hard to close. The sign has been there for several
years. To the people in the shop it must seem like this mysterious
natural phenomenon that the door sticks, and all they can do is put
up a sign warning customers about it. But any carpenter looking at
this situation would think "why don't you just plane off the part
that sticks?"</p><p>Once you're good at programming, all the missing software in the
world starts to become as obvious as a sticking door to a carpenter.
I'll give you a real world example. Back in the 20th century,
American universities used to publish printed directories with all
the students' names and contact info. When I tell you what these
directories were called, you'll know which startup I'm talking
about. They were called facebooks, because they usually had a picture
of each student next to their name.</p><p>So Mark Zuckerberg shows up at Harvard in 2003, and the university
still hasn't gotten the facebook online. Each individual house has
an online facebook, but there isn't one for the whole university.
The university administration has been diligently having meetings
about this, and will probably have solved the problem in another
decade or so. Most of the students don't consciously notice that
anything is wrong. But Mark is a programmer. He looks at this
situation and thinks "Well, this is stupid. I could write a program
to fix this in one night. Just let people upload their own photos
and then combine the data into a new site for the whole university."
So he does. And almost literally overnight he has thousands of
users.</p><p>Of course Facebook was not a startup yet. It was just a... project.
There's that word again. Projects aren't just the best way to learn
about technology. They're also the best source of startup ideas.</p><p>Facebook was not unusual in this respect. Apple and Google also
began as projects. Apple wasn't meant to be a company. Steve Wozniak
just wanted to build his own computer. It only turned into a company
when Steve Jobs said "Hey, I wonder if we could sell plans for this
computer to other people." That's how Apple started. They weren't
even selling computers, just plans for computers. Can you imagine
how lame this company seemed?</p><p>Ditto for Google. Larry and Sergey weren't trying to start a company
at first. They were just trying to make search better. Before Google,
most search engines didn't try to sort the results they gave you
in order of importance. If you searched for "rugby" they just gave
you every web page that contained the word "rugby." And the web was
so small in 1997 that this actually worked! Kind of. There might
only be 20 or 30 pages with the word "rugby," but the web was growing
exponentially, which meant this way of doing search was becoming
exponentially more broken. Most users just thought, "Wow, I sure
have to look through a lot of search results to find what I want."
Door sticks. But like Mark, Larry and Sergey were programmers. Like
Mark, they looked at this situation and thought "Well, this is
stupid. Some pages about rugby matter more than others. Let's figure
out which those are and show them first."</p><p>It's obvious in retrospect that this was a great idea for a startup.
It wasn't obvious at the time. It's never obvious. If it was obviously
a good idea to start Apple or Google or Facebook, someone else would
have already done it. That's why the best startups grow out of
projects that aren't meant to be startups. You're not trying to
start a company. You're just following your instincts about what's
interesting. And if you're young and good at technology, then your
unconscious instincts about what's interesting are better than your
conscious ideas about what would be a good company.</p><p>So it's critical, if you're a young founder, to build things for
yourself and your friends to use. The biggest mistake young founders
make is to build something for some mysterious group of other people.
But if you can make something that you and your friends truly want
to use — something your friends aren't just using out of
loyalty to you, but would be really sad to lose if you shut it down
— then you almost certainly have the germ of a good startup
idea. It may not seem like a startup to you. It may not be obvious
how to make money from it. But trust me, there's a way.</p><p>What you need in a startup idea, and all you need, is something
your friends actually want. And those ideas aren't hard to see once
you're good at technology. There are sticking doors everywhere.
</p><span color="#dddddd">[<a href="#f2n"><span color="#dddddd">2</span></a>]</span><p>Now for the third and final thing you need: a cofounder, or cofounders.
The optimal startup has two or three founders, so you need one or
two cofounders. How do you find them? Can you predict what I'm going
to say next? It's the same thing: projects. You find cofounders by
working on projects with them. What you need in a cofounder is
someone who's good at what they do and that you work well with, and
the only way to judge this is to work with them on things.</p><p>At this point I'm going to tell you something you might not want
to hear. It really matters to do well in your classes, even the
ones that are just memorization or blathering about literature,
because you need to do well in your classes to get into a good
university. And if you want to start a startup you should try to
get into the best university you can, because that's where the best
cofounders are. It's also where the best employees are. When Larry
and Sergey started Google, they began by just hiring all the smartest
people they knew out of Stanford, and this was a real advantage for
them.</p><p>The empirical evidence is clear on this. If you look at where the
largest numbers of successful startups come from, it's pretty much
the same as the list of the most selective universities.</p><p>I don't think it's the prestigious names of these universities that
cause more good startups to come out of them. Nor do I think it's
because the quality of the teaching is better. What's driving this
is simply the difficulty of getting in. You have to be pretty smart
and determined to get into MIT or Cambridge, so if you do manage
to get in, you'll find the other students include a lot of smart
and determined people.
</p><span color="#dddddd">[<a href="#f3n"><span color="#dddddd">3</span></a>]</span><p>You don't have to start a startup with someone you meet at university.
The founders of Twitch met when they were seven. The founders of
Stripe, Patrick and John Collison, met when John was born. But
universities are the main source of cofounders. And because they're
where the cofounders are, they're also where the ideas are, because
the best ideas grow out of projects you do with the people who
become your cofounders.</p><p>So the list of what you need to do to get from here to starting a
startup is quite short. You need to get good at technology, and the
way to do that is to work on your own projects. And you need to do
as well in school as you can, so you can get into a good university,
because that's where the cofounders and the ideas are.</p><p>That's it, just two things, build stuff and do well in school.</p><p><b>Notes</b></p><p>[</p><a name="f1n"><span color="#000000">1</span></a>]
The rhetorical trick in this sentence is that the "Google"s
refer to different things. What I mean is: a company that has as
much chance of growing as big as Google ultimately did as Larry and
Sergey could have reasonably expected Google itself would at the
time they started it. But I think the original version is zippier.<p>[</p><a name="f2n"><span color="#000000">2</span></a>]
Making something for your friends isn't the only source of
startup ideas. It's just the best source for young founders, who
have the least knowledge of what other people want, and whose own
wants are most predictive of future demand anyway.<p>[</p><a name="f3n"><span color="#000000">3</span></a>]
Strangely enough this is particularly true in countries like
the US where undergraduate admissions are done badly. US admissions
departments make applicants jump through a lot of arbitrary hoops
that have little to do with their intellectual ability. But the
more arbitrary a test, the more it becomes a test of mere determination
and resourcefulness. And those are the two most important qualities
in startup founders. So US admissions departments are better at
selecting founders than they would be if they were better at selecting
students.<span color="888888"><b>Thanks</b> to Carolynn Levy, Jessica Livingston and Harj 
Taggar for reading drafts of this.</span></span></td></tr></tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Intermediate Certificates (139 pts)]]></title>
            <link>https://letsencrypt.org/2024/03/19/new-intermediate-certificates.html</link>
            <guid>39756434</guid>
            <pubDate>Tue, 19 Mar 2024 15:02:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letsencrypt.org/2024/03/19/new-intermediate-certificates.html">https://letsencrypt.org/2024/03/19/new-intermediate-certificates.html</a>, See on <a href="https://news.ycombinator.com/item?id=39756434">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<article>
		<p>On Wednesday, March 13, 2024, Let’s Encrypt generated 10 new Intermediate CA Key Pairs, and issued 15 new Intermediate CA Certificates containing the new public keys. These new intermediate certificates provide smaller and more efficient certificate chains to Let’s Encrypt Subscribers, enhancing the overall online experience in terms of speed, security, and accessibility.</p>
<p>First, a bit of history. In September, 2020, Let’s Encrypt issued a <a href="https://letsencrypt.org/2020/09/17/new-root-and-intermediates">new root and collection of intermediate certificates</a>. Those certificates helped us improve the privacy and efficiency of Web security by making ECDSA end-entity certificates widely available. However, those intermediates are approaching their expiration dates, so it is time to replace them.</p>
<p>Our new batch of intermediates are very similar to the ones we issued in 2020, with a few small changes. We’re going to go over what those changes are and why we made them.</p>

<p>We created 5 new 2048-bit RSA intermediate certificates named in sequence from R10 through R14. These are issued by ISRG Root X1. You can think of them as direct replacements for our existing R3 and R4 intermediates.</p>
<p>We also created 5 new P-384 ECDSA intermediate certificates named in sequence from E5 through E9. Each of these is represented by two certificates: one issued by ISRG Root X2 (exactly like our existing E1 and E2), and one issued (or cross-signed) by ISRG Root X1.</p>
<p>You can see details of all of the certificates on our <a href="https://letsencrypt.org/certificates/">updated hierarchy page</a>.</p>
<p><img src="https://letsencrypt.org/images/blog/ChainofTrust2024CeremonyBlogPost.png" alt=""></p>
<h2 id="rotating-issuance">Rotating Issuance</h2>
<p>Rotating the set of intermediates we issue from helps keep the Internet agile and more secure. It encourages automation and efficiency, and discourages outdated practices like key pinning. “Key Pinning” is a practice in which clients — either ACME clients getting certificates for their site, or apps connecting to their own backend servers — decide to trust only a single issuing intermediate certificate rather than delegating trust to the system trust store. Updating pinned keys is a manual process, which leads to an increased risk of errors and potential business continuity failures.</p>
<p>Intermediates usually change only every five years, so this joint is exercised infrequently and client software keeps making the same mistakes. Shortening the lifetime from five years to three years means we will be conducting another ceremony in just two years, ahead of the expiration date on these recently created certificates. This ensures we exercise the joint more frequently than in the past.</p>
<p>We also issued <em>more</em> intermediates this time around. Historically, we’ve had two of each key type (RSA and ECDSA): one for active issuance, and one held as a backup for emergencies. Moving forward we will have five: two conducting active issuance, two waiting in the wings to be introduced in about one year, and one for emergency backup. Randomizing the selected issuer for a given key type means it will be impossible to predict which intermediate a certificate will be issued from. We are very hopeful that these steps will prevent intermediate key pinning altogether, and help the WebPKI remain agile moving forward.</p>
<p>These shorter intermediate lifetimes and randomized intermediate issuance shouldn’t impact the online experience of the general Internet user. Subscribers may be impacted if they are pinning one of our intermediates, though this should be incredibly rare.</p>
<h2 id="providing-smaller-chains">Providing Smaller Chains</h2>
<p>When we issued ISRG Root X2 in 2020, we decided to cross-sign it from ISRG Root X1 so that it would be trusted even by systems that didn’t yet have ISRG Root X2 in their trust store. This meant that Subscribers who wanted issuance from our ECDSA intermediates would have a choice: they could either have a very short, ECDSA-only, but low-compatibility chain terminating at ISRG Root X2, or they could have a longer, high-compatibility chain terminating at ISRG Root X1. At the time, this tradeoff (TLS handshake size vs compatibility) seemed like a reasonable choice to provide, and we provided the high-compatibility chain by default to support the largest number of configurations.</p>
<p>ISRG Root X2 is now trusted by most platforms, and we can now offer an improved version of the same choice. The same very short, ECDSA-only chain will still be available for Subscribers who want to optimize their TLS handshakes at the cost of some compatibility. But the high-compatibility chain will be drastically improving: instead of containing two intermediates (both E1 and the cross-signed ISRG Root X2), it will now contain only a single intermediate: the version of one of our new ECDSA intermediates cross-signed by ISRG Root X1.</p>
<p>This reduces the size of our default ECDSA chain by about a third, and is an important step towards removing our <a href="https://docs.google.com/forms/d/e/1FAIpQLScCWnApP2eUk4cA6y5cFOENlm5S2StVedrqYNzeNdTPoArzwA/viewform">ECDSA allow-list</a>.</p>
<h2 id="other-minor-changes">Other Minor Changes</h2>
<p>We’ve made two other tiny changes that are worth mentioning, but will have no impact on how Subscribers and clients use our certificates:</p>
<ul>
<li>
<p>We’ve changed how the Subject Key ID field is calculated, from a SHA-1 hash of the public key, to a <a href="https://datatracker.ietf.org/doc/html/rfc7093#section-2">truncated SHA-256 hash</a> of the same data. Although this use of SHA-1 was not cryptographically relevant, it is still nice to remove one more usage of that <a href="https://shattered.io/">broken algorithm</a>, helping move towards a world where cryptography libraries don’t need to include SHA-1 support at all.</p>
</li>
<li>
<p>We have removed our CPS OID from the Certificate Policies extension. This saves a few bytes in the certificate, which can add up to a lot of bandwidth saved over the course of billions of TLS handshakes.</p>
</li>
</ul>
<p>Both of these mirror two <a href="https://community.letsencrypt.org/t/enabling-sha256-subject-key-identifiers-for-end-entity-certificates/211453/4">identical</a> <a href="https://community.letsencrypt.org/t/small-change-to-end-entity-certificates-cps-url-and-oid-will-not-be-included-from-june-15/198206/5">changes</a> that we made for our Subscriber Certificates in the past year.</p>
<h2 id="deployment">Deployment</h2>
<p>We intend to put two of each of the new RSA and ECDSA keys into rotation in the next few months. Two of each will be ready to swap in at a future date, and one of each will be held in reserve in case of an emergency. Read more about the strategy in our December 2023 post on the <a href="https://community.letsencrypt.org/t/lets-encrypt-new-intermediate-certificates/209498">Community Forum</a>.</p>
<p>Not familiar with the forum? It’s where Let’s Encrypt publishes updates on our <a href="https://community.letsencrypt.org/c/issuance-tech-questions/12">Issuance Tech</a> and <a href="https://community.letsencrypt.org/c/api-announcements/18">APIs</a>. It’s also where you can go for troubleshooting help from community experts and Let’s Encrypt staff. <a href="https://community.letsencrypt.org/">Check it out</a> and subscribe to alerts for technical updates.</p>
<p>We hope that this has been an interesting and informative tour around our new intermediates, and we look forward to continuing to improve the Internet, one certificate at a time.</p>
<p>We depend on contributions from our community of users and supporters in order to provide our services. If your company or organization would like to <a href="https://www.abetterinternet.org/sponsor/">sponsor</a> Let’s Encrypt please email us at <a href="mailto:sponsor@letsencrypt.org">sponsor@letsencrypt.org</a>. We ask that you make an <a href="https://letsencrypt.org/donate/">individual contribution</a> if it is within your means.</p>

	</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Godspeed is a fast, 100% keyboard oriented todo app for Mac (291 pts)]]></title>
            <link>https://godspeedapp.com/</link>
            <guid>39756325</guid>
            <pubDate>Tue, 19 Mar 2024 14:53:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://godspeedapp.com/">https://godspeedapp.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39756325">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding Google's File System (2020) (130 pts)]]></title>
            <link>https://www.micahlerner.com/2020/03/22/understanding-googles-file-system.html</link>
            <guid>39756262</guid>
            <pubDate>Tue, 19 Mar 2024 14:48:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.micahlerner.com/2020/03/22/understanding-googles-file-system.html">https://www.micahlerner.com/2020/03/22/understanding-googles-file-system.html</a>, See on <a href="https://news.ycombinator.com/item?id=39756262">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="post-content">
<p>
<em>
These paper reviews can <a href="https://newsletter.micahlerner.com/">be delivered weekly to your inbox</a>, or you can subscribe to the <a href="https://www.micahlerner.com/feed.xml">Atom feed</a>. As always, feel free to reach out on <a href="https://twitter.com/micahlerner">Twitter</a> with feedback or suggestions!
</em>
</p>
<p>Today I read <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf">the original paper</a> about the Google File System (GFS), a system that provided the storage layer for many of Google’s applications in the company’s early days. The original implementation has reportedly been replaced by a newer version called Colossus, but reading about the original approach was still illuminating and I thought I’d do a quick write up about it.</p>
<h3 id="why-iswas-gfs-such-a-big-deal">Why is/was GFS such a big deal?</h3>
<p>The original paper was published in 2003 at SOSP (Symposium on Operating Systems Principles - one of, if not the, best conferences for operating systems research).</p>
<p>GFS made it onto the program because of how revolutionary it was at the time - the accompanying paper detailed how Google had successfully implemented academic ideas of weak consistency and reliance on a single master controller (more on this later) at tremendous scale in an industry application.</p>
<p>The ultimate goal of GFS was to provide a replicated storage layer (redundant copies of data are kept across many machines) across the commodity level machines in a Google datacenter. The original motivation for developing such a system was to power batch jobs, although the system eventually powered other projects.</p>
<p>Because GFS was designed for batch jobs, it primarily optimized for appending to, rather than modifying, files. Users of the program were generally writing large files out at once rather than making modifications to specific parts of a file.</p>
<h3 id="what-are-the-abstractions-that-power-gfs">What are the abstractions that power GFS?</h3>
<p>At the core of GFS is a concept called <strong>chunks</strong>. Chunks are used to split up files into fixed-size 64MB segments that are then replicated around the datacenter <a href="#footnotes">†</a>. Chunks are referred to by <strong>chunk handles</strong>, basically unique ids for a chunk. Splitting a large file into many chunks, then replicating those chunks across many machines accomplished two goals: improving performance (as there could now be many readers and writers of a single file), and allowing huge files to exist behind a simple abstraction.</p>
<p>To make the idea of how this abstraction works more concrete, imagine using a library to open a file on a disk. Behind the scenes, that library now goes out and fetches all of the different pieces of the file you requested from computers all around your datacenter, then provides a transparent way to interact with the stitched together data <a href="#footnotes">†</a>.</p>
<p>The aforementioned library (called by your user program, the Client) performs fetching and writing operations by interacting with several components of GFS:</p>
<ul>
<li><strong>Master</strong>: The master has a few responsibilties. To start, it is the first point of contact for a client when they want to interact with GFS. In addition to that function, the master is also responsible for communicating with a set of <strong>chunk servers</strong> that host chunks. To perform its functions, the master stores a few tables in RAM:
<ul>
<li>A mapping from filenames to <strong>chunk handles</strong> (chunk handles are basically IDs for chunks).</li>
<li>A mapping from <strong>chunk handles</strong> to a list of the machines that the chunk is on, versioning information about the chunk (a piece of data to help with managing multiple writes to the same chunk), and two pieces of information related to managing writes to that chunk - the primary and the lease. I’ll cover the primary and the lease in the next section.</li>
</ul>
</li>
<li><strong>Chunk Server</strong>: Chunk servers handle work around writing to and reading from disk. A client starts talking to them after being told to do so by the master.</li>
</ul>
<h3 id="how-does-writing-and-reading-to-gfs-work">How does writing and reading to GFS work?</h3>
<h4 id="reading-from-gfs">Reading from GFS</h4>
<p>To read a file to GFS, a client says to the master, “I would like to read this byte offset in this file”, where the file looks like a regular file system path.</p>
<p>The master then receives the request from the client and calculates which chunk corresponds to the associated file and byte offset. Using the chunk handle of the calculated chunk, the master then gets the list of chunk servers that store the aforementioned chunk and provides it to the client. The client then chooses a chunk server, contacting it with the chunk and offset it wants, then is provided with the requested data.</p>
<p>Along the way, the client also caches information about the chunk and the chunkservers it can find that chunk on if it needs to rerequest the chunk.</p>
<h4 id="writing-to-gfs">Writing to GFS</h4>
<p>Writing (in this case, appending) to files in GFS is significantly more complicated than reading from GFS.</p>
<p>To start a client, asks the master for a specific file’s last chunk (the end of the file is necessary because we are appending). The master then checks its tables for information on that chunk, using the returned chunk handle (the chunk handle is essentially the ID of the chunk).</p>
<p>The master then inspects two pieces of information that it is storing about each chunk - the primary and lease fields.</p>
<p>The <strong>primary</strong> is a reference to a chunk server that has been assigned to coordinate writes among chunk servers. This assignment is short lived, and is governed by the expiration of the <strong>lease</strong>. When the lease runs out, the master can assign a new chunk server to coordinate writes.</p>
<p>If the chunk that a client requests does not have a <strong>primary</strong> assigned, the master assigns one, and increments the version of the data. Incrementing the version number allows the master to keep track of which data is the most recent. If the chunk already has a primary, this step is skipped.</p>
<p>The next step is to transmit information about the primary and secondaries (chunk servers that have the chunk, but aren’t the primary) to the client. From there, the client sends the data it wants to write to the respective chunk servers. After all chunk servers have the data, the client tells the primary to write it. The primary chunk server chooses a byte offset in the chunk (whatever the end of the file is), and sends it to all of the secondaries, after which all of them perform the right.</p>
<p>If the primary and all secondaries write, the client receives a success! If not all secondaries write, the client receives a failure, at which point it needs to recontact the master and repeat the process from the beginning.</p>
<h3 id="wrapping-up">Wrapping up</h3>
<p>I <a href="https://queue.acm.org/detail.cfm?id=1594206">found an interview</a> with one of the engineers who worked on GFS to be fairly interesting. GFS was very successful for the applications it was designed for and reached wide adoption within Google.</p>
<p>Unfortunately, it didn’t scale as well to new use cases for a few reasons. First off, the system used a single master process to store of chunk servers in addition to other metadata. Having all of this information in RAM on a single machine only went so far.</p>
<p>Another issue that GFS ran into was in storing small files. For example, if a user wanted to store many files smaller than the chunk size, the master needed to store an entry for each file, and allocate the full chunk size on disk. Google ended up working on other systems and making tweaks to GFS to solve this problem (in particular, one of the systems that is discusses is BigTable).</p>
<h3 id="footnotes">Footnotes:</h3>
<ul>
<li><a name="#1">[1]</a> Google’s new storage system would try to decrease the chunk size for reasons that I talk about at the end of this post.</li>
<li><a name="#2">[2]</a> Whether the data is actually stitched together or not is somewhat of an implementation detail</li>
</ul>
<h3 id="references">References:</h3>
<ul>
<li>[1] <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf">GFS paper</a></li>
<li>[2] <a href="https://www.youtube.com/watch?v=EpIgvowZr00&amp;feature=emb_title">MIT Distributed Systems lecture on GFS</a></li>
<li>[3] <a href="https://cs.stanford.edu/~matei/courses/2015/6.S897/slides/gfs.pdf">Talk about GFS from Firas Abuzaid</a></li>
</ul>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Okapi (YC W24) – A new, flexible CRM with good UX (117 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39755927</link>
            <guid>39755927</guid>
            <pubDate>Tue, 19 Mar 2024 14:14:24 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39755927">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39755927">
      <td><span></span></td>      <td><center><a id="up_39755927" href="https://news.ycombinator.com/vote?id=39755927&amp;how=up&amp;goto=item%3Fid%3D39755927"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39755927">Launch HN: Okapi (YC W24) – A new, flexible CRM with good UX</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39755927">101 points</span> by <a href="https://news.ycombinator.com/user?id=ucarion">ucarion</a> <span title="2024-03-19T14:14:24"><a href="https://news.ycombinator.com/item?id=39755927">10 hours ago</a></span> <span id="unv_39755927"></span> | <a href="https://news.ycombinator.com/hide?id=39755927&amp;goto=item%3Fid%3D39755927">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Okapi%20(YC%20W24)%20%E2%80%93%20A%20new%2C%20flexible%20CRM%20with%20good%20UX&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39755927&amp;auth=fd3b1bfbfa2ede56a93337bc8a8a5fdbb34db87d">favorite</a> | <a href="https://news.ycombinator.com/item?id=39755927">71&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Hi HN!</p><p>We’re Ulysse and Ned from Okapi. We’re making a modern CRM. It's kind of like if Airtable/Notion built Salesforce today. <a href="https://okapicrm.com/">https://okapicrm.com</a></p><p>When I was fresh out of college, working for startups, Salesforce was this big mystery to me. It's the perpetual second screen for everyone in sales. They were building this gigantic obelisk downtown. What was this thing? And why did every salesperson I knew seem to live in it all day, and yet hate it so much?</p><p>Turns out Salesforce is basically PhpMyAdmin for salespeople. It's just basically two things:</p><p>1. A really generic database, and a UI for CRUDing your data.</p><p>2. An API and ecosystem of integrations.</p><p>In other words, it's the back-office CRUD app to end all back-office CRUD apps. People call the result a "CRM" (Customer Relationship Management).</p><p>Salesforce is slow and clunky. And that doesn’t matter because people don’t buy Salesforce to be delighted. They buy Salesforce to avoid being screwed over.</p><p>SObjects are how Salesforce does that. It’s the best idea Salesforce ever had: make everything in your product be built upon a generic data layer that your users can configure. Just like how you can add a new sheet or column in a spreadsheet, in Salesforce you can add new objects and fields. You can CRUD, report on, and do automations on top of custom data in exactly the same way you do it for built-in data.</p><p>The graveyard of wannabe Salesforces is littered with people who forgot about this key insight.</p><p>We think the way you win in CRM in 2024 is by keeping the flexibility of SObjects, and then tacking on modern UX. That first part takes engineering discipline, but the second part is much easier than Salesforce had it 25 years ago. Just about every part of a modern SaaS product has dozens of vendors that do most of the work for you.</p><p>We're really excited to get feedback from HN on what we have. Here’s a video demo: <a href="https://www.youtube.com/watch?v=4uRBf_9CRyM" rel="nofollow">https://www.youtube.com/watch?v=4uRBf_9CRyM</a> - and you can try it yourself:</p><p><a href="https://app.okapicrm.com/">https://app.okapicrm.com</a></p><p>Email: hn.login.user@okapicrm.com</p><p>Password: sk9ueEAfhXP9j4cuxLCVmw7.</p><p>You can test out our email integration, send an email to: hn.demo@okapicrm.com</p><p>And see that email (up to ~10min latency): <a href="https://app.okapicrm.com/objects/emails/records">https://app.okapicrm.com/objects/emails/records</a></p><p>We’re very eager for your honest feedback. What do you think we're missing? One thing we can’t decide if it’s a need-to-have is Apex -- is having in-transaction custom scripting really necessary for a CRM? Or is that just something they tacked on as Salesforce became more of an app platform and less of a focused CRM product?</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Causal 2.0 – Modern Financial Planning for Startups (227 pts)]]></title>
            <link>https://causal.app</link>
            <guid>39755858</guid>
            <pubDate>Tue, 19 Mar 2024 14:06:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://causal.app">https://causal.app</a>, See on <a href="https://news.ycombinator.com/item?id=39755858">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><h2>Modern financial planning</h2></p><div><h4>Know your runway, plan your growth, and get back to building your business.</h4></div><div id="w-node-_91886231-9d7f-5602-643a-69f0db31642b-7aaf274e"><a href="#"><img src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/646df0ef933b939f7e8e497e_Homepage%20Hero%20Screenshot.png" loading="lazy" sizes="(max-width: 479px) 87vw, (max-width: 767px) 92vw, 94vw" width="1049" alt="" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/646df0ef933b939f7e8e497e_Homepage%20Hero%20Screenshot-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/646df0ef933b939f7e8e497e_Homepage%20Hero%20Screenshot-p-800.png 800w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/646df0ef933b939f7e8e497e_Homepage%20Hero%20Screenshot-p-1080.png 1080w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/646df0ef933b939f7e8e497e_Homepage%20Hero%20Screenshot.png 2472w"><div><p><img src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65c27274ca6082d2d7b15fcf_Triangle.svg" loading="lazy" alt=""></p></div></a></div></div><div><p><h2>Spend less time in spreadsheets, and more time on your business</h2></p><div><div><p><img src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7ad524f9e4d4bd2c45d_Feature%3DFormulas.png" loading="lazy" sizes="(max-width: 479px) 89vw, (max-width: 767px) 86vw, 28vw" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7ad524f9e4d4bd2c45d_Feature%3DFormulas-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7ad524f9e4d4bd2c45d_Feature%3DFormulas.png 728w" alt=""></p><h4>Formulas made for humans</h4><p>Model anything with human-readable formulas — no #REFs or VLOOKUPs.</p></div><div><p><img src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166447926f5f42e3af_Feature%3DIntegrations.png" loading="lazy" sizes="(max-width: 479px) 89vw, (max-width: 767px) 86vw, 28vw" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166447926f5f42e3af_Feature%3DIntegrations-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166447926f5f42e3af_Feature%3DIntegrations-p-800.png 800w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166447926f5f42e3af_Feature%3DIntegrations-p-1080.png 1080w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166447926f5f42e3af_Feature%3DIntegrations.png 1456w" alt=""></p><h4>Connected to your stack</h4><p>Pull live data from your accounting system, HRIS, CRM, data warehouse, and more.</p></div><div><p><img src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166f2c57641a7dc5ff_Feature%3DScenarios.png" loading="lazy" sizes="(max-width: 479px) 89vw, (max-width: 767px) 86vw, 28vw" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166f2c57641a7dc5ff_Feature%3DScenarios-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166f2c57641a7dc5ff_Feature%3DScenarios-p-800.png 800w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166f2c57641a7dc5ff_Feature%3DScenarios-p-1080.png 1080w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb166f2c57641a7dc5ff_Feature%3DScenarios.png 1456w" alt=""></p><h4>Plan for every scenario</h4><p>Spin up new scenarios in 1 click and compare them side-by-side.</p></div></div></div><div><div data-current="Reporting" data-easing="ease" data-duration-in="300" data-duration-out="100"><div data-w-tab="Reporting"><div><h2>"How did we spend so much on contractors last quarter?"</h2><p>1/5</p></div><div><div><p>Pull your historicals directly from QuickBooks/Xero, and drill down into the transactions behind each number without having to jump between browser tabs or chase your accountant.<br></p></div><p><img sizes="(max-width: 479px) 92vw, (max-width: 767px) 91vw, 30vw" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb161d9b9fcc1c71ba4e_Feature%3DDrilldown-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb161d9b9fcc1c71ba4e_Feature%3DDrilldown-p-800.png 800w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb161d9b9fcc1c71ba4e_Feature%3DDrilldown-p-1080.png 1080w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb161d9b9fcc1c71ba4e_Feature%3DDrilldown.png 1456w" alt="" src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adcb161d9b9fcc1c71ba4e_Feature%3DDrilldown.png" loading="lazy"></p></div></div><div data-w-tab="Forecasting"><div><h2>“How can we <em>actually</em> 3x revenue next year?”</h2><p>2/5</p></div><div><div><p>It’s well and good setting an ambitious top-down target, but you also need a credible bottom-up plan to get there. Get started with one of our templates and forecast your business in as much detail as you need.<br></p></div><p><img sizes="(max-width: 479px) 92vw, (max-width: 767px) 91vw, 30vw" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7ad524f9e4d4bd2c45d_Feature%3DFormulas-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7ad524f9e4d4bd2c45d_Feature%3DFormulas.png 728w" alt="" src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7ad524f9e4d4bd2c45d_Feature%3DFormulas.png" loading="lazy"></p></div></div><div data-w-tab="Hiring plans"><div><h2>“What’s our runway if we hire 3 more engineers?”</h2><p>3/5</p></div><div><div><p>Headcount is your biggest expense, so you should know how your current team and planned hires are affecting your bottom line. Start with one of our templates and customise it with your own hiring scenarios.<br></p></div><p><img sizes="(max-width: 479px) 92vw, (max-width: 767px) 91vw, 30vw" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7b67aa1e2b14f46ce8c_Feature%3DScenarios-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7b67aa1e2b14f46ce8c_Feature%3DScenarios.png 728w" alt="" src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65abc7b67aa1e2b14f46ce8c_Feature%3DScenarios.png" loading="lazy"></p></div></div><div data-w-tab="Budgeting"><div><h2>“How are we tracking against our plan for the year?”</h2><p>4/5</p></div><div><div><p>Save versions of your model and compare them side-by-side against the actuals to understand variances. Causal puts this whole process on autopilot —&nbsp;no more manual work in rolling your model forward each month.<br></p></div><p><img sizes="(max-width: 479px) 92vw, (max-width: 767px) 91vw, 30vw" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65ca1bb521453d03f9316559_BvA-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65ca1bb521453d03f9316559_BvA-p-800.png 800w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65ca1bb521453d03f9316559_BvA-p-1080.png 1080w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65ca1bb521453d03f9316559_BvA.png 1456w" alt="" src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65ca1bb521453d03f9316559_BvA.png" loading="lazy"></p></div></div><div data-w-tab="Consolidation"><div><h2>“How is the business performing across all entities?”</h2><p>5/5</p></div><div><div><p>Connect to all of your QuickBooks/Xero entities and Causal will do the currency conversion and consolidation work for you, letting you get a complete picture of your business without any manual work.</p></div><p><img sizes="(max-width: 479px) 92vw, (max-width: 767px) 91vw, 30vw" srcset="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adca6a398892a4a89c6fad_Feature%3DCurrency-p-500.png 500w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adca6a398892a4a89c6fad_Feature%3DCurrency-p-800.png 800w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adca6a398892a4a89c6fad_Feature%3DCurrency-p-1080.png 1080w, https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adca6a398892a4a89c6fad_Feature%3DCurrency.png 1456w" alt="" src="https://assets-global.website-files.com/61e8494b1e8e024a7113bd50/65adca6a398892a4a89c6fad_Feature%3DCurrency.png" loading="lazy"></p></div></div></div><p>→</p></div><div><h2>Connect to your QuickBooks/Xero,<br>and let our AI do the work</h2><p><a href="https://my.causal.app/register" target="_blank">Get started for free →</a></p><div><div><p>01</p><h4>Connect your data</h4><p>Authenticate with QuickBooks/Xero to pull your P&amp;L&nbsp;and Balance Sheet into Causal.</p><p>2 minutes</p></div><div><p>02</p><h4>Run the&nbsp;wizard</h4><p>Our AI&nbsp;wizard will analyse your chart of accounts and generate a financial model for you.</p><p>1 minute</p></div><div><p>03</p><h4>Customise your model</h4><p>Adjust assumptions and formulas, and spin up different scenarios.</p><p>3 minutes</p></div><div><p>04</p><h4>Share with your team</h4><p>Share beautiful dashboards with  your team, with live editing and commenting.</p><p>2 minutes</p></div></div></div><div><p><h2>Bring your business data, wherever it lives.</h2></p></div><div><h2>Put finance on autopilot today</h2><p><a href="https://my.causal.app/register">Get started free →</a></p><div><div><h4><span>1</span>00x</h4><p>fewer formulas vs the same model in Excel/Sheets</p></div><div><h4>20hrs</h4><p>saved per month eliminating manual processes</p></div><div><h4>$50k+</h4><p>saved of unproductive  time per year</p></div></div></div><div><div><p><h2>Don’t just take our word for it</h2></p></div><div><h3>What people are saying</h3><div role="list"><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Mike Overell" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/63fd4204cf1aa8839e9869c7_0i9NYWKh_400x400.jpg"></p><div><p>The rate of product improvements from <a href="https://twitter.com/CausalHQ">@CausalHQ</a> is incredible.</p><p>Classdojo runs all forecasts, scenarios, and financial planning in it, without a single FTE in the function 🤯</p></div></div><nav><div><p><img loading="lazy" alt="Mike Overell" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/63fd4204cf1aa8839e9869c7_0i9NYWKh_400x400.jpg"></p></div><div><p>The rate of product improvements from <a href="https://twitter.com/CausalHQ">@CausalHQ</a> is incredible.</p><p>Classdojo runs all forecasts, scenarios, and financial planning in it, without a single FTE in the function 🤯</p></div></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Jeremy Higgs" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5b500d68fc2832b5e519_4ViSkMPP_400x400.jpeg"></p><p>I’ve been using <a href="https://twitter.com/CausalHQ">@CausalHQ</a> for the past two weeks to rebuild our financial model, and it’s such a breath of fresh air compared to the fragile, static models that I ended up making in Excel/Sheets.</p></div><nav><div><p><img loading="lazy" alt="Jeremy Higgs" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5b500d68fc2832b5e519_4ViSkMPP_400x400.jpeg"></p></div><p>I’ve been using <a href="https://twitter.com/CausalHQ">@CausalHQ</a> for the past two weeks to rebuild our financial model, and it’s such a breath of fresh air compared to the fragile, static models that I ended up making in Excel/Sheets.</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Zuhayeer Musa" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5ab4b60fbe82aabbc422_0Hu6fzCv_400x400.jpeg"></p><p>If you haven’t yet checked out <a href="https://twitter.com/CausalHQ">@CausalHQ</a>, it’s an incredible way to create live-updating forecasts with built-in tools for modeling uncertainty</p></div><nav><div><p><img loading="lazy" alt="Zuhayeer Musa" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5ab4b60fbe82aabbc422_0Hu6fzCv_400x400.jpeg"></p></div><p>If you haven’t yet checked out <a href="https://twitter.com/CausalHQ">@CausalHQ</a>, it’s an incredible way to create live-updating forecasts with built-in tools for modeling uncertainty</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Dries Vaesen" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5a41b60fbe376abbc20d_V0GxkBmz_400x400.jpeg"></p><p>Been playing around with <a href="https://twitter.com/CausalHQ">@CausalHQ</a> for the last hour or so and I'm very impressed by the simplicity yet thoughtfulness of the features and interactions. Great job!</p></div><nav><div><p><img loading="lazy" alt="Dries Vaesen" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5a41b60fbe376abbc20d_V0GxkBmz_400x400.jpeg"></p></div><p>Been playing around with <a href="https://twitter.com/CausalHQ">@CausalHQ</a> for the last hour or so and I'm very impressed by the simplicity yet thoughtfulness of the features and interactions. Great job!</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Tyler Tringas" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d59f8439bcf21130d6a9d_XBVpLgf-_400x400.jpeg"></p><p>I’m a bit of a spreadsheet/model-building nerd and I’ve been super impressed with <a href="https://twitter.com/CausalHQ">@CausalHQ</a>. Among other things I think we’ll rebuild a dynamic version of our founder-break-even calculator. Should be fun.</p></div><nav><div><p><img loading="lazy" alt="Tyler Tringas" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d59f8439bcf21130d6a9d_XBVpLgf-_400x400.jpeg"></p><div><h6>Tyler Tringas</h6><p>@tylertringas</p></div></div><p>I’m a bit of a spreadsheet/model-building nerd and I’ve been super impressed with <a href="https://twitter.com/CausalHQ">@CausalHQ</a>. Among other things I think we’ll rebuild a dynamic version of our founder-break-even calculator. Should be fun.</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="ben 🌐³ bryandigital.io 🔮👀📐🏗️🚀✨🟣" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d596ac3925a8c7550af50_OXjXu8o6_400x400.jpeg"></p><div><p>One of my favourite web apps: <a href="https://www.causal.app/">http://causal.app</a></p><p>Quickly &amp; easily create financial models + automatically generate dynamic presentations based on those models. Brilliant work by <a href="https://twitter.com/CausalHQ">@CausalHQ</a></p></div></div><nav><div><p><img loading="lazy" alt="ben 🌐³ bryandigital.io 🔮👀📐🏗️🚀✨🟣" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d596ac3925a8c7550af50_OXjXu8o6_400x400.jpeg"></p><div><h6>ben 🌐³ bryandigital.io 🔮👀📐🏗️🚀✨🟣</h6><p>@bryandigitalio</p></div></div><div><p>One of my favourite web apps: <a href="https://www.causal.app/">http://causal.app</a></p><p>Quickly &amp; easily create financial models + automatically generate dynamic presentations based on those models. Brilliant work by <a href="https://twitter.com/CausalHQ">@CausalHQ</a></p></div></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Matt 🏴󠁧󠁢󠁷󠁬󠁳󠁿" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d58f0c1a96097f76bc8f7_PAcQCoYn_400x400.jpeg"></p><div><p>On a demo with <a href="https://twitter.com/CausalHQ">@CausalHQ</a> via @makerpad </p><p>It is a game changer for modeling. CFOs and PMs will be chomping at the bit to get their hands on it. </p><p>👏</p></div></div><nav><div><p><img loading="lazy" alt="Matt 🏴󠁧󠁢󠁷󠁬󠁳󠁿" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d58f0c1a96097f76bc8f7_PAcQCoYn_400x400.jpeg"></p><div><h6>Matt 🏴󠁧󠁢󠁷󠁬󠁳󠁿</h6><p>@makermattevans</p></div></div><div><p>On a demo with <a href="https://twitter.com/CausalHQ">@CausalHQ</a> via @makerpad </p><p>It is a game changer for modeling. CFOs and PMs will be chomping at the bit to get their hands on it. </p><p>👏</p></div></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Danae Shell" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d58b3a6484f37f374f6e2_GJfB9xOZ_400x400.jpeg"></p><p>Y'all, I have built a LOT of financial models in my day and <a href="https://twitter.com/CausalHQ">@CausalHQ</a> just made it super easy to build one up from scratch. WAY easier than my spreadsheets. I am obsessed!</p></div><nav><div><p><img loading="lazy" alt="Danae Shell" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d58b3a6484f37f374f6e2_GJfB9xOZ_400x400.jpeg"></p></div><p>Y'all, I have built a LOT of financial models in my day and <a href="https://twitter.com/CausalHQ">@CausalHQ</a> just made it super easy to build one up from scratch. WAY easier than my spreadsheets. I am obsessed!</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Matt Mazzeo" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5854624fddb9802ae910_dDMKej74_400x400.jpeg"></p><div><p>Need to build a model? This product is stunning. </p><p>👊👊 <a href="https://twitter.com/CausalHQ">@CausalHQ</a></p></div></div><nav><div><p><img loading="lazy" alt="Matt Mazzeo" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5854624fddb9802ae910_dDMKej74_400x400.jpeg"></p></div><div><p>Need to build a model? This product is stunning. </p><p>👊👊 <a href="https://twitter.com/CausalHQ">@CausalHQ</a></p></div></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="😵" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d57fe9b947476de815164_A0RFkH6r_400x400.jpeg"></p><p>if you use spreadsheets, you should be using <a href="https://twitter.com/CausalHQ">@causalhq</a> instead. don't play yourself!</p></div><nav><div><p><img loading="lazy" alt="😵" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d57fe9b947476de815164_A0RFkH6r_400x400.jpeg"></p></div><p>if you use spreadsheets, you should be using <a href="https://twitter.com/CausalHQ">@causalhq</a> instead. don't play yourself!</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Vithu G. Namasivayam" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5775850eca99cceed0e5_-y-GmQKN_400x400.jpeg"></p><p>Non-codey people that love math should try <a href="https://twitter.com/CausalHQ">@CausalHQ</a>. I feel like a data scientist using it when I am in fact neither a data nor a scientist.</p></div><nav><div><p><img loading="lazy" alt="Vithu G. Namasivayam" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5775850eca99cceed0e5_-y-GmQKN_400x400.jpeg"></p><div><h6>Vithu G. Namasivayam</h6><p>@VithuNamas</p></div></div><p>Non-codey people that love math should try <a href="https://twitter.com/CausalHQ">@CausalHQ</a>. I feel like a data scientist using it when I am in fact neither a data nor a scientist.</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Ben Sehl" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d569f007942bb31e0de7c_2fjfB5K4_400x400.jpeg"></p><p>If you ever do any modeling/forecasting — I seriously can’t recommend <a href="https://twitter.com/CausalHQ">@CausalHQ</a> enough. It is such a great product.</p></div><nav><div><p><img loading="lazy" alt="Ben Sehl" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d569f007942bb31e0de7c_2fjfB5K4_400x400.jpeg"></p></div><p>If you ever do any modeling/forecasting — I seriously can’t recommend <a href="https://twitter.com/CausalHQ">@CausalHQ</a> enough. It is such a great product.</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Colin Plamondon" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5048fb3be6332c51f467_75e17ZHX_400x400.jpeg"></p><div><p>Throwing together probabilistic models w/ <a href="https://twitter.com/CausalHQ">@CausalHQ</a> is futuristic as hell. </p><p>I was writing a project spec last night. CAC vs. price vs. conversion rates - Causal stuff.</p><p>Within 45m I was exploring scenarios in a detailed, data-backed way.</p><p>Craziest part? It embeds in Notion 🤯</p></div></div><nav><div><p><img loading="lazy" alt="Colin Plamondon" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620d5048fb3be6332c51f467_75e17ZHX_400x400.jpeg"></p><div><h6>Colin Plamondon</h6><p>@colinplamondon</p></div></div><div><p>Throwing together probabilistic models w/ <a href="https://twitter.com/CausalHQ">@CausalHQ</a> is futuristic as hell. </p><p>I was writing a project spec last night. CAC vs. price vs. conversion rates - Causal stuff.</p><p>Within 45m I was exploring scenarios in a detailed, data-backed way.</p><p>Craziest part? It embeds in Notion 🤯</p></div></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Sar Haribhakti" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620a83874e2aec5b0fcf284a_bWI63HA1_400x400.jpeg"></p></div><nav><div><p><img loading="lazy" alt="Sar Haribhakti" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620a83874e2aec5b0fcf284a_bWI63HA1_400x400.jpeg"></p></div></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="Kieran McHugh" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620a79fdba6d094410b82e36_YhuBDlxu_400x400.jpeg"></p><p>Preparing a financial model for Daybridge in <a href="https://twitter.com/CausalHQ">@CausalHQ</a> and I must say it’s a phenomenal piece of software with a boatload of potential. Magical to watch things reload in real time. I need to know their tech stack!</p></div><nav><div><p><img loading="lazy" alt="Kieran McHugh" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/620a79fdba6d094410b82e36_YhuBDlxu_400x400.jpeg"></p></div><p>Preparing a financial model for Daybridge in <a href="https://twitter.com/CausalHQ">@CausalHQ</a> and I must say it’s a phenomenal piece of software with a boatload of potential. Magical to watch things reload in real time. I need to know their tech stack!</p></nav></div><div data-hover="false" data-delay="0" role="listitem"><div><p><img loading="lazy" alt="kempsterrrr.eth | (🧱, 🚀) ᵍᵐ" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/6208e9c1f0c89029a13729dd__wosnH7h_400x400.jpeg"></p><div><p>strong recommend <a href="https://twitter.com/CausalHQ">@CausalHQ</a> for interactive financial modelling. for matter any modelling.</p><p>breath of fresh air from trying to do these things in excel</p></div></div><nav><div><p><img loading="lazy" alt="kempsterrrr.eth | (🧱, 🚀) ᵍᵐ" src="https://assets-global.website-files.com/61eff6b3236cf9057b6c1fac/6208e9c1f0c89029a13729dd__wosnH7h_400x400.jpeg"></p><div><h6>kempsterrrr.eth | (🧱, 🚀) ᵍᵐ</h6><p>@kempsterrrr</p></div></div><div><p>strong recommend <a href="https://twitter.com/CausalHQ">@CausalHQ</a> for interactive financial modelling. for matter any modelling.</p><p>breath of fresh air from trying to do these things in excel</p></div></nav></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Java 22 Released (289 pts)]]></title>
            <link>https://mail.openjdk.org/pipermail/jdk-dev/2024-March/008827.html</link>
            <guid>39755471</guid>
            <pubDate>Tue, 19 Mar 2024 13:22:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/008827.html">https://mail.openjdk.org/pipermail/jdk-dev/2024-March/008827.html</a>, See on <a href="https://news.ycombinator.com/item?id=39755471">Hacker News</a></p>
<div id="readability-page-1" class="page">
   
    <b>Mark Reinhold</b> 
    <a href="mailto:jdk-dev%40openjdk.org?Subject=Re%3A%20Java%2022%20/%20JDK%2022%3A%20General%20Availability&amp;In-Reply-To=%3C20240319132004.DAD846C4EC2%40eggemoggin.niobe.net%3E" title="Java 22 / JDK 22: General Availability">mark.reinhold at oracle.com
       </a><br>
    <i>Tue Mar 19 13:20:07 UTC 2024</i>
    <ul>
        <li>Previous message (by thread): <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/008818.html">Tier1 failure: test machine short of memory?
</a></li>
        <li>Next message (by thread): <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/008828.html">Java 22 / JDK 22: General Availability
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/date.html#8827">[ date ]</a>
              <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/thread.html#8827">[ thread ]</a>
              <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/subject.html#8827">[ subject ]</a>
              <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/author.html#8827">[ author ]</a>
         </li>
       </ul>
    <hr>  
<!--beginarticle-->
<pre>JDK 22, the reference implementation of Java 22, is now Generally
Available.  We shipped build 36 as the second Release Candidate of
JDK 22 on 16 February, and no P1 bugs have been reported since then.
Build 36 is therefore now the GA build, ready for production use.

GPL-licensed OpenJDK builds from Oracle are available here:

  <a href="https://jdk.java.net/22">https://jdk.java.net/22</a>

Builds from other vendors will no doubt be available soon.

This release includes twelve JEPs [1], including the final versions of
the Foreign Function &amp; Memory API (454) and Unnamed Variables &amp; Patterns
(456):

  423: Region Pinning for G1
  447: Statements before super(...) (Preview)
  454: Foreign Function &amp; Memory API
  456: Unnamed Variables &amp; Patterns
  457: Class-File API (Preview)
  458: Launch Multi-File Source-Code Programs
  459: String Templates (Second Preview)
  460: Vector API (Seventh Incubator)
  461: Stream Gatherers (Preview)
  462: Structured Concurrency (Second Preview)
  463: Implicitly Declared Classes and Instance Main Methods (Second Preview)
  464: Scoped Values (Second Preview)

This release also includes, as usual, hundreds of smaller enhancements
and thousands of bug fixes.

Thank you to everyone who contributed this release, whether by designing
and implementing features or enhancements, by fixing bugs, or by
downloading and testing the early-access builds!

- Mark


[1] <a href="https://openjdk.org/projects/jdk/22/">https://openjdk.org/projects/jdk/22/</a>
</pre>



<!--endarticle-->
    <hr>
    <ul>
        <!--threads-->
	<li>Previous message (by thread): <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/008818.html">Tier1 failure: test machine short of memory?
</a></li>
	<li>Next message (by thread): <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/008828.html">Java 22 / JDK 22: General Availability
</a></li>
         <li> <b>Messages sorted by:</b> 
              <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/date.html#8827">[ date ]</a>
              <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/thread.html#8827">[ thread ]</a>
              <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/subject.html#8827">[ subject ]</a>
              <a href="https://mail.openjdk.org/pipermail/jdk-dev/2024-March/author.html#8827">[ author ]</a>
         </li>
       </ul>

<hr>
<a href="https://mail.openjdk.org/mailman/listinfo/jdk-dev">More information about the jdk-dev
mailing list</a><br>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Margaret Mead, John von Neumann, and the Prehistory of AI (126 pts)]]></title>
            <link>https://resobscura.substack.com/p/he-spoke-of-computers-with-some-awe</link>
            <guid>39755407</guid>
            <pubDate>Tue, 19 Mar 2024 13:15:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://resobscura.substack.com/p/he-spoke-of-computers-with-some-awe">https://resobscura.substack.com/p/he-spoke-of-computers-with-some-awe</a>, See on <a href="https://news.ycombinator.com/item?id=39755407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>I recently had the chance to discuss my book </span><em><a href="https://www.amazon.com/Tripping-Utopia-Margaret-Troubled-Psychedelic/dp/1538722372?_encoding=UTF8&amp;tag=ro067-20&amp;linkCode=ur2&amp;linkId=ba347f678c5238772f23489f5065baa6&amp;camp=1789&amp;creative=9325" rel="">Tripping on Utopia</a><span> </span></em><span>with the physicist </span><a href="https://en.wikipedia.org/wiki/Sean_M._Carroll" rel="">Sean Carroll</a><span> on his </span><em>Mindscape </em><span>podcast. Typically, Sean’s guests are scientists, but he’s also spoken to a range of interesting people in the humanities (one favorite: Adrienne Mayor on</span><a href="https://www.youtube.com/watch?v=4vCw0Ybew1g" rel=""> “Gods and Robots in Ancient Mythology</a><span>.”)</span></p><p><span>In the episode, which you can listen to </span><a href="https://www.youtube.com/watch?v=Xl-5dDldg1Y" rel="">here</a><span>, I mentioned a detail I encountered in my research that stuck with me. I think it may be the earliest reference to the “simulation hypothesis”: the idea that the observable universe could be a computer simulation. </span><a href="https://en.wikipedia.org/wiki/Simulation_hypothesis" rel="">Wikipedia</a><span> will tell you that this theory dates to 2003. In an unpublished 1968 interview with Margaret Mead in MIT’s archives, however, we find that the polymathic Hungarian scientist John von Neumann was apparently toying with the idea in the years just after World War II. </span></p><p><span>I’m currently on paternity leave, and have been spending many an hour rocking our newborn daughter to sleep while reading on a Kindle. Lately, that reading has been in Benjamín Labutut’s </span><em><a href="https://www.penguinrandomhouse.com/books/725022/the-maniac-by-benjamin-labatut/" rel="">The MANIAC</a><span> </span></em><span>—&nbsp;an experimental novel about von Neumann told in bite-sized historical vignettes. </span></p><p>This fascinating book, and the conversation with Sean, made me decide to dig up my photographs of the original interview and transcribe them below. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png" width="669" height="846.8179945054945" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1843,&quot;width&quot;:1456,&quot;resizeWidth&quot;:669,&quot;bytes&quot;:1601301,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc2b65dc5-6836-4587-b775-e862a68f9ef7_1537x1946.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><span>A list of attendees of the final Macy cybernetics conference that I made for </span><em>Tripping on Utopia</em><span>.</span></figcaption></figure></div><p><span>In this 1968 interview with a historian of science named Steve Heims, Mead remembers the </span><a href="https://www.asc-cybernetics.org/foundations/history/MacySummary.htm" rel="">Macy cybernetics conferences</a><span>, which brought together the key social and intellectual group involved in what might be called the “prehistory of AI.” The attendees included von Neumann, Claude Shannon, and Mead herself. Interestingly, there were also several important early psychedelic researchers like </span><a href="https://en.wikipedia.org/wiki/Heinrich_Kl%C3%BCver" rel="">Heinrich Klüver</a><span> (</span><em>Tripping on Utopia </em><span>digs into the historical significance of this odd convergence). </span></p><p>When I returned to the original source, I was struck by how prescient Mead was in discussing not just the simulation hypothesis, but also the question of self-improving AI systems. Keep in mind, although her interview was from 1968, she’s discussing events from the 1948-1950 period, when digital computers were just a few years old. </p><p>It’s a strikingly interesting historical document that deserves to be more widely read — and, I hope, discussed in the comments below. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png" width="1456" height="1175" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1175,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4949477,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb6de6e0-5b51-4eed-96e9-45aca3ed3251_1844x1488.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>A photograph of the actual transcript, which can be found in the </span><a href="https://archivesspace.mit.edu/repositories/2/resources/857" rel="">Steve J. Heims</a><span> papers at MIT’s Special Collections and Archives. </span></figcaption></figure></div><p>In this excerpt from the transcript, Mead is recalling that the final two Cybernetics conferences (in 1952 and 1953) did not go well. Von Neumann did not attend. But he did have drinks, she recalls, with her third husband Gregory Bateson. Mead then discusses von Neumann’s previous cybernetics conferences appearances, in 1947, 1948, and 1950, and the heady ideas that emerged from them:</p><blockquote><p><strong>STEVE HEIMS</strong><span>: It puzzled me that he was not even listed as a member the last two times.</span></p><p><strong>MARGARET MEAD</strong><span>: The last two times, but Gregory [Bateson] had lunch with him.</span></p><p><strong>H</strong><span>: With von Neumann?</span></p><p><strong>M</strong><span>: With von Neumann. During that conference, and I remember he said, that he said that he said to von Neumann, one thing we do know, that however the brain works, it doesn't work like one of these computers. Von Neumann agreed with him. I mean, I remember just having that reported to me.</span></p><p><strong>H</strong><span>: You know, that’s interesting. I wonder if, if everybody in the conference when it was all over would agree with that.</span></p><p><strong>M</strong><span>: Well, you see, a lot of us never thought that. We simply thought that this was a form of thought. A model building, out of which you could think in a cross disciplinary style. We didn’t think we were making models of the brain. We thought we were developing a cross-disciplinary language which already had a sufficiently good mathematical base so that it might be a viable cross-disciplinary language. Now, that was the major point. I mean, </span><a href="https://en.wikipedia.org/wiki/Norbert_Wiener" rel="">[Norbert] Wiener</a><span> wasn't primarily interested in making a model of the brain either. He was interested in thinking about systems of this order. But, of course, von Neumann majorly represented, the computer position in the conference. And </span><strong>he would sort of say, you know, we could be being run by the computer right now, it would be possible to conceive a computer that was running, the whole of this, you didn't say planet in those days, but whatever was the current term for this earth, it would be possible to conceive of a computer that could be doing it.</strong><span> Or, you know, you can make big computers that can have little computers. The computer that designed a computer, was still not really operating, I think, at this period. You could check that, but I don't think so, I think that it was a second generation, the computer they called </span><a href="https://www.google.com/books/edition/The_Control_Data_Corporation_s_Early_Sys/7NDQEAAAQBAJ?hl=en&amp;gbpv=1&amp;dq=%22univac+countess%22&amp;pg=PA65&amp;printsec=frontcover" rel="">the Countess</a><span> which was a computer made in Minneapolis by a branch of Remington Rand, I think, for the Navy. And about that time I began hearing accounts of, that you could program one computer to make a better computer than it was.</span></p><p><strong>H: </strong><span>It don't happen very often, in the discussion, that basic issue which, I think, would bother people, unless they had thought it through before hand —</span></p><p><strong>M</strong><span>: </span><strong>No,</strong><span> </span><strong>we weren’t the kind of people that were bothered by that sort of thing. This fear of computers. It is a European fear on the whole. And Americans don't have it, and, von Neuman was the closest to having it. This was when he had three or four drinks. He spoke of computers with some awe. And the real distinction is the people who feel awe for computers. They’re nuts</strong><span>. And if you don’t feel awe for computers, you say, “Look in box 37, that's where the trouble’s likely to be.” Which is the typical American approach to a computer.</span></p><p><strong>H</strong><span>: It has to do with being, feeling that you can repair, that you can work with it?</span></p><p><strong>M:</strong><span> You know that you’re in control of it. The American attitude toward the machine is that it's something we make and it’s something we can fix. In fact, American men like it in a state of continual breakdown so they can fix it, I’m inclined to think. But, all the way, from sort of, futuristic and cubistic, kind of painting and attitudes in World War I, and after World War I, in Europe, [there’s] this fear of the machine. And either the dynamism of the machine so you and your plane dive to death, or some nonsense, or, that the machine was going to take over, was much stronger. But in the average American, this is not [the case]. And most of these people were Americans.</span></p></blockquote><p><span>One of the reasons I spent the past few years researching and thinking about Mead is that I find so many present-day concerns emerging in a very early form in her thought and her social world. Here, to my surprise, is an early —&nbsp;</span><em>very </em><span>early! — commentary on what has since become the central divide in global AI policy. For the </span><a href="https://www.newyorker.com/tech/annals-of-technology/what-can-america-learn-from-europe-about-regulating-big-tech" rel="">past few years</a><span>, </span><a href="https://www.nytimes.com/2023/12/06/technology/ai-regulation-policies.html" rel="">Europe has been following a very different path</a><span> when it comes to regulating digital technologies. Whether that’s because of a neo-Futurist, post-WWI legacy of awestruck fear toward technology is another story, of course. But regardless, I’d say Mead was eerily ahead of her time in terms of even </span><em>thinking </em><span>about such things back in the 1950s. </span></p><p><span>I am writing with baby Nava sleeping beside me, which explains the one month break I’ve taken from writing this newsletter. I plan on getting back into a weekly posting cadence in April. In the meantime, I am enjoying reading </span><em>The MANIAC </em><span>(which, incidentally, makes for a great companion piece to </span><em>Oppenheimer</em><span>). </span></p><p>As I read on, I’ll be imagining what else John von Neumann and Margaret Mead talked about after four cocktails. </p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png" width="158" height="34" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:34,&quot;width&quot;:158,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4426,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F495d8601-9fa5-4dee-9a54-957bc0401db8_158x34.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>•&nbsp;“One of the big turning points in my life was a meeting with Enrico Fermi in the spring of 1953. In a few minutes, Fermi politely but ruthlessly demolished a programme of research that my students and I had been pursuing for several years. He probably saved us from several more years of fruitless wandering along a road that was leading nowhere. I am eternally grateful to him for destroying our illusions.” (Freeman Dyson </span><a href="https://lilith.fisica.ufmg.br/~dsoares/fdyson.htm" rel="">remembering the moment</a><span> when Enrico Fermi changed his life)</span></p><p><span>•&nbsp;“Completed notebooks from the Sol Ross desk are kept at the Archives of the Big Bend... Locals believe that the current desk is the fourth one to occupy this site.” (</span><a href="https://www.atlasobscura.com/places/sul-ross-desk-alpine-texas" rel="">Atlas Obscura</a><span>)</span></p><p><span>•&nbsp;“The Institute for Illegal Images” — an</span><a href="https://www.theparisreview.org/blog/2024/03/04/the-institute-for-illegal-images/" rel=""> excerpt in </a><em><a href="https://www.theparisreview.org/blog/2024/03/04/the-institute-for-illegal-images/" rel="">The Paris Review</a><span> </span></em><span>from Erik Davis’s forthcoming book </span><em><a href="https://mitpress.mit.edu/9780262048507/blotter/" rel="">Blotter: The Untold Story of an Acid Medium</a></em><span> (MIT Press, 2024).</span></p><p><span>•&nbsp;</span><em><a href="https://en.wikipedia.org/wiki/Antarctic_English" rel="">Antarctic English</a><span> features various words that are not used in other varieties of English. Differences in vocabulary include:</span></em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png" width="543" height="456.3510638297872" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:790,&quot;width&quot;:940,&quot;resizeWidth&quot;:543,&quot;bytes&quot;:96972,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c3f4d3-c01b-431e-ab7c-d72463afecd1_940x790.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>If you received this email, it means you signed up for the Res Obscura newsletter, written by me, </span><a href="https://benjaminpbreen.com/" rel="">Benjamin Breen</a><span>. I started Res Obscura (“a hidden thing” in Latin) to communicate my passion for the actual experience of </span><em>doing</em><span> history. Usually that means digging into historical primary sources, in all their strange glory.</span></p><p>If you liked this post, please consider forwarding it to friends. I’d also love to hear from you in the comments below. </p><p data-attrs="{&quot;url&quot;:&quot;https://resobscura.substack.com/p/he-spoke-of-computers-with-some-awe?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://resobscura.substack.com/p/he-spoke-of-computers-with-some-awe?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[USB hubs, printers, Java, and more seemingly broken by macOS 14.4 update (168 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2024/03/usb-hubs-printers-java-and-more-seemingly-broken-by-macos-14-4-update/</link>
            <guid>39755358</guid>
            <pubDate>Tue, 19 Mar 2024 13:10:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2024/03/usb-hubs-printers-java-and-more-seemingly-broken-by-macos-14-4-update/">https://arstechnica.com/gadgets/2024/03/usb-hubs-printers-java-and-more-seemingly-broken-by-macos-14-4-update/</a>, See on <a href="https://news.ycombinator.com/item?id=39755358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      pobody's nerfect    —
</h4>
            
            <h2 itemprop="description">Issues seem to be related to security fixes made in Apple's latest OS.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/09/sonoma-light-800x500.jpg" alt="USB hubs, printers, Java, and more seemingly broken by macOS 14.4 update">
      <figcaption></figcaption>  </figure>

  




<!-- cache hit 44:single/related:9eaea15ff7b90fe849d8bb6d3fe64379 --><!-- empty -->
<p>A couple of weeks ago, Apple <a href="https://arstechnica.com/gadgets/2024/03/visionos-1-1-tries-to-make-personas-less-unsettling-plus-other-apple-os-updates/">released macOS Sonoma 14.4</a> with the usual list of bug fixes, security patches, and a couple of minor new features. Since then, users and companies have been complaining of a long list of incompatibilities, mostly concerning broken external accessories like USB hubs and printers&nbsp;but also extending to software like Java.</p>
<p>MacRumors has <a href="https://www.macrumors.com/2024/03/18/do-not-update-macos-sonoma-14-4/">a good rundown</a> of the list of issues, which has been steadily getting longer as people have run into more problems. It started with reports of malfunctioning USB hubs, sourced from users on <a href="https://www.reddit.com/r/MacOS/comments/1b9volo/usb_hub_on_monitor_no_longer_working_on_macbook/">Reddit</a>, the <a href="https://discussions.apple.com/thread/255518397?answerId=260266531022&amp;sortBy=best#260266531022">Apple Support Communities forums</a>, and <a href="https://forums.macrumors.com/threads/sonoma-14-4-breaks-usb-c-monitor-with-hubs.2421678/">elsewhere</a>—USB hubs built into various displays stopped functioning for Mac users after the 14.4 update.</p>

<p>Other issues surfaced in the days after people started reporting problems with their USB hubs, including <a href="https://discussions.apple.com/thread/255522015?sortBy=best">some instances</a> of broken printer drivers, <a href="https://blogs.oracle.com/java/post/java-on-macos-14-4">unexpected app crashes</a> for some Java users, and problems launching apps that rely on the PACE anti-piracy software (<a href="https://help.ilok.com/faq_licenses.html#sonoma_14_4_plugins">and iLok hardware dongles</a>) to authenticate.</p>
<p>At least some of the problems seem localized to Apple Silicon Macs. In fact, iLok recommends running digital audio software in Rosetta mode as a temporary stopgap while Apple works on a fix. According to iLok, Apple has acknowledged this particular bug and is working on an update, but "[has] not indicated a timeline."</p>                                            
                                                        
<p>The USB hub issue may be related to the USB security prompts that Apple <a href="https://arstechnica.com/gadgets/2022/10/macos-13-ventura-the-ars-technica-review/16/#h2">introduced in macOS 13 Ventura</a>, asking users to confirm whether they wanted to connect to USB-C accessories that they were connecting to their Mac for the first time. Some users have been able to <a href="https://discussions.apple.com/thread/255518397?answerId=255518397021#255518397021">get their USB hubs working again</a> after the 14.4 update by making macOS request permission to connect to the accessory every time the accessory is plugged in; the default behavior is supposed to recognize USB devices that you've already connected to once.</p>
<p>Scanning Apple's <a href="https://developer.apple.com/documentation/macos-release-notes/macos-14_4-release-notes">release notes</a> or <a href="https://support.apple.com/en-us/HT214084">security update disclosures</a> for the update doesn't reveal any smoking guns, but many of the security bugs were addressed with "improved checks" and "improved access permissions," and it's certainly possible that some legitimate accessories and software were broken by one or more of these changes. <a href="https://blogs.oracle.com/java/post/java-on-macos-14-4">The Oracle blog post about the Java problems</a> refers to memory access issues that seem to be causing the crashes, though that may or may not explain the problems people are having with external accessories. The blog post also indicates that these bugs weren't present in the public developer betas of macOS 14.4.</p>
<p>My desktop M2 Mac Studio setup, which is connected to a 4K Gigabyte M28U with a built-in USB hub, hasn't exhibited any unusual behavior since the update, so it's also possible that these issues aren't affecting every user of every Mac. If you haven't updated yet, it may be worth waiting until Apple releases fixes for some or all of these issues, even if you don't think you'll be affected.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Astronaut Thomas Stafford has died (157 pts)]]></title>
            <link>https://apnews.com/article/apollo-10-astronaut-tom-stafford-18600e218bd145ce99a3605b35df7b8c</link>
            <guid>39755267</guid>
            <pubDate>Tue, 19 Mar 2024 13:00:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnews.com/article/apollo-10-astronaut-tom-stafford-18600e218bd145ce99a3605b35df7b8c">https://apnews.com/article/apollo-10-astronaut-tom-stafford-18600e218bd145ce99a3605b35df7b8c</a>, See on <a href="https://news.ycombinator.com/item?id=39755267">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                        <p>WASHINGTON (AP) — Astronaut Thomas P. Stafford, who commanded a dress rehearsal flight for the 1969 moon landing and the first U.S.-Soviet space linkup, died Monday. He was 93. </p><p>Stafford, a retired Air Force three-star general, took part in four space missions. Before Apollo 10, he flew on two Gemini flights, including the first rendezvous of two U.S. capsules in orbit. He died in a hospital near his Space Coast Florida home, said Max Ary, director of the <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://www.staffordmuseum.org/" target="_blank" rel="noopener">Stafford Air &amp; Space Museum</a></span> in Weatherford, Oklahoma.</p><p>Stafford was one of 24 NASA astronauts who flew to the moon, but he did not land on it. Only seven of them are still alive.</p><p>“Today General Tom Stafford went to the eternal heavens which he so courageously explored as a Gemini and Apollo astronaut as well as a peacemaker in Apollo Soyuz,” NASA Administrator Bill Nelson said via X, formerly known as Twitter. “Those of us privileged to know him are very sad but grateful we knew a giant.”</p>
    

<p>After he put away his flight suit, Stafford was the go-to guy for NASA when it sought independent advice on everything from human Mars missions to safety issues to returning to flight after the 2003 space shuttle Columbia accident. He chaired an oversight group that looked into how to fix the then-flawed Hubble Space Telescope, earning a NASA public service award.</p>



<p>“Tom was involved in so many things that most people were not aware of, such as being known as the ‘Father of Stealth’,” Ary said in an email. Stafford was in charge of the famous “Area 51” desert base that was the site of many UFO theories, but the home of testing of Air Force stealth technologies.</p>
    
<p>The Apollo 10 mission in May 1969 set the stage for Apollo 11’s historic mission two months later. Stafford and Gene Cernan took the lunar lander nicknamed Snoopy within 9 miles (14 kilometers) of the moon’s surface. Astronaut John Young stayed behind in the main spaceship dubbed Charlie Brown.</p>
    

<p>“The most impressive sight, I think, that really changed your view of things is when you first see Earth,” Stafford recalled in a 1997 oral history, talking about the view from lunar orbit.</p><p>Then came the moon’s far side: “The Earth disappears. There’s this big black void.” </p><p>Apollo 10’s return to Earth set the world’s record for fastest speed by a crewed vehicle at 24,791 mph (39,897 kph).</p><p>After the moon landings ended, NASA and the Soviet Union decided on a joint docking mission and Stafford, a one-star general at the time, was chosen to command the American side. It meant intensive language training, being followed by the KGB while in the Soviet Union, and lifelong friendships with cosmonauts. The two teams of space travelers even went to Disney World and rode Space Mountain together before going into orbit and joining ships.</p><p>“We have capture,” Stafford radioed in Russian as the Apollo and Soyuz spacecraft hooked up. His Russian counterpart, <span><a data-gtm-enhancement-style="LinkEnhancementA" href="https://apnews.com/article/bccf6f678de1469382936ccd0e202e9e">Alexei Leonov</a></span>, responded in English: “Well done, Tom, it was a good show. I vote for you.”</p><p>The 1975 mission included two days during which the five men worked together on experiments. After, the two teams toured the world together, meeting President Gerald Ford and Soviet leader Leonid Brezhnev.</p>
    

<p>“It helped prove to the rest of the world that two completely opposite political systems could work together,” Stafford recalled at a 30th anniversary gathering in 2005.</p><p>The two crews became so close that years later Leonov arranged for Stafford to be able to adopt two Russian boys when Stafford was in his 70s.</p><p>“We are too old to adopt, but they were too old to be adopted,” Stafford told The Oklahoman in 2004. “They just added so much meaning to our life, and just because you’re retiring doesn’t mean you don’t have anything left to give.”</p><p>Later, Stafford was a central part of discussions in the 1990s that brought Russia into the partnership building and operating the International Space Station.</p><p>Growing up in Weatherford, Oklahoma, Stafford said he would look up and see giant DC-3 airplanes fly overhead on early transcontinental routes. </p>
    

<p>“I wanted to fly since I was 5 or 6 years old seeing those airplanes,” he told NASA historians.</p><p>Stafford went to the U.S. Naval Academy where he graduated in the top 1% of his class and flew in the backseat of some airplanes and loved it. He volunteered for the Air Force and had hoped to fly combat in the Korean War. But by the time he got his wings, the war ended. He went to the Air Force’s experimental test pilot school, graduated first in his class there and stayed on as an instructor.</p><p>In 1962, NASA selected Stafford for its second set of astronauts, which included Neil Armstrong, Frank Borman and Pete Conrad.</p><p>Stafford was assigned along with Wally Schirra to Gemini 6. Their original mission was to rendezvous with an empty spaceship. But their 1965 launch was scrubbed when the spaceship exploded soon after liftoff. NASA improvised and in December, Gemini 6 rendezvoused with but didn’t dock with two astronauts aboard Gemini 7.</p>
    

<p>Stafford’s next flight in 1966 was with Cernan on Gemini 9. Cernan’s spacewalk, connected to a jet-pack like device, didn’t go well. Cernan complained that the sun and machine made him extra hot and hurt his back. Then his visor fogged up and he couldn’t see.</p><p>“Call it quits, Gene. Get out of there,” Stafford, the commander, told Cernan. Stafford talked him back in, saying “move your hand over, start to float up ... stick your hand up ... just walk hand over hand.”</p><p>In all, Stafford logged 507 hours in space and flew four different types of spacecraft and 127 types of aircraft and helicopters.</p><p>After the Apollo-Soyuz mission, Stafford returned to the Air Force and worked in research and commanded the Air Force Flight Test Center before retiring in 1979 as a three-star general.</p><p>Stafford’s Air Force duties not only had him run the military’s top flight school and experimental plane testing base, but he was commanding general of Area 51. A biography from his museum said, that while Stafford was in charge of Area 51 and later as the development and acquisition chief at the Pentagon he “wrote the specs and established the program that led to the development of the F-117 Stealth Fighter, and later, the B-2 Stealth Bomber.”</p><p>Stafford became an executive for an Oklahoma-based transportation company and later moved to Florida, near Cape Canaveral.</p><p>He is survived by his wife. Linda, two sons, two daughters and two stepchildren, according to the museum.</p>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Not sure you're talking to a human? Create a human check (166 pts)]]></title>
            <link>https://r-u-human.com/</link>
            <guid>39755084</guid>
            <pubDate>Tue, 19 Mar 2024 12:39:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://r-u-human.com/">https://r-u-human.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39755084">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Inside the Massive Alleged AT&T Data Breach (230 pts)]]></title>
            <link>https://www.troyhunt.com/inside-the-massive-alleged-att-data-breach/</link>
            <guid>39754330</guid>
            <pubDate>Tue, 19 Mar 2024 10:19:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.troyhunt.com/inside-the-massive-alleged-att-data-breach/">https://www.troyhunt.com/inside-the-massive-alleged-att-data-breach/</a>, See on <a href="https://news.ycombinator.com/item?id=39754330">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
<p>I hate having to use that word - "alleged" - because it's so inconclusive and I know it will leave people with many unanswered questions. But sometimes, "alleged" is just where we need to begin and over the course of time, proper attribution is made and the dots are joined. We're here at "alleged" for two very simple reasons: one is that AT&amp;T is saying "the data didn't come from us", and the other is that I have no way of proving otherwise. But I have proven, with sufficient confidence, that the data is real and the impact is significant. Let me explain:</p><p>Firstly, just as a primer if you're new to this story, <a href="https://www.bleepingcomputer.com/news/security/att-says-leaked-data-of-70-million-people-is-not-from-its-systems/?ref=troyhunt.com" rel="noreferrer">read BleepingComputer's piece on the incident</a>. What it boils down to is in August 2021, someone with a proven history of breaching large organisations <a href="https://www.bleepingcomputer.com/news/security/atandt-denies-data-breach-after-hacker-auctions-70-million-user-database/?ref=troyhunt.com" rel="noreferrer">posted what they claimed were 70 million AT&amp;T records to a popular hacking forum</a> and asked for a very large amount of money should anyone wish to purchase the data. From that story:</p><blockquote>From the samples shared by the threat actor, the database contains customers' names, addresses, phone numbers, Social Security numbers, and date of birth.</blockquote><p>Fast forward two and a half years and the successor to this forum saw a post this week alleging to contain the entire corpus of data. Except that rather than put it up for sale, someone has decided to just dump it all publicly and make it easily accessible to the masses. This isn't unusual: "fresh" data has much greater commercial value and is often tightly held for a long period before being released into the public domain. The Dropbox and LinkedIn breaches, for example, occurred in 2012 before being broadly distributed in 2016 and just like those incidents, the alleged AT&amp;T data is now in <em>very</em> broad circulation. It is undoubtedly in the hands of thousands of internet randos.</p><p>AT&amp;T's position on this is pretty simple:</p><blockquote>AT&amp;T continues to tell BleepingComputer today that they still see no evidence of a breach in their systems and still believe that this data did not originate from them.</blockquote><p>The old adage of "absence of evidence is not evidence of absence" comes to mind (just because they can't find evidence of it doesn't mean it didn't happen), but as I said earlier on, I (and others) have so far been unable to prove otherwise. So, let's focus on what we <em>can</em> prove, starting with the accuracy of the data.</p><p>The linked article talks about the author verifying the data with various people he knows, as well as other well-known infosec identities verifying its accuracy. For my part, I've got 4.8M <a href="https://haveibeenpwned.com/?ref=troyhunt.com" rel="noreferrer">Have I Been Pwned</a> (HIBP) subscribers I can lean on to assist with verification, and it turns out that 153k of them are in this data set. What I'll typically do in a scenario like this is reach out to the 30 newest subscribers (people who will hopefully recall the nature of HIBP from their recent memory), and ask them if they're willing to assist. I linked to the story from the beginning of this blog post and got a handful of willing respondents for whom I sent their data and asked two simple questions:</p><ol><li>Does this data look accurate?</li><li>Are you an AT&amp;T customer and if not, are you a customer of another US telco?</li></ol><p>The first reply I received was simple, but emphatic:</p><figure><img src="https://pbs.twimg.com/media/GI_kjAfbQAA1eHV?format=jpg&amp;name=900x900" alt="Image" loading="lazy" width="880" height="184"></figure><p>This individual had their name, phone number, home address and most importantly, their social security number exposed. Per the linked story, social security numbers and dates of birth exist on most rows of the data in encrypted format, but two supplemental files expose these in plain text. Taken at face value, it looks like whoever snagged this data also obtained the private encryption key and simply decrypted the vast bulk (but not all of) the protected values.</p><figure><img src="https://pbs.twimg.com/media/GI_kxrkbYAAzxjz?format=jpg&amp;name=large" alt="Image" loading="lazy" width="1290" height="290"></figure><p>The above example simply didn't have plain text entries for the encrypted data. Just by way of raw numbers, the file that aligns with the "70M" headline actually has 73,481,539 lines with 49,102,176 unique email addresses. The file with decrypted SSNs has 43,989,217 lines and the decrypted dates of birth file only has 43,524 rows. The last file, for example, has rows that look just like this:</p>

<pre><code>.encrypted_value='*0g91F1wJvGV03zUGm6mBWSg==' .decrypted_value='1996-07-18'</code></pre>

<p>That encrypted value is precisely what appears in the large file hence providing an easy way of matching all the data together. But those numbers also obviously mean that not every impacted individual had their SSN exposed, and <em>most</em> individuals didn't have their date of birth leaked.</p><figure><img src="https://pbs.twimg.com/media/GI_xf24asAEPboF?format=jpg&amp;name=medium" alt="Image" loading="lazy" width="1188" height="495"></figure><p>As I'm fond of saying, there's only one thing worse than your data appearing on the dark web: it's appearing on the clear web. And that's precisely where it is; the forum this was posted to isn't within the shady underbelly of a Tor hidden service, it's out there in plain sight on a public forum easily accessed by a normal web browser. And the data is real.</p><p>That last response is where most people impacted by this will now find themselves - "what do I do?" Usually I'd tell them to get in touch with the impacted organisation and request a copy of their data from the breach, but if AT&amp;T's position is that it didn't come from them then they may not be much help. (Although if you are a current or previous customer, you can certainly request a copy of your personal information regardless of this incident.) I've personally also used identity theft protection services since as far back as the 90's now, simply to know when actions such as credit enquiries appear against my name. In the US, this is what services like <a href="https://www.aura.com/identity-theft-protection?ref=troyhunt.com" rel="noreferrer">Aura</a> do and it's become common practice for breached organisations to provide identity protection subscriptions to impacted customers (full disclosure: Aura is a previous sponsor of this blog, although we have no ongoing or upcoming commercial relationship).</p><p>What I can't do is send you your breached data, or an indication of what fields you had exposed. Whilst I did this in that handful of aforementioned cases as part of the breach verification process, this is something that happens entirely manually and is infeasible en mass. HIBP only ever stores email addresses and never the additional fields of personal information that appear in data breaches. In case you're wondering why that is, we got a solid reminder only a couple of months ago when <a href="https://www.troyhunt.com/the-data-breach-personal-stash-ecosystem/" rel="noreferrer">a service making this sort of data available to the masses had an incident that exposed tens of <em>billions</em> of rows of personal information</a>. That's just an unacceptable risk for which the old adage of "you cannot lose what you do not have" provides the best possible fix.</p><p>As I said in the intro, this is not the conclusive end I wanted for this blog post... yet. As impacted HIBP subscribers receive their notifications and particularly as those monitoring domains learn of the aliases in the breach (many domain owners use unique aliases per service they sign up to), we may see a more conclusive outcome to this incident. That may not necessarily be confirmation that the data did indeed originate from AT&amp;T, it could be that it came from a third party processor they use or from another entity altogether that's entirely unrelated. The truth is somewhere there in the data, I'll add any relevant updates to this blog post if and when it comes out.</p><p>As of now, all 49M impacted email addresses are <a href="https://haveibeenpwned.com/?ref=troyhunt.com" rel="noreferrer">searchable within HIBP</a>.</p>
<section>
<a href="https://www.troyhunt.com/tag/have-i-been-pwned-3f/">Have I Been Pwned</a>
<a href="https://www.troyhunt.com/tag/security/">Security</a>
</section>
</section><div>
<section>
<a href="https://twitter.com/share?text=Troy%20Hunt%3A%20Inside%20the%20Massive%20Alleged%20AT%26T%20Data%20Breach&amp;url=https://www.troyhunt.com/inside-the-massive-alleged-att-data-breach/"><i></i> Tweet</a>
<a href="https://www.facebook.com/sharer/sharer.php?u=https://www.troyhunt.com/inside-the-massive-alleged-att-data-breach/"><i></i> Post</a>
<a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.troyhunt.com/inside-the-massive-alleged-att-data-breach/"><i></i> Update</a>
<a href="mailto:?subject=Troy%20Hunt%3A%20Inside%20the%20Massive%20Alleged%20AT%26T%20Data%20Breach&amp;body=https://www.troyhunt.com/inside-the-massive-alleged-att-data-breach/"><i></i> Email</a>
<a href="https://feeds.feedburner.com/TroyHunt"> RSS</a>
</section>
<div>
<h5 itemprop="author" itemscope="" itemtype="http://schema.org/Person">Troy Hunt</h5>
<p>Hi, I'm Troy Hunt, I write this blog, create courses for Pluralsight and am a Microsoft Regional Director and MVP who travels the world speaking at events and training technology professionals <a href="https://www.troyhunt.com/about"></a></p>
</div>




</div></div>]]></description>
        </item>
    </channel>
</rss>